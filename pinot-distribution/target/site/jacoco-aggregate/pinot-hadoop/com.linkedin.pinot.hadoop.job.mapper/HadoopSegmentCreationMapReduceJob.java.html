<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>HadoopSegmentCreationMapReduceJob.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">pinot-distribution</a> &gt; <a href="../index.html" class="el_bundle">pinot-hadoop</a> &gt; <a href="index.source.html" class="el_package">com.linkedin.pinot.hadoop.job.mapper</a> &gt; <span class="el_source">HadoopSegmentCreationMapReduceJob.java</span></div><h1>HadoopSegmentCreationMapReduceJob.java</h1><pre class="source lang-java linenums">/**
 * Copyright (C) 2014-2016 LinkedIn Corp. (pinot-core@linkedin.com)
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.linkedin.pinot.hadoop.job.mapper;

import com.linkedin.pinot.common.data.Schema;
import com.linkedin.pinot.common.utils.TarGzCompressionUtils;
import com.linkedin.pinot.core.data.readers.CSVRecordReaderConfig;
import com.linkedin.pinot.core.data.readers.FileFormat;
import com.linkedin.pinot.core.data.readers.RecordReaderConfig;
import com.linkedin.pinot.core.data.readers.ThriftRecordReaderConfig;
import com.linkedin.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
import com.linkedin.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
import com.linkedin.pinot.hadoop.job.JobConfigConstants;

import org.apache.commons.io.FileUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.IOException;


<span class="nc" id="L43">public class HadoopSegmentCreationMapReduceJob {</span>

<span class="nc" id="L45">  public static class HadoopSegmentCreationMapper extends Mapper&lt;LongWritable, Text, LongWritable, Text&gt; {</span>
<span class="nc" id="L46">    private static Logger LOGGER = LoggerFactory.getLogger(HadoopSegmentCreationMapper.class);</span>
    private Configuration _properties;

    private String _inputFilePath;
    private String _outputPath;
    private String _tableName;
    private String _postfix;

    private Path _currentHdfsWorkDir;
    private String _currentDiskWorkDir;

    // Temporary HDFS path for local machine
    private String _localHdfsSegmentTarPath;

    private String _localDiskSegmentDirectory;
    private String _localDiskSegmentTarPath;

    @Override
    public void setup(Context context) throws IOException, InterruptedException {


<span class="nc" id="L67">      _currentHdfsWorkDir = FileOutputFormat.getWorkOutputPath(context);</span>
<span class="nc" id="L68">      _currentDiskWorkDir = &quot;pinot_hadoop_tmp&quot;;</span>

      // Temporary HDFS path for local machine
<span class="nc" id="L71">      _localHdfsSegmentTarPath =  _currentHdfsWorkDir + &quot;/segmentTar&quot;;</span>
<span class="nc" id="L72">      _localDiskSegmentTarPath = _currentDiskWorkDir + &quot;/segmentsTar&quot;;</span>



<span class="nc" id="L76">      new File(_localDiskSegmentTarPath).mkdirs();</span>

<span class="nc" id="L78">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L79">      LOGGER.info(&quot;Configurations : {}&quot;, context.getConfiguration().toString());</span>
<span class="nc" id="L80">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L81">      LOGGER.info(&quot;Current HDFS working dir : {}&quot;, _currentHdfsWorkDir);</span>
<span class="nc" id="L82">      LOGGER.info(&quot;Current DISK working dir : {}&quot;, new File(_currentDiskWorkDir).getAbsolutePath());</span>
<span class="nc" id="L83">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L84">      _properties = context.getConfiguration();</span>

<span class="nc" id="L86">      _outputPath = _properties.get(&quot;path.to.output&quot;);</span>
<span class="nc" id="L87">      _tableName = _properties.get(&quot;segment.table.name&quot;);</span>
<span class="nc" id="L88">      _postfix = _properties.get(&quot;segment.name.postfix&quot;, null);</span>

<span class="nc bnc" id="L90" title="All 4 branches missed.">      if (_outputPath == null || _tableName == null) {</span>
<span class="nc" id="L91">        throw new RuntimeException(</span>
            &quot;Missing configs: &quot; +
                &quot;\n\toutputPath: &quot; +
                _properties.get(&quot;path.to.output&quot;) +
                &quot;\n\ttableName: &quot; +
                _properties.get(&quot;segment.table.name&quot;));
      }
<span class="nc" id="L98">    }</span>

    protected String getTableName() {
<span class="nc" id="L101">      return _tableName;</span>
    }

    @Override
    public void cleanup(Context context) throws IOException, InterruptedException {
<span class="nc" id="L106">      FileUtils.deleteQuietly(new File(_currentDiskWorkDir));</span>
<span class="nc" id="L107">    }</span>

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

<span class="nc" id="L112">      String line = value.toString();</span>
<span class="nc" id="L113">      String[] lineSplits = line.split(&quot; &quot;);</span>

<span class="nc" id="L115">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L116">      LOGGER.info(&quot;mapper input : {}&quot;, value);</span>
<span class="nc" id="L117">      LOGGER.info(&quot;PATH_TO_OUTPUT : {}&quot;, _outputPath);</span>
<span class="nc" id="L118">      LOGGER.info(&quot;TABLE_NAME : {}&quot;, _tableName);</span>
<span class="nc" id="L119">      LOGGER.info(&quot;num lines : {}&quot;, lineSplits.length);</span>

<span class="nc bnc" id="L121" title="All 2 branches missed.">      for (String split : lineSplits) {</span>
<span class="nc" id="L122">        LOGGER.info(&quot;Command line : {}&quot;, split);</span>
      }
<span class="nc" id="L124">      LOGGER.info(&quot;*********************************************************************&quot;);</span>

<span class="nc bnc" id="L126" title="All 2 branches missed.">      if (lineSplits.length != 3) {</span>
<span class="nc" id="L127">        throw new RuntimeException(&quot;Input to the mapper is malformed, please contact the pinot team&quot;);</span>
      }
<span class="nc" id="L129">      _inputFilePath = lineSplits[1].trim();</span>

<span class="nc" id="L131">      String segmentDirectory = _tableName + &quot;_&quot; + Integer.parseInt(lineSplits[2]);</span>
<span class="nc" id="L132">      _localDiskSegmentDirectory = _currentDiskWorkDir + &quot;/segments/&quot; + segmentDirectory;</span>

      // To inherit from from the Hadoop Mapper class, you can't directly throw a general exception.
      Schema schema;
<span class="nc" id="L136">      final FileSystem fs = FileSystem.get(new Configuration());</span>
<span class="nc" id="L137">      final Path hdfsAvroPath = new Path(_inputFilePath);</span>
<span class="nc" id="L138">      final File dataPath = new File(_currentDiskWorkDir, &quot;data&quot;);</span>
      try {
<span class="nc bnc" id="L140" title="All 2 branches missed.">        if (dataPath.exists()) {</span>
<span class="nc" id="L141">          dataPath.delete();</span>
        }
<span class="nc" id="L143">        dataPath.mkdir();</span>

<span class="nc" id="L145">        final Path localAvroPath = new Path(dataPath + &quot;/&quot; + hdfsAvroPath.getName());</span>
<span class="nc" id="L146">        LOGGER.info(&quot;Copy from &quot; + hdfsAvroPath + &quot; to &quot; + localAvroPath);</span>
<span class="nc" id="L147">        fs.copyToLocalFile(hdfsAvroPath, localAvroPath);</span>

<span class="nc" id="L149">        String schemaString = context.getConfiguration().get(&quot;data.schema&quot;);</span>
        try {
<span class="nc" id="L151">          schema = Schema.fromString(schemaString);</span>
<span class="nc" id="L152">        } catch (Exception e) {</span>
<span class="nc" id="L153">          LOGGER.error(&quot;Could not get schema from string for value: &quot; + schemaString);</span>
<span class="nc" id="L154">          throw new RuntimeException(e);</span>
<span class="nc" id="L155">        }</span>
<span class="nc" id="L156">      } catch (Exception e) {</span>
<span class="nc" id="L157">        LOGGER.error(&quot;Could not get schema: &quot; + e);</span>
<span class="nc" id="L158">        throw new RuntimeException(e);</span>
<span class="nc" id="L159">      }</span>

<span class="nc" id="L161">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L162">      LOGGER.info(&quot;input data file path : {}&quot;, _inputFilePath);</span>
<span class="nc" id="L163">      LOGGER.info(&quot;local hdfs segment tar path: {}&quot;, _localHdfsSegmentTarPath);</span>
<span class="nc" id="L164">      LOGGER.info(&quot;local disk segment path: {}&quot;, _localDiskSegmentDirectory);</span>
<span class="nc" id="L165">      LOGGER.info(&quot;local disk segment tar path: {}&quot;, _localDiskSegmentTarPath);</span>
<span class="nc" id="L166">      LOGGER.info(&quot;data schema: {}&quot;, _localDiskSegmentTarPath);</span>
<span class="nc" id="L167">      LOGGER.info(&quot;*********************************************************************&quot;);</span>

      try {
<span class="nc" id="L170">        String segmentName = createSegment(_inputFilePath, schema, Integer.parseInt(lineSplits[2]), hdfsAvroPath, dataPath, fs);</span>
<span class="nc" id="L171">        LOGGER.info(segmentName);</span>
<span class="nc" id="L172">        LOGGER.info(&quot;finished segment creation job successfully&quot;);</span>
<span class="nc" id="L173">      } catch (Exception e) {</span>
<span class="nc" id="L174">        LOGGER.error(&quot;Got exceptions during creating segments!&quot;, e);</span>
<span class="nc" id="L175">      }</span>

<span class="nc" id="L177">      context.write(new LongWritable(Long.parseLong(lineSplits[2])),</span>
          new Text(FileSystem.get(_properties).listStatus(new Path(_localHdfsSegmentTarPath + &quot;/&quot;))[0].getPath().getName()));
<span class="nc" id="L179">      LOGGER.info(&quot;finished the job successfully&quot;);</span>
<span class="nc" id="L180">    }</span>

    protected void setSegmentNameGenerator(SegmentGeneratorConfig segmentGeneratorConfig, Integer seqId, Path hdfsAvroPath, File dataPath) {

<span class="nc" id="L184">    }</span>

    protected String createSegment(String dataFilePath, Schema schema, Integer seqId, Path hdfsDataPath, File dataPath, FileSystem fs) throws Exception {
<span class="nc" id="L187">      LOGGER.info(&quot;Data schema is : {}&quot;, schema);</span>
<span class="nc" id="L188">      SegmentGeneratorConfig segmentGeneratorConfig = new SegmentGeneratorConfig(schema);</span>
<span class="nc" id="L189">      segmentGeneratorConfig.setTableName(_tableName);</span>
<span class="nc" id="L190">      setSegmentNameGenerator(segmentGeneratorConfig, seqId, hdfsDataPath, dataPath);</span>

<span class="nc" id="L192">      segmentGeneratorConfig.setInputFilePath(new File(dataPath, hdfsDataPath.getName()).getAbsolutePath());</span>

<span class="nc" id="L194">      FileFormat fileFormat = getFileFormat(dataFilePath);</span>
<span class="nc" id="L195">      segmentGeneratorConfig.setFormat(fileFormat);</span>
<span class="nc" id="L196">      segmentGeneratorConfig.setOnHeap(true);</span>
      
<span class="nc bnc" id="L198" title="All 2 branches missed.">      if (null != _postfix) {</span>
<span class="nc" id="L199">        segmentGeneratorConfig.setSegmentNamePostfix(String.format(&quot;%s-%s&quot;, _postfix, seqId));</span>
      } else {
<span class="nc" id="L201">        segmentGeneratorConfig.setSequenceId(seqId);</span>
      }
<span class="nc" id="L203">      segmentGeneratorConfig.setReaderConfig(getReaderConfig(fileFormat));</span>

<span class="nc" id="L205">      segmentGeneratorConfig.setOutDir(_localDiskSegmentDirectory);</span>

      // Add the current java package version to the segment metadata
      // properties file.
<span class="nc" id="L209">      Package objPackage = this.getClass().getPackage();</span>
<span class="nc bnc" id="L210" title="All 2 branches missed.">      if (null != objPackage) {</span>
<span class="nc" id="L211">        String packageVersion = objPackage.getSpecificationVersion();</span>
<span class="nc bnc" id="L212" title="All 2 branches missed.">        if (null != packageVersion) {</span>
<span class="nc" id="L213">          LOGGER.info(&quot;Pinot Hadoop Package version {}&quot;, packageVersion);</span>
<span class="nc" id="L214">          segmentGeneratorConfig.setCreatorVersion(packageVersion);</span>
        }
      }

<span class="nc" id="L218">      SegmentIndexCreationDriverImpl driver = new SegmentIndexCreationDriverImpl();</span>
<span class="nc" id="L219">      driver.init(segmentGeneratorConfig);</span>
<span class="nc" id="L220">      driver.build();</span>
      // Tar the segment directory into file.
<span class="nc" id="L222">      String segmentName = driver.getSegmentName();</span>
<span class="nc" id="L223">      String localSegmentPath = new File(_localDiskSegmentDirectory, segmentName).getAbsolutePath();</span>

<span class="nc" id="L225">      String localTarPath = _localDiskSegmentTarPath + &quot;/&quot; + segmentName + JobConfigConstants.TARGZ;</span>
<span class="nc" id="L226">      LOGGER.info(&quot;Trying to tar the segment to: {}&quot;, localTarPath);</span>
<span class="nc" id="L227">      TarGzCompressionUtils.createTarGzOfDirectory(localSegmentPath, localTarPath);</span>
<span class="nc" id="L228">      String hdfsTarPath = _localHdfsSegmentTarPath + &quot;/&quot; + segmentName + JobConfigConstants.TARGZ;</span>

<span class="nc" id="L230">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L231">      LOGGER.info(&quot;Copy from : {} to {}&quot;, localTarPath, hdfsTarPath);</span>
<span class="nc" id="L232">      LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L233">      fs.copyFromLocalFile(true, true, new Path(localTarPath), new Path(hdfsTarPath));</span>
<span class="nc" id="L234">      return segmentName;</span>
    }

    private RecordReaderConfig getReaderConfig(FileFormat fileFormat) {
<span class="nc" id="L238">      RecordReaderConfig readerConfig = null;</span>
<span class="nc bnc" id="L239" title="All 5 branches missed.">      switch (fileFormat) {</span>
        case CSV:
<span class="nc" id="L241">          readerConfig = new CSVRecordReaderConfig();</span>
<span class="nc" id="L242">          break;</span>
        case AVRO:
<span class="nc" id="L244">          break;</span>
        case JSON:
<span class="nc" id="L246">          break;</span>
        case THRIFT:
<span class="nc" id="L248">          readerConfig = new ThriftRecordReaderConfig();</span>
        default:
          break;
      }
<span class="nc" id="L252">      return readerConfig;</span>
    }

    private FileFormat getFileFormat(String dataFilePath) {
<span class="nc bnc" id="L256" title="All 2 branches missed.">      if (dataFilePath.endsWith(&quot;.json&quot;)) {</span>
<span class="nc" id="L257">        return FileFormat.JSON;</span>
      }
<span class="nc bnc" id="L259" title="All 2 branches missed.">      if (dataFilePath.endsWith(&quot;.csv&quot;)) {</span>
<span class="nc" id="L260">        return FileFormat.CSV;</span>
      }
<span class="nc bnc" id="L262" title="All 2 branches missed.">      if (dataFilePath.endsWith(&quot;.avro&quot;)) {</span>
<span class="nc" id="L263">        return FileFormat.AVRO;</span>
      }
<span class="nc bnc" id="L265" title="All 2 branches missed.">      if (dataFilePath.endsWith(&quot;.thrift&quot;)) {</span>
<span class="nc" id="L266">        return FileFormat.THRIFT;</span>
      }
<span class="nc" id="L268">      throw new RuntimeException(&quot;Not support file format - &quot; + dataFilePath);</span>
    }
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.7.201606060606</span></div></body></html>