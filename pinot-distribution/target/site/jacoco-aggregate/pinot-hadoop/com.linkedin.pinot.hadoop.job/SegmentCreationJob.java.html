<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>SegmentCreationJob.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">pinot-distribution</a> &gt; <a href="../index.html" class="el_bundle">pinot-hadoop</a> &gt; <a href="index.source.html" class="el_package">com.linkedin.pinot.hadoop.job</a> &gt; <span class="el_source">SegmentCreationJob.java</span></div><h1>SegmentCreationJob.java</h1><pre class="source lang-java linenums">/**
 * Copyright (C) 2014-2016 LinkedIn Corp. (pinot-core@linkedin.com)
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.linkedin.pinot.hadoop.job;

import com.linkedin.pinot.common.Utils;
import com.linkedin.pinot.common.data.Schema;
import com.linkedin.pinot.hadoop.job.mapper.HadoopSegmentCreationMapReduceJob;
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobContext;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class SegmentCreationJob extends Configured {

  private static final String PATH_TO_DEPS_JAR = &quot;path.to.deps.jar&quot;;
  private static final String TEMP = &quot;temp&quot;;
  private static final String PATH_TO_SCHEMA = &quot;path.to.schema&quot;;

<span class="nc" id="L50">  private static final Logger LOGGER = LoggerFactory.getLogger(SegmentCreationJob.class);</span>

  private final String _jobName;
  private final Properties _properties;

  private final String _inputSegmentDir;
  private final String _stagingDir;
  private final Schema _dataSchema;
  private final String _depsJarPath;
  private final String _outputDir;

  public SegmentCreationJob(String jobName, Properties properties) throws Exception {
<span class="nc" id="L62">    super(new Configuration());</span>
<span class="nc" id="L63">    getConf().set(&quot;mapreduce.job.user.classpath.first&quot;, &quot;true&quot;);</span>
<span class="nc" id="L64">    _jobName = jobName;</span>
<span class="nc" id="L65">    _properties = properties;</span>

<span class="nc" id="L67">    _inputSegmentDir = _properties.getProperty(JobConfigConstants.PATH_TO_INPUT);</span>
<span class="nc" id="L68">    String schemaFilePath = _properties.getProperty(PATH_TO_SCHEMA);</span>
<span class="nc" id="L69">    _outputDir = getOutputDir();</span>
<span class="nc" id="L70">    _stagingDir = new File(_outputDir, TEMP).getAbsolutePath();</span>
<span class="nc" id="L71">    _depsJarPath = _properties.getProperty(PATH_TO_DEPS_JAR, null);</span>

<span class="nc" id="L73">    Utils.logVersions();</span>

<span class="nc" id="L75">    LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L76">    LOGGER.info(&quot;path.to.input: {}&quot;, _inputSegmentDir);</span>
<span class="nc" id="L77">    LOGGER.info(&quot;path.to.deps.jar: {}&quot;, _depsJarPath);</span>
<span class="nc" id="L78">    LOGGER.info(&quot;path.to.output: {}&quot;, _outputDir);</span>
<span class="nc" id="L79">    LOGGER.info(&quot;path.to.schema: {}&quot;, schemaFilePath);</span>
<span class="nc bnc" id="L80" title="All 2 branches missed.">    if (schemaFilePath != null) {</span>
<span class="nc" id="L81">      _dataSchema = Schema.fromFile(new File(schemaFilePath));</span>
    } else {
<span class="nc" id="L83">      _dataSchema = null;</span>
    }
<span class="nc" id="L85">    LOGGER.info(&quot;schema: {}&quot;, _dataSchema);</span>
<span class="nc" id="L86">    LOGGER.info(&quot;*********************************************************************&quot;);</span>
<span class="nc" id="L87">  }</span>

  protected String getOutputDir() {
<span class="nc" id="L90">    return _properties.getProperty(JobConfigConstants.PATH_TO_OUTPUT);</span>
  }

  protected String getInputDir() {
<span class="nc" id="L94">    return _inputSegmentDir;</span>
  }

  protected void setOutputPath(Configuration configuration) {

<span class="nc" id="L99">  }</span>

  public void run() throws Exception {
<span class="nc" id="L102">    LOGGER.info(&quot;Starting {}&quot;, getClass().getSimpleName());</span>

<span class="nc" id="L104">    FileSystem fs = FileSystem.get(getConf());</span>
<span class="nc" id="L105">    Path inputPathPattern = new Path(_inputSegmentDir);</span>

<span class="nc bnc" id="L107" title="All 2 branches missed.">    if (fs.exists(new Path(_stagingDir))) {</span>
<span class="nc" id="L108">      LOGGER.warn(&quot;Found the temp folder, deleting it&quot;);</span>
<span class="nc" id="L109">      fs.delete(new Path(_stagingDir), true);</span>
    }
<span class="nc" id="L111">    fs.mkdirs(new Path(_stagingDir));</span>
<span class="nc" id="L112">    fs.mkdirs(new Path(_stagingDir + &quot;/input/&quot;));</span>

<span class="nc bnc" id="L114" title="All 2 branches missed.">    if (fs.exists(new Path(_outputDir))) {</span>
<span class="nc" id="L115">      LOGGER.warn(&quot;Found the output folder {}, deleting it&quot;, _outputDir);</span>
<span class="nc" id="L116">      fs.delete(new Path(_outputDir), true);</span>
    }
<span class="nc" id="L118">    fs.mkdirs(new Path(_outputDir));</span>

<span class="nc" id="L120">    List&lt;FileStatus&gt; inputDataFiles = new ArrayList&lt;FileStatus&gt;();</span>
<span class="nc" id="L121">    FileStatus[] fileStatusArr = fs.globStatus(inputPathPattern);</span>
<span class="nc bnc" id="L122" title="All 2 branches missed.">    for (FileStatus fileStatus : fileStatusArr) {</span>
<span class="nc" id="L123">      inputDataFiles.addAll(getDataFilesFromPath(fs, fileStatus.getPath()));</span>
    }
<span class="nc bnc" id="L125" title="All 2 branches missed.">    if (inputDataFiles.isEmpty()) {</span>
<span class="nc" id="L126">      LOGGER.error(&quot;No Input paths {}&quot;, inputPathPattern);</span>
<span class="nc" id="L127">      throw new RuntimeException(&quot;No input files &quot; + inputPathPattern);</span>
    }

<span class="nc bnc" id="L130" title="All 2 branches missed.">    for (int seqId = 0; seqId &lt; inputDataFiles.size(); ++seqId) {</span>
<span class="nc" id="L131">      FileStatus file = inputDataFiles.get(seqId);</span>
<span class="nc" id="L132">      String completeFilePath = &quot; &quot; + file.getPath().toString() + &quot; &quot; + seqId;</span>
<span class="nc" id="L133">      Path newOutPutFile = new Path((_stagingDir + &quot;/input/&quot; +</span>
          file.getPath().toString().replace('.', '_').replace('/', '_').replace(':', '_') + &quot;.txt&quot;));
<span class="nc" id="L135">      FSDataOutputStream stream = fs.create(newOutPutFile);</span>
<span class="nc" id="L136">      stream.writeUTF(completeFilePath);</span>
<span class="nc" id="L137">      stream.flush();</span>
<span class="nc" id="L138">      stream.close();</span>
    }

<span class="nc" id="L141">    Job job = Job.getInstance(getConf());</span>

<span class="nc" id="L143">    setAdditionalJobProperties(job);</span>

<span class="nc" id="L145">    job.setJarByClass(SegmentCreationJob.class);</span>
<span class="nc" id="L146">    job.setJobName(_jobName);</span>

<span class="nc" id="L148">    setMapperClass(job);</span>

<span class="nc bnc" id="L150" title="All 2 branches missed.">    if (System.getenv(&quot;HADOOP_TOKEN_FILE_LOCATION&quot;) != null) {</span>
<span class="nc" id="L151">      job.getConfiguration().set(&quot;mapreduce.job.credentials.binary&quot;, System.getenv(&quot;HADOOP_TOKEN_FILE_LOCATION&quot;));</span>
    }

<span class="nc" id="L154">    job.setInputFormatClass(TextInputFormat.class);</span>
<span class="nc" id="L155">    job.setOutputFormatClass(TextOutputFormat.class);</span>

<span class="nc" id="L157">    job.setMapOutputKeyClass(LongWritable.class);</span>
<span class="nc" id="L158">    job.setMapOutputValueClass(Text.class);</span>

<span class="nc" id="L160">    FileInputFormat.addInputPath(job, new Path(_stagingDir + &quot;/input/&quot;));</span>
<span class="nc" id="L161">    FileOutputFormat.setOutputPath(job, new Path(_stagingDir + &quot;/output/&quot;));</span>

<span class="nc" id="L163">    job.getConfiguration().setInt(JobContext.NUM_MAPS, inputDataFiles.size());</span>
<span class="nc bnc" id="L164" title="All 2 branches missed.">    if (_dataSchema != null) {</span>
<span class="nc" id="L165">      job.getConfiguration().set(&quot;data.schema&quot;, _dataSchema.toString());</span>
    }
<span class="nc" id="L167">    setOutputPath(job.getConfiguration());</span>

<span class="nc" id="L169">    job.setNumReduceTasks(0);</span>
<span class="nc bnc" id="L170" title="All 2 branches missed.">    for (Object key : _properties.keySet()) {</span>
<span class="nc" id="L171">      job.getConfiguration().set(key.toString(), _properties.getProperty(key.toString()));</span>
<span class="nc" id="L172">    }</span>

<span class="nc bnc" id="L174" title="All 4 branches missed.">    if (_depsJarPath != null &amp;&amp; _depsJarPath.length() &gt; 0) {</span>
<span class="nc" id="L175">      addDepsJarToDistributedCache(new Path(_depsJarPath), job);</span>
    }

    // Submit the job for execution.
<span class="nc" id="L179">    job.waitForCompletion(true);</span>
<span class="nc bnc" id="L180" title="All 2 branches missed.">    if (!job.isSuccessful()) {</span>
<span class="nc" id="L181">      throw new RuntimeException(&quot;Job failed : &quot; + job);</span>
    }

<span class="nc" id="L184">    moveToOutputDirectory(fs);</span>

    // Delete temporary directory.
<span class="nc" id="L187">    LOGGER.info(&quot;Cleanup the working directory.&quot;);</span>
<span class="nc" id="L188">    LOGGER.info(&quot;Deleting the dir: {}&quot;, _stagingDir);</span>
<span class="nc" id="L189">    fs.delete(new Path(_stagingDir), true);</span>
<span class="nc" id="L190">  }</span>

  protected void setAdditionalJobProperties(Job job) throws Exception {

<span class="nc" id="L194">  }</span>

  protected void moveToOutputDirectory(FileSystem fs) throws Exception {
<span class="nc" id="L197">    LOGGER.info(&quot;Moving Segment Tar files from {} to: {}&quot;, _stagingDir + &quot;/output/segmentTar&quot;, _outputDir);</span>
<span class="nc" id="L198">    FileStatus[] segmentArr = fs.listStatus(new Path(_stagingDir + &quot;/output/segmentTar&quot;));</span>
<span class="nc bnc" id="L199" title="All 2 branches missed.">    for (FileStatus segment : segmentArr) {</span>
<span class="nc" id="L200">      fs.rename(segment.getPath(), new Path(_outputDir, segment.getPath().getName()));</span>
    }
<span class="nc" id="L202">  }</span>

  protected Job setMapperClass(Job job) {
<span class="nc" id="L205">    job.setMapperClass(HadoopSegmentCreationMapReduceJob.HadoopSegmentCreationMapper.class);</span>
<span class="nc" id="L206">    return job;</span>

  }

  private void addDepsJarToDistributedCache(Path path, Job job) throws IOException {
<span class="nc" id="L211">    LOGGER.info(&quot;Trying to add all the deps jar files from directory: {}&quot;, path);</span>
<span class="nc" id="L212">    FileSystem fs = FileSystem.get(getConf());</span>
<span class="nc" id="L213">    FileStatus[] fileStatusArr = fs.listStatus(path);</span>
<span class="nc bnc" id="L214" title="All 2 branches missed.">    for (FileStatus fileStatus : fileStatusArr) {</span>
<span class="nc bnc" id="L215" title="All 2 branches missed.">      if (fileStatus.isDirectory()) {</span>
<span class="nc" id="L216">        addDepsJarToDistributedCache(fileStatus.getPath(), job);</span>
      } else {
<span class="nc" id="L218">        Path depJarPath = fileStatus.getPath();</span>
<span class="nc bnc" id="L219" title="All 2 branches missed.">        if (depJarPath.getName().endsWith(&quot;.jar&quot;)) {</span>
<span class="nc" id="L220">          LOGGER.info(&quot;Adding deps jar files: {}&quot;, path);</span>
<span class="nc" id="L221">          job.addCacheArchive(path.toUri());</span>
        }
      }
    }
<span class="nc" id="L225">  }</span>

  private ArrayList&lt;FileStatus&gt; getDataFilesFromPath(FileSystem fs, Path inBaseDir) throws IOException {
<span class="nc" id="L228">    ArrayList&lt;FileStatus&gt; dataFileStatusList = new ArrayList&lt;FileStatus&gt;();</span>
<span class="nc" id="L229">    FileStatus[] fileStatusArr = fs.listStatus(inBaseDir);</span>
<span class="nc bnc" id="L230" title="All 2 branches missed.">    for (FileStatus fileStatus : fileStatusArr) {</span>
<span class="nc bnc" id="L231" title="All 2 branches missed.">      if (fileStatus.isDirectory()) {</span>
<span class="nc" id="L232">        LOGGER.info(&quot;Trying to add all the data files from directory: {}&quot;, fileStatus.getPath());</span>
<span class="nc" id="L233">        dataFileStatusList.addAll(getDataFilesFromPath(fs, fileStatus.getPath()));</span>
      } else {
<span class="nc" id="L235">        String fileName = fileStatus.getPath().getName();</span>
<span class="nc bnc" id="L236" title="All 2 branches missed.">        if (fileName.endsWith(&quot;.avro&quot;)) {</span>
<span class="nc" id="L237">          LOGGER.info(&quot;Adding avro files: {}&quot;, fileStatus.getPath());</span>
<span class="nc" id="L238">          dataFileStatusList.add(fileStatus);</span>
        }
<span class="nc bnc" id="L240" title="All 2 branches missed.">        if (fileName.endsWith(&quot;.csv&quot;)) {</span>
<span class="nc" id="L241">          LOGGER.info(&quot;Adding csv files: {}&quot;, fileStatus.getPath());</span>
<span class="nc" id="L242">          dataFileStatusList.add(fileStatus);</span>
        }
<span class="nc bnc" id="L244" title="All 2 branches missed.">        if (fileName.endsWith(&quot;.json&quot;)) {</span>
<span class="nc" id="L245">          LOGGER.info(&quot;Adding json files: {}&quot;, fileStatus.getPath());</span>
<span class="nc" id="L246">          dataFileStatusList.add(fileStatus);</span>
        }
      }
    }
<span class="nc" id="L250">    return dataFileStatusList;</span>
  }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.eclemma.org/jacoco">JaCoCo</a> 0.7.7.201606060606</span></div></body></html>