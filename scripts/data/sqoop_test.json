{
    "sqoop_0f35d54": {
        "bug_id": "sqoop_0f35d54",
        "commit": "https://github.com/apache/sqoop/commit/0f35d54f2190aa7339c07902d7cba9c30f59bcdb",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/sqoop/blob/0f35d54f2190aa7339c07902d7cba9c30f59bcdb/src/java/com/cloudera/sqoop/mapreduce/ExportJobBase.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/mapreduce/ExportJobBase.java?ref=0f35d54f2190aa7339c07902d7cba9c30f59bcdb",
                "deletions": 8,
                "filename": "src/java/com/cloudera/sqoop/mapreduce/ExportJobBase.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.Counters;\n import org.apache.hadoop.mapreduce.InputFormat;\n import org.apache.hadoop.mapreduce.Job;\n import org.apache.hadoop.mapreduce.Mapper;\n@@ -234,16 +235,23 @@ protected int configureNumTasks(Job job) throws IOException {\n   protected boolean runJob(Job job) throws ClassNotFoundException, IOException,\n       InterruptedException {\n \n-    PerfCounters counters = new PerfCounters();\n-    counters.startClock();\n+    PerfCounters perfCounters = new PerfCounters();\n+    perfCounters.startClock();\n \n     boolean success = job.waitForCompletion(true);\n-    counters.stopClock();\n-    counters.addBytes(job.getCounters().getGroup(\"FileSystemCounters\")\n-      .findCounter(\"HDFS_BYTES_READ\").getValue());\n-    LOG.info(\"Transferred \" + counters.toString());\n-    long numRecords = HadoopShim.get().getNumMapInputRecords(job);\n-    LOG.info(\"Exported \" + numRecords + \" records.\");\n+    perfCounters.stopClock();\n+\n+    Counters jobCounters = job.getCounters();\n+    // If the job has been retired, these may be unavailable.\n+    if (null == jobCounters) {\n+      displayRetiredJobNotice(LOG);\n+    } else {\n+      perfCounters.addBytes(jobCounters.getGroup(\"FileSystemCounters\")\n+        .findCounter(\"HDFS_BYTES_READ\").getValue());\n+      LOG.info(\"Transferred \" + perfCounters.toString());\n+      long numRecords = HadoopShim.get().getNumMapInputRecords(job);\n+      LOG.info(\"Exported \" + numRecords + \" records.\");\n+    }\n \n     return success;\n   }",
                "raw_url": "https://github.com/apache/sqoop/raw/0f35d54f2190aa7339c07902d7cba9c30f59bcdb/src/java/com/cloudera/sqoop/mapreduce/ExportJobBase.java",
                "sha": "1337fad6dc1c865e3af1d0164feeb63cb4305afb",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/sqoop/blob/0f35d54f2190aa7339c07902d7cba9c30f59bcdb/src/java/com/cloudera/sqoop/mapreduce/ImportJobBase.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/mapreduce/ImportJobBase.java?ref=0f35d54f2190aa7339c07902d7cba9c30f59bcdb",
                "deletions": 8,
                "filename": "src/java/com/cloudera/sqoop/mapreduce/ImportJobBase.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.SequenceFile.CompressionType;\n import org.apache.hadoop.io.compress.GzipCodec;\n+import org.apache.hadoop.mapreduce.Counters;\n import org.apache.hadoop.mapreduce.InputFormat;\n import org.apache.hadoop.mapreduce.Job;\n import org.apache.hadoop.mapreduce.Mapper;\n@@ -100,16 +101,23 @@ protected void configureOutputFormat(Job job, String tableName,\n   protected boolean runJob(Job job) throws ClassNotFoundException, IOException,\n       InterruptedException {\n \n-    PerfCounters counters = new PerfCounters();\n-    counters.startClock();\n+    PerfCounters perfCounters = new PerfCounters();\n+    perfCounters.startClock();\n \n     boolean success = job.waitForCompletion(true);\n-    counters.stopClock();\n-    counters.addBytes(job.getCounters().getGroup(\"FileSystemCounters\")\n-      .findCounter(\"HDFS_BYTES_WRITTEN\").getValue());\n-    LOG.info(\"Transferred \" + counters.toString());\n-    long numRecords = HadoopShim.get().getNumMapOutputRecords(job);\n-    LOG.info(\"Retrieved \" + numRecords + \" records.\");\n+    perfCounters.stopClock();\n+\n+    Counters jobCounters = job.getCounters();\n+    // If the job has been retired, these may be unavailable.\n+    if (null == jobCounters) {\n+      displayRetiredJobNotice(LOG);\n+    } else {\n+      perfCounters.addBytes(jobCounters.getGroup(\"FileSystemCounters\")\n+        .findCounter(\"HDFS_BYTES_WRITTEN\").getValue());\n+      LOG.info(\"Transferred \" + perfCounters.toString());\n+      long numRecords = HadoopShim.get().getNumMapOutputRecords(job);\n+      LOG.info(\"Retrieved \" + numRecords + \" records.\");\n+    }\n     return success;\n   }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/0f35d54f2190aa7339c07902d7cba9c30f59bcdb/src/java/com/cloudera/sqoop/mapreduce/ImportJobBase.java",
                "sha": "f4bfe8158a1b1ecb66bc3f7012c919007a054f5e",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/sqoop/blob/0f35d54f2190aa7339c07902d7cba9c30f59bcdb/src/java/com/cloudera/sqoop/mapreduce/JobBase.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/mapreduce/JobBase.java?ref=0f35d54f2190aa7339c07902d7cba9c30f59bcdb",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/mapreduce/JobBase.java",
                "patch": "@@ -193,4 +193,20 @@ protected boolean runJob(Job job) throws ClassNotFoundException, IOException,\n       InterruptedException {\n     return job.waitForCompletion(true);\n   }\n+\n+  /**\n+   * Display a notice on the log that the current MapReduce job has\n+   * been retired, and thus Counters are unavailable.\n+   * @param log the Log to display the info to.\n+   */\n+  protected void displayRetiredJobNotice(Log log) {\n+    log.info(\"The MapReduce job has already been retired. Performance\");\n+    log.info(\"counters are unavailable. To get this information, \");\n+    log.info(\"you will need to enable the completed job store on \");\n+    log.info(\"the jobtracker with:\");\n+    log.info(\"mapreduce.jobtracker.persist.jobstatus.active = true\");\n+    log.info(\"mapreduce.jobtracker.persist.jobstatus.hours = 1\");\n+    log.info(\"A jobtracker restart is required for these settings\");\n+    log.info(\"to take effect.\");\n+  }\n }",
                "raw_url": "https://github.com/apache/sqoop/raw/0f35d54f2190aa7339c07902d7cba9c30f59bcdb/src/java/com/cloudera/sqoop/mapreduce/JobBase.java",
                "sha": "b80cfc97b5f77c4f52e86fac099fb7a9441272d7",
                "status": "modified"
            }
        ],
        "message": "SQOOP-15. Do not assume that Job.getCounters() returns non-null.\n\nIf jobs are already retired, display a useful message on how to\nenable the completed job store and suppress performance counter\nusage, rather than crashing with a NullPointerException.\n\nFrom: Aaron Kimball <aaron@cloudera.com>\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/sqoop/trunk@1149939 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/0fcbcd759a8001ec7b592ea4e8114b19c89df053",
        "patched_files": [
            "JobBase.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestJobBase.java"
        ]
    },
    "sqoop_5dd36c6": {
        "bug_id": "sqoop_5dd36c6",
        "commit": "https://github.com/apache/sqoop/commit/5dd36c62da7c4bb237de73a24c9bfffaf18efee6",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/sqoop/blob/5dd36c62da7c4bb237de73a24c9bfffaf18efee6/build.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/build.xml?ref=5dd36c62da7c4bb237de73a24c9bfffaf18efee6",
                "deletions": 1,
                "filename": "build.xml",
                "patch": "@@ -1017,7 +1017,7 @@\n     <mkdir dir=\"${findbugs.out.dir}\"/>\n     <findbugs home=\"${findbugs.home}\" output=\"xml:withMessages\"\n         outputFile=\"${findbugs.output.xml.file}\" effort=\"max\"\n-        excludeFilter=\"${findbugs.excludes}\">\n+        excludeFilter=\"${findbugs.excludes}\" jvmargs=\"-Xms512m -Xmx512m\">\n       <auxClasspath>\n         <path refid=\"test.classpath\"/>\n       </auxClasspath>",
                "raw_url": "https://github.com/apache/sqoop/raw/5dd36c62da7c4bb237de73a24c9bfffaf18efee6/build.xml",
                "sha": "f108dd58d70f3ff1e8d5fa2093f027788f6fffd8",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/sqoop/blob/5dd36c62da7c4bb237de73a24c9bfffaf18efee6/src/java/com/cloudera/sqoop/Sqoop.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/Sqoop.java?ref=5dd36c62da7c4bb237de73a24c9bfffaf18efee6",
                "deletions": 8,
                "filename": "src/java/com/cloudera/sqoop/Sqoop.java",
                "patch": "@@ -20,15 +20,14 @@\n \n import java.util.Arrays;\n \n-import org.apache.commons.cli.ParseException;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n-import com.cloudera.sqoop.tool.HelpTool;\n+import com.cloudera.sqoop.cli.ToolOptions;\n import com.cloudera.sqoop.tool.SqoopTool;\n \n /**\n@@ -121,13 +120,16 @@ public int run(String [] args) {\n       options = tool.parseArguments(args, null, options, false);\n       tool.appendArgs(this.childPrgmArgs);\n       tool.validateOptions(options);\n-    } catch (ParseException pe) {\n-      // Couldn't parse arguments. Just print a usage message and exit.\n-      new HelpTool().run(new SqoopOptions(getConf()));\n-      return 1;\n-    } catch (SqoopOptions.InvalidOptionsException e) {\n-      // Error validating arguments. Print an error message and exit.\n+    } catch (Exception e) {\n+      // Couldn't parse arguments. \n+      // Log the stack trace for this exception\n+      LOG.debug(e.getMessage(), e);\n+      // Print exception message.\n       System.err.println(e.getMessage());\n+      // Print the tool usage message and exit.\n+      ToolOptions toolOpts = new ToolOptions();\n+      tool.configureOptions(toolOpts);\n+      tool.printHelp(toolOpts);\n       return 1; // Exit on exception here.\n     }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/5dd36c62da7c4bb237de73a24c9bfffaf18efee6/src/java/com/cloudera/sqoop/Sqoop.java",
                "sha": "dcd0cfd13897711990ad0aab4eab294016433022",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/sqoop/blob/5dd36c62da7c4bb237de73a24c9bfffaf18efee6/src/java/com/cloudera/sqoop/tool/HelpTool.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/tool/HelpTool.java?ref=5dd36c62da7c4bb237de73a24c9bfffaf18efee6",
                "deletions": 1,
                "filename": "src/java/com/cloudera/sqoop/tool/HelpTool.java",
                "patch": "@@ -78,7 +78,7 @@ private void printAvailableTools() {\n   /** {@inheritDoc} */\n   public int run(SqoopOptions options) {\n \n-    if (this.extraArguments.length > 0) {\n+    if (this.extraArguments != null && this.extraArguments.length > 0) {\n       if (hasUnrecognizedArgs(extraArguments, 1, extraArguments.length)) {\n         return 1;\n       }",
                "raw_url": "https://github.com/apache/sqoop/raw/5dd36c62da7c4bb237de73a24c9bfffaf18efee6/src/java/com/cloudera/sqoop/tool/HelpTool.java",
                "sha": "dea1bb4f4784b357dedc199739f5be08a247ed30",
                "status": "modified"
            }
        ],
        "message": "SQOOP-114. Fix for NPE.\n\nThis is a fix for a case of malformed command line arguments,\nwhere the tool name is correct, the option is also correct,\nbut the value of the option is missing.\n\nFrom: Ahmed Radwan <ahmed@cloudera.com>\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/sqoop/trunk@1149990 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/e3638e8ce03ddca1dbd696368085d4bacbcf75ed",
        "patched_files": [
            "Sqoop.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "SqoopTest.java"
        ]
    },
    "sqoop_5f888fe": {
        "bug_id": "sqoop_5f888fe",
        "commit": "https://github.com/apache/sqoop/commit/5f888fefdd7d0ab93dfb8ceab38131c0346be35c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/sqoop/blob/5f888fefdd7d0ab93dfb8ceab38131c0346be35c/src/java/org/apache/sqoop/manager/SqlManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/manager/SqlManager.java?ref=5f888fefdd7d0ab93dfb8ceab38131c0346be35c",
                "deletions": 1,
                "filename": "src/java/org/apache/sqoop/manager/SqlManager.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.sqoop.util.SqlTypeMap;\n \n /**\n  * ConnManager implementation for generic SQL-compliant database.\n@@ -195,7 +196,7 @@ protected String getColTypesQuery(String tableName) {\n     }\n \n     try {\n-      Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+      Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n \n       int cols = results.getMetaData().getColumnCount();\n       ResultSetMetaData metadata = results.getMetaData();",
                "raw_url": "https://github.com/apache/sqoop/raw/5f888fefdd7d0ab93dfb8ceab38131c0346be35c/src/java/org/apache/sqoop/manager/SqlManager.java",
                "sha": "ea961cd762227e0e458c93c2de3905c2c04628ed",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/sqoop/blob/5f888fefdd7d0ab93dfb8ceab38131c0346be35c/src/java/org/apache/sqoop/util/SqlTypeMap.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/util/SqlTypeMap.java?ref=5f888fefdd7d0ab93dfb8ceab38131c0346be35c",
                "deletions": 0,
                "filename": "src/java/org/apache/sqoop/util/SqlTypeMap.java",
                "patch": "@@ -0,0 +1,58 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.sqoop.util;\n+\n+import java.util.HashMap;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Using java.utils.HashMap to store primitive data types such as int can\n+ * be unsafe because auto-unboxing a null value in the map can cause a NPE.\n+ *\n+ * SqlTypeMap is meant to be safer because it provides validation for arguments\n+ * and fails fast with informative messages if invalid arguments are given.\n+ */\n+public class SqlTypeMap<K, V> extends HashMap<K, V> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final Log LOG = LogFactory.getLog(SqlTypeMap.class.getName());\n+\n+  @Override\n+  public V get(Object col) {\n+    V sqlType = super.get(col);\n+    if (sqlType == null) {\n+      LOG.error(\"It seems like you are looking up a column that does not\");\n+      LOG.error(\"exist in the table. Please ensure that you've specified\");\n+      LOG.error(\"correct column names in Sqoop options.\");\n+      throw new IllegalArgumentException(\"column not found: \" + col);\n+    }\n+    return sqlType;\n+  }\n+\n+  @Override\n+  public V put(K col, V sqlType) {\n+    if (sqlType == null) {\n+      throw new IllegalArgumentException(\"sql type cannot be null\");\n+    }\n+    return super.put(col, sqlType);\n+  }\n+}",
                "raw_url": "https://github.com/apache/sqoop/raw/5f888fefdd7d0ab93dfb8ceab38131c0346be35c/src/java/org/apache/sqoop/util/SqlTypeMap.java",
                "sha": "cd5d468afeab24704e054c3362ab52f71ac1c735",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/sqoop/blob/5f888fefdd7d0ab93dfb8ceab38131c0346be35c/src/test/com/cloudera/sqoop/hive/TestTableDefWriter.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/hive/TestTableDefWriter.java?ref=5f888fefdd7d0ab93dfb8ceab38131c0346be35c",
                "deletions": 7,
                "filename": "src/test/com/cloudera/sqoop/hive/TestTableDefWriter.java",
                "patch": "@@ -18,14 +18,14 @@\n \n package com.cloudera.sqoop.hive;\n \n-import java.util.HashMap;\n import java.util.Map;\n \n import junit.framework.TestCase;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.sqoop.util.SqlTypeMap;\n \n import com.cloudera.sqoop.SqoopOptions;\n import com.cloudera.sqoop.tool.ImportTool;\n@@ -67,7 +67,7 @@ public void testDifferentTableNames() throws Exception {\n     TableDefWriter writer = new TableDefWriter(options, null,\n         \"inputTable\", \"outputTable\", conf, false);\n \n-    Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+    Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n     writer.setColumnTypes(colTypes);\n \n     String createTable = writer.getCreateTableStmt();\n@@ -95,7 +95,7 @@ public void testDifferentTargetDirs() throws Exception {\n     TableDefWriter writer = new TableDefWriter(options, null,\n         inputTable, outputTable, conf, false);\n \n-    Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+    Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n     writer.setColumnTypes(colTypes);\n \n     String createTable = writer.getCreateTableStmt();\n@@ -122,7 +122,7 @@ public void testPartitions() throws Exception {\n     TableDefWriter writer = new TableDefWriter(options,\n         null, \"inputTable\", \"outputTable\", conf, false);\n \n-    Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+    Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n     writer.setColumnTypes(colTypes);\n \n     String createTable = writer.getCreateTableStmt();\n@@ -148,7 +148,7 @@ public void testLzoSplitting() throws Exception {\n     TableDefWriter writer = new TableDefWriter(options,\n         null, \"inputTable\", \"outputTable\", conf, false);\n \n-    Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+    Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n     writer.setColumnTypes(colTypes);\n \n     String createTable = writer.getCreateTableStmt();\n@@ -175,7 +175,7 @@ public void testUserMapping() throws Exception {\n     TableDefWriter writer = new TableDefWriter(options,\n         null, HsqldbTestServer.getTableName(), \"outputTable\", conf, false);\n \n-    Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+    Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n     colTypes.put(\"id\", Types.INTEGER);\n     colTypes.put(\"value\", Types.VARCHAR);\n     writer.setColumnTypes(colTypes);\n@@ -201,7 +201,7 @@ public void testUserMappingFailWhenCantBeApplied() throws Exception {\n     TableDefWriter writer = new TableDefWriter(options,\n         null, HsqldbTestServer.getTableName(), \"outputTable\", conf, false);\n \n-    Map<String, Integer> colTypes = new HashMap<String, Integer>();\n+    Map<String, Integer> colTypes = new SqlTypeMap<String, Integer>();\n     colTypes.put(\"id\", Types.INTEGER);\n     writer.setColumnTypes(colTypes);\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/5f888fefdd7d0ab93dfb8ceab38131c0346be35c/src/test/com/cloudera/sqoop/hive/TestTableDefWriter.java",
                "sha": "6610b7549a05f8e7569553f14a23155950f8f740",
                "status": "modified"
            }
        ],
        "message": "SQOOP-481. Sqoop import with --hive-import using wrong column names in --columns throws a NPE.\n\n(Cheolsoo Park via Jarek Jarcec Cecho)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/sqoop/trunk@1345225 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/4505205588a13460d1e1183dfebeec56d4f66511",
        "patched_files": [
            "TableDefWriter.java",
            "SqlManager.java",
            "SqlTypeMap.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestSqlManager.java",
            "TestTableDefWriter.java"
        ]
    },
    "sqoop_6174268": {
        "bug_id": "sqoop_6174268",
        "commit": "https://github.com/apache/sqoop/commit/6174268d28ff0c0638fa8db39fb1a1e943daa285",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/sqoop/blob/6174268d28ff0c0638fa8db39fb1a1e943daa285/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java?ref=6174268d28ff0c0638fa8db39fb1a1e943daa285",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "patch": "@@ -57,6 +57,10 @@ public static final String escapeAndEnclose(String str, String escape, String en\n     boolean escapingLegal = (null != escape && escape.length() > 0 && !escape.equals(\"\\000\"));\n     String withEscapes;\n \n+    if (null == str) {\n+      return null;\n+    }\n+\n     if (escapingLegal) {\n       // escaping is legal. Escape any instances of the escape char itself\n       withEscapes = str.replace(escape, escape + escape);",
                "raw_url": "https://github.com/apache/sqoop/raw/6174268d28ff0c0638fa8db39fb1a1e943daa285/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "sha": "a1b6742133b49da13448a6a94d4b7cc87c2bf9cb",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/sqoop/blob/6174268d28ff0c0638fa8db39fb1a1e943daa285/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/org/apache/hadoop/sqoop/TestAllTables.java?ref=6174268d28ff0c0638fa8db39fb1a1e943daa285",
                "deletions": 2,
                "filename": "src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "patch": "@@ -58,6 +58,8 @@\n     args.add(HsqldbTestServer.getUrl());\n     args.add(\"--num-mappers\");\n     args.add(\"1\");\n+    args.add(\"--escaped-by\");\n+    args.add(\"\\\\\");\n \n     return args.toArray(new String[0]);\n   }\n@@ -86,9 +88,18 @@ public void setUp() {\n     // create two tables.\n     this.expectedStrings.add(\"A winner\");\n     this.expectedStrings.add(\"is you!\");\n+    this.expectedStrings.add(null);\n \n+    int i = 0;\n     for (String expectedStr: this.expectedStrings) {\n-      this.createTableForColType(\"VARCHAR(32) PRIMARY KEY\", \"'\" + expectedStr + \"'\");\n+      String wrappedStr = null;\n+      if (expectedStr != null) {\n+        wrappedStr = \"'\" + expectedStr + \"'\";\n+      }\n+\n+      String [] types = { \"INT NOT NULL PRIMARY KEY\", \"VARCHAR(32)\" };\n+      String [] vals = { Integer.toString(i++) , wrappedStr };\n+      this.createTableWithColTypes(types, vals);\n       this.tableNames.add(this.getTableName());\n       this.removeTableDir();\n       incrementTableNum();\n@@ -100,13 +111,15 @@ public void testMultiTableImport() throws IOException {\n     runImport(argv);\n \n     Path warehousePath = new Path(this.getWarehouseDir());\n+    int i = 0;\n     for (String tableName : this.tableNames) {\n       Path tablePath = new Path(warehousePath, tableName);\n       Path filePath = new Path(tablePath, \"part-m-00000\");\n \n       // dequeue the expected value for this table. This\n       // list has the same order as the tableNames list.\n-      String expectedVal = this.expectedStrings.get(0);\n+      String expectedVal = Integer.toString(i++) + \",\"\n+          + this.expectedStrings.get(0);\n       this.expectedStrings.remove(0);\n \n       BufferedReader reader = new BufferedReader(",
                "raw_url": "https://github.com/apache/sqoop/raw/6174268d28ff0c0638fa8db39fb1a1e943daa285/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "sha": "31c5167d4e0d5523ca5c903e6fc5f620d72868e9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/sqoop/blob/6174268d28ff0c0638fa8db39fb1a1e943daa285/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java?ref=6174268d28ff0c0638fa8db39fb1a1e943daa285",
                "deletions": 0,
                "filename": "src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "patch": "@@ -37,6 +37,10 @@ public void testAllEmpty() {\n   public void testNullArgs() {\n     String result = FieldFormatter.escapeAndEnclose(\"\", null, null, null, false);\n     assertEquals(\"\", result);\n+\n+    char [] encloseFor = { '\\\"' };\n+    assertNull(FieldFormatter.escapeAndEnclose(null, \"\\\\\", \"\\\"\", encloseFor,\n+        false));\n   }\n \n   public void testBasicStr() {",
                "raw_url": "https://github.com/apache/sqoop/raw/6174268d28ff0c0638fa8db39fb1a1e943daa285/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "sha": "661a9ac98bb52a2dac6de286bb35a63c81ea1c78",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1313. Fix NPE in Sqoop when table with null fields uses escape\nduring import. Contributed by Aaron Kimball\n\nFrom: Christopher Douglas <cdouglas@apache.org>\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/sqoop/trunk@1149851 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/c7f64e4f8cd15b4f8a71b40d474c29fb2676492d",
        "patched_files": [
            "FieldFormatter.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestAllTables.java",
            "TestFieldFormatter.java"
        ]
    },
    "sqoop_666d549": {
        "bug_id": "sqoop_666d549",
        "commit": "https://github.com/apache/sqoop/commit/666d5499a2343c3489590deed3ef839e5532281d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/sqoop/blob/666d5499a2343c3489590deed3ef839e5532281d/src/java/org/apache/sqoop/manager/SqlManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/manager/SqlManager.java?ref=666d5499a2343c3489590deed3ef839e5532281d",
                "deletions": 4,
                "filename": "src/java/org/apache/sqoop/manager/SqlManager.java",
                "patch": "@@ -199,10 +199,9 @@ protected String getColNamesQuery(String tableName) {\n         results.close();\n         getConnection().commit();\n       }\n-    } catch (SQLException sqlException) {\n-      LOG.error(\"Error reading procedure metadata: \"\n-          + sqlException.toString());\n-      return null;\n+    } catch (SQLException e) {\n+      LOG.error(\"Error reading procedure metadata: \", e);\n+      throw new RuntimeException(\"Can't fetch column names for procedure.\", e);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/666d5499a2343c3489590deed3ef839e5532281d/src/java/org/apache/sqoop/manager/SqlManager.java",
                "sha": "f0a920df4d423ec0caa4efb34cd7678a8b8f54b9",
                "status": "modified"
            }
        ],
        "message": "SQOOP-860: NullPointerException when running procedure export against old database\n\n(Jarek Jarcec Cecho via Cheolsoo Park)",
        "parent": "https://github.com/apache/sqoop/commit/bae5604a3c2dcfa4c908b5370cb55bb4dc54235b",
        "patched_files": [
            "SqlManager.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestSqlManager.java"
        ]
    },
    "sqoop_6efcec0": {
        "bug_id": "sqoop_6efcec0",
        "commit": "https://github.com/apache/sqoop/commit/6efcec0da22fc737e1aa30b65b380b5c497deeb1",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/build.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/build.xml?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 1,
                "filename": "build.xml",
                "patch": "@@ -843,7 +843,7 @@\n         outputFile=\"${findbugs.output.xml.file}\" effort=\"max\"\n         excludeFilter=\"${findbugs.excludes}\">\n       <auxClasspath>\n-        <path refid=\"compile.classpath\"/>\n+        <path refid=\"test.classpath\"/>\n       </auxClasspath>\n       <sourcePath path=\"${src.dir}\" />\n       <sourcePath path=\"${test.dir}\" />",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/build.xml",
                "sha": "9d668ad963b3f566f1b775afdcce525517b2ec11",
                "status": "modified"
            },
            {
                "additions": 189,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/SqoopOptions.java",
                "changes": 200,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/SqoopOptions.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 11,
                "filename": "src/java/com/cloudera/sqoop/SqoopOptions.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.io.File;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Map;\n import java.util.Properties;\n \n import org.apache.commons.logging.Log;\n@@ -31,9 +32,9 @@\n import com.cloudera.sqoop.lib.LargeObjectLoader;\n \n /**\n- * Command-line arguments used by Sqoop.\n+ * Configurable state used by Sqoop tools.\n  */\n-public class SqoopOptions {\n+public class SqoopOptions implements Cloneable {\n \n   public static final Log LOG = LogFactory.getLog(SqoopOptions.class.getName());\n \n@@ -73,6 +74,22 @@ public String toString() {\n     SequenceFile\n   }\n \n+  /**\n+   * Incremental imports support two modes:\n+   * <ul>\n+   * <li>new rows being appended to the end of a table with an\n+   * incrementing id</li>\n+   * <li>new data results in a date-last-modified column being\n+   * updated to NOW(); Sqoop will pull all dirty rows in the next\n+   * incremental import.</li>\n+   * </ul>\n+   */\n+  public enum IncrementalMode {\n+    None,\n+    AppendRows,\n+    DateLastModified,\n+  }\n+\n \n   // TODO(aaron): Adding something here? Add a setter and a getter.\n   // Add a default value in initDefaults() if you need one.  If this value\n@@ -149,6 +166,26 @@ public String toString() {\n   private String hbaseRowKeyCol; // Column of the input to use as the row key.\n   private boolean hbaseCreateTable; // if true, create tables/col families.\n \n+  // col to filter on for incremental imports.\n+  private String incrementalTestCol; \n+  // incremental import mode we're using.\n+  private IncrementalMode incrementalMode;\n+  // What was the last-imported value of incrementalTestCol?\n+  private String incrementalLastValue;\n+\n+\n+  // These next two fields are not serialized to the metastore.\n+  // If this SqoopOptions is created by reading a saved session, these will\n+  // be populated by the SessionStorage to facilitate updating the same\n+  // session.\n+  private String sessionName;\n+  private Map<String, String> sessionStorageDescriptor;\n+\n+  // If we restore a session and then allow the user to apply arguments on\n+  // top, we retain the version without the arguments in a reference to the\n+  // 'parent' SqoopOptions instance, here.\n+  private SqoopOptions parent;\n+\n   public SqoopOptions() {\n     initDefaults(null);\n   }\n@@ -356,10 +393,12 @@ public void loadProperties(Properties props) {\n         this.targetDir);\n     this.append = getBooleanProperty(props, \"hdfs.append.dir\", this.append);\n     \n-    String fileFmtStr = props.getProperty(\"hdfs.file.format\", \"text\");\n-    if (fileFmtStr.equals(\"seq\")) {\n-      this.layout = FileLayout.SequenceFile;\n-    } else {\n+    try {\n+      this.layout = FileLayout.valueOf(\n+        props.getProperty(\"hdfs.file.format\", this.layout.toString()));\n+    } catch (IllegalArgumentException iae) {\n+      LOG.warn(\"Unsupported file format: \"\n+          + props.getProperty(\"hdfs.file.format\", null) + \"; setting to text\");\n       this.layout = FileLayout.TextFile;\n     }\n \n@@ -410,6 +449,20 @@ public void loadProperties(Properties props) {\n         this.hbaseRowKeyCol);\n     this.hbaseCreateTable = getBooleanProperty(props, \"hbase.create.table\",\n         this.hbaseCreateTable);\n+\n+    try {\n+      this.incrementalMode = IncrementalMode.valueOf(props.getProperty(\n+          \"incremental.mode\", this.incrementalMode.toString()));\n+    } catch (IllegalArgumentException iae) {\n+      LOG.warn(\"Invalid incremental import type: \"\n+          + props.getProperty(\"incremental.mode\", null) + \"; setting to None\");\n+      this.incrementalMode = IncrementalMode.None;\n+    }\n+\n+    this.incrementalTestCol = props.getProperty(\"incremental.col\",\n+        this.incrementalTestCol);\n+    this.incrementalLastValue = props.getProperty(\"incremental.last.value\",\n+        this.incrementalLastValue);\n   }\n \n   /**\n@@ -448,11 +501,7 @@ public Properties writeProperties() {\n     putProperty(props, \"hdfs.warehouse.dir\", this.warehouseDir);\n     putProperty(props, \"hdfs.target.dir\", this.targetDir);\n     putProperty(props, \"hdfs.append.dir\", Boolean.toString(this.append));\n-    if (this.layout == FileLayout.SequenceFile) {\n-      putProperty(props, \"hdfs.file.format\", \"seq\");\n-    } else {\n-      putProperty(props, \"hdfs.file.format\", \"text\");\n-    }\n+    putProperty(props, \"hdfs.file.format\", this.layout.toString());\n     putProperty(props, \"direct.import\", Boolean.toString(this.direct));\n     putProperty(props, \"hive.import\", Boolean.toString(this.hiveImport));\n     putProperty(props, \"hive.overwrite.table\",\n@@ -482,9 +531,48 @@ public Properties writeProperties() {\n     putProperty(props, \"hbase.create.table\",\n         Boolean.toString(this.hbaseCreateTable));\n \n+    putProperty(props, \"incremental.mode\", this.incrementalMode.toString());\n+    putProperty(props, \"incremental.col\", this.incrementalTestCol);\n+    putProperty(props, \"incremental.last.value\", this.incrementalLastValue);\n+\n     return props;\n   }\n \n+  @Override\n+  public Object clone() {\n+    try {\n+      SqoopOptions other = (SqoopOptions) super.clone();\n+      if (null != columns) {\n+        other.columns = Arrays.copyOf(columns, columns.length);\n+      }\n+\n+      if (null != dbOutColumns) {\n+        other.dbOutColumns = Arrays.copyOf(dbOutColumns, dbOutColumns.length);\n+      }\n+\n+      if (null != inputDelimiters) {\n+        other.inputDelimiters = (DelimiterSet) inputDelimiters.clone();\n+      }\n+\n+      if (null != outputDelimiters) {\n+        other.outputDelimiters = (DelimiterSet) outputDelimiters.clone();\n+      }\n+\n+      if (null != conf) {\n+        other.conf = new Configuration(conf);\n+      }\n+\n+      if (null != extraArgs) {\n+        other.extraArgs = Arrays.copyOf(extraArgs, extraArgs.length);\n+      }\n+\n+      return other;\n+    } catch (CloneNotSupportedException cnse) {\n+      // Shouldn't happen.\n+      return null;\n+    }\n+  }\n+\n   /**\n    * @return the temp directory to use; this is guaranteed to end with\n    * the file separator character (e.g., '/').\n@@ -536,6 +624,8 @@ private void initDefaults(Configuration baseConfiguration) {\n     this.extraArgs = null;\n \n     this.dbOutColumns = null;\n+\n+    this.incrementalMode = IncrementalMode.None;\n   }\n \n   /**\n@@ -1312,5 +1402,93 @@ public String getHBaseTable() {\n   public void setHBaseTable(String table) {\n     this.hbaseTable = table;\n   }\n+\n+  /**\n+   * Set the column of the import source table to check for incremental import\n+   * state.\n+   */\n+  public void setIncrementalTestColumn(String colName) {\n+    this.incrementalTestCol = colName;\n+  }\n+\n+  /**\n+   * Return the name of the column of the import source table\n+   * to check for incremental import state.\n+   */\n+  public String getIncrementalTestColumn() {\n+    return this.incrementalTestCol;\n+  }\n+\n+  /**\n+   * Set the incremental import mode to use.\n+   */\n+  public void setIncrementalMode(IncrementalMode mode) {\n+    this.incrementalMode = mode;\n+  }\n+\n+  /**\n+   * Get the incremental import mode to use.\n+   */\n+  public IncrementalMode getIncrementalMode() { \n+    return this.incrementalMode;\n+  }\n+\n+  /**\n+   * Set the last imported value of the incremental import test column.\n+   */\n+  public void setIncrementalLastValue(String lastVal) {\n+    this.incrementalLastValue = lastVal;\n+  }\n+\n+  /**\n+   * Get the last imported value of the incremental import test column.\n+   */\n+  public String getIncrementalLastValue() {\n+    return this.incrementalLastValue;\n+  }\n+\n+  /**\n+   * Set the name of the saved session this SqoopOptions belongs to.\n+   */\n+  public void setSessionName(String session) {\n+    this.sessionName = session;\n+  }\n+\n+  /**\n+   * Get the name of the saved session this SqoopOptions belongs to.\n+   */\n+  public String getSessionName() {\n+    return this.sessionName;\n+  }\n+\n+  /**\n+   * Set the SessionStorage descriptor used to open the saved session\n+   * this SqoopOptions belongs to.\n+   */\n+  public void setStorageDescriptor(Map<String, String> descriptor) {\n+    this.sessionStorageDescriptor = descriptor;\n+  }\n+\n+  /**\n+   * Get the SessionStorage descriptor used to open the saved session\n+   * this SqoopOptions belongs to.\n+   */\n+  public Map<String, String> getStorageDescriptor() {\n+    return this.sessionStorageDescriptor;\n+  }\n+\n+  /**\n+   * Return the parent instance this SqoopOptions is derived from.\n+   */\n+  public SqoopOptions getParent() {\n+    return this.parent;\n+  }\n+\n+  /**\n+   * Set the parent instance this SqoopOptions is derived from.\n+   */\n+  public void setParent(SqoopOptions options) {\n+    this.parent = options;\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/SqoopOptions.java",
                "sha": "9b471cd9607a8df624a29096ae8dd5b97272b823",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/ConnManager.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/manager/ConnManager.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/manager/ConnManager.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.sql.Connection;\n import java.sql.ResultSet;\n import java.sql.SQLException;\n+import java.sql.Timestamp;\n import java.util.Map;\n \n import org.apache.commons.logging.Log;\n@@ -200,5 +201,22 @@ public void updateTable(ExportJobContext context)\n    * to close.\n    */\n   public abstract void release();\n+\n+  /**\n+   * Return the current time from the perspective of the database server.\n+   * Return null if this cannot be accessed.\n+   */\n+  public Timestamp getCurrentDbTimestamp() {\n+    LOG.warn(\"getCurrentDbTimestamp(): Using local system timestamp.\");\n+    return new Timestamp(System.currentTimeMillis());\n+  }\n+\n+  /**\n+   * Given a non-null Timestamp, return the quoted string that can\n+   * be inserted into a SQL statement, representing that timestamp.\n+   */\n+  public String timestampToQueryString(Timestamp ts) {\n+    return \"'\" + ts + \"'\";\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/ConnManager.java",
                "sha": "bfa11a2338fb75f9c9e835a3935414eebad6c921",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/HsqldbManager.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/manager/HsqldbManager.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/manager/HsqldbManager.java",
                "patch": "@@ -52,4 +52,14 @@ public HsqldbManager(final SqoopOptions opts) {\n     String [] databases = {HSQL_SCHEMA_NAME};\n     return databases;\n   }\n+\n+  @Override\n+  /**\n+   * {@inheritDoc}\n+   */\n+  protected String getCurTimestampQuery() {\n+    // HSQLDB requires that you select from a table; this table is\n+    // guaranteed to exist.\n+    return \"SELECT CURRENT_TIMESTAMP FROM INFORMATION_SCHEMA.SYSTEM_TABLES\";\n+  }\n }",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/HsqldbManager.java",
                "sha": "3d04e876ff76b48211a8796110e30d9146bb5dbc",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/OracleManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/manager/OracleManager.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/manager/OracleManager.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.sql.DriverManager;\n import java.sql.ResultSet;\n import java.sql.SQLException;\n+import java.sql.Timestamp;\n import java.util.HashMap;\n import java.util.Map;\n import java.lang.reflect.Method;\n@@ -456,5 +457,15 @@ protected void finalize() throws Throwable {\n     close();\n     super.finalize();\n   }\n+\n+  @Override\n+  protected String getCurTimestampQuery() {\n+    return \"SELECT SYSDATE FROM dual\";\n+  }\n+\n+  @Override\n+  public String timestampToQueryString(Timestamp ts) {\n+    return \"TO_TIMESTAMP('\" + ts + \"', 'YYYY-MM-DD HH24:MI:SS.FF')\";\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/OracleManager.java",
                "sha": "285c8c007d4b92ae2018105787f0d5a9031f003c",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/SqlManager.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/manager/SqlManager.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 1,
                "filename": "src/java/com/cloudera/sqoop/manager/SqlManager.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package com.cloudera.sqoop.manager;\n \n+import java.sql.Timestamp;\n+\n import com.cloudera.sqoop.SqoopOptions;\n import com.cloudera.sqoop.hive.HiveTypes;\n import com.cloudera.sqoop.lib.BlobRef;\n@@ -616,13 +618,61 @@ public void release() {\n     }\n   }\n \n+  @Override\n   /**\n-   * @{inheritDoc}\n+   * {@inheritDoc}\n    */\n   public void updateTable(ExportJobContext context)\n       throws IOException, ExportException {\n     context.setConnManager(this);\n     JdbcUpdateExportJob exportJob = new JdbcUpdateExportJob(context);\n     exportJob.runExport();\n   }\n+\n+  /**\n+   * @return a SQL query to retrieve the current timestamp from the db.\n+   */\n+  protected String getCurTimestampQuery() {\n+    return \"SELECT CURRENT_TIMESTAMP()\";\n+  }\n+\n+  @Override\n+  /**\n+   * {@inheritDoc}\n+   */\n+  public Timestamp getCurrentDbTimestamp() {\n+    release(); // Release any previous ResultSet.\n+\n+    Statement s = null;\n+    ResultSet rs = null;\n+    try {\n+      Connection c = getConnection();\n+      s = c.createStatement();\n+      rs = s.executeQuery(getCurTimestampQuery());\n+      if (rs == null || !rs.next()) {\n+        return null; // empty ResultSet.\n+      }\n+\n+      return rs.getTimestamp(1);\n+    } catch (SQLException sqlE) {\n+      LOG.warn(\"SQL exception accessing current timestamp: \" + sqlE);\n+      return null;\n+    } finally {\n+      try {\n+        if (null != rs) {\n+          rs.close();\n+        }\n+      } catch (SQLException sqlE) {\n+        LOG.warn(\"SQL Exception closing resultset: \" + sqlE);\n+      }\n+\n+      try {\n+        if (null != s) {\n+          s.close();\n+        }\n+      } catch (SQLException sqlE) {\n+        LOG.warn(\"SQL Exception closing statement: \" + sqlE);\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/manager/SqlManager.java",
                "sha": "8ba8efffdf8d80d6599072ea0a41c1b23d57de12",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/mapreduce/JobBase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/mapreduce/JobBase.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 2,
                "filename": "src/java/com/cloudera/sqoop/mapreduce/JobBase.java",
                "patch": "@@ -29,8 +29,6 @@\n \n import org.apache.hadoop.conf.Configuration;\n \n-import org.apache.hadoop.filecache.DistributedCache;\n-\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapreduce.InputFormat;",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/mapreduce/JobBase.java",
                "sha": "37483d04f7e8ac1008a66843dab4276debacdaa4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/metastore/hsqldb/AutoHsqldbStorage.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/metastore/hsqldb/AutoHsqldbStorage.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/metastore/hsqldb/AutoHsqldbStorage.java",
                "patch": "@@ -106,6 +106,7 @@ public void open(Map<String, String> descriptor) throws IOException {\n     setMetastoreUser(conf.get(AUTO_STORAGE_USER_KEY, DEFAULT_AUTO_USER));\n     setMetastorePassword(conf.get(AUTO_STORAGE_PASS_KEY,\n         DEFAULT_AUTO_PASSWORD));\n+    setConnectedDescriptor(descriptor);\n \n     init();\n   }",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/metastore/hsqldb/AutoHsqldbStorage.java",
                "sha": "e51190fd74573d93578ea35ba9d454484b5bde8e",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/metastore/hsqldb/HsqldbSessionStorage.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/metastore/hsqldb/HsqldbSessionStorage.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/metastore/hsqldb/HsqldbSessionStorage.java",
                "patch": "@@ -117,6 +117,7 @@\n   private static final String SQOOP_TOOL_KEY = \"sqoop.tool\";\n \n \n+  private Map<String, String> connectedDescriptor;\n   private String metastoreConnectStr;\n   private String metastoreUser;\n   private String metastorePassword;\n@@ -144,6 +145,13 @@ protected void setMetastorePassword(String pass) {\n \n   private static final String DB_DRIVER_CLASS = \"org.hsqldb.jdbcDriver\";\n \n+  /**\n+   * Set the descriptor used to open() this storage.\n+   */\n+  protected void setConnectedDescriptor(Map<String, String> descriptor) {\n+    this.connectedDescriptor = descriptor;\n+  }\n+\n   @Override\n   /**\n    * Initialize the connection to the database.\n@@ -152,6 +160,7 @@ public void open(Map<String, String> descriptor) throws IOException {\n     setMetastoreConnectStr(descriptor.get(META_CONNECT_KEY));\n     setMetastoreUser(descriptor.get(META_USERNAME_KEY));\n     setMetastorePassword(descriptor.get(META_PASSWORD_KEY));\n+    setConnectedDescriptor(descriptor);\n \n     init();\n   }\n@@ -293,6 +302,10 @@ public SessionData read(String sessionName) throws IOException {\n       opts.setConf(conf);\n       opts.loadProperties(sqoopOptProps);\n \n+      // Set the session connection information for this session.\n+      opts.setSessionName(sessionName);\n+      opts.setStorageDescriptor(connectedDescriptor);\n+\n       return new SessionData(opts, tool);\n     } catch (SQLException sqlE) {\n       throw new IOException(\"Error communicating with database\", sqlE);",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/metastore/hsqldb/HsqldbSessionStorage.java",
                "sha": "5009dc3a8cd5c625eb3a677b6a8408841e7a20e7",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/orm/ClassWriter.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/orm/ClassWriter.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/orm/ClassWriter.java",
                "patch": "@@ -909,6 +909,23 @@ public void generate() throws IOException {\n         colNames = connManager.getColumnNamesForQuery(\n             this.options.getSqlQuery());\n       }\n+    } else {\n+      // These column names were provided by the user. They may not be in\n+      // the same case as the keys in the columnTypes map. So make sure\n+      // we add the appropriate aliases in that map.\n+      for (String userColName : colNames) {\n+        for (Map.Entry<String, Integer> typeEntry : columnTypes.entrySet()) {\n+          String typeColName = typeEntry.getKey();\n+          if (typeColName.equalsIgnoreCase(userColName)\n+              && !typeColName.equals(userColName)) {\n+            // We found the correct-case equivalent.\n+            columnTypes.put(userColName, typeEntry.getValue());\n+            // No need to continue iteration; only one could match.\n+            // Also, the use of put() just invalidated the iterator.\n+            break;\n+          }\n+        }\n+      }\n     }\n \n     // Translate all the column names into names that are safe to",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/orm/ClassWriter.java",
                "sha": "52bcb6e0e3d38ff6968d723b73488a06c3308403",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/orm/CompilationManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/orm/CompilationManager.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 4,
                "filename": "src/java/com/cloudera/sqoop/orm/CompilationManager.java",
                "patch": "@@ -23,10 +23,7 @@\n import java.io.FileOutputStream;\n import java.io.IOException;\n import java.io.OutputStream;\n-import java.net.URL;\n-import java.net.URLDecoder;\n import java.util.ArrayList;\n-import java.util.Enumeration;\n import java.util.List;\n import java.util.jar.JarOutputStream;\n import java.util.zip.ZipEntry;\n@@ -42,7 +39,6 @@\n \n import com.cloudera.sqoop.SqoopOptions;\n import com.cloudera.sqoop.util.FileListing;\n-import com.cloudera.sqoop.shims.HadoopShim;\n \n import com.cloudera.sqoop.util.Jars;\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/orm/CompilationManager.java",
                "sha": "4e46c2a17dd0130ee012abbe15d1e81080b8ecab",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/tool/BaseSqoopTool.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/tool/BaseSqoopTool.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/java/com/cloudera/sqoop/tool/BaseSqoopTool.java",
                "patch": "@@ -114,6 +114,11 @@\n   public static final String HELP_ARG = \"help\";\n   public static final String UPDATE_KEY_ARG = \"update-key\";\n \n+  // Arguments for incremental imports.\n+  public static final String INCREMENT_TYPE_ARG = \"incremental\";\n+  public static final String INCREMENT_COL_ARG = \"check-column\";\n+  public static final String INCREMENT_LAST_VAL_ARG = \"last-value\";\n+\n   // HBase arguments.\n   public static final String HBASE_TABLE_ARG = \"hbase-table\";\n   public static final String HBASE_COL_FAM_ARG = \"column-family\";",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/tool/BaseSqoopTool.java",
                "sha": "0320fe3b40d3efdd196af0ab08a721d630d7b491",
                "status": "modified"
            },
            {
                "additions": 339,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/tool/ImportTool.java",
                "changes": 340,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/tool/ImportTool.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 1,
                "filename": "src/java/com/cloudera/sqoop/tool/ImportTool.java",
                "patch": "@@ -19,7 +19,16 @@\n package com.cloudera.sqoop.tool;\n \n import java.io.IOException;\n+\n+import java.math.BigDecimal;\n+\n+import java.sql.Connection;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.sql.Timestamp;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.cli.CommandLine;\n import org.apache.commons.cli.OptionBuilder;\n@@ -35,6 +44,10 @@\n import com.cloudera.sqoop.cli.ToolOptions;\n import com.cloudera.sqoop.hive.HiveImport;\n import com.cloudera.sqoop.manager.ImportJobContext;\n+\n+import com.cloudera.sqoop.metastore.SessionData;\n+import com.cloudera.sqoop.metastore.SessionStorage;\n+import com.cloudera.sqoop.metastore.SessionStorageFactory;\n import com.cloudera.sqoop.util.AppendUtils;\n import com.cloudera.sqoop.util.ImportException;\n import org.apache.hadoop.fs.Path;\n@@ -76,8 +89,239 @@ protected boolean init(SqoopOptions sqoopOpts) {\n   public List<String> getGeneratedJarFiles() {\n     return this.codeGenerator.getGeneratedJarFiles();\n   }\n+\n+  /**\n+   * @return true if the supplied options specify an incremental import.\n+   */\n+  private boolean isIncremental(SqoopOptions options) {\n+    return !options.getIncrementalMode().equals(\n+        SqoopOptions.IncrementalMode.None);\n+  }\n   \n-  protected void importTable(SqoopOptions options, String tableName,\n+  /**\n+   * If this is an incremental import, then we should save the\n+   * user's state back to the metastore (if this session was run\n+   * from the metastore). Otherwise, log to the user what data\n+   * they need to supply next time.\n+   */\n+  private void saveIncrementalState(SqoopOptions options)\n+      throws IOException {\n+    if (!isIncremental(options)) {\n+      return;\n+    }\n+\n+    Map<String, String> descriptor = options.getStorageDescriptor();\n+    String sessionName = options.getSessionName();\n+\n+    if (null != sessionName && null != descriptor) {\n+      // Actually save it back to the metastore.\n+      LOG.info(\"Saving incremental import state to the metastore\");\n+      SessionStorageFactory ssf = new SessionStorageFactory(options.getConf());\n+      SessionStorage storage = ssf.getSessionStorage(descriptor);\n+      storage.open(descriptor);\n+      try {\n+        // Save the 'parent' SqoopOptions; this does not contain the mutations\n+        // to the SqoopOptions state that occurred over the course of this\n+        // execution, except for the one we specifically want to memorize:\n+        // the latest value of the check column.\n+        SessionData data = new SessionData(options.getParent(), this);\n+        storage.update(sessionName, data);\n+        LOG.info(\"Updated data for session: \" + sessionName);\n+      } finally {\n+        storage.close();\n+      }\n+    } else {\n+      // If there wasn't a parent SqoopOptions, then the incremental\n+      // state data was stored in the current SqoopOptions.\n+      LOG.info(\"Incremental import complete! To run another incremental \"\n+          + \"import of all data following this import, supply the \"\n+          + \"following arguments:\");\n+      SqoopOptions.IncrementalMode incrementalMode =\n+          options.getIncrementalMode();\n+      switch (incrementalMode) {\n+      case AppendRows:\n+        LOG.info(\" --incremental append\");\n+        break;\n+      case DateLastModified:\n+        LOG.info(\" --incremental lastmodified\");\n+        break;\n+      default:\n+        LOG.warn(\"Undefined incremental mode: \" + incrementalMode);\n+        break;\n+      }\n+      LOG.info(\"  --check-column \" + options.getIncrementalTestColumn());\n+      LOG.info(\"  --last-value \" + options.getIncrementalLastValue());\n+      LOG.info(\"(Consider saving this with 'sqoop session --create')\");\n+    }\n+  }\n+\n+  /**\n+   * Return the max value in the incremental-import test column. This\n+   * value must be numeric.\n+   */\n+  private BigDecimal getMaxColumnId(SqoopOptions options) throws SQLException {\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"SELECT MAX(\");\n+    sb.append(options.getIncrementalTestColumn());\n+    sb.append(\") FROM \");\n+    sb.append(options.getTableName());\n+\n+    String where = options.getWhereClause();\n+    if (null != where) {\n+      sb.append(\" WHERE \");\n+      sb.append(where);\n+    }\n+\n+    Connection conn = manager.getConnection();\n+    Statement s = null;\n+    ResultSet rs = null;\n+    try {\n+      s = conn.createStatement();\n+      rs = s.executeQuery(sb.toString());\n+      if (!rs.next()) {\n+        // This probably means the table is empty.\n+        LOG.warn(\"Unexpected: empty results for max value query?\");\n+        return null;\n+      }\n+\n+      return rs.getBigDecimal(1);\n+    } finally {\n+      try {\n+        if (null != rs) {\n+          rs.close();\n+        }\n+      } catch (SQLException sqlE) {\n+        LOG.warn(\"SQL Exception closing resultset: \" + sqlE);\n+      }\n+\n+      try {\n+        if (null != s) {\n+          s.close();\n+        }\n+      } catch (SQLException sqlE) {\n+        LOG.warn(\"SQL Exception closing statement: \" + sqlE);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Initialize the constraints which set the incremental import range.\n+   * @return false if an import is not necessary, because the dataset has not\n+   * changed.\n+   */\n+  private boolean initIncrementalConstraints(SqoopOptions options,\n+      ImportJobContext context) throws ImportException, IOException {\n+\n+    // If this is an incremental import, determine the constraints\n+    // to inject in the WHERE clause or $CONDITIONS for a query.\n+    // Also modify the 'last value' field of the SqoopOptions to\n+    // specify the current job start time / start row.\n+\n+    if (!isIncremental(options)) {\n+      return true;\n+    }\n+\n+    SqoopOptions.IncrementalMode incrementalMode = options.getIncrementalMode();\n+    String nextIncrementalValue = null;\n+\n+    switch (incrementalMode) {\n+    case AppendRows:\n+      try {\n+        BigDecimal nextVal = getMaxColumnId(options);\n+        if (null != nextVal) {\n+          nextIncrementalValue = nextVal.toString();\n+        }\n+      } catch (SQLException sqlE) {\n+        throw new IOException(sqlE);\n+      }\n+      break;\n+    case DateLastModified:\n+      Timestamp dbTimestamp = manager.getCurrentDbTimestamp();\n+      if (null == dbTimestamp) {\n+        throw new IOException(\"Could not get current time from database\");\n+      }\n+\n+      nextIncrementalValue = manager.timestampToQueryString(dbTimestamp);\n+      break;\n+    default:\n+      throw new ImportException(\"Undefined incremental import type: \"\n+          + incrementalMode);\n+    }\n+\n+    // Build the WHERE clause components that are used to import\n+    // only this incremental section.\n+    StringBuilder sb = new StringBuilder();\n+    String prevEndpoint = options.getIncrementalLastValue();\n+\n+    String checkColName = manager.escapeColName(\n+        options.getIncrementalTestColumn());\n+    LOG.info(\"Incremental import based on column \" + checkColName);\n+    if (null != prevEndpoint) {\n+      if (prevEndpoint.equals(nextIncrementalValue)) {\n+        LOG.info(\"No new rows detected since last import.\");\n+        return false;\n+      }\n+      LOG.info(\"Lower bound value: \" + prevEndpoint);\n+      sb.append(checkColName);\n+      switch (incrementalMode) {\n+      case AppendRows:\n+        sb.append(\" > \");\n+        break;\n+      case DateLastModified:\n+        sb.append(\" >= \");\n+        break;\n+      default:\n+        throw new ImportException(\"Undefined comparison\");\n+      }\n+      sb.append(prevEndpoint);\n+      sb.append(\" AND \");\n+    }\n+\n+    if (null != nextIncrementalValue) {\n+      sb.append(checkColName);\n+      switch (incrementalMode) {\n+      case AppendRows:\n+        sb.append(\" <= \");\n+        break;\n+      case DateLastModified:\n+        sb.append(\" < \");\n+        break;\n+      default:\n+        throw new ImportException(\"Undefined comparison\");\n+      }\n+      sb.append(nextIncrementalValue);\n+    } else {\n+      sb.append(checkColName);\n+      sb.append(\" IS NULL \");\n+    }\n+\n+    LOG.info(\"Upper bound value: \" + nextIncrementalValue);\n+\n+    String prevWhereClause = options.getWhereClause();\n+    if (null != prevWhereClause) {\n+      sb.append(\" AND (\");\n+      sb.append(prevWhereClause);\n+      sb.append(\")\");\n+    }\n+\n+    String newConstraints = sb.toString();\n+    options.setWhereClause(newConstraints);\n+\n+    // Save this state for next time.\n+    SqoopOptions recordOptions = options.getParent();\n+    if (null == recordOptions) {\n+      recordOptions = options;\n+    }\n+    recordOptions.setIncrementalLastValue(nextIncrementalValue);\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Import a table or query.\n+   * @return true if an import was performed, false otherwise.\n+   */\n+  protected boolean importTable(SqoopOptions options, String tableName,\n       HiveImport hiveImport) throws IOException, ImportException {\n     String jarFile = null;\n \n@@ -88,6 +332,12 @@ protected void importTable(SqoopOptions options, String tableName,\n     ImportJobContext context = new ImportJobContext(tableName, jarFile,\n         options, getOutputPath(options, tableName));\n     \n+    // If we're doing an incremental import, set up the\n+    // filtering conditions used to get the latest records.\n+    if (!initIncrementalConstraints(options, context)) {\n+      return false;\n+    }\n+    \n     if (null != tableName) {\n       manager.importTable(context);\n     } else {\n@@ -103,6 +353,10 @@ protected void importTable(SqoopOptions options, String tableName,\n     if (options.doHiveImport()) {\n       hiveImport.importTable(tableName, options.getHiveTableName(), false);\n     }\n+\n+    saveIncrementalState(options);\n+\n+    return true;\n   }\n   \n   /**   \n@@ -264,12 +518,42 @@ protected RelatedOptions getImportOptions() {\n     return importOpts;\n   }\n \n+  /**\n+   * Return options for incremental import.\n+   */\n+  protected RelatedOptions getIncrementalOptions() {\n+    RelatedOptions incrementalOpts =\n+        new RelatedOptions(\"Incremental import arguments\");\n+\n+    incrementalOpts.addOption(OptionBuilder.withArgName(\"import-type\")\n+        .hasArg()\n+        .withDescription(\n+        \"Define an incremental import of type 'append' or 'lastmodified'\")\n+        .withLongOpt(INCREMENT_TYPE_ARG)\n+        .create());\n+    incrementalOpts.addOption(OptionBuilder.withArgName(\"column\")\n+        .hasArg()\n+        .withDescription(\"Source column to check for incremental change\")\n+        .withLongOpt(INCREMENT_COL_ARG)\n+        .create());\n+    incrementalOpts.addOption(OptionBuilder.withArgName(\"value\")\n+        .hasArg()\n+        .withDescription(\"Last imported value in the incremental check column\")\n+        .withLongOpt(INCREMENT_LAST_VAL_ARG)\n+        .create());\n+\n+    return incrementalOpts;\n+  }\n+\n   @Override\n   /** Configure the command-line arguments we expect to receive */\n   public void configureOptions(ToolOptions toolOptions) {\n \n     toolOptions.addUniqueOptions(getCommonOptions());\n     toolOptions.addUniqueOptions(getImportOptions());\n+    if (!allTables) {\n+      toolOptions.addUniqueOptions(getIncrementalOptions());\n+    }\n     toolOptions.addUniqueOptions(getOutputFormatOptions());\n     toolOptions.addUniqueOptions(getInputFormatOptions());\n     toolOptions.addUniqueOptions(getHiveOptions(true));\n@@ -306,6 +590,32 @@ public void printHelp(ToolOptions toolOptions) {\n         \"after a '--' on the command line.\");\n   }\n \n+  private void applyIncrementalOptions(CommandLine in, SqoopOptions out)\n+      throws InvalidOptionsException  {\n+    if (in.hasOption(INCREMENT_TYPE_ARG)) {\n+      String incrementalTypeStr = in.getOptionValue(INCREMENT_TYPE_ARG);\n+      if (\"append\".equals(incrementalTypeStr)) {\n+        out.setIncrementalMode(SqoopOptions.IncrementalMode.AppendRows);\n+        // This argument implies ability to append to the same directory.\n+        out.setAppendMode(true);\n+      } else if (\"lastmodified\".equals(incrementalTypeStr)) {\n+        out.setIncrementalMode(SqoopOptions.IncrementalMode.DateLastModified);\n+      } else {\n+        throw new InvalidOptionsException(\"Unknown incremental import mode: \"\n+            + incrementalTypeStr + \". Use 'append' or 'lastmodified'.\"\n+            + HELP_STR);\n+      }\n+    }\n+\n+    if (in.hasOption(INCREMENT_COL_ARG)) {\n+      out.setIncrementalTestColumn(in.getOptionValue(INCREMENT_COL_ARG));\n+    }\n+\n+    if (in.hasOption(INCREMENT_LAST_VAL_ARG)) {\n+      out.setIncrementalLastValue(in.getOptionValue(INCREMENT_LAST_VAL_ARG));\n+    }\n+  }\n+\n   @Override\n   /** {@inheritDoc} */\n   public void applyOptions(CommandLine in, SqoopOptions out)\n@@ -382,6 +692,7 @@ public void applyOptions(CommandLine in, SqoopOptions out)\n         out.setExistingJarName(in.getOptionValue(JAR_FILE_NAME_ARG));\n       }\n \n+      applyIncrementalOptions(in, out);\n       applyHiveOptions(in, out);\n       applyOutputFormatOptions(in, out);\n       applyInputFormatOptions(in, out);\n@@ -437,6 +748,32 @@ protected void validateImportOptions(SqoopOptions options)\n     }\n   }\n \n+  /**\n+   * Validate the incremental import options.\n+   */\n+  private void validateIncrementalOptions(SqoopOptions options)\n+      throws InvalidOptionsException {\n+    if (options.getIncrementalMode() != SqoopOptions.IncrementalMode.None\n+        && options.getIncrementalTestColumn() == null) {\n+      throw new InvalidOptionsException(\n+          \"For an incremental import, the check column must be specified \"\n+          + \"with --\" + INCREMENT_COL_ARG + \". \" + HELP_STR);\n+    }\n+\n+    if (options.getIncrementalMode() == SqoopOptions.IncrementalMode.None\n+        && options.getIncrementalTestColumn() != null) {\n+      throw new InvalidOptionsException(\n+          \"You must specify an incremental import mode with --\"\n+          + INCREMENT_TYPE_ARG + \". \" + HELP_STR);\n+    }\n+\n+    if (options.getIncrementalMode() != SqoopOptions.IncrementalMode.None\n+        && options.getTableName() == null) {\n+      throw new InvalidOptionsException(\"Incremental imports require a table.\"\n+          + HELP_STR);\n+    }\n+  }\n+\n   @Override\n   /** {@inheritDoc} */\n   public void validateOptions(SqoopOptions options)\n@@ -452,6 +789,7 @@ public void validateOptions(SqoopOptions options)\n     }\n \n     validateImportOptions(options);\n+    validateIncrementalOptions(options);\n     validateCommonOptions(options);\n     validateCodeGenOptions(options);\n     validateOutputFormatOptions(options);",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/tool/ImportTool.java",
                "sha": "6b298acc316b0fec58057984cfe0c9f402f6394a",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/tool/SessionTool.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/tool/SessionTool.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 2,
                "filename": "src/java/com/cloudera/sqoop/tool/SessionTool.java",
                "patch": "@@ -209,6 +209,12 @@ private int execSession(SqoopOptions opts) throws IOException {\n     SqoopOptions childOpts = data.getSqoopOptions();\n     SqoopTool childTool = data.getSqoopTool();\n \n+    // Don't overwrite the original SqoopOptions with the\n+    // arguments; make a child options.\n+\n+    SqoopOptions clonedOpts = (SqoopOptions) childOpts.clone();\n+    clonedOpts.setParent(childOpts);\n+\n     int dashPos = getDashPosition(extraArguments);\n     String [] childArgv;\n     if (dashPos >= extraArguments.length) {\n@@ -218,13 +224,13 @@ private int execSession(SqoopOptions opts) throws IOException {\n           extraArguments.length);\n     }\n \n-    int confRet = configureChildTool(childOpts, childTool, childArgv);\n+    int confRet = configureChildTool(clonedOpts, childTool, childArgv);\n     if (0 != confRet) {\n       // Error.\n       return confRet;\n     }\n \n-    return childTool.run(childOpts);\n+    return childTool.run(clonedOpts);\n   }\n \n   private int showSession(SqoopOptions opts) throws IOException {",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/java/com/cloudera/sqoop/tool/SessionTool.java",
                "sha": "d395a0b1f1a9fe68f41bfc0a2bbeb054bb3ab1fe",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/test/com/cloudera/sqoop/AllTests.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/AllTests.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/test/com/cloudera/sqoop/AllTests.java",
                "patch": "@@ -38,6 +38,7 @@ public static Test suite() {\n     suite.addTest(ThirdPartyTests.suite());\n     suite.addTestSuite(TestHBaseImport.class);\n     suite.addTestSuite(TestHBaseQueryImport.class);\n+    suite.addTestSuite(TestIncrementalImport.class);\n \n     return suite;\n   }",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/test/com/cloudera/sqoop/AllTests.java",
                "sha": "7bff1e0f6338c418143ab9e0c290a0afa0c74c11",
                "status": "modified"
            },
            {
                "additions": 790,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/test/com/cloudera/sqoop/TestIncrementalImport.java",
                "changes": 790,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/TestIncrementalImport.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 0,
                "filename": "src/test/com/cloudera/sqoop/TestIncrementalImport.java",
                "patch": "@@ -0,0 +1,790 @@\n+/**\n+ * Licensed to Cloudera, Inc. under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  Cloudera, Inc. licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.cloudera.sqoop;\n+\n+import java.io.BufferedReader;\n+import java.io.InputStreamReader;\n+\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Timestamp;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.StringUtils;\n+\n+import com.cloudera.sqoop.manager.ConnManager;\n+import com.cloudera.sqoop.manager.HsqldbManager;\n+import com.cloudera.sqoop.manager.ManagerFactory;\n+import com.cloudera.sqoop.metastore.TestSessions;\n+import com.cloudera.sqoop.testutil.BaseSqoopTestCase;\n+import com.cloudera.sqoop.testutil.CommonArgs;\n+import com.cloudera.sqoop.tool.ImportTool;\n+import com.cloudera.sqoop.tool.SessionTool;\n+\n+import junit.framework.TestCase;\n+\n+import java.sql.Connection;\n+\n+/**\n+ * Test the incremental import functionality.\n+ *\n+ * These all make use of the auto-connect hsqldb-based metastore.\n+ * The metastore URL is configured to be in-memory, and drop all\n+ * state between individual tests.\n+ */\n+public class TestIncrementalImport extends TestCase {\n+\n+  public static final Log LOG = LogFactory.getLog(\n+      TestIncrementalImport.class.getName());\n+\n+  // What database do we read from.\n+  public static final String SOURCE_DB_URL = \"jdbc:hsqldb:mem:incremental\";\n+\n+  @Override\n+  public void setUp() throws Exception {\n+    // Delete db state between tests.\n+    TestSessions.resetSessionSchema();\n+    resetSourceDataSchema();\n+  }\n+\n+  public static void resetSourceDataSchema() throws SQLException {\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    TestSessions.resetSchema(options);\n+  }\n+\n+  public static Configuration newConf() {\n+    return TestSessions.newConf();\n+  }\n+\n+  /**\n+   * Assert that a table has a specified number of rows.\n+   */\n+  private void assertRowCount(String table, int numRows) throws SQLException {\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    HsqldbManager manager = new HsqldbManager(options);\n+    Connection c = manager.getConnection();\n+    PreparedStatement s = null;\n+    ResultSet rs = null;\n+    try {\n+      s = c.prepareStatement(\"SELECT COUNT(*) FROM \" + table);\n+      rs = s.executeQuery();\n+      if (!rs.next()) {\n+        fail(\"No resultset\");\n+      }\n+      int realNumRows = rs.getInt(1);\n+      assertEquals(numRows, realNumRows);\n+      LOG.info(\"Expected \" + numRows + \" rows -- ok.\");\n+    } finally {\n+      if (null != s) {\n+        try {\n+          s.close();\n+        } catch (SQLException sqlE) {\n+          LOG.warn(\"exception: \" + sqlE);\n+        } \n+      }\n+\n+      if (null != rs) {\n+        try {\n+          rs.close();\n+        } catch (SQLException sqlE) {\n+          LOG.warn(\"exception: \" + sqlE);\n+        } \n+      }\n+    }\n+  }\n+\n+  /**\n+   * Insert rows with id = [low, hi) into tableName.\n+   */\n+  private void insertIdRows(String tableName, int low, int hi)\n+      throws SQLException {\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    HsqldbManager manager = new HsqldbManager(options);\n+    Connection c = manager.getConnection();\n+    PreparedStatement s = null;\n+    try {\n+      s = c.prepareStatement(\"INSERT INTO \" + tableName + \" VALUES(?)\");\n+      for (int i = low; i < hi; i++) {\n+        s.setInt(1, i);\n+        s.executeUpdate();\n+      }\n+\n+      c.commit();\n+    } finally {\n+      s.close();\n+    }\n+  }\n+\n+  /**\n+   * Insert rows with id = [low, hi) into tableName with\n+   * the timestamp column set to the specified ts.\n+   */\n+  private void insertIdTimestampRows(String tableName, int low, int hi,\n+      Timestamp ts) throws SQLException {\n+    LOG.info(\"Inserting id rows in [\" + low + \", \" + hi + \") @ \" + ts);\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    HsqldbManager manager = new HsqldbManager(options);\n+    Connection c = manager.getConnection();\n+    PreparedStatement s = null;\n+    try {\n+      s = c.prepareStatement(\"INSERT INTO \" + tableName + \" VALUES(?,?)\");\n+      for (int i = low; i < hi; i++) {\n+        s.setInt(1, i);\n+        s.setTimestamp(2, ts);\n+        s.executeUpdate();\n+      }\n+\n+      c.commit();\n+    } finally {\n+      s.close();\n+    }\n+  }\n+\n+  /**\n+   * Create a table with an 'id' column full of integers.\n+   */\n+  private void createIdTable(String tableName, int insertRows)\n+      throws SQLException {\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    HsqldbManager manager = new HsqldbManager(options);\n+    Connection c = manager.getConnection();\n+    PreparedStatement s = null;\n+    try {\n+      s = c.prepareStatement(\"CREATE TABLE \" + tableName + \"(id INT NOT NULL)\");\n+      s.executeUpdate();\n+      c.commit();\n+      insertIdRows(tableName, 0, insertRows);\n+    } finally {\n+      s.close();\n+    }\n+  }\n+\n+  /**\n+   * Create a table with an 'id' column full of integers and a \n+   * last_modified column with timestamps.\n+   */\n+  private void createTimestampTable(String tableName, int insertRows,\n+      Timestamp baseTime) throws SQLException {\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    HsqldbManager manager = new HsqldbManager(options);\n+    Connection c = manager.getConnection();\n+    PreparedStatement s = null;\n+    try {\n+      s = c.prepareStatement(\"CREATE TABLE \" + tableName + \"(id INT NOT NULL, \"\n+          + \"last_modified TIMESTAMP)\");\n+      s.executeUpdate();\n+      c.commit();\n+      insertIdTimestampRows(tableName, 0, insertRows, baseTime);\n+    } finally {\n+      s.close();\n+    }\n+  }\n+\n+  /**\n+   * Delete all files in a directory for a table.\n+   */\n+  public void clearDir(String tableName) {\n+    try {\n+      FileSystem fs = FileSystem.getLocal(new Configuration());\n+      Path warehouse = new Path(BaseSqoopTestCase.LOCAL_WAREHOUSE_DIR);\n+      Path tableDir = new Path(warehouse, tableName);\n+      fs.delete(tableDir, true);\n+    } catch (Exception e) {\n+      fail(\"Got unexpected exception: \" + StringUtils.stringifyException(e));\n+    }\n+  }\n+\n+  /**\n+   * Look at a directory that should contain files full of an imported 'id'\n+   * column. Assert that all numbers in [0, expectedNums) are present\n+   * in order.\n+   */\n+  public void assertDirOfNumbers(String tableName, int expectedNums) {\n+    try {\n+      FileSystem fs = FileSystem.getLocal(new Configuration());\n+      Path warehouse = new Path(BaseSqoopTestCase.LOCAL_WAREHOUSE_DIR);\n+      Path tableDir = new Path(warehouse, tableName);\n+      FileStatus [] stats = fs.listStatus(tableDir);\n+      String [] fileNames = new String[stats.length];\n+      for (int i = 0; i < stats.length; i++) {\n+        fileNames[i] = stats[i].getPath().toString();\n+      }\n+\n+      Arrays.sort(fileNames);\n+\n+      // Read all the files in sorted order, adding the value lines to the list.\n+      List<String> receivedNums = new ArrayList<String>();\n+      for (String fileName : fileNames) {\n+        if (fileName.startsWith(\"_\") || fileName.startsWith(\".\")) {\n+          continue;\n+        }\n+\n+        BufferedReader r = new BufferedReader(\n+            new InputStreamReader(fs.open(new Path(fileName))));\n+        try {\n+          while (true) {\n+            String s = r.readLine();\n+            if (null == s) {\n+              break;\n+            }\n+\n+            receivedNums.add(s.trim());\n+          }\n+        } finally {\n+          r.close();\n+        }\n+      }\n+\n+      assertEquals(expectedNums, receivedNums.size());\n+\n+      // Compare the received values with the expected set.\n+      for (int i = 0; i < expectedNums; i++) {\n+        assertEquals((int) i, (int) Integer.valueOf(receivedNums.get(i)));\n+      }\n+    } catch (Exception e) {\n+      fail(\"Got unexpected exception: \" + StringUtils.stringifyException(e));\n+    }\n+  }\n+\n+  /**\n+   * Assert that a directory contains a file with exactly one line\n+   * in it, containing the prescribed number 'val'.\n+   */\n+  public void assertSpecificNumber(String tableName, int val) {\n+    try {\n+      FileSystem fs = FileSystem.getLocal(new Configuration());\n+      Path warehouse = new Path(BaseSqoopTestCase.LOCAL_WAREHOUSE_DIR);\n+      Path tableDir = new Path(warehouse, tableName);\n+      FileStatus [] stats = fs.listStatus(tableDir);\n+      String [] fileNames = new String[stats.length];\n+      for (int i = 0; i < stats.length; i++) {\n+        fileNames[i] = stats[i].getPath().toString();\n+      }\n+\n+      // Read the first file that is not a hidden file.\n+      boolean foundVal = false;\n+      for (String fileName : fileNames) {\n+        if (fileName.startsWith(\"_\") || fileName.startsWith(\".\")) {\n+          continue;\n+        }\n+\n+        if (foundVal) {\n+          // Make sure we don't have two or more \"real\" files in the dir.\n+          fail(\"Got an extra data-containing file in this directory.\");\n+        }\n+\n+        BufferedReader r = new BufferedReader(\n+            new InputStreamReader(fs.open(new Path(fileName))));\n+        try {\n+          String s = r.readLine();\n+          if (null == s) {\n+            fail(\"Unexpected empty file \" + fileName + \".\");\n+          }\n+          assertEquals(val, (int) Integer.valueOf(s.trim()));\n+\n+          String nextLine = r.readLine();\n+          if (nextLine != null) {\n+            fail(\"Expected only one result, but got another line: \" + nextLine);\n+          }\n+\n+          // Successfully got the value we were looking for.\n+          foundVal = true;\n+        } finally {\n+          r.close();\n+        }\n+      }\n+    } catch (Exception e) {\n+      fail(\"Got unexpected exception: \" + StringUtils.stringifyException(e));\n+    }\n+  }\n+\n+  public void runImport(SqoopOptions options, List<String> args) {\n+    try {\n+      Sqoop importer = new Sqoop(new ImportTool(), options.getConf(), options);\n+      int ret = Sqoop.runSqoop(importer, args.toArray(new String[0]));\n+      assertEquals(\"Failure during job\", 0, ret);\n+    } catch (Exception e) {\n+      LOG.error(\"Got exception running Sqoop: \"\n+          + StringUtils.stringifyException(e));\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Return a list of arguments to import the specified table.\n+   */\n+  private List<String> getArgListForTable(String tableName, boolean commonArgs,\n+      boolean isAppend) {\n+    List<String> args = new ArrayList<String>();\n+    if (commonArgs) {\n+      CommonArgs.addHadoopFlags(args);\n+    }\n+    args.add(\"--connect\");\n+    args.add(SOURCE_DB_URL);\n+    args.add(\"--table\");\n+    args.add(tableName);\n+    args.add(\"--warehouse-dir\");\n+    args.add(BaseSqoopTestCase.LOCAL_WAREHOUSE_DIR);\n+    if (isAppend) {\n+      args.add(\"--incremental\");\n+      args.add(\"append\");\n+      args.add(\"--check-column\");\n+      args.add(\"id\");\n+    } else {\n+      args.add(\"--incremental\");\n+      args.add(\"lastmodified\");\n+      args.add(\"--check-column\");\n+      args.add(\"last_modified\");\n+    }\n+    args.add(\"--columns\");\n+    args.add(\"id\");\n+    args.add(\"-m\");\n+    args.add(\"1\");\n+    \n+    return args;\n+  }\n+\n+  /**\n+   * Create a session with the specified name, where the session performs\n+   * an import configured with 'sessionArgs'.\n+   */\n+  private void createSession(String sessionName, List<String> sessionArgs) {\n+    createSession(sessionName, sessionArgs, newConf());\n+  }\n+\n+  /**\n+   * Create a session with the specified name, where the session performs\n+   * an import configured with 'sessionArgs', using the provided configuration\n+   * as defaults.\n+   */\n+  private void createSession(String sessionName, List<String> sessionArgs,\n+      Configuration conf) {\n+    try {\n+      SqoopOptions options = new SqoopOptions();\n+      options.setConf(conf);\n+      Sqoop makeSession = new Sqoop(new SessionTool(), conf, options);\n+      \n+      List<String> args = new ArrayList<String>();\n+      args.add(\"--create\");\n+      args.add(sessionName);\n+      args.add(\"--\");\n+      args.add(\"import\");\n+      args.addAll(sessionArgs);\n+\n+      int ret = Sqoop.runSqoop(makeSession, args.toArray(new String[0]));\n+      assertEquals(\"Failure during job to create session\", 0, ret);\n+    } catch (Exception e) {\n+      LOG.error(\"Got exception running Sqoop to create session: \"\n+          + StringUtils.stringifyException(e));\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Run the specified session.\n+   */\n+  private void runSession(String sessionName) {\n+    runSession(sessionName, newConf());\n+  }\n+\n+  /**\n+   * Run the specified session.\n+   */\n+  private void runSession(String sessionName, Configuration conf) {\n+    try {\n+      SqoopOptions options = new SqoopOptions();\n+      options.setConf(conf);\n+      Sqoop runSession = new Sqoop(new SessionTool(), conf, options);\n+      \n+      List<String> args = new ArrayList<String>();\n+      args.add(\"--exec\");\n+      args.add(sessionName);\n+\n+      int ret = Sqoop.runSqoop(runSession, args.toArray(new String[0]));\n+      assertEquals(\"Failure during job to run session\", 0, ret);\n+    } catch (Exception e) {\n+      LOG.error(\"Got exception running Sqoop to run session: \"\n+          + StringUtils.stringifyException(e));\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  // Incremental import of an empty table, no metastore.\n+  public void testEmptyAppendImport() throws Exception {\n+    final String TABLE_NAME = \"emptyAppend1\";\n+    createIdTable(TABLE_NAME, 0);\n+    List<String> args = getArgListForTable(TABLE_NAME, true, true);\n+\n+    Configuration conf = newConf();\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConf(conf);\n+    runImport(options, args);\n+\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+  }\n+\n+  // Incremental import of a filled table, no metastore.\n+  public void testFullAppendImport() throws Exception {\n+    final String TABLE_NAME = \"fullAppend1\";\n+    createIdTable(TABLE_NAME, 10);\n+    List<String> args = getArgListForTable(TABLE_NAME, true, true);\n+\n+    Configuration conf = newConf();\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConf(conf);\n+    runImport(options, args);\n+\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+  }\n+\n+  public void testEmptySessionAppend() throws Exception {\n+    // Create a session and run an import on an empty table.\n+    // Nothing should happen.\n+\n+    final String TABLE_NAME = \"emptySession\";\n+    createIdTable(TABLE_NAME, 0);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, true);\n+    createSession(\"emptySession\", args);\n+    runSession(\"emptySession\");\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+\n+    // Running the session a second time should result in\n+    // nothing happening, it's still empty.\n+    runSession(\"emptySession\");\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+  }\n+\n+  public void testEmptyThenFullSessionAppend() throws Exception {\n+    // Create an empty table. Import it; nothing happens.\n+    // Add some rows. Verify they are appended.\n+\n+    final String TABLE_NAME = \"emptyThenFull\";\n+    createIdTable(TABLE_NAME, 0);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, true);\n+    createSession(TABLE_NAME, args);\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+\n+    // Now add some rows.\n+    insertIdRows(TABLE_NAME, 0, 10);\n+\n+    // Running the session a second time should import 10 rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+\n+    // Add some more rows.\n+    insertIdRows(TABLE_NAME, 10, 20);\n+\n+    // Import only those rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 20);\n+  }\n+\n+  public void testAppend() throws Exception {\n+    // Create a table with data in it; import it.\n+    // Then add more data, verify that only the incremental data is pulled.\n+\n+    final String TABLE_NAME = \"append\";\n+    createIdTable(TABLE_NAME, 10);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, true);\n+    createSession(TABLE_NAME, args);\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+\n+    // Add some more rows.\n+    insertIdRows(TABLE_NAME, 10, 20);\n+\n+    // Import only those rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 20);\n+  }\n+\n+  public void testEmptyLastModified() throws Exception {\n+    final String TABLE_NAME = \"emptyLastModified\";\n+    createTimestampTable(TABLE_NAME, 0, null);\n+    List<String> args = getArgListForTable(TABLE_NAME, true, false);\n+\n+    Configuration conf = newConf();\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConf(conf);\n+    runImport(options, args);\n+\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+  }\n+\n+  public void testFullLastModifiedImport() throws Exception {\n+    // Given a table of rows imported in the past,\n+    // see that they are imported.\n+    final String TABLE_NAME = \"fullLastModified\";\n+    Timestamp thePast = new Timestamp(System.currentTimeMillis() - 100); \n+    createTimestampTable(TABLE_NAME, 10, thePast);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, true, false);\n+\n+    Configuration conf = newConf();\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConf(conf);\n+    runImport(options, args);\n+\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+  }\n+\n+  public void testNoImportFromTheFuture() throws Exception {\n+    // If last-modified dates for writes are serialized to be in the\n+    // future w.r.t. an import, do not import these rows.\n+\n+    final String TABLE_NAME = \"futureLastModified\";\n+    Timestamp theFuture = new Timestamp(System.currentTimeMillis() + 1000000);\n+    createTimestampTable(TABLE_NAME, 10, theFuture);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, true, false);\n+\n+    Configuration conf = newConf();\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConf(conf);\n+    runImport(options, args);\n+\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+  }\n+\n+  public void testEmptySessionLastMod() throws Exception {\n+    // Create a session and run an import on an empty table.\n+    // Nothing should happen.\n+\n+    final String TABLE_NAME = \"emptySessionLastMod\";\n+    createTimestampTable(TABLE_NAME, 0, null);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, false);\n+    args.add(\"--append\");\n+    createSession(\"emptySessionLastMod\", args);\n+    runSession(\"emptySessionLastMod\");\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+\n+    // Running the session a second time should result in\n+    // nothing happening, it's still empty.\n+    runSession(\"emptySessionLastMod\");\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+  }\n+\n+  public void testEmptyThenFullSessionLastMod() throws Exception {\n+    // Create an empty table. Import it; nothing happens.\n+    // Add some rows. Verify they are appended.\n+\n+    final String TABLE_NAME = \"emptyThenFullTimestamp\";\n+    createTimestampTable(TABLE_NAME, 0, null);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, false);\n+    args.add(\"--append\");\n+    createSession(TABLE_NAME, args);\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 0);\n+\n+    long importWasBefore = System.currentTimeMillis();\n+\n+    // Let some time elapse.\n+    Thread.sleep(50);\n+\n+    long rowsAddedTime = System.currentTimeMillis() - 5;\n+\n+    // Check: we are adding rows after the previous import time\n+    // and before the current time.\n+    assertTrue(rowsAddedTime > importWasBefore);\n+    assertTrue(rowsAddedTime < System.currentTimeMillis());\n+\n+    insertIdTimestampRows(TABLE_NAME, 0, 10, new Timestamp(rowsAddedTime));\n+\n+    // Running the session a second time should import 10 rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+\n+    // Add some more rows.\n+    importWasBefore = System.currentTimeMillis();\n+    Thread.sleep(50);\n+    rowsAddedTime = System.currentTimeMillis() - 5;\n+    assertTrue(rowsAddedTime > importWasBefore);\n+    assertTrue(rowsAddedTime < System.currentTimeMillis());\n+    insertIdTimestampRows(TABLE_NAME, 10, 20, new Timestamp(rowsAddedTime));\n+\n+    // Import only those rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 20);\n+  }\n+\n+  public void testAppendWithTimestamp() throws Exception {\n+    // Create a table with data in it; import it.\n+    // Then add more data, verify that only the incremental data is pulled.\n+\n+    final String TABLE_NAME = \"appendTimestamp\";\n+    Timestamp thePast = new Timestamp(System.currentTimeMillis() - 100);\n+    createTimestampTable(TABLE_NAME, 10, thePast);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, false);\n+    args.add(\"--append\");\n+    createSession(TABLE_NAME, args);\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+\n+    // Add some more rows.\n+    long importWasBefore = System.currentTimeMillis();\n+    Thread.sleep(50);\n+    long rowsAddedTime = System.currentTimeMillis() - 5;\n+    assertTrue(rowsAddedTime > importWasBefore);\n+    assertTrue(rowsAddedTime < System.currentTimeMillis());\n+    insertIdTimestampRows(TABLE_NAME, 10, 20, new Timestamp(rowsAddedTime));\n+\n+    // Import only those rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 20);\n+  }\n+\n+  public void testModifyWithTimestamp() throws Exception {\n+    // Create a table with data in it; import it.\n+    // Then modify some existing rows, and verify that we only grab\n+    // those rows.\n+\n+    final String TABLE_NAME = \"modifyTimestamp\";\n+    Timestamp thePast = new Timestamp(System.currentTimeMillis() - 100);\n+    createTimestampTable(TABLE_NAME, 10, thePast);\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, false);\n+    createSession(TABLE_NAME, args);\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+\n+    // Modify a row.\n+    long importWasBefore = System.currentTimeMillis();\n+    Thread.sleep(50);\n+    long rowsAddedTime = System.currentTimeMillis() - 5;\n+    assertTrue(rowsAddedTime > importWasBefore);\n+    assertTrue(rowsAddedTime < System.currentTimeMillis());\n+    SqoopOptions options = new SqoopOptions();\n+    options.setConnectString(SOURCE_DB_URL);\n+    HsqldbManager manager = new HsqldbManager(options);\n+    Connection c = manager.getConnection();\n+    PreparedStatement s = null;\n+    try {\n+      s = c.prepareStatement(\"UPDATE \" + TABLE_NAME\n+          + \" SET id=?, last_modified=? WHERE id=?\");\n+      s.setInt(1, 4000); // the first row should have '4000' in it now.\n+      s.setTimestamp(2, new Timestamp(rowsAddedTime));\n+      s.setInt(3, 0);\n+      s.executeUpdate();\n+      c.commit();\n+    } finally {\n+      s.close();\n+    }\n+\n+    // Import only the new row.\n+    clearDir(TABLE_NAME);\n+    runSession(TABLE_NAME);\n+    assertSpecificNumber(TABLE_NAME, 4000);\n+  }\n+\n+  /**\n+   * ManagerFactory returning an HSQLDB ConnManager which allows you to\n+   * specify the current database timestamp.\n+   */\n+  public static class InstrumentHsqldbManagerFactory extends ManagerFactory {\n+    @Override\n+    public ConnManager accept(SqoopOptions options) {\n+      LOG.info(\"Using instrumented manager\");\n+      return new InstrumentHsqldbManager(options);\n+    }\n+  }\n+\n+  /**\n+   * Hsqldb ConnManager that lets you set the current reported timestamp\n+   * from the database, to allow testing of boundary conditions for imports.\n+   */\n+  public static class InstrumentHsqldbManager extends HsqldbManager {\n+    private static Timestamp curTimestamp;\n+\n+    public InstrumentHsqldbManager(SqoopOptions options) {\n+      super(options);\n+    }\n+\n+    @Override\n+    public Timestamp getCurrentDbTimestamp() {\n+      return InstrumentHsqldbManager.curTimestamp;\n+    }\n+\n+    public static void setCurrentDbTimestamp(Timestamp t) {\n+      InstrumentHsqldbManager.curTimestamp = t;\n+    }\n+  }\n+\n+  public void testTimestampBoundary() throws Exception {\n+    // Run an import, and then insert rows with the last-modified timestamp\n+    // set to the exact time when the first import runs. Run a second import\n+    // and ensure that we pick up the new data.\n+\n+    long now = System.currentTimeMillis();\n+\n+    final String TABLE_NAME = \"boundaryTimestamp\";\n+    Timestamp thePast = new Timestamp(now - 100);\n+    createTimestampTable(TABLE_NAME, 10, thePast);\n+\n+    Timestamp firstJobTime = new Timestamp(now);\n+    InstrumentHsqldbManager.setCurrentDbTimestamp(firstJobTime);\n+\n+    // Configure the job to use the instrumented Hsqldb manager.\n+    Configuration conf = newConf();\n+    conf.set(ConnFactory.FACTORY_CLASS_NAMES_KEY,\n+        InstrumentHsqldbManagerFactory.class.getName());\n+\n+    List<String> args = getArgListForTable(TABLE_NAME, false, false);\n+    args.add(\"--append\");\n+    createSession(TABLE_NAME, args, conf);\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 10);\n+\n+    // Add some more rows with the timestamp equal to the job run timestamp.\n+    insertIdTimestampRows(TABLE_NAME, 10, 20, firstJobTime);\n+    assertRowCount(TABLE_NAME, 20);\n+\n+    // Run a second job with the clock advanced by 100 ms.\n+    Timestamp secondJobTime = new Timestamp(now + 100);\n+    InstrumentHsqldbManager.setCurrentDbTimestamp(secondJobTime);\n+\n+    // Import only those rows.\n+    runSession(TABLE_NAME);\n+    assertDirOfNumbers(TABLE_NAME, 20);\n+  }\n+}\n+",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/test/com/cloudera/sqoop/TestIncrementalImport.java",
                "sha": "10a7231f00a050838ba72916f6e0b56c2158d200",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/sqoop/blob/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/test/com/cloudera/sqoop/metastore/TestSessions.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/metastore/TestSessions.java?ref=6efcec0da22fc737e1aa30b65b380b5c497deeb1",
                "deletions": 1,
                "filename": "src/test/com/cloudera/sqoop/metastore/TestSessions.java",
                "patch": "@@ -46,7 +46,8 @@\n  */\n public class TestSessions extends TestCase {\n \n-  public static final String TEST_AUTOCONNECT_URL = \"jdbc:hsqldb:mem:testdb\";\n+  public static final String TEST_AUTOCONNECT_URL =\n+      \"jdbc:hsqldb:mem:sqoopmetastore\";\n   public static final String TEST_AUTOCONNECT_USER = \"SA\";\n   public static final String TEST_AUTOCONNECT_PASS = \"\";\n \n@@ -62,6 +63,13 @@ public static void resetSessionSchema() throws SQLException {\n     options.setUsername(TEST_AUTOCONNECT_USER);\n     options.setPassword(TEST_AUTOCONNECT_PASS);\n \n+    resetSchema(options);\n+  }\n+\n+  /**\n+   * Drop all tables in the configured HSQLDB-based schema/user/pass.\n+   */\n+  public static void resetSchema(SqoopOptions options) throws SQLException {\n     HsqldbManager manager = new HsqldbManager(options);\n     Connection c = manager.getConnection();\n     Statement s = c.createStatement();",
                "raw_url": "https://github.com/apache/sqoop/raw/6efcec0da22fc737e1aa30b65b380b5c497deeb1/src/test/com/cloudera/sqoop/metastore/TestSessions.java",
                "sha": "8d1e853fb7ce09ecdea50ea15c82741d69ba839f",
                "status": "modified"
            }
        ],
        "message": "SQOOP-41. Add support for incremental imports.\n\nModify ImportTool, SessionTool, to support incremental imports.\nAdd TestIncrementalImport to unit test incremental imports.\nSqoopOptions now implements Cloneable.\nSQOOP-44. Bugfix in ClassWriter: fix NPE if the case of column names specified\nwith --columns do not match the case reported by the database.\n\nFrom: Aaron Kimball <aaron@cloudera.com>\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/sqoop/trunk@1149944 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/41cee93c4099ac2237e5e73e69d2b1b7abe6c9e8",
        "patched_files": [
            "CompilationManager.java",
            "SqlManager.java",
            "AllTests.java",
            "HsqldbSessionStorage.java",
            "build.java",
            "ImportTool.java",
            "OracleManager.java",
            "ClassWriter.java",
            "JobBase.java",
            "ConnManager.java",
            "AutoHsqldbStorage.java",
            "SqoopOptions.java",
            "BaseSqoopTool.java",
            "SessionTool.java",
            "HsqldbManager.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestImportTool.java",
            "OracleManagerTest.java",
            "TestSqoopOptions.java",
            "TestClassWriter.java",
            "TestHsqldbManager.java",
            "TestSqlManager.java",
            "TestSessions.java",
            "TestBaseSqoopTool.java",
            "TestCompilationManager.java",
            "TestJobBase.java",
            "TestIncrementalImport.java"
        ]
    },
    "sqoop_72e3bfd": {
        "bug_id": "sqoop_72e3bfd",
        "commit": "https://github.com/apache/sqoop/commit/72e3bfdd6677f8630678d8aa94da7a8652692033",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/sqoop/blob/72e3bfdd6677f8630678d8aa94da7a8652692033/src/java/org/apache/sqoop/orm/ClassWriter.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/orm/ClassWriter.java?ref=72e3bfdd6677f8630678d8aa94da7a8652692033",
                "deletions": 0,
                "filename": "src/java/org/apache/sqoop/orm/ClassWriter.java",
                "patch": "@@ -1118,6 +1118,13 @@ public void generate() throws IOException {\n \n     String[] colNames = getColumnNames(columnTypes);\n \n+    // Column number should be more than 0\n+    if (colNames == null || colNames.length == 0) {\n+      throw new IllegalArgumentException(\"There is no column found in the \"\n+              + \"target table \" + tableName\n+              + \". Please ensure that your table name is correct.\");\n+    }\n+\n     // Translate all the column names into names that are safe to\n     // use as identifiers.\n     String [] cleanedColNames = cleanColNames(colNames);",
                "raw_url": "https://github.com/apache/sqoop/raw/72e3bfdd6677f8630678d8aa94da7a8652692033/src/java/org/apache/sqoop/orm/ClassWriter.java",
                "sha": "df1ab727ce14e50ad67e15c75f4df3016af6e7b8",
                "status": "modified"
            }
        ],
        "message": "SQOOP-1117: when failed to import a non-existing table, the failure information includes NullPointerException\n\n(sam liu via Jarek Jarcec Cecho)",
        "parent": "https://github.com/apache/sqoop/commit/a803f27ddefc800cd1173002b92939bed613504b",
        "patched_files": [
            "ClassWriter.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestClassWriter.java"
        ]
    },
    "sqoop_79b7172": {
        "bug_id": "sqoop_79b7172",
        "commit": "https://github.com/apache/sqoop/commit/79b71726946f1bc792e034a32ee10835c58d5033",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/sqoop/blob/79b71726946f1bc792e034a32ee10835c58d5033/src/java/org/apache/sqoop/ConnFactory.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/ConnFactory.java?ref=79b71726946f1bc792e034a32ee10835c58d5033",
                "deletions": 1,
                "filename": "src/java/org/apache/sqoop/ConnFactory.java",
                "patch": "@@ -294,7 +294,20 @@ private Configuration loadManagersFromConfDir(Configuration conf) {\n     if (mgrDir.exists() && mgrDir.isDirectory()) {\n       // We have a managers.d subdirectory. Get the file list, sort it,\n       // and process them in order.\n-      String [] fileNames = mgrDir.list();\n+      String[] fileNames;\n+\n+      try {\n+        fileNames = mgrDir.list();\n+      } catch (SecurityException e) {\n+        fileNames = null;\n+      }\n+\n+      if (null == fileNames) {\n+        LOG.warn(\"Sqoop cannot read $SQOOP_CONF_DIR/managers.d. \"\n+            + \"Please check the permissions on managers.d.\");\n+        return conf;\n+      }\n+\n       Arrays.sort(fileNames);\n \n       for (String fileName : fileNames) {",
                "raw_url": "https://github.com/apache/sqoop/raw/79b71726946f1bc792e034a32ee10835c58d5033/src/java/org/apache/sqoop/ConnFactory.java",
                "sha": "335b03755dfcdbb6b7110dd0fe003954a287851a",
                "status": "modified"
            }
        ],
        "message": "SQOOP-1387: Incorrect permissions on manager.d directory can lead to NPE",
        "parent": "https://github.com/apache/sqoop/commit/ad53e4e334885980bf4fcefc0e85336c458dba7b",
        "patched_files": [
            "ConnFactory.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestConnFactory.java"
        ]
    },
    "sqoop_9147967": {
        "bug_id": "sqoop_9147967",
        "commit": "https://github.com/apache/sqoop/commit/9147967eee4c72997fa858e2bbc8b9b7a58b994c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/sqoop/blob/9147967eee4c72997fa858e2bbc8b9b7a58b994c/src/java/org/apache/sqoop/tool/CodeGenTool.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/tool/CodeGenTool.java?ref=9147967eee4c72997fa858e2bbc8b9b7a58b994c",
                "deletions": 2,
                "filename": "src/java/org/apache/sqoop/tool/CodeGenTool.java",
                "patch": "@@ -93,8 +93,8 @@ public String generateORM(SqoopOptions options, String tableName)\n \n     if (options.getFileLayout() == SqoopOptions.FileLayout.ParquetFile) {\n       String className = options.getClassName() != null ?\n-          options.getClassName() : options.getTableName();\n-      if (className.equalsIgnoreCase(options.getTableName())) {\n+          options.getClassName() : tableName;\n+      if (className.equalsIgnoreCase(tableName)) {\n         className = \"codegen_\" + className;\n         options.setClassName(className);\n         LOG.info(\"Will generate java class as \" + options.getClassName());",
                "raw_url": "https://github.com/apache/sqoop/raw/9147967eee4c72997fa858e2bbc8b9b7a58b994c/src/java/org/apache/sqoop/tool/CodeGenTool.java",
                "sha": "22ab030ca0070434fc8e721a3c03833f7c9e223d",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/sqoop/blob/9147967eee4c72997fa858e2bbc8b9b7a58b994c/src/test/com/cloudera/sqoop/TestAllTables.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/TestAllTables.java?ref=9147967eee4c72997fa858e2bbc8b9b7a58b994c",
                "deletions": 7,
                "filename": "src/test/com/cloudera/sqoop/TestAllTables.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.ArrayList;\n import java.util.List;\n \n+import org.apache.avro.generic.GenericRecord;\n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileSystem;\n@@ -34,6 +35,9 @@\n import com.cloudera.sqoop.testutil.CommonArgs;\n import com.cloudera.sqoop.testutil.ImportJobTestCase;\n import com.cloudera.sqoop.tool.ImportAllTablesTool;\n+import org.kitesdk.data.Dataset;\n+import org.kitesdk.data.DatasetReader;\n+import org.kitesdk.data.Datasets;\n \n /**\n  * Test the --all-tables functionality that can import multiple tables.\n@@ -44,13 +48,10 @@\n    * Create the argv to pass to Sqoop.\n    * @return the argv as an array of strings.\n    */\n-  private String [] getArgv(boolean includeHadoopFlags, String[] excludeTables) {\n+  private String [] getArgv(String[] extraArgs, String[] excludeTables) {\n     ArrayList<String> args = new ArrayList<String>();\n \n-    if (includeHadoopFlags) {\n-      CommonArgs.addHadoopFlags(args);\n-    }\n-\n+    CommonArgs.addHadoopFlags(args);\n     args.add(\"--warehouse-dir\");\n     args.add(getWarehouseDir());\n     args.add(\"--connect\");\n@@ -63,6 +64,11 @@\n       args.add(\"--exclude-tables\");\n       args.add(StringUtils.join(excludeTables, \",\"));\n     }\n+    if (extraArgs != null) {\n+      for (String arg : extraArgs) {\n+        args.add(arg);\n+      }\n+    }\n \n     return args.toArray(new String[0]);\n   }\n@@ -124,7 +130,7 @@ public void tearDown() {\n   }\n \n   public void testMultiTableImport() throws IOException {\n-    String [] argv = getArgv(true, null);\n+    String [] argv = getArgv(null, null);\n     runImport(new ImportAllTablesTool(), argv);\n \n     Path warehousePath = new Path(this.getWarehouseDir());\n@@ -159,9 +165,38 @@ public void testMultiTableImport() throws IOException {\n     }\n   }\n \n+  public void testMultiTableImportAsParquetFormat() throws IOException {\n+    String [] argv = getArgv(new String[]{\"--as-parquetfile\"}, null);\n+    runImport(new ImportAllTablesTool(), argv);\n+\n+    Path warehousePath = new Path(this.getWarehouseDir());\n+    int i = 0;\n+    for (String tableName : this.tableNames) {\n+      Path tablePath = new Path(warehousePath, tableName);\n+      Dataset dataset = Datasets.load(\"dataset:file:\" + tablePath);\n+\n+      // dequeue the expected value for this table. This\n+      // list has the same order as the tableNames list.\n+      String expectedVal = Integer.toString(i++) + \",\"\n+          + this.expectedStrings.get(0);\n+      this.expectedStrings.remove(0);\n+\n+      DatasetReader<GenericRecord> reader = dataset.newReader();\n+      try {\n+        GenericRecord record = reader.next();\n+        String line = record.get(0) + \",\" + record.get(1);\n+        assertEquals(\"Table \" + tableName + \" expected a different string\",\n+            expectedVal, line);\n+        assertFalse(reader.hasNext());\n+      } finally {\n+        reader.close();\n+      }\n+    }\n+  }\n+\n   public void testMultiTableImportWithExclude() throws IOException {\n     String exclude = this.tableNames.get(0);\n-    String [] argv = getArgv(true, new String[]{ exclude });\n+    String [] argv = getArgv(null, new String[]{ exclude });\n     runImport(new ImportAllTablesTool(), argv);\n \n     Path warehousePath = new Path(this.getWarehouseDir());",
                "raw_url": "https://github.com/apache/sqoop/raw/9147967eee4c72997fa858e2bbc8b9b7a58b994c/src/test/com/cloudera/sqoop/TestAllTables.java",
                "sha": "f98102436979365bb540aae002f71a8141a81f03",
                "status": "modified"
            }
        ],
        "message": "SQOOP-2372: Import all tables as parquet will throw NPE\n\n(Qian Xu via Abraham Elmahrek)",
        "parent": "https://github.com/apache/sqoop/commit/974b886c4a51a0c116caefaaad94edcfb54de162",
        "patched_files": [
            "CodeGenTool.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestAllTables.java"
        ]
    },
    "sqoop_9aac957": {
        "bug_id": "sqoop_9aac957",
        "commit": "https://github.com/apache/sqoop/commit/9aac957b9c0e9f6c644df5cb529d5cd1a118dff8",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/sqoop/blob/9aac957b9c0e9f6c644df5cb529d5cd1a118dff8/src/java/org/apache/sqoop/accumulo/AccumuloUtil.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/accumulo/AccumuloUtil.java?ref=9aac957b9c0e9f6c644df5cb529d5cd1a118dff8",
                "deletions": 6,
                "filename": "src/java/org/apache/sqoop/accumulo/AccumuloUtil.java",
                "patch": "@@ -105,13 +105,22 @@ public static void addJars(Job job, SqoopOptions options) throws IOException {\n         .addAll(conf.getStringCollection(\n           ConfigurationConstants.MAPRED_DISTCACHE_CONF_PARAM));\n \n-      String dir = accumuloHome + File.separator + \"lib\";\n-      LOG.info(\"Adding jar files under \" + dir + \" to distributed cache\");\n-      addDirToCache(new File(dir), fs, localUrls, false);\n+      if (null == accumuloHome) {\n+        throw new IllegalArgumentException(\"ACCUMULO_HOME is not set.\");\n+      } else {\n+        File dir = new File(accumuloHome, \"lib\");\n+        String path = dir.getPath();\n+        LOG.info(\"Adding jar files under \" + path + \" to distributed cache\");\n+        addDirToCache(dir, fs, localUrls, false);\n+      }\n \n-      dir = zookeeperHome;\n-      LOG.info(\"Adding jar files under \" + dir + \" to distributed cache\");\n-      addDirToCache(new File(dir), fs, localUrls, false);\n+      if (null == zookeeperHome) {\n+        throw new IllegalArgumentException(\"ZOOKEEPER_HOME is not set.\");\n+      } else {\n+        String dir = zookeeperHome;\n+        LOG.info(\"Adding jar files under \" + dir + \" to distributed cache\");\n+        addDirToCache(new File(dir), fs, localUrls, false);\n+      }\n \n       String tmpjars = conf\n         .get(ConfigurationConstants.MAPRED_DISTCACHE_CONF_PARAM);",
                "raw_url": "https://github.com/apache/sqoop/raw/9aac957b9c0e9f6c644df5cb529d5cd1a118dff8/src/java/org/apache/sqoop/accumulo/AccumuloUtil.java",
                "sha": "06888c5d7e660bea4306ed282915c7805f0de649",
                "status": "modified"
            }
        ],
        "message": "SQOOP-1370: AccumuloUtils can throw NPE when zookeeper or accumulo home is null\n\n(Mike Drob via Jarek Jarcec Cecho)",
        "parent": "https://github.com/apache/sqoop/commit/92d363da65b8b4e0ba52695ebff049a9253cb52b",
        "patched_files": [
            "AccumuloUtil.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestAccumuloUtil.java"
        ]
    },
    "sqoop_bcefb92": {
        "bug_id": "sqoop_bcefb92",
        "commit": "https://github.com/apache/sqoop/commit/bcefb92277444aab49ee229dd6db542e7f914a3b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/sqoop/blob/bcefb92277444aab49ee229dd6db542e7f914a3b/src/test/com/cloudera/sqoop/TestIncrementalImport.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/TestIncrementalImport.java?ref=bcefb92277444aab49ee229dd6db542e7f914a3b",
                "deletions": 0,
                "filename": "src/test/com/cloudera/sqoop/TestIncrementalImport.java",
                "patch": "@@ -547,6 +547,8 @@ public void runImport(SqoopOptions options, List<String> args) {\n     args.add(SOURCE_DB_URL);\n     args.add(\"--query\");\n     args.add(query);\n+    args.add(\"--class-name\");\n+    args.add(directoryName);\n     args.add(\"--target-dir\");\n     args.add(BaseSqoopTestCase.LOCAL_WAREHOUSE_DIR\n       + System.getProperty(\"file.separator\") + directoryName);",
                "raw_url": "https://github.com/apache/sqoop/raw/bcefb92277444aab49ee229dd6db542e7f914a3b/src/test/com/cloudera/sqoop/TestIncrementalImport.java",
                "sha": "b456ca6a6868d5d6cb2ea78f7fdd81f503468f37",
                "status": "modified"
            }
        ],
        "message": "SQOOP-1759: TestIncrementalImport fails with NPE on Windows\n\n(Venkat Ranganathan via Jarek Jarcec Cecho)",
        "parent": "https://github.com/apache/sqoop/commit/eceff4c8f42df5a4f13ea5bf69eb33c3020f12a3",
        "patched_files": [],
        "repo": "sqoop",
        "unit_tests": [
            "TestIncrementalImport.java"
        ]
    },
    "sqoop_beb0b2e": {
        "bug_id": "sqoop_beb0b2e",
        "commit": "https://github.com/apache/sqoop/commit/beb0b2e1c227eaa76c560f87588e0963063aedef",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/sqoop/blob/beb0b2e1c227eaa76c560f87588e0963063aedef/src/java/com/cloudera/sqoop/util/AppendUtils.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/util/AppendUtils.java?ref=beb0b2e1c227eaa76c560f87588e0963063aedef",
                "deletions": 4,
                "filename": "src/java/com/cloudera/sqoop/util/AppendUtils.java",
                "patch": "@@ -77,7 +77,16 @@ public void append() throws IOException {\n \n     int nextPartition = 0;\n \n-    // Create directory in case\n+    if (!fs.exists(tempDir)) {\n+      // This occurs if there was no source (tmp) dir. This might happen\n+      // if the import was an HBase-target import, but the user specified\n+      // --append anyway. This is a warning, not an error.\n+      LOG.warn(\"Cannot append files to target dir; no such directory: \"\n+          + tempDir);\n+      return;\n+    }\n+\n+    // Create target directory.\n     if (!fs.exists(userDestDir)) {\n       LOG.info(\"Creating missing output directory - \" + userDestDir.getName());\n       fs.mkdirs(userDestDir);\n@@ -92,9 +101,8 @@ public void append() throws IOException {\n     moveFiles(fs, tempDir, userDestDir, nextPartition);\n \n     // delete temporary path\n-    LOG.debug(\"Deleting temporary folder \"\n-            + context.getDestination().getName());\n-    fs.delete(context.getDestination(), true);\n+    LOG.debug(\"Deleting temporary folder \" + tempDir.getName());\n+    fs.delete(tempDir, true);\n   }\n \n   /**\n@@ -141,6 +149,13 @@ private void moveFiles(FileSystem fs, Path sourceDir, Path targetDir,\n     numpart.setGroupingUsed(false);\n     Pattern patt = Pattern.compile(\"part.*-([0-9][0-9][0-9][0-9][0-9]).*\");\n     FileStatus[] tempFiles = fs.listStatus(sourceDir);\n+\n+    if (null == tempFiles) {\n+      // If we've already checked that the dir exists, and now it can't be\n+      // listed, this is a genuine error (permissions, fs integrity, or other).\n+      throw new IOException(\"Could not list files from \" + sourceDir);\n+    }\n+\n     // Move and rename files & directories from temporary to target-dir thus\n     // appending file's next partition\n     for (FileStatus fileStat : tempFiles) {",
                "raw_url": "https://github.com/apache/sqoop/raw/beb0b2e1c227eaa76c560f87588e0963063aedef/src/java/com/cloudera/sqoop/util/AppendUtils.java",
                "sha": "8f26cdf7946b6a62b78d2208599153fe6ec6af7b",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/sqoop/blob/beb0b2e1c227eaa76c560f87588e0963063aedef/src/test/com/cloudera/sqoop/TestAppendUtils.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/TestAppendUtils.java?ref=beb0b2e1c227eaa76c560f87588e0963063aedef",
                "deletions": 0,
                "filename": "src/test/com/cloudera/sqoop/TestAppendUtils.java",
                "patch": "@@ -34,10 +34,13 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n \n+import com.cloudera.sqoop.manager.ImportJobContext;\n+\n import com.cloudera.sqoop.testutil.CommonArgs;\n import com.cloudera.sqoop.testutil.HsqldbTestServer;\n import com.cloudera.sqoop.testutil.ImportJobTestCase;\n import com.cloudera.sqoop.tool.ImportTool;\n+import com.cloudera.sqoop.util.AppendUtils;\n \n /**\n  * Test that --append works.\n@@ -261,5 +264,22 @@ public void testAppendToTargetDir() throws IOException {\n     runAppendTest(args, output);\n   }\n \n+  /**\n+   * If the append source does not exist, don't crash.\n+   */\n+  public void testAppendSrcDoesNotExist() throws IOException {\n+    Configuration conf = new Configuration();\n+    conf.set(\"fs.default.name\", \"file:///\");\n+    SqoopOptions options = new SqoopOptions(conf);\n+    options.setTableName(\"meep\");\n+    Path missingPath = new Path(\"doesNotExistForAnyReason\");\n+    FileSystem local = FileSystem.getLocal(conf);\n+    assertFalse(local.exists(missingPath));\n+    ImportJobContext importContext = new ImportJobContext(\"meep\", null,\n+        options, missingPath);\n+    AppendUtils utils = new AppendUtils(importContext);\n+    utils.append();\n+  }\n+\n }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/beb0b2e1c227eaa76c560f87588e0963063aedef/src/test/com/cloudera/sqoop/TestAppendUtils.java",
                "sha": "83cd7785cc405cc8c6cbfc735850c40205c718b8",
                "status": "modified"
            }
        ],
        "message": "SQOOP-47. NPE if --append is specified with HBase import target.\n\nAppendUtils checks for missing append source and exits gracefully.\n\nFrom: Aaron Kimball <aaron@cloudera.com>\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/sqoop/trunk@1149964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/66c753c2e7bcbbdbce5df4329953bfb0ad17cb68",
        "patched_files": [
            "AppendUtils.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestAppendUtils.java"
        ]
    },
    "sqoop_c32010f": {
        "bug_id": "sqoop_c32010f",
        "commit": "https://github.com/apache/sqoop/commit/c32010f440f139b37840df6a3433f875898136d5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/sqoop/blob/c32010f440f139b37840df6a3433f875898136d5/src/java/org/apache/sqoop/manager/DirectPostgresqlManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/manager/DirectPostgresqlManager.java?ref=c32010f440f139b37840df6a3433f875898136d5",
                "deletions": 0,
                "filename": "src/java/org/apache/sqoop/manager/DirectPostgresqlManager.java",
                "patch": "@@ -348,6 +348,8 @@ private String writeCopyCommand(String command) throws IOException {\n   public void importTable(com.cloudera.sqoop.manager.ImportJobContext context)\n     throws IOException, ImportException {\n \n+    context.setConnManager(this);\n+\n     String tableName = context.getTableName();\n     SqoopOptions options = context.getOptions();\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/c32010f440f139b37840df6a3433f875898136d5/src/java/org/apache/sqoop/manager/DirectPostgresqlManager.java",
                "sha": "63b070416b1dc0672ddf8b80941a782807e0e63b",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/sqoop/blob/c32010f440f139b37840df6a3433f875898136d5/src/test/com/cloudera/sqoop/manager/PostgresqlImportTest.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/manager/PostgresqlImportTest.java?ref=c32010f440f139b37840df6a3433f875898136d5",
                "deletions": 3,
                "filename": "src/test/com/cloudera/sqoop/manager/PostgresqlImportTest.java",
                "patch": "@@ -235,15 +235,18 @@ public void setUpData(String tableName, String schema, boolean nullEntry) {\n     return args.toArray(new String[0]);\n   }\n \n-  private void doImportAndVerify(boolean isDirect, String [] expectedResults,\n+  private void doImportAndVerify(boolean isDirect, String[] expectedResults,\n       String tableName, String... extraArgs) throws IOException {\n \n     Path warehousePath = new Path(this.getWarehouseDir());\n     Path tablePath = new Path(warehousePath, tableName);\n-    Path filePath = new Path(tablePath, \"part-m-00000\");\n+\n+    // if importing with merge step, directory should exist and output should be from a reducer\n+    boolean isMerge = Arrays.asList(extraArgs).contains(\"--merge-key\");\n+    Path filePath = new Path(tablePath, isMerge ? \"part-r-00000\" : \"part-m-00000\");\n \n     File tableFile = new File(tablePath.toString());\n-    if (tableFile.exists() && tableFile.isDirectory()) {\n+    if (tableFile.exists() && tableFile.isDirectory() && !isMerge) {\n       // remove the directory before running the import.\n       FileListing.recursiveDeleteDir(tableFile);\n     }\n@@ -329,6 +332,34 @@ public void testIncrementalImport() throws IOException {\n     doImportAndVerify(false, expectedResults, TABLE_NAME, extraArgs);\n   }\n \n+  public void testDirectIncrementalImport() throws IOException {\n+    String [] expectedResults = { };\n+\n+    String [] extraArgs = { \"--incremental\", \"lastmodified\",\n+            \"--check-column\", \"start_date\",\n+    };\n+\n+    doImportAndVerify(true, expectedResults, TABLE_NAME, extraArgs);\n+  }\n+\n+  public void testDirectIncrementalImportMerge() throws IOException {\n+    String [] expectedResults = { };\n+\n+    String [] extraArgs = { \"--incremental\", \"lastmodified\",\n+            \"--check-column\", \"start_date\",\n+    };\n+\n+    doImportAndVerify(true, expectedResults, TABLE_NAME, extraArgs);\n+\n+    extraArgs = new String[] { \"--incremental\", \"lastmodified\",\n+            \"--check-column\", \"start_date\",\n+            \"--merge-key\", \"id\",\n+            \"--last-value\", \"2009-04-20\"\n+    };\n+\n+    doImportAndVerify(true, expectedResults, TABLE_NAME, extraArgs);\n+  }\n+\n  @Test\n   public void testDifferentSchemaImport() throws IOException {\n     String [] expectedResults = {",
                "raw_url": "https://github.com/apache/sqoop/raw/c32010f440f139b37840df6a3433f875898136d5/src/test/com/cloudera/sqoop/manager/PostgresqlImportTest.java",
                "sha": "57dd338eb5692b0491158fbcb92ac8d4e8065ba0",
                "status": "modified"
            }
        ],
        "message": "SQOOP-1826: NPE in ImportTool.lastModifiedMerge during postgres import\n\n(Ricky Nguyen via Jarek Jarcec Cecho)",
        "parent": "https://github.com/apache/sqoop/commit/8b6eb33f1720f9b25732903a5519d91587e31c39",
        "patched_files": [
            "DirectPostgresqlManager.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "PostgresqlImportTest.java"
        ]
    },
    "sqoop_cb911f3": {
        "bug_id": "sqoop_cb911f3",
        "commit": "https://github.com/apache/sqoop/commit/cb911f34b4b2c33047834bdbb4c5ce685cdf958c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/sqoop/blob/cb911f34b4b2c33047834bdbb4c5ce685cdf958c/src/java/com/cloudera/sqoop/tool/ImportTool.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/com/cloudera/sqoop/tool/ImportTool.java?ref=cb911f34b4b2c33047834bdbb4c5ce685cdf958c",
                "deletions": 1,
                "filename": "src/java/com/cloudera/sqoop/tool/ImportTool.java",
                "patch": "@@ -653,7 +653,11 @@ public void applyOptions(CommandLine in, SqoopOptions out)\n         }\n \n         if (in.hasOption(COLUMNS_ARG)) {\n-          out.setColumns(in.getOptionValue(COLUMNS_ARG).split(\",\"));\n+          String[] cols= in.getOptionValue(COLUMNS_ARG).split(\",\");\n+          for (int i=0; i<cols.length; i++) {\n+            cols[i] = cols[i].trim();\n+          }\n+          out.setColumns(cols);\n         }\n \n         if (in.hasOption(SPLIT_BY_ARG)) {",
                "raw_url": "https://github.com/apache/sqoop/raw/cb911f34b4b2c33047834bdbb4c5ce685cdf958c/src/java/com/cloudera/sqoop/tool/ImportTool.java",
                "sha": "c62b46e8b1dda049360299e88826e01a8d3f18b8",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/sqoop/blob/cb911f34b4b2c33047834bdbb4c5ce685cdf958c/src/test/com/cloudera/sqoop/TestMultiCols.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/test/com/cloudera/sqoop/TestMultiCols.java?ref=cb911f34b4b2c33047834bdbb4c5ce685cdf958c",
                "deletions": 0,
                "filename": "src/test/com/cloudera/sqoop/TestMultiCols.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package com.cloudera.sqoop;\n \n+import java.io.IOException;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n \n@@ -210,4 +212,39 @@ public void testSkipThirdCol() {\n     verifyTypes(types, insertVals, validateVals, validateLine, loadCols);\n   }\n \n+  /**\n+   * This tests that the columns argument can handle comma-separated column\n+   * names.  So this is like having:\n+   *   --columns \"DATA_COL0,DATA_COL1,DATA_COL2\"\n+   * as two args on a sqoop command line\n+   *\n+   * @throws IOException\n+   */\n+  public void testSingleColumnsArg() throws IOException {\n+    String [] types = { \"VARCHAR(32)\", \"VARCHAR(32)\", \"VARCHAR(32)\" };\n+    String [] insertVals = { \"'foo'\", \"'bar'\", \"'baz'\" };\n+    String [] validateVals = { \"foo\", \"bar\", \"baz\" };\n+    String validateLine = \"foo,bar,baz\";\n+    String [] loadCols = {\"DATA_COL0,DATA_COL1,DATA_COL2\"};\n+\n+    verifyTypes(types, insertVals, validateVals, validateLine, loadCols);\n+  }\n+\n+  /**\n+   * This tests that the columns argument can handle spaces between column\n+   * names.  So this is like having:\n+   *   --columns \"DATA_COL0, DATA_COL1, DATA_COL2\"\n+   * as two args on a sqoop command line\n+   *\n+   * @throws IOException\n+   */\n+  public void testColumnsWithSpaces() throws IOException {\n+    String [] types = { \"VARCHAR(32)\", \"VARCHAR(32)\", \"VARCHAR(32)\" };\n+    String [] insertVals = { \"'foo'\", \"'bar'\", \"'baz'\" };\n+    String [] validateVals = { \"foo\", \"bar\", \"baz\" };\n+    String validateLine = \"foo,bar,baz\";\n+    String [] loadCols = {\"DATA_COL0, DATA_COL1, DATA_COL2\"};\n+\n+    verifyTypes(types, insertVals, validateVals, validateLine, loadCols);\n+  }\n }",
                "raw_url": "https://github.com/apache/sqoop/raw/cb911f34b4b2c33047834bdbb4c5ce685cdf958c/src/test/com/cloudera/sqoop/TestMultiCols.java",
                "sha": "31bf69b6cb7725c916eb098407e086c4fd0fb354",
                "status": "modified"
            }
        ],
        "message": "SQOOP-67. NPE when column name list contains spaces\n- e.g. sqoop -import --columns \"col1, col2\"\n\nFrom: Jonathan Hsieh <jon@cloudera.com>\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/sqoop/trunk@1150042 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/sqoop/commit/c0ca0c100b56da90cce6023a57485bb090a7512f",
        "patched_files": [
            "ImportTool.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestImportTool.java",
            "TestMultiCols.java"
        ]
    },
    "sqoop_e90e244": {
        "bug_id": "sqoop_e90e244",
        "commit": "https://github.com/apache/sqoop/commit/e90e244396ecffeb332633c89e641357adcf8eda",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/sqoop/blob/e90e244396ecffeb332633c89e641357adcf8eda/src/java/org/apache/sqoop/util/SqoopJsonUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/sqoop/contents/src/java/org/apache/sqoop/util/SqoopJsonUtil.java?ref=e90e244396ecffeb332633c89e641357adcf8eda",
                "deletions": 1,
                "filename": "src/java/org/apache/sqoop/util/SqoopJsonUtil.java",
                "patch": "@@ -40,7 +40,8 @@ private SqoopJsonUtil() {\n   }\n \n   public static String getJsonStringforMap(Map<String, String> map) {\n-    JSONObject pathPartMap = new JSONObject(map);\n+    Map<String, String> mapToUse = (map == null) ? Collections.emptyMap() : map;\n+    JSONObject pathPartMap = new JSONObject(mapToUse);\n     return pathPartMap.toString();\n   }\n ",
                "raw_url": "https://github.com/apache/sqoop/raw/e90e244396ecffeb332633c89e641357adcf8eda/src/java/org/apache/sqoop/util/SqoopJsonUtil.java",
                "sha": "9e8ba95c8522fec57d865d15f41c4d4b0c6e2845",
                "status": "modified"
            }
        ],
        "message": "SQOOP-3435: Avoid NullPointerException due to different JSONObject library in classpath",
        "parent": "https://github.com/apache/sqoop/commit/0216f7fbb7b71f610d02dfa95dafb2c81acbc690",
        "patched_files": [
            "SqoopJsonUtil.java"
        ],
        "repo": "sqoop",
        "unit_tests": [
            "TestSqoopJsonUtil.java"
        ]
    }
}