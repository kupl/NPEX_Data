[
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/2a91bb64bf0c31ca889dac707ac2162f9ad95d15",
        "bug_id": "hadoop-ozone_1",
        "file": [
            {
                "additions": 4,
                "sha": "500eaeaa626eb292c6adc445c710ada1f8ab040b",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java",
                "deletions": 0,
                "filename": "hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java?ref=9b7220857129ba89b1a5ada01406b05d49430b9d",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.ozone.s3.header;\n \n import org.apache.hadoop.ozone.s3.exception.OS3Exception;\n+import org.apache.hadoop.ozone.s3.exception.S3ErrorTable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -37,6 +38,9 @@\n   private String accessKeyID;\n \n   public void parse() throws OS3Exception {\n+    if (authHeader == null) {\n+      throw S3ErrorTable.MALFORMED_HEADER;\n+    }\n     if (authHeader.startsWith(\"AWS4\")) {\n       LOG.debug(\"V4 Header {}\", authHeader);\n       AuthorizationHeaderV4 authorizationHeader = new AuthorizationHeaderV4(",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java"
            },
            {
                "additions": 114,
                "sha": "86c040438fec4ddc2fd29d6b1d058b1e9a55f5df",
                "status": "added",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java",
                "deletions": 0,
                "filename": "hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java?ref=9b7220857129ba89b1a5ada01406b05d49430b9d",
                "patch": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.ozone.s3.endpoint;\n+\n+import javax.ws.rs.core.Response;\n+\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.client.OzoneClientStub;\n+\n+import org.apache.hadoop.ozone.s3.exception.OS3Exception;\n+import org.apache.hadoop.ozone.s3.header.AuthenticationHeaderParser;\n+\n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static org.apache.hadoop.ozone.s3.AWSV4AuthParser.DATE_FORMATTER;\n+import static org.apache.hadoop.ozone.s3.exception.S3ErrorTable.MALFORMED_HEADER;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.time.LocalDate;\n+\n+/**\n+ * This class test Create Bucket functionality.\n+ */\n+public class TestBucketPut {\n+\n+  private String bucketName = OzoneConsts.BUCKET;\n+  private OzoneClientStub clientStub;\n+  private BucketEndpoint bucketEndpoint;\n+\n+  @Before\n+  public void setup() throws Exception {\n+\n+    //Create client stub and object store stub.\n+    clientStub = new OzoneClientStub();\n+\n+    // Create HeadBucket and setClient to OzoneClientStub\n+    bucketEndpoint = new BucketEndpoint();\n+    bucketEndpoint.setClient(clientStub);\n+  }\n+\n+  @Test\n+  public void testBucketFailWithAuthHeaderMissing() throws Exception {\n+\n+    bucketEndpoint.setAuthenticationHeaderParser(\n+        new AuthenticationHeaderParser());\n+    bucketEndpoint.getAuthenticationHeaderParser().setAuthHeader(null);\n+    try {\n+      bucketEndpoint.put(bucketName, null);\n+    } catch (OS3Exception ex) {\n+      Assert.assertEquals(HTTP_NOT_FOUND, ex.getHttpCode());\n+      Assert.assertEquals(MALFORMED_HEADER.getCode(), ex.getCode());\n+    }\n+  }\n+\n+  @Test\n+  public void testBucketPut() throws Exception {\n+    String auth = generateAuthHeader();\n+    bucketEndpoint.setAuthenticationHeaderParser(\n+        new AuthenticationHeaderParser());\n+    bucketEndpoint.getAuthenticationHeaderParser().setAuthHeader(auth);\n+    Response response = bucketEndpoint.put(bucketName, null);\n+    assertEquals(200, response.getStatus());\n+    assertNotNull(response.getLocation());\n+  }\n+\n+  @Test\n+  public void testBucketFailWithInvalidHeader() throws Exception {\n+    bucketEndpoint.setAuthenticationHeaderParser(\n+        new AuthenticationHeaderParser());\n+    bucketEndpoint.getAuthenticationHeaderParser().setAuthHeader(\"auth\");\n+    try {\n+      bucketEndpoint.put(bucketName, null);\n+    } catch (OS3Exception ex) {\n+      Assert.assertEquals(HTTP_NOT_FOUND, ex.getHttpCode());\n+      Assert.assertEquals(MALFORMED_HEADER.getCode(), ex.getCode());\n+    }\n+  }\n+\n+  /**\n+   * Generate dummy auth header.\n+   * @return auth header.\n+   */\n+  private String generateAuthHeader() {\n+    LocalDate now = LocalDate.now();\n+    String curDate = DATE_FORMATTER.format(now);\n+    return  \"AWS4-HMAC-SHA256 \" +\n+        \"Credential=ozone/\" + curDate + \"/us-east-1/s3/aws4_request, \" +\n+        \"SignedHeaders=host;range;x-amz-date, \" +\n+        \"Signature=fe5f80f77d5fa3beca038a248ff027\";\n+  }\n+\n+}",
                "changes": 114,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2630. NullPointerException in S3g. (#336)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/9b7220857129ba89b1a5ada01406b05d49430b9d"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/640255a2dea6e8292494d18e6fb890ba5a198c15",
        "bug_id": "hadoop-ozone_2",
        "file": [
            {
                "additions": 3,
                "sha": "b15828a153098650236023fb3f307ea35c111283",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "deletions": 1,
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java?ref=2862cddabec7913e2bbca234ce48a3dbe5eadd24",
                "patch": "@@ -78,7 +78,9 @@\n   private boolean isSecurityEnabled;\n   private final boolean topologyAwareRead;\n   /**\n-   * Creates a new XceiverClientManager.\n+   * Creates a new XceiverClientManager for non secured ozone cluster.\n+   * For security enabled ozone cluster, client should use the other constructor\n+   * with a valid ca certificate in pem string format.\n    *\n    * @param conf configuration\n    */",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java"
            },
            {
                "additions": 19,
                "sha": "0b5c18e8205cb5d11d0c5ba7435d4e441d4e3d8c",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java",
                "deletions": 2,
                "filename": "hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java?ref=2862cddabec7913e2bbca234ce48a3dbe5eadd24",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.hdds.cli.GenericCli;\n import org.apache.hadoop.hdds.cli.HddsVersionProvider;\n import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.SCMSecurityProtocol;\n import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n import org.apache.hadoop.hdds.scm.XceiverClientManager;\n import org.apache.hadoop.hdds.scm.cli.container.ContainerCommands;\n@@ -36,17 +37,20 @@\n import org.apache.hadoop.hdds.scm.protocolPB\n     .StorageContainerLocationProtocolClientSideTranslatorPB;\n import org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolPB;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n import org.apache.hadoop.hdds.tracing.TracingUtil;\n import org.apache.hadoop.ipc.Client;\n import org.apache.hadoop.ipc.ProtobufRpcEngine;\n import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.OzoneSecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.util.NativeCodeLoader;\n \n import org.apache.commons.lang3.StringUtils;\n import static org.apache.hadoop.hdds.HddsUtils.getScmAddressForClients;\n+import static org.apache.hadoop.hdds.HddsUtils.getScmSecurityClient;\n import static org.apache.hadoop.hdds.scm.ScmConfigKeys\n     .OZONE_SCM_CLIENT_ADDRESS_KEY;\n import static org.apache.hadoop.hdds.scm.ScmConfigKeys.OZONE_SCM_CONTAINER_SIZE;\n@@ -136,8 +140,21 @@ public ScmClient createScmClient()\n                 NetUtils.getDefaultSocketFactory(ozoneConf),\n                 Client.getRpcTimeout(ozoneConf))),\n             StorageContainerLocationProtocol.class, ozoneConf);\n-    return new ContainerOperationClient(\n-        client, new XceiverClientManager(ozoneConf));\n+\n+    XceiverClientManager xceiverClientManager = null;\n+    if (OzoneSecurityUtil.isSecurityEnabled(ozoneConf)) {\n+      SecurityConfig securityConfig = new SecurityConfig(ozoneConf);\n+      SCMSecurityProtocol scmSecurityProtocolClient = getScmSecurityClient(\n+          (OzoneConfiguration) securityConfig.getConfiguration());\n+      String caCertificate =\n+          scmSecurityProtocolClient.getCACertificate();\n+      xceiverClientManager = new XceiverClientManager(ozoneConf,\n+          OzoneConfiguration.of(ozoneConf).getObject(XceiverClientManager\n+              .ScmClientConfig.class), caCertificate);\n+    } else {\n+      xceiverClientManager = new XceiverClientManager(ozoneConf);\n+    }\n+    return new ContainerOperationClient(client, xceiverClientManager);\n   }\n \n   public void checkContainerExists(ScmClient scmClient, long containerId)",
                "changes": 21,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java"
            },
            {
                "additions": 2,
                "sha": "f32846386a9f772bbd14615d378bd8bde304e06a",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh",
                "deletions": 0,
                "filename": "hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh?ref=2862cddabec7913e2bbca234ce48a3dbe5eadd24",
                "patch": "@@ -35,6 +35,8 @@ execute_robot_test scm ozonefs/ozonefs.robot\n \n execute_robot_test s3g s3\n \n+execute_robot_test scm scmcli\n+\n stop_docker_env\n \n generate_report",
                "changes": 2,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh"
            },
            {
                "additions": 28,
                "sha": "6a6f0b0eb782aa1c7f2ba9744616b9f1a4bfafe9",
                "status": "added",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot",
                "deletions": 0,
                "filename": "hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot?ref=2862cddabec7913e2bbca234ce48a3dbe5eadd24",
                "patch": "@@ -0,0 +1,28 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+*** Settings ***\n+Documentation       Smoketest ozone cluster startup\n+Library             OperatingSystem\n+Library             BuiltIn\n+Resource            ../commonlib.robot\n+\n+*** Variables ***\n+\n+\n+*** Test Cases ***\n+Run list pipeline\n+    ${output} =         Execute          ozone scmcli pipeline list\n+                        Should contain   ${output}   Type:RATIS, Factor:ONE, State:OPEN\n\\ No newline at end of file",
                "changes": 28,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/2862cddabec7913e2bbca234ce48a3dbe5eadd24/hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2282. scmcli pipeline list command throws NullPointerException. Contributed by Xiaoyu Yao. (#1642)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/2862cddabec7913e2bbca234ce48a3dbe5eadd24"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/d7784729eb6c58c5e8e048dfdfa361e86eee2e11",
        "bug_id": "hadoop-ozone_3",
        "file": [
            {
                "additions": 3,
                "sha": "bf25b049f8f49a96db65e7319481d9a6057e2d65",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d57b868af49aae33bcdcf90deb836d55c7131dbb/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconServer.java",
                "deletions": 1,
                "filename": "hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconServer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconServer.java?ref=d57b868af49aae33bcdcf90deb836d55c7131dbb",
                "patch": "@@ -98,7 +98,9 @@ protected void configureServlets() {\n \n   void stop() throws Exception {\n     LOG.info(\"Stopping Recon server\");\n-    httpServer.stop();\n+    if (httpServer != null) {\n+      httpServer.stop();\n+    }\n     OzoneManagerServiceProvider ozoneManagerServiceProvider = injector\n         .getInstance(OzoneManagerServiceProvider.class);\n     ozoneManagerServiceProvider.stop();",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d57b868af49aae33bcdcf90deb836d55c7131dbb/hadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/ReconServer.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2776. NPE when stop recon while start recon failed (#376)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/d57b868af49aae33bcdcf90deb836d55c7131dbb"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/b1ac520ed95fe35b454e55695a05ed71299d8797",
        "bug_id": "hadoop-ozone_4",
        "file": [
            {
                "additions": 5,
                "sha": "6dc9f96576a80aae4751d635a0c8053f54f61bea",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/e47acd129380dc0b570a38f1767a9c388d45c09f/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/TraceAllMethod.java",
                "deletions": 0,
                "filename": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/TraceAllMethod.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/TraceAllMethod.java?ref=e47acd129380dc0b570a38f1767a9c388d45c09f",
                "patch": "@@ -59,6 +59,11 @@ public TraceAllMethod(T delegate, String name) {\n   public Object invoke(Object proxy, Method method, Object[] args)\n       throws Throwable {\n     Method delegateMethod = findDelegatedMethod(method);\n+    if (delegateMethod == null) {\n+      throw new NoSuchMethodException(\"Method not found: \" +\n+        method.getName());\n+    }\n+\n     try (Scope scope = GlobalTracer.get().buildSpan(\n         name + \".\" + method.getName())\n         .startActive(true)) {",
                "changes": 5,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/e47acd129380dc0b570a38f1767a9c388d45c09f/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/tracing/TraceAllMethod.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2512 Sonar TraceAllMethod NPE Could be Thrown (#193)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/e47acd129380dc0b570a38f1767a9c388d45c09f"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/8af5ab8dacdea4e882491ea0d9ebe81806ebf9fa",
        "bug_id": "hadoop-ozone_5",
        "file": [
            {
                "additions": 68,
                "sha": "9cb0eb16a78c0539a7012a04d3d4a34afae7fabb",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java",
                "deletions": 73,
                "filename": "hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java?ref=89bdb6a69e6314384e79f76e509cf99437877780",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -141,23 +141,17 @@ public Response put(\n       String copyHeader = headers.getHeaderString(COPY_SOURCE_HEADER);\n       String storageType = headers.getHeaderString(STORAGE_CLASS_HEADER);\n \n-      ReplicationType replicationType;\n-      ReplicationFactor replicationFactor;\n+      S3StorageType s3StorageType;\n       boolean storageTypeDefault;\n       if (storageType == null || storageType.equals(\"\")) {\n-        replicationType = S3StorageType.getDefault().getType();\n-        replicationFactor = S3StorageType.getDefault().getFactor();\n+        s3StorageType = S3StorageType.getDefault();\n         storageTypeDefault = true;\n       } else {\n-        try {\n-          replicationType = S3StorageType.valueOf(storageType).getType();\n-          replicationFactor = S3StorageType.valueOf(storageType).getFactor();\n-        } catch (IllegalArgumentException ex) {\n-          throw S3ErrorTable.newError(S3ErrorTable.INVALID_ARGUMENT,\n-              storageType);\n-        }\n+        s3StorageType = toS3StorageType(storageType);\n         storageTypeDefault = false;\n       }\n+      ReplicationType replicationType = s3StorageType.getType();\n+      ReplicationFactor replicationFactor = s3StorageType.getFactor();\n \n       if (copyHeader != null) {\n         //Copy object, as copy source available.\n@@ -214,10 +208,7 @@ public Response get(\n \n       if (uploadId != null) {\n         // When we have uploadId, this is the request for list Parts.\n-        int partMarker = 0;\n-        if (partNumberMarker != null) {\n-          partMarker = Integer.parseInt(partNumberMarker);\n-        }\n+        int partMarker = parsePartNumberMarker(partNumberMarker);\n         return listParts(bucketName, keyPath, uploadId,\n             partMarker, maxParts);\n       }\n@@ -233,16 +224,12 @@ public Response get(\n       String rangeHeaderVal = headers.getHeaderString(RANGE_HEADER);\n       RangeHeader rangeHeader = null;\n \n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"range Header provided value is {}\", rangeHeaderVal);\n-      }\n+      LOG.debug(\"range Header provided value: {}\", rangeHeaderVal);\n \n       if (rangeHeaderVal != null) {\n         rangeHeader = RangeHeaderParserUtil.parseRangeHeader(rangeHeaderVal,\n             length);\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"range Header provided value is {}\", rangeHeader);\n-        }\n+        LOG.debug(\"range Header provided: {}\", rangeHeader);\n         if (rangeHeader.isInValidRange()) {\n           throw S3ErrorTable.newError(\n               S3ErrorTable.INVALID_RANGE, rangeHeaderVal);\n@@ -261,20 +248,13 @@ public Response get(\n             .header(CONTENT_LENGTH, keyDetails.getDataSize());\n \n       } else {\n-        LOG.debug(\"range Header provided value is {}\", rangeHeader);\n         OzoneInputStream key = bucket.readKey(keyPath);\n \n         long startOffset = rangeHeader.getStartOffset();\n         long endOffset = rangeHeader.getEndOffset();\n-        long copyLength;\n-        if (startOffset == endOffset) {\n-          // if range header is given as bytes=0-0, then we should return 1\n-          // byte from start offset\n-          copyLength = 1;\n-        } else {\n-          copyLength = rangeHeader.getEndOffset() - rangeHeader\n-              .getStartOffset() + 1;\n-        }\n+        // eg. if range header is given as bytes=0-0, then we should return 1\n+        // byte from start offset\n+        long copyLength = endOffset - startOffset + 1;\n         StreamingOutput output = dest -> {\n           try (S3WrapperInputStream s3WrapperInputStream =\n               new S3WrapperInputStream(\n@@ -334,7 +314,8 @@ private void addLastModifiedDate(\n   @HEAD\n   public Response head(\n       @PathParam(\"bucket\") String bucketName,\n-      @PathParam(\"path\") String keyPath) throws Exception {\n+      @PathParam(\"path\") String keyPath) throws IOException, OS3Exception {\n+\n     OzoneKeyDetails key;\n \n     try {\n@@ -441,20 +422,14 @@ public Response initializeMultipartUpload(\n       OzoneBucket ozoneBucket = getBucket(bucket);\n       String storageType = headers.getHeaderString(STORAGE_CLASS_HEADER);\n \n-      ReplicationType replicationType;\n-      ReplicationFactor replicationFactor;\n+      S3StorageType s3StorageType;\n       if (storageType == null || storageType.equals(\"\")) {\n-        replicationType = S3StorageType.getDefault().getType();\n-        replicationFactor = S3StorageType.getDefault().getFactor();\n+        s3StorageType = S3StorageType.getDefault();\n       } else {\n-        try {\n-          replicationType = S3StorageType.valueOf(storageType).getType();\n-          replicationFactor = S3StorageType.valueOf(storageType).getFactor();\n-        } catch (IllegalArgumentException ex) {\n-          throw S3ErrorTable.newError(S3ErrorTable.INVALID_ARGUMENT,\n-              storageType);\n-        }\n+        s3StorageType = toS3StorageType(storageType);\n       }\n+      ReplicationType replicationType = s3StorageType.getType();\n+      ReplicationFactor replicationFactor = s3StorageType.getFactor();\n \n       OmMultipartInfo multipartInfo = ozoneBucket\n           .initiateMultipartUpload(key, replicationType, replicationFactor);\n@@ -541,9 +516,10 @@ private Response createMultipartKey(String bucket, String key, long length,\n     try {\n       OzoneBucket ozoneBucket = getBucket(bucket);\n       String copyHeader;\n-      OmMultipartCommitUploadPartInfo omMultipartCommitUploadPartInfo;\n-      try (OzoneOutputStream ozoneOutputStream = ozoneBucket.createMultipartKey(\n-          key, length, partNumber, uploadID)) {\n+      OzoneOutputStream ozoneOutputStream = null;\n+      try {\n+        ozoneOutputStream = ozoneBucket.createMultipartKey(\n+            key, length, partNumber, uploadID);\n         copyHeader = headers.getHeaderString(COPY_SOURCE_HEADER);\n         if (copyHeader != null) {\n           Pair<String, String> result = parseSourceHeader(copyHeader);\n@@ -570,9 +546,12 @@ private Response createMultipartKey(String bucket, String key, long length,\n         } else {\n           IOUtils.copy(body, ozoneOutputStream);\n         }\n-        omMultipartCommitUploadPartInfo = ozoneOutputStream\n-            .getCommitUploadPartInfo();\n+      } finally {\n+        IOUtils.closeQuietly(ozoneOutputStream);\n       }\n+\n+      OmMultipartCommitUploadPartInfo omMultipartCommitUploadPartInfo =\n+          ozoneOutputStream.getCommitUploadPartInfo();\n       String eTag = omMultipartCommitUploadPartInfo.getPartName();\n \n       if (copyHeader != null) {\n@@ -672,30 +651,28 @@ private CopyObjectResponse copyObject(String copyHeader,\n     try {\n       // Checking whether we trying to copying to it self.\n \n-      if (sourceBucket.equals(destBucket)) {\n-        if (sourceKey.equals(destkey)) {\n-          // When copying to same storage type when storage type is provided,\n-          // we should not throw exception, as aws cli checks if any of the\n-          // options like storage type are provided or not when source and\n-          // dest are given same\n-          if (storageTypeDefault) {\n-            OS3Exception ex = S3ErrorTable.newError(S3ErrorTable\n-                .INVALID_REQUEST, copyHeader);\n-            ex.setErrorMessage(\"This copy request is illegal because it is \" +\n-                \"trying to copy an object to it self itself without changing \" +\n-                \"the object's metadata, storage class, website redirect \" +\n-                \"location or encryption attributes.\");\n-            throw ex;\n-          } else {\n-            // TODO: Actually here we should change storage type, as ozone\n-            // still does not support this just returning dummy response\n-            // for now\n-            CopyObjectResponse copyObjectResponse = new CopyObjectResponse();\n-            copyObjectResponse.setETag(OzoneUtils.getRequestID());\n-            copyObjectResponse.setLastModified(Instant.ofEpochMilli(\n-                Time.now()));\n-            return copyObjectResponse;\n-          }\n+      if (sourceBucket.equals(destBucket) && sourceKey.equals(destkey)) {\n+        // When copying to same storage type when storage type is provided,\n+        // we should not throw exception, as aws cli checks if any of the\n+        // options like storage type are provided or not when source and\n+        // dest are given same\n+        if (storageTypeDefault) {\n+          OS3Exception ex = S3ErrorTable.newError(S3ErrorTable\n+              .INVALID_REQUEST, copyHeader);\n+          ex.setErrorMessage(\"This copy request is illegal because it is \" +\n+              \"trying to copy an object to it self itself without changing \" +\n+              \"the object's metadata, storage class, website redirect \" +\n+              \"location or encryption attributes.\");\n+          throw ex;\n+        } else {\n+          // TODO: Actually here we should change storage type, as ozone\n+          // still does not support this just returning dummy response\n+          // for now\n+          CopyObjectResponse copyObjectResponse = new CopyObjectResponse();\n+          copyObjectResponse.setETag(OzoneUtils.getRequestID());\n+          copyObjectResponse.setLastModified(Instant.ofEpochMilli(\n+              Time.now()));\n+          return copyObjectResponse;\n         }\n       }\n \n@@ -766,4 +743,22 @@ private CopyObjectResponse copyObject(String copyHeader,\n \n     return Pair.of(header.substring(0, pos), header.substring(pos + 1));\n   }\n+\n+  private static S3StorageType toS3StorageType(String storageType)\n+      throws OS3Exception {\n+    try {\n+      return S3StorageType.valueOf(storageType);\n+    } catch (IllegalArgumentException ex) {\n+      throw S3ErrorTable.newError(S3ErrorTable.INVALID_ARGUMENT,\n+          storageType);\n+    }\n+  }\n+\n+  private static int parsePartNumberMarker(String partNumberMarker) {\n+    int partMarker = 0;\n+    if (partNumberMarker != null) {\n+      partMarker = Integer.parseInt(partNumberMarker);\n+    }\n+    return partMarker;\n+  }\n }",
                "changes": 141,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java"
            },
            {
                "additions": 6,
                "sha": "83cb90799f1c7a67e7e7132e7ceaf46152d82369",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java",
                "deletions": 2,
                "filename": "hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java?ref=89bdb6a69e6314384e79f76e509cf99437877780",
                "patch": "@@ -32,6 +32,7 @@\n public class OzoneOutputStreamStub extends OzoneOutputStream {\n \n   private final String partName;\n+  private boolean closed;\n \n   /**\n    * Constructs OzoneOutputStreamStub with outputStream and partName.\n@@ -62,12 +63,15 @@ public synchronized void flush() throws IOException {\n   @Override\n   public synchronized void close() throws IOException {\n     //commitKey can be done here, if needed.\n-    getOutputStream().close();\n+    if (!closed) {\n+      getOutputStream().close();\n+      closed = true;\n+    }\n   }\n \n   @Override\n   public OmMultipartCommitUploadPartInfo getCommitUploadPartInfo() {\n-    return new OmMultipartCommitUploadPartInfo(partName);\n+    return closed ? new OmMultipartCommitUploadPartInfo(partName) : null;\n   }\n \n }",
                "changes": 8,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2521. Multipart upload failing with NPE\n\nCloses #206",
        "commit": "https://github.com/apache/hadoop-ozone/commit/89bdb6a69e6314384e79f76e509cf99437877780"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/27b6042f0e99ebcfdbbe21ee905084be3aadd6b1",
        "bug_id": "hadoop-ozone_6",
        "file": [
            {
                "additions": 23,
                "sha": "236370b2ab9eb24e5001529c61a70ceb105e0188",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "deletions": 15,
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java?ref=d3021fb2bc895b0dfa25b0d562e4a2664164532a",
                "patch": "@@ -152,8 +152,11 @@ public void connect(String encodedToken) throws Exception {\n     connectToDatanode(dn, encodedToken);\n   }\n \n-  private void connectToDatanode(DatanodeDetails dn, String encodedToken)\n-      throws IOException {\n+  private synchronized void connectToDatanode(DatanodeDetails dn,\n+      String encodedToken) throws IOException {\n+    if (isConnected(dn)){\n+      return;\n+    }\n     // read port from the data node, on failure use default configured\n     // port.\n     int port = dn.getPort(DatanodeDetails.Port.Name.STANDALONE).getValue();\n@@ -208,7 +211,7 @@ private boolean isConnected(ManagedChannel channel) {\n   }\n \n   @Override\n-  public void close() {\n+  public synchronized void close() {\n     closed = true;\n     for (ManagedChannel channel : channels.values()) {\n       channel.shutdownNow();\n@@ -397,19 +400,9 @@ public XceiverClientReply sendCommandAsync(\n \n   private XceiverClientReply sendCommandAsync(\n       ContainerCommandRequestProto request, DatanodeDetails dn)\n-      throws IOException, ExecutionException, InterruptedException {\n-    if (closed) {\n-      throw new IOException(\"This channel is not connected.\");\n-    }\n-\n+      throws IOException, InterruptedException {\n+    checkOpen(dn, request.getEncodedToken());\n     UUID dnId = dn.getUuid();\n-    ManagedChannel channel = channels.get(dnId);\n-    // If the channel doesn't exist for this specific datanode or the channel\n-    // is closed, just reconnect\n-    String token = request.getEncodedToken();\n-    if (!isConnected(channel)) {\n-      reconnect(dn, token);\n-    }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Send command {} to datanode {}\",\n           request.getCmdType().toString(), dn.getNetworkFullPath());\n@@ -456,6 +449,21 @@ public void onCompleted() {\n     return new XceiverClientReply(replyFuture);\n   }\n \n+  private synchronized void checkOpen(DatanodeDetails dn, String encodedToken)\n+      throws IOException{\n+    if (closed) {\n+      throw new IOException(\"This channel is not connected.\");\n+    }\n+\n+    ManagedChannel channel = channels.get(dn.getUuid());\n+    // If the channel doesn't exist for this specific datanode or the channel\n+    // is closed, just reconnect\n+    if (!isConnected(channel)) {\n+      reconnect(dn, encodedToken);\n+    }\n+\n+  }\n+\n   private void reconnect(DatanodeDetails dn, String encodedToken)\n       throws IOException {\n     ManagedChannel channel;",
                "changes": 38,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java"
            },
            {
                "additions": 1,
                "sha": "648d22dbb064107f82662d4dead7e6f1cb10e1c8",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "deletions": 1,
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java?ref=d3021fb2bc895b0dfa25b0d562e4a2664164532a",
                "patch": "@@ -231,7 +231,6 @@ public XceiverClientSpi call() throws Exception {\n             case RATIS:\n               client = XceiverClientRatis.newXceiverClientRatis(pipeline, conf,\n                   caCert);\n-              client.connect();\n               break;\n             case STAND_ALONE:\n               client = new XceiverClientGrpc(pipeline, conf, caCert);\n@@ -240,6 +239,7 @@ public XceiverClientSpi call() throws Exception {\n             default:\n               throw new IOException(\"not implemented\" + pipeline.getType());\n             }\n+            client.connect();\n             return client;\n           }\n         });",
                "changes": 2,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2347 XCeiverClientGrpc's parallel use leads to NPE (#81)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/d3021fb2bc895b0dfa25b0d562e4a2664164532a"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/f7ba61684460dbeff67886c6b7cbfcda7688ec2c",
        "bug_id": "hadoop-ozone_7",
        "file": [
            {
                "additions": 5,
                "sha": "1feb5001dcabb06ac7fc9bb5fe7178cb5bcb0f39",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java",
                "deletions": 0,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -321,6 +321,11 @@ private ContainerCommandResponseProto dispatchRequest(\n         // Once container is marked unhealthy, all the subsequent write\n         // transactions will fail with UNHEALTHY_CONTAINER exception.\n \n+        if (container == null) {\n+          throw new NullPointerException(\n+              \"Error on creating containers \" + result + \" \" + responseProto\n+                  .getMessage());\n+        }\n         // For container to be moved to unhealthy state here, the container can\n         // only be in open or closing state.\n         State containerState = container.getContainerData().getState();",
                "changes": 5,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java"
            },
            {
                "additions": 28,
                "sha": "c623e1419b40d2e6a4bf7d01ae32255e42a3edab",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java",
                "deletions": 32,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -18,25 +18,19 @@\n \n package org.apache.hadoop.ozone.container.common.interfaces;\n \n-\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n+import java.util.function.Consumer;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerCommandRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerCommandResponseProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerType;\n-import org.apache.hadoop.hdds.protocol.proto\n-    .StorageContainerDatanodeProtocolProtos.IncrementalContainerReportProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerType;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n import org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n-import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.transport.server.ratis.DispatcherContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n@@ -54,41 +48,46 @@\n   protected final VolumeSet volumeSet;\n   protected String scmID;\n   protected final ContainerMetrics metrics;\n+  protected String datanodeId;\n+  private Consumer<ContainerReplicaProto> icrSender;\n \n-  private final StateContext context;\n-  private final DatanodeDetails datanodeDetails;\n-\n-  protected Handler(Configuration config, StateContext context,\n+  protected Handler(Configuration config, String datanodeId,\n       ContainerSet contSet, VolumeSet volumeSet,\n-      ContainerMetrics containerMetrics) {\n+      ContainerMetrics containerMetrics,\n+      Consumer<ContainerReplicaProto> icrSender) {\n     this.conf = config;\n-    this.context = context;\n     this.containerSet = contSet;\n     this.volumeSet = volumeSet;\n     this.metrics = containerMetrics;\n-    this.datanodeDetails = context.getParent().getDatanodeDetails();\n+    this.datanodeId = datanodeId;\n+    this.icrSender = icrSender;\n   }\n \n   public static Handler getHandlerForContainerType(\n       final ContainerType containerType, final Configuration config,\n-      final StateContext context, final ContainerSet contSet,\n-      final VolumeSet volumeSet, final ContainerMetrics metrics) {\n+      final String datanodeId, final ContainerSet contSet,\n+      final VolumeSet volumeSet, final ContainerMetrics metrics,\n+      Consumer<ContainerReplicaProto> icrSender) {\n     switch (containerType) {\n     case KeyValueContainer:\n-      return new KeyValueHandler(config, context, contSet, volumeSet, metrics);\n+      return new KeyValueHandler(config,\n+          datanodeId, contSet, volumeSet, metrics,\n+          icrSender);\n     default:\n       throw new IllegalArgumentException(\"Handler for ContainerType: \" +\n-        containerType + \"doesn't exist.\");\n+          containerType + \"doesn't exist.\");\n     }\n   }\n \n   /**\n    * Returns the Id of this datanode.\n+   *\n    * @return datanode Id\n    */\n-  protected DatanodeDetails getDatanodeDetails() {\n-    return datanodeDetails;\n+  protected String getDatanodeId() {\n+    return datanodeId;\n   }\n+\n   /**\n    * This should be called whenever there is state change. It will trigger\n    * an ICR to SCM.\n@@ -97,12 +96,8 @@ protected DatanodeDetails getDatanodeDetails() {\n    */\n   protected void sendICR(final Container container)\n       throws StorageContainerException {\n-    IncrementalContainerReportProto icr = IncrementalContainerReportProto\n-        .newBuilder()\n-        .addReport(container.getContainerReport())\n-        .build();\n-    context.addReport(icr);\n-    context.getParent().triggerHeartbeat();\n+    ContainerReplicaProto containerReport = container.getContainerReport();\n+    icrSender.accept(containerReport);\n   }\n \n   public abstract ContainerCommandResponseProto handle(\n@@ -175,8 +170,9 @@ public abstract void closeContainer(Container container)\n    * Deletes the given container.\n    *\n    * @param container container to be deleted\n-   * @param force if this is set to true, we delete container without checking\n-   * state of the container.\n+   * @param force     if this is set to true, we delete container without\n+   *                  checking\n+   *                  state of the container.\n    * @throws IOException\n    */\n   public abstract void deleteContainer(Container container, boolean force)",
                "changes": 60,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java"
            },
            {
                "additions": 7,
                "sha": "5da0b8fcb8bcba414cb89b24c081720a0663dfa6",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "deletions": 5,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Consumer;\n import java.util.function.Function;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -47,6 +48,7 @@\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n     .PutSmallFileRequestProto;\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Type;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n import org.apache.hadoop.hdds.scm.ByteStringConversion;\n import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n import org.apache.hadoop.hdds.scm.container.common.helpers\n@@ -60,7 +62,6 @@\n import org.apache.hadoop.ozone.container.common.interfaces.Container;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.interfaces.VolumeChoosingPolicy;\n-import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.transport.server.ratis\n     .DispatcherContext;\n import org.apache.hadoop.ozone.container.common.transport.server.ratis\n@@ -109,9 +110,10 @@\n   private final AutoCloseableLock containerCreationLock;\n   private final boolean doSyncWrite;\n \n-  public KeyValueHandler(Configuration config, StateContext context,\n-      ContainerSet contSet, VolumeSet volSet, ContainerMetrics metrics) {\n-    super(config, context, contSet, volSet, metrics);\n+  public KeyValueHandler(Configuration config, String datanodeId,\n+      ContainerSet contSet, VolumeSet volSet, ContainerMetrics metrics,\n+      Consumer<ContainerReplicaProto> icrSender) {\n+    super(config, datanodeId, contSet, volSet, metrics, icrSender);\n     containerType = ContainerType.KeyValueContainer;\n     blockManager = new BlockManagerImpl(config);\n     doSyncWrite =\n@@ -220,7 +222,7 @@ ContainerCommandResponseProto handleCreateContainer(\n \n     KeyValueContainerData newContainerData = new KeyValueContainerData(\n         containerID, maxContainerSize, request.getPipelineID(),\n-        getDatanodeDetails().getUuidString());\n+        getDatanodeId());\n     // TODO: Add support to add metadataList to ContainerData. Add metadata\n     // to container during creation.\n     KeyValueContainer newContainer = new KeyValueContainer(",
                "changes": 12,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java"
            },
            {
                "additions": 36,
                "sha": "328cd91e82afd59e50584c7676c3e7630f2733fc",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java",
                "deletions": 23,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -18,16 +18,21 @@\n \n package org.apache.hadoop.ozone.container.ozoneimpl;\n \n-import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.collect.Maps;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+\n import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n-import org.apache.hadoop.hdds.protocol.datanode.proto\n-    .ContainerProtos.ContainerType;\n-import org.apache.hadoop.hdds.protocol.proto\n-    .StorageContainerDatanodeProtocolProtos;\n-import org.apache.hadoop.hdds.protocol.proto\n-        .StorageContainerDatanodeProtocolProtos.PipelineReportsProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerType;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.IncrementalContainerReportProto;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.PipelineReportsProto;\n import org.apache.hadoop.hdds.security.token.BlockTokenVerifier;\n import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n import org.apache.hadoop.hdds.security.x509.certificate.client.CertificateClient;\n@@ -42,24 +47,20 @@\n import org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis;\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n-\n import org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService;\n import org.apache.hadoop.ozone.container.replication.GrpcReplicationService;\n-import org.apache.hadoop.ozone.container.replication\n-    .OnDemandContainerReplicationSource;\n+import org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.Maps;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_INTERVAL;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_INTERVAL_DEFAULT;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_TIMEOUT;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_TIMEOUT_DEFAULT;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.*;\n-import java.util.ArrayList;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.TimeUnit;\n-\n-import static org.apache.hadoop.ozone.OzoneConfigKeys.*;\n-\n /**\n  * Ozone main class sets up the network servers and initializes the container\n  * layer.\n@@ -100,10 +101,22 @@ public OzoneContainer(DatanodeDetails datanodeDetails, OzoneConfiguration\n     buildContainerSet();\n     final ContainerMetrics metrics = ContainerMetrics.create(conf);\n     this.handlers = Maps.newHashMap();\n+\n+    Consumer<ContainerReplicaProto> icrSender = containerReplicaProto -> {\n+      IncrementalContainerReportProto icr = IncrementalContainerReportProto\n+          .newBuilder()\n+          .addReport(containerReplicaProto)\n+          .build();\n+      context.addReport(icr);\n+      context.getParent().triggerHeartbeat();\n+    };\n+\n     for (ContainerType containerType : ContainerType.values()) {\n       handlers.put(containerType,\n           Handler.getHandlerForContainerType(\n-              containerType, conf, context, containerSet, volumeSet, metrics));\n+              containerType, conf,\n+              context.getParent().getDatanodeDetails().getUuidString(),\n+              containerSet, volumeSet, metrics, icrSender));\n     }\n \n     SecurityConfig secConf = new SecurityConfig(conf);\n@@ -169,7 +182,6 @@ private void buildContainerSet() {\n \n   }\n \n-\n   /**\n    * Start background daemon thread for performing container integrity checks.\n    */\n@@ -240,13 +252,14 @@ public void stop() {\n     ContainerMetrics.remove();\n   }\n \n-\n   @VisibleForTesting\n   public ContainerSet getContainerSet() {\n     return containerSet;\n   }\n+\n   /**\n    * Returns container report.\n+   *\n    * @return - container report.\n    */\n ",
                "changes": 59,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java"
            },
            {
                "additions": 12,
                "sha": "9db11d03f5c4fab373fd12fcc193bc7e6beb59e9",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java",
                "deletions": 4,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -36,6 +36,7 @@\n     .WriteChunkRequestProto;\n import org.apache.hadoop.hdds.protocol.proto\n     .StorageContainerDatanodeProtocolProtos.ContainerAction;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n import org.apache.hadoop.ozone.common.Checksum;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerUtils;\n@@ -48,6 +49,7 @@\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n import org.apache.hadoop.test.GenericTestUtils;\n+\n import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -57,6 +59,7 @@\n import java.io.IOException;\n import java.util.Map;\n import java.util.UUID;\n+import java.util.function.Consumer;\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.hadoop.hdds.scm.ScmConfigKeys.HDDS_DATANODE_DIR_KEY;\n@@ -69,6 +72,9 @@\n  */\n public class TestHddsDispatcher {\n \n+  public static final Consumer<ContainerReplicaProto> NO_OP_ICR_SENDER =\n+      c -> {};\n+\n   @Test\n   public void testContainerCloseActionWhenFull() throws IOException {\n     String testDir = GenericTestUtils.getTempPath(\n@@ -98,8 +104,9 @@ public void testContainerCloseActionWhenFull() throws IOException {\n       Map<ContainerType, Handler> handlers = Maps.newHashMap();\n       for (ContainerType containerType : ContainerType.values()) {\n         handlers.put(containerType,\n-            Handler.getHandlerForContainerType(containerType, conf, context,\n-                containerSet, volumeSet, metrics));\n+            Handler.getHandlerForContainerType(containerType, conf,\n+                context.getParent().getDatanodeDetails().getUuidString(),\n+                containerSet, volumeSet, metrics, NO_OP_ICR_SENDER));\n       }\n       HddsDispatcher hddsDispatcher = new HddsDispatcher(\n           conf, containerSet, volumeSet, handlers, context, metrics, null);\n@@ -214,8 +221,9 @@ private HddsDispatcher createDispatcher(DatanodeDetails dd, UUID scmId,\n     Map<ContainerType, Handler> handlers = Maps.newHashMap();\n     for (ContainerType containerType : ContainerType.values()) {\n       handlers.put(containerType,\n-          Handler.getHandlerForContainerType(containerType, conf, context,\n-              containerSet, volumeSet, metrics));\n+          Handler.getHandlerForContainerType(containerType, conf,\n+              context.getParent().getDatanodeDetails().getUuidString(),\n+              containerSet, volumeSet, metrics, NO_OP_ICR_SENDER));\n     }\n \n     HddsDispatcher hddsDispatcher = new HddsDispatcher(",
                "changes": 16,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java"
            },
            {
                "additions": 7,
                "sha": "086b0fea9116db9b9497a88716131c752eacb880",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -19,16 +19,19 @@\n package org.apache.hadoop.ozone.container.common.interfaces;\n \n import com.google.common.collect.Maps;\n+\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n@@ -69,7 +72,10 @@ public void setup() throws Exception {\n         ContainerProtos.ContainerType.values()) {\n       handlers.put(containerType,\n           Handler.getHandlerForContainerType(\n-              containerType, conf, context, containerSet, volumeSet, metrics));\n+              containerType, conf,\n+              context.getParent().getDatanodeDetails().getUuidString(),\n+              containerSet, volumeSet, metrics,\n+              TestHddsDispatcher.NO_OP_ICR_SENDER));\n     }\n     this.dispatcher = new HddsDispatcher(\n         conf, containerSet, volumeSet, handlers, null, metrics, null);",
                "changes": 8,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java"
            },
            {
                "additions": 7,
                "sha": "05b92cc825396d6a367ccf6f447fdc72e97a2a26",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -244,8 +244,10 @@ public void testVolumeSetInKeyValueHandler() throws Exception{\n       Mockito.when(stateMachine.getDatanodeDetails())\n           .thenReturn(datanodeDetails);\n       Mockito.when(context.getParent()).thenReturn(stateMachine);\n-      KeyValueHandler keyValueHandler = new KeyValueHandler(conf, context, cset,\n-          volumeSet, metrics);\n+      KeyValueHandler keyValueHandler = new KeyValueHandler(conf,\n+          context.getParent().getDatanodeDetails().getUuidString(), cset,\n+          volumeSet, metrics, c -> {\n+      });\n       assertEquals(\"org.apache.hadoop.ozone.container.common\" +\n           \".volume.RoundRobinVolumeChoosingPolicy\",\n           keyValueHandler.getVolumeChoosingPolicyForTesting()\n@@ -255,7 +257,9 @@ public void testVolumeSetInKeyValueHandler() throws Exception{\n       conf.set(HDDS_DATANODE_VOLUME_CHOOSING_POLICY,\n           \"org.apache.hadoop.ozone.container.common.impl.HddsDispatcher\");\n       try {\n-        new KeyValueHandler(conf, context, cset, volumeSet, metrics);\n+        new KeyValueHandler(conf,\n+            context.getParent().getDatanodeDetails().getUuidString(),\n+            cset, volumeSet, metrics, c->{});\n       } catch (RuntimeException ex) {\n         GenericTestUtils.assertExceptionContains(\"class org.apache.hadoop\" +\n             \".ozone.container.common.impl.HddsDispatcher not org.apache\" +",
                "changes": 10,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java"
            },
            {
                "additions": 4,
                "sha": "26603b6d130db9da9d857ec8374af9f481f92009",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -24,9 +24,11 @@\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n+\n import org.junit.Assert;\n import org.junit.Test;\n import org.slf4j.Logger;\n@@ -41,7 +43,6 @@\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n-\n /**\n  * Test that KeyValueHandler fails certain operations when the\n  * container is unhealthy.\n@@ -147,10 +148,10 @@ private KeyValueHandler getDummyHandler() throws IOException {\n \n     return new KeyValueHandler(\n         new OzoneConfiguration(),\n-        context,\n+        stateMachine.getDatanodeDetails().getUuidString(),\n         mock(ContainerSet.class),\n         mock(VolumeSet.class),\n-        mock(ContainerMetrics.class));\n+        mock(ContainerMetrics.class), TestHddsDispatcher.NO_OP_ICR_SENDER);\n   }\n \n   private KeyValueContainer getMockUnhealthyContainer() {",
                "changes": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java"
            },
            {
                "additions": 7,
                "sha": "0598f15e63f7afed848a26031d19304ff66b05fb",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java",
                "deletions": 2,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -22,6 +22,7 @@\n import static org.apache.hadoop.test.MetricsAsserts.getMetrics;\n \n import com.google.common.collect.Maps;\n+\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.hdds.client.BlockID;\n import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n@@ -40,6 +41,7 @@\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n@@ -51,6 +53,7 @@\n import org.apache.hadoop.ozone.container.replication.GrpcReplicationService;\n import org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource;\n import org.apache.hadoop.test.GenericTestUtils;\n+\n import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n@@ -104,8 +107,10 @@ public void testContainerMetrics() throws Exception {\n       for (ContainerProtos.ContainerType containerType :\n           ContainerProtos.ContainerType.values()) {\n         handlers.put(containerType,\n-            Handler.getHandlerForContainerType(containerType, conf, context,\n-                containerSet, volumeSet, metrics));\n+            Handler.getHandlerForContainerType(containerType, conf,\n+                context.getParent().getDatanodeDetails().getUuidString(),\n+                containerSet, volumeSet, metrics,\n+                TestHddsDispatcher.NO_OP_ICR_SENDER));\n       }\n       HddsDispatcher dispatcher = new HddsDispatcher(conf, containerSet,\n           volumeSet, handlers, context, metrics, null);",
                "changes": 9,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java"
            },
            {
                "additions": 5,
                "sha": "e3827347dd6874d682a9db4de99cfc9dd1f6258c",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java",
                "deletions": 2,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n@@ -224,8 +225,10 @@ public void testClientServerWithContainerDispatcher() throws Exception {\n       for (ContainerProtos.ContainerType containerType :\n           ContainerProtos.ContainerType.values()) {\n         handlers.put(containerType,\n-            Handler.getHandlerForContainerType(containerType, conf, context,\n-                containerSet, volumeSet, metrics));\n+            Handler.getHandlerForContainerType(containerType, conf,\n+                context.getParent().getDatanodeDetails().getUuidString(),\n+                containerSet, volumeSet, metrics,\n+                TestHddsDispatcher.NO_OP_ICR_SENDER));\n       }\n       HddsDispatcher dispatcher = new HddsDispatcher(\n           conf, containerSet, volumeSet, handlers, context, metrics, null);",
                "changes": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java"
            },
            {
                "additions": 5,
                "sha": "33f93e5288e74fe4b79086e671ca875c8243b75a",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java",
                "deletions": 2,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n@@ -155,8 +156,10 @@ private static HddsDispatcher createDispatcher(DatanodeDetails dd, UUID scmId,\n     for (ContainerProtos.ContainerType containerType :\n         ContainerProtos.ContainerType.values()) {\n       handlers.put(containerType,\n-          Handler.getHandlerForContainerType(containerType, conf, context,\n-              containerSet, volumeSet, metrics));\n+          Handler.getHandlerForContainerType(containerType, conf,\n+              dd.getUuid().toString(),\n+              containerSet, volumeSet, metrics,\n+              TestHddsDispatcher.NO_OP_ICR_SENDER));\n     }\n     HddsDispatcher hddsDispatcher = new HddsDispatcher(\n         conf, containerSet, volumeSet, handlers, context, metrics,",
                "changes": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java"
            },
            {
                "additions": 44,
                "sha": "1d087f93d78c9a5e61660c73fcc5722e269daad3",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java",
                "deletions": 38,
                "filename": "hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -17,54 +17,49 @@\n  */\n package org.apache.hadoop.ozone.genesis;\n \n-import com.google.common.collect.Maps;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hdds.HddsUtils;\n+import org.apache.hadoop.hdds.client.BlockID;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumData;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumType;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChunkInfo;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.GetBlockRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.PutBlockRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ReadChunkRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.WriteChunkRequestProto;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n-import org.apache.hadoop.ozone.container.common.statemachine\n-    .DatanodeStateMachine.DatanodeStates;\n+import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.DatanodeStates;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n-import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n+\n+import com.google.common.collect.Maps;\n import org.apache.commons.codec.digest.DigestUtils;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.RandomStringUtils;\n import org.apache.commons.lang3.RandomUtils;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdds.client.BlockID;\n-import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n-\n+import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n import org.openjdk.jmh.annotations.Benchmark;\n import org.openjdk.jmh.annotations.Level;\n import org.openjdk.jmh.annotations.Scope;\n import org.openjdk.jmh.annotations.Setup;\n import org.openjdk.jmh.annotations.State;\n import org.openjdk.jmh.annotations.TearDown;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.UUID;\n-import java.util.concurrent.atomic.AtomicInteger;\n-\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .PutBlockRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .GetBlockRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerCommandRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ReadChunkRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .WriteChunkRequestProto;\n-\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n-\n /**\n  * Benchmarks DatanodeDispatcher class.\n  */\n@@ -83,6 +78,7 @@\n   private static final int INIT_CONTAINERS = 100;\n   private static final int INIT_KEYS = 50;\n   private static final int INIT_CHUNKS = 100;\n+  public static final int CHUNK_SIZE = 1048576;\n \n   private List<Long> containers;\n   private List<Long> keys;\n@@ -94,15 +90,18 @@ public void initialize() throws IOException {\n     datanodeUuid = UUID.randomUUID().toString();\n \n     // 1 MB of data\n-    data = ByteString.copyFromUtf8(RandomStringUtils.randomAscii(1048576));\n-    random  = new Random();\n+    data = ByteString.copyFromUtf8(RandomStringUtils.randomAscii(CHUNK_SIZE));\n+    random = new Random();\n     Configuration conf = new OzoneConfiguration();\n     baseDir = System.getProperty(\"java.io.tmpdir\") + File.separator +\n         datanodeUuid;\n \n     // data directory\n     conf.set(\"dfs.datanode.data.dir\", baseDir + File.separator + \"data\");\n \n+    //We need 100 * container size minimum space\n+    conf.set(\"ozone.scm.container.size\", \"10MB\");\n+\n     ContainerSet containerSet = new ContainerSet();\n     volumeSet = new VolumeSet(datanodeUuid, conf);\n     StateContext context = new StateContext(\n@@ -111,9 +110,12 @@ public void initialize() throws IOException {\n     Map<ContainerProtos.ContainerType, Handler> handlers = Maps.newHashMap();\n     for (ContainerProtos.ContainerType containerType :\n         ContainerProtos.ContainerType.values()) {\n-      handlers.put(containerType,\n-          Handler.getHandlerForContainerType(\n-              containerType, conf, context, containerSet, volumeSet, metrics));\n+      Handler handler = Handler.getHandlerForContainerType(\n+          containerType, conf, \"datanodeid\",\n+          containerSet, volumeSet, metrics,\n+          c -> {});\n+      handler.setScmID(\"scm\");\n+      handlers.put(containerType, handler);\n     }\n     dispatcher = new HddsDispatcher(conf, containerSet, volumeSet, handlers,\n         context, metrics, null);\n@@ -217,11 +219,16 @@ private ContainerCommandRequestProto getReadChunkCommand(\n   private ContainerProtos.ChunkInfo getChunkInfo(\n       BlockID blockID, String chunkName) {\n     ContainerProtos.ChunkInfo.Builder builder =\n-        ContainerProtos.ChunkInfo.newBuilder()\n+        ChunkInfo.newBuilder()\n             .setChunkName(\n                 DigestUtils.md5Hex(chunkName)\n                     + \"_stream_\" + blockID.getContainerID() + \"_block_\"\n                     + blockID.getLocalID())\n+            .setChecksumData(\n+                ChecksumData.newBuilder()\n+                    .setBytesPerChecksum(4)\n+                    .setType(ChecksumType.CRC32)\n+                    .build())\n             .setOffset(0).setLen(data.size());\n     return builder.build();\n   }\n@@ -245,7 +252,7 @@ private ContainerCommandRequestProto getPutBlockCommand(\n   private ContainerCommandRequestProto getGetBlockCommand(BlockID blockID) {\n     GetBlockRequestProto.Builder readBlockRequest =\n         GetBlockRequestProto.newBuilder()\n-        .setBlockID(blockID.getDatanodeBlockIDProtobuf());\n+            .setBlockID(blockID.getDatanodeBlockIDProtobuf());\n     ContainerCommandRequestProto.Builder request = ContainerCommandRequestProto\n         .newBuilder()\n         .setCmdType(ContainerProtos.Type.GetBlock)\n@@ -274,7 +281,6 @@ public void createContainer(BenchMarkDatanodeDispatcher bmdd) {\n     bmdd.containerCount.getAndIncrement();\n   }\n \n-\n   @Benchmark\n   public void writeChunk(BenchMarkDatanodeDispatcher bmdd) {\n     bmdd.dispatcher.dispatch(getWriteChunkCommand(",
                "changes": 82,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java"
            },
            {
                "additions": 2,
                "sha": "436166645c3d188a389b1211ee528b8c1873eab3",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java",
                "deletions": 1,
                "filename": "hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -41,7 +41,8 @@\n \n   // After adding benchmark in genesis package add the benchmark name in the\n   // description for this option.\n-  @Option(names = \"-benchmark\", split = \",\", description =\n+  @Option(names = {\"-b\", \"-benchmark\", \"--benchmark\"},\n+      split = \",\", description =\n       \"Option used for specifying benchmarks to run.\\n\"\n           + \"Ex. ozone genesis -benchmark BenchMarkContainerStateMap,\"\n           + \"BenchMarkOMKeyAllocation.\\n\"",
                "changes": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2325. BenchMarkDatanodeDispatcher genesis test is failing with NPE\n\nCloses #60",
        "commit": "https://github.com/apache/hadoop-ozone/commit/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/2be51cf94b0666bbce8fed32cd83a3d59fa468cd",
        "bug_id": "hadoop-ozone_8",
        "file": [
            {
                "additions": 4,
                "sha": "7e03095cdc45cd7bfa7df4fd2a1e047b38f70adb",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java?ref=67100ff6cf1becdfc51a4bf75907e97acf12a635",
                "patch": "@@ -84,13 +84,16 @@\n    * milliseconds\n    * @param dtRemoverScanInterval how often the tokens are scanned for expired\n    * tokens in milliseconds\n+   * @param certClient certificate client to SCM CA\n    */\n   public OzoneDelegationTokenSecretManager(OzoneConfiguration conf,\n       long tokenMaxLifetime, long tokenRenewInterval,\n       long dtRemoverScanInterval, Text service,\n-      S3SecretManager s3SecretManager) throws IOException {\n+      S3SecretManager s3SecretManager, CertificateClient certClient)\n+      throws IOException {\n     super(new SecurityConfig(conf), tokenMaxLifetime, tokenRenewInterval,\n         service, LOG);\n+    setCertClient(certClient);\n     currentTokens = new ConcurrentHashMap();\n     this.tokenRemoverScanInterval = dtRemoverScanInterval;\n     this.s3SecretManager = (S3SecretManagerImpl) s3SecretManager;",
                "changes": 5,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java"
            },
            {
                "additions": 6,
                "sha": "78f0565b81dc7678b74152d7933a671bf16c8d06",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java?ref=67100ff6cf1becdfc51a4bf75907e97acf12a635",
                "patch": "@@ -70,6 +70,7 @@\n    * @param tokenRenewInterval how often the tokens must be renewed in\n    * milliseconds\n    * @param service name of service\n+   * @param logger logger for the secret manager\n    */\n   public OzoneSecretManager(SecurityConfig secureConf, long tokenMaxLifetime,\n       long tokenRenewInterval, Text service, Logger logger) {\n@@ -188,7 +189,7 @@ public String formatTokenId(T id) {\n   public synchronized void start(CertificateClient client)\n       throws IOException {\n     Preconditions.checkState(!isRunning());\n-    this.certClient = client;\n+    setCertClient(client);\n     updateCurrentKey(new KeyPair(certClient.getPublicKey(),\n         certClient.getPrivateKey()));\n     setIsRunning(true);\n@@ -247,5 +248,9 @@ public AtomicInteger getTokenSequenceNumber() {\n   public CertificateClient getCertClient() {\n     return certClient;\n   }\n+\n+  public void setCertClient(CertificateClient client) {\n+    this.certClient = client;\n+  }\n }\n ",
                "changes": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java"
            },
            {
                "additions": 1,
                "sha": "a6503d73140a301adc9383d2a4e818c532cf9b8f",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "deletions": 1,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java?ref=67100ff6cf1becdfc51a4bf75907e97acf12a635",
                "patch": "@@ -627,7 +627,7 @@ private OzoneDelegationTokenSecretManager createDelegationTokenSecretManager(\n \n     return new OzoneDelegationTokenSecretManager(conf, tokenMaxLifetime,\n         tokenRenewInterval, tokenRemoverScanInterval, omRpcAddressTxt,\n-        s3SecretManager);\n+        s3SecretManager, certClient);\n   }\n \n   private OzoneBlockTokenSecretManager createBlockTokenSecretManager(",
                "changes": 2,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java"
            },
            {
                "additions": 26,
                "sha": "874252d171faf53308c1a746a158e186142bad95",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java",
                "deletions": 3,
                "filename": "hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java?ref=67100ff6cf1becdfc51a4bf75907e97acf12a635",
                "patch": "@@ -169,19 +169,41 @@ public void testCreateToken() throws Exception {\n     validateHash(token.getPassword(), token.getIdentifier());\n   }\n \n-  @Test\n-  public void testRenewTokenSuccess() throws Exception {\n+  private void restartSecretManager() throws IOException {\n+    secretManager.stop();\n+    secretManager = null;\n+    secretManager = createSecretManager(conf, tokenMaxLifetime,\n+        expiryTime, tokenRemoverScanInterval);\n+  }\n+\n+  private void testRenewTokenSuccessHelper(boolean restartSecretManager)\n+      throws Exception {\n     secretManager = createSecretManager(conf, tokenMaxLifetime,\n         expiryTime, tokenRemoverScanInterval);\n     secretManager.start(certificateClient);\n     Token<OzoneTokenIdentifier> token = secretManager.createToken(TEST_USER,\n         TEST_USER,\n         TEST_USER);\n     Thread.sleep(10 * 5);\n+\n+    if (restartSecretManager) {\n+      restartSecretManager();\n+    }\n+\n     long renewalTime = secretManager.renewToken(token, TEST_USER.toString());\n     Assert.assertTrue(renewalTime > 0);\n   }\n \n+  @Test\n+  public void testReloadAndRenewToken() throws Exception {\n+    testRenewTokenSuccessHelper(true);\n+  }\n+\n+  @Test\n+  public void testRenewTokenSuccess() throws Exception {\n+    testRenewTokenSuccessHelper(false);\n+  }\n+\n   /**\n    * Tests failure for mismatch in renewer.\n    */\n@@ -375,6 +397,7 @@ private void validateHash(byte[] hash, byte[] identifier) throws Exception {\n       createSecretManager(OzoneConfiguration config, long tokenMaxLife,\n       long expiry, long tokenRemoverScanTime) throws IOException {\n     return new OzoneDelegationTokenSecretManager(config, tokenMaxLife,\n-        expiry, tokenRemoverScanTime, serviceRpcAdd, s3SecretManager);\n+        expiry, tokenRemoverScanTime, serviceRpcAdd, s3SecretManager,\n+        certificateClient);\n   }\n }\n\\ No newline at end of file",
                "changes": 29,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/67100ff6cf1becdfc51a4bf75907e97acf12a635/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2228. Fix NPE in OzoneDelegationTokenManager#addPersistedDelegat\u2026 (#1571)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/67100ff6cf1becdfc51a4bf75907e97acf12a635"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/693440200895ee451dfe3fc724a0312679d71c19",
        "bug_id": "hadoop-ozone_9",
        "file": [
            {
                "additions": 0,
                "sha": "354c9075e3e97c7934cf95e227271eb3ead3a151",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b6461c37eb27032440182ab4e990a94f458d827c/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java",
                "deletions": 1,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java?ref=b6461c37eb27032440182ab4e990a94f458d827c",
                "patch": "@@ -193,7 +193,6 @@ private KeyManagerImpl(OzoneManager om, ScmClient scmClient,\n     this.secretManager = secretManager;\n     this.kmsProvider = kmsProvider;\n \n-    start(conf);\n   }\n \n   @Override",
                "changes": 1,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b6461c37eb27032440182ab4e990a94f458d827c/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java"
            },
            {
                "additions": 3,
                "sha": "3c707ba1e18bde091d3a4f900eaebfd71a6cac9f",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b6461c37eb27032440182ab4e990a94f458d827c/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java",
                "deletions": 0,
                "filename": "hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java?ref=b6461c37eb27032440182ab4e990a94f458d827c",
                "patch": "@@ -95,6 +95,7 @@ public void checkIfDeleteServiceisDeletingKeys()\n         new KeyManagerImpl(\n             new ScmBlockLocationTestingClient(null, null, 0),\n             metaMgr, conf, UUID.randomUUID().toString(), null);\n+    keyManager.start(conf);\n     final int keyCount = 100;\n     createAndDeleteKeys(keyManager, keyCount, 1);\n     KeyDeletingService keyDeletingService =\n@@ -117,6 +118,7 @@ public void checkIfDeleteServiceWithFailingSCM()\n         new KeyManagerImpl(\n             new ScmBlockLocationTestingClient(null, null, 1),\n             metaMgr, conf, UUID.randomUUID().toString(), null);\n+    keyManager.start(conf);\n     final int keyCount = 100;\n     createAndDeleteKeys(keyManager, keyCount, 1);\n     KeyDeletingService keyDeletingService =\n@@ -144,6 +146,7 @@ public void checkDeletionForEmptyKey()\n         new KeyManagerImpl(\n             new ScmBlockLocationTestingClient(null, null, 1),\n             metaMgr, conf, UUID.randomUUID().toString(), null);\n+    keyManager.start(conf);\n     final int keyCount = 100;\n     createAndDeleteKeys(keyManager, keyCount, 0);\n     KeyDeletingService keyDeletingService =",
                "changes": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b6461c37eb27032440182ab4e990a94f458d827c/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2237. KeyDeletingService throws NPE if it's started too early (#1584)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/b6461c37eb27032440182ab4e990a94f458d827c"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/b0859b49c4a196bbdc16605f38ff51a59968b718",
        "bug_id": "hadoop-ozone_10",
        "file": [
            {
                "additions": 2,
                "sha": "be88b43df4dbb3ff3a4e6af13673965cfebc10af",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/27871092359d4f1689f4513bdc20c69c9bd6db3b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMGetDelegationTokenRequest.java",
                "deletions": 2,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMGetDelegationTokenRequest.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMGetDelegationTokenRequest.java?ref=27871092359d4f1689f4513bdc20c69c9bd6db3b",
                "patch": "@@ -147,8 +147,8 @@ public OMClientResponse validateAndUpdateCache(OzoneManager ozoneManager,\n     OMMetadataManager omMetadataManager = ozoneManager.getMetadataManager();\n \n     try {\n-      OzoneTokenIdentifier ozoneTokenIdentifier =\n-          ozoneTokenIdentifierToken.decodeIdentifier();\n+      OzoneTokenIdentifier ozoneTokenIdentifier = OzoneTokenIdentifier.\n+          readProtoBuf(ozoneTokenIdentifierToken.getIdentifier());\n \n       // Update in memory map of token.\n       long renewTime = ozoneManager.getDelegationTokenMgr()",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/27871092359d4f1689f4513bdc20c69c9bd6db3b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMGetDelegationTokenRequest.java"
            },
            {
                "additions": 2,
                "sha": "11c0c82b5283546a877c05d46110c4de7766d464",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/27871092359d4f1689f4513bdc20c69c9bd6db3b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMRenewDelegationTokenRequest.java",
                "deletions": 2,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMRenewDelegationTokenRequest.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMRenewDelegationTokenRequest.java?ref=27871092359d4f1689f4513bdc20c69c9bd6db3b",
                "patch": "@@ -123,8 +123,8 @@ public OMClientResponse validateAndUpdateCache(OzoneManager ozoneManager,\n             .setSuccess(true);\n     try {\n \n-      OzoneTokenIdentifier ozoneTokenIdentifier =\n-          ozoneTokenIdentifierToken.decodeIdentifier();\n+      OzoneTokenIdentifier ozoneTokenIdentifier = OzoneTokenIdentifier.\n+          readProtoBuf(ozoneTokenIdentifierToken.getIdentifier());\n \n       // Update in memory map of token.\n       ozoneManager.getDelegationTokenMgr()",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/27871092359d4f1689f4513bdc20c69c9bd6db3b/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/request/security/OMRenewDelegationTokenRequest.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-2078. Get/Renew DelegationToken NPE after HDDS-1909\n\nCloses #1444",
        "commit": "https://github.com/apache/hadoop-ozone/commit/27871092359d4f1689f4513bdc20c69c9bd6db3b"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/1a71d601d7c2ca4da6ff8e5aa68aa97dedcf42a1",
        "bug_id": "hadoop-ozone_11",
        "file": [
            {
                "additions": 5,
                "sha": "a8dff405e4386a30cbcaf1d3b94dd1b3977b8fb2",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "deletions": 2,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java?ref=a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5",
                "patch": "@@ -486,8 +486,11 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n-        inflightReplication.get(id).stream().map(r -> r.datanode)\n-            .forEach(excludeList::add);\n+        List<InflightAction> actionList = inflightReplication.get(id);\n+        if (actionList != null) {\n+          actionList.stream().map(r -> r.datanode)\n+              .forEach(excludeList::add);\n+        }\n         final List<DatanodeDetails> selectedDatanodes = containerPlacement\n             .chooseDatanodes(excludeList, null, delta,\n                 container.getUsedBytes());",
                "changes": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1882. TestReplicationManager failed with NPE. (#1197)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/d20288628908d21d629c568fc03e5d9b7f1d8281",
        "bug_id": "hadoop-ozone_12",
        "file": [
            {
                "additions": 3,
                "sha": "77cdd83f7938e36c958dc368ee4ebdd627e29602",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/e4e248004e7c091cae835a2ec0b4355c49786443/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java?ref=e4e248004e7c091cae835a2ec0b4355c49786443",
                "patch": "@@ -109,7 +109,9 @@ public Configuration getConf() {\n       int nodesRequired, final long sizeRequired) throws SCMException {\n     List<DatanodeDetails> healthyNodes =\n         nodeManager.getNodes(HddsProtos.NodeState.HEALTHY);\n-    healthyNodes.removeAll(excludedNodes);\n+    if (excludedNodes != null) {\n+      healthyNodes.removeAll(excludedNodes);\n+    }\n     String msg;\n     if (healthyNodes.size() == 0) {\n       msg = \"No healthy node found to allocate container.\";",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/e4e248004e7c091cae835a2ec0b4355c49786443/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java"
            },
            {
                "additions": 17,
                "sha": "456b44bbc549502fefcbd5d20ca3faa1cdae367b",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/e4e248004e7c091cae835a2ec0b4355c49786443/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java",
                "deletions": 8,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java?ref=e4e248004e7c091cae835a2ec0b4355c49786443",
                "patch": "@@ -63,6 +63,13 @@\n   public void setup() {\n     //initialize network topology instance\n     conf = new OzoneConfiguration();\n+  }\n+\n+  @Test\n+  public void testRackAwarePolicy() throws IOException {\n+    conf.set(ScmConfigKeys.OZONE_SCM_CONTAINER_PLACEMENT_IMPL_KEY,\n+        SCMContainerPlacementRackAware.class.getName());\n+\n     NodeSchema[] schemas = new NodeSchema[]\n         {ROOT_SCHEMA, RACK_SCHEMA, LEAF_SCHEMA};\n     NodeSchemaManager.getInstance().init(schemas, true);\n@@ -91,11 +98,7 @@ public void setup() {\n         .thenReturn(new SCMNodeMetric(storageCapacity, 80L, 20L));\n     when(nodeManager.getNodeStat(datanodes.get(4)))\n         .thenReturn(new SCMNodeMetric(storageCapacity, 70L, 30L));\n-  }\n-\n \n-  @Test\n-  public void testDefaultPolicy() throws IOException {\n     ContainerPlacementPolicy policy = ContainerPlacementPolicyFactory\n         .getPolicy(conf, nodeManager, cluster, true);\n \n@@ -111,14 +114,21 @@ public void testDefaultPolicy() throws IOException {\n         datanodeDetails.get(2)));\n   }\n \n+  @Test\n+  public void testDefaultPolicy() throws IOException {\n+    ContainerPlacementPolicy policy = ContainerPlacementPolicyFactory\n+        .getPolicy(conf, null, null, true);\n+    Assert.assertSame(SCMContainerPlacementRandom.class, policy.getClass());\n+  }\n+\n   /**\n    * A dummy container placement implementation for test.\n    */\n-  public class DummyImpl implements ContainerPlacementPolicy {\n+  public static class DummyImpl implements ContainerPlacementPolicy {\n     @Override\n     public List<DatanodeDetails> chooseDatanodes(\n         List<DatanodeDetails> excludedNodes, List<DatanodeDetails> favoredNodes,\n-        int nodesRequired, long sizeRequired) throws IOException {\n+        int nodesRequired, long sizeRequired) {\n       return null;\n     }\n   }\n@@ -127,8 +137,7 @@ public void testDefaultPolicy() throws IOException {\n   public void testConstuctorNotFound() throws SCMException {\n     // set a placement class which does't have the right constructor implemented\n     conf.set(ScmConfigKeys.OZONE_SCM_CONTAINER_PLACEMENT_IMPL_KEY,\n-        \"org.apache.hadoop.hdds.scm.container.placement.algorithms.\" +\n-            \"TestContainerPlacementFactory$DummyImpl\");\n+        DummyImpl.class.getName());\n     ContainerPlacementPolicyFactory.getPolicy(conf, null, null, true);\n   }\n ",
                "changes": 25,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/e4e248004e7c091cae835a2ec0b4355c49786443/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1822. NPE in SCMCommonPolicy.chooseDatanodes (#1120)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/e4e248004e7c091cae835a2ec0b4355c49786443"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/4ba5aa488dde255cabc1da572329c207bf7f27fe",
        "bug_id": "hadoop-ozone_13",
        "file": [
            {
                "additions": 3,
                "sha": "523e63ff91ffb37c019bff52d02936b4b8060bbe",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/7f9d9aee1bca5984a5270102463b182d017a246a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerController.java",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerController.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerController.java?ref=7f9d9aee1bca5984a5270102463b182d017a246a",
                "patch": "@@ -137,7 +137,9 @@ public Container importContainer(final ContainerType type,\n   public void deleteContainer(final long containerId, boolean force)\n       throws IOException {\n     final Container container = containerSet.getContainer(containerId);\n-    getHandler(container).deleteContainer(container, force);\n+    if (container != null) {\n+      getHandler(container).deleteContainer(container, force);\n+    }\n   }\n \n   /**",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/7f9d9aee1bca5984a5270102463b182d017a246a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/ContainerController.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1756. DeleteContainerCommandHandler fails with NPE. Contributed by Nanda kumar(#1095).",
        "commit": "https://github.com/apache/hadoop-ozone/commit/7f9d9aee1bca5984a5270102463b182d017a246a"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/dc7c21c952693b85326b32229032c8ef85a6a4a8",
        "bug_id": "hadoop-ozone_14",
        "file": [
            {
                "additions": 10,
                "sha": "eaa2255cb0db360b4ef57b1aa0d1a8576b3a4ebc",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "deletions": 13,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java?ref=aa26930092486c66ccdbdc3730bef6f5146fbdf6",
                "patch": "@@ -30,7 +30,7 @@\n import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n import org.apache.hadoop.hdds.scm.node.states.NodeAlreadyExistsException;\n import org.apache.hadoop.hdds.scm.node.states.NodeNotFoundException;\n-import org.apache.hadoop.hdds.scm.server.StorageContainerManager;\n+import org.apache.hadoop.hdds.scm.server.SCMStorageConfig;\n import org.apache.hadoop.hdds.scm.VersionInfo;\n import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMNodeMetric;\n import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMNodeStat;\n@@ -94,32 +94,30 @@\n       LoggerFactory.getLogger(SCMNodeManager.class);\n \n   private final NodeStateManager nodeStateManager;\n-  private final String clusterID;\n   private final VersionInfo version;\n   private final CommandQueue commandQueue;\n   private final SCMNodeMetrics metrics;\n   // Node manager MXBean\n   private ObjectName nmInfoBean;\n-  private final StorageContainerManager scmManager;\n+  private final SCMStorageConfig scmStorageConfig;\n   private final NetworkTopology clusterMap;\n   private final DNSToSwitchMapping dnsToSwitchMapping;\n   private final boolean useHostname;\n \n   /**\n    * Constructs SCM machine Manager.\n    */\n-  public SCMNodeManager(OzoneConfiguration conf, String clusterID,\n-      StorageContainerManager scmManager, EventPublisher eventPublisher)\n-      throws IOException {\n+  public SCMNodeManager(OzoneConfiguration conf,\n+      SCMStorageConfig scmStorageConfig, EventPublisher eventPublisher,\n+      NetworkTopology networkTopology) {\n     this.nodeStateManager = new NodeStateManager(conf, eventPublisher);\n-    this.clusterID = clusterID;\n     this.version = VersionInfo.getLatestVersion();\n     this.commandQueue = new CommandQueue();\n-    this.scmManager = scmManager;\n+    this.scmStorageConfig = scmStorageConfig;\n     LOG.info(\"Entering startup safe mode.\");\n     registerMXBean();\n     this.metrics = SCMNodeMetrics.create(this);\n-    this.clusterMap = scmManager.getClusterMap();\n+    this.clusterMap = networkTopology;\n     Class<? extends DNSToSwitchMapping> dnsToSwitchMappingClass =\n         conf.getClass(DFSConfigKeys.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n             TableMapping.class, DNSToSwitchMapping.class);\n@@ -221,9 +219,8 @@ public VersionResponse getVersion(SCMVersionRequestProto versionRequest) {\n     return VersionResponse.newBuilder()\n         .setVersion(this.version.getVersion())\n         .addValue(OzoneConsts.SCM_ID,\n-            this.scmManager.getScmStorageConfig().getScmId())\n-        .addValue(OzoneConsts.CLUSTER_ID, this.scmManager.getScmStorageConfig()\n-            .getClusterID())\n+            this.scmStorageConfig.getScmId())\n+        .addValue(OzoneConsts.CLUSTER_ID, this.scmStorageConfig.getClusterID())\n         .build();\n   }\n \n@@ -274,7 +271,7 @@ public RegisteredCommand register(\n \n     return RegisteredCommand.newBuilder().setErrorCode(ErrorCode.success)\n         .setDatanodeUUID(datanodeDetails.getUuidString())\n-        .setClusterID(this.clusterID)\n+        .setClusterID(this.scmStorageConfig.getClusterID())\n         .setHostname(datanodeDetails.getHostName())\n         .setIpAddress(datanodeDetails.getIpAddress())\n         .build();",
                "changes": 23,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java"
            },
            {
                "additions": 1,
                "sha": "08712ccdbc5b1f13437910865ee591a6df119a20",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java?ref=aa26930092486c66ccdbdc3730bef6f5146fbdf6",
                "patch": "@@ -378,7 +378,7 @@ private void initializeSystemManagers(OzoneConfiguration conf,\n       scmNodeManager = configurator.getScmNodeManager();\n     } else {\n       scmNodeManager = new SCMNodeManager(\n-          conf, scmStorageConfig.getClusterID(), this, eventQueue);\n+          conf, scmStorageConfig, eventQueue, clusterMap);\n     }\n \n     ContainerPlacementPolicy containerPlacementPolicy =",
                "changes": 2,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java"
            },
            {
                "additions": 6,
                "sha": "ec0c4c3447042e9211be2ed1d28d21c4ff01f623",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java",
                "deletions": 2,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java?ref=aa26930092486c66ccdbdc3730bef6f5146fbdf6",
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.hadoop.hdds.scm.events.SCMEvents;\n import org.apache.hadoop.hdds.scm.pipeline.PipelineManager;\n import org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager;\n+import org.apache.hadoop.hdds.scm.server.SCMStorageConfig;\n import org.apache.hadoop.hdds.server.events.EventQueue;\n import org.apache.hadoop.ozone.OzoneConsts;\n import org.apache.hadoop.test.PathUtils;\n@@ -48,7 +49,6 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.List;\n-import java.util.UUID;\n import java.util.concurrent.TimeoutException;\n \n import static org.apache.hadoop.hdds.scm.ScmConfigKeys\n@@ -94,8 +94,12 @@ SCMNodeManager createNodeManager(OzoneConfiguration config)\n         Mockito.mock(StaleNodeHandler.class));\n     eventQueue.addHandler(SCMEvents.DEAD_NODE,\n         Mockito.mock(DeadNodeHandler.class));\n+\n+    SCMStorageConfig storageConfig = Mockito.mock(SCMStorageConfig.class);\n+    Mockito.when(storageConfig.getClusterID()).thenReturn(\"cluster1\");\n+\n     SCMNodeManager nodeManager = new SCMNodeManager(config,\n-        UUID.randomUUID().toString(), null, eventQueue);\n+        storageConfig, eventQueue, null);\n     return nodeManager;\n   }\n ",
                "changes": 8,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java"
            },
            {
                "additions": 8,
                "sha": "88de27d996507e937a0d9d511d4bb889b57601eb",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java?ref=aa26930092486c66ccdbdc3730bef6f5146fbdf6",
                "patch": "@@ -24,14 +24,17 @@\n import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.StorageReportProto;\n import org.apache.hadoop.hdds.scm.TestUtils;\n import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMNodeMetric;\n+import org.apache.hadoop.hdds.scm.net.NetworkTopology;\n import org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher.NodeReportFromDatanode;\n+import org.apache.hadoop.hdds.scm.server.SCMStorageConfig;\n import org.apache.hadoop.hdds.server.events.Event;\n import org.apache.hadoop.hdds.server.events.EventPublisher;\n import org.apache.hadoop.hdds.server.events.EventQueue;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n+import org.mockito.Mockito;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -50,7 +53,11 @@\n   @Before\n   public void resetEventCollector() throws IOException {\n     OzoneConfiguration conf = new OzoneConfiguration();\n-    nodeManager = new SCMNodeManager(conf, \"cluster1\", null, new EventQueue());\n+    SCMStorageConfig storageConfig = Mockito.mock(SCMStorageConfig.class);\n+    Mockito.when(storageConfig.getClusterID()).thenReturn(\"cluster1\");\n+    nodeManager =\n+        new SCMNodeManager(conf, storageConfig, new EventQueue(), Mockito.mock(\n+            NetworkTopology.class));\n     nodeReportHandler = new NodeReportHandler(nodeManager);\n   }\n ",
                "changes": 9,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/aa26930092486c66ccdbdc3730bef6f5146fbdf6/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1694. TestNodeReportHandler is failing with NPE\n\nCloses #978",
        "commit": "https://github.com/apache/hadoop-ozone/commit/aa26930092486c66ccdbdc3730bef6f5146fbdf6"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/4bfdf05a67644266bbfcfb3152bc5227d4d1a904",
        "bug_id": "hadoop-ozone_15",
        "file": [
            {
                "additions": 15,
                "sha": "e4e9d35bdf89c9d44273ec4f26f3f5aea08cdd37",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "deletions": 3,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java?ref=b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.hdds.protocol.proto\n         .StorageContainerDatanodeProtocolProtos.PipelineReportsProto;\n import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.net.InnerNode;\n import org.apache.hadoop.hdds.scm.net.NetConstants;\n import org.apache.hadoop.hdds.scm.net.NetworkTopology;\n import org.apache.hadoop.hdds.scm.net.Node;\n@@ -566,9 +567,20 @@ public DatanodeDetails getNode(String address) {\n       node = clusterMap.getNode(location + NetConstants.PATH_SEPARATOR_STR +\n           address);\n     }\n-    LOG.debug(\"Get node for {} return {}\", address, (node == null ?\n-        \"not found\" : node.getNetworkFullPath()));\n-    return node == null ? null : (DatanodeDetails)node;\n+\n+    if (node != null) {\n+      if (node instanceof InnerNode) {\n+        LOG.warn(\"Get node for {} return {}, it's an inner node, \" +\n+            \"not a datanode\", address, node.getNetworkFullPath());\n+      } else {\n+        LOG.debug(\"Get node for {} return {}\", address,\n+            node.getNetworkFullPath());\n+        return (DatanodeDetails)node;\n+      }\n+    } else {\n+      LOG.warn(\"Cannot find node for {}\", address);\n+    }\n+    return null;\n   }\n \n   private String nodeResolve(String hostname) {",
                "changes": 18,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java"
            },
            {
                "additions": 6,
                "sha": "1c76070d3bdb44656d4b7e99f2bc0abcb9c21b87",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java?ref=b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61",
                "patch": "@@ -290,7 +290,12 @@ public ScmInfo getScmInfo() throws IOException {\n       NodeManager nodeManager = scm.getScmNodeManager();\n       Node client = nodeManager.getNode(clientMachine);\n       List<Node> nodeList = new ArrayList();\n-      nodes.stream().forEach(path -> nodeList.add(nodeManager.getNode(path)));\n+      nodes.stream().forEach(path -> {\n+        DatanodeDetails node = nodeManager.getNode(path);\n+        if (node != null) {\n+          nodeList.add(nodeManager.getNode(path));\n+        }\n+      });\n       List<? extends Node> sortedNodeList = scm.getClusterMap()\n           .sortByDistanceCost(client, nodeList, nodes.size());\n       List<DatanodeDetails> ret = new ArrayList<>();",
                "changes": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java"
            },
            {
                "additions": 153,
                "sha": "ba78f4c9df2a33b80e937482df6bd37746dc8088",
                "status": "added",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java",
                "deletions": 0,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java?ref=b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61",
                "patch": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.server;\n+\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos;\n+import org.apache.hadoop.hdds.scm.TestUtils;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.ozone.protocolPB\n+    .ScmBlockLocationProtocolServerSideTranslatorPB;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Test class for @{@link SCMBlockProtocolServer}.\n+ * */\n+public class TestSCMBlockProtocolServer {\n+  private OzoneConfiguration config;\n+  private SCMBlockProtocolServer server;\n+  private StorageContainerManager scm;\n+  private NodeManager nodeManager;\n+  private ScmBlockLocationProtocolServerSideTranslatorPB service;\n+  private final int nodeCount = 10;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    config = new OzoneConfiguration();\n+    File dir = GenericTestUtils.getRandomizedTestDir();\n+    config.set(HddsConfigKeys.OZONE_METADATA_DIRS, dir.toString());\n+    SCMConfigurator configurator = new SCMConfigurator();\n+    scm = TestUtils.getScm(config, configurator);\n+    scm.start();\n+    scm.exitSafeMode();\n+    // add nodes to scm node manager\n+    nodeManager = scm.getScmNodeManager();\n+    for (int i = 0; i < nodeCount; i++) {\n+      nodeManager.register(TestUtils.randomDatanodeDetails(), null, null);\n+\n+    }\n+    server = scm.getBlockProtocolServer();\n+    service = new ScmBlockLocationProtocolServerSideTranslatorPB(server);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (scm != null) {\n+      scm.stop();\n+      scm.join();\n+    }\n+  }\n+\n+  @Test\n+  public void testSortDatanodes() throws Exception {\n+    List<String> nodes = new ArrayList();\n+    nodeManager.getAllNodes().stream().forEach(\n+        node -> nodes.add(node.getNetworkName()));\n+\n+    // sort normal datanodes\n+    String client;\n+    client = nodes.get(0);\n+    List<DatanodeDetails> datanodeDetails =\n+        server.sortDatanodes(nodes, client);\n+    System.out.println(\"client = \" + client);\n+    datanodeDetails.stream().forEach(\n+        node -> System.out.println(node.toString()));\n+    Assert.assertTrue(datanodeDetails.size() == nodeCount);\n+\n+    // illegal client 1\n+    client += \"X\";\n+    datanodeDetails = server.sortDatanodes(nodes, client);\n+    System.out.println(\"client = \" + client);\n+    datanodeDetails.stream().forEach(\n+        node -> System.out.println(node.toString()));\n+    Assert.assertTrue(datanodeDetails.size() == nodeCount);\n+    // illegal client 2\n+    client = \"/default-rack\";\n+    datanodeDetails = server.sortDatanodes(nodes, client);\n+    System.out.println(\"client = \" + client);\n+    datanodeDetails.stream().forEach(\n+        node -> System.out.println(node.toString()));\n+    Assert.assertTrue(datanodeDetails.size() == nodeCount);\n+\n+    // illegal nodes to sort 1\n+    nodes.add(\"/default-rack\");\n+    ScmBlockLocationProtocolProtos.SortDatanodesRequestProto request =\n+        ScmBlockLocationProtocolProtos.SortDatanodesRequestProto\n+            .newBuilder()\n+            .addAllNodeNetworkName(nodes)\n+            .setClient(client)\n+            .build();\n+    ScmBlockLocationProtocolProtos.SortDatanodesResponseProto resp =\n+        service.sortDatanodes(request);\n+    Assert.assertTrue(resp.getNodeList().size() == nodeCount);\n+    System.out.println(\"client = \" + client);\n+    resp.getNodeList().stream().forEach(\n+        node -> System.out.println(node.getNetworkName()));\n+\n+    // illegal nodes to sort 2\n+    nodes.remove(\"/default-rack\");\n+    nodes.add(nodes.get(0) + \"X\");\n+    request = ScmBlockLocationProtocolProtos.SortDatanodesRequestProto\n+            .newBuilder()\n+            .addAllNodeNetworkName(nodes)\n+            .setClient(client)\n+            .build();\n+    resp = service.sortDatanodes(request);\n+    Assert.assertTrue(resp.getNodeList().size() == nodeCount);\n+    System.out.println(\"client = \" + client);\n+    resp.getNodeList().stream().forEach(\n+        node -> System.out.println(node.getNetworkName()));\n+\n+    // all illegal nodes\n+    nodes.clear();\n+    nodes.add(\"/default-rack\");\n+    nodes.add(\"/default-rack-1\");\n+    nodes.add(\"/default-rack-2\");\n+    request = ScmBlockLocationProtocolProtos.SortDatanodesRequestProto\n+        .newBuilder()\n+        .addAllNodeNetworkName(nodes)\n+        .setClient(client)\n+        .build();\n+    resp = service.sortDatanodes(request);\n+    System.out.println(\"client = \" + client);\n+    Assert.assertTrue(resp.getNodeList().size() == 0);\n+    resp.getNodeList().stream().forEach(\n+        node -> System.out.println(node.getNetworkName()));\n+  }\n+}\n\\ No newline at end of file",
                "changes": 153,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1787. NPE thrown while trying to find DN closest to client. Contributed by Sammi Chen. (#1094)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/b7d9d493485b39ee7159eb6a42fdd0bd9ebb3f61"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/8f1b68a18ef1eeb2ebd80a4b8bcfedcc78614fec",
        "bug_id": "hadoop-ozone_16",
        "file": [
            {
                "additions": 18,
                "sha": "7e064730063287fd063f2f253bed79ebd574215d",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/290d252e1f699c7c308a5660557026d6f7eeb992/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/StateContext.java",
                "deletions": 12,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/StateContext.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/StateContext.java?ref=290d252e1f699c7c308a5660557026d6f7eeb992",
                "patch": "@@ -348,20 +348,26 @@ public void execute(ExecutorService service, long time, TimeUnit unit)\n       throws InterruptedException, ExecutionException, TimeoutException {\n     stateExecutionCount.incrementAndGet();\n     DatanodeState<DatanodeStateMachine.DatanodeStates> task = getTask();\n-    if (this.isEntering()) {\n-      task.onEnter();\n-    }\n-    task.execute(service);\n-    DatanodeStateMachine.DatanodeStates newState = task.await(time, unit);\n-    if (this.state != newState) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Task {} executed, state transited from {} to {}\",\n-            task.getClass().getSimpleName(), this.state, newState);\n+\n+    // Adding not null check, in a case where datanode is still starting up, but\n+    // we called stop DatanodeStateMachine, this sets state to SHUTDOWN, and\n+    // there is a chance of getting task as null.\n+    if (task != null) {\n+      if (this.isEntering()) {\n+        task.onEnter();\n       }\n-      if (isExiting(newState)) {\n-        task.onExit();\n+      task.execute(service);\n+      DatanodeStateMachine.DatanodeStates newState = task.await(time, unit);\n+      if (this.state != newState) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Task {} executed, state transited from {} to {}\",\n+              task.getClass().getSimpleName(), this.state, newState);\n+        }\n+        if (isExiting(newState)) {\n+          task.onExit();\n+        }\n+        this.setState(newState);\n       }\n-      this.setState(newState);\n     }\n   }\n ",
                "changes": 30,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/290d252e1f699c7c308a5660557026d6f7eeb992/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/StateContext.java"
            },
            {
                "additions": 10,
                "sha": "6b596fe14f4c3ee7f6e1b74c4b5fa7b29c35d4cd",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/290d252e1f699c7c308a5660557026d6f7eeb992/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/datanode/RunningDatanodeState.java",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/datanode/RunningDatanodeState.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/datanode/RunningDatanodeState.java?ref=290d252e1f699c7c308a5660557026d6f7eeb992",
                "patch": "@@ -86,7 +86,16 @@ public void execute(ExecutorService executor) {\n     for (EndpointStateMachine endpoint : connectionManager.getValues()) {\n       Callable<EndpointStateMachine.EndPointStates> endpointTask\n           = getEndPointTask(endpoint);\n-      ecs.submit(endpointTask);\n+      if (endpointTask != null) {\n+        ecs.submit(endpointTask);\n+      } else {\n+        // This can happen if a task is taking more time than the timeOut\n+        // specified for the task in await, and when it is completed the task\n+        // has set the state to Shutdown, we may see the state as shutdown\n+        // here. So, we need to Shutdown DatanodeStateMachine.\n+        LOG.error(\"State is Shutdown in RunningDatanodeState\");\n+        context.setState(DatanodeStateMachine.DatanodeStates.SHUTDOWN);\n+      }\n     }\n   }\n   //TODO : Cache some of these tasks instead of creating them",
                "changes": 11,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/290d252e1f699c7c308a5660557026d6f7eeb992/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/datanode/RunningDatanodeState.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1370. Command Execution in Datanode fails because of NPE (#715)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/290d252e1f699c7c308a5660557026d6f7eeb992"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/2be22f4d8bc890c3afb7cf13cd1c46f78ef91ff7",
        "bug_id": "hadoop-ozone_17",
        "file": [
            {
                "additions": 3,
                "sha": "6be77709d4bc882163f3dbb53c2c46bc97fa3e4f",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "deletions": 0,
                "filename": "hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java?ref=b353ee9fdca93792415cd5d928d0f3677a25294f",
                "patch": "@@ -128,6 +128,9 @@ public static URI getKeyProviderUri(UserGroupInformation ugi,\n \n   public static KeyProvider getKeyProvider(final Configuration conf,\n       final URI serverProviderUri) throws IOException{\n+    if (serverProviderUri == null) {\n+      throw new IOException(\"KMS serverProviderUri is not configured.\");\n+    }\n     return KMSUtil.createKeyProviderFromUri(conf, serverProviderUri);\n   }\n ",
                "changes": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java"
            },
            {
                "additions": 51,
                "sha": "49fb5e335110f74f7204b33121110bb3b4fa88f7",
                "status": "added",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "deletions": 0,
                "filename": "hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java?ref=b353ee9fdca93792415cd5d928d0f3677a25294f",
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.client.rpc;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Test class for {@link OzoneKMSUtil}.\n+ * */\n+public class TestOzoneKMSUtil {\n+  private OzoneConfiguration config;\n+\n+  @Before\n+  public void setUp() {\n+    config = new OzoneConfiguration();\n+    config.setBoolean(OzoneConfigKeys.OZONE_SECURITY_ENABLED_KEY, true);\n+  }\n+\n+  @Test\n+  public void getKeyProvider() {\n+    try {\n+      OzoneKMSUtil.getKeyProvider(config, null);\n+      fail(\"Expected IOException.\");\n+    } catch (IOException ioe) {\n+      assertEquals(ioe.getMessage(), \"KMS serverProviderUri is \" +\n+          \"not configured.\");\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "changes": 51,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java"
            },
            {
                "additions": 7,
                "sha": "983c5a9d46568c4a2fdd43741224339c019af16f",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "deletions": 1,
                "filename": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java?ref=b353ee9fdca93792415cd5d928d0f3677a25294f",
                "patch": "@@ -59,7 +59,13 @@ public URI getKeyProviderUri() throws IOException {\n   @Override\n   public DelegationTokenIssuer[] getAdditionalTokenIssuers()\n       throws IOException {\n-    KeyProvider keyProvider = getKeyProvider();\n+    KeyProvider keyProvider;\n+    try {\n+      keyProvider = getKeyProvider();\n+    } catch (IOException ioe) {\n+      LOG.error(\"Error retrieving KeyProvider.\", ioe);\n+      return null;\n+    }\n     if (keyProvider instanceof DelegationTokenIssuer) {\n       return new DelegationTokenIssuer[]{(DelegationTokenIssuer)keyProvider};\n     }",
                "changes": 8,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1430. NPE if secure ozone if KMS uri is not defined. Contributed by Ajay Kumar. (#752)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/b353ee9fdca93792415cd5d928d0f3677a25294f"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74",
        "bug_id": "hadoop-ozone_18",
        "file": [
            {
                "additions": 4,
                "sha": "eb9d100beb21623285a74589dc644dd0390eb94a",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/ba71ff7f174be836f0e7749090fb98b45298799c/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "deletions": 2,
                "filename": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java?ref=ba71ff7f174be836f0e7749090fb98b45298799c",
                "patch": "@@ -528,7 +528,9 @@ boolean processKey(String key) throws IOException {\n         // traverse the parent tree structure of this key until we get the\n         // immediate child of the input directory.\n         Path immediateChildPath = getImmediateChildPath(keyPath.getParent());\n-        addSubDirStatus(immediateChildPath);\n+        if (immediateChildPath != null) {\n+          addSubDirStatus(immediateChildPath);\n+        }\n       }\n       return true;\n     }\n@@ -565,7 +567,7 @@ void addSubDirStatus(Path dirPath) throws FileNotFoundException {\n     Path getImmediateChildPath(Path keyPath) {\n       Path path = keyPath;\n       Path parent = path.getParent();\n-      while (parent != null && !parent.isRoot()) {\n+      while (parent != null) {\n         if (pathToKey(parent).equals(pathToKey(f))) {\n           return path;\n         }",
                "changes": 6,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/ba71ff7f174be836f0e7749090fb98b45298799c/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java"
            },
            {
                "additions": 66,
                "sha": "e1ba2e78632167b766e7088a1c81d1a8c5b4b1e2",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/ba71ff7f174be836f0e7749090fb98b45298799c/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java",
                "deletions": 0,
                "filename": "hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java?ref=ba71ff7f174be836f0e7749090fb98b45298799c",
                "patch": "@@ -47,6 +47,7 @@\n import org.junit.rules.Timeout;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n /**\n@@ -192,6 +193,71 @@ public void testListStatus() throws Exception {\n         3, fileStatuses.length);\n   }\n \n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\");\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = o3fs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path dir1 = new Path(\"/dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(\"/dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = o3fs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n   private KeyInfo getKey(Path keyPath, boolean isDirectory)\n       throws IOException, OzoneException {\n     String key = o3fs.pathToKey(keyPath);",
                "changes": 66,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/ba71ff7f174be836f0e7749090fb98b45298799c/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1013. NPE while listing directories.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/ba71ff7f174be836f0e7749090fb98b45298799c"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/0492e240ae5335ccf621c09475c50d2f1ea84df8",
        "bug_id": "hadoop-ozone_19",
        "file": [
            {
                "additions": 3,
                "sha": "9a1d4b3c77992f481fedfa572c7a02b32860afc6",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/6669fcd56fedab2cfb89aaace38eea118c562e5b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "deletions": 7,
                "filename": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java?ref=6669fcd56fedab2cfb89aaace38eea118c562e5b",
                "patch": "@@ -65,14 +65,10 @@\n   public BaseHttpServer(Configuration conf, String name) throws IOException {\n     this.name = name;\n     this.conf = conf;\n+    policy = DFSUtil.getHttpPolicy(conf);\n     if (isEnabled()) {\n-      policy = DFSUtil.getHttpPolicy(conf);\n-      if (policy.isHttpEnabled()) {\n-        this.httpAddress = getHttpBindAddress();\n-      }\n-      if (policy.isHttpsEnabled()) {\n-        this.httpsAddress = getHttpsBindAddress();\n-      }\n+      this.httpAddress = getHttpBindAddress();\n+      this.httpsAddress = getHttpsBindAddress();\n       HttpServer2.Builder builder = null;\n       builder = DFSUtil.httpServerTemplateForNNAndJN(conf, this.httpAddress,\n           this.httpsAddress, name, getSpnegoPrincipal(), getKeytabFile());",
                "changes": 10,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/6669fcd56fedab2cfb89aaace38eea118c562e5b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1235. BaseHttpServer NPE is HTTP policy is HTTPS_ONLY. Contributed by Xiaoyu Yao. \n\nCloses #572",
        "commit": "https://github.com/apache/hadoop-ozone/commit/6669fcd56fedab2cfb89aaace38eea118c562e5b"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/5f6d87157896a2bd228509b7091ebbe6b3dcb131",
        "bug_id": "hadoop-ozone_20",
        "file": [
            {
                "additions": 3,
                "sha": "14ed5f665b267ae704bd0826effdb3f28fe6c38d",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java?ref=91e42a3c1e722ce338fd7c074acd1aa65d8e4d74",
                "patch": "@@ -219,7 +219,9 @@ public Builder addMetadata(String key, String value) {\n     }\n \n     public Builder addAllMetadata(Map<String, String> additionalMetadata) {\n-      metadata.putAll(additionalMetadata);\n+      if (additionalMetadata != null) {\n+        metadata.putAll(additionalMetadata);\n+      }\n       return this;\n     }\n ",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-1011. Fix NPE BucketManagerImpl.setBucketProperty. Contributed by Xiaoyu Yao.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/b37e93a6352329e106e964beceddff5942bf82a0",
        "bug_id": "hadoop-ozone_21",
        "file": [
            {
                "additions": 8,
                "sha": "f41eb88c2f508c51397a6da10b94437b996b67e3",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/f27d1ad29170da809462ffe78d47e638ad7c465d/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "deletions": 5,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java?ref=f27d1ad29170da809462ffe78d47e638ad7c465d",
                "patch": "@@ -1438,11 +1438,14 @@ public OzoneManagerHttpServer getHttpServer() {\n           .setNodeType(HddsProtos.NodeType.DATANODE)\n           .setHostname(datanode.getHostName());\n \n-      dnServiceInfoBuilder.addServicePort(ServicePort.newBuilder()\n-          .setType(ServicePort.Type.HTTP)\n-          .setValue(DatanodeDetails.getFromProtoBuf(datanode)\n-              .getPort(DatanodeDetails.Port.Name.REST).getValue())\n-          .build());\n+      if(DatanodeDetails.getFromProtoBuf(datanode)\n+          .getPort(DatanodeDetails.Port.Name.REST) != null) {\n+        dnServiceInfoBuilder.addServicePort(ServicePort.newBuilder()\n+            .setType(ServicePort.Type.HTTP)\n+            .setValue(DatanodeDetails.getFromProtoBuf(datanode)\n+                .getPort(DatanodeDetails.Port.Name.REST).getValue())\n+            .build());\n+      }\n \n       services.add(dnServiceInfoBuilder.build());\n     }",
                "changes": 13,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/f27d1ad29170da809462ffe78d47e638ad7c465d/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-908: NPE in TestOzoneRpcClient.\nContributed by Ajay Kumar.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/f27d1ad29170da809462ffe78d47e638ad7c465d"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/eb46b4581e584f65a8bc16ff730417b35a8e80fd",
        "bug_id": "hadoop-ozone_22",
        "file": [
            {
                "additions": 1,
                "sha": "d391516b6ca22a9b6af01aaa1f53e647a40f8201",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/93eaeb7f7f4ee526968fc925a7ec09e380b5ec49/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "deletions": 0,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java?ref=93eaeb7f7f4ee526968fc925a7ec09e380b5ec49",
                "patch": "@@ -787,6 +787,7 @@ public void start() throws IOException {\n     keyManager.start(configuration);\n     omRpcServer.start();\n     try {\n+      httpServer = new OzoneManagerHttpServer(configuration, this);\n       httpServer.start();\n     } catch (Exception ex) {\n       // Allow OM to start as Http Server failure is not fatal.",
                "changes": 1,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/93eaeb7f7f4ee526968fc925a7ec09e380b5ec49/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-884. Fix merge issue that causes NPE OzoneManager#httpServer. Contributed by Xiaoyu Yao.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/93eaeb7f7f4ee526968fc925a7ec09e380b5ec49"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/035e8353755fc664d507cc82f6ba693bf52a566d",
        "bug_id": "hadoop-ozone_23",
        "file": [
            {
                "additions": 13,
                "sha": "22243008992a7e2bb40f2a5ac51a4b799bafc936",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/f2e72d065392e5517cde9524ae86fe84bdf95f47/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java",
                "deletions": 1,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java?ref=f2e72d065392e5517cde9524ae86fe84bdf95f47",
                "patch": "@@ -32,6 +32,8 @@\n import org.apache.hadoop.hdds.scm.XceiverClientGrpc;\n import org.apache.hadoop.hdds.scm.XceiverClientSpi;\n import org.apache.hadoop.hdds.scm.pipeline.Pipeline;\n+import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n+import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.token.Token;\n@@ -45,6 +47,7 @@\n import org.junit.rules.Timeout;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n+import org.mockito.Mockito;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -133,7 +136,7 @@ public void testCreateOzoneContainer() throws Exception {\n           OzoneConfigKeys.DFS_CONTAINER_IPC_RANDOM_PORT, false);\n \n       DatanodeDetails dn = TestUtils.randomDatanodeDetails();\n-      container = new OzoneContainer(dn, conf, null);\n+      container = new OzoneContainer(dn, conf, getContext(dn));\n       //Setting scmId, as we start manually ozone container.\n       container.getDispatcher().setScmId(UUID.randomUUID().toString());\n       container.start();\n@@ -206,4 +209,13 @@ public static void createContainerForTesting(XceiverClientSpi client,\n     Assert.assertNotNull(response);\n     Assert.assertTrue(request.getTraceID().equals(response.getTraceID()));\n   }\n+\n+  private StateContext getContext(DatanodeDetails datanodeDetails) {\n+    DatanodeStateMachine stateMachine = Mockito.mock(\n+        DatanodeStateMachine.class);\n+    StateContext context = Mockito.mock(StateContext.class);\n+    Mockito.when(stateMachine.getDatanodeDetails()).thenReturn(datanodeDetails);\n+    Mockito.when(context.getParent()).thenReturn(stateMachine);\n+    return context;\n+  }\n }\n\\ No newline at end of file",
                "changes": 14,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/f2e72d065392e5517cde9524ae86fe84bdf95f47/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-873. Fix TestSecureOzoneContainer NPE after HDDS-837. Contributed by Xiaoyu Yao.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/f2e72d065392e5517cde9524ae86fe84bdf95f47"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/0af66c461cab4378dce6160ac8b0cc131c55511c",
        "bug_id": "hadoop-ozone_24",
        "file": [
            {
                "additions": 9,
                "sha": "1ee6375a562f0f7c5f34e64a92168aa6227eeda3",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/56340777f3b9081fdb6a5026e594fca71d1c5a79/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java?ref=56340777f3b9081fdb6a5026e594fca71d1c5a79",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.ozone.container.common.states.endpoint;\n \n import com.google.common.base.Preconditions;\n+import com.google.protobuf.Descriptors;\n import com.google.protobuf.GeneratedMessage;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n@@ -125,9 +126,14 @@ public void setDatanodeDetailsProto(DatanodeDetailsProto\n    */\n   private void addReports(SCMHeartbeatRequestProto.Builder requestBuilder) {\n     for (GeneratedMessage report : context.getAllAvailableReports()) {\n-      requestBuilder.setField(\n-          SCMHeartbeatRequestProto.getDescriptor().findFieldByName(\n-              report.getDescriptorForType().getName()), report);\n+      String reportName = report.getDescriptorForType().getFullName();\n+      for (Descriptors.FieldDescriptor descriptor :\n+          SCMHeartbeatRequestProto.getDescriptor().getFields()) {\n+        String heartbeatFieldName = descriptor.getMessageType().getFullName();\n+        if (heartbeatFieldName.equals(reportName)) {\n+          requestBuilder.setField(descriptor, report);\n+        }\n+      }\n     }\n   }\n ",
                "changes": 12,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/56340777f3b9081fdb6a5026e594fca71d1c5a79/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java"
            },
            {
                "additions": 79,
                "sha": "5fd9cf6047998dc920f4d62aa9fc509d5101f282",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/56340777f3b9081fdb6a5026e594fca71d1c5a79/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java",
                "deletions": 0,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java?ref=56340777f3b9081fdb6a5026e594fca71d1c5a79",
                "patch": "@@ -18,13 +18,25 @@\n package org.apache.hadoop.ozone.container.common.report;\n \n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.google.protobuf.Descriptors;\n import com.google.protobuf.GeneratedMessage;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto\n+    .StorageContainerDatanodeProtocolProtos.ContainerReportsProto;\n+import org.apache.hadoop.hdds.protocol.proto\n+    .StorageContainerDatanodeProtocolProtos.NodeReportProto;\n+import org.apache.hadoop.hdds.protocol.proto\n+    .StorageContainerDatanodeProtocolProtos.SCMHeartbeatRequestProto;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.util.concurrent.HadoopExecutors;\n import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n+import java.util.Random;\n+import java.util.UUID;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n \n@@ -103,4 +115,71 @@ public void testPublishReport() throws InterruptedException {\n \n   }\n \n+  @Test\n+  public void testAddingReportToHeartbeat() {\n+    Configuration conf = new OzoneConfiguration();\n+    ReportPublisherFactory factory = new ReportPublisherFactory(conf);\n+    ReportPublisher nodeReportPublisher = factory.getPublisherFor(\n+        NodeReportProto.class);\n+    ReportPublisher containerReportPubliser = factory.getPublisherFor(\n+        ContainerReportsProto.class);\n+    GeneratedMessage nodeReport = nodeReportPublisher.getReport();\n+    GeneratedMessage containerReport = containerReportPubliser.getReport();\n+    SCMHeartbeatRequestProto.Builder heartbeatBuilder =\n+        SCMHeartbeatRequestProto.newBuilder();\n+    heartbeatBuilder.setDatanodeDetails(\n+        getDatanodeDetails().getProtoBufMessage());\n+    addReport(heartbeatBuilder, nodeReport);\n+    addReport(heartbeatBuilder, containerReport);\n+    SCMHeartbeatRequestProto heartbeat = heartbeatBuilder.build();\n+    Assert.assertTrue(heartbeat.hasNodeReport());\n+    Assert.assertTrue(heartbeat.hasContainerReport());\n+  }\n+\n+  /**\n+   * Get a datanode details.\n+   *\n+   * @return DatanodeDetails\n+   */\n+  private static DatanodeDetails getDatanodeDetails() {\n+    String uuid = UUID.randomUUID().toString();\n+    Random random = new Random();\n+    String ipAddress =\n+        random.nextInt(256) + \".\" + random.nextInt(256) + \".\" + random\n+            .nextInt(256) + \".\" + random.nextInt(256);\n+\n+    DatanodeDetails.Port containerPort = DatanodeDetails.newPort(\n+        DatanodeDetails.Port.Name.STANDALONE, 0);\n+    DatanodeDetails.Port ratisPort = DatanodeDetails.newPort(\n+        DatanodeDetails.Port.Name.RATIS, 0);\n+    DatanodeDetails.Port restPort = DatanodeDetails.newPort(\n+        DatanodeDetails.Port.Name.REST, 0);\n+    DatanodeDetails.Builder builder = DatanodeDetails.newBuilder();\n+    builder.setUuid(uuid)\n+        .setHostName(\"localhost\")\n+        .setIpAddress(ipAddress)\n+        .addPort(containerPort)\n+        .addPort(ratisPort)\n+        .addPort(restPort);\n+    return builder.build();\n+  }\n+\n+  /**\n+   * Adds the report to heartbeat.\n+   *\n+   * @param requestBuilder builder to which the report has to be added.\n+   * @param report the report to be added.\n+   */\n+  private static void addReport(SCMHeartbeatRequestProto.Builder requestBuilder,\n+                          GeneratedMessage report) {\n+    String reportName = report.getDescriptorForType().getFullName();\n+    for (Descriptors.FieldDescriptor descriptor :\n+        SCMHeartbeatRequestProto.getDescriptor().getFields()) {\n+      String heartbeatFieldName = descriptor.getMessageType().getFullName();\n+      if (heartbeatFieldName.equals(reportName)) {\n+        requestBuilder.setField(descriptor, report);\n+      }\n+    }\n+  }\n+\n }",
                "changes": 79,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/56340777f3b9081fdb6a5026e594fca71d1c5a79/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-158. DatanodeStateMachine endPoint task throws NullPointerException. Contributed by Nanda Kumar.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/56340777f3b9081fdb6a5026e594fca71d1c5a79"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/632a1e43461140ad8b7bad0d46c5bf796eadee7d",
        "bug_id": "hadoop-ozone_25",
        "file": [
            {
                "additions": 17,
                "sha": "0de9f18cf981f6036d551a3bfd4d1ecc593b2046",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java",
                "deletions": 2,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -95,15 +95,30 @@ private VolumeInfo(Builder b) throws IOException {\n     this.usage = new VolumeUsage(root, b.conf);\n   }\n \n-  public long getCapacity() {\n-    return configuredCapacity < 0 ? usage.getCapacity() : configuredCapacity;\n+  public long getCapacity() throws IOException {\n+    if (configuredCapacity < 0) {\n+      if (usage == null) {\n+        throw new IOException(\"Volume Usage thread is not running. This error\" +\n+            \" is usually seen during DataNode shutdown.\");\n+      }\n+      return usage.getCapacity();\n+    }\n+    return configuredCapacity;\n   }\n \n   public long getAvailable() throws IOException {\n+    if (usage == null) {\n+      throw new IOException(\"Volume Usage thread is not running. This error \" +\n+          \"is usually seen during DataNode shutdown.\");\n+    }\n     return usage.getAvailable();\n   }\n \n   public long getScmUsed() throws IOException {\n+    if (usage == null) {\n+      throw new IOException(\"Volume Usage thread is not running. This error \" +\n+          \"is usually seen during DataNode shutdown.\");\n+    }\n     return usage.getScmUsed();\n   }\n ",
                "changes": 19,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java"
            },
            {
                "additions": 7,
                "sha": "d30dd89ee05c29a658a37911a9301ef2e9a7c5b6",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "deletions": 4,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -372,18 +372,21 @@ public void shutdown() {\n       for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n         hddsVolume = entry.getValue();\n         VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n-        long scmUsed = 0;\n-        long remaining = 0;\n+        long scmUsed;\n+        long remaining;\n+        long capacity;\n         failed = false;\n         try {\n           scmUsed = volumeInfo.getScmUsed();\n           remaining = volumeInfo.getAvailable();\n+          capacity = volumeInfo.getCapacity();\n         } catch (IOException ex) {\n           LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n-              \"storage location {}\", volumeInfo.getRootDir());\n+              \"storage location {}\", volumeInfo.getRootDir(), ex);\n           // reset scmUsed and remaining if df/du failed.\n           scmUsed = 0;\n           remaining = 0;\n+          capacity = 0;\n           failed = true;\n         }\n \n@@ -392,7 +395,7 @@ public void shutdown() {\n         builder.setStorageLocation(volumeInfo.getRootDir())\n             .setId(hddsVolume.getStorageID())\n             .setFailed(failed)\n-            .setCapacity(hddsVolume.getCapacity())\n+            .setCapacity(capacity)\n             .setRemaining(remaining)\n             .setScmUsed(scmUsed)\n             .setStorageType(hddsVolume.getStorageType());",
                "changes": 11,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java"
            },
            {
                "additions": 6,
                "sha": "6b467622738bbcfad11ff6b0cb71268f7d362852",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -31,6 +31,7 @@\n import org.mockito.Mockito;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.Properties;\n import java.util.UUID;\n \n@@ -134,12 +135,14 @@ public void testShutdown() throws Exception{\n         scmUsedFile.exists());\n \n     try {\n-      // Volume.getAvailable() should fail with NullPointerException as usage\n-      // is shutdown.\n+      // Volume.getAvailable() should fail with IOException\n+      // as usage thread is shutdown.\n       volume.getAvailable();\n       fail(\"HddsVolume#shutdown test failed\");\n     } catch (Exception ex){\n-      assertTrue(ex instanceof NullPointerException);\n+      assertTrue(ex instanceof IOException);\n+      assertTrue(ex.getMessage().contains(\n+          \"Volume Usage thread is not running.\"));\n     }\n   }\n }",
                "changes": 9,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java"
            },
            {
                "additions": 3,
                "sha": "7bb8a43d7b4c64e44b7340de30046cccfc85e5dc",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -222,8 +222,10 @@ public void testShutdown() throws Exception {\n         // getAvailable() should throw null pointer exception as usage is null.\n         volume.getAvailable();\n         fail(\"Volume shutdown failed.\");\n-      } catch (NullPointerException ex) {\n+      } catch (IOException ex) {\n         // Do Nothing. Exception is expected.\n+        assertTrue(ex.getMessage().contains(\n+            \"Volume Usage thread is not running.\"));\n       }\n     }\n   }",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java"
            },
            {
                "additions": 2,
                "sha": "618cd8e3a12ab92156224a9e390a5630d05a70aa",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java",
                "deletions": 1,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -99,7 +99,7 @@ public static void shutdown() {\n     }\n   }\n \n-  @Test\n+  @Test(timeout = 300_000L)\n   public void testPipelineFail() throws InterruptedException, IOException,\n       TimeoutException {\n     Assert.assertEquals(ratisContainer1.getPipeline().getPipelineState(),\n@@ -118,6 +118,7 @@ public void testPipelineFail() throws InterruptedException, IOException,\n         pipelineManager.getPipeline(ratisContainer2.getPipeline().getId())\n             .getPipelineState());\n     // Now restart the datanode and make sure that a new pipeline is created.\n+    cluster.setWaitForClusterToBeReadyTimeout(300000);\n     cluster.restartHddsDatanode(dnToFail, true);\n     ContainerWithPipeline ratisContainer3 =\n         containerManager.allocateContainer(RATIS, THREE, \"testOwner\");",
                "changes": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java"
            },
            {
                "additions": 8,
                "sha": "15bf8d099ab8a803e4defd28f9faa679f21a78e3",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java",
                "deletions": 0,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -66,6 +66,14 @@ static Builder newBuilder(OzoneConfiguration conf) {\n    */\n   void waitForClusterToBeReady() throws TimeoutException, InterruptedException;\n \n+  /**\n+   * Sets the timeout value after which\n+   * {@link MiniOzoneCluster#waitForClusterToBeReady} times out.\n+   *\n+   * @param timeoutInMs timeout value in milliseconds\n+   */\n+  void setWaitForClusterToBeReadyTimeout(int timeoutInMs);\n+\n   /**\n    * Waits/blocks till the cluster is out of chill mode.\n    *",
                "changes": 8,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java"
            },
            {
                "additions": 15,
                "sha": "6c0f408123a4adf9bf466d860776e567f87598d4",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java",
                "deletions": 1,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java?ref=cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf",
                "patch": "@@ -90,6 +90,9 @@\n   private final OzoneManager ozoneManager;\n   private final List<HddsDatanodeService> hddsDatanodes;\n \n+  // Timeout for the cluster to be ready\n+  private int waitForClusterToBeReadyTimeout = 60000; // 1 min\n+\n   /**\n    * Creates a new MiniOzoneCluster.\n    *\n@@ -122,7 +125,18 @@ public void waitForClusterToBeReady()\n           isReady? \"Cluster is ready\" : \"Waiting for cluster to be ready\",\n           healthy, hddsDatanodes.size());\n       return isReady;\n-    }, 1000, 60 * 1000); //wait for 1 min.\n+    }, 1000, waitForClusterToBeReadyTimeout);\n+  }\n+\n+  /**\n+   * Sets the timeout value after which\n+   * {@link MiniOzoneClusterImpl#waitForClusterToBeReady} times out.\n+   *\n+   * @param timeoutInMs timeout value in milliseconds\n+   */\n+  @Override\n+  public void setWaitForClusterToBeReadyTimeout(int timeoutInMs) {\n+    waitForClusterToBeReadyTimeout = timeoutInMs;\n   }\n \n   /**",
                "changes": 16,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-754. VolumeInfo#getScmUsed throws NPE.\nContributed by Hanisha Koneru.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/cbb65206f2be00a6764f2ac4ccde26a9c0c5ccbf"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/0349106adf627a4f5018abcc0af3643a2519189c",
        "bug_id": "hadoop-ozone_26",
        "file": [
            {
                "additions": 4,
                "sha": "29d74c2ab10e832dba7d6e5065873c546e775d1e",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b92897370d6d417ab5b78a220f255d25a4413152/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "deletions": 0,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java?ref=b92897370d6d417ab5b78a220f255d25a4413152",
                "patch": "@@ -90,6 +90,10 @@ public static void setup() throws StorageContainerException {\n     Mockito.when(handler.handle(any(), any())).thenCallRealMethod();\n     doCallRealMethod().when(dispatcher).setMetricsForTesting(any());\n     dispatcher.setMetricsForTesting(Mockito.mock(ContainerMetrics.class));\n+    Mockito.when(dispatcher.buildAuditMessageForFailure(any(), any(), any()))\n+        .thenCallRealMethod();\n+    Mockito.when(dispatcher.buildAuditMessageForSuccess(any(), any()))\n+        .thenCallRealMethod();\n   }\n \n   @Test",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b92897370d6d417ab5b78a220f255d25a4413152/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-849. Fix NPE in TestKeyValueHandler because of audit log write.\nContributed by Dinesh Chitlangia.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/b92897370d6d417ab5b78a220f255d25a4413152"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/bda2e78da6eebf1d4f2d773a9a1289709d00a5c6",
        "bug_id": "hadoop-ozone_27",
        "file": [
            {
                "additions": 0,
                "sha": "e8aa22c2ac03de14cd404926f268efafe70b49c1",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "deletions": 11,
                "filename": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "patch": "@@ -286,17 +286,6 @@\n   public static final double\n       HDDS_DATANODE_STORAGE_UTILIZATION_CRITICAL_THRESHOLD_DEFAULT = 0.75;\n \n-  public static final String\n-      HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY =\n-      \"hdds.write.lock.reporting.threshold.ms\";\n-  public static final long\n-      HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_DEFAULT = 5000L;\n-  public static final String\n-      HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_KEY =\n-      \"hdds.lock.suppress.warning.interval.ms\";\n-  public static final long\n-      HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_DEAFULT = 10000L;\n-\n   public static final String OZONE_CONTAINER_COPY_WORKDIR =\n       \"hdds.datanode.replication.work.dir\";\n ",
                "changes": 11,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java"
            },
            {
                "additions": 26,
                "sha": "2d0467706ec921ee55cfea0711332ff0298e4b42",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "deletions": 18,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "patch": "@@ -69,31 +69,39 @@ public VersionEndpointTask(EndpointStateMachine rpcEndPoint,\n       VersionResponse response = VersionResponse.getFromProtobuf(\n           versionResponse);\n       rpcEndPoint.setVersion(response);\n-      VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n-      Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n \n       String scmId = response.getValue(OzoneConsts.SCM_ID);\n       String clusterId = response.getValue(OzoneConsts.CLUSTER_ID);\n \n-      Preconditions.checkNotNull(scmId, \"Reply from SCM: scmId cannot be \" +\n-          \"null\");\n-      Preconditions.checkNotNull(clusterId, \"Reply from SCM: clusterId \" +\n-          \"cannot be null\");\n+      // Check volumes\n+      VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n+      volumeSet.readLock();\n+      try {\n+        Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n+\n+        Preconditions.checkNotNull(scmId, \"Reply from SCM: scmId cannot be \" +\n+            \"null\");\n+        Preconditions.checkNotNull(clusterId, \"Reply from SCM: clusterId \" +\n+            \"cannot be null\");\n \n-      // If version file does not exist create version file and also set scmId\n-      for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n-        HddsVolume hddsVolume = entry.getValue();\n-        boolean result = HddsVolumeUtil.checkVolume(hddsVolume, scmId,\n-            clusterId, LOG);\n-        if (!result) {\n-          volumeSet.failVolume(hddsVolume.getHddsRootDir().getPath());\n+        // If version file does not exist create version file and also set scmId\n+        for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n+          HddsVolume hddsVolume = entry.getValue();\n+          boolean result = HddsVolumeUtil.checkVolume(hddsVolume, scmId,\n+              clusterId, LOG);\n+          if (!result) {\n+            volumeSet.failVolume(hddsVolume.getHddsRootDir().getPath());\n+          }\n         }\n+        if (volumeSet.getVolumesList().size() == 0) {\n+          // All volumes are inconsistent state\n+          throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n+              \"Inconsistent State\");\n+        }\n+      } finally {\n+        volumeSet.readUnlock();\n       }\n-      if (volumeSet.getVolumesList().size() == 0) {\n-        // All volumes are inconsistent state\n-        throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n-            \"Inconsistent State\");\n-      }\n+\n       ozoneContainer.getDispatcher().setScmId(scmId);\n \n       EndpointStateMachine.EndPointStates nextState =",
                "changes": 44,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java"
            },
            {
                "additions": 1,
                "sha": "cb356dadeb236360a214d729cd511e29a3047c6c",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "patch": "@@ -164,7 +164,7 @@ private static String getProperty(Properties props, String propName, File\n   }\n \n   /**\n-   * Check Volume is consistent state or not.\n+   * Check Volume is in consistent state or not.\n    * @param hddsVolume\n    * @param scmId\n    * @param clusterId",
                "changes": 2,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java"
            },
            {
                "additions": 97,
                "sha": "5b6b823c9c69ef69fb14d0735bd42c0a84a534b2",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "deletions": 81,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "patch": "@@ -33,15 +33,11 @@\n     .StorageContainerDatanodeProtocolProtos;\n import org.apache.hadoop.hdds.protocol.proto\n     .StorageContainerDatanodeProtocolProtos.NodeReportProto;\n-import org.apache.hadoop.ozone.OzoneConfigKeys;\n import org.apache.hadoop.ozone.common.InconsistentStorageStateException;\n import org.apache.hadoop.ozone.container.common.impl.StorageLocationReport;\n import org.apache.hadoop.ozone.container.common.utils.HddsVolumeUtil;\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume.VolumeState;\n-import org.apache.hadoop.ozone.container.common.interfaces.VolumeChoosingPolicy;\n-import org.apache.hadoop.util.AutoCloseableLock;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n-import org.apache.hadoop.util.InstrumentedLock;\n import org.apache.hadoop.util.ShutdownHookManager;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -53,8 +49,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n /**\n  * VolumeSet to manage volumes in a DataNode.\n@@ -84,11 +79,12 @@\n   private EnumMap<StorageType, List<HddsVolume>> volumeStateMap;\n \n   /**\n-   * Lock to synchronize changes to the VolumeSet. Any update to\n-   * {@link VolumeSet#volumeMap}, {@link VolumeSet#failedVolumeMap}, or\n-   * {@link VolumeSet#volumeStateMap} should be done after acquiring this lock.\n+   * A Reentrant Read Write Lock to synchronize volume operations in VolumeSet.\n+   * Any update to {@link VolumeSet#volumeMap},\n+   * {@link VolumeSet#failedVolumeMap}, or {@link VolumeSet#volumeStateMap}\n+   * should be done after acquiring the write lock.\n    */\n-  private final AutoCloseableLock volumeSetLock;\n+  private final ReentrantReadWriteLock volumeSetRWLock;\n \n   private final String datanodeUuid;\n   private String clusterID;\n@@ -105,17 +101,7 @@ public VolumeSet(String dnUuid, String clusterID, Configuration conf)\n     this.datanodeUuid = dnUuid;\n     this.clusterID = clusterID;\n     this.conf = conf;\n-    this.volumeSetLock = new AutoCloseableLock(\n-        new InstrumentedLock(getClass().getName(), LOG,\n-            new ReentrantLock(true),\n-            conf.getTimeDuration(\n-                OzoneConfigKeys.HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY,\n-                OzoneConfigKeys.HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_DEFAULT,\n-                TimeUnit.MILLISECONDS),\n-            conf.getTimeDuration(\n-                OzoneConfigKeys.HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_KEY,\n-                OzoneConfigKeys.HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_DEAFULT,\n-                TimeUnit.MILLISECONDS)));\n+    this.volumeSetRWLock = new ReentrantReadWriteLock();\n \n     initializeVolumeSet();\n   }\n@@ -198,14 +184,35 @@ private void checkAndSetClusterID(String idFromVersionFile)\n     }\n   }\n \n-  public void acquireLock() {\n-    volumeSetLock.acquire();\n+  /**\n+   * Acquire Volume Set Read lock.\n+   */\n+  public void readLock() {\n+    volumeSetRWLock.readLock().lock();\n+  }\n+\n+  /**\n+   * Release Volume Set Read lock.\n+   */\n+  public void readUnlock() {\n+    volumeSetRWLock.readLock().unlock();\n   }\n \n-  public void releaseLock() {\n-    volumeSetLock.release();\n+  /**\n+   * Acquire Volume Set Write lock.\n+   */\n+  public void writeLock() {\n+    volumeSetRWLock.writeLock().lock();\n+  }\n+\n+  /**\n+   * Release Volume Set Write lock.\n+   */\n+  public void writeUnlock() {\n+    volumeSetRWLock.writeLock().unlock();\n   }\n \n+\n   private HddsVolume createVolume(String locationString,\n       StorageType storageType) throws IOException {\n     HddsVolume.Builder volumeBuilder = new HddsVolume.Builder(locationString)\n@@ -227,7 +234,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(volumeRoot);\n     boolean success;\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         LOG.warn(\"Volume : {} already exists in VolumeMap\", hddsRoot);\n         success = false;\n@@ -247,6 +255,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n     } catch (IOException ex) {\n       LOG.error(\"Failed to add volume \" + volumeRoot + \" to VolumeSet\", ex);\n       success = false;\n+    } finally {\n+      this.writeUnlock();\n     }\n     return success;\n   }\n@@ -255,7 +265,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n   public void failVolume(String dataDir) {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(dataDir);\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         HddsVolume hddsVolume = volumeMap.get(hddsRoot);\n         hddsVolume.failVolume();\n@@ -270,14 +281,17 @@ public void failVolume(String dataDir) {\n       } else {\n         LOG.warn(\"Volume : {} does not exist in VolumeSet\", hddsRoot);\n       }\n+    } finally {\n+      this.writeUnlock();\n     }\n   }\n \n   // Remove a volume from the VolumeSet completely.\n   public void removeVolume(String dataDir) throws IOException {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(dataDir);\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         HddsVolume hddsVolume = volumeMap.get(hddsRoot);\n         hddsVolume.shutdown();\n@@ -295,14 +309,11 @@ public void removeVolume(String dataDir) throws IOException {\n       } else {\n         LOG.warn(\"Volume : {} does not exist in VolumeSet\", hddsRoot);\n       }\n+    } finally {\n+      this.writeUnlock();\n     }\n   }\n \n-  public HddsVolume chooseVolume(long containerSize,\n-      VolumeChoosingPolicy choosingPolicy) throws IOException {\n-    return choosingPolicy.chooseVolume(getVolumesList(), containerSize);\n-  }\n-\n   /**\n    * This method, call shutdown on each volume to shutdown volume usage\n    * thread and write scmUsed on each volume.\n@@ -352,55 +363,60 @@ public void shutdown() {\n   public StorageContainerDatanodeProtocolProtos.NodeReportProto getNodeReport()\n       throws IOException {\n     boolean failed;\n-    StorageLocationReport[] reports = new StorageLocationReport[volumeMap\n-        .size() + failedVolumeMap.size()];\n-    int counter = 0;\n-    HddsVolume hddsVolume;\n-    for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n-      hddsVolume = entry.getValue();\n-      VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n-      long scmUsed = 0;\n-      long remaining = 0;\n-      failed = false;\n-      try {\n-        scmUsed = volumeInfo.getScmUsed();\n-        remaining = volumeInfo.getAvailable();\n-      } catch (IOException ex) {\n-        LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n-            \"storage location {}\", volumeInfo.getRootDir());\n-        // reset scmUsed and remaining if df/du failed.\n-        scmUsed = 0;\n-        remaining = 0;\n-        failed = true;\n-      }\n+    this.readLock();\n+    try {\n+      StorageLocationReport[] reports = new StorageLocationReport[volumeMap\n+          .size() + failedVolumeMap.size()];\n+      int counter = 0;\n+      HddsVolume hddsVolume;\n+      for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n+        hddsVolume = entry.getValue();\n+        VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n+        long scmUsed = 0;\n+        long remaining = 0;\n+        failed = false;\n+        try {\n+          scmUsed = volumeInfo.getScmUsed();\n+          remaining = volumeInfo.getAvailable();\n+        } catch (IOException ex) {\n+          LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n+              \"storage location {}\", volumeInfo.getRootDir());\n+          // reset scmUsed and remaining if df/du failed.\n+          scmUsed = 0;\n+          remaining = 0;\n+          failed = true;\n+        }\n \n-      StorageLocationReport.Builder builder =\n-          StorageLocationReport.newBuilder();\n-      builder.setStorageLocation(volumeInfo.getRootDir())\n-          .setId(hddsVolume.getStorageID())\n-          .setFailed(failed)\n-          .setCapacity(hddsVolume.getCapacity())\n-          .setRemaining(remaining)\n-          .setScmUsed(scmUsed)\n-          .setStorageType(hddsVolume.getStorageType());\n-      StorageLocationReport r = builder.build();\n-      reports[counter++] = r;\n-    }\n-    for (Map.Entry<String, HddsVolume> entry : failedVolumeMap.entrySet()) {\n-      hddsVolume = entry.getValue();\n-      StorageLocationReport.Builder builder = StorageLocationReport\n-          .newBuilder();\n-      builder.setStorageLocation(hddsVolume.getHddsRootDir()\n-          .getAbsolutePath()).setId(hddsVolume.getStorageID()).setFailed(true)\n-          .setCapacity(0).setRemaining(0).setScmUsed(0).setStorageType(\n-              hddsVolume.getStorageType());\n-      StorageLocationReport r = builder.build();\n-      reports[counter++] = r;\n-    }\n-    NodeReportProto.Builder nrb = NodeReportProto.newBuilder();\n-    for (int i = 0; i < reports.length; i++) {\n-      nrb.addStorageReport(reports[i].getProtoBufMessage());\n+        StorageLocationReport.Builder builder =\n+            StorageLocationReport.newBuilder();\n+        builder.setStorageLocation(volumeInfo.getRootDir())\n+            .setId(hddsVolume.getStorageID())\n+            .setFailed(failed)\n+            .setCapacity(hddsVolume.getCapacity())\n+            .setRemaining(remaining)\n+            .setScmUsed(scmUsed)\n+            .setStorageType(hddsVolume.getStorageType());\n+        StorageLocationReport r = builder.build();\n+        reports[counter++] = r;\n+      }\n+      for (Map.Entry<String, HddsVolume> entry : failedVolumeMap.entrySet()) {\n+        hddsVolume = entry.getValue();\n+        StorageLocationReport.Builder builder = StorageLocationReport\n+            .newBuilder();\n+        builder.setStorageLocation(hddsVolume.getHddsRootDir()\n+            .getAbsolutePath()).setId(hddsVolume.getStorageID()).setFailed(true)\n+            .setCapacity(0).setRemaining(0).setScmUsed(0).setStorageType(\n+            hddsVolume.getStorageType());\n+        StorageLocationReport r = builder.build();\n+        reports[counter++] = r;\n+      }\n+      NodeReportProto.Builder nrb = NodeReportProto.newBuilder();\n+      for (int i = 0; i < reports.length; i++) {\n+        nrb.addStorageReport(reports[i].getProtoBufMessage());\n+      }\n+      return nrb.build();\n+    } finally {\n+      this.readUnlock();\n     }\n-    return nrb.build();\n   }\n }\n\\ No newline at end of file",
                "changes": 178,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java"
            },
            {
                "additions": 3,
                "sha": "e5b344de483770fc5e7b201fe12220438ebe7cbe",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "patch": "@@ -108,8 +108,8 @@ public void create(VolumeSet volumeSet, VolumeChoosingPolicy\n     Preconditions.checkNotNull(scmId, \"scmId cannot be null\");\n \n     File containerMetaDataPath = null;\n-    //acquiring volumeset lock and container lock\n-    volumeSet.acquireLock();\n+    //acquiring volumeset read lock\n+    volumeSet.readLock();\n     long maxSize = containerData.getMaxSize();\n     try {\n       HddsVolume containerVolume = volumeChoosingPolicy.chooseVolume(volumeSet\n@@ -166,7 +166,7 @@ public void create(VolumeSet volumeSet, VolumeChoosingPolicy\n       throw new StorageContainerException(\"Container creation failed.\", ex,\n           CONTAINER_INTERNAL_ERROR);\n     } finally {\n-      volumeSet.releaseLock();\n+      volumeSet.readUnlock();\n     }\n   }\n ",
                "changes": 6,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java"
            },
            {
                "additions": 2,
                "sha": "922db2ad888581d88778787c2ecd7576e9f7073a",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "deletions": 2,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "patch": "@@ -271,14 +271,14 @@ ContainerCommandResponseProto handleCreateContainer(\n \n   public void populateContainerPathFields(KeyValueContainer container,\n       long maxSize) throws IOException {\n-    volumeSet.acquireLock();\n+    volumeSet.readLock();\n     try {\n       HddsVolume containerVolume = volumeChoosingPolicy.chooseVolume(volumeSet\n           .getVolumesList(), maxSize);\n       String hddsVolumeDir = containerVolume.getHddsRootDir().toString();\n       container.populatePathFields(scmID, containerVolume, hddsVolumeDir);\n     } finally {\n-      volumeSet.releaseLock();\n+      volumeSet.readUnlock();\n     }\n   }\n ",
                "changes": 4,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-354. VolumeInfo.getScmUsed throws NPE. Contributed by Hanisha Koneru.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd"
    },
    {
        "parent": "https://github.com/apache/hadoop-ozone/commit/7e3ee2bc7566670e2ebd4ed87a019d4ceb5e8fcd",
        "bug_id": "hadoop-ozone_28",
        "file": [
            {
                "additions": 5,
                "sha": "a8df11440fa8f78d314524c516ea6cae522f2df4",
                "status": "modified",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/920b9aa2fddb0771559f674ebb8d96ab4120c0fc/hadoop-ozone/objectstore-service/src/main/java/org/apache/hadoop/ozone/web/storage/DistributedStorageHandler.java",
                "deletions": 0,
                "filename": "hadoop-ozone/objectstore-service/src/main/java/org/apache/hadoop/ozone/web/storage/DistributedStorageHandler.java",
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/objectstore-service/src/main/java/org/apache/hadoop/ozone/web/storage/DistributedStorageHandler.java?ref=920b9aa2fddb0771559f674ebb8d96ab4120c0fc",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.ozone.web.storage;\n \n import com.google.common.base.Strings;\n+import org.apache.hadoop.hdds.client.ReplicationType;\n import org.apache.hadoop.hdds.scm.client.HddsClientUtils;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.io.retry.RetryPolicy;\n@@ -486,6 +487,7 @@ public KeyInfo getKeyInfo(KeyArgs args) throws IOException, OzoneException {\n         HddsClientUtils.formatDateTime(omKeyInfo.getCreationTime()));\n     keyInfo.setModifiedOn(\n         HddsClientUtils.formatDateTime(omKeyInfo.getModificationTime()));\n+    keyInfo.setType(ReplicationType.valueOf(omKeyInfo.getType().toString()));\n     return keyInfo;\n   }\n \n@@ -510,6 +512,8 @@ public KeyInfo getKeyInfoDetails(KeyArgs args) throws IOException{\n     keyInfoDetails.setModifiedOn(\n         HddsClientUtils.formatDateTime(omKeyInfo.getModificationTime()));\n     keyInfoDetails.setKeyLocations(keyLocations);\n+    keyInfoDetails.setType(ReplicationType.valueOf(omKeyInfo.getType()\n+        .toString()));\n     return keyInfoDetails;\n   }\n \n@@ -553,6 +557,7 @@ public ListKeys listKeys(ListArgs args) throws IOException, OzoneException {\n             HddsClientUtils.formatDateTime(info.getCreationTime()));\n         tempInfo.setModifiedOn(\n             HddsClientUtils.formatDateTime(info.getModificationTime()));\n+        tempInfo.setType(ReplicationType.valueOf(info.getType().toString()));\n \n         result.addKey(tempInfo);\n       }",
                "changes": 5,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/920b9aa2fddb0771559f674ebb8d96ab4120c0fc/hadoop-ozone/objectstore-service/src/main/java/org/apache/hadoop/ozone/web/storage/DistributedStorageHandler.java"
            }
        ],
        "repo": "hadoop-ozone",
        "message": "HDDS-823. OzoneRestClient is failing with NPE on getKeyDetails call. Contributed by Bharat Viswanadham.",
        "commit": "https://github.com/apache/hadoop-ozone/commit/920b9aa2fddb0771559f674ebb8d96ab4120c0fc"
    }
]