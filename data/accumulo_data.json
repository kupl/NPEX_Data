[
    {
        "commit": "https://github.com/apache/accumulo/commit/f3b3590a5b4f8adf80f0acffe768a8509d409c51",
        "file": [
            {
                "patch": "@@ -102,7 +102,8 @@ public boolean hasNext() {\n \n       currentTabletKeys = scanToPrevEndRow();\n       if (currentTabletKeys.size() == 0) {\n-        // Always expect the default tablet to exist for a table. The following checks for the case when\n+        // Always expect the default tablet to exist for a table. The following checks for the case\n+        // when\n         // the default tablet was not seen when it should have been seen.\n         if (lastTablet != null) {\n           KeyExtent lastExtent = new KeyExtent(lastTablet, (Text) null);",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/base/src/main/java/org/apache/accumulo/server/util/TabletIterator.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "b9a5a716f3103fedf22f5d22c8be50c927e747de",
                "blob_url": "https://github.com/apache/accumulo/blob/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/base/src/main/java/org/apache/accumulo/server/util/TabletIterator.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/TabletIterator.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/TabletIterator.java?ref=f3b3590a5b4f8adf80f0acffe768a8509d409c51"
            },
            {
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.accumulo.monitor.servlets;\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n+import static org.apache.commons.lang.StringEscapeUtils.escapeHtml;\n \n import java.io.IOException;\n import java.io.PrintWriter;\n@@ -133,8 +134,8 @@ protected void pageStart(HttpServletRequest req, HttpServletResponse resp, Strin\n \n     // BEGIN HEADER\n     sb.append(\"<head>\\n\");\n-    sb.append(\"<title>\").append(getTitle(req)).append(\" - Accumulo \").append(Constants.VERSION)\n-        .append(\"</title>\\n\");\n+    sb.append(\"<title>\").append(escapeHtml(getTitle(req))).append(\" - Accumulo \")\n+        .append(Constants.VERSION).append(\"</title>\\n\");\n     if ((refresh > 0) && (req.getRequestURI().startsWith(\"/vis\") == false)\n         && (req.getRequestURI().startsWith(\"/shell\") == false))\n       sb.append(\"<meta http-equiv='refresh' content='\" + refresh + \"' />\\n\");",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/BasicServlet.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "dfc81de67f5eecfdb98e550f65f6dcf712f00469",
                "blob_url": "https://github.com/apache/accumulo/blob/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/BasicServlet.java",
                "filename": "server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/BasicServlet.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/BasicServlet.java?ref=f3b3590a5b4f8adf80f0acffe768a8509d409c51"
            },
            {
                "patch": "@@ -51,7 +51,7 @@\n \n   public static String getStringParameter(HttpServletRequest req, String name,\n       String defaultValue) {\n-    String result = req.getParameter(name).replaceAll(\"[^A-Za-z]\", \"\");\n+    final String result = req.getParameter(name);\n     if (result == null) {\n       return defaultValue;\n     }",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/Basic.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "09b7ccf47bcd353b70ddd4528aec1a2b6a62a9da",
                "blob_url": "https://github.com/apache/accumulo/blob/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/Basic.java",
                "filename": "server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/Basic.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/Basic.java?ref=f3b3590a5b4f8adf80f0acffe768a8509d409c51"
            },
            {
                "patch": "@@ -16,6 +16,8 @@\n  */\n package org.apache.accumulo.monitor.servlets.trace;\n \n+import static org.apache.commons.lang.StringEscapeUtils.escapeHtml;\n+\n import java.security.PrivilegedAction;\n import java.util.Map.Entry;\n \n@@ -61,7 +63,7 @@ public void pageBody(HttpServletRequest req, HttpServletResponse resp, StringBui\n     Range range = new Range(new Text(\"start:\" + Long.toHexString(startTime)),\n         new Text(\"start:\" + Long.toHexString(endTime)));\n     scanner.setRange(range);\n-    final Table trace = new Table(\"trace\", \"Traces for \" + getType(req));\n+    final Table trace = new Table(\"trace\", \"Traces for \" + escapeHtml(type));\n     trace.addSortableColumn(\"Start\", new ShowTraceLinkType(), \"Start Time\");\n     trace.addSortableColumn(\"ms\", new DurationType(), \"Span time\");\n     trace.addUnsortableColumn(\"Source\", new StringType<String>(), \"Service and location\");",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ListType.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "babeb02300ba0e8cdad5ecc4ba81f56d5ac63563",
                "blob_url": "https://github.com/apache/accumulo/blob/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ListType.java",
                "filename": "server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ListType.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ListType.java?ref=f3b3590a5b4f8adf80f0acffe768a8509d409c51"
            },
            {
                "patch": "@@ -22,6 +22,7 @@\n import java.util.Collection;\n import java.util.Map.Entry;\n import java.util.Set;\n+import java.util.regex.Pattern;\n \n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n@@ -45,9 +46,14 @@\n   private static final long serialVersionUID = 1L;\n   private static final String checkboxIdSuffix = \"_checkbox\";\n   private static final String pageLoadFunctionName = \"pageload\";\n+  private static final Pattern TRACE_ID_PATTERN = Pattern.compile(\"\\\\p{XDigit}{16}\");\n \n   String getTraceId(HttpServletRequest req) {\n-    return getStringParameter(req, \"id\", null);\n+    final String stringValue = getStringParameter(req, \"id\", null);\n+    if (stringValue == null) {\n+      return null;\n+    }\n+    return TRACE_ID_PATTERN.matcher(stringValue).matches() ? stringValue : null;\n   }\n \n   @Override",
                "additions": 7,
                "raw_url": "https://github.com/apache/accumulo/raw/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ShowTrace.java",
                "status": "modified",
                "changes": 8,
                "deletions": 1,
                "sha": "55a09a479dcdbaf432f68972842e5b7ddb2fb3a5",
                "blob_url": "https://github.com/apache/accumulo/blob/f3b3590a5b4f8adf80f0acffe768a8509d409c51/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ShowTrace.java",
                "filename": "server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ShowTrace.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ShowTrace.java?ref=f3b3590a5b4f8adf80f0acffe768a8509d409c51"
            },
            {
                "patch": "@@ -1,55 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to you under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- * http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.accumulo.monitor.servlets.trace;\n-\n-import static org.junit.Assert.assertEquals;\n-\n-import javax.servlet.http.HttpServletRequest;\n-\n-import org.easymock.EasyMock;\n-import org.junit.Before;\n-import org.junit.Test;\n-\n-/**\n- * Test class for {@link Basic}.\n- */\n-public class BasicTest {\n-  private HttpServletRequest request;\n-\n-  @Before\n-  public void setupMocks() {\n-    request = EasyMock.createMock(HttpServletRequest.class);\n-  }\n-\n-  @Test\n-  public void testSanitizedStringParameters() {\n-    final String safeKey = \"safeKey\";\n-    final String bogusKey = \"bogusKey\";\n-    final String safeValue = \"value\";\n-    final String bogusValue = \"value<script>alert('ohnoes');</script>\";\n-    final String sanitizedBogusValue = \"valuescriptalertohnoesscript\";\n-\n-    EasyMock.expect(request.getParameter(safeKey)).andReturn(safeValue).atLeastOnce();\n-    EasyMock.expect(request.getParameter(bogusKey)).andReturn(bogusValue).atLeastOnce();\n-    EasyMock.replay(request);\n-    String actualValue = Basic.getStringParameter(request, safeKey, null);\n-    assertEquals(safeValue, actualValue);\n-    actualValue = Basic.getStringParameter(request, bogusKey, null);\n-    assertEquals(sanitizedBogusValue, actualValue);\n-    EasyMock.verify(request);\n-  }\n-}",
                "additions": 0,
                "raw_url": "https://github.com/apache/accumulo/raw/328ffa0849981e0f113dfbf539c832b447e06902/server/monitor/src/test/java/org/apache/accumulo/monitor/servlets/trace/BasicTest.java",
                "status": "removed",
                "changes": 55,
                "deletions": 55,
                "sha": "c9174734024e0ad0e3c8dabd2f36ea0467e40c5d",
                "blob_url": "https://github.com/apache/accumulo/blob/328ffa0849981e0f113dfbf539c832b447e06902/server/monitor/src/test/java/org/apache/accumulo/monitor/servlets/trace/BasicTest.java",
                "filename": "server/monitor/src/test/java/org/apache/accumulo/monitor/servlets/trace/BasicTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/test/java/org/apache/accumulo/monitor/servlets/trace/BasicTest.java?ref=328ffa0849981e0f113dfbf539c832b447e06902"
            }
        ],
        "bug_id": "accumulo_1",
        "parent": "https://github.com/apache/accumulo/commit/328ffa0849981e0f113dfbf539c832b447e06902",
        "message": "Fix #1401, trace servlet parameter handling (#1409)\n\n* corrected servlet parameter handling so that it does not break things\r\n\r\nThis resolves #1401.\r\n\r\nI removed the previous implementation of  string sanitization, replacing it with more specific checks at the places where the parameters are used.\r\n\r\nBasicTest was deleted because it was exclusively testing the broken\r\nsanitization mechanism.\r\n\r\n* Update server/monitor/src/main/java/org/apache/accumulo/monitor/servlets/trace/ShowTrace.java\r\n\r\nAdded null check to avoid NPE\r\n\r\nCo-Authored-By: Christopher Tubbs <ctubbsii@apache.org>\r\n\r\n* automatically adjusted formatting",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/b52c16576e45c59190f8334301ccb120e7c627d5",
        "file": [
            {
                "patch": "@@ -18,12 +18,15 @@\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n@@ -194,69 +197,109 @@ public void changeTableStateTest() throws Exception {\n    * Validate the the AdminUtil.getStatus works correctly after refactor and validate that\n    * getTransactionStatus can be called without lock map(s). The test starts a long running fate\n    * transaction (slow compaction) and the calls AdminUtil functions to get the FATE.\n-   *\n-   * @throws Exception\n-   *           any exception is a test failure\n    */\n   @Test\n-  public void getFateStatus() throws Exception {\n+  public void getFateStatus() {\n \n-    assertEquals(\"verify table online after created\", TableState.ONLINE, getTableState(tableName));\n+    Instance instance = connector.getInstance();\n+    String tableId;\n+\n+    try {\n+\n+      assertEquals(\"verify table online after created\", TableState.ONLINE,\n+          getTableState(tableName));\n+\n+      tableId = Tables.getTableId(instance, tableName);\n+\n+      log.trace(\"tid: {}\", tableId);\n+\n+    } catch (TableNotFoundException ex) {\n+      throw new IllegalStateException(\n+          String.format(\"Table %s does not exist, failing test\", tableName));\n+    }\n \n     Future<?> compactTask = startCompactTask();\n \n     assertTrue(\"compaction fate transaction exits\", findFate(tableName));\n \n-    Instance instance = connector.getInstance();\n+    AdminUtil.FateStatus withLocks = null;\n+    List<AdminUtil.TransactionStatus> noLocks = null;\n+\n+    int maxRetries = 3;\n+\n     AdminUtil<String> admin = new AdminUtil<>(false);\n \n-    try {\n+    while (maxRetries > 0) {\n \n-      String tableId = Tables.getTableId(instance, tableName);\n+      try {\n \n-      log.trace(\"tid: {}\", tableId);\n+        IZooReaderWriter zk = new ZooReaderWriterFactory().getZooReaderWriter(\n+            instance.getZooKeepers(), instance.getZooKeepersSessionTimeOut(), secret);\n \n-      IZooReaderWriter zk = new ZooReaderWriterFactory().getZooReaderWriter(\n-          instance.getZooKeepers(), instance.getZooKeepersSessionTimeOut(), secret);\n-      ZooStore<String> zs = new ZooStore<>(ZooUtil.getRoot(instance) + Constants.ZFATE, zk);\n+        ZooStore<String> zs = new ZooStore<>(ZooUtil.getRoot(instance) + Constants.ZFATE, zk);\n \n-      AdminUtil.FateStatus withLocks = admin.getStatus(zs, zk,\n-          ZooUtil.getRoot(instance) + Constants.ZTABLE_LOCKS + \"/\" + tableId, null, null);\n+        withLocks = admin.getStatus(zs, zk,\n+            ZooUtil.getRoot(instance) + Constants.ZTABLE_LOCKS + \"/\" + tableId, null, null);\n \n-      // call method that does not use locks.\n-      List<AdminUtil.TransactionStatus> noLocks = admin.getTransactionStatus(zs, null, null);\n+        // call method that does not use locks.\n+        noLocks = admin.getTransactionStatus(zs, null, null);\n \n-      // fast check - count number of transactions\n-      assertEquals(withLocks.getTransactions().size(), noLocks.size());\n+        // no zk exception, no need to retry\n+        break;\n \n-      int matchCount = 0;\n+      } catch (InterruptedException ex) {\n+        Thread.currentThread().interrupt();\n+        fail(\"Interrupt received - test failed\");\n+        return;\n+      } catch (KeeperException ex) {\n+        maxRetries--;\n+        try {\n+          Thread.sleep(1000);\n+        } catch (InterruptedException intr_ex) {\n+          Thread.currentThread().interrupt();\n+          return;\n+        }\n+      }\n+    }\n \n-      for (AdminUtil.TransactionStatus tx : withLocks.getTransactions()) {\n+    assertNotNull(withLocks);\n+    assertNotNull(noLocks);\n \n-        log.trace(\"Fate id: {}, status: {}\", tx.getTxid(), tx.getStatus());\n+    // fast check - count number of transactions\n+    assertEquals(withLocks.getTransactions().size(), noLocks.size());\n+\n+    int matchCount = 0;\n \n-        if (tx.getTop().contains(\"CompactionDriver\") && tx.getDebug().contains(\"CompactRange\")) {\n+    for (AdminUtil.TransactionStatus tx : withLocks.getTransactions()) {\n \n-          for (AdminUtil.TransactionStatus tx2 : noLocks) {\n-            if (tx2.getTxid().equals(tx.getTxid())) {\n-              matchCount++;\n-            }\n+      if (isCompaction(tx)) {\n+\n+        log.trace(\"Fate id: {}, status: {}\", tx.getTxid(), tx.getStatus());\n+\n+        for (AdminUtil.TransactionStatus tx2 : noLocks) {\n+          if (tx2.getTxid().equals(tx.getTxid())) {\n+            matchCount++;\n           }\n         }\n       }\n+    }\n \n-      assertTrue(\"Number of fates matches should be > 0\", matchCount > 0);\n+    assertTrue(\"Number of fates matches should be > 0\", matchCount > 0);\n \n-    } catch (KeeperException | TableNotFoundException | InterruptedException ex) {\n-      throw new IllegalStateException(ex);\n-    }\n+    try {\n \n-    // test complete, cancel compaction and move on.\n-    connector.tableOperations().cancelCompaction(tableName);\n+      // test complete, cancel compaction and move on.\n+      connector.tableOperations().cancelCompaction(tableName);\n \n-    // block if compaction still running\n-    compactTask.get();\n+      // block if compaction still running\n+      compactTask.get();\n \n+    } catch (InterruptedException ex) {\n+      Thread.currentThread().interrupt();\n+    } catch (TableNotFoundException | AccumuloSecurityException | AccumuloException\n+        | ExecutionException ex) {\n+      log.debug(\"Could not cancel compaction\", ex);\n+    }\n   }\n \n   /**\n@@ -340,11 +383,8 @@ private boolean findFate(final String tableName) {\n \n       for (AdminUtil.TransactionStatus tx : fateStatus.getTransactions()) {\n \n-        log.trace(\"Fate id: {}, status: {}\", tx.getTxid(), tx.getStatus());\n-\n-        if (tx.getTop().contains(\"CompactionDriver\") && tx.getDebug().contains(\"CompactRange\")) {\n+        if (isCompaction(tx))\n           return true;\n-        }\n       }\n \n     } catch (KeeperException | TableNotFoundException | InterruptedException ex) {\n@@ -355,6 +395,31 @@ private boolean findFate(final String tableName) {\n     return Boolean.FALSE;\n   }\n \n+  /**\n+   * Test that the transaction top contains \"CompactionDriver\" and the debug message contains\n+   * \"CompactRange\"\n+   *\n+   * @param tx\n+   *          transaction status\n+   * @return true if tx top and debug have compaction messages.\n+   */\n+  private boolean isCompaction(AdminUtil.TransactionStatus tx) {\n+\n+    if (tx == null) {\n+      log.trace(\"Fate tx is null\");\n+      return false;\n+    }\n+\n+    log.trace(\"Fate id: {}, status: {}\", tx.getTxid(), tx.getStatus());\n+\n+    String top = tx.getTop();\n+    String debug = tx.getDebug();\n+\n+    return top != null && debug != null && top.contains(\"CompactionDriver\")\n+        && tx.getDebug().contains(\"CompactRange\");\n+\n+  }\n+\n   /**\n    * Returns the current table state (ONLINE, OFFLINE,...) of named table.\n    *\n@@ -400,19 +465,11 @@ private void createData(final String tableName) {\n \n       long startTimestamp = System.nanoTime();\n \n-      Scanner scanner = connector.createScanner(tableName, Authorizations.EMPTY);\n-      int count = 0;\n-      for (Map.Entry<Key,Value> elt : scanner) {\n-        String expected = String.format(\"%05d\", count);\n-        assert (elt.getKey().getRow().toString().equals(expected));\n-        count++;\n-      }\n+      int count = scanCount(tableName);\n \n       log.trace(\"Scan time for {} rows {} ms\", NUM_ROWS, TimeUnit.MILLISECONDS\n           .convert((System.nanoTime() - startTimestamp), TimeUnit.NANOSECONDS));\n \n-      scanner.close();\n-\n       if (count != NUM_ROWS) {\n         throw new IllegalStateException(\n             String.format(\"Number of rows %1$d does not match expected %2$d\", count, NUM_ROWS));\n@@ -423,6 +480,21 @@ private void createData(final String tableName) {\n     }\n   }\n \n+  private int scanCount(String tableName) throws TableNotFoundException {\n+\n+    Scanner scanner = connector.createScanner(tableName, Authorizations.EMPTY);\n+    int count = 0;\n+    for (Map.Entry<Key,Value> elt : scanner) {\n+      String expected = String.format(\"%05d\", count);\n+      assert (elt.getKey().getRow().toString().equals(expected));\n+      count++;\n+    }\n+\n+    scanner.close();\n+\n+    return count;\n+  }\n+\n   /**\n    * Provides timing information for online operation.\n    */\n@@ -533,14 +605,7 @@ public void run() {\n \n         // validate expected data created and exists in table.\n \n-        Scanner scanner = connector.createScanner(tableName, Authorizations.EMPTY);\n-\n-        int count = 0;\n-        for (Map.Entry<Key,Value> elt : scanner) {\n-          String expected = String.format(\"%05d\", count);\n-          assert (elt.getKey().getRow().toString().equals(expected));\n-          count++;\n-        }\n+        int count = scanCount(tableName);\n \n         log.trace(\"After compaction, scan time for {} rows {} ms\", NUM_ROWS, TimeUnit.MILLISECONDS\n             .convert((System.nanoTime() - startTimestamp), TimeUnit.NANOSECONDS));",
                "additions": 120,
                "raw_url": "https://github.com/apache/accumulo/raw/b52c16576e45c59190f8334301ccb120e7c627d5/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java",
                "status": "modified",
                "changes": 175,
                "deletions": 55,
                "sha": "dc1c225eb5c39bcf1920b8b12211ceb634b3c1c8",
                "blob_url": "https://github.com/apache/accumulo/blob/b52c16576e45c59190f8334301ccb120e7c627d5/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/functional/FateConcurrencyIT.java?ref=b52c16576e45c59190f8334301ccb120e7c627d5"
            }
        ],
        "bug_id": "accumulo_2",
        "parent": "https://github.com/apache/accumulo/commit/382c0a5b461720bc5e2ca1b29d5c22cd740b7b92",
        "message": "Improve FateConcurrencyIT test (#1061)\n\n- Removes NPE that sometimes caused the test to fail during release testing.",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/8a85a3847bf77a11569f115eda17c7b7045431d3",
        "file": [
            {
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.accumulo.core.crypto.CryptoUtils;\n import org.apache.accumulo.core.cryptoImpl.CryptoEnvironmentImpl;\n import org.apache.accumulo.core.cryptoImpl.NoFileDecrypter;\n+import org.apache.accumulo.core.cryptoImpl.NoFileEncrypter;\n import org.apache.accumulo.core.file.rfile.bcfile.Compression.Algorithm;\n import org.apache.accumulo.core.file.rfile.bcfile.Utils.Version;\n import org.apache.accumulo.core.file.streams.BoundedRangeFileInputStream;\n@@ -634,6 +635,7 @@ public long getRawSize() {\n       // backwards compatibility\n       if (version.equals(API_VERSION_1)) {\n         LOG.trace(\"Found a version 1 file to read.\");\n+        decryptionParams = new NoFileEncrypter().getDecryptionParameters();\n         this.decrypter = new NoFileDecrypter();\n       } else {\n         // read crypto parameters and get decrypter",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/8a85a3847bf77a11569f115eda17c7b7045431d3/core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BCFile.java",
                "status": "modified",
                "changes": 2,
                "deletions": 0,
                "sha": "d19db5beb18c5aa58ec4d5029924c087990168d0",
                "blob_url": "https://github.com/apache/accumulo/blob/8a85a3847bf77a11569f115eda17c7b7045431d3/core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BCFile.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BCFile.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/file/rfile/bcfile/BCFile.java?ref=8a85a3847bf77a11569f115eda17c7b7045431d3"
            },
            {
                "patch": "@@ -1702,28 +1702,28 @@ public void testReseekUnconsumed() throws Exception {\n \n   @Test(expected = NullPointerException.class)\n   public void testMissingUnreleasedVersions() throws Exception {\n-    runVersionTest(5, DefaultConfiguration.getInstance());\n+    runVersionTest(5, getAccumuloConfig(CryptoTest.CRYPTO_OFF_CONF));\n   }\n \n   @Test\n   public void testOldVersions() throws Exception {\n-    AccumuloConfiguration defaultConfiguration = DefaultConfiguration.getInstance();\n-    runVersionTest(3, defaultConfiguration);\n-    runVersionTest(4, defaultConfiguration);\n-    runVersionTest(6, defaultConfiguration);\n-    runVersionTest(7, defaultConfiguration);\n+    ConfigurationCopy defaultConf = getAccumuloConfig(CryptoTest.CRYPTO_OFF_CONF);\n+    runVersionTest(3, defaultConf);\n+    runVersionTest(4, defaultConf);\n+    runVersionTest(6, defaultConf);\n+    runVersionTest(7, defaultConf);\n   }\n \n   @Test\n   public void testOldVersionsWithCrypto() throws Exception {\n-    AccumuloConfiguration cryptoOnConf = getAccumuloConfig(CryptoTest.CRYPTO_ON_CONF);\n+    ConfigurationCopy cryptoOnConf = getAccumuloConfig(CryptoTest.CRYPTO_ON_CONF);\n     runVersionTest(3, cryptoOnConf);\n     runVersionTest(4, cryptoOnConf);\n     runVersionTest(6, cryptoOnConf);\n     runVersionTest(7, cryptoOnConf);\n   }\n \n-  private void runVersionTest(int version, AccumuloConfiguration aconf) throws IOException {\n+  private void runVersionTest(int version, ConfigurationCopy aconf) throws Exception {\n     InputStream in = this.getClass().getClassLoader()\n         .getResourceAsStream(\"org/apache/accumulo/core/file/rfile/ver_\" + version + \".rf\");\n     ByteArrayOutputStream baos = new ByteArrayOutputStream();\n@@ -1735,8 +1735,15 @@ private void runVersionTest(int version, AccumuloConfiguration aconf) throws IOE\n     byte[] data = baos.toByteArray();\n     SeekableByteArrayInputStream bais = new SeekableByteArrayInputStream(data);\n     FSDataInputStream in2 = new FSDataInputStream(bais);\n+    aconf.set(Property.TSERV_CACHE_MANAGER_IMPL, LruBlockCacheManager.class.getName());\n+    aconf.set(Property.TSERV_DEFAULT_BLOCKSIZE, Long.toString(100000));\n+    aconf.set(Property.TSERV_DATACACHE_SIZE, Long.toString(100000000));\n+    aconf.set(Property.TSERV_INDEXCACHE_SIZE, Long.toString(100000000));\n+    BlockCacheManager manager = BlockCacheManagerFactory.getInstance(aconf);\n+    manager.start(new BlockCacheConfiguration(aconf));\n     CachableBuilder cb = new CachableBuilder().input(in2).length(data.length).conf(hadoopConf)\n-        .cryptoService(CryptoServiceFactory.newInstance(aconf, ClassloaderType.JAVA));\n+        .cryptoService(CryptoServiceFactory.newInstance(aconf, ClassloaderType.JAVA))\n+        .index(manager.getBlockCache(CacheType.INDEX)).data(manager.getBlockCache(CacheType.DATA));\n     Reader reader = new RFile.Reader(cb);\n     checkIndex(reader);\n \n@@ -1779,10 +1786,11 @@ private void runVersionTest(int version, AccumuloConfiguration aconf) throws IOE\n       assertFalse(iter.hasTop());\n     }\n \n+    manager.stop();\n     reader.close();\n   }\n \n-  public static AccumuloConfiguration getAccumuloConfig(String cryptoOn) {\n+  public static ConfigurationCopy getAccumuloConfig(String cryptoOn) {\n     ConfigurationCopy cfg = new ConfigurationCopy(DefaultConfiguration.getInstance());\n     switch (cryptoOn) {\n       case CryptoTest.CRYPTO_ON_CONF:",
                "additions": 18,
                "raw_url": "https://github.com/apache/accumulo/raw/8a85a3847bf77a11569f115eda17c7b7045431d3/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java",
                "status": "modified",
                "changes": 28,
                "deletions": 10,
                "sha": "9e70010b617080c0f99f99df2fd9c28b30d2224a",
                "blob_url": "https://github.com/apache/accumulo/blob/8a85a3847bf77a11569f115eda17c7b7045431d3/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java",
                "filename": "core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/test/java/org/apache/accumulo/core/file/rfile/RFileTest.java?ref=8a85a3847bf77a11569f115eda17c7b7045431d3"
            }
        ],
        "bug_id": "accumulo_3",
        "parent": "https://github.com/apache/accumulo/commit/b17d0ae6f9a5d6d15145ffbca9700d8fcbd6058b",
        "message": "Fix NPE in BCFile. Fixes #1021 (#1022)\n\n* Add cache to old RFile tests to test for this error",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/2de1f9028424d4c74415ffb95ab396c3a4a26881",
        "file": [
            {
                "patch": "@@ -184,7 +184,7 @@ private void splitPartiallyAndRecover(AccumuloServerContext context, KeyExtent e\n       MetadataTableUtil.finishSplit(high, highDatafileSizes, highDatafilesToRemove, context, zl);\n     }\n \n-    TabletServer.verifyTabletInformation(context, high, instance, null, \"127.0.0.1:0\", zl);\n+    TabletServer.verifyTabletInformation(context, high, instance, new TreeMap<>(), \"127.0.0.1:0\", zl);\n \n     if (steps >= 1) {\n       ensureTabletHasNoUnexpectedMetadataEntries(context, low, lowDatafileSizes);",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/2de1f9028424d4c74415ffb95ab396c3a4a26881/test/src/main/java/org/apache/accumulo/test/functional/SplitRecoveryIT.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "6865f6213b242d813a309c822d5594797d267e7b",
                "blob_url": "https://github.com/apache/accumulo/blob/2de1f9028424d4c74415ffb95ab396c3a4a26881/test/src/main/java/org/apache/accumulo/test/functional/SplitRecoveryIT.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/functional/SplitRecoveryIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/functional/SplitRecoveryIT.java?ref=2de1f9028424d4c74415ffb95ab396c3a4a26881"
            }
        ],
        "bug_id": "accumulo_4",
        "parent": "https://github.com/apache/accumulo/commit/814d28dfa74823f2cd63ec30c9729baaed68cf36",
        "message": "ACCUMULO-3331 Fix NPE bug in SplitRecoveryIT\n\nFix NPE in test due to previous Objects.requireNonNull check added to\nTabletServer.",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/208cfeaf57e910683cd0215ea2f5487a98dab22b",
        "file": [
            {
                "patch": "@@ -111,7 +111,7 @@\n     // Number of files per target we have to replicate\n     Map<ReplicationTarget,Long> targetCounts = new HashMap<>();\n \n-    Map<String,Table.ID> tableNameToId = Tables.getNameToIdMap(null);\n+    Map<String,Table.ID> tableNameToId = Tables.getNameToIdMap(conn.getInstance());\n     Map<Table.ID,String> tableIdToName = invert(tableNameToId);\n \n     for (String table : tops.list()) {",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/208cfeaf57e910683cd0215ea2f5487a98dab22b/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/replication/ReplicationResource.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "f256ebb91f8f5af4c0a45f00994a798332c92d78",
                "blob_url": "https://github.com/apache/accumulo/blob/208cfeaf57e910683cd0215ea2f5487a98dab22b/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/replication/ReplicationResource.java",
                "filename": "server/monitor/src/main/java/org/apache/accumulo/monitor/rest/replication/ReplicationResource.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/main/java/org/apache/accumulo/monitor/rest/replication/ReplicationResource.java?ref=208cfeaf57e910683cd0215ea2f5487a98dab22b"
            }
        ],
        "bug_id": "accumulo_5",
        "parent": "https://github.com/apache/accumulo/commit/6e10803de039b7e97b09a5930e2813896f49d223",
        "message": "ACCUMULO-4760 Fix NPE in Monitor replication rest",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/6a442154a12151fed83de549ef398a27a72ac8ab",
        "file": [
            {
                "patch": "@@ -30,9 +30,10 @@\n import org.apache.accumulo.core.conf.ConfigurationTypeHelper;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.security.ColumnVisibility;\n-import org.apache.accumulo.core.trace.TraceUtil;\n+import org.apache.htrace.NullScope;\n import org.apache.htrace.Sampler;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n import org.apache.log4j.Level;\n import org.apache.log4j.Logger;\n \n@@ -138,21 +139,15 @@ public void startDebugLogging() {\n   @Parameter(names = \"--keytab\", description = \"Kerberos keytab on the local filesystem\")\n   private String keytabPath = null;\n \n-  public void startTracing(String applicationName) {\n-    if (trace) {\n-      Trace.startSpan(applicationName, Sampler.ALWAYS);\n-    }\n-  }\n-\n-  public void stopTracing() {\n-    TraceUtil.off();\n+  public TraceScope parseArgsAndTrace(String programName, String[] args, Object... others) {\n+    parseArgs(programName, args, others);\n+    return trace ? Trace.startSpan(programName, Sampler.ALWAYS) : NullScope.INSTANCE;\n   }\n \n   @Override\n   public void parseArgs(String programName, String[] args, Object... others) {\n     super.parseArgs(programName, args, others);\n     startDebugLogging();\n-    startTracing(programName);\n   }\n \n   private Properties cachedProps = null;",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java",
                "status": "modified",
                "changes": 15,
                "deletions": 10,
                "sha": "bc39e00fb330f71db76275e993f851fba12da125",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -83,6 +83,7 @@\n import org.apache.commons.collections.map.LRUMap;\n import org.apache.commons.lang.mutable.MutableLong;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.Trace;\n import org.apache.thrift.TApplicationException;\n import org.apache.thrift.TException;\n import org.apache.thrift.TServiceClient;\n@@ -328,7 +329,7 @@ private void queue(String location, TabletServerMutations<QCMutation> mutations)\n       serverQueue.queue.add(mutations);\n       // never execute more than one task per server\n       if (!serverQueue.taskQueued) {\n-        threadPool.execute(new LoggingRunnable(log, TraceUtil.wrap(new SendTask(location))));\n+        threadPool.execute(new LoggingRunnable(log, Trace.wrap(new SendTask(location))));\n         serverQueue.taskQueued = true;\n       }\n     }\n@@ -347,7 +348,7 @@ private void reschedule(SendTask task) {\n \n     synchronized (serverQueue) {\n       if (serverQueue.queue.size() > 0)\n-        threadPool.execute(new LoggingRunnable(log, TraceUtil.wrap(task)));\n+        threadPool.execute(new LoggingRunnable(log, Trace.wrap(task)));\n       else\n         serverQueue.taskQueued = false;\n     }",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "0561c1917b5f0e98b8a1cefde65cc84afacca4c8",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -715,7 +715,7 @@ else if (Tables.getTableState(context, tableId) == TableState.OFFLINE)\n     void queueMutations(final MutationSet mutationsToSend) {\n       if (mutationsToSend == null)\n         return;\n-      binningThreadPool.execute(TraceUtil.wrap(() -> {\n+      binningThreadPool.execute(Trace.wrap(() -> {\n         if (mutationsToSend != null) {\n           try {\n             log.trace(\"{} - binning {} mutations\", Thread.currentThread().getName(),\n@@ -777,7 +777,7 @@ private synchronized void addMutations(\n \n       for (String server : servers)\n         if (!queued.contains(server)) {\n-          sendThreadPool.submit(TraceUtil.wrap(new SendTask(server)));\n+          sendThreadPool.submit(Trace.wrap(new SendTask(server)));\n           queued.add(server);\n         }\n     }",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "ad7b29bf4d206229fad2e3d89196f9198b229aad",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -16,8 +16,6 @@\n  */\n package org.apache.accumulo.core.clientImpl;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import java.io.IOException;\n import java.security.SecureRandom;\n import java.util.ArrayList;\n@@ -316,8 +314,7 @@ else if (log.isTraceEnabled())\n \n         try (TraceScope scanLocation = Trace.startSpan(\"scan:location\")) {\n           if (scanLocation.getSpan() != null) {\n-            scanLocation.getSpan().addKVAnnotation(\"tserver\".getBytes(UTF_8),\n-                loc.tablet_location.getBytes(UTF_8));\n+            scanLocation.getSpan().addKVAnnotation(\"tserver\", loc.tablet_location);\n           }\n           results = scan(loc, scanState, context);\n         } catch (AccumuloSecurityException e) {",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java",
                "status": "modified",
                "changes": 5,
                "deletions": 4,
                "sha": "7ca36cfeeccf8d74dc667eb10c953eee0922857a",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -42,10 +42,8 @@\n import org.apache.htrace.Trace;\n import org.apache.htrace.TraceInfo;\n import org.apache.htrace.TraceScope;\n-import org.apache.htrace.Tracer;\n import org.apache.htrace.impl.CountSampler;\n import org.apache.htrace.impl.ProbabilitySampler;\n-import org.apache.htrace.wrappers.TraceRunnable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -116,9 +114,7 @@ private static void enableTracing(String hostname, String service, String spanRe\n     if (service != null) {\n       htraceConfigProps.put(TRACE_SERVICE_PROPERTY, service);\n     }\n-    Trace.setProcessId(service);\n     ShutdownHookManager.get().addShutdownHook(() -> {\n-      TraceUtil.off();\n       disable();\n     }, 0);\n     synchronized (receivers) {\n@@ -163,19 +159,6 @@ public static void disable() {\n     }\n   }\n \n-  /**\n-   * Finish the current trace.\n-   */\n-  public static void off() {\n-    Span span = Trace.currentSpan();\n-    if (span != null) {\n-      span.stop();\n-      // close() will no-op, but ensure safety if the implementation changes\n-      // the main reason for doing the following is to remove the current span in the current thread\n-      Tracer.getInstance().continueSpan(null).close();\n-    }\n-  }\n-\n   /**\n    * Continue a trace by starting a new span with a given parent and description.\n    */\n@@ -184,17 +167,6 @@ public static TraceScope trace(TInfo info, String description) {\n         : Trace.startSpan(description, new TraceInfo(info.traceId, info.parentId));\n   }\n \n-  /**\n-   * Wrap a runnable in a TraceRunnable, if tracing.\n-   */\n-  public static Runnable wrap(Runnable runnable) {\n-    if (Trace.isTracing()) {\n-      return new TraceRunnable(Trace.currentSpan(), runnable);\n-    } else {\n-      return runnable;\n-    }\n-  }\n-\n   private static final TInfo DONT_TRACE = new TInfo(0, 0);\n \n   /**",
                "additions": 0,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java",
                "status": "modified",
                "changes": 28,
                "deletions": 28,
                "sha": "16c73fa802e6b3edfe8c8236d1a38c6de22c8a09",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -79,24 +80,25 @@ public Text convert(String value) {\n \n   public void start(String[] args) throws MergeException {\n     Opts opts = new Opts();\n-    opts.parseArgs(Merge.class.getName(), args);\n+    try (TraceScope clientTrace = opts.parseArgsAndTrace(Merge.class.getName(), args)) {\n \n-    try (AccumuloClient client = opts.createClient()) {\n+      try (AccumuloClient client = opts.createClient()) {\n \n-      if (!client.tableOperations().exists(opts.getTableName())) {\n-        System.err.println(\"table \" + opts.getTableName() + \" does not exist\");\n-        return;\n-      }\n-      if (opts.goalSize == null || opts.goalSize < 1) {\n-        AccumuloConfiguration tableConfig = new ConfigurationCopy(\n-            client.tableOperations().getProperties(opts.getTableName()));\n-        opts.goalSize = tableConfig.getAsBytes(Property.TABLE_SPLIT_THRESHOLD);\n-      }\n+        if (!client.tableOperations().exists(opts.getTableName())) {\n+          System.err.println(\"table \" + opts.getTableName() + \" does not exist\");\n+          return;\n+        }\n+        if (opts.goalSize == null || opts.goalSize < 1) {\n+          AccumuloConfiguration tableConfig = new ConfigurationCopy(\n+              client.tableOperations().getProperties(opts.getTableName()));\n+          opts.goalSize = tableConfig.getAsBytes(Property.TABLE_SPLIT_THRESHOLD);\n+        }\n \n-      message(\"Merging tablets in table %s to %d bytes\", opts.getTableName(), opts.goalSize);\n-      mergomatic(client, opts.getTableName(), opts.begin, opts.end, opts.goalSize, opts.force);\n-    } catch (Exception ex) {\n-      throw new MergeException(ex);\n+        message(\"Merging tablets in table %s to %d bytes\", opts.getTableName(), opts.goalSize);\n+        mergomatic(client, opts.getTableName(), opts.begin, opts.end, opts.goalSize, opts.force);\n+      } catch (Exception ex) {\n+        throw new MergeException(ex);\n+      }\n     }\n   }\n ",
                "additions": 17,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/util/Merge.java",
                "status": "modified",
                "changes": 32,
                "deletions": 15,
                "sha": "9865782ccee7dc97d7d22b5392e046bbcb0e06f3",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/util/Merge.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/util/Merge.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/Merge.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -126,7 +126,7 @@\n     <hadoop.version>3.1.1</hadoop.version>\n     <hk2.version>2.5.0-b62</hk2.version>\n     <htrace.hadoop.version>4.1.0-incubating</htrace.hadoop.version>\n-    <htrace.version>3.1.0-incubating</htrace.version>\n+    <htrace.version>3.2.0-incubating</htrace.version>\n     <it.failIfNoSpecifiedTests>false</it.failIfNoSpecifiedTests>\n     <jackson.version>2.9.8</jackson.version>\n     <javax.el.version>2.2.4</javax.el.version>",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/pom.xml",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "df5140346b35e4f8fc2afc90fe1c069767a1212f",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/pom.xml",
                "filename": "pom.xml",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/pom.xml?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.htrace.TraceScope;\n import org.apache.zookeeper.ZooDefs.Ids;\n import org.apache.zookeeper.data.ACL;\n import org.apache.zookeeper.data.Stat;\n@@ -69,19 +70,21 @@ public static void main(String[] args) throws Exception {\n     argsList.add(\"--old\");\n     argsList.add(\"--new\");\n     argsList.addAll(Arrays.asList(args));\n-    opts.parseArgs(ChangeSecret.class.getName(), argsList.toArray(new String[0]));\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(ChangeSecret.class.getName(),\n+        argsList.toArray(new String[0]))) {\n \n-    ServerContext context = opts.getServerContext();\n-    verifyAccumuloIsDown(context, opts.oldPass);\n+      ServerContext context = opts.getServerContext();\n+      verifyAccumuloIsDown(context, opts.oldPass);\n \n-    final String newInstanceId = UUID.randomUUID().toString();\n-    updateHdfs(fs, newInstanceId);\n-    rewriteZooKeeperInstance(context, newInstanceId, opts.oldPass, opts.newPass);\n-    if (opts.oldPass != null) {\n-      deleteInstance(context, opts.oldPass);\n+      final String newInstanceId = UUID.randomUUID().toString();\n+      updateHdfs(fs, newInstanceId);\n+      rewriteZooKeeperInstance(context, newInstanceId, opts.oldPass, opts.newPass);\n+      if (opts.oldPass != null) {\n+        deleteInstance(context, opts.oldPass);\n+      }\n+      System.out.println(\"New instance id is \" + newInstanceId);\n+      System.out.println(\"Be sure to put your new secret in accumulo.properties\");\n     }\n-    System.out.println(\"New instance id is \" + newInstanceId);\n-    System.out.println(\"Be sure to put your new secret in accumulo.properties\");\n   }\n \n   interface Visitor {",
                "additions": 13,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java",
                "status": "modified",
                "changes": 23,
                "deletions": 10,
                "sha": "3b7f16415c74923055236971a63e51e447ca83e1",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.server.cli.ServerUtilOpts;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n \n public class CheckForMetadataProblems {\n   private static boolean sawProblems = false;\n@@ -165,13 +166,14 @@ public static void checkMetadataAndRootTableEntries(String tableNameToCheck, Ser\n \n   public static void main(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(CheckForMetadataProblems.class.getName(), args);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(CheckForMetadataProblems.class.getName(),\n+        args)) {\n \n-    checkMetadataAndRootTableEntries(RootTable.NAME, opts);\n-    checkMetadataAndRootTableEntries(MetadataTable.NAME, opts);\n-    opts.stopTracing();\n-    if (sawProblems)\n-      throw new RuntimeException();\n+      checkMetadataAndRootTableEntries(RootTable.NAME, opts);\n+      checkMetadataAndRootTableEntries(MetadataTable.NAME, opts);\n+      if (sawProblems)\n+        throw new RuntimeException();\n+    }\n   }\n \n }",
                "additions": 8,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java",
                "status": "modified",
                "changes": 14,
                "deletions": 6,
                "sha": "0fe3fdf44ef7e7cd1d70fb62ccb95d125ad55705",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.accumulo.server.master.state.TabletLocationState;\n import org.apache.accumulo.server.master.state.TabletState;\n import org.apache.accumulo.server.master.state.ZooTabletStateStore;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -46,9 +47,10 @@\n \n   public static void main(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(FindOfflineTablets.class.getName(), args);\n-    ServerContext context = opts.getServerContext();\n-    findOffline(context, null);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(FindOfflineTablets.class.getName(), args)) {\n+      ServerContext context = opts.getServerContext();\n+      findOffline(context, null);\n+    }\n   }\n \n   static int findOffline(ServerContext context, String tableName) throws TableNotFoundException {",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "eb8c2ec8d73dc5920a9bc34dee1223ad6f7c8dbd",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -37,46 +37,48 @@\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.htrace.TraceScope;\n \n public class LocalityCheck {\n \n   public int run(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(LocalityCheck.class.getName(), args);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(LocalityCheck.class.getName(), args)) {\n \n-    VolumeManager fs = opts.getServerContext().getVolumeManager();\n-    try (AccumuloClient accumuloClient = opts.createClient()) {\n-      Scanner scanner = accumuloClient.createScanner(MetadataTable.NAME, Authorizations.EMPTY);\n-      scanner.fetchColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME);\n-      scanner.fetchColumnFamily(DataFileColumnFamily.NAME);\n-      scanner.setRange(MetadataSchema.TabletsSection.getRange());\n+      VolumeManager fs = opts.getServerContext().getVolumeManager();\n+      try (AccumuloClient accumuloClient = opts.createClient()) {\n+        Scanner scanner = accumuloClient.createScanner(MetadataTable.NAME, Authorizations.EMPTY);\n+        scanner.fetchColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME);\n+        scanner.fetchColumnFamily(DataFileColumnFamily.NAME);\n+        scanner.setRange(MetadataSchema.TabletsSection.getRange());\n \n-      Map<String,Long> totalBlocks = new HashMap<>();\n-      Map<String,Long> localBlocks = new HashMap<>();\n-      ArrayList<String> files = new ArrayList<>();\n+        Map<String,Long> totalBlocks = new HashMap<>();\n+        Map<String,Long> localBlocks = new HashMap<>();\n+        ArrayList<String> files = new ArrayList<>();\n \n-      for (Entry<Key,Value> entry : scanner) {\n-        Key key = entry.getKey();\n-        if (key.compareColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME) == 0) {\n-          String location = entry.getValue().toString();\n-          String[] parts = location.split(\":\");\n-          String host = parts[0];\n-          addBlocks(fs, host, files, totalBlocks, localBlocks);\n-          files.clear();\n-        } else if (key.compareColumnFamily(DataFileColumnFamily.NAME) == 0) {\n+        for (Entry<Key,Value> entry : scanner) {\n+          Key key = entry.getKey();\n+          if (key.compareColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME) == 0) {\n+            String location = entry.getValue().toString();\n+            String[] parts = location.split(\":\");\n+            String host = parts[0];\n+            addBlocks(fs, host, files, totalBlocks, localBlocks);\n+            files.clear();\n+          } else if (key.compareColumnFamily(DataFileColumnFamily.NAME) == 0) {\n \n-          files.add(fs.getFullPath(key).toString());\n+            files.add(fs.getFullPath(key).toString());\n+          }\n+        }\n+        System.out.println(\" Server         %local  total blocks\");\n+        for (Entry<String,Long> entry : totalBlocks.entrySet()) {\n+          final String host = entry.getKey();\n+          final Long blocksForHost = entry.getValue();\n+          System.out.println(String.format(\"%15s %5.1f %8d\", host,\n+              (localBlocks.get(host) * 100.) / blocksForHost, blocksForHost));\n         }\n       }\n-      System.out.println(\" Server         %local  total blocks\");\n-      for (Entry<String,Long> entry : totalBlocks.entrySet()) {\n-        final String host = entry.getKey();\n-        final Long blocksForHost = entry.getValue();\n-        System.out.println(String.format(\"%15s %5.1f %8d\", host,\n-            (localBlocks.get(host) * 100.) / blocksForHost, blocksForHost));\n-      }\n+      return 0;\n     }\n-    return 0;\n   }\n \n   private void addBlocks(VolumeManager fs, String host, ArrayList<String> files,",
                "additions": 30,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java",
                "status": "modified",
                "changes": 58,
                "deletions": 28,
                "sha": "8998323739333a81dd425d05f4f062dffc1ecf3e",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Value;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -96,24 +97,26 @@ public static void main(String[] args) throws Exception {\n     Opts opts = new Opts(table_name);\n     opts.setPrincipal(\"root\");\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n-    opts.parseArgs(RandomWriter.class.getName(), args, bwOpts);\n-\n-    long start = System.currentTimeMillis();\n-    log.info(\"starting at {} for user {}\", start, opts.getPrincipal());\n-    try (AccumuloClient accumuloClient = opts.createClient()) {\n-      BatchWriter bw = accumuloClient.createBatchWriter(opts.getTableName(),\n-          bwOpts.getBatchWriterConfig());\n-      log.info(\"Writing {} mutations...\", opts.count);\n-      bw.addMutations(new RandomMutationGenerator(opts.count));\n-      bw.close();\n-    } catch (Exception e) {\n-      log.error(\"{}\", e.getMessage(), e);\n-      throw e;\n-    }\n-    long stop = System.currentTimeMillis();\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(RandomWriter.class.getName(), args,\n+        bwOpts)) {\n+\n+      long start = System.currentTimeMillis();\n+      log.info(\"starting at {} for user {}\", start, opts.getPrincipal());\n+      try (AccumuloClient accumuloClient = opts.createClient()) {\n+        BatchWriter bw = accumuloClient.createBatchWriter(opts.getTableName(),\n+            bwOpts.getBatchWriterConfig());\n+        log.info(\"Writing {} mutations...\", opts.count);\n+        bw.addMutations(new RandomMutationGenerator(opts.count));\n+        bw.close();\n+      } catch (Exception e) {\n+        log.error(\"{}\", e.getMessage(), e);\n+        throw e;\n+      }\n+      long stop = System.currentTimeMillis();\n \n-    log.info(\"stopping at {}\", stop);\n-    log.info(\"elapsed: {}\", (((double) stop - (double) start) / 1000.0));\n+      log.info(\"stopping at {}\", stop);\n+      log.info(\"elapsed: {}\", (((double) stop - (double) start) / 1000.0));\n+    }\n   }\n \n }",
                "additions": 20,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java",
                "status": "modified",
                "changes": 37,
                "deletions": 17,
                "sha": "56f60aa5012b6e54f381c9351babcedef0e864ea",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -54,14 +55,15 @@\n \n   public static void main(String[] args) {\n     ServerUtilOnRequiredTable opts = new ServerUtilOnRequiredTable();\n-    opts.parseArgs(RandomizeVolumes.class.getName(), args);\n-    ServerContext context = opts.getServerContext();\n-    try {\n-      int status = randomize(context, opts.getTableName());\n-      System.exit(status);\n-    } catch (Exception ex) {\n-      log.error(\"{}\", ex.getMessage(), ex);\n-      System.exit(4);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(RandomizeVolumes.class.getName(), args)) {\n+      ServerContext context = opts.getServerContext();\n+      try {\n+        int status = randomize(context, opts.getTableName());\n+        System.exit(status);\n+      } catch (Exception ex) {\n+        log.error(\"{}\", ex.getMessage(), ex);\n+        System.exit(4);\n+      }\n     }\n   }\n ",
                "additions": 10,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java",
                "status": "modified",
                "changes": 18,
                "deletions": 8,
                "sha": "e4cb695a9eab94fc611e2c81d2f8d34eb449a4f0",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -48,6 +48,7 @@\n import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.commons.collections.map.LRUMap;\n import org.apache.hadoop.fs.Path;\n+import org.apache.htrace.TraceScope;\n \n import com.beust.jcommander.Parameter;\n \n@@ -205,8 +206,9 @@ public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n     ScannerOpts scanOpts = new ScannerOpts();\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n-    opts.parseArgs(RemoveEntriesForMissingFiles.class.getName(), args, scanOpts, bwOpts);\n-\n-    checkAllTables(opts.getServerContext(), opts.fix);\n+    try (TraceScope clientSpan = opts\n+        .parseArgsAndTrace(RemoveEntriesForMissingFiles.class.getName(), args, scanOpts, bwOpts)) {\n+      checkAllTables(opts.getServerContext(), opts.fix);\n+    }\n   }\n }",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "ebc51135035cc4e9948a54056bf694f3c46d29df",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -296,10 +297,12 @@ public static void printDiskUsage(Collection<String> tableNames, VolumeManager f\n \n   public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n-    opts.parseArgs(TableDiskUsage.class.getName(), args);\n-    try (AccumuloClient client = opts.createClient()) {\n-      VolumeManager fs = opts.getServerContext().getVolumeManager();\n-      org.apache.accumulo.server.util.TableDiskUsage.printDiskUsage(opts.tables, fs, client, false);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(TableDiskUsage.class.getName(), args)) {\n+      try (AccumuloClient client = opts.createClient()) {\n+        VolumeManager fs = opts.getServerContext().getVolumeManager();\n+        org.apache.accumulo.server.util.TableDiskUsage.printDiskUsage(opts.tables, fs, client,\n+            false);\n+      }\n     }\n   }\n ",
                "additions": 7,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java",
                "status": "modified",
                "changes": 11,
                "deletions": 4,
                "sha": "fa36a5d6b3adae6e62674418f5e55a80f6c09fb2",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -53,6 +53,7 @@\n import org.apache.accumulo.core.util.HostAndPort;\n import org.apache.accumulo.server.cli.ServerUtilOpts;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.thrift.TException;\n import org.apache.thrift.TServiceClient;\n import org.slf4j.Logger;\n@@ -71,11 +72,12 @@\n \n   public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n-    opts.parseArgs(VerifyTabletAssignments.class.getName(), args);\n-\n-    try (AccumuloClient client = opts.createClient()) {\n-      for (String table : client.tableOperations().list())\n-        checkTable((ClientContext) client, opts, table, null);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(VerifyTabletAssignments.class.getName(),\n+        args)) {\n+      try (AccumuloClient client = opts.createClient()) {\n+        for (String table : client.tableOperations().list())\n+          checkTable((ClientContext) client, opts, table, null);\n+      }\n     }\n   }\n ",
                "additions": 7,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java",
                "status": "modified",
                "changes": 12,
                "deletions": 5,
                "sha": "4346eba41feb30bf2421488c9327b02fc37dceaf",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -522,67 +522,65 @@ private void run() {\n         .probabilitySampler(getConfiguration().getFraction(Property.GC_TRACE_PERCENT));\n \n     while (true) {\n-      Trace.startSpan(\"gc\", sampler);\n+      try (TraceScope gcOuterSpan = Trace.startSpan(\"gc\", sampler)) {\n+        try (TraceScope gcSpan = Trace.startSpan(\"loop\")) {\n+          tStart = System.currentTimeMillis();\n+          try {\n+            System.gc(); // make room\n \n-      try (TraceScope gcSpan = Trace.startSpan(\"loop\")) {\n-        tStart = System.currentTimeMillis();\n-        try {\n-          System.gc(); // make room\n+            status.current.started = System.currentTimeMillis();\n \n-          status.current.started = System.currentTimeMillis();\n+            new GarbageCollectionAlgorithm().collect(new GCEnv(RootTable.NAME));\n+            new GarbageCollectionAlgorithm().collect(new GCEnv(MetadataTable.NAME));\n \n-          new GarbageCollectionAlgorithm().collect(new GCEnv(RootTable.NAME));\n-          new GarbageCollectionAlgorithm().collect(new GCEnv(MetadataTable.NAME));\n+            log.info(\"Number of data file candidates for deletion: {}\", status.current.candidates);\n+            log.info(\"Number of data file candidates still in use: {}\", status.current.inUse);\n+            log.info(\"Number of successfully deleted data files: {}\", status.current.deleted);\n+            log.info(\"Number of data files delete failures: {}\", status.current.errors);\n \n-          log.info(\"Number of data file candidates for deletion: {}\", status.current.candidates);\n-          log.info(\"Number of data file candidates still in use: {}\", status.current.inUse);\n-          log.info(\"Number of successfully deleted data files: {}\", status.current.deleted);\n-          log.info(\"Number of data files delete failures: {}\", status.current.errors);\n+            status.current.finished = System.currentTimeMillis();\n+            status.last = status.current;\n+            status.current = new GcCycleStats();\n \n-          status.current.finished = System.currentTimeMillis();\n-          status.last = status.current;\n-          status.current = new GcCycleStats();\n+          } catch (Exception e) {\n+            log.error(\"{}\", e.getMessage(), e);\n+          }\n \n-        } catch (Exception e) {\n-          log.error(\"{}\", e.getMessage(), e);\n-        }\n+          tStop = System.currentTimeMillis();\n+          log.info(String.format(\"Collect cycle took %.2f seconds\", ((tStop - tStart) / 1000.0)));\n+\n+          /*\n+           * We want to prune references to fully-replicated WALs from the replication table which\n+           * are no longer referenced in the metadata table before running\n+           * GarbageCollectWriteAheadLogs to ensure we delete as many files as possible.\n+           */\n+          try (TraceScope replSpan = Trace.startSpan(\"replicationClose\")) {\n+            CloseWriteAheadLogReferences closeWals = new CloseWriteAheadLogReferences(context);\n+            closeWals.run();\n+          } catch (Exception e) {\n+            log.error(\"Error trying to close write-ahead logs for replication table\", e);\n+          }\n \n-        tStop = System.currentTimeMillis();\n-        log.info(String.format(\"Collect cycle took %.2f seconds\", ((tStop - tStart) / 1000.0)));\n-\n-        /*\n-         * We want to prune references to fully-replicated WALs from the replication table which are\n-         * no longer referenced in the metadata table before running GarbageCollectWriteAheadLogs to\n-         * ensure we delete as many files as possible.\n-         */\n-        try (TraceScope replSpan = Trace.startSpan(\"replicationClose\")) {\n-          CloseWriteAheadLogReferences closeWals = new CloseWriteAheadLogReferences(context);\n-          closeWals.run();\n-        } catch (Exception e) {\n-          log.error(\"Error trying to close write-ahead logs for replication table\", e);\n+          // Clean up any unused write-ahead logs\n+          try (TraceScope waLogs = Trace.startSpan(\"walogs\")) {\n+            GarbageCollectWriteAheadLogs walogCollector = new GarbageCollectWriteAheadLogs(context,\n+                fs, isUsingTrash());\n+            log.info(\"Beginning garbage collection of write-ahead logs\");\n+            walogCollector.collect(status);\n+          } catch (Exception e) {\n+            log.error(\"{}\", e.getMessage(), e);\n+          }\n         }\n \n-        // Clean up any unused write-ahead logs\n-        try (TraceScope waLogs = Trace.startSpan(\"walogs\")) {\n-          GarbageCollectWriteAheadLogs walogCollector = new GarbageCollectWriteAheadLogs(context,\n-              fs, isUsingTrash());\n-          log.info(\"Beginning garbage collection of write-ahead logs\");\n-          walogCollector.collect(status);\n+        // we just made a lot of metadata changes: flush them out\n+        try {\n+          AccumuloClient accumuloClient = getClient();\n+          accumuloClient.tableOperations().compact(MetadataTable.NAME, null, null, true, true);\n+          accumuloClient.tableOperations().compact(RootTable.NAME, null, null, true, true);\n         } catch (Exception e) {\n-          log.error(\"{}\", e.getMessage(), e);\n+          log.warn(\"{}\", e.getMessage(), e);\n         }\n       }\n-\n-      // we just made a lot of metadata changes: flush them out\n-      try {\n-        AccumuloClient accumuloClient = getClient();\n-        accumuloClient.tableOperations().compact(MetadataTable.NAME, null, null, true, true);\n-        accumuloClient.tableOperations().compact(RootTable.NAME, null, null, true, true);\n-      } catch (Exception e) {\n-        log.warn(\"{}\", e.getMessage(), e);\n-      }\n-\n-      TraceUtil.off();\n       try {\n         long gcDelay = getConfiguration().getTimeInMillis(Property.GC_CYCLE_DELAY);\n         log.debug(\"Sleeping for {} milliseconds\", gcDelay);",
                "additions": 47,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java",
                "status": "modified",
                "changes": 96,
                "deletions": 49,
                "sha": "4f9e72ac0aab1a7e869ccc875121505093353f78",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java",
                "filename": "server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.accumulo.fate.util.UtilWaitThread;\n import org.apache.accumulo.master.Master;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n import org.apache.htrace.impl.ProbabilitySampler;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -70,43 +71,44 @@ public void run() {\n         rcrr = new RemoveCompleteReplicationRecords(client);\n       }\n \n-      Trace.startSpan(\"masterReplicationDriver\", sampler);\n+      try (TraceScope replicationDriver = Trace.startSpan(\"masterReplicationDriver\", sampler)) {\n+\n+        // Make status markers from replication records in metadata, removing entries in\n+        // metadata which are no longer needed (closed records)\n+        // This will end up creating the replication table too\n+        try {\n+          statusMaker.run();\n+        } catch (Exception e) {\n+          log.error(\"Caught Exception trying to create Replication status records\", e);\n+        }\n+\n+        // Tell the work maker to make work\n+        try {\n+          workMaker.run();\n+        } catch (Exception e) {\n+          log.error(\"Caught Exception trying to create Replication work records\", e);\n+        }\n+\n+        // Update the status records from the work records\n+        try {\n+          finishedWorkUpdater.run();\n+        } catch (Exception e) {\n+          log.error(\n+              \"Caught Exception trying to update Replication records using finished work records\",\n+              e);\n+        }\n+\n+        // Clean up records we no longer need.\n+        // It must be running at the same time as the StatusMaker or WorkMaker\n+        // So it's important that we run these sequentially and not concurrently\n+        try {\n+          rcrr.run();\n+        } catch (Exception e) {\n+          log.error(\"Caught Exception trying to remove finished Replication records\", e);\n+        }\n \n-      // Make status markers from replication records in metadata, removing entries in\n-      // metadata which are no longer needed (closed records)\n-      // This will end up creating the replication table too\n-      try {\n-        statusMaker.run();\n-      } catch (Exception e) {\n-        log.error(\"Caught Exception trying to create Replication status records\", e);\n-      }\n-\n-      // Tell the work maker to make work\n-      try {\n-        workMaker.run();\n-      } catch (Exception e) {\n-        log.error(\"Caught Exception trying to create Replication work records\", e);\n       }\n \n-      // Update the status records from the work records\n-      try {\n-        finishedWorkUpdater.run();\n-      } catch (Exception e) {\n-        log.error(\n-            \"Caught Exception trying to update Replication records using finished work records\", e);\n-      }\n-\n-      // Clean up records we no longer need.\n-      // It must be running at the same time as the StatusMaker or WorkMaker\n-      // So it's important that we run these sequentially and not concurrently\n-      try {\n-        rcrr.run();\n-      } catch (Exception e) {\n-        log.error(\"Caught Exception trying to remove finished Replication records\", e);\n-      }\n-\n-      TraceUtil.off();\n-\n       // Sleep for a bit\n       long sleepMillis = conf.getTimeInMillis(Property.MASTER_REPLICATION_SCAN_INTERVAL);\n       log.debug(\"Sleeping for {}ms before re-running\", sleepMillis);",
                "additions": 36,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java",
                "status": "modified",
                "changes": 70,
                "deletions": 34,
                "sha": "f648006c5c7a956a07c9296516a7204fcf93e441",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java",
                "filename": "server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.accumulo.server.master.state.TabletState;\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.zookeeper.data.Stat;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -260,24 +261,25 @@ private boolean verifyMergeConsistency(AccumuloClient accumuloClient, CurrentSta\n \n   public static void main(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(MergeStats.class.getName(), args);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(MergeStats.class.getName(), args)) {\n \n-    try (AccumuloClient client = opts.createClient()) {\n-      Map<String,String> tableIdMap = client.tableOperations().tableIdMap();\n-      ZooReaderWriter zooReaderWriter = opts.getServerContext().getZooReaderWriter();\n-      for (Entry<String,String> entry : tableIdMap.entrySet()) {\n-        final String table = entry.getKey(), tableId = entry.getValue();\n-        String path = ZooUtil.getRoot(client.instanceOperations().getInstanceID())\n-            + Constants.ZTABLES + \"/\" + tableId + \"/merge\";\n-        MergeInfo info = new MergeInfo();\n-        if (zooReaderWriter.exists(path)) {\n-          byte[] data = zooReaderWriter.getData(path, new Stat());\n-          DataInputBuffer in = new DataInputBuffer();\n-          in.reset(data, data.length);\n-          info.readFields(in);\n+      try (AccumuloClient client = opts.createClient()) {\n+        Map<String,String> tableIdMap = client.tableOperations().tableIdMap();\n+        ZooReaderWriter zooReaderWriter = opts.getServerContext().getZooReaderWriter();\n+        for (Entry<String,String> entry : tableIdMap.entrySet()) {\n+          final String table = entry.getKey(), tableId = entry.getValue();\n+          String path = ZooUtil.getRoot(client.instanceOperations().getInstanceID())\n+              + Constants.ZTABLES + \"/\" + tableId + \"/merge\";\n+          MergeInfo info = new MergeInfo();\n+          if (zooReaderWriter.exists(path)) {\n+            byte[] data = zooReaderWriter.getData(path, new Stat());\n+            DataInputBuffer in = new DataInputBuffer();\n+            in.reset(data, data.length);\n+            info.readFields(in);\n+          }\n+          System.out.println(String.format(\"%25s  %10s %10s %s\", table, info.getState(),\n+              info.getOperation(), info.getExtent()));\n         }\n-        System.out.println(String.format(\"%25s  %10s %10s %s\", table, info.getState(),\n-            info.getOperation(), info.getExtent()));\n       }\n     }\n   }",
                "additions": 18,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java",
                "status": "modified",
                "changes": 34,
                "deletions": 16,
                "sha": "efd9422934e2e7c4f58a741e4911a665002a3ef7",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java",
                "filename": "server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -26,8 +26,8 @@\n \n public class ShowTraceLinkTypeTest {\n   private static RemoteSpan rs(long start, long stop, String description) {\n-    return new RemoteSpan(\"sender\", \"svc\", 0L, 0L, 0L, start, stop, description,\n-        Collections.emptyMap(), Collections.emptyList());\n+    return new RemoteSpan(\"sender\", \"svc\", 0L, 0L, Collections.singletonList(0L), start, stop,\n+        description, Collections.emptyMap(), Collections.emptyList());\n   }\n \n   @Test",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "54b44a10628b0a385ffc9e1c37846662254cb7b7",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java",
                "filename": "server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -43,6 +43,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.primitives.Longs;\n+\n /**\n  * Deliver Span information periodically to a destination.\n  * <ul>\n@@ -175,7 +177,7 @@ public void receiveSpan(Span s) {\n       return;\n     }\n \n-    Map<String,String> data = convertToStrings(s.getKVAnnotations());\n+    Map<String,String> data = s.getKVAnnotations();\n \n     SpanKey dest = getSpanKey(data);\n     if (dest != null) {\n@@ -192,8 +194,8 @@ public void receiveSpan(Span s) {\n         return;\n       }\n       sendQueue.add(new RemoteSpan(host, service == null ? processId : service, s.getTraceId(),\n-          s.getSpanId(), s.getParentId(), s.getStartTimeMillis(), s.getStopTimeMillis(),\n-          s.getDescription(), data, annotations));\n+          s.getSpanId(), Longs.asList(s.getParents()), s.getStartTimeMillis(),\n+          s.getStopTimeMillis(), s.getDescription(), data, annotations));\n       sendQueueSize.incrementAndGet();\n     }\n   }",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "fde4a326e9e1bbab6987ec1bc6b669cf0f1df5bb",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java",
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -24,25 +24,27 @@\n import java.util.Set;\n \n import org.apache.accumulo.tracer.thrift.RemoteSpan;\n-import org.apache.htrace.Span;\n \n public class SpanTree {\n   final Map<Long,List<Long>> parentChildren = new HashMap<>();\n   public final Map<Long,RemoteSpan> nodes = new HashMap<>();\n+  private final List<Long> rootSpans = new ArrayList<>();\n \n   public void addNode(RemoteSpan span) {\n     nodes.put(span.spanId, span);\n-    if (parentChildren.get(span.parentId) == null)\n-      parentChildren.put(span.parentId, new ArrayList<>());\n-    parentChildren.get(span.parentId).add(span.spanId);\n+    if (span.getParentIdsSize() == 0) {\n+      rootSpans.add(span.spanId);\n+    }\n+    for (Long parentId : span.getParentIds()) {\n+      parentChildren.computeIfAbsent(parentId, id -> new ArrayList<>()).add(span.spanId);\n+    }\n   }\n \n   public Set<Long> visit(SpanTreeVisitor visitor) {\n     Set<Long> visited = new HashSet<>();\n-    List<Long> root = parentChildren.get(Span.ROOT_SPAN_ID);\n-    if (root == null || root.isEmpty())\n+    if (rootSpans.isEmpty())\n       return visited;\n-    RemoteSpan rootSpan = nodes.get(root.iterator().next());\n+    RemoteSpan rootSpan = nodes.get(rootSpans.iterator().next());\n     if (rootSpan == null)\n       return visited;\n     recurse(0, rootSpan, visitor, visited);",
                "additions": 9,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java",
                "status": "modified",
                "changes": 16,
                "deletions": 7,
                "sha": "90dec989849b04036a9c267e9aaed49ad39e645b",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java",
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -36,7 +36,6 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.tracer.thrift.RemoteSpan;\n import org.apache.hadoop.io.Text;\n-import org.apache.htrace.Span;\n \n import com.beust.jcommander.Parameter;\n \n@@ -126,7 +125,7 @@ public static int printTrace(Scanner scanner, final Printer out) {\n       RemoteSpan span = TraceFormatter.getRemoteSpan(entry);\n       tree.addNode(span);\n       start = min(start, span.start);\n-      if (span.parentId == Span.ROOT_SPAN_ID)\n+      if (span.getParentIdsSize() == 0)\n         count++;\n     }\n     if (start == Long.MAX_VALUE) {",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "76e1a7e48f94a261c33c37727be1701310a1d76a",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java",
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -20,6 +20,7 @@\n import java.util.Date;\n import java.util.Iterator;\n import java.util.Map.Entry;\n+import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Value;\n@@ -83,7 +84,10 @@ public String next() {\n       result.append(String.format(\" %12s:%s%n\", \"trace\", Long.toHexString(span.traceId)));\n       result.append(String.format(\" %12s:%s%n\", \"loc\", span.svc + \"@\" + span.sender));\n       result.append(String.format(\" %12s:%s%n\", \"span\", Long.toHexString(span.spanId)));\n-      result.append(String.format(\" %12s:%s%n\", \"parent\", Long.toHexString(span.parentId)));\n+      String parentString = span.getParentIdsSize() == 0 ? \"\"\n+          : span.getParentIds().stream().map(x -> Long.toHexString(x)).collect(Collectors.toList())\n+              .toString();\n+      result.append(String.format(\" %12s:%s%n\", \"parent\", parentString));\n       result.append(String.format(\" %12s:%s%n\", \"start\", dateFormatter.format(span.start)));\n       result.append(String.format(\" %12s:%s%n\", \"ms\", span.stop - span.start));\n       if (span.data != null) {",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java",
                "status": "modified",
                "changes": 6,
                "deletions": 1,
                "sha": "9700482e69eecd61be2b1d32156d2386651ccf88",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java",
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -28,6 +28,7 @@\n import java.util.Map.Entry;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.Accumulo;\n@@ -63,7 +64,6 @@\n import org.apache.accumulo.tracer.thrift.SpanReceiver.Iface;\n import org.apache.accumulo.tracer.thrift.SpanReceiver.Processor;\n import org.apache.hadoop.io.Text;\n-import org.apache.htrace.Span;\n import org.apache.thrift.TByteArrayOutputStream;\n import org.apache.thrift.TException;\n import org.apache.thrift.protocol.TCompactProtocol;\n@@ -149,14 +149,14 @@ public void span(RemoteSpan s) throws TException {\n       ByteArrayTransport transport = new ByteArrayTransport();\n       TCompactProtocol protocol = new TCompactProtocol(transport);\n       s.write(protocol);\n-      String parentString = Long.toHexString(s.parentId);\n-      if (s.parentId == Span.ROOT_SPAN_ID)\n-        parentString = \"\";\n+      String parentString = s.getParentIdsSize() == 0 ? \"\"\n+          : s.getParentIds().stream().map(x -> Long.toHexString(x)).collect(Collectors.toList())\n+              .toString();\n       put(spanMutation, \"span\", parentString + \":\" + Long.toHexString(s.spanId), transport.get(),\n           transport.len());\n       // Map the root span to time so we can look up traces by time\n       Mutation timeMutation = null;\n-      if (s.parentId == Span.ROOT_SPAN_ID) {\n+      if (s.getParentIdsSize() == 0) {\n         timeMutation = new Mutation(new Text(\"start:\" + startString));\n         put(timeMutation, \"id\", idString, transport.get(), transport.len());\n       }",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java",
                "status": "modified",
                "changes": 10,
                "deletions": 5,
                "sha": "da0f74ebf48d6c1efffaf31263ad37d3a5daf4bb",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java",
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -31,7 +31,7 @@\n   private static final org.apache.thrift.protocol.TField SVC_FIELD_DESC = new org.apache.thrift.protocol.TField(\"svc\", org.apache.thrift.protocol.TType.STRING, (short)2);\n   private static final org.apache.thrift.protocol.TField TRACE_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"traceId\", org.apache.thrift.protocol.TType.I64, (short)3);\n   private static final org.apache.thrift.protocol.TField SPAN_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"spanId\", org.apache.thrift.protocol.TType.I64, (short)4);\n-  private static final org.apache.thrift.protocol.TField PARENT_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"parentId\", org.apache.thrift.protocol.TType.I64, (short)5);\n+  private static final org.apache.thrift.protocol.TField PARENT_IDS_FIELD_DESC = new org.apache.thrift.protocol.TField(\"parentIds\", org.apache.thrift.protocol.TType.LIST, (short)11);\n   private static final org.apache.thrift.protocol.TField START_FIELD_DESC = new org.apache.thrift.protocol.TField(\"start\", org.apache.thrift.protocol.TType.I64, (short)6);\n   private static final org.apache.thrift.protocol.TField STOP_FIELD_DESC = new org.apache.thrift.protocol.TField(\"stop\", org.apache.thrift.protocol.TType.I64, (short)7);\n   private static final org.apache.thrift.protocol.TField DESCRIPTION_FIELD_DESC = new org.apache.thrift.protocol.TField(\"description\", org.apache.thrift.protocol.TType.STRING, (short)8);\n@@ -45,7 +45,7 @@\n   public java.lang.String svc; // required\n   public long traceId; // required\n   public long spanId; // required\n-  public long parentId; // required\n+  public java.util.List<java.lang.Long> parentIds; // required\n   public long start; // required\n   public long stop; // required\n   public java.lang.String description; // required\n@@ -58,7 +58,7 @@\n     SVC((short)2, \"svc\"),\n     TRACE_ID((short)3, \"traceId\"),\n     SPAN_ID((short)4, \"spanId\"),\n-    PARENT_ID((short)5, \"parentId\"),\n+    PARENT_IDS((short)11, \"parentIds\"),\n     START((short)6, \"start\"),\n     STOP((short)7, \"stop\"),\n     DESCRIPTION((short)8, \"description\"),\n@@ -86,8 +86,8 @@ public static _Fields findByThriftId(int fieldId) {\n           return TRACE_ID;\n         case 4: // SPAN_ID\n           return SPAN_ID;\n-        case 5: // PARENT_ID\n-          return PARENT_ID;\n+        case 11: // PARENT_IDS\n+          return PARENT_IDS;\n         case 6: // START\n           return START;\n         case 7: // STOP\n@@ -140,9 +140,8 @@ public short getThriftFieldId() {\n   // isset id assignments\n   private static final int __TRACEID_ISSET_ID = 0;\n   private static final int __SPANID_ISSET_ID = 1;\n-  private static final int __PARENTID_ISSET_ID = 2;\n-  private static final int __START_ISSET_ID = 3;\n-  private static final int __STOP_ISSET_ID = 4;\n+  private static final int __START_ISSET_ID = 2;\n+  private static final int __STOP_ISSET_ID = 3;\n   private byte __isset_bitfield = 0;\n   public static final java.util.Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;\n   static {\n@@ -155,8 +154,9 @@ public short getThriftFieldId() {\n         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n     tmpMap.put(_Fields.SPAN_ID, new org.apache.thrift.meta_data.FieldMetaData(\"spanId\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n-    tmpMap.put(_Fields.PARENT_ID, new org.apache.thrift.meta_data.FieldMetaData(\"parentId\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n-        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n+    tmpMap.put(_Fields.PARENT_IDS, new org.apache.thrift.meta_data.FieldMetaData(\"parentIds\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n+        new org.apache.thrift.meta_data.ListMetaData(org.apache.thrift.protocol.TType.LIST, \n+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64))));\n     tmpMap.put(_Fields.START, new org.apache.thrift.meta_data.FieldMetaData(\"start\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n     tmpMap.put(_Fields.STOP, new org.apache.thrift.meta_data.FieldMetaData(\"stop\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n@@ -182,7 +182,7 @@ public RemoteSpan(\n     java.lang.String svc,\n     long traceId,\n     long spanId,\n-    long parentId,\n+    java.util.List<java.lang.Long> parentIds,\n     long start,\n     long stop,\n     java.lang.String description,\n@@ -196,8 +196,7 @@ public RemoteSpan(\n     setTraceIdIsSet(true);\n     this.spanId = spanId;\n     setSpanIdIsSet(true);\n-    this.parentId = parentId;\n-    setParentIdIsSet(true);\n+    this.parentIds = parentIds;\n     this.start = start;\n     setStartIsSet(true);\n     this.stop = stop;\n@@ -220,7 +219,10 @@ public RemoteSpan(RemoteSpan other) {\n     }\n     this.traceId = other.traceId;\n     this.spanId = other.spanId;\n-    this.parentId = other.parentId;\n+    if (other.isSetParentIds()) {\n+      java.util.List<java.lang.Long> __this__parentIds = new java.util.ArrayList<java.lang.Long>(other.parentIds);\n+      this.parentIds = __this__parentIds;\n+    }\n     this.start = other.start;\n     this.stop = other.stop;\n     if (other.isSetDescription()) {\n@@ -251,8 +253,7 @@ public void clear() {\n     this.traceId = 0;\n     setSpanIdIsSet(false);\n     this.spanId = 0;\n-    setParentIdIsSet(false);\n-    this.parentId = 0;\n+    this.parentIds = null;\n     setStartIsSet(false);\n     this.start = 0;\n     setStopIsSet(false);\n@@ -356,27 +357,43 @@ public void setSpanIdIsSet(boolean value) {\n     __isset_bitfield = org.apache.thrift.EncodingUtils.setBit(__isset_bitfield, __SPANID_ISSET_ID, value);\n   }\n \n-  public long getParentId() {\n-    return this.parentId;\n+  public int getParentIdsSize() {\n+    return (this.parentIds == null) ? 0 : this.parentIds.size();\n+  }\n+\n+  public java.util.Iterator<java.lang.Long> getParentIdsIterator() {\n+    return (this.parentIds == null) ? null : this.parentIds.iterator();\n+  }\n+\n+  public void addToParentIds(long elem) {\n+    if (this.parentIds == null) {\n+      this.parentIds = new java.util.ArrayList<java.lang.Long>();\n+    }\n+    this.parentIds.add(elem);\n+  }\n+\n+  public java.util.List<java.lang.Long> getParentIds() {\n+    return this.parentIds;\n   }\n \n-  public RemoteSpan setParentId(long parentId) {\n-    this.parentId = parentId;\n-    setParentIdIsSet(true);\n+  public RemoteSpan setParentIds(java.util.List<java.lang.Long> parentIds) {\n+    this.parentIds = parentIds;\n     return this;\n   }\n \n-  public void unsetParentId() {\n-    __isset_bitfield = org.apache.thrift.EncodingUtils.clearBit(__isset_bitfield, __PARENTID_ISSET_ID);\n+  public void unsetParentIds() {\n+    this.parentIds = null;\n   }\n \n-  /** Returns true if field parentId is set (has been assigned a value) and false otherwise */\n-  public boolean isSetParentId() {\n-    return org.apache.thrift.EncodingUtils.testBit(__isset_bitfield, __PARENTID_ISSET_ID);\n+  /** Returns true if field parentIds is set (has been assigned a value) and false otherwise */\n+  public boolean isSetParentIds() {\n+    return this.parentIds != null;\n   }\n \n-  public void setParentIdIsSet(boolean value) {\n-    __isset_bitfield = org.apache.thrift.EncodingUtils.setBit(__isset_bitfield, __PARENTID_ISSET_ID, value);\n+  public void setParentIdsIsSet(boolean value) {\n+    if (!value) {\n+      this.parentIds = null;\n+    }\n   }\n \n   public long getStart() {\n@@ -557,11 +574,11 @@ public void setFieldValue(_Fields field, java.lang.Object value) {\n       }\n       break;\n \n-    case PARENT_ID:\n+    case PARENT_IDS:\n       if (value == null) {\n-        unsetParentId();\n+        unsetParentIds();\n       } else {\n-        setParentId((java.lang.Long)value);\n+        setParentIds((java.util.List<java.lang.Long>)value);\n       }\n       break;\n \n@@ -622,8 +639,8 @@ public void setFieldValue(_Fields field, java.lang.Object value) {\n     case SPAN_ID:\n       return getSpanId();\n \n-    case PARENT_ID:\n-      return getParentId();\n+    case PARENT_IDS:\n+      return getParentIds();\n \n     case START:\n       return getStart();\n@@ -659,8 +676,8 @@ public boolean isSet(_Fields field) {\n       return isSetTraceId();\n     case SPAN_ID:\n       return isSetSpanId();\n-    case PARENT_ID:\n-      return isSetParentId();\n+    case PARENT_IDS:\n+      return isSetParentIds();\n     case START:\n       return isSetStart();\n     case STOP:\n@@ -726,12 +743,12 @@ public boolean equals(RemoteSpan that) {\n         return false;\n     }\n \n-    boolean this_present_parentId = true;\n-    boolean that_present_parentId = true;\n-    if (this_present_parentId || that_present_parentId) {\n-      if (!(this_present_parentId && that_present_parentId))\n+    boolean this_present_parentIds = true && this.isSetParentIds();\n+    boolean that_present_parentIds = true && that.isSetParentIds();\n+    if (this_present_parentIds || that_present_parentIds) {\n+      if (!(this_present_parentIds && that_present_parentIds))\n         return false;\n-      if (this.parentId != that.parentId)\n+      if (!this.parentIds.equals(that.parentIds))\n         return false;\n     }\n \n@@ -799,7 +816,9 @@ public int hashCode() {\n \n     hashCode = hashCode * 8191 + org.apache.thrift.TBaseHelper.hashCode(spanId);\n \n-    hashCode = hashCode * 8191 + org.apache.thrift.TBaseHelper.hashCode(parentId);\n+    hashCode = hashCode * 8191 + ((isSetParentIds()) ? 131071 : 524287);\n+    if (isSetParentIds())\n+      hashCode = hashCode * 8191 + parentIds.hashCode();\n \n     hashCode = hashCode * 8191 + org.apache.thrift.TBaseHelper.hashCode(start);\n \n@@ -868,12 +887,12 @@ public int compareTo(RemoteSpan other) {\n         return lastComparison;\n       }\n     }\n-    lastComparison = java.lang.Boolean.valueOf(isSetParentId()).compareTo(other.isSetParentId());\n+    lastComparison = java.lang.Boolean.valueOf(isSetParentIds()).compareTo(other.isSetParentIds());\n     if (lastComparison != 0) {\n       return lastComparison;\n     }\n-    if (isSetParentId()) {\n-      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.parentId, other.parentId);\n+    if (isSetParentIds()) {\n+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.parentIds, other.parentIds);\n       if (lastComparison != 0) {\n         return lastComparison;\n       }\n@@ -972,8 +991,12 @@ public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.\n     sb.append(this.spanId);\n     first = false;\n     if (!first) sb.append(\", \");\n-    sb.append(\"parentId:\");\n-    sb.append(this.parentId);\n+    sb.append(\"parentIds:\");\n+    if (this.parentIds == null) {\n+      sb.append(\"null\");\n+    } else {\n+      sb.append(this.parentIds);\n+    }\n     first = false;\n     if (!first) sb.append(\", \");\n     sb.append(\"start:\");\n@@ -1084,10 +1107,20 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, RemoteSpan struct)\n               org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);\n             }\n             break;\n-          case 5: // PARENT_ID\n-            if (schemeField.type == org.apache.thrift.protocol.TType.I64) {\n-              struct.parentId = iprot.readI64();\n-              struct.setParentIdIsSet(true);\n+          case 11: // PARENT_IDS\n+            if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {\n+              {\n+                org.apache.thrift.protocol.TList _list0 = iprot.readListBegin();\n+                struct.parentIds = new java.util.ArrayList<java.lang.Long>(_list0.size);\n+                long _elem1;\n+                for (int _i2 = 0; _i2 < _list0.size; ++_i2)\n+                {\n+                  _elem1 = iprot.readI64();\n+                  struct.parentIds.add(_elem1);\n+                }\n+                iprot.readListEnd();\n+              }\n+              struct.setParentIdsIsSet(true);\n             } else { \n               org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);\n             }\n@@ -1119,15 +1152,15 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, RemoteSpan struct)\n           case 9: // DATA\n             if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {\n               {\n-                org.apache.thrift.protocol.TMap _map0 = iprot.readMapBegin();\n-                struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map0.size);\n-                java.lang.String _key1;\n-                java.lang.String _val2;\n-                for (int _i3 = 0; _i3 < _map0.size; ++_i3)\n+                org.apache.thrift.protocol.TMap _map3 = iprot.readMapBegin();\n+                struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map3.size);\n+                java.lang.String _key4;\n+                java.lang.String _val5;\n+                for (int _i6 = 0; _i6 < _map3.size; ++_i6)\n                 {\n-                  _key1 = iprot.readString();\n-                  _val2 = iprot.readString();\n-                  struct.data.put(_key1, _val2);\n+                  _key4 = iprot.readString();\n+                  _val5 = iprot.readString();\n+                  struct.data.put(_key4, _val5);\n                 }\n                 iprot.readMapEnd();\n               }\n@@ -1139,14 +1172,14 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, RemoteSpan struct)\n           case 10: // ANNOTATIONS\n             if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {\n               {\n-                org.apache.thrift.protocol.TList _list4 = iprot.readListBegin();\n-                struct.annotations = new java.util.ArrayList<Annotation>(_list4.size);\n-                Annotation _elem5;\n-                for (int _i6 = 0; _i6 < _list4.size; ++_i6)\n+                org.apache.thrift.protocol.TList _list7 = iprot.readListBegin();\n+                struct.annotations = new java.util.ArrayList<Annotation>(_list7.size);\n+                Annotation _elem8;\n+                for (int _i9 = 0; _i9 < _list7.size; ++_i9)\n                 {\n-                  _elem5 = new Annotation();\n-                  _elem5.read(iprot);\n-                  struct.annotations.add(_elem5);\n+                  _elem8 = new Annotation();\n+                  _elem8.read(iprot);\n+                  struct.annotations.add(_elem8);\n                 }\n                 iprot.readListEnd();\n               }\n@@ -1186,9 +1219,6 @@ public void write(org.apache.thrift.protocol.TProtocol oprot, RemoteSpan struct)\n       oprot.writeFieldBegin(SPAN_ID_FIELD_DESC);\n       oprot.writeI64(struct.spanId);\n       oprot.writeFieldEnd();\n-      oprot.writeFieldBegin(PARENT_ID_FIELD_DESC);\n-      oprot.writeI64(struct.parentId);\n-      oprot.writeFieldEnd();\n       oprot.writeFieldBegin(START_FIELD_DESC);\n       oprot.writeI64(struct.start);\n       oprot.writeFieldEnd();\n@@ -1204,10 +1234,10 @@ public void write(org.apache.thrift.protocol.TProtocol oprot, RemoteSpan struct)\n         oprot.writeFieldBegin(DATA_FIELD_DESC);\n         {\n           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.data.size()));\n-          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter7 : struct.data.entrySet())\n+          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter10 : struct.data.entrySet())\n           {\n-            oprot.writeString(_iter7.getKey());\n-            oprot.writeString(_iter7.getValue());\n+            oprot.writeString(_iter10.getKey());\n+            oprot.writeString(_iter10.getValue());\n           }\n           oprot.writeMapEnd();\n         }\n@@ -1217,9 +1247,21 @@ public void write(org.apache.thrift.protocol.TProtocol oprot, RemoteSpan struct)\n         oprot.writeFieldBegin(ANNOTATIONS_FIELD_DESC);\n         {\n           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.annotations.size()));\n-          for (Annotation _iter8 : struct.annotations)\n+          for (Annotation _iter11 : struct.annotations)\n           {\n-            _iter8.write(oprot);\n+            _iter11.write(oprot);\n+          }\n+          oprot.writeListEnd();\n+        }\n+        oprot.writeFieldEnd();\n+      }\n+      if (struct.parentIds != null) {\n+        oprot.writeFieldBegin(PARENT_IDS_FIELD_DESC);\n+        {\n+          oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, struct.parentIds.size()));\n+          for (long _iter12 : struct.parentIds)\n+          {\n+            oprot.writeI64(_iter12);\n           }\n           oprot.writeListEnd();\n         }\n@@ -1255,7 +1297,7 @@ public void write(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct)\n       if (struct.isSetSpanId()) {\n         optionals.set(3);\n       }\n-      if (struct.isSetParentId()) {\n+      if (struct.isSetParentIds()) {\n         optionals.set(4);\n       }\n       if (struct.isSetStart()) {\n@@ -1286,8 +1328,14 @@ public void write(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct)\n       if (struct.isSetSpanId()) {\n         oprot.writeI64(struct.spanId);\n       }\n-      if (struct.isSetParentId()) {\n-        oprot.writeI64(struct.parentId);\n+      if (struct.isSetParentIds()) {\n+        {\n+          oprot.writeI32(struct.parentIds.size());\n+          for (long _iter13 : struct.parentIds)\n+          {\n+            oprot.writeI64(_iter13);\n+          }\n+        }\n       }\n       if (struct.isSetStart()) {\n         oprot.writeI64(struct.start);\n@@ -1301,19 +1349,19 @@ public void write(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct)\n       if (struct.isSetData()) {\n         {\n           oprot.writeI32(struct.data.size());\n-          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter9 : struct.data.entrySet())\n+          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter14 : struct.data.entrySet())\n           {\n-            oprot.writeString(_iter9.getKey());\n-            oprot.writeString(_iter9.getValue());\n+            oprot.writeString(_iter14.getKey());\n+            oprot.writeString(_iter14.getValue());\n           }\n         }\n       }\n       if (struct.isSetAnnotations()) {\n         {\n           oprot.writeI32(struct.annotations.size());\n-          for (Annotation _iter10 : struct.annotations)\n+          for (Annotation _iter15 : struct.annotations)\n           {\n-            _iter10.write(oprot);\n+            _iter15.write(oprot);\n           }\n         }\n       }\n@@ -1340,8 +1388,17 @@ public void read(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct) t\n         struct.setSpanIdIsSet(true);\n       }\n       if (incoming.get(4)) {\n-        struct.parentId = iprot.readI64();\n-        struct.setParentIdIsSet(true);\n+        {\n+          org.apache.thrift.protocol.TList _list16 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());\n+          struct.parentIds = new java.util.ArrayList<java.lang.Long>(_list16.size);\n+          long _elem17;\n+          for (int _i18 = 0; _i18 < _list16.size; ++_i18)\n+          {\n+            _elem17 = iprot.readI64();\n+            struct.parentIds.add(_elem17);\n+          }\n+        }\n+        struct.setParentIdsIsSet(true);\n       }\n       if (incoming.get(5)) {\n         struct.start = iprot.readI64();\n@@ -1357,29 +1414,29 @@ public void read(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct) t\n       }\n       if (incoming.get(8)) {\n         {\n-          org.apache.thrift.protocol.TMap _map11 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());\n-          struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map11.size);\n-          java.lang.String _key12;\n-          java.lang.String _val13;\n-          for (int _i14 = 0; _i14 < _map11.size; ++_i14)\n+          org.apache.thrift.protocol.TMap _map19 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());\n+          struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map19.size);\n+          java.lang.String _key20;\n+          java.lang.String _val21;\n+          for (int _i22 = 0; _i22 < _map19.size; ++_i22)\n           {\n-            _key12 = iprot.readString();\n-            _val13 = iprot.readString();\n-            struct.data.put(_key12, _val13);\n+            _key20 = iprot.readString();\n+            _val21 = iprot.readString();\n+            struct.data.put(_key20, _val21);\n           }\n         }\n         struct.setDataIsSet(true);\n       }\n       if (incoming.get(9)) {\n         {\n-          org.apache.thrift.protocol.TList _list15 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());\n-          struct.annotations = new java.util.ArrayList<Annotation>(_list15.size);\n-          Annotation _elem16;\n-          for (int _i17 = 0; _i17 < _list15.size; ++_i17)\n+          org.apache.thrift.protocol.TList _list23 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());\n+          struct.annotations = new java.util.ArrayList<Annotation>(_list23.size);\n+          Annotation _elem24;\n+          for (int _i25 = 0; _i25 < _list23.size; ++_i25)\n           {\n-            _elem16 = new Annotation();\n-            _elem16.read(iprot);\n-            struct.annotations.add(_elem16);\n+            _elem24 = new Annotation();\n+            _elem24.read(iprot);\n+            struct.annotations.add(_elem24);\n           }\n         }\n         struct.setAnnotationsIsSet(true);",
                "additions": 155,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java",
                "status": "modified",
                "changes": 253,
                "deletions": 98,
                "sha": "819185a96eda7c98602b1a27adf43d1e635634b5",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java",
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -29,7 +29,7 @@ struct RemoteSpan {\n   2:string svc\n   3:i64 traceId\n   4:i64 spanId\n-  5:i64 parentId\n+  11:list<i64> parentIds\n   6:i64 start\n   7:i64 stop\n   8:string description",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/thrift/tracer.thrift",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "a66479df2a811abe96febbe9d97b54a3d73f1d40",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/thrift/tracer.thrift",
                "filename": "server/tracer/src/main/thrift/tracer.thrift",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/thrift/tracer.thrift?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.htrace.SpanReceiver;\n import org.apache.htrace.Trace;\n import org.apache.htrace.TraceScope;\n+import org.apache.htrace.Tracer;\n import org.apache.htrace.wrappers.TraceProxy;\n import org.apache.thrift.protocol.TBinaryProtocol;\n import org.apache.thrift.server.TServer;\n@@ -51,16 +52,18 @@\n import org.junit.Before;\n import org.junit.Test;\n \n+import com.google.common.primitives.Longs;\n+\n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n \n public class TracerTest {\n   static class SpanStruct {\n-    public SpanStruct(long traceId, long spanId, long parentId, long start, long stop,\n-        String description, Map<byte[],byte[]> data) {\n+    public SpanStruct(long traceId, long spanId, List<Long> parentIds, long start, long stop,\n+        String description, Map<String,String> data) {\n       super();\n       this.traceId = traceId;\n       this.spanId = spanId;\n-      this.parentId = parentId;\n+      this.parentIds = parentIds;\n       this.start = start;\n       this.stop = stop;\n       this.description = description;\n@@ -69,11 +72,11 @@ public SpanStruct(long traceId, long spanId, long parentId, long start, long sto\n \n     public long traceId;\n     public long spanId;\n-    public long parentId;\n+    public List<Long> parentIds;\n     public long start;\n     public long stop;\n     public String description;\n-    public Map<byte[],byte[]> data;\n+    public Map<String,String> data;\n \n     public long millis() {\n       return stop - start;\n@@ -88,7 +91,7 @@ public TestReceiver() {}\n     @Override\n     public void receiveSpan(Span s) {\n       long traceId = s.getTraceId();\n-      SpanStruct span = new SpanStruct(traceId, s.getSpanId(), s.getParentId(),\n+      SpanStruct span = new SpanStruct(traceId, s.getSpanId(), Longs.asList(s.getParents()),\n           s.getStartTimeMillis(), s.getStopTimeMillis(), s.getDescription(), s.getKVAnnotations());\n       if (!traces.containsKey(traceId))\n         traces.put(traceId, new ArrayList<>());\n@@ -134,13 +137,14 @@ public void testTrace() throws Exception {\n     assertEquals(2, tracer.traces.get(traceId).size());\n     assertTrue(tracer.traces.get(traceId).get(1).millis() >= 100);\n \n-    Thread t = new Thread(TraceUtil.wrap(() -> assertTrue(Trace.isTracing())), \"My Task\");\n+    Thread t = new Thread(Trace.wrap(() -> assertTrue(Trace.isTracing())), \"My Task\");\n     t.start();\n     t.join();\n \n     assertEquals(3, tracer.traces.get(traceId).size());\n     assertEquals(\"My Task\", tracer.traces.get(traceId).get(2).description);\n-    TraceUtil.off();\n+    Trace.currentSpan().stop();\n+    Tracer.getInstance().continueSpan(null);\n     assertFalse(Trace.isTracing());\n   }\n ",
                "additions": 12,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java",
                "status": "modified",
                "changes": 20,
                "deletions": 8,
                "sha": "df23b7500e2be267a2ca2a45659405a540ddf686",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java",
                "filename": "server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -16,7 +16,6 @@\n  */\n package org.apache.accumulo.tserver.replication;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n import static java.util.Objects.requireNonNull;\n import static org.apache.accumulo.fate.util.UtilWaitThread.sleepUninterruptibly;\n \n@@ -209,10 +208,9 @@ public Status replicate(final Path p, final Status status, final ReplicationTarg\n   private Status _replicate(final Path p, final Status status, final ReplicationTarget target,\n       final ReplicaSystemHelper helper, final AccumuloConfiguration localConf,\n       final ClientContext peerContext, final UserGroupInformation accumuloUgi) {\n-    try {\n-      double tracePercent = localConf.getFraction(Property.REPLICATION_TRACE_PERCENT);\n-      ProbabilitySampler sampler = TraceUtil.probabilitySampler(tracePercent);\n-      Trace.startSpan(\"AccumuloReplicaSystem\", sampler);\n+    double tracePercent = localConf.getFraction(Property.REPLICATION_TRACE_PERCENT);\n+    ProbabilitySampler sampler = TraceUtil.probabilitySampler(tracePercent);\n+    try (TraceScope replicaSpan = Trace.startSpan(\"AccumuloReplicaSystem\", sampler)) {\n \n       // Remote identifier is an integer (table id) in this case.\n       final String remoteTableId = target.getRemoteIdentifier();\n@@ -277,8 +275,6 @@ private Status _replicate(final Path p, final Status status, final ReplicationTa\n \n       // We made no status, punt on it for now, and let it re-queue itself for work\n       return status;\n-    } finally {\n-      TraceUtil.off();\n     }\n   }\n \n@@ -337,7 +333,7 @@ protected Status replicateLogs(ClientContext peerContext, final HostAndPort peer\n       log.debug(\"Skipping unwanted data in WAL\");\n       try (TraceScope span = Trace.startSpan(\"Consume WAL prefix\")) {\n         if (span.getSpan() != null) {\n-          span.getSpan().addKVAnnotation(\"file\".getBytes(UTF_8), p.toString().getBytes(UTF_8));\n+          span.getSpan().addKVAnnotation(\"file\", p.toString());\n         }\n         // We want to read all records in the WAL up to the \"begin\" offset contained in the Status\n         // message,\n@@ -358,15 +354,11 @@ protected Status replicateLogs(ClientContext peerContext, final HostAndPort peer\n         try (TraceScope span = Trace.startSpan(\"Replicate WAL batch\")) {\n           if (span.getSpan() != null) {\n             // Set some trace context\n-            span.getSpan().addKVAnnotation(\"Batch size (bytes)\".getBytes(UTF_8),\n-                Long.toString(sizeLimit).getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"File\".getBytes(UTF_8), p.toString().getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"Peer instance name\".getBytes(UTF_8),\n-                peerContext.getInstanceName().getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"Peer tserver\".getBytes(UTF_8),\n-                peerTserver.toString().getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"Remote table ID\".getBytes(UTF_8),\n-                remoteTableId.getBytes(UTF_8));\n+            span.getSpan().addKVAnnotation(\"Batch size (bytes)\", Long.toString(sizeLimit));\n+            span.getSpan().addKVAnnotation(\"File\", p.toString());\n+            span.getSpan().addKVAnnotation(\"Peer instance name\", peerContext.getInstanceName());\n+            span.getSpan().addKVAnnotation(\"Peer tserver\", peerTserver.toString());\n+            span.getSpan().addKVAnnotation(\"Remote table ID\", remoteTableId);\n           }\n \n           // Read and send a batch of mutations\n@@ -634,7 +626,7 @@ protected ClientContext getContextForPeer(AccumuloConfiguration localConf,\n   public DataInputStream getWalStream(Path p, FSDataInputStream input) throws IOException {\n     try (TraceScope span = Trace.startSpan(\"Read WAL header\")) {\n       if (span.getSpan() != null) {\n-        span.getSpan().addKVAnnotation(\"file\".getBytes(UTF_8), p.toString().getBytes(UTF_8));\n+        span.getSpan().addKVAnnotation(\"file\", p.toString());\n       }\n       DFSLoggerInputStreams streams = DfsLogger.readHeaderAndReturnStream(input, conf);\n       return streams.getDecryptingInputStream();",
                "additions": 10,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java",
                "status": "modified",
                "changes": 28,
                "deletions": 18,
                "sha": "fe85ab49e1c11db4f807476bd4b848f2047f6605",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java",
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -16,8 +16,6 @@\n  */\n package org.apache.accumulo.tserver.tablet;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import java.io.IOException;\n \n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n@@ -94,12 +92,10 @@ public void run() {\n         }\n \n         if (minorCompaction.getSpan() != null) {\n-          minorCompaction.getSpan().addKVAnnotation(\"extent\".getBytes(UTF_8),\n-              tablet.getExtent().toString().getBytes(UTF_8));\n-          minorCompaction.getSpan().addKVAnnotation(\"numEntries\".getBytes(UTF_8),\n-              Long.toString(this.stats.getNumEntries()).getBytes(UTF_8));\n-          minorCompaction.getSpan().addKVAnnotation(\"size\".getBytes(UTF_8),\n-              Long.toString(this.stats.getSize()).getBytes(UTF_8));\n+          minorCompaction.getSpan().addKVAnnotation(\"extent\", tablet.getExtent().toString());\n+          minorCompaction.getSpan().addKVAnnotation(\"numEntries\",\n+              Long.toString(this.stats.getNumEntries()));\n+          minorCompaction.getSpan().addKVAnnotation(\"size\", Long.toString(this.stats.getSize()));\n         }\n       }\n ",
                "additions": 4,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java",
                "status": "modified",
                "changes": 12,
                "deletions": 8,
                "sha": "7aaa150f1a5270743c4a59b57c5a3e1b9d9d4964",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java",
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -1998,12 +1998,9 @@ public RateLimiter getWriteLimiter() {\n           CompactionStats mcs = compactor.call();\n \n           if (span.getSpan() != null) {\n-            span.getSpan().addKVAnnotation(\"files\".getBytes(UTF_8),\n-                (\"\" + smallestFiles.size()).getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"read\".getBytes(UTF_8),\n-                (\"\" + mcs.getEntriesRead()).getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"written\".getBytes(UTF_8),\n-                (\"\" + mcs.getEntriesWritten()).getBytes(UTF_8));\n+            span.getSpan().addKVAnnotation(\"files\", (\"\" + smallestFiles.size()));\n+            span.getSpan().addKVAnnotation(\"read\", (\"\" + mcs.getEntriesRead()));\n+            span.getSpan().addKVAnnotation(\"written\", (\"\" + mcs.getEntriesWritten()));\n           }\n           majCStats.add(mcs);\n \n@@ -2124,13 +2121,10 @@ CompactionStats majorCompact(MajorCompactionReason reason, long queued) {\n             .enqueueMasterMessage(new TabletStatusMessage(TabletLoadState.CHOPPED, extent));\n       }\n       if (span.getSpan() != null) {\n-        span.getSpan().addKVAnnotation(\"extent\".getBytes(UTF_8),\n-            (\"\" + getExtent()).getBytes(UTF_8));\n+        span.getSpan().addKVAnnotation(\"extent\", (\"\" + getExtent()));\n         if (majCStats != null) {\n-          span.getSpan().addKVAnnotation(\"read\".getBytes(UTF_8),\n-              (\"\" + majCStats.getEntriesRead()).getBytes(UTF_8));\n-          span.getSpan().addKVAnnotation(\"written\".getBytes(UTF_8),\n-              (\"\" + majCStats.getEntriesWritten()).getBytes(UTF_8));\n+          span.getSpan().addKVAnnotation(\"read\", (\"\" + majCStats.getEntriesRead()));\n+          span.getSpan().addKVAnnotation(\"written\", (\"\" + majCStats.getEntriesWritten()));\n         }\n       }\n       success = true;",
                "additions": 6,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java",
                "status": "modified",
                "changes": 18,
                "deletions": 12,
                "sha": "d8b10e26a65f432b25b9c5f0ee75a874c23e49a3",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java",
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -26,27 +26,33 @@\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.security.Authorizations;\n-import org.apache.accumulo.core.trace.TraceUtil;\n import org.apache.accumulo.core.util.BadArgumentException;\n import org.apache.accumulo.shell.Shell;\n import org.apache.accumulo.tracer.TraceDump;\n import org.apache.commons.cli.CommandLine;\n import org.apache.hadoop.io.Text;\n import org.apache.htrace.Sampler;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n \n public class TraceCommand extends DebugCommand {\n \n+  private TraceScope traceScope = null;\n+\n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n       throws IOException {\n     if (cl.getArgs().length == 1) {\n       if (cl.getArgs()[0].equalsIgnoreCase(\"on\")) {\n-        Trace.startSpan(\"shell:\" + shellState.getAccumuloClient().whoami(), Sampler.ALWAYS);\n+        if (traceScope == null) {\n+          traceScope = Trace.startSpan(\"shell:\" + shellState.getAccumuloClient().whoami(),\n+              Sampler.ALWAYS);\n+        }\n       } else if (cl.getArgs()[0].equalsIgnoreCase(\"off\")) {\n-        if (Trace.isTracing()) {\n-          final long trace = Trace.currentSpan().getTraceId();\n-          TraceUtil.off();\n+        if (traceScope != null) {\n+          final long trace = traceScope.getSpan().getTraceId();\n+          traceScope.close();\n+          traceScope = null;\n           StringBuilder sb = new StringBuilder();\n           int traceCount = 0;\n           for (int i = 0; i < 30; i++) {",
                "additions": 11,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java",
                "status": "modified",
                "changes": 16,
                "deletions": 5,
                "sha": "67e2a5b0089e14ada1e1533832b97b773958a3ec",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java",
                "filename": "shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -54,6 +54,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.log4j.Level;\n import org.apache.log4j.Logger;\n \n@@ -188,12 +189,10 @@ public static void main(String[] args) {\n \n     Opts opts = new Opts();\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n-    opts.parseArgs(TestIngest.class.getName(), args, bwOpts);\n \n     String name = TestIngest.class.getSimpleName();\n     TraceUtil.enableClientTraces(null, name, new Properties());\n-    try {\n-      opts.startTracing(name);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(name, args, bwOpts)) {\n       if (opts.debug)\n         Logger.getLogger(TabletServerBatchWriter.class.getName()).setLevel(Level.TRACE);\n \n@@ -203,7 +202,6 @@ public static void main(String[] args) {\n     } catch (Exception e) {\n       throw new RuntimeException(e);\n     } finally {\n-      opts.stopTracing();\n       TraceUtil.disable();\n     }\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/TestIngest.java",
                "status": "modified",
                "changes": 6,
                "deletions": 4,
                "sha": "c2b390080fcfcf9dfb6483525ddab1353b7099e0",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/TestIngest.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/TestIngest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/TestIngest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -16,8 +16,6 @@\n  */\n package org.apache.accumulo.test;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import java.util.Arrays;\n import java.util.Iterator;\n import java.util.Map.Entry;\n@@ -40,6 +38,7 @@\n import org.apache.htrace.Sampler;\n import org.apache.htrace.Span;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -68,23 +67,20 @@ public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n     ScannerOpts scanOpts = new ScannerOpts();\n     opts.parseArgs(VerifyIngest.class.getName(), args, scanOpts);\n-    try {\n-      if (opts.trace) {\n-        String name = VerifyIngest.class.getSimpleName();\n-        TraceUtil.enableClientTraces(null, null, new Properties());\n-        Trace.startSpan(name, Sampler.ALWAYS);\n-        Span span = Trace.currentSpan();\n-        if (span != null)\n-          span.addKVAnnotation(\"cmdLine\".getBytes(UTF_8),\n-              Arrays.asList(args).toString().getBytes(UTF_8));\n-      }\n+    if (opts.trace) {\n+      TraceUtil.enableClientTraces(null, null, new Properties());\n+    }\n+    try (TraceScope clientSpan = Trace.startSpan(VerifyIngest.class.getSimpleName(),\n+        Sampler.ALWAYS)) {\n+      Span span = clientSpan.getSpan();\n+      if (span != null)\n+        span.addKVAnnotation(\"cmdLine\", Arrays.asList(args).toString());\n \n       try (AccumuloClient client = opts.createClient()) {\n         verifyIngest(client, opts, scanOpts);\n       }\n \n     } finally {\n-      TraceUtil.off();\n       TraceUtil.disable();\n     }\n   }",
                "additions": 9,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/VerifyIngest.java",
                "status": "modified",
                "changes": 22,
                "deletions": 13,
                "sha": "f099fa8be56443522ae6087ed2876a5585b07839",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/VerifyIngest.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/VerifyIngest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/VerifyIngest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.htrace.wrappers.TraceProxy;\n \n public class ContinuousIngest {\n@@ -82,103 +83,104 @@ public static void main(String[] args) throws Exception {\n     ContinuousOpts opts = new ContinuousOpts();\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n     ClientOnDefaultTable clientOpts = new ClientOnDefaultTable(\"ci\");\n-    clientOpts.parseArgs(ContinuousIngest.class.getName(), args, bwOpts, opts);\n+    try (TraceScope clientSpan = clientOpts.parseArgsAndTrace(ContinuousIngest.class.getName(),\n+        args, bwOpts, opts)) {\n \n-    initVisibilities(opts);\n+      initVisibilities(opts);\n \n-    if (opts.min < 0 || opts.max < 0 || opts.max <= opts.min) {\n-      throw new IllegalArgumentException(\"bad min and max\");\n-    }\n-    try (AccumuloClient client = clientOpts.createClient()) {\n-\n-      if (!client.tableOperations().exists(clientOpts.getTableName())) {\n-        throw new TableNotFoundException(null, clientOpts.getTableName(),\n-            \"Consult the README and create the table before starting ingest.\");\n+      if (opts.min < 0 || opts.max < 0 || opts.max <= opts.min) {\n+        throw new IllegalArgumentException(\"bad min and max\");\n       }\n+      try (AccumuloClient client = clientOpts.createClient()) {\n \n-      BatchWriter bw = client.createBatchWriter(clientOpts.getTableName(),\n-          bwOpts.getBatchWriterConfig());\n-      bw = TraceProxy.trace(bw, TraceUtil.countSampler(1024));\n-\n-      Random r = new SecureRandom();\n-\n-      byte[] ingestInstanceId = UUID.randomUUID().toString().getBytes(UTF_8);\n-\n-      System.out.printf(\"UUID %d %s%n\", System.currentTimeMillis(),\n-          new String(ingestInstanceId, UTF_8));\n-\n-      long count = 0;\n-      final int flushInterval = 1000000;\n-      final int maxDepth = 25;\n+        if (!client.tableOperations().exists(clientOpts.getTableName())) {\n+          throw new TableNotFoundException(null, clientOpts.getTableName(),\n+              \"Consult the README and create the table before starting ingest.\");\n+        }\n \n-      // always want to point back to flushed data. This way the previous item should\n-      // always exist in accumulo when verifying data. To do this make insert N point\n-      // back to the row from insert (N - flushInterval). The array below is used to keep\n-      // track of this.\n-      long prevRows[] = new long[flushInterval];\n-      long firstRows[] = new long[flushInterval];\n-      int firstColFams[] = new int[flushInterval];\n-      int firstColQuals[] = new int[flushInterval];\n+        BatchWriter bw = client.createBatchWriter(clientOpts.getTableName(),\n+            bwOpts.getBatchWriterConfig());\n+        bw = TraceProxy.trace(bw, TraceUtil.countSampler(1024));\n \n-      long lastFlushTime = System.currentTimeMillis();\n+        Random r = new SecureRandom();\n \n-      out: while (true) {\n-        // generate first set of nodes\n-        ColumnVisibility cv = getVisibility(r);\n+        byte[] ingestInstanceId = UUID.randomUUID().toString().getBytes(UTF_8);\n \n-        for (int index = 0; index < flushInterval; index++) {\n-          long rowLong = genLong(opts.min, opts.max, r);\n-          prevRows[index] = rowLong;\n-          firstRows[index] = rowLong;\n+        System.out.printf(\"UUID %d %s%n\", System.currentTimeMillis(),\n+            new String(ingestInstanceId, UTF_8));\n \n-          int cf = r.nextInt(opts.maxColF);\n-          int cq = r.nextInt(opts.maxColQ);\n+        long count = 0;\n+        final int flushInterval = 1000000;\n+        final int maxDepth = 25;\n \n-          firstColFams[index] = cf;\n-          firstColQuals[index] = cq;\n+        // always want to point back to flushed data. This way the previous item should\n+        // always exist in accumulo when verifying data. To do this make insert N point\n+        // back to the row from insert (N - flushInterval). The array below is used to keep\n+        // track of this.\n+        long prevRows[] = new long[flushInterval];\n+        long firstRows[] = new long[flushInterval];\n+        int firstColFams[] = new int[flushInterval];\n+        int firstColQuals[] = new int[flushInterval];\n \n-          Mutation m = genMutation(rowLong, cf, cq, cv, ingestInstanceId, count, null,\n-              opts.checksum);\n-          count++;\n-          bw.addMutation(m);\n-        }\n+        long lastFlushTime = System.currentTimeMillis();\n \n-        lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n-        if (count >= opts.num)\n-          break out;\n+        out: while (true) {\n+          // generate first set of nodes\n+          ColumnVisibility cv = getVisibility(r);\n \n-        // generate subsequent sets of nodes that link to previous set of nodes\n-        for (int depth = 1; depth < maxDepth; depth++) {\n           for (int index = 0; index < flushInterval; index++) {\n             long rowLong = genLong(opts.min, opts.max, r);\n-            byte[] prevRow = genRow(prevRows[index]);\n             prevRows[index] = rowLong;\n-            Mutation m = genMutation(rowLong, r.nextInt(opts.maxColF), r.nextInt(opts.maxColQ), cv,\n-                ingestInstanceId, count, prevRow, opts.checksum);\n+            firstRows[index] = rowLong;\n+\n+            int cf = r.nextInt(opts.maxColF);\n+            int cq = r.nextInt(opts.maxColQ);\n+\n+            firstColFams[index] = cf;\n+            firstColQuals[index] = cq;\n+\n+            Mutation m = genMutation(rowLong, cf, cq, cv, ingestInstanceId, count, null,\n+                opts.checksum);\n             count++;\n             bw.addMutation(m);\n           }\n \n           lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n           if (count >= opts.num)\n             break out;\n-        }\n \n-        // create one big linked list, this makes all of the first inserts\n-        // point to something\n-        for (int index = 0; index < flushInterval - 1; index++) {\n-          Mutation m = genMutation(firstRows[index], firstColFams[index], firstColQuals[index], cv,\n-              ingestInstanceId, count, genRow(prevRows[index + 1]), opts.checksum);\n-          count++;\n-          bw.addMutation(m);\n+          // generate subsequent sets of nodes that link to previous set of nodes\n+          for (int depth = 1; depth < maxDepth; depth++) {\n+            for (int index = 0; index < flushInterval; index++) {\n+              long rowLong = genLong(opts.min, opts.max, r);\n+              byte[] prevRow = genRow(prevRows[index]);\n+              prevRows[index] = rowLong;\n+              Mutation m = genMutation(rowLong, r.nextInt(opts.maxColF), r.nextInt(opts.maxColQ),\n+                  cv, ingestInstanceId, count, prevRow, opts.checksum);\n+              count++;\n+              bw.addMutation(m);\n+            }\n+\n+            lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n+            if (count >= opts.num)\n+              break out;\n+          }\n+\n+          // create one big linked list, this makes all of the first inserts\n+          // point to something\n+          for (int index = 0; index < flushInterval - 1; index++) {\n+            Mutation m = genMutation(firstRows[index], firstColFams[index], firstColQuals[index],\n+                cv, ingestInstanceId, count, genRow(prevRows[index + 1]), opts.checksum);\n+            count++;\n+            bw.addMutation(m);\n+          }\n+          lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n+          if (count >= opts.num)\n+            break out;\n         }\n-        lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n-        if (count >= opts.num)\n-          break out;\n-      }\n \n-      bw.close();\n-      clientOpts.stopTracing();\n+        bw.close();\n+      }\n     }\n   }\n ",
                "additions": 73,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java",
                "status": "modified",
                "changes": 144,
                "deletions": 71,
                "sha": "43b09e25d404051744380aadceb5e2db08fb7a5d",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            },
            {
                "patch": "@@ -58,7 +58,6 @@ public void paramsSetThreadsTest() {\n     assertEquals(\"Check tablename is set\", 0, tablename.compareTo(opts.getTableName()));\n     assertEquals(\"Check numThreads is set\", 99, opts.numThreads);\n \n-    System.out.println(opts.columns);\n   }\n \n }",
                "additions": 0,
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java",
                "status": "modified",
                "changes": 1,
                "deletions": 1,
                "sha": "a5c71ac62965bc8b7ff5cda0fcfcf151b6bce3be",
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab"
            }
        ],
        "bug_id": "accumulo_6",
        "parent": "https://github.com/apache/accumulo/commit/426c93b83f16dbf638713d3b704f5a4a63f9c30e",
        "message": "Update to HTrace 3.2\n\nAPI fixes specifically for HTrace version 3.2:\n\n* Use String-based annotations, instead of deprecated byte arrays API\n* Update thrift and other utils to support multiple span parents and\nmore explicit tracking of root spans (those without parents)\n* Remove broken API call to set the Tracer processId field\n\nGeneral fixes found broken, or generally lacking with 3.2, but were in\nneed of fix anyway:\n\n* Properly close spans, via containing TraceScope instead of trying to\nmanually stop and set it to null by messing with HTrace internal\ntracking (avoid doing Tracer.getInstance().continueSpan(null), since the\nresulting Closeable can't be closed, because it throws an NPE)\n* Ensure TraceScopes are closed via try-with-resources wherever we were\nmanually trying to stop the span\n* Explicitly support client-side tracing in various command-line tools\nby parsing arguments with an option to return a TraceScope object, which\nis then put in a try-with-resources block\n\nTrivial unrelated changes:\n\n* Remove random print statement of a null field in CollectTabletStatsTest",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/79182991d0da0f7b7836a901cec78b3ea570808c",
        "file": [
            {
                "patch": "@@ -320,6 +320,9 @@ public long balance(SortedMap<TServerInstance,TabletServerStatus> current, Set<K\n             }\n             try {\n               List<TabletStats> outOfBoundsTablets = getOnlineTabletsForTable(e.getKey(), tid);\n+              if (null == outOfBoundsTablets) {\n+                continue;\n+              }\n               Random random = new Random();\n               for (TabletStats ts : outOfBoundsTablets) {\n                 KeyExtent ke = new KeyExtent(ts.getExtent());",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/79182991d0da0f7b7836a901cec78b3ea570808c/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "status": "modified",
                "changes": 3,
                "deletions": 0,
                "sha": "d19ab82d078bbced21ea832f34ed8ec46640f020",
                "blob_url": "https://github.com/apache/accumulo/blob/79182991d0da0f7b7836a901cec78b3ea570808c/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java?ref=79182991d0da0f7b7836a901cec78b3ea570808c"
            }
        ],
        "bug_id": "accumulo_7",
        "parent": "https://github.com/apache/accumulo/commit/e77bdd7de25af87c53da6e5a3727c028956c1e00",
        "message": "ACCUMULO-4196: Protect against NPE",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/17d26bda481a5b62295d9c5f56b58680005d6b14",
        "file": [
            {
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.accumulo.core.master.thrift.TabletServerStatus;\n import org.apache.accumulo.core.tabletserver.thrift.TabletStats;\n import org.apache.accumulo.server.conf.ServerConfiguration;\n+import org.apache.accumulo.server.conf.ServerConfigurationFactory;\n import org.apache.accumulo.server.master.state.TServerInstance;\n import org.apache.accumulo.server.master.state.TabletMigration;\n import org.apache.commons.lang.builder.ToStringBuilder;\n@@ -254,7 +255,7 @@ public int getMaxConcurrentMigrations() {\n   }\n \n   @Override\n-  public void init(ServerConfiguration conf) {\n+  public void init(ServerConfigurationFactory conf) {\n     super.init(conf);\n     parseConfiguration(conf);\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/17d26bda481a5b62295d9c5f56b58680005d6b14/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "6b26dc4c7ffee019527788b388d91fb94099469b",
                "blob_url": "https://github.com/apache/accumulo/blob/17d26bda481a5b62295d9c5f56b58680005d6b14/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java?ref=17d26bda481a5b62295d9c5f56b58680005d6b14"
            }
        ],
        "bug_id": "accumulo_8",
        "parent": "https://github.com/apache/accumulo/commit/3bd701b883492f06766631af30e7b08f14d3454c",
        "message": "[ACCUMULO-4535] Fix NPE in HostRegexTableLoadBalancer\n\nSigned-off-by: Josh Elser <elserj@apache.org>",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/a247f694ac080004ff932b7131152111ec8e7179",
        "file": [
            {
                "patch": "@@ -113,6 +113,7 @@ public static boolean exists(Instance instance, String tableId) {\n   public static void clearCache(Instance instance) {\n     cacheResetCount.incrementAndGet();\n     getZooCache(instance).clear(ZooUtil.getRoot(instance) + Constants.ZTABLES);\n+    getZooCache(instance).clear(ZooUtil.getRoot(instance) + Constants.ZNAMESPACES);\n   }\n   \n   public static String getPrintableTableNameFromId(Map<String,String> tidToNameMap, String tableId) {\n@@ -181,7 +182,6 @@ public static String extractTableName(String tableName) {\n   \n   public static String getNamespace(Instance instance, String tableId) {\n     ZooCache zc = getZooCache(instance);\n-    zc.clear();\n     byte[] n = zc.get(ZooUtil.getRoot(instance) + Constants.ZTABLES + \"/\" + tableId + Constants.ZTABLE_NAMESPACE);\n     return new String(n, Constants.UTF8);\n   }",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/client/impl/Tables.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "248bc6bc87dd9f57d92e5f16a050a78040ae39fd",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/client/impl/Tables.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/client/impl/Tables.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/client/impl/Tables.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -28,6 +28,7 @@\n \n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n+import org.apache.accumulo.core.client.TableNamespaceNotFoundException;\n import org.apache.accumulo.core.client.TableNotFoundException;\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.conf.Property;\n@@ -43,7 +44,7 @@\n import org.apache.commons.cli.Options;\n \n public class ConfigCommand extends Command {\n-  private Option tableOpt, deleteOpt, setOpt, filterOpt, disablePaginationOpt, outputFileOpt;\n+  private Option tableOpt, deleteOpt, setOpt, filterOpt, disablePaginationOpt, outputFileOpt, tableNamespaceOpt;\n   \n   private int COL1 = 8, COL2 = 7;\n   private ConsoleReader reader;\n@@ -62,13 +63,17 @@ public void registerCompletion(final Token root, final Map<Command.CompletionSet\n   }\n   \n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState) throws AccumuloException, AccumuloSecurityException,\n-      TableNotFoundException, IOException, ClassNotFoundException {\n+      TableNotFoundException, IOException, ClassNotFoundException, TableNamespaceNotFoundException {\n     reader = shellState.getReader();\n     \n     final String tableName = cl.getOptionValue(tableOpt.getOpt());\n     if (tableName != null && !shellState.getConnector().tableOperations().exists(tableName)) {\n       throw new TableNotFoundException(null, tableName, null);\n     }\n+    final String tableNamespace = cl.getOptionValue(tableNamespaceOpt.getOpt());\n+    if (tableNamespace != null && !shellState.getConnector().tableNamespaceOperations().exists(tableNamespace)) {\n+      throw new TableNotFoundException(null, tableName, null);\n+    }\n     if (cl.hasOption(deleteOpt.getOpt())) {\n       // delete property from table\n       String property = cl.getOptionValue(deleteOpt.getOpt());\n@@ -81,6 +86,12 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n         }\n         shellState.getConnector().tableOperations().removeProperty(tableName, property);\n         Shell.log.debug(\"Successfully deleted table configuration option.\");\n+      } else if (tableNamespace != null) {\n+        if (!Property.isValidTablePropertyKey(property)) {\n+          Shell.log.warn(\"Invalid per-table property : \" + property + \", still removing from zookeeper if it's there.\");\n+        }\n+        shellState.getConnector().tableNamespaceOperations().removeProperty(tableNamespace, property);\n+        Shell.log.debug(\"Successfully deleted table namespace configuration option.\");\n       } else {\n         if (!Property.isValidZooPropertyKey(property)) {\n           Shell.log.warn(\"Invalid per-table property : \" + property + \", still removing from zookeeper if it's there.\");\n@@ -107,6 +118,15 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n         }\n         shellState.getConnector().tableOperations().setProperty(tableName, property, value);\n         Shell.log.debug(\"Successfully set table configuration option.\");\n+      } else if (tableNamespace != null) {\n+        if (!Property.isValidTablePropertyKey(property)) {\n+          throw new BadArgumentException(\"Invalid per-table property.\", fullCommand, fullCommand.indexOf(property));\n+        }\n+        if (property.equals(Property.TABLE_DEFAULT_SCANTIME_VISIBILITY.getKey())) {\n+          new ColumnVisibility(value); // validate that it is a valid expression\n+        }\n+        shellState.getConnector().tableNamespaceOperations().setProperty(tableNamespace, property, value);\n+        Shell.log.debug(\"Successfully set table configuration option.\");\n       } else {\n         if (!Property.isValidZooPropertyKey(property)) {\n           throw new BadArgumentException(\"Property cannot be modified in zookeeper\", fullCommand, fullCommand.indexOf(property));\n@@ -132,6 +152,8 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n       Iterable<Entry<String,String>> acuconf = shellState.getConnector().instanceOperations().getSystemConfiguration().entrySet();\n       if (tableName != null) {\n         acuconf = shellState.getConnector().tableOperations().getProperties(tableName);\n+      } else if (tableNamespace != null) {\n+        acuconf = shellState.getConnector().tableNamespaceOperations().getProperties(tableNamespace);\n       }\n       final TreeMap<String,String> sortedConf = new TreeMap<String,String>();\n       for (Entry<String,String> propEntry : acuconf) {\n@@ -145,7 +167,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n         if (cl.hasOption(filterOpt.getOpt()) && !key.contains(cl.getOptionValue(filterOpt.getOpt()))) {\n           continue;\n         }\n-        if (tableName != null && !Property.isValidTablePropertyKey(key)) {\n+        if ((tableName != null || tableNamespace != null) && !Property.isValidTablePropertyKey(key)) {\n           continue;\n         }\n         COL2 = Math.max(COL2, propEntry.getKey().length() + 3);\n@@ -162,7 +184,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n         if (cl.hasOption(filterOpt.getOpt()) && !key.contains(cl.getOptionValue(filterOpt.getOpt()))) {\n           continue;\n         }\n-        if (tableName != null && !Property.isValidTablePropertyKey(key)) {\n+        if ((tableName != null || tableNamespace != null) && !Property.isValidTablePropertyKey(key)) {\n           continue;\n         }\n         String siteVal = siteConfig.get(key);\n@@ -190,7 +212,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n         }\n         \n         // show per-table value only if it is different (overridden)\n-        if (tableName != null && !curVal.equals(sysVal)) {\n+        if ((tableName != null || tableNamespace != null) && !curVal.equals(sysVal)) {\n           printConfLine(output, \"table\", printed ? \"   @override\" : key, curVal);\n         }\n       }\n@@ -231,25 +253,31 @@ public String description() {\n   public Options getOptions() {\n     final Options o = new Options();\n     final OptionGroup og = new OptionGroup();\n+    final OptionGroup tgroup = new OptionGroup();\n     \n     tableOpt = new Option(Shell.tableOption, \"table\", true, \"table to display/set/delete properties for\");\n     deleteOpt = new Option(\"d\", \"delete\", true, \"delete a per-table property\");\n     setOpt = new Option(\"s\", \"set\", true, \"set a per-table property\");\n     filterOpt = new Option(\"f\", \"filter\", true, \"show only properties that contain this string\");\n     disablePaginationOpt = new Option(\"np\", \"no-pagination\", false, \"disables pagination of output\");\n     outputFileOpt = new Option(\"o\", \"output\", true, \"local file to write the scan output to\");\n+    tableNamespaceOpt = new Option(\"tn\", \"table-namespace\", true, \"table namespace to display/set/delete properties for\");\n \n     tableOpt.setArgName(\"table\");\n     deleteOpt.setArgName(\"property\");\n     setOpt.setArgName(\"property=value\");\n     filterOpt.setArgName(\"string\");\n     outputFileOpt.setArgName(\"file\");\n+    tableNamespaceOpt.setArgName(\"tableNamespace\");\n     \n     og.addOption(deleteOpt);\n     og.addOption(setOpt);\n     og.addOption(filterOpt);\n     \n-    o.addOption(tableOpt);\n+    tgroup.addOption(tableOpt);\n+    tgroup.addOption(tableNamespaceOpt);\n+    \n+    o.addOptionGroup(tgroup);\n     o.addOptionGroup(og);\n     o.addOption(disablePaginationOpt);\n     o.addOption(outputFileOpt);",
                "additions": 34,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/util/shell/commands/ConfigCommand.java",
                "status": "modified",
                "changes": 40,
                "deletions": 6,
                "sha": "dcfc48cb37de0c9947c7c1fc88986bb2cf08bb00",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/util/shell/commands/ConfigCommand.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/util/shell/commands/ConfigCommand.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/shell/commands/ConfigCommand.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -30,10 +30,11 @@\n import org.apache.accumulo.core.util.shell.Shell.Command;\n import org.apache.commons.cli.CommandLine;\n import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.OptionGroup;\n import org.apache.commons.cli.Options;\n \n public class CreateNamespaceCommand extends Command {\n-  private Option createTableOptCopyConfig;\n+  private Option createTableOptCopyConfig, createTableNamespaceOptCopyConfig;\n   private Option base64Opt;\n   \n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState) throws AccumuloException, AccumuloSecurityException,\n@@ -48,14 +49,22 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n     shellState.getConnector().tableNamespaceOperations().create(namespace);\n     \n     // Copy options if flag was set\n-    if (cl.hasOption(createTableOptCopyConfig.getOpt())) {\n+    Iterable<Entry<String,String>> configuration = null;\n+    if (cl.hasOption(createTableNamespaceOptCopyConfig.getOpt())) {\n+      String copy = cl.getOptionValue(createTableNamespaceOptCopyConfig.getOpt());\n+      if (shellState.getConnector().tableNamespaceOperations().exists(namespace)) {\n+        configuration = shellState.getConnector().tableNamespaceOperations().getProperties(copy);\n+      }\n+    } else if (cl.hasOption(createTableOptCopyConfig.getOpt())) {\n       String copy = cl.getOptionValue(createTableOptCopyConfig.getOpt());\n       if (shellState.getConnector().tableNamespaceOperations().exists(namespace)) {\n-        Iterable<Entry<String,String>> configuration = shellState.getConnector().tableNamespaceOperations().getProperties(copy);\n-        for (Entry<String,String> entry : configuration) {\n-          if (Property.isValidTablePropertyKey(entry.getKey())) {\n-            shellState.getConnector().tableNamespaceOperations().setProperty(namespace, entry.getKey(), entry.getValue());\n-          }\n+        configuration = shellState.getConnector().tableOperations().getProperties(copy);\n+      }\n+    }\n+    if (configuration != null) {\n+      for (Entry<String,String> entry : configuration) {\n+        if (Property.isValidTablePropertyKey(entry.getKey())) {\n+          shellState.getConnector().tableNamespaceOperations().setProperty(namespace, entry.getKey(), entry.getValue());\n         }\n       }\n     }\n@@ -77,13 +86,19 @@ public String usage() {\n   public Options getOptions() {\n     final Options o = new Options();\n     \n-    createTableOptCopyConfig = new Option(\"cc\", \"copy-config\", true, \"table namespace to copy configuration from\");\n+    createTableNamespaceOptCopyConfig = new Option(\"cc\", \"copy-config\", true, \"table namespace to copy configuration from\");\n+    createTableNamespaceOptCopyConfig.setArgName(\"tableNamespace\");\n     \n-    createTableOptCopyConfig.setArgName(\"tableNamespace\");\n+    createTableOptCopyConfig = new Option(\"ctc\", \"copy-table-config\", true, \"table to copy configuration from\");\n+    createTableOptCopyConfig.setArgName(\"tableName\");\n     \n     base64Opt = new Option(\"b64\", \"base64encoded\", false, \"decode encoded split points\");\n     o.addOption(base64Opt);\n-    o.addOption(createTableOptCopyConfig);\n+    OptionGroup ogp = new OptionGroup();\n+    ogp.addOption(createTableOptCopyConfig);\n+    ogp.addOption(createTableNamespaceOptCopyConfig);\n+    \n+    o.addOptionGroup(ogp);\n     \n     return o;\n   }",
                "additions": 25,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/util/shell/commands/CreateNamespaceCommand.java",
                "status": "modified",
                "changes": 35,
                "deletions": 10,
                "sha": "a869070ec4d74f5adf2844fc1a2f4b8d5d7677dd",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/util/shell/commands/CreateNamespaceCommand.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/util/shell/commands/CreateNamespaceCommand.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/shell/commands/CreateNamespaceCommand.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -51,7 +51,7 @@ public int execute(final String fullCommand, final CommandLine cl, final Shell s\n       tableSet.add(cl.getOptionValue(optTableName.getOpt()));\n     } else if (cl.hasOption(optTableNamespace.getOpt())) {\n       Instance instance = shellState.getInstance();\n-      String namespaceId = TableNamespaces.getNamespaceId(instance, optTableNamespace.getValue());\n+      String namespaceId = TableNamespaces.getNamespaceId(instance, cl.getOptionValue(optTableNamespace.getOpt()));\n       for (String tableId : TableNamespaces.getTableIds(instance, namespaceId)) {\n         tableSet.add(Tables.getTableName(instance, tableId));\n       }\n@@ -108,8 +108,8 @@ public Options getOptions() {\n     optTableName = new Option(Shell.tableOption, \"table\", true, \"name of a table to operate on\");\n     optTableName.setArgName(\"tableName\");\n     \n-    optTableNamespace = new Option(\"tn\", \"table-namespace\", true, \"name of a table namespace to operate on\");\n-    optTableName.setArgName(\"table-namespace\");\n+    optTableNamespace = new Option(\"tn\", \"tableNamespace\", true, \"name of a table namespace to operate on\");\n+    optTableNamespace.setArgName(\"tableNamespace\");\n     \n     final OptionGroup opg = new OptionGroup();\n     ",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/util/shell/commands/TableOperation.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "4b8b304da265be8bc7162582387352b2116c592c",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/core/src/main/java/org/apache/accumulo/core/util/shell/commands/TableOperation.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/util/shell/commands/TableOperation.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/shell/commands/TableOperation.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -30,10 +30,12 @@\n import org.apache.accumulo.core.client.admin.TableOperations;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.KeyExtent;\n+import org.apache.accumulo.core.master.state.tables.TableState;\n import org.apache.accumulo.core.master.thrift.TabletServerStatus;\n import org.apache.accumulo.server.master.state.TServerInstance;\n import org.apache.accumulo.server.master.state.TabletMigration;\n import org.apache.accumulo.server.security.SystemCredentials;\n+import org.apache.accumulo.server.master.state.tables.TableManager;\n import org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader;\n import org.apache.log4j.Logger;\n \n@@ -50,7 +52,9 @@ private TabletBalancer constructNewBalancerForTable(String clazzName, String tab\n   }\n   \n   protected String getLoadBalancerClassNameForTable(String table) {\n-    return configuration.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n+    if (TableManager.getInstance().getTableState(table).equals(TableState.ONLINE))\n+      return configuration.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n+    return null;\n   }\n   \n   protected TabletBalancer getBalancerForTable(String table) {",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "status": "modified",
                "changes": 6,
                "deletions": 1,
                "sha": "9de6ec2388f28f09fd9c7846da08ee8955218a1e",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -151,11 +151,7 @@ public long isReady(long tid, Master environment) throws Exception {\n       \n       String namespace = Tables.extractNamespace(cloneInfo.tableName);\n       String namespaceId = TableNamespaces.getNamespaceId(instance, namespace);\n-      TableManager tm = TableManager.getInstance();\n-      if (!TableNamespaces.getNameToIdMap(instance).containsKey(namespace)) {\n-        tm.addNamespace(namespaceId, namespace, NodeExistsPolicy.SKIP);\n-      }\n-      tm.addNamespaceToTable(cloneInfo.tableId, namespaceId);\n+      TableManager.getInstance().addNamespaceToTable(cloneInfo.tableId, namespaceId);\n       \n       return new CloneMetadata(cloneInfo);\n     } finally {",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/server/master/src/main/java/org/apache/accumulo/master/tableOps/CloneTable.java",
                "status": "modified",
                "changes": 6,
                "deletions": 5,
                "sha": "384146304971b12c6816d7586814921500680046",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/server/master/src/main/java/org/apache/accumulo/master/tableOps/CloneTable.java",
                "filename": "server/master/src/main/java/org/apache/accumulo/master/tableOps/CloneTable.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/tableOps/CloneTable.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.accumulo.core.client.Connector;\n import org.apache.accumulo.core.client.TableExistsException;\n+import org.apache.accumulo.core.client.impl.Tables;\n import org.apache.accumulo.test.randomwalk.State;\n import org.apache.accumulo.test.randomwalk.Test;\n \n@@ -39,6 +40,10 @@ public void visit(State state, Properties props) throws Exception {\n     String tableName = tableNames.get(rand.nextInt(tableNames.size()));\n     \n     try {\n+      String n = Tables.extractNamespace(tableName);\n+      if (!conn.tableNamespaceOperations().exists(n)) {\n+        conn.tableNamespaceOperations().create(n);\n+      }\n       conn.tableOperations().create(tableName);\n       log.debug(\"Created table \" + tableName);\n     } catch (TableExistsException e) {",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/CreateTable.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "bbd18a9f6ccc5735edd4123feff126a58eab95a9",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/CreateTable.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/CreateTable.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/CreateTable.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.accumulo.core.client.Connector;\n import org.apache.accumulo.core.client.TableExistsException;\n import org.apache.accumulo.core.client.TableNotFoundException;\n+import org.apache.accumulo.core.client.impl.Tables;\n import org.apache.accumulo.test.randomwalk.State;\n import org.apache.accumulo.test.randomwalk.Test;\n \n@@ -41,6 +42,10 @@ public void visit(State state, Properties props) throws Exception {\n     String newTableName = tableNames.get(rand.nextInt(tableNames.size()));\n     \n     try {\n+      String n = Tables.extractNamespace(newTableName);\n+      if (!conn.tableNamespaceOperations().exists(n)) {\n+        conn.tableNamespaceOperations().create(n);\n+      }\n       conn.tableOperations().rename(srcTableName, newTableName);\n       log.debug(\"Renamed table \" + srcTableName + \" \" + newTableName);\n     } catch (TableExistsException e) {",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/RenameTable.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "690d6cf9fa22718a154f2aefbb32a45c900ea4ba",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/RenameTable.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/RenameTable.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/RenameTable.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -34,8 +34,11 @@ public void visit(State state, Properties props) throws Exception {\n     int numTables = Integer.parseInt(props.getProperty(\"numTables\", \"5\"));\n     log.debug(\"numTables = \" + numTables);\n     List<String> tables = new ArrayList<String>();\n-    for (int i = 0; i < numTables; i++)\n-      tables.add(String.format(\"ctt_%03d\", i));\n+    for (int i = 0; i < numTables - 1; i++) {\n+      tables.add(String.format(\"nspace_%03d.ctt_%03d\", i, i));\n+    }\n+    tables.add(String.format(\"ctt_%03d\", numTables - 1));\n+\n     state.set(\"tables\", tables);\n     \n     int numUsers = Integer.parseInt(props.getProperty(\"numUsers\", \"5\"));",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/Setup.java",
                "status": "modified",
                "changes": 7,
                "deletions": 2,
                "sha": "5a9edca4e6a50f0c05040ccdbd7aafa9afeb72d3",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/Setup.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/Setup.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/Setup.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            },
            {
                "patch": "@@ -278,4 +278,50 @@ public void testNamespaceRename() throws Exception {\n     String tnamespace = TableNamespaces.getNamespaceName(instance, tnid);\n     assertTrue(namespace2.equals(tnamespace));\n   }\n+  \n+  /**\n+   * This test clones a table to a different namespace and ensures it's properties are correct\n+   */\n+  @Test\n+  public void testCloneTableProperties() throws Exception {\n+    String n1 = \"namespace1\";\n+    String n2 = \"namespace2\";\n+    String t1 = n1 + \".table\";\n+    String t2 = n2 + \".table\";\n+    \n+    String propKey = Property.TABLE_FILE_MAX.getKey();\n+    String propVal1 = \"55\";\n+    String propVal2 = \"66\";\n+    \n+    Connector c = accumulo.getConnector(\"root\", secret);\n+    \n+    c.tableNamespaceOperations().create(n1);\n+    c.tableOperations().create(t1);\n+    \n+    c.tableOperations().removeProperty(t1, Property.TABLE_FILE_MAX.getKey());\n+    c.tableNamespaceOperations().setProperty(n1, propKey, propVal1);\n+    \n+    boolean itWorked = false;\n+    for (Entry<String,String> prop : c.tableOperations().getProperties(t1)) {\n+      if (prop.getKey().equals(propKey) && prop.getValue().equals(propVal1)) {\n+        itWorked = true;\n+        break;\n+      }\n+    }\n+    assertTrue(itWorked);\n+    \n+    c.tableNamespaceOperations().create(n2);\n+    c.tableNamespaceOperations().setProperty(n2, propKey, propVal2);\n+    c.tableOperations().clone(t1, t2, true, null, null);\n+    c.tableOperations().removeProperty(t2, propKey);\n+    \n+    itWorked = false;\n+    for (Entry<String,String> prop : c.tableOperations().getProperties(t2)) {\n+      if (prop.getKey().equals(propKey) && prop.getValue().equals(propVal2)) {\n+        itWorked = true;\n+        break;\n+      }\n+    }\n+    assertTrue(itWorked);\n+  }\n }",
                "additions": 46,
                "raw_url": "https://github.com/apache/accumulo/raw/a247f694ac080004ff932b7131152111ec8e7179/test/src/test/java/org/apache/accumulo/test/TableNamespacesTest.java",
                "status": "modified",
                "changes": 46,
                "deletions": 0,
                "sha": "8c1a54a186665034d79d11495614840e7c0e3dad",
                "blob_url": "https://github.com/apache/accumulo/blob/a247f694ac080004ff932b7131152111ec8e7179/test/src/test/java/org/apache/accumulo/test/TableNamespacesTest.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/TableNamespacesTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/TableNamespacesTest.java?ref=a247f694ac080004ff932b7131152111ec8e7179"
            }
        ],
        "bug_id": "accumulo_9",
        "parent": "https://github.com/apache/accumulo/commit/da42532a400de18e3cba7c651a305fb5a6f9825f",
        "message": "ACCUMULO-802 Fixed NullPointerException during CloneTable due to loadbalancer trying to get the configuration before the namespace is put under the table in zookeeper. Fixed another NullPointerException with the -tn shell option for online, offline, and du. Added -tn option to shell config command. First attempt at adding a randomwalk test for namespaces, fails currently.",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/18a3ef6b1b85c5b5043880f218567071950cc10b",
        "file": [
            {
                "patch": "@@ -435,6 +435,10 @@ public void undo(long tid, Master env) throws Exception {\n \n     log.info(\"Looking for matching filesystem for \" + exportDir + \" from options \" + Arrays.toString(tableDirs));\n     Path base = master.getFileSystem().matchingFileSystem(exportDir, tableDirs);\n+    if (base == null) {\n+      throw new IOException(tableInfo.exportDir + \" is not in a volume configured for Accumulo\");\n+    }\n+\n     log.info(\"Chose base table directory of \" + base);\n     Path directory = new Path(base, tableInfo.tableId);\n ",
                "additions": 4,
                "raw_url": "https://github.com/apache/accumulo/raw/18a3ef6b1b85c5b5043880f218567071950cc10b/server/master/src/main/java/org/apache/accumulo/master/tableOps/ImportTable.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "4bf6959e05a788f7114159a0620b29b10549b92e",
                "blob_url": "https://github.com/apache/accumulo/blob/18a3ef6b1b85c5b5043880f218567071950cc10b/server/master/src/main/java/org/apache/accumulo/master/tableOps/ImportTable.java",
                "filename": "server/master/src/main/java/org/apache/accumulo/master/tableOps/ImportTable.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/tableOps/ImportTable.java?ref=18a3ef6b1b85c5b5043880f218567071950cc10b"
            },
            {
                "patch": "@@ -294,7 +294,7 @@ public void run() {\n   private static long jitter(long ms) {\n     Random r = new Random();\n     // add a random 10% wait\n-    return (long)((1. + (r.nextDouble() / 10)) * ms);\n+    return (long) ((1. + (r.nextDouble() / 10)) * ms);\n   }\n \n   private synchronized static void logGCInfo(AccumuloConfiguration conf) {\n@@ -2538,6 +2538,10 @@ public void removeLogs(TInfo tinfo, TCredentials credentials, List<String> filen\n           Path source = new Path(filename);\n           if (acuConf.getBoolean(Property.TSERV_ARCHIVE_WALOGS)) {\n             Path walogArchive = fs.matchingFileSystem(source, ServerConstants.getWalogArchives());\n+            if (walogArchive == null) {\n+              throw new IOException(filename + \" is not in a volume configured for Accumulo\");\n+            }\n+\n             fs.mkdirs(walogArchive);\n             Path dest = new Path(walogArchive, source.getName());\n             log.info(\"Archiving walog \" + source + \" to \" + dest);",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/18a3ef6b1b85c5b5043880f218567071950cc10b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java",
                "status": "modified",
                "changes": 6,
                "deletions": 1,
                "sha": "0446da3d42a3141e86d2d3c4fc39f46c2a61836d",
                "blob_url": "https://github.com/apache/accumulo/blob/18a3ef6b1b85c5b5043880f218567071950cc10b/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java",
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java?ref=18a3ef6b1b85c5b5043880f218567071950cc10b"
            }
        ],
        "bug_id": "accumulo_10",
        "parent": "https://github.com/apache/accumulo/commit/8c0d881c32c107039f29a4f43df230e2a098804f",
        "message": "ACCUMULO-3216 Swap NPE for more informative error",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/204bcfedcd9840b8b95bf48b81d5d1e4eaed5016",
        "file": [
            {
                "patch": "@@ -1478,7 +1478,7 @@ private void flush(UpdateSession us) {\n \n             KeyExtent extent = commitSession.getExtent();\n \n-            if (extent == us.currentTablet.getExtent()) {\n+            if (us.currentTablet != null && extent == us.currentTablet.getExtent()) {\n               // because constraint violations may filter out some\n               // mutations, for proper accounting with the client code, \n               // need to increment the count based on the original ",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/204bcfedcd9840b8b95bf48b81d5d1e4eaed5016/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "ce84bd38061224a627047983aeea1492ddb825a5",
                "blob_url": "https://github.com/apache/accumulo/blob/204bcfedcd9840b8b95bf48b81d5d1e4eaed5016/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java",
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java?ref=204bcfedcd9840b8b95bf48b81d5d1e4eaed5016"
            }
        ],
        "bug_id": "accumulo_11",
        "parent": "https://github.com/apache/accumulo/commit/8071252801817dc88d5e6b50fa58ad3b0a629446",
        "message": "ACCUMULO-2255 fix NPE during commit",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/47ca312c950042c4b277a98cbb4eed8a37bf9afd",
        "file": [
            {
                "patch": "@@ -235,12 +235,11 @@ public void run() {\n           } catch (IllegalAccessException e) {\n             LOG.error(\"Illegal acess exception\", e);\n             bloomFilter = null;\n-          } catch (NullPointerException npe) {\n+          } catch (RuntimeException rte) {\n             if (!closed)\n-              throw npe;\n+              throw rte;\n             else\n-              LOG.debug(\"Can't open BloomFilter, NPE after closed \", npe);\n-            \n+              LOG.debug(\"Can't open BloomFilter, RTE after closed \", rte);\n           } finally {\n             if (in != null) {\n               try {",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/47ca312c950042c4b277a98cbb4eed8a37bf9afd/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java",
                "status": "modified",
                "changes": 7,
                "deletions": 4,
                "sha": "e79da373146428749fdcf87f26f721aaf9bc932f",
                "blob_url": "https://github.com/apache/accumulo/blob/47ca312c950042c4b277a98cbb4eed8a37bf9afd/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java?ref=47ca312c950042c4b277a98cbb4eed8a37bf9afd"
            }
        ],
        "bug_id": "accumulo_12",
        "parent": "https://github.com/apache/accumulo/commit/d183132c40efdbc2fcacf92a64c43f87454792ee",
        "message": "ACCUMULO-2202 catch all runtime exceptions, not just NPE",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/91be551f655bf0a1d80493c95eafadd0025cfe40",
        "file": [
            {
                "patch": "@@ -249,11 +249,11 @@ public void run() {\n           } catch (IllegalAccessException e) {\n             LOG.error(\"Illegal acess exception\", e);\n             bloomFilter = null;\n-          } catch (NullPointerException npe) {\n+          } catch (RuntimeException rte) {\n             if (!closed)\n-              throw npe;\n+              throw rte;\n             else\n-              LOG.debug(\"Can't open BloomFilter, NPE after closed \", npe);\n+              LOG.debug(\"Can't open BloomFilter, RTE after closed \", rte);\n \n           } finally {\n             if (in != null) {",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/91be551f655bf0a1d80493c95eafadd0025cfe40/src/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "98f82e1e1a7510dbb1e09210c3edd7233d82f8a3",
                "blob_url": "https://github.com/apache/accumulo/blob/91be551f655bf0a1d80493c95eafadd0025cfe40/src/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java",
                "filename": "src/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/core/src/main/java/org/apache/accumulo/core/file/BloomFilterLayer.java?ref=91be551f655bf0a1d80493c95eafadd0025cfe40"
            }
        ],
        "bug_id": "accumulo_13",
        "parent": "https://github.com/apache/accumulo/commit/5fdecd72e10bb3531797d365946e657b154ffca7",
        "message": "ACCUMULO-2202 catch all runtime exceptions, not just NPE",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/0e3fd3b3cc26496f5a3e898c52f8fded16a5f203",
        "file": [
            {
                "patch": "@@ -52,7 +52,10 @@ private TabletBalancer constructNewBalancerForTable(String clazzName, String tab\n   }\n \n   protected String getLoadBalancerClassNameForTable(String table) {\n-    if (TableManager.getInstance().getTableState(table).equals(TableState.ONLINE))\n+    TableState tableState = TableManager.getInstance().getTableState(table);\n+    if (tableState == null)\n+      return null;\n+    if (tableState.equals(TableState.ONLINE))\n       return configuration.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n     return null;\n   }",
                "additions": 4,
                "raw_url": "https://github.com/apache/accumulo/raw/0e3fd3b3cc26496f5a3e898c52f8fded16a5f203/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "f2478b1979b6d9a528556989d346ca8b9ec1b72c",
                "blob_url": "https://github.com/apache/accumulo/blob/0e3fd3b3cc26496f5a3e898c52f8fded16a5f203/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=0e3fd3b3cc26496f5a3e898c52f8fded16a5f203"
            }
        ],
        "bug_id": "accumulo_14",
        "parent": "https://github.com/apache/accumulo/commit/37c9a76017eac42577fbbc4c217f2af76ea84779",
        "message": "ACCUMULO-2028 avoid NPE during a table delete when balancing",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/1c44069a35a1acfb2b806e1103c1992190c389eb",
        "file": [
            {
                "patch": "@@ -113,6 +113,8 @@\n   private Instance instance;\n   private TCredentials credentials;\n   \n+  public static final String CLONE_EXCLUDE_PREFIX = \"!\";\n+\n   private static final Logger log = Logger.getLogger(TableOperations.class);\n   \n   /**\n@@ -673,9 +675,15 @@ public void clone(String srcTableName, String newTableName, boolean flush, Map<S\n     \n     List<ByteBuffer> args = Arrays.asList(ByteBuffer.wrap(srcTableId.getBytes()), ByteBuffer.wrap(newTableName.getBytes()));\n     Map<String,String> opts = new HashMap<String,String>();\n-    opts.putAll(propertiesToSet);\n-    for (String prop : propertiesToExclude)\n-      opts.put(prop, null);\n+    for (Entry<String,String> entry : propertiesToSet.entrySet()) {\n+      if (entry.getKey().startsWith(CLONE_EXCLUDE_PREFIX))\n+        throw new IllegalArgumentException(\"Property can not start with \" + CLONE_EXCLUDE_PREFIX);\n+      opts.put(entry.getKey(), entry.getValue());\n+    }\n+    \n+    for (String prop : propertiesToExclude) {\n+      opts.put(CLONE_EXCLUDE_PREFIX + prop, \"\");\n+    }\n     \n     doTableOperation(TableOperation.CLONE, args, opts);\n   }",
                "additions": 11,
                "raw_url": "https://github.com/apache/accumulo/raw/1c44069a35a1acfb2b806e1103c1992190c389eb/core/src/main/java/org/apache/accumulo/core/client/admin/TableOperationsImpl.java",
                "status": "modified",
                "changes": 14,
                "deletions": 3,
                "sha": "3aca348b29d2f3c4c920534f5591434224ed8f74",
                "blob_url": "https://github.com/apache/accumulo/blob/1c44069a35a1acfb2b806e1103c1992190c389eb/core/src/main/java/org/apache/accumulo/core/client/admin/TableOperationsImpl.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/client/admin/TableOperationsImpl.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/client/admin/TableOperationsImpl.java?ref=1c44069a35a1acfb2b806e1103c1992190c389eb"
            },
            {
                "patch": "@@ -50,6 +50,7 @@\n import org.apache.accumulo.core.client.RowIterator;\n import org.apache.accumulo.core.client.Scanner;\n import org.apache.accumulo.core.client.TableNotFoundException;\n+import org.apache.accumulo.core.client.admin.TableOperationsImpl;\n import org.apache.accumulo.core.client.impl.Tables;\n import org.apache.accumulo.core.client.impl.ThriftTransportPool;\n import org.apache.accumulo.core.client.impl.thrift.SecurityErrorCode;\n@@ -661,7 +662,7 @@ private void alterTableProperty(TCredentials c, String tableName, String propert\n         throw new ThriftSecurityException(c.getPrincipal(), SecurityErrorCode.PERMISSION_DENIED);\n       \n       try {\n-        if (value == null) {\n+        if (value == null || value.isEmpty()) {\n           TablePropUtil.removeTableProperty(tableId, property);\n         } else if (!TablePropUtil.setTableProperty(tableId, property, value)) {\n           throw new Exception(\"Invalid table property.\");\n@@ -861,8 +862,8 @@ public void executeTableOperation(TInfo tinfo, TCredentials c, long opid, org.ap\n           Set<String> propertiesToExclude = new HashSet<String>();\n           \n           for (Entry<String,String> entry : options.entrySet()) {\n-            if (entry.getValue() == null) {\n-              propertiesToExclude.add(entry.getKey());\n+            if (entry.getKey().startsWith(TableOperationsImpl.CLONE_EXCLUDE_PREFIX)) {\n+              propertiesToExclude.add(entry.getKey().substring(TableOperationsImpl.CLONE_EXCLUDE_PREFIX.length()));\n               continue;\n             }\n             ",
                "additions": 4,
                "raw_url": "https://github.com/apache/accumulo/raw/1c44069a35a1acfb2b806e1103c1992190c389eb/server/src/main/java/org/apache/accumulo/server/master/Master.java",
                "status": "modified",
                "changes": 7,
                "deletions": 3,
                "sha": "64e29306db0df1b2324748ff3b2b76e6362f3b60",
                "blob_url": "https://github.com/apache/accumulo/blob/1c44069a35a1acfb2b806e1103c1992190c389eb/server/src/main/java/org/apache/accumulo/server/master/Master.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/master/Master.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/master/Master.java?ref=1c44069a35a1acfb2b806e1103c1992190c389eb"
            },
            {
                "patch": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.test;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+\n+import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.client.BatchWriter;\n+import org.apache.accumulo.core.client.BatchWriterConfig;\n+import org.apache.accumulo.core.client.Connector;\n+import org.apache.accumulo.core.client.Scanner;\n+import org.apache.accumulo.core.client.ZooKeeperInstance;\n+import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.minicluster.MiniAccumuloCluster;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+/**\n+ * \n+ */\n+public class CloneTest {\n+  \n+  public static TemporaryFolder folder = new TemporaryFolder();\n+  private MiniAccumuloCluster accumulo;\n+  private String secret = \"secret\";\n+  \n+  @Before\n+  public void setUp() throws Exception {\n+    folder.create();\n+    accumulo = new MiniAccumuloCluster(folder.getRoot(), secret);\n+    accumulo.start();\n+  }\n+  \n+  @After\n+  public void tearDown() throws Exception {\n+    accumulo.stop();\n+    folder.delete();\n+  }\n+  \n+  @Test(timeout = 120 * 1000)\n+  public void run() throws Exception {\n+    String table1 = \"clone1\";\n+    String table2 = \"clone2\";\n+    \n+    ZooKeeperInstance zki = new ZooKeeperInstance(accumulo.getInstanceName(), accumulo.getZooKeepers());\n+    Connector c = zki.getConnector(\"root\", new PasswordToken(secret));\n+    \n+    c.tableOperations().create(table1);\n+    \n+    c.tableOperations().setProperty(table1, Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey(), \"1M\");\n+    c.tableOperations().setProperty(table1, Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE_INDEX.getKey(), \"2M\");\n+    c.tableOperations().setProperty(table1, Property.TABLE_FILE_MAX.getKey(), \"23\");\n+    \n+    BatchWriter bw = c.createBatchWriter(table1, new BatchWriterConfig());\n+    \n+    Mutation m1 = new Mutation(\"001\");\n+    m1.put(\"data\", \"x\", \"9\");\n+    m1.put(\"data\", \"y\", \"7\");\n+    \n+    Mutation m2 = new Mutation(\"008\");\n+    m2.put(\"data\", \"x\", \"3\");\n+    m2.put(\"data\", \"y\", \"4\");\n+    \n+    bw.addMutation(m1);\n+    bw.addMutation(m2);\n+    \n+    bw.flush();\n+    \n+    Map<String,String> props = new HashMap<String,String>();\n+    props.put(Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey(), \"500K\");\n+    \n+    Set<String> exclude = new HashSet<String>();\n+    exclude.add(Property.TABLE_FILE_MAX.getKey());\n+    \n+    c.tableOperations().clone(table1, table2, true, props, exclude);\n+    \n+    Mutation m3 = new Mutation(\"009\");\n+    m3.put(\"data\", \"x\", \"1\");\n+    m3.put(\"data\", \"y\", \"2\");\n+    bw.addMutation(m3);\n+    bw.close();\n+\n+    Scanner scanner = c.createScanner(table2, Constants.NO_AUTHS);\n+    \n+    HashMap<String,String> expected = new HashMap<String,String>();\n+    expected.put(\"001:x\", \"9\");\n+    expected.put(\"001:y\", \"7\");\n+    expected.put(\"008:x\", \"3\");\n+    expected.put(\"008:y\", \"4\");\n+    \n+    HashMap<String,String> actual = new HashMap<String,String>();\n+    \n+    for (Entry<Key,Value> entry : scanner)\n+      actual.put(entry.getKey().getRowData().toString() + \":\" + entry.getKey().getColumnQualifierData().toString(), entry.getValue().toString());\n+    \n+    Assert.assertEquals(expected, actual);\n+    \n+    HashMap<String,String> tableProps = new HashMap<String,String>();\n+    for (Entry<String,String> prop : c.tableOperations().getProperties(table2)) {\n+      tableProps.put(prop.getKey(), prop.getValue());\n+    }\n+\n+    Assert.assertEquals(\"500K\", tableProps.get(Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE.getKey()));\n+    Assert.assertEquals(Property.TABLE_FILE_MAX.getDefaultValue(), tableProps.get(Property.TABLE_FILE_MAX.getKey()));\n+    Assert.assertEquals(\"2M\", tableProps.get(Property.TABLE_FILE_COMPRESSED_BLOCK_SIZE_INDEX.getKey()));\n+    \n+  }\n+}",
                "additions": 133,
                "raw_url": "https://github.com/apache/accumulo/raw/1c44069a35a1acfb2b806e1103c1992190c389eb/test/src/test/java/org/apache/accumulo/test/CloneTest.java",
                "status": "added",
                "changes": 133,
                "deletions": 0,
                "sha": "01affd98c4eb0faaeafc255006ae585e42c48a13",
                "blob_url": "https://github.com/apache/accumulo/blob/1c44069a35a1acfb2b806e1103c1992190c389eb/test/src/test/java/org/apache/accumulo/test/CloneTest.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/CloneTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/CloneTest.java?ref=1c44069a35a1acfb2b806e1103c1992190c389eb"
            }
        ],
        "bug_id": "accumulo_15",
        "parent": "https://github.com/apache/accumulo/commit/d8e5de664d12440df3ac629d53fb67e67b14db89",
        "message": "ACCUMULO-1565 fixed clone table NPE when excluding props",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12",
        "file": [
            {
                "patch": "@@ -246,6 +246,13 @@\n           <plugin>\n             <groupId>org.apache.maven.plugins</groupId>\n             <artifactId>maven-jar-plugin</artifactId>\n+            <configuration>\n+              <archive>\n+                <manifestEntries>\n+                  <Sealed>false</Sealed>\n+                </manifestEntries>\n+              </archive>\n+            </configuration>\n             <executions>\n               <execution>\n                 <id>make-test-jar</id>",
                "additions": 7,
                "raw_url": "https://github.com/apache/accumulo/raw/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/pom.xml",
                "status": "modified",
                "changes": 7,
                "deletions": 0,
                "sha": "c68e158c9b90671b66eca5a60d3c23f31c732ae1",
                "blob_url": "https://github.com/apache/accumulo/blob/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/pom.xml",
                "filename": "test/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/pom.xml?ref=f6bd3eecd8e7f5704ae9804e9cf07b205de78a12"
            },
            {
                "patch": "@@ -32,8 +32,11 @@\n import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n+import org.junit.runner.Description;\n import org.junit.runner.JUnitCore;\n import org.junit.runner.Result;\n+import org.junit.runner.notification.Failure;\n+import org.junit.runner.notification.RunListener;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -52,8 +55,27 @@ protected void map(LongWritable key, Text value, Mapper<LongWritable,Text,IntWri\n       } catch (ClassNotFoundException e) {\n         log.debug(\"Error finding class {}\", className, e);\n         context.write(new IntWritable(-1), new Text(e.toString()));\n+        return;\n       }\n       JUnitCore core = new JUnitCore();\n+      core.addListener(new RunListener() {\n+\n+        @Override\n+        public void testStarted(Description description) throws Exception {\n+          log.info(\"Starting {}\", description);\n+        }\n+\n+        @Override\n+        public void testFinished(Description description) throws Exception {\n+          log.info(\"Finished {}\", description);\n+        }\n+\n+        @Override\n+        public void testFailure(Failure failure) throws Exception {\n+          log.info(\"Test failed: {}\", failure.getDescription(), failure.getException());\n+        }\n+\n+      });\n       log.info(\"Running test {}\", className);\n       Result result = core.run(test);\n       if (result.wasSuccessful()) {",
                "additions": 22,
                "raw_url": "https://github.com/apache/accumulo/raw/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/IntegrationTestMapReduce.java",
                "status": "modified",
                "changes": 22,
                "deletions": 0,
                "sha": "42c10959913591c4867e385686656f925008e026",
                "blob_url": "https://github.com/apache/accumulo/blob/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/IntegrationTestMapReduce.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/IntegrationTestMapReduce.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/IntegrationTestMapReduce.java?ref=f6bd3eecd8e7f5704ae9804e9cf07b205de78a12"
            },
            {
                "patch": "@@ -105,7 +105,7 @@ public void resetMetadataPermission() throws Exception {\n   public void testEmptyWalRecoveryCompletes() throws Exception {\n     Connector conn = getConnector();\n     MiniAccumuloClusterImpl cluster = getCluster();\n-    FileSystem fs = FileSystem.get(new Configuration());\n+    FileSystem fs = cluster.getFileSystem();\n \n     // Fake out something that looks like host:port, it's irrelevant\n     String fakeServer = \"127.0.0.1:12345\";\n@@ -157,7 +157,7 @@ public void testEmptyWalRecoveryCompletes() throws Exception {\n   public void testPartialHeaderWalRecoveryCompletes() throws Exception {\n     Connector conn = getConnector();\n     MiniAccumuloClusterImpl cluster = getCluster();\n-    FileSystem fs = FileSystem.get(new Configuration());\n+    FileSystem fs = getCluster().getFileSystem();\n \n     // Fake out something that looks like host:port, it's irrelevant\n     String fakeServer = \"127.0.0.1:12345\";",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/MissingWalHeaderCompletesRecoveryIT.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "81c5d84be6d227f3d36355f26c7963f2ff8577e5",
                "blob_url": "https://github.com/apache/accumulo/blob/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/MissingWalHeaderCompletesRecoveryIT.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/MissingWalHeaderCompletesRecoveryIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/MissingWalHeaderCompletesRecoveryIT.java?ref=f6bd3eecd8e7f5704ae9804e9cf07b205de78a12"
            },
            {
                "patch": "@@ -49,7 +49,6 @@\n import org.apache.accumulo.harness.AccumuloClusterIT;\n import org.apache.accumulo.minicluster.impl.MiniAccumuloClusterImpl;\n import org.apache.accumulo.server.ServerConstants;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -209,7 +208,7 @@ public void testDeleteClone() throws Exception {\n     writeData(table3, c).close();\n     c.tableOperations().flush(table3, null, null, true);\n     // check for files\n-    FileSystem fs = FileSystem.get(new Configuration());\n+    FileSystem fs = getCluster().getFileSystem();\n     String id = c.tableOperations().tableIdMap().get(table3);\n     FileStatus[] status = fs.listStatus(new Path(rootPath + \"/accumulo/tables/\" + id));\n     assertTrue(status.length > 0);",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/functional/CloneTestIT.java",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "f7a91651b374063a655867e30b6e4f9f0afc7cfe",
                "blob_url": "https://github.com/apache/accumulo/blob/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/functional/CloneTestIT.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/functional/CloneTestIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/functional/CloneTestIT.java?ref=f6bd3eecd8e7f5704ae9804e9cf07b205de78a12"
            },
            {
                "patch": "@@ -16,6 +16,7 @@\n  */\n package org.apache.accumulo.test.replication;\n \n+import java.util.Arrays;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -57,7 +58,6 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.RawLocalFileSystem;\n import org.apache.hadoop.io.Text;\n-import org.bouncycastle.util.Arrays;\n import org.junit.Assert;\n import org.junit.Test;\n import org.slf4j.Logger;\n@@ -242,7 +242,7 @@ public void testActiveWalPrecludesClosing() throws Exception {\n \n     // Use the rfile which was just replaced by the MajC to determine when the GC has ran\n     Path fileToBeDeleted = new Path(filesForTable.iterator().next());\n-    FileSystem fs = fileToBeDeleted.getFileSystem(new Configuration());\n+    FileSystem fs = getCluster().getFileSystem();\n \n     boolean fileExists = fs.exists(fileToBeDeleted);\n     while (fileExists) {\n@@ -327,7 +327,7 @@ public void testUnreferencedWalInTserverIsClosed() throws Exception {\n \n     // Use the rfile which was just replaced by the MajC to determine when the GC has ran\n     Path fileToBeDeleted = new Path(filesForTable.iterator().next());\n-    FileSystem fs = fileToBeDeleted.getFileSystem(new Configuration());\n+    FileSystem fs = getCluster().getFileSystem();\n \n     boolean fileExists = fs.exists(fileToBeDeleted);\n     while (fileExists) {",
                "additions": 3,
                "raw_url": "https://github.com/apache/accumulo/raw/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/replication/GarbageCollectorCommunicatesWithTServersIT.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "38d227657d989554aaac4883a94320c3f21bca49",
                "blob_url": "https://github.com/apache/accumulo/blob/f6bd3eecd8e7f5704ae9804e9cf07b205de78a12/test/src/test/java/org/apache/accumulo/test/replication/GarbageCollectorCommunicatesWithTServersIT.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/replication/GarbageCollectorCommunicatesWithTServersIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/replication/GarbageCollectorCommunicatesWithTServersIT.java?ref=f6bd3eecd8e7f5704ae9804e9cf07b205de78a12"
            }
        ],
        "bug_id": "accumulo_16",
        "parent": "https://github.com/apache/accumulo/commit/c5e6804871548addccaab77e9bdd33191a6bd0ee",
        "message": "ACCUMULO-3871 fixed NPE and added more output to m/r runner; fixed some tests",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/079ef51c7c254f1f7bd7bd4d83ea405ae635b433",
        "file": [
            {
                "patch": "@@ -106,28 +106,28 @@\n \n public class BulkImport extends MasterRepo {\n   public static final String FAILURES_TXT = \"failures.txt\";\n-  \n+\n   private static final long serialVersionUID = 1L;\n-  \n+\n   private static final Logger log = Logger.getLogger(BulkImport.class);\n-  \n+\n   private String tableId;\n   private String sourceDir;\n   private String errorDir;\n   private boolean setTime;\n-  \n+\n   public BulkImport(String tableId, String sourceDir, String errorDir, boolean setTime) {\n     this.tableId = tableId;\n     this.sourceDir = sourceDir;\n     this.errorDir = errorDir;\n     this.setTime = setTime;\n   }\n-  \n+\n   @Override\n   public long isReady(long tid, Master master) throws Exception {\n     if (!Utils.getReadLock(tableId, tid).tryLock())\n       return 100;\n-    \n+\n     Instance instance = HdfsZooInstance.getInstance();\n     Tables.clearCache(instance);\n     if (Tables.getTableState(instance, tableId) == TableState.ONLINE) {\n@@ -140,18 +140,18 @@ public long isReady(long tid, Master master) throws Exception {\n       throw new ThriftTableOperationException(tableId, null, TableOperation.BULK_IMPORT, TableOperationExceptionType.OFFLINE, null);\n     }\n   }\n-  \n+\n   @Override\n   //TODO Remove deprecation warning suppression when Hadoop1 support is dropped\n   @SuppressWarnings(\"deprecation\")\n   public Repo<Master> call(long tid, Master master) throws Exception {\n     log.debug(\" tid \" + tid + \" sourceDir \" + sourceDir);\n-    \n+\n     Utils.getReadLock(tableId, tid).lock();\n-    \n+\n     // check that the error directory exists and is empty\n     VolumeManager fs = master.getFileSystem();\n-    \n+\n     Path errorPath = new Path(errorDir);\n     FileStatus errorStatus = null;\n     try {\n@@ -168,9 +168,9 @@ public long isReady(long tid, Master master) throws Exception {\n     if (fs.listStatus(errorPath).length != 0)\n       throw new ThriftTableOperationException(tableId, null, TableOperation.BULK_IMPORT, TableOperationExceptionType.BULK_BAD_ERROR_DIRECTORY, errorDir\n           + \" is not empty\");\n-    \n+\n     ZooArbitrator.start(Constants.BULK_ARBITRATOR_TYPE, tid);\n-    \n+\n     // move the files into the directory\n     try {\n       String bulkDir = prepareBulkImport(fs, sourceDir, tableId);\n@@ -182,32 +182,34 @@ public long isReady(long tid, Master master) throws Exception {\n           + ex);\n     }\n   }\n-  \n+\n   private Path createNewBulkDir(VolumeManager fs, String tableId) throws IOException {\n-    \n-    String tableDir = fs.matchingFileSystem(new Path(sourceDir), ServerConstants.getTablesDirs()).toString();\n-    \n+    Path tempPath = fs.matchingFileSystem(new Path(sourceDir), ServerConstants.getTablesDirs());\n+    if (tempPath == null)\n+      throw new IllegalStateException(sourceDir + \" is not in a known namespace\");\n+\n+    String tableDir = tempPath.toString();\n     if (tableDir == null)\n       throw new IllegalStateException(sourceDir + \" is not in a known namespace\");\n     Path directory = new Path(tableDir + \"/\" + tableId);\n     fs.mkdirs(directory);\n-    \n+\n     // only one should be able to create the lock file\n     // the purpose of the lock file is to avoid a race\n     // condition between the call to fs.exists() and\n     // fs.mkdirs()... if only hadoop had a mkdir() function\n     // that failed when the dir existed\n-    \n+\n     UniqueNameAllocator namer = UniqueNameAllocator.getInstance();\n-    \n+\n     while (true) {\n       Path newBulkDir = new Path(directory, Constants.BULK_PREFIX + namer.getNextName());\n       if (fs.exists(newBulkDir)) // sanity check\n         throw new IllegalStateException(\"Dir exist when it should not \" + newBulkDir);\n       if (fs.mkdirs(newBulkDir))\n         return newBulkDir;\n       log.warn(\"Failed to create \" + newBulkDir + \" for unknown reason\");\n-      \n+\n       UtilWaitThread.sleep(3000);\n     }\n   }\n@@ -216,20 +218,20 @@ private Path createNewBulkDir(VolumeManager fs, String tableId) throws IOExcepti\n   @SuppressWarnings(\"deprecation\")\n   private String prepareBulkImport(VolumeManager fs, String dir, String tableId) throws IOException {\n     Path bulkDir = createNewBulkDir(fs, tableId);\n-    \n+\n     MetadataTableUtil.addBulkLoadInProgressFlag(\"/\" + bulkDir.getParent().getName() + \"/\" + bulkDir.getName());\n-    \n+\n     Path dirPath = new Path(dir);\n     FileStatus[] mapFiles = fs.listStatus(dirPath);\n-    \n+\n     UniqueNameAllocator namer = UniqueNameAllocator.getInstance();\n-    \n+\n     for (FileStatus fileStatus : mapFiles) {\n       String sa[] = fileStatus.getPath().getName().split(\"\\\\.\");\n       String extension = \"\";\n       if (sa.length > 1) {\n         extension = sa[sa.length - 1];\n-        \n+\n         if (!FileOperations.getValidExtensions().contains(extension)) {\n           log.warn(fileStatus.getPath() + \" does not have a valid extension, ignoring\");\n           continue;\n@@ -238,13 +240,13 @@ private String prepareBulkImport(VolumeManager fs, String dir, String tableId) t\n         // assume it is a map file\n         extension = Constants.MAPFILE_EXTENSION;\n       }\n-      \n+\n       if (extension.equals(Constants.MAPFILE_EXTENSION)) {\n         if (!fileStatus.isDir()) {\n           log.warn(fileStatus.getPath() + \" is not a map file, ignoring\");\n           continue;\n         }\n-        \n+\n         if (fileStatus.getPath().getName().equals(\"_logs\")) {\n           log.info(fileStatus.getPath() + \" is probably a log directory from a map/reduce task, skipping\");\n           continue;\n@@ -260,7 +262,7 @@ private String prepareBulkImport(VolumeManager fs, String dir, String tableId) t\n           continue;\n         }\n       }\n-      \n+\n       String newName = \"I\" + namer.getNextName() + \".\" + extension;\n       Path newPath = new Path(bulkDir, newName);\n       try {\n@@ -272,7 +274,7 @@ private String prepareBulkImport(VolumeManager fs, String dir, String tableId) t\n     }\n     return bulkDir.toString();\n   }\n-  \n+\n   @Override\n   public void undo(long tid, Master environment) throws Exception {\n     // unreserve source/error directories\n@@ -283,23 +285,23 @@ public void undo(long tid, Master environment) throws Exception {\n }\n \n class CleanUpBulkImport extends MasterRepo {\n-  \n+\n   private static final long serialVersionUID = 1L;\n-  \n+\n   private static final Logger log = Logger.getLogger(CleanUpBulkImport.class);\n-  \n+\n   private String tableId;\n   private String source;\n   private String bulk;\n   private String error;\n-  \n+\n   public CleanUpBulkImport(String tableId, String source, String bulk, String error) {\n     this.tableId = tableId;\n     this.source = source;\n     this.bulk = bulk;\n     this.error = error;\n   }\n-  \n+\n   @Override\n   public Repo<Master> call(long tid, Master master) throws Exception {\n     log.debug(\"removing the bulk processing flag file in \" + bulk);\n@@ -320,21 +322,21 @@ public CleanUpBulkImport(String tableId, String source, String bulk, String erro\n }\n \n class CompleteBulkImport extends MasterRepo {\n-  \n+\n   private static final long serialVersionUID = 1L;\n-  \n+\n   private String tableId;\n   private String source;\n   private String bulk;\n   private String error;\n-  \n+\n   public CompleteBulkImport(String tableId, String source, String bulk, String error) {\n     this.tableId = tableId;\n     this.source = source;\n     this.bulk = bulk;\n     this.error = error;\n   }\n-  \n+\n   @Override\n   public Repo<Master> call(long tid, Master master) throws Exception {\n     ZooArbitrator.stop(Constants.BULK_ARBITRATOR_TYPE, tid);\n@@ -343,21 +345,21 @@ public CompleteBulkImport(String tableId, String source, String bulk, String err\n }\n \n class CopyFailed extends MasterRepo {\n-  \n+\n   private static final long serialVersionUID = 1L;\n-  \n+\n   private String tableId;\n   private String source;\n   private String bulk;\n   private String error;\n-  \n+\n   public CopyFailed(String tableId, String source, String bulk, String error) {\n     this.tableId = tableId;\n     this.source = source;\n     this.bulk = bulk;\n     this.error = error;\n   }\n-  \n+\n   @Override\n   public long isReady(long tid, Master master) throws Exception {\n     Set<TServerInstance> finished = new HashSet<TServerInstance>();\n@@ -375,19 +377,19 @@ public long isReady(long tid, Master master) throws Exception {\n       return 0;\n     return 500;\n   }\n-  \n+\n   @Override\n   public Repo<Master> call(long tid, Master master) throws Exception {\n     // This needs to execute after the arbiter is stopped\n-    \n+\n     VolumeManager fs = master.getFileSystem();\n-    \n+\n     if (!fs.exists(new Path(error, BulkImport.FAILURES_TXT)))\n       return new CleanUpBulkImport(tableId, source, bulk, error);\n-    \n+\n     HashMap<String,String> failures = new HashMap<String,String>();\n     HashMap<String,String> loadedFailures = new HashMap<String,String>();\n-    \n+\n     FSDataInputStream failFile = fs.open(new Path(error, BulkImport.FAILURES_TXT));\n     BufferedReader in = new BufferedReader(new InputStreamReader(failFile, Constants.UTF8));\n     try {\n@@ -400,18 +402,18 @@ public long isReady(long tid, Master master) throws Exception {\n     } finally {\n       failFile.close();\n     }\n-    \n+\n     /*\n      * I thought I could move files that have no file references in the table. However its possible a clone references a file. Therefore only move files that\n      * have no loaded markers.\n      */\n-    \n+\n     // determine which failed files were loaded\n     Connector conn = master.getConnector();\n     Scanner mscanner = new IsolatedScanner(conn.createScanner(MetadataTable.NAME, Authorizations.EMPTY));\n     mscanner.setRange(new KeyExtent(new Text(tableId), null, null).toMetadataRange());\n     mscanner.fetchColumnFamily(TabletsSection.BulkFileColumnFamily.NAME);\n-    \n+\n     for (Entry<Key,Value> entry : mscanner) {\n       if (Long.parseLong(entry.getValue().toString()) == tid) {\n         String loadedFile = entry.getKey().getColumnQualifier().toString();\n@@ -421,63 +423,63 @@ public long isReady(long tid, Master master) throws Exception {\n         }\n       }\n     }\n-    \n+\n     // move failed files that were not loaded\n     for (String failure : failures.values()) {\n       Path orig = new Path(failure);\n       Path dest = new Path(error, orig.getName());\n       fs.rename(orig, dest);\n       log.debug(\"tid \" + tid + \" renamed \" + orig + \" to \" + dest + \": import failed\");\n     }\n-    \n+\n     if (loadedFailures.size() > 0) {\n       DistributedWorkQueue bifCopyQueue = new DistributedWorkQueue(Constants.ZROOT + \"/\" + HdfsZooInstance.getInstance().getInstanceID()\n           + Constants.ZBULK_FAILED_COPYQ);\n-      \n+\n       HashSet<String> workIds = new HashSet<String>();\n-      \n+\n       for (String failure : loadedFailures.values()) {\n         Path orig = new Path(failure);\n         Path dest = new Path(error, orig.getName());\n-        \n+\n         if (fs.exists(dest))\n           continue;\n-        \n+\n         bifCopyQueue.addWork(orig.getName(), (failure + \",\" + dest).getBytes(Constants.UTF8));\n         workIds.add(orig.getName());\n         log.debug(\"tid \" + tid + \" added to copyq: \" + orig + \" to \" + dest + \": failed\");\n       }\n-      \n+\n       bifCopyQueue.waitUntilDone(workIds);\n     }\n-    \n+\n     fs.deleteRecursively(new Path(error, BulkImport.FAILURES_TXT));\n     return new CleanUpBulkImport(tableId, source, bulk, error);\n   }\n-  \n+\n }\n \n class LoadFiles extends MasterRepo {\n-  \n+\n   private static final long serialVersionUID = 1L;\n-  \n+\n   private static ExecutorService threadPool = null;\n   private static final Logger log = Logger.getLogger(BulkImport.class);\n-  \n+\n   private String tableId;\n   private String source;\n   private String bulk;\n   private String errorDir;\n   private boolean setTime;\n-  \n+\n   public LoadFiles(String tableId, String source, String bulk, String errorDir, boolean setTime) {\n     this.tableId = tableId;\n     this.source = source;\n     this.bulk = bulk;\n     this.errorDir = errorDir;\n     this.setTime = setTime;\n   }\n-  \n+\n   @Override\n   public long isReady(long tid, Master master) throws Exception {\n     if (master.onlineTabletServers().size() == 0)\n@@ -505,7 +507,7 @@ private static synchronized ExecutorService getThreadPool(Master master) {\n       files.add(entry);\n     }\n     log.debug(\"tid \" + tid + \" importing \" + files.size() + \" files\");\n-    \n+\n     Path writable = new Path(this.errorDir, \".iswritable\");\n     if (!fs.createNewFile(writable)) {\n       // Maybe this is a re-try... clear the flag and try again\n@@ -515,22 +517,22 @@ private static synchronized ExecutorService getThreadPool(Master master) {\n             \"Unable to write to \" + this.errorDir);\n     }\n     fs.delete(writable);\n-    \n+\n     final Set<String> filesToLoad = Collections.synchronizedSet(new HashSet<String>());\n     for (FileStatus f : files)\n       filesToLoad.add(f.getPath().toString());\n-    \n+\n     final int RETRIES = Math.max(1, conf.getCount(Property.MASTER_BULK_RETRIES));\n     for (int attempt = 0; attempt < RETRIES && filesToLoad.size() > 0; attempt++) {\n       List<Future<List<String>>> results = new ArrayList<Future<List<String>>>();\n-      \n+\n       if (master.onlineTabletServers().size() == 0)\n         log.warn(\"There are no tablet server to process bulk import, waiting (tid = \" + tid + \")\");\n-      \n+\n       while (master.onlineTabletServers().size() == 0) {\n         UtilWaitThread.sleep(500);\n       }\n-      \n+\n       // Use the threadpool to assign files one-at-a-time to the server\n       final List<String> loaded = Collections.synchronizedList(new ArrayList<String>());\n       for (final String file : filesToLoad) {\n@@ -575,7 +577,7 @@ private static synchronized ExecutorService getThreadPool(Master master) {\n         UtilWaitThread.sleep(100);\n       }\n     }\n-    \n+\n     FSDataOutputStream failFile = fs.create(new Path(errorDir, BulkImport.FAILURES_TXT), true);\n     BufferedWriter out = new BufferedWriter(new OutputStreamWriter(failFile, Constants.UTF8));\n     try {\n@@ -586,11 +588,11 @@ private static synchronized ExecutorService getThreadPool(Master master) {\n     } finally {\n       out.close();\n     }\n-    \n+\n     // return the next step, which will perform cleanup\n     return new CompleteBulkImport(tableId, source, bulk, errorDir);\n   }\n-  \n+\n   static String sampleList(Collection<?> potentiallyLongList, int max) {\n     StringBuffer result = new StringBuffer();\n     result.append(\"[\");\n@@ -610,5 +612,5 @@ static String sampleList(Collection<?> potentiallyLongList, int max) {\n     result.append(\"]\");\n     return result.toString();\n   }\n-  \n+\n }",
                "additions": 76,
                "raw_url": "https://github.com/apache/accumulo/raw/079ef51c7c254f1f7bd7bd4d83ea405ae635b433/server/master/src/main/java/org/apache/accumulo/master/tableOps/BulkImport.java",
                "status": "modified",
                "changes": 150,
                "deletions": 74,
                "sha": "e42fee65249d2838026f24ee29db6b4173a69586",
                "blob_url": "https://github.com/apache/accumulo/blob/079ef51c7c254f1f7bd7bd4d83ea405ae635b433/server/master/src/main/java/org/apache/accumulo/master/tableOps/BulkImport.java",
                "filename": "server/master/src/main/java/org/apache/accumulo/master/tableOps/BulkImport.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/tableOps/BulkImport.java?ref=079ef51c7c254f1f7bd7bd4d83ea405ae635b433"
            }
        ],
        "bug_id": "accumulo_17",
        "parent": "https://github.com/apache/accumulo/commit/5c409b0ad7e83410c393f8bb63576e03678e23cb",
        "message": "ACCUMULO-2854 Added additional check to prevent NPE\n\nSigned-off-by: Bill Havanki <bhavanki@cloudera.com>",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/88b6202f5b78838e7e6187010792261eff9c4bc8",
        "file": [
            {
                "patch": "@@ -227,16 +227,16 @@ protected void confirmDeletesFromReplication(Iterator<Entry<String,Status>> repl\n         pendingReplication.next();\n       } else if (comparison > 1) {\n         candidates.next();\n-      }\n-\n-      // We want to advance both \n-      candidates.next();\n-      pendingReplication.next();\n-\n-      // We cannot delete a file if it is still needed for replication\n-      if (!StatusUtil.isSafeForRemoval(pendingReplica.getValue())) {\n-        // If it must be replicated, we must remove it from the candidate set to prevent deletion\n-        candidates.remove();\n+      } else {\n+        // We want to advance both, and try to delete the candidate if we can\n+        candidates.next();\n+        pendingReplication.next();\n+  \n+        // We cannot delete a file if it is still needed for replication\n+        if (!StatusUtil.isSafeForRemoval(pendingReplica.getValue())) {\n+          // If it must be replicated, we must remove it from the candidate set to prevent deletion\n+          candidates.remove();\n+        }\n       }\n     }\n   }",
                "additions": 10,
                "raw_url": "https://github.com/apache/accumulo/raw/88b6202f5b78838e7e6187010792261eff9c4bc8/server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java",
                "status": "modified",
                "changes": 20,
                "deletions": 10,
                "sha": "93622649942d399cda085661305bd8a05f9c399e",
                "blob_url": "https://github.com/apache/accumulo/blob/88b6202f5b78838e7e6187010792261eff9c4bc8/server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java",
                "filename": "server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/gc/src/main/java/org/apache/accumulo/gc/GarbageCollectionAlgorithm.java?ref=88b6202f5b78838e7e6187010792261eff9c4bc8"
            },
            {
                "patch": "@@ -366,4 +366,9 @@ public void removeReplicationEntries() throws Exception {\n \n     Assert.assertEquals(0, nameToFileMap.size());\n   }\n+\n+  @Test\n+  public void confirmDeletesFromReplication() throws Exception {\n+    Assert.fail(\"Implement me, dummy\");\n+  }\n }",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/88b6202f5b78838e7e6187010792261eff9c4bc8/server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogsTest.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "b20c2ca7366d59c4314ac6d78416232ecafb0865",
                "blob_url": "https://github.com/apache/accumulo/blob/88b6202f5b78838e7e6187010792261eff9c4bc8/server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogsTest.java",
                "filename": "server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogsTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/gc/src/test/java/org/apache/accumulo/gc/GarbageCollectWriteAheadLogsTest.java?ref=88b6202f5b78838e7e6187010792261eff9c4bc8"
            }
        ],
        "bug_id": "accumulo_18",
        "parent": "https://github.com/apache/accumulo/commit/22917ba4eac60567f499d469eedc5c30217450e2",
        "message": "ACCUMULO-2575 Incorrect conditionals caused an NPE.\n\nStubbed a test for implementing later",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/b33fbf40d0b16ee59d74341e568fab4abb160017",
        "file": [
            {
                "patch": "@@ -68,7 +68,7 @@ else if (dice == 2)\n       if (cause != null && cause instanceof ThriftTableOperationException) {\n         ThriftTableOperationException toe = (ThriftTableOperationException)cause.getCause();\n         if (toe.type == TableOperationExceptionType.NAMESPACE_NOTFOUND) {\n-          log.debug(\"Unable to change user permissions: \" + toe.getCause());\n+          log.debug(\"Unable to change user permissions: \" + toe);\n           return;\n         }\n       }",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/b33fbf40d0b16ee59d74341e568fab4abb160017/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/ChangePermissions.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "73e617595ad2d7afd0f050eb8ddc896b6066d0fe",
                "blob_url": "https://github.com/apache/accumulo/blob/b33fbf40d0b16ee59d74341e568fab4abb160017/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/ChangePermissions.java",
                "filename": "test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/ChangePermissions.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/randomwalk/concurrent/ChangePermissions.java?ref=b33fbf40d0b16ee59d74341e568fab4abb160017"
            }
        ],
        "bug_id": "accumulo_19",
        "parent": "https://github.com/apache/accumulo/commit/1d411fbe829cd72d9c7d888c829c545e4da8cb16",
        "message": "ACCUMULO-2312 NPE fixed by 2316/2318, but making message better",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/80f8afb10c69bf337ffd7301e21f729c1c0ce27b",
        "file": [
            {
                "patch": "@@ -78,6 +78,10 @@ static TabletTime getInstance(String metadataValue) {\n   }\n   \n   public static String maxMetadataTime(String mv1, String mv2) {\n+    if (mv1 == null && mv2 == null) {\n+      return null;\n+    }\n+    \n     if (mv1 == null) {\n       checkType(mv2);\n       return mv2;",
                "additions": 4,
                "raw_url": "https://github.com/apache/accumulo/raw/80f8afb10c69bf337ffd7301e21f729c1c0ce27b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "3ba84459f4307a74790f48d84549a55811121080",
                "blob_url": "https://github.com/apache/accumulo/blob/80f8afb10c69bf337ffd7301e21f729c1c0ce27b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java",
                "filename": "src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java?ref=80f8afb10c69bf337ffd7301e21f729c1c0ce27b"
            }
        ],
        "bug_id": "accumulo_20",
        "parent": "https://github.com/apache/accumulo/commit/92c41719bfc9cce7cb222c96155650bc11a288fb",
        "message": "ACCUMULO-2523 TabletTime.maxMetadataTime NPE if both arguments are null\n\nSigned-off-by: Bill Havanki <bhavanki@cloudera.com>",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/9cd8962772c983fb6df579e2728107fc9f1d256a",
        "file": [
            {
                "patch": "@@ -67,17 +67,16 @@ public void restoreConf() {\n \n   @Test\n   public void test() throws Exception {\n-    runTest(getConnector(), getUniqueNames(1)[0], this.getClass().getName(), testName.getMethodName());\n+    runTest(getConnector(), getCluster().getFileSystem(), getCluster().getTemporaryPath(), getAdminPrincipal(), getUniqueNames(1)[0], this.getClass().getName(),\n+        testName.getMethodName());\n   }\n \n-  static void runTest(Connector c, String tableName, String filePrefix, String dirSuffix) throws AccumuloException, AccumuloSecurityException,\n-      TableExistsException, IOException, TableNotFoundException, MutationsRejectedException {\n+  static void runTest(Connector c, FileSystem fs, Path basePath, String principal, String tableName, String filePrefix, String dirSuffix)\n+      throws AccumuloException, AccumuloSecurityException, TableExistsException, IOException, TableNotFoundException, MutationsRejectedException {\n     c.tableOperations().create(tableName);\n-    FileSystem fs = cluster.getFileSystem();\n     CachedConfiguration.setInstance(fs.getConf());\n \n-    Path tempPath = cluster.getTemporaryPath();\n-    Path base = new Path(tempPath, \"testBulkFail_\" + dirSuffix);\n+    Path base = new Path(basePath, \"testBulkFail_\" + dirSuffix);\n     fs.delete(base, true);\n     fs.mkdirs(base);\n     Path bulkFailures = new Path(base, \"failures\");\n@@ -109,7 +108,7 @@ static void runTest(Connector c, String tableName, String filePrefix, String dir\n     VerifyIngest.Opts vopts = new VerifyIngest.Opts();\n     vopts.setTableName(tableName);\n     vopts.random = 56;\n-    vopts.setPrincipal(getAdminPrincipal());\n+    vopts.setPrincipal(principal);\n     for (int i = 0; i < COUNT; i++) {\n       vopts.startRow = i * N;\n       vopts.rows = N;",
                "additions": 6,
                "raw_url": "https://github.com/apache/accumulo/raw/9cd8962772c983fb6df579e2728107fc9f1d256a/test/src/test/java/org/apache/accumulo/test/functional/BulkIT.java",
                "status": "modified",
                "changes": 13,
                "deletions": 7,
                "sha": "a2a05f6475c555d815c4136c149c14dadf2f0dfa",
                "blob_url": "https://github.com/apache/accumulo/blob/9cd8962772c983fb6df579e2728107fc9f1d256a/test/src/test/java/org/apache/accumulo/test/functional/BulkIT.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/functional/BulkIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/functional/BulkIT.java?ref=9cd8962772c983fb6df579e2728107fc9f1d256a"
            },
            {
                "patch": "@@ -20,6 +20,8 @@\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.minicluster.impl.MiniAccumuloConfigImpl;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.junit.Test;\n \n /**\n@@ -58,7 +60,8 @@ public void adminStop() throws Exception {\n \n   @Test\n   public void bulk() throws Exception {\n-    BulkIT.runTest(getConnector(), getUniqueNames(1)[0], this.getClass().getName(), testName.getMethodName());\n+    BulkIT.runTest(getConnector(), FileSystem.getLocal(new Configuration(false)), new Path(getCluster().getConfig().getDir().getAbsolutePath(), \"tmp\"), \"root\",\n+        getUniqueNames(1)[0], this.getClass().getName(), testName.getMethodName());\n   }\n \n   @Test",
                "additions": 4,
                "raw_url": "https://github.com/apache/accumulo/raw/9cd8962772c983fb6df579e2728107fc9f1d256a/test/src/test/java/org/apache/accumulo/test/functional/SslIT.java",
                "status": "modified",
                "changes": 5,
                "deletions": 1,
                "sha": "daa6bb3ead80428cebe936597f48b2735c35e9e9",
                "blob_url": "https://github.com/apache/accumulo/blob/9cd8962772c983fb6df579e2728107fc9f1d256a/test/src/test/java/org/apache/accumulo/test/functional/SslIT.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/functional/SslIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/functional/SslIT.java?ref=9cd8962772c983fb6df579e2728107fc9f1d256a"
            }
        ],
        "bug_id": "accumulo_21",
        "parent": "https://github.com/apache/accumulo/commit/c939dac61a48ff857565687c6da611558353d884",
        "message": "ACCUMULO-3599 Work around Ssl*IT still only using MAC\n\nThese tests were throwing an NPE. Use the getCluster()\nmethod to get an extra initialization check.",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/c5aac49ed299b7c6eee534333cf080b638bcdecd",
        "file": [
            {
                "patch": "@@ -148,12 +148,7 @@ public boolean process(TProtocol in, TProtocol out) throws TException {\n         metrics.add(ThriftMetrics.idle, (now - idleStart));\n       }\n       try {\n-        try {\n-          return other.process(in, out);\n-        } catch (NullPointerException ex) {\n-          // THRIFT-1447 - remove with thrift 0.9\n-          return true;\n-        }\n+        return other.process(in, out);\n       } finally {\n         if (metrics.isEnabled()) {\n           idleStart = System.currentTimeMillis();",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/c5aac49ed299b7c6eee534333cf080b638bcdecd/server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java",
                "status": "modified",
                "changes": 7,
                "deletions": 6,
                "sha": "f185661dcfd8a70e69616ec228dbe65a544c3908",
                "blob_url": "https://github.com/apache/accumulo/blob/c5aac49ed299b7c6eee534333cf080b638bcdecd/server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java?ref=c5aac49ed299b7c6eee534333cf080b638bcdecd"
            }
        ],
        "bug_id": "accumulo_22",
        "parent": "https://github.com/apache/accumulo/commit/53ec68999a416d96a0bd3c8d92a6311c8b69987b",
        "message": "ACCUMULO-2410 TServerUtils no longer needs to catch NPE as a thrift bug workaround\n\nSigned-off-by: John Vines <vines@apache.org>",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/1d608a81f488c2bf371fc81f83e0022bd2943a36",
        "file": [
            {
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.accumulo.core.conf.DefaultConfiguration;\n import org.apache.accumulo.core.conf.SiteConfiguration;\n import org.apache.accumulo.core.data.KeyExtent;\n+import org.apache.accumulo.server.client.HdfsZooInstance;\n \n public class ServerConfiguration {\n   ",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "86532745d69302069d67f08cc4fbc0d8f9edcaa6",
                "blob_url": "https://github.com/apache/accumulo/blob/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java?ref=1d608a81f488c2bf371fc81f83e0022bd2943a36"
            },
            {
                "patch": "@@ -37,37 +37,46 @@\n \n public class TableConfiguration extends AccumuloConfiguration {\n   private static final Logger log = Logger.getLogger(TableConfiguration.class);\n-  \n+\n   // Need volatile keyword to ensure double-checked locking works as intended\n   private static volatile ZooCache tablePropCache = null;\n+  private static final Object initLock = new Object();\n+\n   private final String instanceId;\n+  private final Instance instance;\n   private final AccumuloConfiguration parent;\n-  \n+\n   private String table = null;\n   private Set<ConfigurationObserver> observers;\n-  \n+\n   public TableConfiguration(String instanceId, String table, AccumuloConfiguration parent) {\n+    this(instanceId, HdfsZooInstance.getInstance(), table, parent);\n+  }\n+\n+  public TableConfiguration(String instanceId, Instance instance, String table, AccumuloConfiguration parent) {\n     this.instanceId = instanceId;\n+    this.instance = instance;\n     this.table = table;\n     this.parent = parent;\n-    \n+\n     this.observers = Collections.synchronizedSet(new HashSet<ConfigurationObserver>());\n   }\n-  \n-  /**\n-   * @deprecated not for client use\n-   */\n-  @Deprecated\n-  private static ZooCache getTablePropCache() {\n-    Instance inst = HdfsZooInstance.getInstance();\n-    if (tablePropCache == null)\n-      synchronized (TableConfiguration.class) {\n-        if (tablePropCache == null)\n-          tablePropCache = new ZooCache(inst.getZooKeepers(), inst.getZooKeepersSessionTimeOut(), new TableConfWatcher(inst));\n+\n+  private void initializeZooCache() {\n+    synchronized (initLock) {\n+      if (null == tablePropCache) {\n+        tablePropCache = new ZooCache(instance.getZooKeepers(), instance.getZooKeepersSessionTimeOut(), new TableConfWatcher(instance));\n       }\n+    }\n+  }\n+\n+  private ZooCache getTablePropCache() {\n+    if (null == tablePropCache) {\n+      initializeZooCache();\n+    }\n     return tablePropCache;\n   }\n-  \n+\n   public void addObserver(ConfigurationObserver co) {\n     if (table == null) {\n       String err = \"Attempt to add observer for non-table configuration\";\n@@ -77,7 +86,7 @@ public void addObserver(ConfigurationObserver co) {\n     iterator();\n     observers.add(co);\n   }\n-  \n+\n   public void removeObserver(ConfigurationObserver configObserver) {\n     if (table == null) {\n       String err = \"Attempt to remove observer for non-table configuration\";\n@@ -86,53 +95,54 @@ public void removeObserver(ConfigurationObserver configObserver) {\n     }\n     observers.remove(configObserver);\n   }\n-  \n+\n   public void expireAllObservers() {\n     Collection<ConfigurationObserver> copy = Collections.unmodifiableCollection(observers);\n     for (ConfigurationObserver co : copy)\n       co.sessionExpired();\n   }\n-  \n+\n   public void propertyChanged(String key) {\n     Collection<ConfigurationObserver> copy = Collections.unmodifiableCollection(observers);\n     for (ConfigurationObserver co : copy)\n       co.propertyChanged(key);\n   }\n-  \n+\n   public void propertiesChanged(String key) {\n     Collection<ConfigurationObserver> copy = Collections.unmodifiableCollection(observers);\n     for (ConfigurationObserver co : copy)\n       co.propertiesChanged();\n   }\n-  \n+\n   public String get(Property property) {\n     String key = property.getKey();\n     String value = get(key);\n-    \n+\n     if (value == null || !property.getType().isValidFormat(value)) {\n       if (value != null)\n         log.error(\"Using default value for \" + key + \" due to improperly formatted \" + property.getType() + \": \" + value);\n       value = parent.get(property);\n     }\n     return value;\n   }\n-  \n+\n   private String get(String key) {\n     String zPath = ZooUtil.getRoot(instanceId) + Constants.ZTABLES + \"/\" + table + Constants.ZTABLE_CONF + \"/\" + key;\n+\n     byte[] v = getTablePropCache().get(zPath);\n     String value = null;\n     if (v != null)\n       value = new String(v, Constants.UTF8);\n     return value;\n   }\n-  \n+\n   @Override\n   public Iterator<Entry<String,String>> iterator() {\n     TreeMap<String,String> entries = new TreeMap<String,String>();\n-    \n+\n     for (Entry<String,String> parentEntry : parent)\n       entries.put(parentEntry.getKey(), parentEntry.getValue());\n-    \n+\n     List<String> children = getTablePropCache().getChildren(ZooUtil.getRoot(instanceId) + Constants.ZTABLES + \"/\" + table + Constants.ZTABLE_CONF);\n     if (children != null) {\n       for (String child : children) {\n@@ -141,22 +151,26 @@ private String get(String key) {\n           entries.put(child, value);\n       }\n     }\n-    \n+\n     return entries.entrySet().iterator();\n   }\n-  \n+\n   public String getTableId() {\n     return table;\n   }\n \n   @Override\n   public void invalidateCache() {\n     if (null != tablePropCache) {\n-      synchronized (TableConfiguration.class) {\n-        if (null != tablePropCache) {\n-          tablePropCache = null;\n-        }\n-      }\n+      tablePropCache.clear();\n     }\n+    // Else, if the cache is null, we could lock and double-check\n+    // to see if it happened to be created so we could invalidate it\n+    // but I don't see much benefit coming from that extra check.\n+  }\n+  \n+  @Override\n+  public String toString() {\n+    return this.getClass().getSimpleName();\n   }\n }",
                "additions": 47,
                "raw_url": "https://github.com/apache/accumulo/raw/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java",
                "status": "modified",
                "changes": 80,
                "deletions": 33,
                "sha": "7a3d6e44c493b950720b482aa9a6e33bd335e65c",
                "blob_url": "https://github.com/apache/accumulo/blob/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java?ref=1d608a81f488c2bf371fc81f83e0022bd2943a36"
            },
            {
                "patch": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.test;\n+\n+import java.util.ArrayList;\n+import java.util.Random;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.accumulo.core.client.Connector;\n+import org.apache.accumulo.core.client.ZooKeeperInstance;\n+import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n+import org.apache.accumulo.core.conf.AccumuloConfiguration;\n+import org.apache.accumulo.core.conf.DefaultConfiguration;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.minicluster.MiniAccumuloCluster;\n+import org.apache.accumulo.server.conf.TableConfiguration;\n+import org.apache.log4j.Logger;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TableConfigurationUpdateTest {\n+  private static final Logger log = Logger.getLogger(TableConfigurationUpdateTest.class);\n+\n+  public static TemporaryFolder folder = new TemporaryFolder();\n+  private MiniAccumuloCluster accumulo;\n+  private String secret = \"secret\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    folder.create();\n+    accumulo = new MiniAccumuloCluster(folder.getRoot(), secret);\n+    accumulo.start();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    accumulo.stop();\n+    folder.delete();\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    ZooKeeperInstance inst = new ZooKeeperInstance(accumulo.getInstanceName(), accumulo.getZooKeepers(), 60 * 1000);\n+    Connector conn = inst.getConnector(\"root\", new PasswordToken(secret));\n+\n+    String table = \"foo\";\n+    conn.tableOperations().create(table);\n+\n+    final DefaultConfiguration defaultConf = AccumuloConfiguration.getDefaultConfiguration();\n+\n+    // Cache invalidates 25% of the time\n+    int randomMax = 4;\n+    // Number of threads\n+    int numThreads = 2;\n+    // Number of iterations per thread\n+    int iterations = 100000;\n+    AccumuloConfiguration tableConf = new TableConfiguration(inst.getInstanceID(), inst, table, defaultConf);\n+    \n+    long start = System.currentTimeMillis();\n+    ExecutorService svc = Executors.newFixedThreadPool(numThreads);\n+    CountDownLatch countDown = new CountDownLatch(numThreads);\n+    ArrayList<Future<Exception>> futures = new ArrayList<Future<Exception>>(numThreads);\n+\n+    for (int i = 0; i < numThreads; i++) {\n+      futures.add(svc.submit(new TableConfRunner(randomMax, iterations, tableConf, countDown)));\n+    }\n+\n+    svc.shutdown();\n+    Assert.assertTrue(svc.awaitTermination(60, TimeUnit.MINUTES));\n+\n+    for (Future<Exception> fut : futures) {\n+      Exception e = fut.get();\n+      if (null != e) {\n+        Assert.fail(\"Thread failed with exception \" + e);\n+      }\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    log.debug(tableConf + \" with \" + iterations + \" iterations and \" + numThreads + \" threads and cache invalidates \"\n+        + ((1. / randomMax) * 100.) + \"% took \" + (end - start) / 1000 + \" second(s)\");\n+  }\n+\n+  public static class TableConfRunner implements Callable<Exception> {\n+    private static final Property prop = Property.TABLE_SPLIT_THRESHOLD;\n+    private AccumuloConfiguration tableConf;\n+    private CountDownLatch countDown;\n+    private int iterations, randMax;\n+\n+    public TableConfRunner(int randMax, int iterations, AccumuloConfiguration tableConf, CountDownLatch countDown) {\n+      this.randMax = randMax;\n+      this.iterations = iterations;\n+      this.tableConf = tableConf;\n+      this.countDown = countDown;\n+    }\n+\n+    @Override\n+    public Exception call() {\n+      Random r = new Random();\n+      countDown.countDown();\n+      try {\n+        countDown.await();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        return e;\n+      }\n+\n+      String t = Thread.currentThread().getName() + \" \";\n+      try {\n+        for (int i = 0; i < iterations; i++) {\n+          // if (i % 10000 == 0) {\n+          // log.info(t + \" \" + i);\n+          // }\n+          int choice = r.nextInt(randMax);\n+          if (choice < 1) {\n+            tableConf.invalidateCache();\n+          } else {\n+            tableConf.get(prop);\n+          }\n+        }\n+      } catch (Exception e) {\n+        log.error(t, e);\n+        return e;\n+      }\n+\n+      return null;\n+    }\n+\n+  }\n+\n+}",
                "additions": 152,
                "raw_url": "https://github.com/apache/accumulo/raw/1d608a81f488c2bf371fc81f83e0022bd2943a36/test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java",
                "status": "added",
                "changes": 152,
                "deletions": 0,
                "sha": "30da268f747618c5fd76f563bc30230d58f41f3d",
                "blob_url": "https://github.com/apache/accumulo/blob/1d608a81f488c2bf371fc81f83e0022bd2943a36/test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java?ref=1d608a81f488c2bf371fc81f83e0022bd2943a36"
            }
        ],
        "bug_id": "accumulo_23",
        "parent": "https://github.com/apache/accumulo/commit/63d5e55a0b03910246b9b21efecfde5ac5e709f0",
        "message": "ACCUMULO-2489 Fixes race condition in TableConfiguration where NPE may occur.\n\nMake invalidateCache much more efficient by calling clear on ZooCache\ninstead of creating a new one.",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/e3a743cb445723a3d5664a4bf1ebf37833152aae",
        "file": [
            {
                "patch": "@@ -60,16 +60,15 @@ public static Volume getDefaultVolume(Configuration conf, AccumuloConfiguration\n   }\n \n   /**\n-   * @see org.apache.accumulo.core.volume.VolumeConfiguration#getVolumeUris(AccumuloConfiguration)\n+   * @see org.apache.accumulo.core.volume.VolumeConfiguration#getVolumeUris(AccumuloConfiguration,Configuration)\n    */\n   @Deprecated\n-  public static String getConfiguredBaseDir(AccumuloConfiguration conf) {\n+  public static String getConfiguredBaseDir(AccumuloConfiguration conf, Configuration hadoopConfig) {\n     String singleNamespace = conf.get(Property.INSTANCE_DFS_DIR);\n     String dfsUri = conf.get(Property.INSTANCE_DFS_URI);\n     String baseDir;\n \n     if (dfsUri == null || dfsUri.isEmpty()) {\n-      Configuration hadoopConfig = CachedConfiguration.getInstance();\n       try {\n         baseDir = FileSystem.get(hadoopConfig).getUri().toString() + singleNamespace;\n       } catch (IOException e) {\n@@ -85,15 +84,20 @@ public static String getConfiguredBaseDir(AccumuloConfiguration conf) {\n \n   /**\n    * Compute the URIs to be used by Accumulo\n+   * \n    */\n   public static String[] getVolumeUris(AccumuloConfiguration conf) {\n+    return getVolumeUris(conf, CachedConfiguration.getInstance());\n+  }\n+  \n+  public static String[] getVolumeUris(AccumuloConfiguration conf, Configuration hadoopConfig) {\n     String ns = conf.get(Property.INSTANCE_VOLUMES);\n \n     String configuredBaseDirs[];\n \n     if (ns == null || ns.isEmpty()) {\n       // Fall back to using the old config values\n-      configuredBaseDirs = new String[] {getConfiguredBaseDir(conf)};\n+      configuredBaseDirs = new String[] {getConfiguredBaseDir(conf, hadoopConfig)};\n     } else {\n       String namespaces[] = ns.split(\",\");\n       configuredBaseDirs = new String[namespaces.length];",
                "additions": 8,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java",
                "status": "modified",
                "changes": 12,
                "deletions": 4,
                "sha": "8d56d9e991142ef85cd1ce8e61faafbe53fe24f0",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -25,31 +25,34 @@\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.util.CachedConfiguration;\n import org.apache.accumulo.core.volume.VolumeConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.log4j.Logger;\n \n public class ZooUtil extends org.apache.accumulo.fate.zookeeper.ZooUtil {\n-  \n+\n   private static final Logger log = Logger.getLogger(ZooUtil.class);\n-  \n+\n   public static String getRoot(final Instance instance) {\n     return getRoot(instance.getInstanceID());\n   }\n-  \n+\n   public static String getRoot(final String instanceId) {\n     return Constants.ZROOT + \"/\" + instanceId;\n   }\n-  \n+\n   /**\n    * Utility to support certain client side utilities to minimize command-line options.\n    */\n-\n   public static String getInstanceIDFromHdfs(Path instanceDirectory, AccumuloConfiguration conf) {\n-    try {\n+    return getInstanceIDFromHdfs(instanceDirectory, conf, CachedConfiguration.getInstance());\n+  }\n \n-      FileSystem fs = VolumeConfiguration.getVolume(instanceDirectory.toString(), CachedConfiguration.getInstance(), conf).getFileSystem();\n+  public static String getInstanceIDFromHdfs(Path instanceDirectory, AccumuloConfiguration conf, Configuration hadoopConf) {\n+    try {\n+      FileSystem fs = VolumeConfiguration.getVolume(instanceDirectory.toString(), hadoopConf, conf).getFileSystem();\n       FileStatus[] files = null;\n       try {\n         files = fs.listStatus(instanceDirectory);",
                "additions": 10,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java",
                "status": "modified",
                "changes": 17,
                "deletions": 7,
                "sha": "9ee7c992aa3b1273f57141d326d05e51ba1c9c5b",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.accumulo.minicluster;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.Map;\n \n import org.apache.accumulo.minicluster.impl.MiniAccumuloConfigImpl;\n@@ -95,8 +96,7 @@ public MiniAccumuloConfig setZooKeeperPort(int zooKeeperPort) {\n   }\n \n   /**\n-   * Configure the time to wait for ZooKeeper to startup.\n-   * Calling this method is optional. The default is 20000 milliseconds\n+   * Configure the time to wait for ZooKeeper to startup. Calling this method is optional. The default is 20000 milliseconds\n    * \n    * @param zooKeeperStartupTime\n    *          Time to wait for ZooKeeper to startup, in milliseconds\n@@ -252,4 +252,19 @@ public MiniAccumuloConfig setNativeLibPaths(String... nativePathItems) {\n     impl.setNativeLibPaths(nativePathItems);\n     return this;\n   }\n+\n+  /**\n+   * Informs MAC that it's running against an existing accumulo instance. It is assumed that it's already initialized and hdfs/zookeeper are already running.\n+   *\n+   * @param accumuloSite\n+   *          a File representation of the accumulo-site.xml file for the instance being run\n+   * @param hadoopConfDir\n+   *          a File representation of the hadoop configuration directory containing core-site.xml and hdfs-site.xml\n+   *\n+   * @since 1.6.2\n+   */\n+  public MiniAccumuloConfig useExistingInstance(File accumuloSite, File hadoopConfDir) throws IOException {\n+    impl.useExistingInstance(accumuloSite, hadoopConfDir);\n+    return this;\n+  }\n }",
                "additions": 17,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java",
                "status": "modified",
                "changes": 19,
                "deletions": 2,
                "sha": "68e30fa4eb8dfd0add7dd74a47927162cdbe4325",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java",
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -21,7 +21,6 @@\n import java.io.IOException;\n import java.io.InputStream;\n import java.net.ServerSocket;\n-import java.net.Socket;\n import java.util.Date;\n import java.util.HashMap;\n import java.util.Map;\n@@ -31,6 +30,7 @@\n import org.apache.accumulo.core.cli.Help;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.commons.io.FileUtils;\n+\n import com.beust.jcommander.IStringConverter;\n import com.beust.jcommander.Parameter;\n import com.google.common.io.Files;",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "22eca84ef0b91e28e09278c54bc7e82e699f36e8",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java",
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -50,6 +50,7 @@\n import java.util.concurrent.TimeoutException;\n \n import org.apache.accumulo.cluster.AccumuloCluster;\n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.ClientConfiguration;\n@@ -59,22 +60,31 @@\n import org.apache.accumulo.core.client.impl.MasterClient;\n import org.apache.accumulo.core.client.impl.thrift.ThriftSecurityException;\n import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n+import org.apache.accumulo.core.conf.ConfigurationCopy;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.master.thrift.MasterClientService;\n import org.apache.accumulo.core.master.thrift.MasterGoalState;\n import org.apache.accumulo.core.master.thrift.MasterMonitorInfo;\n+import org.apache.accumulo.core.util.CachedConfiguration;\n import org.apache.accumulo.core.util.Daemon;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.core.util.StringUtil;\n import org.apache.accumulo.core.util.UtilWaitThread;\n+import org.apache.accumulo.core.zookeeper.ZooUtil;\n+import org.apache.accumulo.fate.zookeeper.IZooReaderWriter;\n import org.apache.accumulo.gc.SimpleGarbageCollector;\n import org.apache.accumulo.master.Master;\n import org.apache.accumulo.master.state.SetGoalState;\n import org.apache.accumulo.minicluster.ServerType;\n+import org.apache.accumulo.server.Accumulo;\n+import org.apache.accumulo.server.fs.VolumeManager;\n+import org.apache.accumulo.server.fs.VolumeManagerImpl;\n import org.apache.accumulo.server.init.Initialize;\n import org.apache.accumulo.server.security.SystemCredentials;\n+import org.apache.accumulo.server.util.AccumuloStatus;\n import org.apache.accumulo.server.util.PortUtils;\n import org.apache.accumulo.server.util.time.SimpleTimer;\n+import org.apache.accumulo.server.zookeeper.ZooReaderWriterFactory;\n import org.apache.accumulo.start.Main;\n import org.apache.accumulo.start.classloader.vfs.MiniDFSUtil;\n import org.apache.accumulo.trace.instrument.Tracer;\n@@ -84,12 +94,17 @@\n import org.apache.commons.vfs2.FileObject;\n import org.apache.commons.vfs2.impl.VFSClassLoader;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n-import org.apache.log4j.Logger;\n import org.apache.thrift.TException;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.data.Stat;\n import org.apache.zookeeper.server.ZooKeeperServerMain;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import com.google.common.base.Predicate;\n import com.google.common.collect.Maps;\n@@ -101,7 +116,7 @@\n  * @since 1.6.0\n  */\n public class MiniAccumuloClusterImpl implements AccumuloCluster {\n-  private static final Logger log = Logger.getLogger(MiniAccumuloClusterImpl.class);\n+  private static final Logger log = LoggerFactory.getLogger(MiniAccumuloClusterImpl.class);\n \n   public static class LogWriter extends Daemon {\n     private BufferedReader in;\n@@ -219,6 +234,9 @@ private String getClasspath() throws IOException {\n       StringBuilder classpathBuilder = new StringBuilder();\n       classpathBuilder.append(config.getConfDir().getAbsolutePath());\n \n+      if (config.getHadoopConfDir() != null)\n+        classpathBuilder.append(File.pathSeparator).append(config.getHadoopConfDir().getAbsolutePath());\n+\n       if (config.getClasspathItems() == null) {\n \n         // assume 0 is the system classloader and skip it\n@@ -268,7 +286,8 @@ private Process _exec(Class<?> clazz, List<String> extraJvmOpts, String... args)\n     for (Entry<String,String> sysProp : config.getSystemProperties().entrySet()) {\n       argList.add(String.format(\"-D%s=%s\", sysProp.getKey(), sysProp.getValue()));\n     }\n-    argList.addAll(Arrays.asList(\"-XX:+UseConcMarkSweepGC\", \"-XX:CMSInitiatingOccupancyFraction=75\", \"-Dapple.awt.UIElement=true\", Main.class.getName(), className));\n+    argList.addAll(Arrays.asList(\"-XX:+UseConcMarkSweepGC\", \"-XX:CMSInitiatingOccupancyFraction=75\", \"-Dapple.awt.UIElement=true\", Main.class.getName(),\n+        className));\n     argList.addAll(Arrays.asList(args));\n \n     ProcessBuilder builder = new ProcessBuilder(argList);\n@@ -290,6 +309,8 @@ private Process _exec(Class<?> clazz, List<String> extraJvmOpts, String... args)\n     builder.environment().put(\"ACCUMULO_CONF_DIR\", config.getConfDir().getAbsolutePath());\n     // hadoop-2.2 puts error messages in the logs if this is not set\n     builder.environment().put(\"HADOOP_HOME\", config.getDir().getAbsolutePath());\n+    if (config.getHadoopConfDir() != null)\n+      builder.environment().put(\"HADOOP_CONF_DIR\", config.getHadoopConfDir().getAbsolutePath());\n \n     Process process = builder.start();\n \n@@ -339,13 +360,16 @@ public MiniAccumuloClusterImpl(MiniAccumuloConfigImpl config) throws IOException\n     this.config = config.initialize();\n \n     config.getConfDir().mkdirs();\n-    config.getAccumuloDir().mkdirs();\n-    config.getZooKeeperDir().mkdirs();\n     config.getLogDir().mkdirs();\n-    config.getWalogDir().mkdirs();\n     config.getLibDir().mkdirs();\n     config.getLibExtDir().mkdirs();\n \n+    if (!config.useExistingInstance()) {\n+      config.getZooKeeperDir().mkdirs();\n+      config.getWalogDir().mkdirs();\n+      config.getAccumuloDir().mkdirs();\n+    }\n+\n     if (config.useMiniDFS()) {\n       File nn = new File(config.getAccumuloDir(), \"nn\");\n       nn.mkdirs();\n@@ -378,6 +402,8 @@ public MiniAccumuloClusterImpl(MiniAccumuloConfigImpl config) throws IOException\n       siteConfig.put(Property.INSTANCE_DFS_URI.getKey(), dfsUri);\n       siteConfig.put(Property.INSTANCE_DFS_DIR.getKey(), \"/accumulo\");\n       config.setSiteConfig(siteConfig);\n+    } else if (config.useExistingInstance()) {\n+      dfsUri = CachedConfiguration.getInstance().get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY);\n     } else {\n       dfsUri = \"file://\";\n     }\n@@ -394,20 +420,22 @@ public boolean apply(Entry<String,String> v) {\n     File siteFile = new File(config.getConfDir(), \"accumulo-site.xml\");\n     writeConfig(siteFile, config.getSiteConfig().entrySet());\n \n-    zooCfgFile = new File(config.getConfDir(), \"zoo.cfg\");\n-    FileWriter fileWriter = new FileWriter(zooCfgFile);\n-\n-    // zookeeper uses Properties to read its config, so use that to write in order to properly escape things like Windows paths\n-    Properties zooCfg = new Properties();\n-    zooCfg.setProperty(\"tickTime\", \"2000\");\n-    zooCfg.setProperty(\"initLimit\", \"10\");\n-    zooCfg.setProperty(\"syncLimit\", \"5\");\n-    zooCfg.setProperty(\"clientPort\", config.getZooKeeperPort() + \"\");\n-    zooCfg.setProperty(\"maxClientCnxns\", \"1000\");\n-    zooCfg.setProperty(\"dataDir\", config.getZooKeeperDir().getAbsolutePath());\n-    zooCfg.store(fileWriter, null);\n-\n-    fileWriter.close();\n+    if (!config.useExistingInstance()) {\n+      zooCfgFile = new File(config.getConfDir(), \"zoo.cfg\");\n+      FileWriter fileWriter = new FileWriter(zooCfgFile);\n+\n+      // zookeeper uses Properties to read its config, so use that to write in order to properly escape things like Windows paths\n+      Properties zooCfg = new Properties();\n+      zooCfg.setProperty(\"tickTime\", \"2000\");\n+      zooCfg.setProperty(\"initLimit\", \"10\");\n+      zooCfg.setProperty(\"syncLimit\", \"5\");\n+      zooCfg.setProperty(\"clientPort\", config.getZooKeeperPort() + \"\");\n+      zooCfg.setProperty(\"maxClientCnxns\", \"1000\");\n+      zooCfg.setProperty(\"dataDir\", config.getZooKeeperDir().getAbsolutePath());\n+      zooCfg.store(fileWriter, null);\n+\n+      fileWriter.close();\n+    }\n \n     // disable audit logging for mini....\n     InputStream auditStream = this.getClass().getResourceAsStream(\"/auditLog.xml\");\n@@ -445,58 +473,98 @@ private void writeConfigProperties(File file, Map<String,String> settings) throw\n    */\n   @Override\n   public synchronized void start() throws IOException, InterruptedException {\n+    if (config.useExistingInstance()) {\n+      Configuration acuConf = config.getAccumuloConfiguration();\n+      Configuration hadoopConf = config.getHadoopConfiguration();\n+\n+      ConfigurationCopy cc = new ConfigurationCopy(acuConf);\n+      VolumeManager fs;\n+      try {\n+        fs = VolumeManagerImpl.get(cc, hadoopConf);\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+      Path instanceIdPath = Accumulo.getAccumuloInstanceIdPath(fs);\n \n-    if (!initialized) {\n+      String instanceIdFromFile = ZooUtil.getInstanceIDFromHdfs(instanceIdPath, cc, hadoopConf);\n+      IZooReaderWriter zrw = new ZooReaderWriterFactory().getZooReaderWriter(cc.get(Property.INSTANCE_ZK_HOST),\n+          (int) cc.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT), cc.get(Property.INSTANCE_SECRET));\n \n-      Runtime.getRuntime().addShutdownHook(new Thread() {\n-        @Override\n-        public void run() {\n-          try {\n-            MiniAccumuloClusterImpl.this.stop();\n-          } catch (IOException e) {\n-            e.printStackTrace();\n-          } catch (InterruptedException e) {\n-            e.printStackTrace();\n+      String rootPath = ZooUtil.getRoot(instanceIdFromFile);\n+\n+      String instanceName = null;\n+      try {\n+        for (String name : zrw.getChildren(Constants.ZROOT + Constants.ZINSTANCES)) {\n+          String instanceNamePath = Constants.ZROOT + Constants.ZINSTANCES + \"/\" + name;\n+          byte[] bytes = zrw.getData(instanceNamePath, new Stat());\n+          String iid = new String(bytes, Constants.UTF8);\n+          if (iid.equals(instanceIdFromFile)) {\n+            instanceName = name;\n           }\n         }\n-      });\n-    }\n+      } catch (KeeperException e) {\n+        throw new RuntimeException(\"Unable to read instance name from zookeeper.\", e);\n+      }\n+      if (instanceName == null)\n+        throw new RuntimeException(\"Unable to read instance name from zookeeper.\");\n \n-    if (zooKeeperProcess == null) {\n-      zooKeeperProcess = _exec(ZooKeeperServerMain.class, ServerType.ZOOKEEPER, zooCfgFile.getAbsolutePath());\n-    }\n+      config.setInstanceName(instanceName);\n+      if (!AccumuloStatus.isAccumuloOffline(zrw, rootPath))\n+        throw new RuntimeException(\"The Accumulo instance being used is already running. Aborting.\");\n+    } else {\n+      if (!initialized) {\n+        Runtime.getRuntime().addShutdownHook(new Thread() {\n+          @Override\n+          public void run() {\n+            try {\n+              MiniAccumuloClusterImpl.this.stop();\n+            } catch (IOException e) {\n+              e.printStackTrace();\n+            } catch (InterruptedException e) {\n+              e.printStackTrace();\n+            }\n+          }\n+        });\n+      }\n+\n+      if (zooKeeperProcess == null) {\n+        zooKeeperProcess = _exec(ZooKeeperServerMain.class, ServerType.ZOOKEEPER, zooCfgFile.getAbsolutePath());\n+      }\n \n-    if (!initialized) {\n-      // sleep a little bit to let zookeeper come up before calling init, seems to work better\n-      long startTime = System.currentTimeMillis();\n-      while (true) {\n-        Socket s = null;\n-        try {\n-          s = new Socket(\"localhost\", config.getZooKeeperPort());\n-          s.getOutputStream().write(\"ruok\\n\".getBytes());\n-          s.getOutputStream().flush();\n-          byte buffer[] = new byte[100];\n-          int n = s.getInputStream().read(buffer);\n-          if (n >= 4 && new String(buffer, 0, 4).equals(\"imok\"))\n-            break;\n-        } catch (Exception e) {\n-          if (System.currentTimeMillis() - startTime >= config.getZooKeeperStartupTime()) {\n-            throw new ZooKeeperBindException(\"Zookeeper did not start within \" + (config.getZooKeeperStartupTime() / 1000) + \" seconds. Check the logs in \"\n-                + config.getLogDir() + \" for errors.  Last exception: \" + e);\n+      if (!initialized) {\n+        // sleep a little bit to let zookeeper come up before calling init, seems to work better\n+        long startTime = System.currentTimeMillis();\n+        while (true) {\n+          Socket s = null;\n+          try {\n+            s = new Socket(\"localhost\", config.getZooKeeperPort());\n+            s.getOutputStream().write(\"ruok\\n\".getBytes());\n+            s.getOutputStream().flush();\n+            byte buffer[] = new byte[100];\n+            int n = s.getInputStream().read(buffer);\n+            if (n >= 4 && new String(buffer, 0, 4).equals(\"imok\"))\n+              break;\n+          } catch (Exception e) {\n+            if (System.currentTimeMillis() - startTime >= config.getZooKeeperStartupTime()) {\n+              throw new ZooKeeperBindException(\"Zookeeper did not start within \" + (config.getZooKeeperStartupTime() / 1000) + \" seconds. Check the logs in \"\n+                  + config.getLogDir() + \" for errors.  Last exception: \" + e);\n+            }\n+          } finally {\n+            if (s != null)\n+              s.close();\n           }\n-          UtilWaitThread.sleep(250);\n-        } finally {\n-          if (s != null)\n-            s.close();\n         }\n+        Process initProcess = exec(Initialize.class, \"--instance-name\", config.getInstanceName(), \"--password\", config.getRootPassword());\n+        int ret = initProcess.waitFor();\n+        if (ret != 0) {\n+          throw new RuntimeException(\"Initialize process returned \" + ret + \". Check the logs in \" + config.getLogDir() + \" for errors.\");\n+        }\n+        initialized = true;\n       }\n-      Process initProcess = exec(Initialize.class, \"--instance-name\", config.getInstanceName(), \"--password\", config.getRootPassword());\n-      int ret = initProcess.waitFor();\n-      if (ret != 0) {\n-        throw new RuntimeException(\"Initialize process returned \" + ret + \". Check the logs in \" + config.getLogDir() + \" for errors.\");\n-      }\n-      initialized = true;\n     }\n+    \n+    log.info(\"Starting MAC against instance {} and zookeeper(s) {}.\", config.getInstanceName(), config.getZooKeepers());\n+    \n     synchronized (tabletServerProcesses) {\n       for (int i = tabletServerProcesses.size(); i < config.getNumTservers(); i++) {\n         tabletServerProcesses.add(_exec(TabletServer.class, ServerType.TABLET_SERVER));\n@@ -744,11 +812,11 @@ protected ExecutorService getShutdownExecutor() {\n \n   private int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {\n     FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>() {\n-        @Override\n-        public Integer call() throws InterruptedException {\n-          proc.destroy();\n-          return proc.waitFor();\n-        }\n+      @Override\n+      public Integer call() throws InterruptedException {\n+        proc.destroy();\n+        return proc.waitFor();\n+      }\n     });\n \n     executor.execute(future);\n@@ -757,9 +825,9 @@ public Integer call() throws InterruptedException {\n   }\n \n   /**\n-   * Get programmatic interface to information available in a normal monitor.\n-   * XXX the returned structure won't contain information about the metadata table until there is data in it.\n-   * e.g. if you want to see the metadata table you should create a table.\n+   * Get programmatic interface to information available in a normal monitor. XXX the returned structure won't contain information about the metadata table\n+   * until there is data in it. e.g. if you want to see the metadata table you should create a table.\n+   * \n    * @since 1.6.1\n    */\n   public MasterMonitorInfo getMasterMonitorInfo() throws AccumuloException, AccumuloSecurityException {",
                "additions": 138,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java",
                "status": "modified",
                "changes": 208,
                "deletions": 70,
                "sha": "d5dc1e9704efc7f509c19f83dc9dd2bcb4044be3",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java",
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -18,13 +18,15 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.MalformedURLException;\n import java.util.HashMap;\n import java.util.Iterator;\n import java.util.Map;\n import java.util.Map.Entry;\n \n import org.apache.accumulo.cluster.AccumuloConfig;\n import org.apache.accumulo.core.conf.CredentialProviderFactoryShim;\n+import org.apache.accumulo.core.conf.DefaultConfiguration;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.minicluster.MemoryUnit;\n import org.apache.accumulo.minicluster.ServerType;\n@@ -55,18 +57,20 @@\n   private File libDir;\n   private File libExtDir;\n   private File confDir;\n+  private File hadoopConfDir = null;\n   private File zooKeeperDir;\n   private File accumuloDir;\n   private File logDir;\n   private File walogDir;\n \n   private int zooKeeperPort = 0;\n   private int configuredZooKeeperPort = 0;\n-  private long zooKeeperStartupTime = 20*1000;\n+  private long zooKeeperStartupTime = 20 * 1000;\n \n   private long defaultMemorySize = 128 * 1024 * 1024;\n \n   private boolean initialized = false;\n+  private Boolean existingInstance = null;\n \n   private boolean useMiniDFS = false;\n \n@@ -76,6 +80,10 @@\n \n   private String[] nativePathItems = null;\n \n+  // These are only used on top of existing instances\n+  private Configuration hadoopConf;\n+  private Configuration accumuloConf;\n+\n   /**\n    * @param dir\n    *          An empty or nonexistant directory that Accumulo and Zookeeper can store data in. Creating the directory is left to the user. Java 7, Guava, and\n@@ -109,18 +117,22 @@ MiniAccumuloConfigImpl initialize() {\n       logDir = new File(dir, \"logs\");\n       walogDir = new File(dir, \"walogs\");\n \n-      // TODO ACCUMULO-XXXX replace usage of instance.dfs.{dir,uri} with instance.volumes\n-      setInstanceLocation();\n+      // Never want to override these if an existing instance, which may be using the defaults\n+      if (existingInstance == null || !existingInstance) {\n+        existingInstance = false;\n+        // TODO ACCUMULO-XXXX replace usage of instance.dfs.{dir,uri} with instance.volumes\n+        setInstanceLocation();\n+        mergeProp(Property.INSTANCE_SECRET.getKey(), DEFAULT_INSTANCE_SECRET);\n+        mergeProp(Property.LOGGER_DIR.getKey(), walogDir.getAbsolutePath());\n+        mergeProp(Property.TRACE_TOKEN_PROPERTY_PREFIX.getKey() + \"password\", getRootPassword());\n+      }\n \n-      mergeProp(Property.INSTANCE_SECRET.getKey(), DEFAULT_INSTANCE_SECRET);\n       mergeProp(Property.TSERV_PORTSEARCH.getKey(), \"true\");\n-      mergeProp(Property.LOGGER_DIR.getKey(), walogDir.getAbsolutePath());\n       mergeProp(Property.TSERV_DATACACHE_SIZE.getKey(), \"10M\");\n       mergeProp(Property.TSERV_INDEXCACHE_SIZE.getKey(), \"10M\");\n       mergeProp(Property.TSERV_MAXMEM.getKey(), \"50M\");\n       mergeProp(Property.TSERV_WALOG_MAX_SIZE.getKey(), \"100M\");\n       mergeProp(Property.TSERV_NATIVEMAP_ENABLED.getKey(), \"false\");\n-      mergeProp(Property.TRACE_TOKEN_PROPERTY_PREFIX.getKey() + \"password\", getRootPassword());\n       // since there is a small amount of memory, check more frequently for majc... setting may not be needed in 1.5\n       mergeProp(Property.TSERV_MAJC_DELAY.getKey(), \"3\");\n       mergeProp(Property.GENERAL_CLASSPATHS.getKey(), libDir.getAbsolutePath() + \"/[^.].*[.]jar\");\n@@ -138,10 +150,13 @@ MiniAccumuloConfigImpl initialize() {\n         updateConfigForCredentialProvider();\n       }\n \n-      // zookeeper port should be set explicitly in this class, not just on the site config\n-      if (zooKeeperPort == 0)\n-        zooKeeperPort = PortUtils.getRandomFreePort();\n-      siteConfig.put(Property.INSTANCE_ZK_HOST.getKey(), \"localhost:\" + zooKeeperPort);\n+      if (existingInstance == null || !existingInstance) {\n+        existingInstance = false;\n+        // zookeeper port should be set explicitly in this class, not just on the site config\n+        if (zooKeeperPort == 0)\n+          zooKeeperPort = PortUtils.getRandomFreePort();\n+        siteConfig.put(Property.INSTANCE_ZK_HOST.getKey(), \"localhost:\" + zooKeeperPort);\n+      }\n       initialized = true;\n     }\n     return this;\n@@ -244,6 +259,15 @@ public MiniAccumuloConfigImpl setInstanceName(String instanceName) {\n    */\n   @Override\n   public MiniAccumuloConfigImpl setSiteConfig(Map<String,String> siteConfig) {\n+    if (existingInstance != null && existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set set config info when using an existing instance.\");\n+\n+    this.existingInstance = Boolean.FALSE;\n+\n+    return _setSiteConfig(siteConfig);\n+  }\n+\n+  private MiniAccumuloConfigImpl _setSiteConfig(Map<String,String> siteConfig) {\n     this.siteConfig = new HashMap<String,String>(siteConfig);\n     this.configuredSiteConig = new HashMap<String,String>(siteConfig);\n     return this;\n@@ -259,14 +283,19 @@ public MiniAccumuloConfigImpl setSiteConfig(Map<String,String> siteConfig) {\n    */\n   @Override\n   public MiniAccumuloConfigImpl setZooKeeperPort(int zooKeeperPort) {\n+    if (existingInstance != null && existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set zookeeper info when using an existing instance.\");\n+\n+    this.existingInstance = Boolean.FALSE;\n+\n     this.configuredZooKeeperPort = zooKeeperPort;\n     this.zooKeeperPort = zooKeeperPort;\n     return this;\n   }\n \n   /**\n-   * Configure the time to wait for ZooKeeper to startup.\n-   * Calling this method is optional. The default is 20000 milliseconds\n+   * <<<<<<< HEAD Configure the time to wait for ZooKeeper to startup. Calling this method is optional. The default is 20000 milliseconds ======= Configure the\n+   * time to wait for ZooKeeper to startup. Calling this method is optional. The default is 20000 milliseconds >>>>>>> ACCUMULO-2984\n    *\n    * @param zooKeeperStartupTime\n    *          Time to wait for ZooKeeper to startup, in milliseconds\n@@ -275,6 +304,11 @@ public MiniAccumuloConfigImpl setZooKeeperPort(int zooKeeperPort) {\n    */\n   @Override\n   public MiniAccumuloConfigImpl setZooKeeperStartupTime(long zooKeeperStartupTime) {\n+    if (existingInstance != null && existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set zookeeper info when using an existing instance.\");\n+\n+    this.existingInstance = Boolean.FALSE;\n+\n     this.zooKeeperStartupTime = zooKeeperStartupTime;\n     return this;\n   }\n@@ -557,7 +591,8 @@ public boolean isUseCredentialProvider() {\n   }\n \n   /**\n-   * @param useCredentialProvider the useCredentialProvider to set\n+   * @param useCredentialProvider\n+   *          the useCredentialProvider to set\n    */\n   public void setUseCredentialProvider(boolean useCredentialProvider) {\n     this.useCredentialProvider = useCredentialProvider;\n@@ -567,4 +602,88 @@ public void setUseCredentialProvider(boolean useCredentialProvider) {\n   public MiniAccumuloClusterImpl build() throws IOException {\n     return new MiniAccumuloClusterImpl(this);\n   }\n+\n+  /**\n+   * Informs MAC that it's running against an existing accumulo instance. It is assumed that it's already initialized and hdfs/zookeeper are already running.\n+   *\n+   * @param accumuloSite\n+   *          a File representation of the accumulo-site.xml file for the instance being run\n+   * @param hadoopConfDir\n+   *          a File representation of the hadoop configuration directory containing core-site.xml and hdfs-site.xml\n+   * \n+   * @return MiniAccumuloConfigImpl which uses an existing accumulo configuration\n+   *\n+   * @since 1.6.2\n+   *\n+   * @throws IOException\n+   *           when there are issues converting the provided Files to URLs\n+   */\n+  public MiniAccumuloConfigImpl useExistingInstance(File accumuloSite, File hadoopConfDir) throws IOException {\n+    if (existingInstance != null && !existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set to useExistingInstance after specifying config/zookeeper\");\n+\n+    this.existingInstance = Boolean.TRUE;\n+\n+    System.setProperty(\"org.apache.accumulo.config.file\", \"accumulo-site.xml\");\n+    this.hadoopConfDir = hadoopConfDir;\n+    hadoopConf = new Configuration(false);\n+    accumuloConf = new Configuration(false);\n+    File coreSite = new File(hadoopConfDir, \"core-site.xml\");\n+    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\n+\n+    try {\n+      accumuloConf.addResource(accumuloSite.toURI().toURL());\n+      hadoopConf.addResource(coreSite.toURI().toURL());\n+      hadoopConf.addResource(hdfsSite.toURI().toURL());\n+    } catch (MalformedURLException e1) {\n+      throw e1;\n+    }\n+\n+    Map<String,String> siteConfigMap = new HashMap<String,String>();\n+    for (Entry<String,String> e : accumuloConf) {\n+      siteConfigMap.put(e.getKey(), e.getValue());\n+    }\n+    _setSiteConfig(siteConfigMap);\n+    \n+    for (Entry<String,String> entry : DefaultConfiguration.getDefaultConfiguration())\n+      accumuloConf.setIfUnset(entry.getKey(), entry.getValue());\n+\n+    return this;\n+  }\n+\n+  /**\n+   * @return MAC should run assuming it's configured for an initialized accumulo instance\n+   *\n+   * @since 1.6.2\n+   */\n+  public boolean useExistingInstance() {\n+    return existingInstance != null && existingInstance;\n+  }\n+\n+  /**\n+   * @return hadoop configuration directory being used\n+   *\n+   * @since 1.6.2\n+   */\n+  public File getHadoopConfDir() {\n+    return this.hadoopConfDir;\n+  }\n+\n+  /**\n+   * @return accumulo Configuration being used\n+   * \n+   * @since 1.6.2\n+   */\n+  public Configuration getAccumuloConfiguration() {\n+    return accumuloConf;\n+  }\n+\n+  /**\n+   * @return hadoop Configuration being used\n+   * \n+   * @since 1.6.2\n+   */\n+  public Configuration getHadoopConfiguration() {\n+    return hadoopConf;\n+  }\n }",
                "additions": 132,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java",
                "status": "modified",
                "changes": 145,
                "deletions": 13,
                "sha": "2d7103e200978e7a68875f2c5acdcc58f11ce98b",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java",
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -98,5 +98,4 @@ public synchronized AccumuloConfiguration getConfiguration() {\n   public Instance getInstance() {\n     return scf.getInstance();\n   }\n-\n }",
                "additions": 0,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "status": "modified",
                "changes": 1,
                "deletions": 1,
                "sha": "904f4839b39b7571815e379f1f5717a1132aefb9",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -391,11 +391,14 @@ public static VolumeManager get() throws IOException {\n   static private final String DEFAULT = \"\";\n \n   public static VolumeManager get(AccumuloConfiguration conf) throws IOException {\n+    return get(conf, CachedConfiguration.getInstance());\n+  }\n+\n+  public static VolumeManager get(AccumuloConfiguration conf, final Configuration hadoopConf) throws IOException {\n     final Map<String,Volume> volumes = new HashMap<String,Volume>();\n-    final Configuration hadoopConf = CachedConfiguration.getInstance();\n \n     // The \"default\" Volume for Accumulo (in case no volumes are specified)\n-    for (String volumeUriOrDir : VolumeConfiguration.getVolumeUris(conf)) {\n+    for (String volumeUriOrDir : VolumeConfiguration.getVolumeUris(conf, hadoopConf)) {\n       if (volumeUriOrDir.equals(DEFAULT))\n         throw new IllegalArgumentException(\"Cannot re-define the default volume\");\n ",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java",
                "status": "modified",
                "changes": 7,
                "deletions": 2,
                "sha": "54d7e2a3f5af285f3ad99d19e340b99b990a0dd6",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.server.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.zookeeper.ZooUtil;\n+import org.apache.accumulo.fate.zookeeper.IZooReader;\n+import org.apache.accumulo.server.client.HdfsZooInstance;\n+import org.apache.zookeeper.KeeperException;\n+\n+public class AccumuloStatus {\n+  /**\n+   * Determines if there could be an accumulo instance running via zookeeper lock checking\n+   *\n+   * @param reader\n+   *\n+   * @return true iff all servers show no indication of being registered in zookeeper, otherwise false\n+   * @throws IOException\n+   *           if there are issues connecting to ZooKeeper to determine service status\n+   */\n+  public static boolean isAccumuloOffline(IZooReader reader) throws IOException {\n+    String rootPath = ZooUtil.getRoot(HdfsZooInstance.getInstance());\n+    return isAccumuloOffline(reader, rootPath);\n+  }\n+\n+  /**\n+   * Determines if there could be an accumulo instance running via zookeeper lock checking\n+   *\n+   * @param reader\n+   * @param rootPath\n+   *\n+   * @return true iff all servers show no indication of being registered in zookeeper, otherwise false\n+   * @throws IOException\n+   *           if there are issues connecting to ZooKeeper to determine service status\n+   */\n+  public static boolean isAccumuloOffline(IZooReader reader, String rootPath) throws IOException {\n+    try {\n+      for (String child : reader.getChildren(rootPath + Constants.ZTSERVERS)) {\n+        if (!reader.getChildren(rootPath + Constants.ZTSERVERS + \"/\" + child).isEmpty())\n+          return false;\n+      }\n+      if (!reader.getChildren(rootPath + Constants.ZTRACERS).isEmpty())\n+        return false;\n+      if (!reader.getChildren(rootPath + Constants.ZMASTER_LOCK).isEmpty())\n+        return false;\n+      if (!reader.getChildren(rootPath + Constants.ZMONITOR_LOCK).isEmpty())\n+        return false;\n+      if (!reader.getChildren(rootPath + Constants.ZGC_LOCK).isEmpty())\n+        return false;\n+    } catch (KeeperException e) {\n+      throw new IOException(\"Issues contacting ZooKeeper to get Accumulo status.\", e);\n+    } catch (InterruptedException e) {\n+      throw new IOException(\"Issues contacting ZooKeeper to get Accumulo status.\", e);\n+    }\n+    return true;\n+  }\n+\n+}",
                "additions": 74,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java",
                "status": "added",
                "changes": 74,
                "deletions": 0,
                "sha": "7e1cc9714f5fdd0fec118cb2be95fdb9036a248d",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java",
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            },
            {
                "patch": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.Collection;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+\n+import org.apache.accumulo.core.client.BatchWriter;\n+import org.apache.accumulo.core.client.BatchWriterConfig;\n+import org.apache.accumulo.core.client.Connector;\n+import org.apache.accumulo.core.client.Scanner;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.metadata.MetadataTable;\n+import org.apache.accumulo.core.metadata.RootTable;\n+import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.core.util.UtilWaitThread;\n+import org.apache.accumulo.minicluster.MiniAccumuloCluster;\n+import org.apache.accumulo.minicluster.MiniAccumuloConfig;\n+import org.apache.accumulo.minicluster.ServerType;\n+import org.apache.accumulo.minicluster.impl.MiniAccumuloConfigImpl;\n+import org.apache.accumulo.minicluster.impl.ProcessReference;\n+import org.apache.accumulo.test.functional.ConfigurableMacIT;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class ExistingMacIT extends ConfigurableMacIT {\n+  @Override\n+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {\n+    cfg.setProperty(Property.INSTANCE_ZK_TIMEOUT, \"5s\");\n+\n+    // use raw local file system so walogs sync and flush will work\n+    hadoopCoreSite.set(\"fs.file.impl\", RawLocalFileSystem.class.getName());\n+  }\n+\n+  private void createEmptyConfig(File confFile) throws IOException {\n+    Configuration conf = new Configuration(false);\n+    OutputStream hcOut = new FileOutputStream(confFile);\n+    conf.writeXml(hcOut);\n+    hcOut.close();\n+  }\n+\n+  @Test\n+  public void testExistingInstance() throws Exception {\n+\n+    Connector conn = getCluster().getConnector(\"root\", ROOT_PASSWORD);\n+\n+    conn.tableOperations().create(\"table1\");\n+\n+    BatchWriter bw = conn.createBatchWriter(\"table1\", new BatchWriterConfig());\n+\n+    Mutation m1 = new Mutation(\"00081\");\n+    m1.put(\"math\", \"sqroot\", \"9\");\n+    m1.put(\"math\", \"sq\", \"6560\");\n+\n+    bw.addMutation(m1);\n+    bw.close();\n+\n+    conn.tableOperations().flush(\"table1\", null, null, true);\n+    // TOOD use constants\n+    conn.tableOperations().flush(MetadataTable.NAME, null, null, true);\n+    conn.tableOperations().flush(RootTable.NAME, null, null, true);\n+\n+    Set<Entry<ServerType,Collection<ProcessReference>>> procs = getCluster().getProcesses().entrySet();\n+    for (Entry<ServerType,Collection<ProcessReference>> entry : procs) {\n+      if (entry.getKey() == ServerType.ZOOKEEPER)\n+        continue;\n+      for (ProcessReference pr : entry.getValue())\n+        getCluster().killProcess(entry.getKey(), pr);\n+    }\n+\n+    // TODO clean out zookeeper? following sleep waits for ephemeral nodes to go away\n+    UtilWaitThread.sleep(10000);\n+\n+    File hadoopConfDir = createTestDir(ExistingMacIT.class.getSimpleName() + \"_hadoop_conf\");\n+    FileUtils.deleteQuietly(hadoopConfDir);\n+    hadoopConfDir.mkdirs();\n+    createEmptyConfig(new File(hadoopConfDir, \"core-site.xml\"));\n+    createEmptyConfig(new File(hadoopConfDir, \"hdfs-site.xml\"));\n+\n+    File testDir2 = createTestDir(ExistingMacIT.class.getSimpleName() + \"_2\");\n+    FileUtils.deleteQuietly(testDir2);\n+\n+    MiniAccumuloConfig macConfig2 = new MiniAccumuloConfig(testDir2, \"notused\");\n+    macConfig2.useExistingInstance(new File(getCluster().getConfig().getConfDir(), \"accumulo-site.xml\"), hadoopConfDir);\n+\n+    MiniAccumuloCluster accumulo2 = new MiniAccumuloCluster(macConfig2);\n+    accumulo2.start();\n+\n+    conn = accumulo2.getConnector(\"root\", ROOT_PASSWORD);\n+\n+    Scanner scanner = conn.createScanner(\"table1\", Authorizations.EMPTY);\n+\n+    int sum = 0;\n+    for (Entry<Key,Value> entry : scanner) {\n+      sum += Integer.parseInt(entry.getValue().toString());\n+    }\n+\n+    Assert.assertEquals(6569, sum);\n+\n+    accumulo2.stop();\n+  }\n+\n+  @Test\n+  public void testExistingRunningInstance() throws Exception {\n+    File hadoopConfDir = createTestDir(ExistingMacIT.class.getSimpleName() + \"_hadoop_conf_2\");\n+    FileUtils.deleteQuietly(hadoopConfDir);\n+    hadoopConfDir.mkdirs();\n+    createEmptyConfig(new File(hadoopConfDir, \"core-site.xml\"));\n+    createEmptyConfig(new File(hadoopConfDir, \"hdfs-site.xml\"));\n+\n+    File testDir2 = createTestDir(ExistingMacIT.class.getSimpleName() + \"_3\");\n+    FileUtils.deleteQuietly(testDir2);\n+\n+    MiniAccumuloConfig macConfig2 = new MiniAccumuloConfig(testDir2, \"notused\");\n+    macConfig2.useExistingInstance(new File(getCluster().getConfig().getConfDir(), \"accumulo-site.xml\"), hadoopConfDir);\n+\n+    System.out.println(\"conf \" + new File(getCluster().getConfig().getConfDir(), \"accumulo-site.xml\"));\n+\n+    MiniAccumuloCluster accumulo2 = new MiniAccumuloCluster(macConfig2);\n+    try {\n+      accumulo2.start();\n+      Assert.fail();\n+    } catch (RuntimeException e) {\n+      // TODO check message or throw more explicit exception\n+    }\n+  }\n+}",
                "additions": 152,
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java",
                "status": "added",
                "changes": 152,
                "deletions": 0,
                "sha": "5d1978e1f27a4637bd432edf289a54ceb57774c8",
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java",
                "filename": "test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae"
            }
        ],
        "bug_id": "accumulo_24",
        "parent": "https://github.com/apache/accumulo/commit/758a364bda0b47520daf251d88e0057187884d66",
        "message": "ACCUMULO-2984 adding ability to run MAC against a permanent accumulo instance\n\nAddressing NPEs with existingInstance\n\nACCUMULO-2984 add test for existing mac using existing instance\n\nFixing my NPE fix and integrating Keith's test\n\nAdding ability to clean out singleton stuff\n\nNow no longer doing weird things with statics\n\nWarning cleanup\n\nRemoving added features no longer needed\n\nCleaning up minor ticket items\n\nFixing another Configuration plumbing issue and adding some info statements",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/fb2c0c75141177da429bc9094d6d5b15ba12f05e",
        "file": [
            {
                "patch": "@@ -244,11 +244,14 @@ public MiniAccumuloCluster(MiniAccumuloConfig config) throws IOException {\n \n     File nativeMap = new File(config.getLibDir().getAbsolutePath() + \"/native/map\");\n     nativeMap.mkdirs();\n-    String testRoot = new File(new File(System.getProperty(\"user.dir\")).getParent() + \"/server/src/main/c++/nativeMap\").getAbsolutePath();\n-    for (String file : new File(testRoot).list()) {\n-      File src = new File(testRoot, file);\n-      if (src.isFile() && file.startsWith(\"libNativeMap\"))\n-        FileUtils.copyFile(src, new File(nativeMap, file));\n+    File testRoot = new File(new File(new File(System.getProperty(\"user.dir\")).getParent() + \"/server/src/main/c++/nativeMap\").getAbsolutePath());\n+    \n+    if (testRoot.exists()) {\n+      for (String file : testRoot.list()) {\n+        File src = new File(testRoot, file);\n+        if (src.isFile() && file.startsWith(\"libNativeMap\"))\n+          FileUtils.copyFile(src, new File(nativeMap, file));\n+      }\n     }\n   }\n ",
                "additions": 8,
                "raw_url": "https://github.com/apache/accumulo/raw/fb2c0c75141177da429bc9094d6d5b15ba12f05e/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java",
                "status": "modified",
                "changes": 13,
                "deletions": 5,
                "sha": "c492e1b0ead9a0339987a788d187f1be7c1b1c30",
                "blob_url": "https://github.com/apache/accumulo/blob/fb2c0c75141177da429bc9094d6d5b15ba12f05e/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java",
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java?ref=fb2c0c75141177da429bc9094d6d5b15ba12f05e"
            }
        ],
        "bug_id": "accumulo_25",
        "parent": "https://github.com/apache/accumulo/commit/720e27a59a3a12ca70ee20d85eaf7c6afab83429",
        "message": "ACCUMULO-1537 fixed NPE in MAC\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1502381 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/fc34457d480a03b5df064ee0b26db34b8337a37e",
        "file": [
            {
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.accumulo.core.util.shell;\n \n import java.io.File;\n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n \n@@ -80,7 +81,7 @@ public AuthenticationToken convert(String value) {\n   private boolean hdfsZooInstance;\n   \n   @Parameter(names = {\"-z\", \"--zooKeeperInstance\"}, description = \"use a zookeeper instance with the given instance name and list of zoo hosts\", arity = 2)\n-  private List<String> zooKeeperInstance;\n+  private List<String> zooKeeperInstance = new ArrayList<String>();\n   \n   @Parameter(names = \"--auth-timeout\", description = \"minutes the shell can be idle without re-entering a password\")\n   private int authTimeout = 60; // TODO Add validator for positive number",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/fc34457d480a03b5df064ee0b26db34b8337a37e/core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "33b3eacd06849ad2aa91098d1afa3756fce4fa39",
                "blob_url": "https://github.com/apache/accumulo/blob/fc34457d480a03b5df064ee0b26db34b8337a37e/core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java",
                "filename": "core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java?ref=fc34457d480a03b5df064ee0b26db34b8337a37e"
            }
        ],
        "bug_id": "accumulo_26",
        "parent": "https://github.com/apache/accumulo/commit/98261a88635f6e481cd439843e4ffd310b57e486",
        "message": "ACCUMULO-1478 prevent NPE when starting the shell\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1490335 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/64b2b47986201fc2100b4182a9785d30652a0ec3",
        "file": [
            {
                "patch": "@@ -897,12 +897,13 @@ public void run() {\n         String oldThreadName = Thread.currentThread().getName();\n \n         try {\n-          runState.set(ScanRunState.RUNNING);\n+          if (isCancelled() || scanSession == null)\n+            return;\n+          \n           Thread.currentThread().setName(\n               \"User: \" + scanSession.user + \" Start: \" + scanSession.startTime + \" Client: \" + scanSession.client + \" Tablet: \" + scanSession.extent);\n \n-          if (isCancelled() || scanSession == null)\n-            return;\n+          runState.set(ScanRunState.RUNNING);\n           \n           Tablet tablet = onlineTablets.get(scanSession.extent);\n           \n@@ -932,7 +933,7 @@ public void run() {\n         } catch (TooManyFilesException tmfe) {\n           addResult(tmfe);\n         } catch (Throwable e) {\n-          log.warn(\"exception while scanning tablet \" + scanSession.extent, e);\n+          log.warn(\"exception while scanning tablet \" + (scanSession == null ? \"(unknown)\" : scanSession.extent), e);\n           addResult(e);\n         } finally {\n           runState.set(ScanRunState.FINISHED);",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/64b2b47986201fc2100b4182a9785d30652a0ec3/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java",
                "status": "modified",
                "changes": 9,
                "deletions": 4,
                "sha": "09be9077b280bed4d28a829cb9758ec6e1d5d0d9",
                "blob_url": "https://github.com/apache/accumulo/blob/64b2b47986201fc2100b4182a9785d30652a0ec3/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java",
                "filename": "src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletServer.java?ref=64b2b47986201fc2100b4182a9785d30652a0ec3"
            }
        ],
        "bug_id": "accumulo_27",
        "parent": "https://github.com/apache/accumulo/commit/d8897047b8165da66b9fe5fae7665dc8879a883b",
        "message": "ACCUMUL-909 found some other NPE opportunities\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/branches/1.4@1438669 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/35a67466b6685b6cb03e727585701b3029163cdf",
        "file": [
            {
                "patch": "@@ -70,6 +70,7 @@ protected TabletBalancer getBalancerForTable(String table) {\n           if (newBalancer != null) {\n             balancer = newBalancer;\n             perTableBalancers.put(table, balancer);\n+            balancer.init(configuration);\n           }\n         } catch (Exception e) {\n           log.warn(\"Failed to load table balancer class \", e);\n@@ -89,6 +90,7 @@ protected TabletBalancer getBalancerForTable(String table) {\n         balancer = new DefaultLoadBalancer(table);\n       }\n       perTableBalancers.put(table, balancer);\n+      balancer.init(configuration);\n     }\n     return balancer;\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/35a67466b6685b6cb03e727585701b3029163cdf/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "status": "modified",
                "changes": 2,
                "deletions": 0,
                "sha": "c57e1e182941a30f979f0b69dba544eec8558df8",
                "blob_url": "https://github.com/apache/accumulo/blob/35a67466b6685b6cb03e727585701b3029163cdf/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=35a67466b6685b6cb03e727585701b3029163cdf"
            }
        ],
        "bug_id": "accumulo_28",
        "parent": "https://github.com/apache/accumulo/commit/8fad5e736750868dc563caeb9cf89c38005f034a",
        "message": "ACCUMULO-590 fixed NPE in Table load balancer\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1337149 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/8fad5e736750868dc563caeb9cf89c38005f034a",
        "file": [
            {
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.accumulo.core.data.KeyExtent;\n import org.apache.accumulo.core.master.thrift.TabletServerStatus;\n import org.apache.accumulo.server.client.HdfsZooInstance;\n-import org.apache.accumulo.server.conf.ServerConfiguration;\n import org.apache.accumulo.server.master.state.TServerInstance;\n import org.apache.accumulo.server.master.state.TabletMigration;\n import org.apache.accumulo.server.security.SecurityConstants;\n@@ -44,11 +43,6 @@\n   private static final Logger log = Logger.getLogger(TableLoadBalancer.class);\n   \n   Map<String,TabletBalancer> perTableBalancers = new HashMap<String,TabletBalancer>();\n-  ServerConfiguration config;\n-  \n-  public void init(ServerConfiguration config) {\n-    this.config = config;\n-  }\n   \n   private TabletBalancer constructNewBalancerForTable(String clazzName, String table) throws Exception {\n     Class<? extends TabletBalancer> clazz = AccumuloClassLoader.loadClass(clazzName, TabletBalancer.class);\n@@ -57,7 +51,7 @@ private TabletBalancer constructNewBalancerForTable(String clazzName, String tab\n   }\n   \n   protected String getLoadBalancerClassNameForTable(String table) {\n-    return config.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n+    return configuration.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n   }\n   \n   protected TabletBalancer getBalancerForTable(String table) {",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/8fad5e736750868dc563caeb9cf89c38005f034a/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "status": "modified",
                "changes": 8,
                "deletions": 7,
                "sha": "3598504ac332bb057a06bb914905e15a2936db89",
                "blob_url": "https://github.com/apache/accumulo/blob/8fad5e736750868dc563caeb9cf89c38005f034a/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=8fad5e736750868dc563caeb9cf89c38005f034a"
            }
        ],
        "bug_id": "accumulo_29",
        "parent": "https://github.com/apache/accumulo/commit/4a7d4b91461454ac047611dd51e0d20a51e92a2b",
        "message": "ACCUMULO-590 fixed NPE in Table load balancer\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1337131 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/e600d5010fe1277c33ed64c3a16108dfca9f72d1",
        "file": [
            {
                "patch": "@@ -36,6 +36,7 @@\n \n public class FileCountMR extends Configured implements Tool {\n   private static final String OUTPUT_VIS = FileCountMR.class.getSimpleName() + \".output.vis\";\n+  private static final Text EMPTY = new Text();\n   \n   public static class FileCountMapper extends Mapper<Key,Value,Key,Value> {\n     long dirCount = 0;\n@@ -114,7 +115,7 @@ protected void reduce(Key key, Iterable<Value> values, Context context) throws I\n       }\n       Mutation m = new Mutation(QueryUtil.getRow(key.getRow().toString()));\n       m.put(QueryUtil.DIR_COLF, QueryUtil.COUNTS_COLQ, colvis, sas.aggregate());\n-      context.write(null, m);\n+      context.write(EMPTY, m);\n     }\n     \n     @Override",
                "additions": 2,
                "raw_url": "https://github.com/apache/accumulo/raw/e600d5010fe1277c33ed64c3a16108dfca9f72d1/src/examples/src/main/java/org/apache/accumulo/examples/dirlist/FileCountMR.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "7678137d6348e1e5da636d45d7b7205e00645fb3",
                "blob_url": "https://github.com/apache/accumulo/blob/e600d5010fe1277c33ed64c3a16108dfca9f72d1/src/examples/src/main/java/org/apache/accumulo/examples/dirlist/FileCountMR.java",
                "filename": "src/examples/src/main/java/org/apache/accumulo/examples/dirlist/FileCountMR.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/examples/src/main/java/org/apache/accumulo/examples/dirlist/FileCountMR.java?ref=e600d5010fe1277c33ed64c3a16108dfca9f72d1"
            }
        ],
        "bug_id": "accumulo_30",
        "parent": "https://github.com/apache/accumulo/commit/44cfb3d8652a402ece6a5468be7d201353a4092b",
        "message": "ACCUMULO-86: example is getting NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1196054 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/3c513c894d711dd6d52e798a239b893ea94aa124",
        "file": [
            {
                "patch": "@@ -3118,9 +3118,15 @@ private SplitRowSpec findSplitRow(Collection<String> files) {\n       } else {\n         lastRow = extent.getEndRow();\n       }\n-      \n+\n+      // We expect to get a midPoint for this set of files. If we don't get one, we have a problem.\n+      final Key mid = keys.get(.5);\n+      if (null == mid) {\n+        throw new IllegalStateException(\"Could not determine midpoint for files\");\n+      }\n+\n       // check to see that the midPoint is not equal to the end key\n-      if (keys.get(.5).compareRow(lastRow) == 0) {\n+      if (mid.compareRow(lastRow) == 0) {\n         if (keys.firstKey() < .5) {\n           Key candidate = keys.get(keys.firstKey());\n           if (candidate.compareRow(lastRow) != 0) {\n@@ -3140,8 +3146,7 @@ private SplitRowSpec findSplitRow(Collection<String> files) {\n         \n         return null;\n       }\n-      Key mid = keys.get(.5);\n-      Text text = (mid == null) ? null : mid.getRow();\n+      Text text = mid.getRow();\n       SortedMap<Double,Key> firstHalf = keys.headMap(.5);\n       if (firstHalf.size() > 0) {\n         Text beforeMid = firstHalf.get(firstHalf.lastKey()).getRow();",
                "additions": 9,
                "raw_url": "https://github.com/apache/accumulo/raw/3c513c894d711dd6d52e798a239b893ea94aa124/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "status": "modified",
                "changes": 13,
                "deletions": 4,
                "sha": "a1fc70747f46b6edb8ef3586e80aede239b1b34f",
                "blob_url": "https://github.com/apache/accumulo/blob/3c513c894d711dd6d52e798a239b893ea94aa124/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "filename": "server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java?ref=3c513c894d711dd6d52e798a239b893ea94aa124"
            }
        ],
        "bug_id": "accumulo_31",
        "parent": "https://github.com/apache/accumulo/commit/9f3cbb30de0fa1115d70cacdccda4e290ad657be",
        "message": "ACCUMULO-2870 Fail hard and fast if we don't calculate a midpoint.\n\nBy failing early, we catch the change in functionality from FileUtil.findMidPoint\nwhich always returns a mid-point. Also prevents an NPE from unexpectedly being thrown.",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/1693809434b0aa954d67a832ad3b9412703e1405",
        "file": [
            {
                "patch": "@@ -133,23 +133,33 @@ public synchronized void close() throws NoSuchLogIDException, LoggerClosedExcept\n     }\n   }\n   \n+  private void checkClosed() throws LoggerClosedException {\n+    if (client == null)\n+      throw new LoggerClosedException();\n+  }\n+  \n   public synchronized void defineTablet(int seq, int tid, KeyExtent tablet) throws NoSuchLogIDException, LoggerClosedException, TException {\n+    checkClosed();\n     client.defineTablet(null, logFile.id, seq, tid, tablet.toThrift());\n   }\n   \n   public synchronized void log(int seq, int tid, Mutation mutation) throws NoSuchLogIDException, LoggerClosedException, TException {\n+    checkClosed();\n     client.log(null, logFile.id, seq, tid, mutation.toThrift());\n   }\n   \n   public synchronized void logManyTablets(List<TabletMutations> mutations) throws NoSuchLogIDException, LoggerClosedException, TException {\n+    checkClosed();\n     client.logManyTablets(null, logFile.id, mutations);\n   }\n   \n   public synchronized void minorCompactionFinished(int seq, int tid, String fqfn) throws NoSuchLogIDException, LoggerClosedException, TException {\n+    checkClosed();\n     client.minorCompactionFinished(null, logFile.id, seq, tid, fqfn);\n   }\n   \n   public synchronized void minorCompactionStarted(int seq, int tid, String fqfn) throws NoSuchLogIDException, LoggerClosedException, TException {\n+    checkClosed();\n     client.minorCompactionStarted(null, logFile.id, seq, tid, fqfn);\n   }\n   ",
                "additions": 10,
                "raw_url": "https://github.com/apache/accumulo/raw/1693809434b0aa954d67a832ad3b9412703e1405/src/server/src/main/java/org/apache/accumulo/server/tabletserver/log/RemoteLogger.java",
                "status": "modified",
                "changes": 10,
                "deletions": 0,
                "sha": "b0bf66a37e2315e25f8b3d29659c052ab049acb5",
                "blob_url": "https://github.com/apache/accumulo/blob/1693809434b0aa954d67a832ad3b9412703e1405/src/server/src/main/java/org/apache/accumulo/server/tabletserver/log/RemoteLogger.java",
                "filename": "src/server/src/main/java/org/apache/accumulo/server/tabletserver/log/RemoteLogger.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/tabletserver/log/RemoteLogger.java?ref=1693809434b0aa954d67a832ad3b9412703e1405"
            }
        ],
        "bug_id": "accumulo_32",
        "parent": "https://github.com/apache/accumulo/commit/c70a4b380248945c028b90daaf58cbf7e4070e7a",
        "message": "ACCUMULO-888 concurrent use of a logger can result in an NPE; make the error explicitly a LoggerClosedException\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/branches/1.4@1416580 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/80577de0e039fb391223928831ae89798d654a0e",
        "file": [
            {
                "patch": "@@ -223,7 +223,7 @@ public static String putEphemeralSequential(ZooKeeper zk, String zPath, byte[] d\n     \n     List<String> children = zc.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return null;\n     }\n     ",
                "additions": 1,
                "raw_url": "https://github.com/apache/accumulo/raw/80577de0e039fb391223928831ae89798d654a0e/fate/src/main/java/org/apache/accumulo/fate/zookeeper/ZooUtil.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "0172b2a082348a64eb2de3c065d8f5e4372515a4",
                "blob_url": "https://github.com/apache/accumulo/blob/80577de0e039fb391223928831ae89798d654a0e/fate/src/main/java/org/apache/accumulo/fate/zookeeper/ZooUtil.java",
                "filename": "fate/src/main/java/org/apache/accumulo/fate/zookeeper/ZooUtil.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/fate/src/main/java/org/apache/accumulo/fate/zookeeper/ZooUtil.java?ref=80577de0e039fb391223928831ae89798d654a0e"
            }
        ],
        "bug_id": "accumulo_33",
        "parent": "https://github.com/apache/accumulo/commit/efb002ba6542f8fc09649591ef582d220618d52a",
        "message": "ACCUMULO-684: prevent an NPE when a user is being deleted while we are changing their permissions\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1360187 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/c79cb06b345dbf43ebe34ab17bf60f517f27ca5f",
        "file": [
            {
                "patch": "@@ -305,7 +305,7 @@ public static boolean isLockHeld(ZooKeeper zk, LockID lid) throws KeeperExceptio\n     \n     List<String> children = zk.getChildren(lid.path, false);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return false;\n     }\n     \n@@ -323,7 +323,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n     \n     List<String> children = zc.getChildren(lid.path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return false;\n     }\n     \n@@ -341,7 +341,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n   public static byte[] getLockData(ZooKeeper zk, String path) throws KeeperException, InterruptedException {\n     List<String> children = zk.getChildren(path, false);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return null;\n     }\n     \n@@ -356,7 +356,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n     \n     List<String> children = zc.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return null;\n     }\n     \n@@ -381,7 +381,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n   public static long getSessionId(ZooCache zc, String path) throws KeeperException, InterruptedException {\n     List<String> children = zc.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return 0;\n     }\n     \n@@ -406,7 +406,7 @@ public static void deleteLock(String path) throws InterruptedException, KeeperEx\n     IZooReaderWriter zk = ZooReaderWriter.getInstance();\n     children = zk.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       throw new IllegalStateException(\"No lock is held at \" + path);\n     }\n     \n@@ -428,7 +428,7 @@ public static boolean deleteLock(String path, String lockData) throws Interrupte\n     IZooReaderWriter zk = ZooReaderWriter.getInstance();\n     children = zk.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       throw new IllegalStateException(\"No lock is held at \" + path);\n     }\n     ",
                "additions": 7,
                "raw_url": "https://github.com/apache/accumulo/raw/c79cb06b345dbf43ebe34ab17bf60f517f27ca5f/src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java",
                "status": "modified",
                "changes": 14,
                "deletions": 7,
                "sha": "06d136f69de16962d3eb382c15caab4676be8b8d",
                "blob_url": "https://github.com/apache/accumulo/blob/c79cb06b345dbf43ebe34ab17bf60f517f27ca5f/src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java",
                "filename": "src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java?ref=c79cb06b345dbf43ebe34ab17bf60f517f27ca5f"
            }
        ],
        "bug_id": "accumulo_34",
        "parent": "https://github.com/apache/accumulo/commit/75202271ee82e1149c0140915f6389621be8271c",
        "message": "ACCUMULO-93: tighten up the lock check, ensure lock checking does not throw NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1190505 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/75202271ee82e1149c0140915f6389621be8271c",
        "file": [
            {
                "patch": "@@ -38,12 +38,19 @@\n import org.apache.accumulo.core.util.ArgumentChecker;\n import org.apache.accumulo.core.util.ThriftUtil;\n import org.apache.accumulo.core.zookeeper.ZooCache;\n+import org.apache.accumulo.core.zookeeper.ZooLock;\n import org.apache.accumulo.core.zookeeper.ZooUtil;\n+import org.apache.log4j.Logger;\n+import org.apache.thrift.TApplicationException;\n+import org.apache.thrift.TException;\n+import org.apache.thrift.TServiceClient;\n+import org.apache.thrift.transport.TTransportException;\n \n /**\n  * Provides a class for administering the accumulo instance\n  */\n public class InstanceOperations {\n+  private static final Logger log = Logger.getLogger(InstanceOperations.class);\n   private Instance instance;\n   private AuthInfo credentials;\n   \n@@ -132,9 +139,13 @@ public void execute(MasterClientService.Iface client) throws Exception {\n     String path = ZooUtil.getRoot(instance) + Constants.ZTSERVERS;\n     List<String> results = new ArrayList<String>();\n     for (String candidate : cache.getChildren(path)) {\n-      List<String> lockEntries = cache.getChildren(path + \"/\" + candidate);\n-      if (lockEntries != null && lockEntries.size() == 2) {\n-        results.add(candidate);\n+      try {\n+        byte[] data = ZooLock.getLockData(cache, path + \"/\" + candidate);\n+        if (data != null && !\"master\".equals(new String(data))) {\n+          results.add(candidate);\n+        }\n+      } catch (Exception ex) {\n+        log.error(\"Unable to read lock data:\" + path);\n       }\n     }\n     return results;",
                "additions": 14,
                "raw_url": "https://github.com/apache/accumulo/raw/75202271ee82e1149c0140915f6389621be8271c/src/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java",
                "status": "modified",
                "changes": 17,
                "deletions": 3,
                "sha": "ba0e9ffb3412ade0f4a01fe93cd868f697f16b0d",
                "blob_url": "https://github.com/apache/accumulo/blob/75202271ee82e1149c0140915f6389621be8271c/src/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java",
                "filename": "src/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/core/src/main/java/org/apache/accumulo/core/client/admin/InstanceOperations.java?ref=75202271ee82e1149c0140915f6389621be8271c"
            }
        ],
        "bug_id": "accumulo_35",
        "parent": "https://github.com/apache/accumulo/commit/22047d6d1aa51ef9ad5b3633e78645c3f63975dd",
        "message": "ACCUMULO-93: tighten up the lock check, ensure lock checking does not throw NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1190503 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/0dfaa714a27d208a2ecf35cca50d241db0e7dca9",
        "file": [
            {
                "patch": "@@ -26,28 +26,40 @@\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.WatchedEvent;\n import org.apache.zookeeper.Watcher;\n+import org.apache.zookeeper.Watcher.Event.EventType;\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n /**\n  * Find a Span collector via zookeeper and push spans there via Thrift RPC\n  * \n  */\n-public class ZooSpanClient extends SendSpansViaThrift implements Watcher {\n+public class ZooSpanClient extends SendSpansViaThrift {\n   \n   private static final Logger log = Logger.getLogger(ZooSpanClient.class);\n   private static final int TOTAL_TIME_WAIT_CONNECT_MS = 10 * 1000;\n   private static final int TIME_WAIT_CONNECT_CHECK_MS = 100;\n   \n-  final ZooKeeper zoo;\n+  ZooKeeper zoo = null;\n   final String path;\n   final Random random = new Random();\n   final List<String> hosts = new ArrayList<String>();\n   \n-  public ZooSpanClient(String keepers, String path, String host, String service, long millis) throws IOException, KeeperException, InterruptedException {\n+  public ZooSpanClient(String keepers, final String path, String host, String service, long millis) throws IOException, KeeperException, InterruptedException {\n     super(host, service, millis);\n     this.path = path;\n-    zoo = new ZooKeeper(keepers, 30 * 1000, this);\n+    zoo = new ZooKeeper(keepers, 30 * 1000, new Watcher() {\n+      @Override\n+      public void process(WatchedEvent event) {\n+        try {\n+          if (zoo != null) {\n+            updateHosts(path, zoo.getChildren(path, null));\n+          }\n+        } catch (Exception ex) {\n+          log.error(\"unable to get destination hosts in zookeeper\", ex);\n+        }\n+      }\n+    });\n     for (int i = 0; i < TOTAL_TIME_WAIT_CONNECT_MS; i += TIME_WAIT_CONNECT_CHECK_MS) {\n       if (zoo.getState().equals(States.CONNECTED)) break;\n       try {\n@@ -84,13 +96,4 @@ synchronized protected String getSpanKey(Map<String,String> data) {\n     }\n     return null;\n   }\n-  \n-  @Override\n-  public void process(WatchedEvent event) {\n-    try {\n-      updateHosts(path, zoo.getChildren(path, null));\n-    } catch (Exception ex) {\n-      log.error(\"unable to get destination hosts in zookeeper\", ex);\n-    }\n-  }\n }",
                "additions": 16,
                "raw_url": "https://github.com/apache/accumulo/raw/0dfaa714a27d208a2ecf35cca50d241db0e7dca9/src/trace/src/main/java/cloudtrace/instrument/receivers/ZooSpanClient.java",
                "status": "modified",
                "changes": 29,
                "deletions": 13,
                "sha": "2c32836e9e559da35b7a7405ba4416ff70ccac9d",
                "blob_url": "https://github.com/apache/accumulo/blob/0dfaa714a27d208a2ecf35cca50d241db0e7dca9/src/trace/src/main/java/cloudtrace/instrument/receivers/ZooSpanClient.java",
                "filename": "src/trace/src/main/java/cloudtrace/instrument/receivers/ZooSpanClient.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/trace/src/main/java/cloudtrace/instrument/receivers/ZooSpanClient.java?ref=0dfaa714a27d208a2ecf35cca50d241db0e7dca9"
            }
        ],
        "bug_id": "accumulo_36",
        "parent": "https://github.com/apache/accumulo/commit/27725386429f25948cbae449f3811392143bceaa",
        "message": "ACCUMULO-66: defend against NPE due to \"this\" escaping the constructor; inline callback \n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1188619 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    },
    {
        "commit": "https://github.com/apache/accumulo/commit/c3ca5e770792368311f55717d8c4b0a0bdc5b7e7",
        "file": [
            {
                "patch": "@@ -1388,10 +1388,6 @@ private Tablet(final TabletServer tabletServer,\n \t\ttabletTime = TabletTime.getInstance(time);\n \t\tpersistedTime = tabletTime.getTime();\n \t\t\n-\t\tconstraintChecker.set(ConstraintLoader.load(extent.getTableId().toString()));\n-\n-\t\t\n-\n \t\tacuTableConf.addObserver(configObserver = new ConfigurationObserver(){\n \n \t\t\tprivate void reloadConstraints(){\n@@ -3049,6 +3045,9 @@ private CompactionStats _majorCompact(MajorCompactionReason reason) throws IOExc\n \t\tMap<String, Long> filesToCompact;\n \t\t\n \t\tint maxFilesToCompact = acuTableConf.getCount(Property.TSERV_MAJC_THREAD_MAXOPEN);\n+\t\t\n+\t\tCompactionStats majCStats = new CompactionStats();\n+\n \t\tsynchronized(this) {\n \t\t\t//plan all that work that needs to be done in the sync block... then do the actual work\n \t\t\t//outside the sync block\n@@ -3078,7 +3077,7 @@ private CompactionStats _majorCompact(MajorCompactionReason reason) throws IOExc\n \t\t\tCompactionTuple ret = getFilesToCompact(reason, falks);\n \t\t\tif(ret == null){\n \t\t\t\t//nothing to compact\n-\t\t\t\treturn null;\n+        return majCStats;\n \t\t\t}\n \t\t\tfilesToCompact = ret.getFilesToCompact();\n \n@@ -3105,8 +3104,6 @@ private CompactionStats _majorCompact(MajorCompactionReason reason) throws IOExc\n \t\t\t\t//compacting everything, so update the compaction id in !METADATA\n \t\t\t\tcompactionId = getCompactionID();\n \t\t\t}\n-\t\t\t\n-\t\t\tCompactionStats majCStats = new CompactionStats();\n \n \t\t\t//need to handle case where only one file is being major compacted\n \t\t\twhile(filesToCompact.size() > 0) {\n@@ -3252,7 +3249,7 @@ private CompactionStats majorCompact(MajorCompactionReason reason) {\n \t\t\t\tmajorCompactionInProgress = false;\n \t\t\t\tthis.notifyAll();\n \t\t\t}\n-\t\t\t//TODO majCStats could be null\n+\n \t\t\tSpan curr = Trace.currentTrace();\n \t\t\tcurr.data(\"extent\",  \"\"+getExtent());\n \t\t\tcurr.data(\"read\",    \"\"+majCStats.getEntriesRead());",
                "additions": 5,
                "raw_url": "https://github.com/apache/accumulo/raw/c3ca5e770792368311f55717d8c4b0a0bdc5b7e7/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "status": "modified",
                "changes": 13,
                "deletions": 8,
                "sha": "3f5fbeef7c4af295bb9368a32b9934913fbdc3b2",
                "blob_url": "https://github.com/apache/accumulo/blob/c3ca5e770792368311f55717d8c4b0a0bdc5b7e7/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "filename": "src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java?ref=c3ca5e770792368311f55717d8c4b0a0bdc5b7e7"
            }
        ],
        "bug_id": "accumulo_37",
        "parent": "https://github.com/apache/accumulo/commit/f13ce13b70ef0f5858738ab4f3ec08483fe227c3",
        "message": "ACCUMULO-20 committing patch that fixes NPE and removes redundant reload of contraints\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1182972 13f79535-47bb-0310-9956-ffa450edef68",
        "repo": "accumulo"
    }
]