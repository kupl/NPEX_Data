[
    {
        "repo": "hbase",
        "message": "HBASE-23308: Review of NullPointerExceptions (#836)\n\nSigned-off-by: stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/33bedf8d4d7ec320c5cc01c1c031035a1523f973",
        "parent": "https://github.com/apache/hbase/commit/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
        "bug_id": "hbase_1",
        "file": [
            {
                "sha": "cfccf30e3a0d46540deea94ddf6a3d32354232e7",
                "filename": "hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/BackupDriver.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -40,6 +40,7 @@\n \n import java.io.IOException;\n import java.net.URI;\n+import java.util.Objects;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n@@ -187,10 +188,7 @@ public static void main(String[] args) throws Exception {\n \n   @Override\n   public int run(String[] args) throws IOException {\n-    if (conf == null) {\n-      LOG.error(\"Tool configuration is not initialized\");\n-      throw new NullPointerException(\"conf\");\n-    }\n+    Objects.requireNonNull(conf, \"Tool configuration is not initialized\");\n \n     CommandLine cmd;\n     try {",
                "deletions": 4
            },
            {
                "sha": "39cc440a4edcb036f8e39a114b56811581ba0422",
                "filename": "hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/RestoreDriver.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -35,6 +35,7 @@\n import java.io.IOException;\n import java.net.URI;\n import java.util.List;\n+import java.util.Objects;\n \n import org.apache.commons.lang3.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n@@ -232,10 +233,7 @@ public static void main(String[] args) throws Exception {\n \n   @Override\n   public int run(String[] args) {\n-    if (conf == null) {\n-      LOG.error(\"Tool configuration is not initialized\");\n-      throw new NullPointerException(\"conf\");\n-    }\n+    Objects.requireNonNull(conf, \"Tool configuration is not initialized\");\n \n     CommandLine cmd;\n     try {",
                "deletions": 4
            },
            {
                "sha": "65a3393cf8e8c765023db39becbce1e0e70cabbe",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -31,6 +31,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.NavigableMap;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.regex.Matcher;\n@@ -230,13 +231,13 @@ public static void fullScanTables(Connection connection, final Visitor visitor)\n    * Callers should call close on the returned {@link Table} instance.\n    * @param connection connection we're using to access Meta\n    * @return An {@link Table} for <code>hbase:meta</code>\n+   * @throws NullPointerException if {@code connection} is {@code null}\n    */\n   public static Table getMetaHTable(final Connection connection)\n   throws IOException {\n     // We used to pass whole CatalogTracker in here, now we just pass in Connection\n-    if (connection == null) {\n-      throw new NullPointerException(\"No connection\");\n-    } else if (connection.isClosed()) {\n+    Objects.requireNonNull(connection, \"Connection cannot be null\");\n+    if (connection.isClosed()) {\n       throw new IOException(\"connection is closed\");\n     }\n     return connection.getTable(TableName.META_TABLE_NAME);",
                "deletions": 3
            },
            {
                "sha": "2dcfe0e7d6b3373883ca5c7b4722f7e1d1e3c5b6",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -38,6 +38,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.NavigableSet;\n+import java.util.Objects;\n import java.util.function.Function;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.Cell;\n@@ -1594,14 +1595,13 @@ public static ScanMetrics toScanMetrics(final byte[] bytes) {\n   }\n \n   /**\n-   * Unwraps an exception from a protobuf service into the underlying (expected) IOException.\n-   * This method will <strong>always</strong> throw an exception.\n+   * Unwraps an exception from a protobuf service into the underlying (expected) IOException. This\n+   * method will <strong>always</strong> throw an exception.\n    * @param se the {@code ServiceException} instance to convert into an {@code IOException}\n+   * @throws NullPointerException if {@code se} is {@code null}\n    */\n   public static void toIOException(ServiceException se) throws IOException {\n-    if (se == null) {\n-      throw new NullPointerException(\"Null service exception passed!\");\n-    }\n+    Objects.requireNonNull(se, \"Service exception cannot be null\");\n \n     Throwable cause = se.getCause();\n     if (cause != null && cause instanceof IOException) {",
                "deletions": 5
            },
            {
                "sha": "b3152c3c0eff9135bf5459f765edaf89484edf5e",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java",
                "status": "modified",
                "changes": 8,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlUtil.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -21,6 +21,7 @@\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n \n import org.apache.commons.lang3.StringUtils;\n import org.apache.hadoop.hbase.HConstants;\n@@ -56,6 +57,7 @@ private AccessControlUtil() {}\n    * @param qualifier optional qualifier\n    * @param actions the permissions to be granted\n    * @return A {@link AccessControlProtos} GrantRequest\n+   * @throws NullPointerException if {@code tableName} is {@code null}\n    */\n   public static AccessControlProtos.GrantRequest buildGrantRequest(\n       String username, TableName tableName, byte[] family, byte[] qualifier,\n@@ -67,9 +69,9 @@ private AccessControlUtil() {}\n     for (AccessControlProtos.Permission.Action a : actions) {\n       permissionBuilder.addAction(a);\n     }\n-    if (tableName == null) {\n-      throw new NullPointerException(\"TableName cannot be null\");\n-    }\n+\n+    Objects.requireNonNull(tableName, \"TableName cannot be null\");\n+\n     permissionBuilder.setTableName(ProtobufUtil.toProtoTableName(tableName));\n \n     if (family != null) {",
                "deletions": 3
            },
            {
                "sha": "94a2805b61cc4f73ae720d1c711d83fbd6e2bf80",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -35,6 +35,7 @@\n import java.util.Map;\n import java.util.Map.Entry;\n import java.util.NavigableSet;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.Callable;\n@@ -1965,14 +1966,13 @@ public static ScanMetrics toScanMetrics(final byte[] bytes) {\n   }\n \n   /**\n-   * Unwraps an exception from a protobuf service into the underlying (expected) IOException.\n-   * This method will <strong>always</strong> throw an exception.\n+   * Unwraps an exception from a protobuf service into the underlying (expected) IOException. This\n+   * method will <strong>always</strong> throw an exception.\n    * @param se the {@code ServiceException} instance to convert into an {@code IOException}\n+   * @throws NullPointerException if {@code se} is {@code null}\n    */\n   public static void toIOException(ServiceException se) throws IOException {\n-    if (se == null) {\n-      throw new NullPointerException(\"Null service exception passed!\");\n-    }\n+    Objects.requireNonNull(se, \"Service exception cannot be null\");\n \n     Throwable cause = se.getCause();\n     if (cause != null && cause instanceof IOException) {",
                "deletions": 5
            },
            {
                "sha": "13025569276f8696a3d7cb24d64bba401835a024",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java",
                "status": "modified",
                "changes": 26,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/ByteBufferWriterOutputStream.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.IOException;\n import java.io.OutputStream;\n import java.nio.ByteBuffer;\n+import java.util.Objects;\n \n import org.apache.hadoop.hbase.io.util.StreamUtils;\n import org.apache.hadoop.hbase.util.ByteBufferUtils;\n@@ -58,22 +59,21 @@ public ByteBufferWriterOutputStream(OutputStream os, int size) {\n   }\n \n   /**\n-   * Writes len bytes from the specified ByteBuffer starting at offset off to\n-   * this OutputStream. If b is null, a NullPointerException is thrown. If off\n-   * is negative or larger than the ByteBuffer then an ArrayIndexOutOfBoundsException\n-   * is thrown. If len is greater than the length of the ByteBuffer, then an\n-   * ArrayIndexOutOfBoundsException is thrown. This method does not change the\n-   * position of the ByteBuffer.\n-   *\n-   * @param b    the ByteBuffer\n-   * @param off  the start offset in the data\n-   * @param len  the number of bytes to write\n-   * @throws IOException\n-   *             if an I/O error occurs. In particular, an IOException is thrown\n-   *             if the output stream is closed.\n+   * Writes len bytes from the specified ByteBuffer starting at offset off to this OutputStream. If\n+   * off is negative or larger than the ByteBuffer then an ArrayIndexOutOfBoundsException is thrown.\n+   * If len is greater than the length of the ByteBuffer, then an ArrayIndexOutOfBoundsException is\n+   * thrown. This method does not change the position of the ByteBuffer.\n+   * @param b the ByteBuffer\n+   * @param off the start offset in the data\n+   * @param len the number of bytes to write\n+   * @throws IOException if an I/O error occurs. In particular, an IOException is thrown if the\n+   *           output stream is closed.\n+   * @throws NullPointerException if {@code b} is {@code null}\n    */\n   @Override\n   public void write(ByteBuffer b, int off, int len) throws IOException {\n+    Objects.requireNonNull(b);\n+\n     // Lazily load in the event that this version of 'write' is not invoked\n     if (this.buf == null) {\n       this.buf = new byte[this.bufSize];",
                "deletions": 13
            },
            {
                "sha": "fd512ec4cfdb4cc89b477b90d21a391a4bd9b819",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java",
                "status": "modified",
                "changes": 4,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/crypto/aes/AESDecryptor.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -83,10 +83,8 @@ public void reset() {\n   }\n \n   protected void init() {\n+    Preconditions.checkState(iv != null, \"IV is null\");\n     try {\n-      if (iv == null) {\n-        throw new NullPointerException(\"IV is null\");\n-      }\n       cipher.init(javax.crypto.Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));\n     } catch (InvalidKeyException e) {\n       throw new RuntimeException(e);",
                "deletions": 3
            },
            {
                "sha": "adb69ff45fa8347fb2658e71a588f375dcac677c",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -21,6 +21,7 @@\n import java.util.Comparator;\n import java.util.HashMap;\n import java.util.List;\n+import java.util.Objects;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n@@ -115,10 +116,8 @@ public void setConf(Configuration conf) {\n   @Override\n   public int run(String[] args) throws IOException {\n     cmdLineArgs = args;\n-    if (conf == null) {\n-      LOG.error(\"Tool configuration is not initialized\");\n-      throw new NullPointerException(\"conf\");\n-    }\n+\n+    Objects.requireNonNull(conf, \"Tool configuration is not initialized\");\n \n     CommandLine cmd;\n     List<String> argsList = new ArrayList<>(args.length);",
                "deletions": 4
            },
            {
                "sha": "6a0c1cd55c935a72c9fb27eb85a9a875b2b9db1d",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.concurrent.ConcurrentHashMap;\n import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.conf.Configuration;\n@@ -1018,19 +1019,18 @@ public static FSDataOutputStream createForWal(FileSystem fs, Path path, boolean\n   }\n \n   /**\n-   * If our FileSystem version includes the StreamCapabilities class, check if\n-   * the given stream has a particular capability.\n+   * If our FileSystem version includes the StreamCapabilities class, check if the given stream has\n+   * a particular capability.\n    * @param stream capabilities are per-stream instance, so check this one specifically. must not be\n-   *        null\n+   *          null\n    * @param capability what to look for, per Hadoop Common's FileSystem docs\n    * @return true if there are no StreamCapabilities. false if there are, but this stream doesn't\n    *         implement it. return result of asking the stream otherwise.\n+   * @throws NullPointerException if {@code stream} is {@code null}\n    */\n   public static boolean hasCapability(FSDataOutputStream stream, String capability) {\n     // be consistent whether or not StreamCapabilities is present\n-    if (stream == null) {\n-      throw new NullPointerException(\"stream parameter must not be null.\");\n-    }\n+    Objects.requireNonNull(stream, \"stream cannot be null\");\n     // If o.a.h.fs.StreamCapabilities doesn't exist, assume everyone does everything\n     // otherwise old versions of Hadoop will break.\n     boolean result = true;",
                "deletions": 6
            },
            {
                "sha": "30f5a4636103774483810ae93353f168f30fbe9b",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java",
                "status": "modified",
                "changes": 16,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ObjectPool.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -20,6 +20,7 @@\n \n import java.lang.ref.Reference;\n import java.lang.ref.ReferenceQueue;\n+import java.util.Objects;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.locks.Lock;\n@@ -75,7 +76,7 @@\n    *\n    * @param objectFactory the factory to supply new objects on demand\n    *\n-   * @throws NullPointerException if {@code objectFactory} is null\n+   * @throws NullPointerException if {@code objectFactory} is {@code null}\n    */\n   public ObjectPool(ObjectFactory<K, V> objectFactory) {\n     this(objectFactory, DEFAULT_INITIAL_CAPACITY, DEFAULT_CONCURRENCY_LEVEL);\n@@ -88,7 +89,7 @@ public ObjectPool(ObjectFactory<K, V> objectFactory) {\n    * @param objectFactory the factory to supply new objects on demand\n    * @param initialCapacity the initial capacity to keep objects in the pool\n    *\n-   * @throws NullPointerException if {@code objectFactory} is null\n+   * @throws NullPointerException if {@code objectFactory} is {@code null}\n    * @throws IllegalArgumentException if {@code initialCapacity} is negative\n    */\n   public ObjectPool(ObjectFactory<K, V> objectFactory, int initialCapacity) {\n@@ -103,7 +104,7 @@ public ObjectPool(ObjectFactory<K, V> objectFactory, int initialCapacity) {\n    * @param initialCapacity the initial capacity to keep objects in the pool\n    * @param concurrencyLevel the estimated count of concurrently accessing threads\n    *\n-   * @throws NullPointerException if {@code objectFactory} is null\n+   * @throws NullPointerException if {@code objectFactory} is {@code null}\n    * @throws IllegalArgumentException if {@code initialCapacity} is negative or\n    *    {@code concurrencyLevel} is non-positive\n    */\n@@ -112,10 +113,7 @@ public ObjectPool(\n       int initialCapacity,\n       int concurrencyLevel) {\n \n-    if (objectFactory == null) {\n-      throw new NullPointerException(\"Given object factory instance is NULL\");\n-    }\n-    this.objectFactory = objectFactory;\n+    this.objectFactory = Objects.requireNonNull(objectFactory, \"Object factory cannot be null\");\n \n     this.referenceCache =\n         new ConcurrentHashMap<K, Reference<V>>(initialCapacity, 0.75f, concurrencyLevel);\n@@ -164,10 +162,10 @@ public void purge() {\n   /**\n    * Returns a shared object associated with the given {@code key},\n    * which is identified by the {@code equals} method.\n-   * @throws NullPointerException if {@code key} is null\n+   * @throws NullPointerException if {@code key} is {@code null}\n    */\n   public V get(K key) {\n-    Reference<V> ref = referenceCache.get(key);\n+    Reference<V> ref = referenceCache.get(Objects.requireNonNull(key));\n     if (ref != null) {\n       V obj = ref.get();\n       if (obj != null) {",
                "deletions": 9
            },
            {
                "sha": "b127493fc5c287ecf2264d4e96aa86ea36e5bf30",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java",
                "status": "modified",
                "changes": 14,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/filter/FilterWrapper.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.util.List;\n+import java.util.Objects;\n \n import org.apache.hadoop.hbase.Cell;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -40,12 +41,13 @@\n final public class FilterWrapper extends Filter {\n   Filter filter = null;\n \n-  public FilterWrapper( Filter filter ) {\n-    if (null == filter) {\n-      // ensure the filter instance is not null\n-      throw new NullPointerException(\"Cannot create FilterWrapper with null Filter\");\n-    }\n-    this.filter = filter;\n+  /**\n+   * Constructor.\n+   * @param filter filter to wrap\n+   * @throws NullPointerException if {@code filter} is {@code null}\n+   */\n+  public FilterWrapper(Filter filter) {\n+    this.filter = Objects.requireNonNull(filter, \"Cannot create FilterWrapper with null Filter\");\n   }\n \n   /**",
                "deletions": 6
            },
            {
                "sha": "228b54c7ab0033a3c28dd3e056431aee9105471e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -21,6 +21,7 @@\n import java.io.DataOutput;\n import java.io.IOException;\n import java.util.ArrayDeque;\n+import java.util.Objects;\n import java.util.Queue;\n \n import org.apache.hadoop.hbase.Cell;\n@@ -163,8 +164,7 @@ private void enqueueReadyChunk(boolean closing) {\n \n   @Override\n   public void append(Cell cell) throws IOException {\n-    if (cell == null)\n-      throw new NullPointerException();\n+    Objects.requireNonNull(cell);\n \n     enqueueReadyChunk(false);\n ",
                "deletions": 2
            },
            {
                "sha": "89b3d34a7750751f753f2fd3b156506ea31e9fe4",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java",
                "status": "modified",
                "changes": 8,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileInfo.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -29,6 +29,7 @@\n import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.TreeMap;\n@@ -122,12 +123,13 @@ public HFileInfo(ReaderContext context, Configuration conf) throws IOException {\n    *          with the reserved prefix\n    * @return this file info object\n    * @throws IOException if the key or value is invalid\n+   * @throws NullPointerException if {@code key} or {@code value} is {@code null}\n    */\n   public HFileInfo append(final byte[] k, final byte[] v,\n       final boolean checkPrefix) throws IOException {\n-    if (k == null || v == null) {\n-      throw new NullPointerException(\"Key nor value may be null\");\n-    }\n+    Objects.requireNonNull(k, \"key cannot be null\");\n+    Objects.requireNonNull(v, \"value cannot be null\");\n+\n     if (checkPrefix && isReservedFileInfoKey(k)) {\n       throw new IOException(\"Keys with a \" + HFileInfo.RESERVED_PREFIX\n           + \" are reserved\");",
                "deletions": 3
            },
            {
                "sha": "99043e83caaf456a2c0c2f7e4efb260247714f2e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 11,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -43,6 +43,7 @@\n import java.util.Map.Entry;\n import java.util.NavigableMap;\n import java.util.NavigableSet;\n+import java.util.Objects;\n import java.util.Optional;\n import java.util.RandomAccess;\n import java.util.Set;\n@@ -7322,13 +7323,14 @@ public static HRegion openHRegion(final Configuration conf, final FileSystem fs,\n    * @param rsServices An interface we can request flushes against.\n    * @param reporter An interface we can report progress against.\n    * @return new HRegion\n+   * @throws NullPointerException if {@code info} is {@code null}\n    */\n   public static HRegion openHRegion(final Configuration conf, final FileSystem fs,\n       final Path rootDir, final Path tableDir, final RegionInfo info, final TableDescriptor htd,\n       final WAL wal, final RegionServerServices rsServices,\n       final CancelableProgressable reporter)\n       throws IOException {\n-    if (info == null) throw new NullPointerException(\"Passed region info is null\");\n+    Objects.requireNonNull(info, \"RegionInfo cannot be null\");\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Opening region: \" + info);\n     }\n@@ -7403,12 +7405,11 @@ protected HRegion openHRegion(final CancelableProgressable reporter)\n    * @param info Info for region to be opened.\n    * @param htd the table descriptor\n    * @return new HRegion\n+   * @throws NullPointerException if {@code info} is {@code null}\n    */\n   public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, final FileSystem fs,\n       final Path tableDir, RegionInfo info, final TableDescriptor htd) throws IOException {\n-    if (info == null) {\n-      throw new NullPointerException(\"Passed region info is null\");\n-    }\n+    Objects.requireNonNull(info, \"RegionInfo cannot be null\");\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Opening region (readOnly filesystem): \" + info);\n     }\n@@ -7426,7 +7427,7 @@ public static void warmupHRegion(final RegionInfo info,\n       final CancelableProgressable reporter)\n       throws IOException {\n \n-    if (info == null) throw new NullPointerException(\"Passed region info is null\");\n+    Objects.requireNonNull(info, \"RegionInfo cannot be null\");\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"HRegion.Warming up region: \" + info);",
                "deletions": 5
            },
            {
                "sha": "fa152c5061aeca63e7b9493dcda79d2e44e4a155",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStoreFile.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -46,7 +46,9 @@\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n+import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;\n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n \n /**\n@@ -269,9 +271,7 @@ public boolean isHFile() {\n \n   @Override\n   public boolean isMajorCompactionResult() {\n-    if (this.majorCompaction == null) {\n-      throw new NullPointerException(\"This has not been set yet\");\n-    }\n+    Preconditions.checkState(this.majorCompaction != null, \"Major compation has not been set yet\");\n     return this.majorCompaction.get();\n   }\n ",
                "deletions": 3
            },
            {
                "sha": "7719b53230a4329245900892c6a286a85c79f4a2",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java",
                "status": "modified",
                "changes": 10,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -135,7 +135,7 @@\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-\n+import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;\n import org.apache.hbase.thirdparty.com.google.common.collect.ArrayListMultimap;\n import org.apache.hbase.thirdparty.com.google.common.collect.ImmutableSet;\n import org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap;\n@@ -716,11 +716,9 @@ public void start(CoprocessorEnvironment env) throws IOException {\n       }\n     }\n \n-    if (zkPermissionWatcher == null) {\n-      throw new NullPointerException(\"ZKPermissionWatcher is null\");\n-    } else if (accessChecker == null) {\n-      throw new NullPointerException(\"AccessChecker is null\");\n-    }\n+    Preconditions.checkState(zkPermissionWatcher != null, \"ZKPermissionWatcher is null\");\n+    Preconditions.checkState(accessChecker != null, \"AccessChecker is null\");\n+\n     // set the user-provider.\n     this.userProvider = UserProvider.instantiate(env.getConfiguration());\n     tableAcls = new MapMaker().weakValues().makeMap();",
                "deletions": 6
            },
            {
                "sha": "efff41e11c865cca24f4185bda27788125ca4b5c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java",
                "blob_url": "https://github.com/apache/hbase/blob/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java",
                "raw_url": "https://github.com/apache/hbase/raw/33bedf8d4d7ec320c5cc01c1c031035a1523f973/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.java?ref=33bedf8d4d7ec320c5cc01c1c031035a1523f973",
                "patch": "@@ -25,6 +25,7 @@\n import java.util.Collection;\n import java.util.Comparator;\n import java.util.Iterator;\n+import java.util.Objects;\n import java.util.AbstractQueue;\n \n import org.apache.yetus.audience.InterfaceAudience;\n@@ -157,7 +158,7 @@ public BoundedPriorityBlockingQueue(int capacity,\n \n   @Override\n   public boolean offer(E e) {\n-    if (e == null) throw new NullPointerException();\n+    Objects.requireNonNull(e);\n \n     lock.lock();\n     try {\n@@ -174,7 +175,7 @@ public boolean offer(E e) {\n \n   @Override\n   public void put(E e) throws InterruptedException {\n-    if (e == null) throw new NullPointerException();\n+    Objects.requireNonNull(e);\n \n     lock.lock();\n     try {\n@@ -191,7 +192,7 @@ public void put(E e) throws InterruptedException {\n   @Override\n   public boolean offer(E e, long timeout, TimeUnit unit)\n       throws InterruptedException {\n-    if (e == null) throw new NullPointerException();\n+    Objects.requireNonNull(e);\n     long nanos = unit.toNanos(timeout);\n \n     lock.lockInterruptibly();\n@@ -321,8 +322,7 @@ public int drainTo(Collection<? super E> c) {\n \n   @Override\n   public int drainTo(Collection<? super E> c, int maxElements) {\n-    if (c == null)\n-        throw new NullPointerException();\n+    Objects.requireNonNull(c);\n     if (c == this)\n         throw new IllegalArgumentException();\n     if (maxElements <= 0)",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "fix 500/NPE of region.jsp (#1033)\n\nSigned-off-by: Wellington Chevreuil <wchevreuil@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/d60ce17c1765a445e944738f49953579bdf0bba6",
        "parent": "https://github.com/apache/hbase/commit/c1ba3bfa1227dc7202dc0d278fa2667e7523b5d9",
        "bug_id": "hbase_2",
        "file": [
            {
                "sha": "382783ea4bad5f6774de5878ba559838ac40c329",
                "filename": "hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/d60ce17c1765a445e944738f49953579bdf0bba6/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/d60ce17c1765a445e944738f49953579bdf0bba6/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "status": "modified",
                "changes": 9,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp?ref=d60ce17c1765a445e944738f49953579bdf0bba6",
                "patch": "@@ -32,8 +32,13 @@\n   HRegionServer rs = (HRegionServer) getServletContext().getAttribute(HRegionServer.REGIONSERVER);\n \n   Region region = rs.getRegion(regionName);\n-  String displayName = RegionInfoDisplay.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n-    rs.getConfiguration());\n+  String displayName;\n+  if (region != null) {\n+    displayName = RegionInfoDisplay.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n+            rs.getConfiguration());\n+  } else {\n+    displayName = \"region {\" + regionName + \"} is not currently online on this region server\";\n+  }\n   pageContext.setAttribute(\"pageTitle\", \"HBase RegionServer: \" + rs.getServerName());\n %>\n <jsp:include page=\"header.jsp\">",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23677 fix 500/NPE of region.jsp (#1033)\n\nSigned-off-by: Wellington Chevreuil <wchevreuil@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/a44f3b50e4899f77a8260d5161502b313000e5b6",
        "parent": "https://github.com/apache/hbase/commit/ceaeece2c5c1edf5719141fd4e1dc2129b21a77b",
        "bug_id": "hbase_3",
        "file": [
            {
                "sha": "382783ea4bad5f6774de5878ba559838ac40c329",
                "filename": "hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/a44f3b50e4899f77a8260d5161502b313000e5b6/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/a44f3b50e4899f77a8260d5161502b313000e5b6/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "status": "modified",
                "changes": 9,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp?ref=a44f3b50e4899f77a8260d5161502b313000e5b6",
                "patch": "@@ -32,8 +32,13 @@\n   HRegionServer rs = (HRegionServer) getServletContext().getAttribute(HRegionServer.REGIONSERVER);\n \n   Region region = rs.getRegion(regionName);\n-  String displayName = RegionInfoDisplay.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n-    rs.getConfiguration());\n+  String displayName;\n+  if (region != null) {\n+    displayName = RegionInfoDisplay.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n+            rs.getConfiguration());\n+  } else {\n+    displayName = \"region {\" + regionName + \"} is not currently online on this region server\";\n+  }\n   pageContext.setAttribute(\"pageTitle\", \"HBase RegionServer: \" + rs.getServerName());\n %>\n <jsp:include page=\"header.jsp\">",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"fix 500/NPE of region.jsp (#1033)\"\n\nMissed the jira number on that commit message. Will re-apply it with the jira number.\n\nThis reverts commit d60ce17c1765a445e944738f49953579bdf0bba6.",
        "commit": "https://github.com/apache/hbase/commit/ceaeece2c5c1edf5719141fd4e1dc2129b21a77b",
        "parent": "https://github.com/apache/hbase/commit/fd05aabf02e8385512eb6f62b362a8554153b770",
        "bug_id": "hbase_4",
        "file": [
            {
                "sha": "8183c69bcb5d6ccd23dc1a15ee0d36396956bb9e",
                "filename": "hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/ceaeece2c5c1edf5719141fd4e1dc2129b21a77b/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/ceaeece2c5c1edf5719141fd4e1dc2129b21a77b/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "status": "modified",
                "changes": 9,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp?ref=ceaeece2c5c1edf5719141fd4e1dc2129b21a77b",
                "patch": "@@ -32,13 +32,8 @@\n   HRegionServer rs = (HRegionServer) getServletContext().getAttribute(HRegionServer.REGIONSERVER);\n \n   Region region = rs.getRegion(regionName);\n-  String displayName;\n-  if (region != null) {\n-    displayName = RegionInfoDisplay.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n-            rs.getConfiguration());\n-  } else {\n-    displayName = \"region {\" + regionName + \"} is not currently online on this region server\";\n-  }\n+  String displayName = RegionInfoDisplay.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n+    rs.getConfiguration());\n   pageContext.setAttribute(\"pageTitle\", \"HBase RegionServer: \" + rs.getServerName());\n %>\n <jsp:include page=\"header.jsp\">",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23376 NPE happens while replica region is moving (#906)\n\nSigned-off-by: Duo Zhang <zhangduo@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/270eb9886efd351d4346bb9de0f4fc3c17cf1910",
        "parent": "https://github.com/apache/hbase/commit/80ba354e2e05899d00da59c07817f73bbbb45585",
        "bug_id": "hbase_5",
        "file": [
            {
                "sha": "d36ffea9b6e8f29348947f7c51fd47a452c87ff5",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "blob_url": "https://github.com/apache/hbase/blob/270eb9886efd351d4346bb9de0f4fc3c17cf1910/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "raw_url": "https://github.com/apache/hbase/raw/270eb9886efd351d4346bb9de0f4fc3c17cf1910/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java?ref=270eb9886efd351d4346bb9de0f4fc3c17cf1910",
                "patch": "@@ -237,7 +237,7 @@ private boolean isEqual(RegionLocations locs1, RegionLocations locs2) {\n   // which prevents us being added. The upper layer can use this value to complete pending requests.\n   private RegionLocations addToCache(TableCache tableCache, RegionLocations locs) {\n     LOG.trace(\"Try adding {} to cache\", locs);\n-    byte[] startKey = locs.getDefaultRegionLocation().getRegion().getStartKey();\n+    byte[] startKey = locs.getRegionLocation().getRegion().getStartKey();\n     for (;;) {\n       RegionLocations oldLocs = tableCache.cache.putIfAbsent(startKey, locs);\n       if (oldLocs == null) {",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23342 : Handle NPE while closing compressingStream (#877)\n\nSigned-off-by Anoop Sam John <anoopsamjohn@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/142997c04e796784b3f0aa40818aa4b7c59664d9",
        "parent": "https://github.com/apache/hbase/commit/d69ecf6092c08323f49d8bb4131312b8f4981aa9",
        "bug_id": "hbase_6",
        "file": [
            {
                "sha": "afaf1976dc6d97878621254fb35d2a7d043de7ff",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java",
                "blob_url": "https://github.com/apache/hbase/blob/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java",
                "raw_url": "https://github.com/apache/hbase/raw/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java?ref=142997c04e796784b3f0aa40818aa4b7c59664d9",
                "patch": "@@ -210,7 +210,9 @@ public static int getCompressedSize(Algorithm algo, Compressor compressor,\n     } finally {\n       nullOutputStream.close();\n       compressedStream.close();\n-      compressingStream.close();\n+      if (compressingStream != null) {\n+        compressingStream.close();\n+      }\n     }\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "e2cfa45d6b7d20e5e96cf0d9657de703d41d31ac",
                "filename": "hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java",
                "blob_url": "https://github.com/apache/hbase/blob/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java",
                "raw_url": "https://github.com/apache/hbase/raw/142997c04e796784b3f0aa40818aa4b7c59664d9/hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java",
                "status": "added",
                "changes": 65,
                "additions": 65,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedDataBlock.java?ref=142997c04e796784b3f0aa40818aa4b7c59664d9",
                "patch": "@@ -0,0 +1,65 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.io.encoding;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.io.compress.Compression.Algorithm;\n+import org.apache.hadoop.hbase.testclassification.MiscTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.mockito.Mockito;\n+\n+/**\n+ * Test for EncodedDataBlock\n+ */\n+@Category({MiscTests.class, SmallTests.class})\n+public class TestEncodedDataBlock {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestEncodedDataBlock.class);\n+\n+  private Algorithm algo;\n+  private static final byte[] INPUT_BYTES = new byte[]{0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0,\n+    1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0, 0, 1, 0, 0, 1, 2, 3, 0};\n+\n+  @Before\n+  public void setUp() throws IOException {\n+    algo = Mockito.mock(Algorithm.class);\n+  }\n+\n+  @Test\n+  public void testGetCompressedSize() throws Exception {\n+    Mockito.when(algo.createCompressionStream(Mockito.any(), Mockito.any(), Mockito.anyInt()))\n+      .thenThrow(IOException.class);\n+    try {\n+      EncodedDataBlock.getCompressedSize(algo, null, INPUT_BYTES, 0, 0);\n+      throw new RuntimeException(\"Should not reach here\");\n+    } catch (IOException e) {\n+      Mockito.verify(algo, Mockito.times(1)).createCompressionStream(Mockito.any(),\n+        Mockito.any(), Mockito.anyInt());\n+    }\n+  }\n+\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23159 HStore#getStorefilesSize may throw NPE",
        "commit": "https://github.com/apache/hbase/commit/9e628b715d68c180a73f89b82ef8203fd66ef39d",
        "parent": "https://github.com/apache/hbase/commit/73d69c6157515c28479fc78f224c8065d0c00abc",
        "bug_id": "hbase_7",
        "file": [
            {
                "sha": "10bb030babe3c437ce8bc4474acd410afb78e865",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/9e628b715d68c180a73f89b82ef8203fd66ef39d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/9e628b715d68c180a73f89b82ef8203fd66ef39d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "status": "modified",
                "changes": 35,
                "additions": 24,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=9e628b715d68c180a73f89b82ef8203fd66ef39d",
                "patch": "@@ -2177,24 +2177,37 @@ public long getHFilesSize() {\n   }\n \n   private long getTotalUmcompressedBytes(List<HStoreFile> files) {\n-    return files.stream().filter(f -> f != null && f.getReader() != null)\n-        .mapToLong(f -> f.getReader().getTotalUncompressedBytes()).sum();\n+    return files.stream().filter(f -> f != null).mapToLong(f -> {\n+      StoreFileReader reader = f.getReader();\n+      if (reader == null) {\n+        return 0;\n+      } else {\n+        return reader.getTotalUncompressedBytes();\n+      }\n+    }).sum();\n   }\n \n   private long getStorefilesSize(Collection<HStoreFile> files, Predicate<HStoreFile> predicate) {\n     return files.stream().filter(f -> f != null && f.getReader() != null).filter(predicate)\n-        .mapToLong(f -> f.getReader().length()).sum();\n+        .mapToLong(f -> {\n+          StoreFileReader reader = f.getReader();\n+          if (reader == null) {\n+            return 0;\n+          } else {\n+            return reader.length();\n+          }\n+        }).sum();\n   }\n \n   private long getStoreFileFieldSize(ToLongFunction<StoreFileReader> f) {\n-    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n-      if (sf.getReader() == null) {\n-        LOG.warn(\"StoreFile {} has a null Reader\", sf);\n-        return false;\n-      } else {\n-        return true;\n-      }\n-    }).map(HStoreFile::getReader).mapToLong(f).sum();\n+    return this.storeEngine.getStoreFileManager().getStorefiles().stream()\n+        .map(HStoreFile::getReader).filter(reader -> {\n+          if (reader == null) {\n+            return false;\n+          } else {\n+            return true;\n+          }\n+        }).mapToLong(f).sum();\n   }\n \n   @Override",
                "deletions": 11
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23263 NPE in Quotas.jsp (#800)\n\nSigned-off-by: Guangxu Cheng <guangxucheng@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/264d3e3b168169be88207478f51b23f0ba0c4e7a",
        "parent": "https://github.com/apache/hbase/commit/c348e642d969ff633920ade19a48f0d19f5b6fa2",
        "bug_id": "hbase_8",
        "file": [
            {
                "sha": "780a8d4b360599cf533ad2b40cc716e8502b56ad",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/quotas.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/264d3e3b168169be88207478f51b23f0ba0c4e7a/hbase-server/src/main/resources/hbase-webapps/master/quotas.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/264d3e3b168169be88207478f51b23f0ba0c4e7a/hbase-server/src/main/resources/hbase-webapps/master/quotas.jsp",
                "status": "modified",
                "changes": 33,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/quotas.jsp?ref=264d3e3b168169be88207478f51b23f0ba0c4e7a",
                "patch": "@@ -23,6 +23,7 @@\n   import=\"java.util.List\"\n   import=\"org.apache.hadoop.conf.Configuration\"\n   import=\"org.apache.hadoop.hbase.master.HMaster\"\n+  import=\"org.apache.hadoop.hbase.quotas.MasterQuotaManager\"\n   import=\"org.apache.hadoop.hbase.quotas.QuotaRetriever\"\n   import=\"org.apache.hadoop.hbase.quotas.QuotaSettings\"\n   import=\"org.apache.hadoop.hbase.quotas.ThrottleSettings\"\n@@ -31,20 +32,24 @@\n   HMaster master = (HMaster) getServletContext().getAttribute(HMaster.MASTER);\n   Configuration conf = master.getConfiguration();\n   pageContext.setAttribute(\"pageTitle\", \"HBase Master Quotas: \" + master.getServerName());\n-  boolean exceedThrottleQuotaEnabled = master.getMasterQuotaManager().isExceedThrottleQuotaEnabled();\n   List<ThrottleSettings> regionServerThrottles = new ArrayList<>();\n   List<ThrottleSettings> namespaceThrottles = new ArrayList<>();\n   List<ThrottleSettings> userThrottles = new ArrayList<>();\n-  try (QuotaRetriever scanner = QuotaRetriever.open(conf, null)) {\n-    for (QuotaSettings quota : scanner) {\n-      if (quota instanceof ThrottleSettings) {\n-        ThrottleSettings throttle = (ThrottleSettings) quota;\n-        if (throttle.getUserName() != null) {\n-          userThrottles.add(throttle);\n-        } else if (throttle.getNamespace() != null) {\n-          namespaceThrottles.add(throttle);\n-        } else if (throttle.getRegionServer() != null) {\n-          regionServerThrottles.add(throttle);\n+  MasterQuotaManager quotaManager = master.getMasterQuotaManager();\n+  boolean exceedThrottleQuotaEnabled = false;\n+  if (quotaManager != null) {\n+    exceedThrottleQuotaEnabled = quotaManager.isExceedThrottleQuotaEnabled();\n+    try (QuotaRetriever scanner = QuotaRetriever.open(conf, null)) {\n+      for (QuotaSettings quota : scanner) {\n+        if (quota instanceof ThrottleSettings) {\n+          ThrottleSettings throttle = (ThrottleSettings) quota;\n+          if (throttle.getUserName() != null) {\n+            userThrottles.add(throttle);\n+          } else if (throttle.getNamespace() != null) {\n+            namespaceThrottles.add(throttle);\n+          } else if (throttle.getRegionServer() != null) {\n+            regionServerThrottles.add(throttle);\n+          }\n         }\n       }\n     }\n@@ -61,13 +66,15 @@\n   </div>\n </div>\n \n+<%if (quotaManager != null) {%>\n+\n <div class=\"container-fluid content\">\n   <div class=\"row\">\n     <div class=\"page-header\">\n       <h2>Rpc Throttle Enabled</h2>\n     </div>\n   </div>\n-  <%if (master.getMasterQuotaManager().isRpcThrottleEnabled()) {%>\n+  <%if (quotaManager.isRpcThrottleEnabled()) {%>\n   <div class=\"alert alert-success\">\n     Rpc throttle is enabled.\n   </div>\n@@ -201,4 +208,6 @@\n   <% } %>\n </div>\n \n+<% } %>\n+\n <jsp:include page=\"footer.jsp\" />",
                "deletions": 12
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23155 May NPE when concurrent AsyncNonMetaRegionLocator#updateCachedLocationOnError (#718)",
        "commit": "https://github.com/apache/hbase/commit/7924ba39e7ce573369deda84f55e2a0e6ecb4872",
        "parent": "https://github.com/apache/hbase/commit/6aec958d66fe88be8cbc175f52162645d22e996f",
        "bug_id": "hbase_9",
        "file": [
            {
                "sha": "0cdfcdd083c1dd424b922e503bd3183eb39855f8",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "blob_url": "https://github.com/apache/hbase/blob/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "raw_url": "https://github.com/apache/hbase/raw/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncNonMetaRegionLocator.java?ref=7924ba39e7ce573369deda84f55e2a0e6ecb4872",
                "patch": "@@ -570,6 +570,9 @@ private void removeLocationFromCache(HRegionLocation loc) {\n     byte[] startKey = loc.getRegion().getStartKey();\n     for (;;) {\n       RegionLocations oldLocs = tableCache.cache.get(startKey);\n+      if (oldLocs == null) {\n+        return;\n+      }\n       HRegionLocation oldLoc = oldLocs.getRegionLocation(loc.getRegion().getReplicaId());\n       if (!canUpdateOnError(loc, oldLoc)) {\n         return;",
                "deletions": 0
            },
            {
                "sha": "d1ed5b7cda88c789024da3e952e27c16c9e2c0c2",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java",
                "blob_url": "https://github.com/apache/hbase/blob/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java",
                "raw_url": "https://github.com/apache/hbase/raw/7924ba39e7ce573369deda84f55e2a0e6ecb4872/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java",
                "status": "modified",
                "changes": 10,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncNonMetaRegionLocator.java?ref=7924ba39e7ce573369deda84f55e2a0e6ecb4872",
                "patch": "@@ -399,4 +399,14 @@ public void testLocateBeforeInOnlyRegion() throws IOException, InterruptedExcept\n     assertArrayEquals(loc.getRegion().getStartKey(), EMPTY_START_ROW);\n     assertArrayEquals(loc.getRegion().getEndKey(), EMPTY_END_ROW);\n   }\n+\n+  @Test\n+  public void testConcurrentUpdateCachedLocationOnError() throws Exception {\n+    createSingleRegionTable();\n+    HRegionLocation loc =\n+        getDefaultRegionLocation(TABLE_NAME, EMPTY_START_ROW, RegionLocateType.CURRENT, false)\n+            .get();\n+    IntStream.range(0, 100).parallel()\n+        .forEach(i -> LOCATOR.updateCachedLocationOnError(loc, new NotServingRegionException()));\n+  }\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23244 NPEs running Canary (#784)\n\nAddendum to fix findbugs complaint.",
        "commit": "https://github.com/apache/hbase/commit/3ccfd50bd937571aeed3033561b7ca52c967f105",
        "parent": "https://github.com/apache/hbase/commit/c58e80fbe6db6426e652ec363149b92f9e33fbb0",
        "bug_id": "hbase_10",
        "file": [
            {
                "sha": "4f59cf3328463517244f41c3ee0ff38fb3c3a6d7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/3ccfd50bd937571aeed3033561b7ca52c967f105/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/3ccfd50bd937571aeed3033561b7ca52c967f105/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=3ccfd50bd937571aeed3033561b7ca52c967f105",
                "patch": "@@ -492,7 +492,7 @@ public Void read() {\n           sink.publishReadTiming(serverName, region, column, stopWatch.getTime());\n         } catch (Exception e) {\n           sink.publishReadFailure(serverName, region, column, e);\n-          sink.updateReadFailures(region == null? \"NULL\": region.getRegionNameAsString(),\n+          sink.updateReadFailures(region.getRegionNameAsString(),\n               serverName == null? \"NULL\": serverName.getHostname());\n         } finally {\n           if (rs != null) {",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23244 NPEs running Canary (#784)\n\nSigned-off-by: Viraj Jasani <virajjasani007@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/c58e80fbe6db6426e652ec363149b92f9e33fbb0",
        "parent": "https://github.com/apache/hbase/commit/b8a4504a2609d6d6d26e0b839a368912b3276feb",
        "bug_id": "hbase_11",
        "file": [
            {
                "sha": "a967dab81750d974a570f908a02de69bd8fc4666",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/c58e80fbe6db6426e652ec363149b92f9e33fbb0/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/c58e80fbe6db6426e652ec363149b92f9e33fbb0/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "status": "modified",
                "changes": 11,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=c58e80fbe6db6426e652ec363149b92f9e33fbb0",
                "patch": "@@ -492,7 +492,8 @@ public Void read() {\n           sink.publishReadTiming(serverName, region, column, stopWatch.getTime());\n         } catch (Exception e) {\n           sink.publishReadFailure(serverName, region, column, e);\n-          sink.updateReadFailures(region.getRegionNameAsString(), serverName.getHostname());\n+          sink.updateReadFailures(region == null? \"NULL\": region.getRegionNameAsString(),\n+              serverName == null? \"NULL\": serverName.getHostname());\n         } finally {\n           if (rs != null) {\n             rs.close();\n@@ -1579,6 +1580,10 @@ private void createWriteTable(int numberOfServers) throws IOException {\n       try (RegionLocator regionLocator =\n                admin.getConnection().getRegionLocator(tableDesc.getTableName())) {\n         for (HRegionLocation location: regionLocator.getAllRegionLocations()) {\n+          if (location == null) {\n+            LOG.warn(\"Null location\");\n+            continue;\n+          }\n           ServerName rs = location.getServerName();\n           RegionInfo region = location.getRegion();\n           tasks.add(new RegionTask(admin.getConnection(), region, rs, (RegionStdOutSink)sink,\n@@ -1795,6 +1800,10 @@ private void monitorRegionServers(Map<String, List<RegionInfo>> rsAndRMap, Regio\n           try (RegionLocator regionLocator =\n                    this.admin.getConnection().getRegionLocator(tableDesc.getTableName())) {\n             for (HRegionLocation location : regionLocator.getAllRegionLocations()) {\n+              if (location == null) {\n+                LOG.warn(\"Null location\");\n+                continue;\n+              }\n               ServerName rs = location.getServerName();\n               String rsName = rs.getHostname();\n               RegionInfo r = location.getRegion();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22441 BucketCache NullPointerException in cacheBlock",
        "commit": "https://github.com/apache/hbase/commit/5dcfe5f8d85198bac81793c42d3ebceb47154bd7",
        "parent": "https://github.com/apache/hbase/commit/998b8416cabccd3743a4385b219eb1593746da5d",
        "bug_id": "hbase_12",
        "file": [
            {
                "sha": "0cb2bd1acb4622f70f0b8c75acb2f784bea58c81",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/5dcfe5f8d85198bac81793c42d3ebceb47154bd7/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/5dcfe5f8d85198bac81793c42d3ebceb47154bd7/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.java?ref=5dcfe5f8d85198bac81793c42d3ebceb47154bd7",
                "patch": "@@ -229,6 +229,10 @@ public static int validateBlockAddition(Cacheable existing, Cacheable newBlock,\n   public static boolean shouldReplaceExistingCacheBlock(BlockCache blockCache,\n       BlockCacheKey cacheKey, Cacheable newBlock) {\n     Cacheable existingBlock = blockCache.getBlock(cacheKey, false, false, false);\n+    if (null == existingBlock) {\n+      // Not exist now.\n+      return true;\n+    }\n     try {\n       int comparison = BlockCacheUtil.validateBlockAddition(existingBlock, newBlock, cacheKey);\n       if (comparison < 0) {",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23203 NPE in RSGroup info (#747)\n\nSigned-off-by: Duo Zhang <zhangduo@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
        "parent": "https://github.com/apache/hbase/commit/65ee17086a04f61c9cf407596fdc7efd9690d801",
        "bug_id": "hbase_13",
        "file": [
            {
                "sha": "5419c169830538b768288340d49a76ccc95230cf",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java",
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java",
                "status": "modified",
                "changes": 8,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "patch": "@@ -124,6 +124,14 @@\n    */\n   List<TableDescriptor> listTableDescriptors() throws IOException;\n \n+  /**\n+   * List all userspace tables and whether or not include system tables.\n+   *\n+   * @return a list of TableDescriptors\n+   * @throws IOException if a remote or network exception occurs\n+   */\n+  List<TableDescriptor> listTableDescriptors(boolean includeSysTables) throws IOException;\n+\n   /**\n    * List all the userspace tables that match the given pattern.\n    *",
                "deletions": 0
            },
            {
                "sha": "f1f5b2ae2d0d3ad95a912bda745285ddd25f2520",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AdminOverAsyncAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "patch": "@@ -131,6 +131,12 @@ public boolean tableExists(TableName tableName) throws IOException {\n     return get(admin.listTableDescriptors());\n   }\n \n+  @Override\n+  public List<TableDescriptor> listTableDescriptors(boolean includeSysTables)\n+      throws IOException {\n+    return get(admin.listTableDescriptors(includeSysTables));\n+  }\n+\n   @Override\n   public List<TableDescriptor> listTableDescriptors(Pattern pattern, boolean includeSysTables)\n       throws IOException {",
                "deletions": 0
            },
            {
                "sha": "8c6ec0de8a35404e6bfb57aef16e5baa11764e76",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "patch": "@@ -493,15 +493,17 @@ public void run(PRESP resp) {\n   public CompletableFuture<List<TableDescriptor>> listTableDescriptors(Pattern pattern,\n       boolean includeSysTables) {\n     Preconditions.checkNotNull(pattern,\n-      \"pattern is null. If you don't specify a pattern, use listTables(boolean) instead\");\n+      \"pattern is null. If you don't specify a pattern, \"\n+          + \"use listTableDescriptors(boolean) instead\");\n     return getTableDescriptors(RequestConverter.buildGetTableDescriptorsRequest(pattern,\n       includeSysTables));\n   }\n \n   @Override\n   public CompletableFuture<List<TableDescriptor>> listTableDescriptors(List<TableName> tableNames) {\n     Preconditions.checkNotNull(tableNames,\n-      \"tableNames is null. If you don't specify tableNames, \" + \"use listTables(boolean) instead\");\n+      \"tableNames is null. If you don't specify tableNames, \"\n+          + \"use listTableDescriptors(boolean) instead\");\n     if (tableNames.isEmpty()) {\n       return CompletableFuture.completedFuture(Collections.emptyList());\n     }",
                "deletions": 2
            },
            {
                "sha": "86260347d17e488bcbc66ee8c121038ac4e30523",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/rsgroup.jsp?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "patch": "@@ -437,7 +437,7 @@\n     <% if (rsGroupTables != null && rsGroupTables.size() > 0) {\n         List<TableDescriptor> tables;\n         try (Admin admin = master.getConnection().getAdmin()) {\n-            tables = master.isInitialized() ? admin.listTableDescriptors((Pattern)null, true) : null;\n+            tables = master.isInitialized() ? admin.listTableDescriptors(true) : null;\n         }\n          Map<TableName, HTableDescriptor> tableDescriptors\n             = tables.stream().collect(Collectors.toMap(TableDescriptor::getTableName, p -> new HTableDescriptor(p)));",
                "deletions": 1
            },
            {
                "sha": "d1ba1a7107574449f2c7fd5c8dd9f9fea13b0edf",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "status": "modified",
                "changes": 12,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "patch": "@@ -57,6 +57,18 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(TestAdmin.class);\n \n+  @Test\n+  public void testListTableDescriptors() throws IOException{\n+    TableDescriptor metaTableDescriptor =  TEST_UTIL.getAdmin().\n+        getDescriptor(TableName.META_TABLE_NAME);\n+    List<TableDescriptor> tableDescriptors = TEST_UTIL.getAdmin().\n+        listTableDescriptors(true);\n+    assertTrue(tableDescriptors.contains(metaTableDescriptor));\n+    tableDescriptors = TEST_UTIL.getAdmin().\n+        listTableDescriptors(false);\n+    assertFalse(tableDescriptors.contains(metaTableDescriptor));\n+  }\n+\n   @Test\n   public void testCreateTable() throws IOException {\n     List<TableDescriptor> tables = ADMIN.listTableDescriptors();",
                "deletions": 0
            },
            {
                "sha": "f3fdde72667aa1e37bb5e7aa2d87f5cb1877e0cf",
                "filename": "hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/8f92a14cd17a28bbc9888941e5e2b6ba55af9319/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/client/ThriftAdmin.java?ref=8f92a14cd17a28bbc9888941e5e2b6ba55af9319",
                "patch": "@@ -139,6 +139,11 @@ public Connection getConnection() {\n     return listTableDescriptors((Pattern) null);\n   }\n \n+  @Override\n+  public List<TableDescriptor> listTableDescriptors(boolean includeSysTables) throws IOException {\n+    return listTableDescriptors(null, includeSysTables);\n+  }\n+\n   @Override\n   public List<TableDescriptor> listTableDescriptors(Pattern pattern) throws IOException {\n     return listTableDescriptors(pattern, false);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22941 Addendum fix NPE in UT",
        "commit": "https://github.com/apache/hbase/commit/e890776fe04697a32a71ac6a4a5dada9bc549d80",
        "parent": "https://github.com/apache/hbase/commit/49718b8b46cd9e06aeef3f74647b32c6df99f7ae",
        "bug_id": "hbase_14",
        "file": [
            {
                "sha": "04389bb66833790095447f85e99e9554f0be5e41",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java",
                "blob_url": "https://github.com/apache/hbase/blob/e890776fe04697a32a71ac6a4a5dada9bc549d80/hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java",
                "raw_url": "https://github.com/apache/hbase/raw/e890776fe04697a32a71ac6a4a5dada9bc549d80/hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java",
                "status": "modified",
                "changes": 10,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestSplitMerge.java?ref=e890776fe04697a32a71ac6a4a5dada9bc549d80",
                "patch": "@@ -109,7 +109,6 @@ public String explainFailure() throws Exception {\n \n   @Test\n   public void testMergeRegionOrder() throws Exception {\n-\n     int regionCount= 20;\n \n     TableName tableName = TableName.valueOf(\"MergeRegionOrder\");\n@@ -143,14 +142,13 @@ public void testMergeRegionOrder() throws Exception {\n     RegionInfo mergedRegion = mergedRegions.get(0);\n \n     List<RegionInfo> mergeParentRegions = MetaTableAccessor.getMergeRegions(UTIL.getConnection(),\n-      mergedRegion.getEncodedNameAsBytes());\n+      mergedRegion.getRegionName());\n \n     assertEquals(mergeParentRegions.size(), regionCount);\n \n-    for (int c = 0; c < regionCount-1; c++) {\n-      assertTrue(Bytes.compareTo(\n-        mergeParentRegions.get(c).getStartKey(),\n-        mergeParentRegions.get(c+1).getStartKey()) < 0);\n+    for (int c = 0; c < regionCount - 1; c++) {\n+      assertTrue(Bytes.compareTo(mergeParentRegions.get(c).getStartKey(),\n+        mergeParentRegions.get(c + 1).getStartKey()) < 0);\n     }\n   }\n }",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22453 A NullPointerException could be thrown (#272)",
        "commit": "https://github.com/apache/hbase/commit/2e9087bfb737c4a1a028afff13a6e15fc9c88dc6",
        "parent": "https://github.com/apache/hbase/commit/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
        "bug_id": "hbase_15",
        "file": [
            {
                "sha": "8de7118fc585b8c5a02f4b45f641fa45ecd002a5",
                "filename": "hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/2e9087bfb737c4a1a028afff13a6e15fc9c88dc6/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/2e9087bfb737c4a1a028afff13a6e15fc9c88dc6/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/util/RestoreTool.java?ref=2e9087bfb737c4a1a028afff13a6e15fc9c88dc6",
                "patch": "@@ -169,6 +169,9 @@ public void incrementalRestoreTable(Connection conn, Path tableBackupPath, Path[\n       for (int i = 0; i < tableNames.length; i++) {\n         TableName tableName = tableNames[i];\n         TableDescriptor tableDescriptor = getTableDescriptor(fileSys, tableName, incrBackupId);\n+        if (tableDescriptor == null) {\n+          throw new IOException(\"Can't find \" + tableName + \"'s descriptor.\");\n+        }\n         LOG.debug(\"Found descriptor \" + tableDescriptor + \" through \" + incrBackupId);\n \n         TableName newTableName = newTableNames[i];\n@@ -456,7 +459,7 @@ private void createAndRestoreTable(Connection conn, TableName tableName, TableNa\n \n   /**\n    * Prepare the table for bulkload, most codes copied from\n-   * {@link LoadIncrementalHFiles#createTable(TableName, String, Admin)}\n+   * {@link LoadIncrementalHFiles#createTable(TableName, Path, Admin)}\n    * @param conn connection\n    * @param tableBackupPath path\n    * @param tableName table name",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22553 NPE in RegionReplicaReplicationEndpoint",
        "commit": "https://github.com/apache/hbase/commit/621dc88c7940ac8ab3719872e4cc1643dcf87da8",
        "parent": "https://github.com/apache/hbase/commit/6278c98f5d0b19ff830368c47204232760e2b4ea",
        "bug_id": "hbase_16",
        "file": [
            {
                "sha": "2c3b19b6a45c3aa8820b49bc600b04efd7a37ef3",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "blob_url": "https://github.com/apache/hbase/blob/621dc88c7940ac8ab3719872e4cc1643dcf87da8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "raw_url": "https://github.com/apache/hbase/raw/621dc88c7940ac8ab3719872e4cc1643dcf87da8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "status": "modified",
                "changes": 51,
                "additions": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java?ref=621dc88c7940ac8ab3719872e4cc1643dcf87da8",
                "patch": "@@ -32,12 +32,15 @@\n import java.util.concurrent.atomic.AtomicReference;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.CellUtil;\n+import org.apache.hadoop.hbase.HBaseIOException;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.RegionLocations;\n import org.apache.hadoop.hbase.TableDescriptors;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.client.AsyncClusterConnection;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.RegionReplicaUtil;\n import org.apache.hadoop.hbase.client.TableDescriptor;\n import org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint;\n import org.apache.hadoop.hbase.replication.WALEntryFilter;\n@@ -162,9 +165,9 @@ private void getRegionLocations(CompletableFuture<RegionLocations> future,\n           return;\n         }\n         // check if the number of region replicas is correct, and also the primary region name\n-        // matches, and also there is no null elements in the returned RegionLocations\n+        // matches.\n         if (locs.size() == tableDesc.getRegionReplication() &&\n-          locs.size() == locs.numNonNullElements() &&\n+          locs.getDefaultRegionLocation() != null &&\n           Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n             encodedRegionName)) {\n           future.complete(locs);\n@@ -182,16 +185,16 @@ private void replicate(CompletableFuture<Long> future, RegionLocations locs,\n       future.complete(Long.valueOf(entries.size()));\n       return;\n     }\n-    if (!Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n-      encodedRegionName)) {\n+    RegionInfo defaultReplica = locs.getDefaultRegionLocation().getRegion();\n+    if (!Bytes.equals(defaultReplica.getEncodedNameAsBytes(), encodedRegionName)) {\n       // the region name is not equal, this usually means the region has been split or merged, so\n       // give up replicating as the new region(s) should already have all the data of the parent\n       // region(s).\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\n           \"Skipping {} entries in table {} because located region {} is different than\" +\n             \" the original region {} from WALEdit\",\n-          tableDesc.getTableName(), locs.getDefaultRegionLocation().getRegion().getEncodedName(),\n+          tableDesc.getTableName(), defaultReplica.getEncodedName(),\n           Bytes.toStringBinary(encodedRegionName));\n       }\n       future.complete(Long.valueOf(entries.size()));\n@@ -202,24 +205,26 @@ private void replicate(CompletableFuture<Long> future, RegionLocations locs,\n     AtomicLong skippedEdits = new AtomicLong(0);\n \n     for (int i = 1, n = locs.size(); i < n; i++) {\n-      final int replicaId = i;\n-      FutureUtils.addListener(connection.replay(tableDesc.getTableName(),\n-        locs.getRegionLocation(replicaId).getRegion().getEncodedNameAsBytes(), row, entries,\n-        replicaId, numRetries, operationTimeoutNs), (r, e) -> {\n-          if (e != null) {\n-            LOG.warn(\"Failed to replicate to {}\", locs.getRegionLocation(replicaId), e);\n-            error.compareAndSet(null, e);\n-          } else {\n-            AtomicUtils.updateMax(skippedEdits, r.longValue());\n-          }\n-          if (remainingTasks.decrementAndGet() == 0) {\n-            if (error.get() != null) {\n-              future.completeExceptionally(error.get());\n+      // Do not use the elements other than the default replica as they may be null. We will fail\n+      // earlier if the location for default replica is null.\n+      final RegionInfo replica = RegionReplicaUtil.getRegionInfoForReplica(defaultReplica, i);\n+      FutureUtils\n+        .addListener(connection.replay(tableDesc.getTableName(), replica.getEncodedNameAsBytes(),\n+          row, entries, replica.getReplicaId(), numRetries, operationTimeoutNs), (r, e) -> {\n+            if (e != null) {\n+              LOG.warn(\"Failed to replicate to {}\", replica, e);\n+              error.compareAndSet(null, e);\n             } else {\n-              future.complete(skippedEdits.get());\n+              AtomicUtils.updateMax(skippedEdits, r.longValue());\n             }\n-          }\n-        });\n+            if (remainingTasks.decrementAndGet() == 0) {\n+              if (error.get() != null) {\n+                future.completeExceptionally(error.get());\n+              } else {\n+                future.complete(skippedEdits.get());\n+              }\n+            }\n+          });\n     }\n   }\n \n@@ -245,6 +250,10 @@ private void logSkipped(TableName tableName, List<Entry> entries, String reason)\n     FutureUtils.addListener(locateFuture, (locs, error) -> {\n       if (error != null) {\n         future.completeExceptionally(error);\n+      } else if (locs.getDefaultRegionLocation() == null) {\n+        future.completeExceptionally(\n+          new HBaseIOException(\"No location found for default replica of table=\" +\n+            tableDesc.getTableName() + \" row='\" + Bytes.toStringBinary(row) + \"'\"));\n       } else {\n         replicate(future, locs, tableDesc, encodedRegionName, row, entries);\n       }",
                "deletions": 21
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22328 NPE in RegionReplicaReplicationEndpoint",
        "commit": "https://github.com/apache/hbase/commit/a95eb6559d69781c20e05b88b5b9ba773328c691",
        "parent": "https://github.com/apache/hbase/commit/6855d5837955c31f4774d42694d64a6d921bc1f5",
        "bug_id": "hbase_17",
        "file": [
            {
                "sha": "cc2650f803f2aa165c353c752d2ff15b14ad4960",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "blob_url": "https://github.com/apache/hbase/blob/a95eb6559d69781c20e05b88b5b9ba773328c691/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "raw_url": "https://github.com/apache/hbase/raw/a95eb6559d69781c20e05b88b5b9ba773328c691/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java",
                "status": "modified",
                "changes": 14,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java?ref=a95eb6559d69781c20e05b88b5b9ba773328c691",
                "patch": "@@ -151,21 +151,23 @@ private boolean requiresReplication(Optional<TableDescriptor> tableDesc, Entry e\n   private void getRegionLocations(CompletableFuture<RegionLocations> future,\n       TableDescriptor tableDesc, byte[] encodedRegionName, byte[] row, boolean reload) {\n     FutureUtils.addListener(connection.getRegionLocations(tableDesc.getTableName(), row, reload),\n-      (r, e) -> {\n+      (locs, e) -> {\n         if (e != null) {\n           future.completeExceptionally(e);\n           return;\n         }\n         // if we are not loading from cache, just return\n         if (reload) {\n-          future.complete(r);\n+          future.complete(locs);\n           return;\n         }\n         // check if the number of region replicas is correct, and also the primary region name\n-        // matches\n-        if (r.size() == tableDesc.getRegionReplication() && Bytes.equals(\n-          r.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(), encodedRegionName)) {\n-          future.complete(r);\n+        // matches, and also there is no null elements in the returned RegionLocations\n+        if (locs.size() == tableDesc.getRegionReplication() &&\n+          locs.size() == locs.numNonNullElements() &&\n+          Bytes.equals(locs.getDefaultRegionLocation().getRegion().getEncodedNameAsBytes(),\n+            encodedRegionName)) {\n+          future.complete(locs);\n         } else {\n           // reload again as the information in cache maybe stale\n           getRegionLocations(future, tableDesc, encodedRegionName, row, true);",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22904 NPE occurs when RS send space quota usage report during HMaster init (#529)\n\n* NPE occurs when RS send space quota usage report during HMaster init\r\n\r\n* Added the debug log\r\n\r\n Signed-off-by: Sakthi <sakthi@apache.org>\r\n Signed-off-by: stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/00581ac911fe012f31e95af03d3943bea9b93205",
        "parent": "https://github.com/apache/hbase/commit/ae107bdb964e3a9815dea967d1dfd6071b81664f",
        "bug_id": "hbase_18",
        "file": [
            {
                "sha": "e1c21c6e503665b658fe13330128ab6956be57f9",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/00581ac911fe012f31e95af03d3943bea9b93205/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/00581ac911fe012f31e95af03d3943bea9b93205/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
                "status": "modified",
                "changes": 16,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java?ref=00581ac911fe012f31e95af03d3943bea9b93205",
                "patch": "@@ -2227,10 +2227,15 @@ public RegionSpaceUseReportResponse reportRegionSpaceUse(RpcController controlle\n         return RegionSpaceUseReportResponse.newBuilder().build();\n       }\n       MasterQuotaManager quotaManager = this.master.getMasterQuotaManager();\n-      final long now = EnvironmentEdgeManager.currentTime();\n-      for (RegionSpaceUse report : request.getSpaceUseList()) {\n-        quotaManager.addRegionSize(ProtobufUtil.toRegionInfo(\n-            report.getRegionInfo()), report.getRegionSize(), now);\n+      if (quotaManager != null) {\n+        final long now = EnvironmentEdgeManager.currentTime();\n+        for (RegionSpaceUse report : request.getSpaceUseList()) {\n+          quotaManager.addRegionSize(ProtobufUtil.toRegionInfo(report.getRegionInfo()),\n+            report.getRegionSize(), now);\n+        }\n+      } else {\n+        LOG.debug(\n+          \"Received region space usage report but HMaster is not ready to process it, skipping\");\n       }\n       return RegionSpaceUseReportResponse.newBuilder().build();\n     } catch (Exception e) {\n@@ -2265,6 +2270,9 @@ public GetSpaceQuotaRegionSizesResponse getSpaceQuotaRegionSizes(\n               .setSize(tableSize.getValue()).build());\n         }\n         return builder.build();\n+      } else {\n+        LOG.debug(\n+          \"Received space quota region size report but HMaster is not ready to process it, skipping\");\n       }\n       return builder.build();\n     } catch (Exception e) {",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22520 Avoid possible NPE while performing seekBefore in Hal\u2026 (#281)\n\nHBASE-22520 Avoid possible NPE while performing seekBefore in HalfStoreFileReader",
        "commit": "https://github.com/apache/hbase/commit/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
        "parent": "https://github.com/apache/hbase/commit/6ea2566ac3f1d1cb76cedf87f00cda6583013b2f",
        "bug_id": "hbase_19",
        "file": [
            {
                "sha": "11ab068ef3d15b358faeec46102b53ada57244d2",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=13c5af38dad6a5c9d97ef6cf950ef3d01989c3b2",
                "patch": "@@ -60,9 +60,9 @@\n   // i.e. empty column and a timestamp of LATEST_TIMESTAMP.\n   protected final byte [] splitkey;\n \n-  protected final Cell splitCell;\n+  private final Cell splitCell;\n \n-  private Optional<Cell> firstKey = null;\n+  private Optional<Cell> firstKey = Optional.empty();\n \n   private boolean firstKeySeeked = false;\n \n@@ -269,7 +269,8 @@ public int reseekTo(Cell key) throws IOException {\n       public boolean seekBefore(Cell key) throws IOException {\n         if (top) {\n           Optional<Cell> fk = getFirstKey();\n-          if (PrivateCellUtil.compareKeyIgnoresMvcc(getComparator(), key, fk.get()) <= 0) {\n+          if (fk.isPresent() &&\n+                  PrivateCellUtil.compareKeyIgnoresMvcc(getComparator(), key, fk.get()) <= 0) {\n             return false;\n           }\n         } else {",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22860 Master's webui returns NPE/HTTP 500 under maintenance mode\n\nSigned-off-by: Wellington Chevreuil <wchevreuil@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/3eb602c7f7081cd1a55deb40797f95adbc9e7758",
        "parent": "https://github.com/apache/hbase/commit/94af65163bc0e111f0ef02e1dfc5a53f9d7589f5",
        "bug_id": "hbase_20",
        "file": [
            {
                "sha": "5423c171989145610500622d8a43da68e3a233da",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/3eb602c7f7081cd1a55deb40797f95adbc9e7758/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/3eb602c7f7081cd1a55deb40797f95adbc9e7758/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "status": "modified",
                "changes": 16,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon?ref=3eb602c7f7081cd1a55deb40797f95adbc9e7758",
                "patch": "@@ -192,8 +192,8 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n         </%if>\n         <%if master.isInMaintenanceMode() %>\n           <div class=\"alert alert-warning\">\n-          Your Master is in maintenance mode. This may be because of HBCK aborting while\n-          running in repair mode. Please re-run HBCK in repair mode.\n+          Your Master is in maintenance mode. This is because hbase.master.maintenance_mode is\n+          set to true. Under the maintenance mode, no quota or no Master coprocessor is loaded.\n           </div>\n         </%if>\n         <%if !master.isBalancerOn() %>\n@@ -220,11 +220,13 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n         <%if master.getAssignmentManager() != null %>\n           <& AssignmentManagerStatusTmpl; assignmentManager=master.getAssignmentManager()&>\n         </%if>\n-        <%if master.getMasterCoprocessorHost().findCoprocessor(\"RSGroupAdminEndpoint\") != null %>\n-          <section>\n-            <h2><a name=\"rsgroup\">RSGroup</a></h2>\n-            <& RSGroupListTmpl; master= master; serverManager= serverManager&>\n-          </section>\n+        <%if !master.isInMaintenanceMode() %>\n+          <%if master.getMasterCoprocessorHost().findCoprocessor(\"RSGroupAdminEndpoint\") != null %>\n+            <section>\n+              <h2><a name=\"rsgroup\">RSGroup</a></h2>\n+              <& RSGroupListTmpl; master= master; serverManager= serverManager&>\n+            </section>\n+          </%if>\n         </%if>\n         <section>\n             <h2><a name=\"regionservers\">Region Servers</a></h2>",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22704 Avoid NPE when access table.jsp and snapshot.jsp but master not finish initialization (#389)",
        "commit": "https://github.com/apache/hbase/commit/2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3",
        "parent": "https://github.com/apache/hbase/commit/2afd5d05d46d6a2bfad5cbcad8678526dd81e3bb",
        "bug_id": "hbase_21",
        "file": [
            {
                "sha": "1ea2b40a4e57d4f572bdf5f122bf66bcd5ed48b4",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3/hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3/hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp",
                "status": "modified",
                "changes": 11,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp?ref=2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3",
                "patch": "@@ -36,7 +36,7 @@\n   SnapshotInfo.SnapshotStats stats = null;\n   TableName snapshotTable = null;\n   boolean tableExists = false;\n-  if(snapshotName != null) {\n+  if(snapshotName != null && master.isInitialized()) {\n     try (Admin admin = master.getConnection().getAdmin()) {\n       for (SnapshotDescription snapshotDesc: admin.listSnapshots()) {\n         if (snapshotName.equals(snapshotDesc.getName())) {\n@@ -66,7 +66,14 @@\n </jsp:include>\n \n <div class=\"container-fluid content\">\n-<% if (snapshot == null) { %>\n+<% if (!master.isInitialized()) { %>\n+    <div class=\"row inner_header\">\n+    <div class=\"page-header\">\n+    <h1>Master is not initialized</h1>\n+    </div>\n+    </div>\n+    <jsp:include page=\"redirect.jsp\" />\n+<% } else if (snapshot == null) { %>\n   <div class=\"row inner_header\">\n     <div class=\"page-header\">\n       <h1>Snapshot \"<%= snapshotName %>\" does not exist</h1>",
                "deletions": 2
            },
            {
                "sha": "07f2050b717f36101274b79fd23a83d50f5691ed",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3/hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3/hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/table.jsp?ref=2b7e33fe52d4a6b7b82dfed5f141428d1a798cd3",
                "patch": "@@ -129,7 +129,7 @@\n </jsp:include>\n \n <%\n-if ( fqtn != null ) {\n+if (fqtn != null && master.isInitialized()) {\n   try {\n   table = master.getConnection().getTable(TableName.valueOf(fqtn));\n   if (table.getDescriptor().getRegionReplication() > 1) {\n@@ -703,7 +703,7 @@ Actions:\n   </div> <%\n   }\n }\n-  else { // handle the case for fqtn is null with error message + redirect\n+  else { // handle the case for fqtn is null or master is not initialized with error message + redirect\n %>\n <div class=\"container-fluid content\">\n     <div class=\"row inner_header\">",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22440. Override getWalGroupsReplicationStatus to avoid NPE\n\nSigned-off-by: Duo Zhang <zhangduo@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/f773043f87a5da115e138811248755642884cc55",
        "parent": "https://github.com/apache/hbase/commit/f1a8aa4fd76f6d118fb35f6b30e48d11556eba3d",
        "bug_id": "hbase_22",
        "file": [
            {
                "sha": "137b55845b546cdafaa8f0b4ab9bb43883670e42",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/f773043f87a5da115e138811248755642884cc55/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/f773043f87a5da115e138811248755642884cc55/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 9,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=f773043f87a5da115e138811248755642884cc55",
                "patch": "@@ -192,6 +192,7 @@\n import org.apache.hadoop.hbase.replication.master.ReplicationHFileCleaner;\n import org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner;\n import org.apache.hadoop.hbase.replication.master.ReplicationPeerConfigUpgrader;\n+import org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus;\n import org.apache.hadoop.hbase.security.AccessDeniedException;\n import org.apache.hadoop.hbase.security.SecurityConstants;\n import org.apache.hadoop.hbase.security.UserProvider;\n@@ -3991,4 +3992,12 @@ public SnapshotQuotaObserverChore getSnapshotQuotaObserverChore() {\n   public SyncReplicationReplayWALManager getSyncReplicationReplayWALManager() {\n     return this.syncReplicationReplayWALManager;\n   }\n+\n+  @Override\n+  public Map<String, ReplicationStatus> getWalGroupsReplicationStatus() {\n+    if (!this.isOnline() || !LoadBalancer.isMasterCanHostUserRegions(conf)) {\n+      return new HashMap<>();\n+    }\n+    return super.getWalGroupsReplicationStatus();\n+  }\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23315 Miscellaneous HBCK Report page cleanup\n\n * Add a bit of javadoc around SerialReplicationChecker.\n * Miniscule edit to the profiler jsp page and then a bit of doc on how to make it work that might help.\n * Add some detail if NPE getting BitSetNode to help w/ debug.\n * Change HbckChore to log region names instead of encoded names; helps doing diagnostics; can take region name and query in shell to find out all about the region according to hbase:meta.\n * Add some fix-it help inline in the HBCK Report page \u2013 how to fix.\n * Add counts in procedures page so can see if making progress; move listing of WALs to end of the page.",
        "commit": "https://github.com/apache/hbase/commit/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
        "parent": "https://github.com/apache/hbase/commit/e83bb205f4fc3555abcdca7c4c985a3f76150f16",
        "bug_id": "hbase_23",
        "file": [
            {
                "sha": "f93c3cce9fa6168ec5fe87d6e373f97c9f6f525e",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -136,7 +136,7 @@\n  *                             columns: info:merge0001, info:merge0002. You make also see 'mergeA',\n  *                             and 'mergeB'. This is old form replaced by the new format that allows\n  *                             for more than two parents to be merged at a time.\n- * TODO: Add rep_barrier for serial replication explaination.\n+ * TODO: Add rep_barrier for serial replication explaination. See SerialReplicationChecker.\n  * </pre>\n  * </p>\n  * <p>\n@@ -607,6 +607,7 @@ private static Scan getMetaScan(Connection connection, int rowUpperLimit) {\n    * @param excludeOfflinedSplitParents don't return split parents\n    * @return Return list of regioninfos and server addresses.\n    */\n+  // What happens here when 1M regions in hbase:meta? This won't scale?\n   public static List<Pair<RegionInfo, ServerName>> getTableRegionsAndLocations(\n       Connection connection, @Nullable final TableName tableName,\n       final boolean excludeOfflinedSplitParents) throws IOException {\n@@ -1928,6 +1929,9 @@ public static Put makePutForReplicationBarrier(RegionInfo regionInfo, long openS\n     return put;\n   }\n \n+  /**\n+   * See class comment on SerialReplicationChecker\n+   */\n   public static void addReplicationBarrier(Put put, long openSeqNum) throws IOException {\n     put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY)\n       .setRow(put.getRow())",
                "deletions": 1
            },
            {
                "sha": "fc75530cc50fcb5c6937df2798e98c308b0bbdb5",
                "filename": "hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-http/src/main/java/org/apache/hadoop/hbase/http/ProfileServlet.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -270,7 +270,7 @@ protected void doGet(final HttpServletRequest req, final HttpServletResponse res\n             resp.getWriter().write(\n               \"Started [\" + event.getInternalName() +\n               \"] profiling. This page will automatically redirect to \" +\n-              relativeUrl + \" after \" + duration + \" seconds.\\n\\ncommand:\\n\" +\n+              relativeUrl + \" after \" + duration + \" seconds.\\n\\nCommand:\\n\" +\n               Joiner.on(\" \").join(cmd));\n \n             // to avoid auto-refresh by ProfileOutputServlet, refreshDelay can be specified\n@@ -395,4 +395,4 @@ protected void doGet(final HttpServletRequest req, final HttpServletResponse res\n \n   }\n \n-}\n\\ No newline at end of file\n+}",
                "deletions": 2
            },
            {
                "sha": "78d2d91ca8661e961539fc2f479e21bed87086ac",
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java",
                "status": "modified",
                "changes": 12,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/BitSetNode.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -407,7 +407,15 @@ void updateState(long procId, boolean isDeleted) {\n     int wordIndex = bitmapIndex >> ADDRESS_BITS_PER_WORD;\n     long value = (1L << bitmapIndex);\n \n-    modified[wordIndex] |= value;\n+    try {\n+      modified[wordIndex] |= value;\n+    } catch (ArrayIndexOutOfBoundsException aioobe) {\n+      // We've gotten a AIOOBE in here; add detail to help debug.\n+      ArrayIndexOutOfBoundsException aioobe2 =\n+          new ArrayIndexOutOfBoundsException(\"pid=\" + procId + \", deleted=\" + isDeleted);\n+      aioobe2.initCause(aioobe);\n+      throw aioobe2;\n+    }\n     if (isDeleted) {\n       deleted[wordIndex] |= value;\n     } else {\n@@ -431,4 +439,4 @@ private static long alignUp(final long x) {\n   private static long alignDown(final long x) {\n     return x & -BITS_PER_WORD;\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 2
            },
            {
                "sha": "cf4368581e708413894a022108d81bcd0a25d97a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java",
                "status": "modified",
                "changes": 22,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HbckChore.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -190,10 +190,10 @@ private void loadRegionsFromInMemoryState() {\n       RegionInfo regionInfo = regionState.getRegion();\n       if (master.getTableStateManager()\n           .isTableState(regionInfo.getTable(), TableState.State.DISABLED)) {\n-        disabledTableRegions.add(regionInfo.getEncodedName());\n+        disabledTableRegions.add(regionInfo.getRegionNameAsString());\n       }\n       if (regionInfo.isSplitParent()) {\n-        splitParentRegions.add(regionInfo.getEncodedName());\n+        splitParentRegions.add(regionInfo.getRegionNameAsString());\n       }\n       HbckRegionInfo.MetaEntry metaEntry =\n           new HbckRegionInfo.MetaEntry(regionInfo, regionState.getServerName(),\n@@ -212,7 +212,7 @@ private void loadRegionsFromRSReport() {\n         String encodedRegionName = RegionInfo.encodeRegionName(regionName);\n         HbckRegionInfo hri = regionInfoMap.get(encodedRegionName);\n         if (hri == null) {\n-          orphanRegionsOnRS.put(encodedRegionName, serverName);\n+          orphanRegionsOnRS.put(RegionInfo.getRegionNameAsString(regionName), serverName);\n           continue;\n         }\n         hri.addServer(hri.getMetaEntry(), serverName);\n@@ -223,29 +223,31 @@ private void loadRegionsFromRSReport() {\n         numRegions, rsReports.size(), orphanRegionsOnFS.size());\n \n     for (Map.Entry<String, HbckRegionInfo> entry : regionInfoMap.entrySet()) {\n-      String encodedRegionName = entry.getKey();\n       HbckRegionInfo hri = entry.getValue();\n       ServerName locationInMeta = hri.getMetaEntry().getRegionServer();\n       if (hri.getDeployedOn().size() == 0) {\n         if (locationInMeta == null) {\n           continue;\n         }\n         // skip the offline region which belong to disabled table.\n-        if (disabledTableRegions.contains(encodedRegionName)) {\n+        if (disabledTableRegions.contains(hri.getRegionNameAsString())) {\n           continue;\n         }\n         // skip the split parent regions\n-        if (splitParentRegions.contains(encodedRegionName)) {\n+        if (splitParentRegions.contains(hri.getRegionNameAsString())) {\n           continue;\n         }\n         // Master thought this region opened, but no regionserver reported it.\n-        inconsistentRegions.put(encodedRegionName, new Pair<>(locationInMeta, new LinkedList<>()));\n+        inconsistentRegions.put(hri.getRegionNameAsString(),\n+            new Pair<>(locationInMeta, new LinkedList<>()));\n       } else if (hri.getDeployedOn().size() > 1) {\n         // More than one regionserver reported opened this region\n-        inconsistentRegions.put(encodedRegionName, new Pair<>(locationInMeta, hri.getDeployedOn()));\n+        inconsistentRegions.put(hri.getRegionNameAsString(),\n+            new Pair<>(locationInMeta, hri.getDeployedOn()));\n       } else if (!hri.getDeployedOn().get(0).equals(locationInMeta)) {\n         // Master thought this region opened on Server1, but regionserver reported Server2\n-        inconsistentRegions.put(encodedRegionName, new Pair<>(locationInMeta, hri.getDeployedOn()));\n+        inconsistentRegions.put(hri.getRegionNameAsString(),\n+            new Pair<>(locationInMeta, hri.getDeployedOn()));\n       }\n     }\n   }\n@@ -339,4 +341,4 @@ public long getCheckingStartTimestamp() {\n   public long getCheckingEndTimestamp() {\n     return this.checkingEndTimestamp;\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 10
            },
            {
                "sha": "321bbb420bc74a5a6c9d352b4950d9286453fd10",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java",
                "status": "modified",
                "changes": 9,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -50,12 +50,13 @@\n  * </p>\n  * <p>\n  * We record all the open sequence number for a region in a special family in meta, which is called\n- * 'barrier', so there will be a sequence of open sequence number (b1, b2, b3, ...). We call [bn,\n- * bn+1) a range, and it is obvious that a region will always be on the same RS within a range.\n+ * 'rep_barrier', so there will be a sequence of open sequence number (b1, b2, b3, ...). We call\n+ * [bn, bn+1) a range, and it is obvious that a region will always be on the same RS within a\n+ * range.\n  * <p>\n  * When split and merge, we will also record the parent for the generated region(s) in the special\n- * family in meta. And also, we will write an extra 'open sequence number' for the parent region(s),\n- * which is the max sequence id of the region plus one.\n+ * family in meta. And also, we will write an extra 'open sequence number' for the parent\n+ * region(s), which is the max sequence id of the region plus one.\n  * </p>\n  * </p>\n  * <p>",
                "deletions": 4
            },
            {
                "sha": "af9b879bfe625262fd0ba02a043721640d16fb22",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/CanaryTool.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -277,13 +277,15 @@ public void publishReadTiming(String znode, String server, long msTime) {\n \n     public void publishReadFailure(ServerName serverName, RegionInfo region, Exception e) {\n       incReadFailureCount();\n-      LOG.error(\"Read from {} on {} failed\", region.getRegionNameAsString(), serverName, e);\n+      LOG.error(\"Read from {} on serverName={} failed\",\n+          region.getRegionNameAsString(), serverName, e);\n     }\n \n     public void publishReadFailure(ServerName serverName, RegionInfo region,\n         ColumnFamilyDescriptor column, Exception e) {\n       incReadFailureCount();\n-      LOG.error(\"Read from {} on {} {} failed\", region.getRegionNameAsString(), serverName,\n+      LOG.error(\"Read from {} on serverName={}, columnFamily={} failed\",\n+          region.getRegionNameAsString(), serverName,\n           column.getNameAsString(), e);\n     }\n ",
                "deletions": 2
            },
            {
                "sha": "f89aac8bce6fdf49739ba08d0c017da5cba35b2a",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp",
                "status": "modified",
                "changes": 14,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/hbck.jsp?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -78,7 +78,7 @@\n \n   <div class=\"row\">\n     <div class=\"page-header\">\n-  <p><span>This page displays two reports: the 'HBCK Chore Report' and the 'CatalogJanitor Consistency Issues' report. Only titles show if there are no problems to report. Note some conditions are transitory as regions migrate.</span></p>\n+      <p><span>This page displays two reports: the 'HBCK Chore Report' and the 'CatalogJanitor Consistency Issues' report. Only titles show if there are no problems to report. Note some conditions are <em>transitory</em> as regions migrate.</span></p>\n     </div>\n   </div>\n   <div class=\"row\">\n@@ -119,7 +119,7 @@\n \n   <table class=\"table table-striped\">\n     <tr>\n-      <th>Region Encoded Name</th>\n+      <th>Region Name</th>\n       <th>Location in META</th>\n       <th>Reported Online RegionServers</th>\n     </tr>\n@@ -142,10 +142,18 @@\n       <h2>Orphan Regions on RegionServer</h2>\n     </div>\n   </div>\n+      <p>\n+        <span>\n+          The below are Regions we've lost account of. To be safe, run bulk load of any data found in these Region orphan directories back into the HBase cluster.\n+          First make sure hbase:meta is in healthy state; run 'hbkc2 fixMeta' to be sure. Once this is done, per Region below, run a bulk\n+          load -- '$ hbase completebulkload REGION_DIR_PATH TABLE_NAME' -- and then delete the desiccated directory content (HFiles are removed upon successful load; all that is left are empty directories\n+          and occasionally a seqid marking file).\n+        </span>\n+      </p>\n \n   <table class=\"table table-striped\">\n     <tr>\n-      <th>Region Encoded Name</th>\n+      <th>Region Name</th>\n       <th>Reported Online RegionServer</th>\n     </tr>\n     <% for (Map.Entry<String, ServerName> entry : orphanRegionsOnRS.entrySet()) { %>",
                "deletions": 3
            },
            {
                "sha": "ea252cff39a1fb8c2bbc398a42b0ec08ddc25b04",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp",
                "status": "modified",
                "changes": 98,
                "additions": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -81,11 +81,14 @@\n         <th>Errors</th>\n         <th>Parameters</th>\n     </tr>\n-    <% for (Procedure<?> proc : procedures) { \n+    <%\n+      int displayCount = 0;\n+      for (Procedure<?> proc : procedures) {\n       // Don't show SUCCESS procedures.\n       if (proc.isSuccess()) {\n         continue;\n       }\n+      displayCount++;\n     %>\n       <tr>\n         <td><%= proc.getProcId() %></td>\n@@ -99,9 +102,63 @@\n         <td><%= escapeXml(proc.toString()) %></td>\n       </tr>\n     <% } %>\n+    <%\n+    if (displayCount > 0) {\n+    %>\n+      <p><%= displayCount %> procedure(s).</p>\n+    <%\n+    }\n+    %>\n   </table>\n </div>\n <br />\n+<div class=\"container-fluid content\">\n+  <div class=\"row\">\n+      <div class=\"page-header\">\n+          <h1>Locks</h1>\n+      </div>\n+  </div>\n+    <%\n+    if (lockedResources.size() > 0) {\n+    %>\n+    <p><%= lockedResources.size() %> lock(s).</p>\n+    <%\n+    }\n+    %>\n+  <% for (LockedResource lockedResource : lockedResources) { %>\n+    <h2><%= lockedResource.getResourceType() %>: <%= lockedResource.getResourceName() %></h2>\n+    <%\n+      switch (lockedResource.getLockType()) {\n+      case EXCLUSIVE:\n+    %>\n+    <p>Lock type: EXCLUSIVE</p>\n+    <p>Owner procedure: <%= escapeXml(lockedResource.getExclusiveLockOwnerProcedure().toStringDetails()) %></p>\n+    <%\n+        break;\n+      case SHARED:\n+    %>\n+    <p>Lock type: SHARED</p>\n+    <p>Number of shared locks: <%= lockedResource.getSharedLockCount() %></p>\n+    <%\n+        break;\n+      }\n+\n+      List<Procedure<?>> waitingProcedures = lockedResource.getWaitingProcedures();\n+\n+      if (!waitingProcedures.isEmpty()) {\n+    %>\n+        <h3>Waiting procedures</h3>\n+        <table class=\"table table-striped\" width=\"90%\" >\n+        <% for (Procedure<?> proc : procedures) { %>\n+         <tr>\n+            <td><%= escapeXml(proc.toStringDetails()) %></td>\n+          </tr>\n+        <% } %>\n+        </table>\n+    <% } %>\n+  <% } %>\n+</div>\n+<br />\n <div class=\"container-fluid content\">\n   <div class=\"row\">\n     <div class=\"page-header\">\n@@ -206,44 +263,5 @@\n   </div>\n </div>\n <br />\n-<div class=\"container-fluid content\">\n-  <div class=\"row\">\n-      <div class=\"page-header\">\n-          <h1>Locks</h1>\n-      </div>\n-  </div>\n-  <% for (LockedResource lockedResource : lockedResources) { %>\n-    <h2><%= lockedResource.getResourceType() %>: <%= lockedResource.getResourceName() %></h2>\n-    <%\n-      switch (lockedResource.getLockType()) {\n-      case EXCLUSIVE:\n-    %>\n-    <p>Lock type: EXCLUSIVE</p>\n-    <p>Owner procedure: <%= escapeXml(lockedResource.getExclusiveLockOwnerProcedure().toStringDetails()) %></p>\n-    <%\n-        break;\n-      case SHARED:\n-    %>\n-    <p>Lock type: SHARED</p>\n-    <p>Number of shared locks: <%= lockedResource.getSharedLockCount() %></p>\n-    <%\n-        break;\n-      }\n-\n-      List<Procedure<?>> waitingProcedures = lockedResource.getWaitingProcedures();\n-\n-      if (!waitingProcedures.isEmpty()) {\n-    %>\n-        <h3>Waiting procedures</h3>\n-        <table class=\"table table-striped\" width=\"90%\" >\n-        <% for (Procedure<?> proc : procedures) { %>\n-         <tr>\n-            <td><%= escapeXml(proc.toStringDetails()) %></td>\n-          </tr>\n-        <% } %>\n-        </table>\n-    <% } %>\n-  <% } %>\n-</div>\n \n <jsp:include page=\"footer.jsp\" />",
                "deletions": 40
            },
            {
                "sha": "ea7050824942762fb5f21ed9e6a272de32646e62",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java",
                "blob_url": "https://github.com/apache/hbase/blob/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java",
                "raw_url": "https://github.com/apache/hbase/raw/ca6e67a6de242d681b6e6f3d53a0db5b10d1450a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestHbckChore.java?ref=ca6e67a6de242d681b6e6f3d53a0db5b10d1450a",
                "patch": "@@ -69,7 +69,7 @@ public void setUp() throws Exception {\n   @Test\n   public void testForMeta() {\n     byte[] metaRegionNameAsBytes = RegionInfoBuilder.FIRST_META_REGIONINFO.getRegionName();\n-    String metaRegionName = RegionInfoBuilder.FIRST_META_REGIONINFO.getEncodedName();\n+    String metaRegionName = RegionInfoBuilder.FIRST_META_REGIONINFO.getRegionNameAsString();\n     List<ServerName> serverNames = master.getServerManager().getOnlineServersList();\n     assertEquals(NSERVERS, serverNames.size());\n \n@@ -96,7 +96,7 @@ public void testForMeta() {\n   public void testForUserTable() throws Exception {\n     TableName tableName = TableName.valueOf(\"testForUserTable\");\n     RegionInfo hri = createRegionInfo(tableName, 1);\n-    String regionName = hri.getEncodedName();\n+    String regionName = hri.getRegionNameAsString();\n     rsDispatcher.setMockRsExecutor(new GoodRsExecutor());\n     Future<byte[]> future = submitProcedure(createAssignProcedure(hri));\n     waitOnFuture(future);\n@@ -154,7 +154,7 @@ public void testForUserTable() throws Exception {\n   public void testForDisabledTable() throws Exception {\n     TableName tableName = TableName.valueOf(\"testForDisabledTable\");\n     RegionInfo hri = createRegionInfo(tableName, 1);\n-    String regionName = hri.getEncodedName();\n+    String regionName = hri.getRegionNameAsString();\n     rsDispatcher.setMockRsExecutor(new GoodRsExecutor());\n     Future<byte[]> future = submitProcedure(createAssignProcedure(hri));\n     waitOnFuture(future);",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21840 TestHRegionWithInMemoryFlush fails with NPE",
        "commit": "https://github.com/apache/hbase/commit/d69c3e5d489a2ec24d28594efc471ac6621254b9",
        "parent": "https://github.com/apache/hbase/commit/8d75ac80a81b8cf8e5a3ba06ee95b6da69d15bd4",
        "bug_id": "hbase_24",
        "file": [
            {
                "sha": "595ae7a42f29f19ca4b9a441647679290971d919",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java",
                "blob_url": "https://github.com/apache/hbase/blob/d69c3e5d489a2ec24d28594efc471ac6621254b9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java",
                "raw_url": "https://github.com/apache/hbase/raw/d69c3e5d489a2ec24d28594efc471ac6621254b9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java",
                "status": "modified",
                "changes": 18,
                "additions": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServicesForStores.java?ref=d69c3e5d489a2ec24d28594efc471ac6621254b9",
                "patch": "@@ -18,14 +18,16 @@\n  */\n package org.apache.hadoop.hbase.regionserver;\n \n+import java.util.concurrent.LinkedBlockingQueue;\n import java.util.concurrent.ThreadPoolExecutor;\n-\n+import java.util.concurrent.TimeUnit;\n import org.apache.hadoop.hbase.client.RegionInfo;\n import org.apache.hadoop.hbase.executor.ExecutorType;\n import org.apache.hadoop.hbase.wal.WAL;\n import org.apache.yetus.audience.InterfaceAudience;\n \n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n+import org.apache.hbase.thirdparty.com.google.common.util.concurrent.ThreadFactoryBuilder;\n \n /**\n  * Services a Store needs from a Region.\n@@ -64,12 +66,24 @@ public WAL getWAL() {\n     return region.getWAL();\n   }\n \n+  private static ThreadPoolExecutor INMEMORY_COMPACTION_POOL_FOR_TEST;\n+\n+  private static synchronized ThreadPoolExecutor getInMemoryCompactionPoolForTest() {\n+    if (INMEMORY_COMPACTION_POOL_FOR_TEST == null) {\n+      INMEMORY_COMPACTION_POOL_FOR_TEST = new ThreadPoolExecutor(10, 10, 60, TimeUnit.SECONDS,\n+        new LinkedBlockingQueue<>(), new ThreadFactoryBuilder().setDaemon(true)\n+          .setNameFormat(\"InMemoryCompactionsForTest-%d\").build());\n+    }\n+    return INMEMORY_COMPACTION_POOL_FOR_TEST;\n+  }\n+\n   ThreadPoolExecutor getInMemoryCompactionPool() {\n     if (rsServices != null) {\n       return rsServices.getExecutorService().getExecutorLazily(ExecutorType.RS_IN_MEMORY_COMPACTION,\n         inMemoryPoolSize);\n     } else {\n-      return null;\n+      // this could only happen in tests\n+      return getInMemoryCompactionPoolForTest();\n     }\n   }\n ",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-23369 Auto-close 'unknown' Regions reported as OPEN on RegionServers\n\nMaster force-closes unknown/incorrect Regions OPEN on RS\n\nM hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java\n Added a note and small refactor.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java\n Fix an NPE when CJ ran.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java\n Minor clean up of log message; make it clearer.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java\n Make it so closeRegionSilentlyAndWait can be used w/o timeout.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java\n If a RegionServer Report notes a Region is OPEN and the Master does not\n know of said Region, close it (We used to crash out the RegionServer)\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java\n Minor tweak of toString -- label should be state, not rit (confusing).\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java\n Doc.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure.java\n Add region name to exception.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/HBCKServerCrashProcedure.java\n Be more careful about which Regions we queue up for reassign. This\n procedure is run by the operator so could happen at any time. We\n will likely be running this when Master has some accounting of\n cluster members so check its answers for what Regions were on\n server before running.\n\nM hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\n Doc and we were misrepresenting the case where a Region as not in RIT\n when we got CLOSE -- we were reporting it as though it was already\n trying to CLOSE.",
        "commit": "https://github.com/apache/hbase/commit/d75a7001b176bfb189aee83f89c834821b50e4a7",
        "parent": "https://github.com/apache/hbase/commit/85ecdd10a997780f7d0e23466ab1d1b30faaf298",
        "bug_id": "hbase_25",
        "file": [
            {
                "sha": "cba3f2dec1f89c29ac1188c1399cd9e8474e9402",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/MetaTableAccessor.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -320,6 +320,7 @@ public static HRegionLocation getRegionLocation(Connection connection, RegionInf\n    * is stored in the name, so the returned object should only be used for the fields\n    * in the regionName.\n    */\n+  // This should be moved to RegionInfo? TODO.\n   public static RegionInfo parseRegionInfoFromRegionName(byte[] regionName) throws IOException {\n     byte[][] fields = RegionInfo.parseRegionName(regionName);\n     long regionId = Long.parseLong(Bytes.toString(fields[2]));\n@@ -1249,9 +1250,7 @@ public static int getRegionCount(final Connection connection, final TableName ta\n    * Generates and returns a Put containing the region into for the catalog table\n    */\n   public static Put makePutFromRegionInfo(RegionInfo regionInfo, long ts) throws IOException {\n-    Put put = new Put(regionInfo.getRegionName(), ts);\n-    addRegionInfo(put, regionInfo);\n-    return put;\n+    return addRegionInfo(new Put(regionInfo.getRegionName(), ts), regionInfo);\n   }\n \n   /**",
                "deletions": 3
            },
            {
                "sha": "4b792f9d86725fcfee2236a728c863f8bc473493",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -735,7 +735,7 @@ private void checkServer(RegionLocations locations) {\n           continue;\n         }\n         RegionState rs = this.services.getAssignmentManager().getRegionStates().getRegionState(ri);\n-        if (rs.isClosedOrAbnormallyClosed()) {\n+        if (rs == null || rs.isClosedOrAbnormallyClosed()) {\n           // If closed against an 'Unknown Server', that is should be fine.\n           continue;\n         }",
                "deletions": 1
            },
            {
                "sha": "ceef6b59ebcadf3353b0ef7b8144ff6dc7034142",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -1855,7 +1855,7 @@ public boolean balance(boolean force) throws IOException {\n         } catch (HBaseIOException hioe) {\n           //should ignore failed plans here, avoiding the whole balance plans be aborted\n           //later calls of balance() can fetch up the failed and skipped plans\n-          LOG.warn(\"Failed balance plan: {}, just skip it\", plan, hioe);\n+          LOG.warn(\"Failed balance plan {}, skipping...\", plan, hioe);\n         }\n         //rpCount records balance plans processed, does not care if a plan succeeds\n         rpCount++;",
                "deletions": 1
            },
            {
                "sha": "a61f96ad9cc5b69534e357223776ada7e97078ee",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -675,8 +675,9 @@ public synchronized boolean addServerToDrainList(final ServerName sn) {\n   }\n \n   /**\n-   * Contacts a region server and waits up to timeout ms to close the region. This bypasses the\n-   * active hmaster.\n+   * Contacts a region server and waits up to timeout ms\n+   * to close the region.  This bypasses the active hmaster.\n+   * Pass -1 as timeout if you do not want to wait on result.\n    */\n   public static void closeRegionSilentlyAndWait(AsyncClusterConnection connection,\n       ServerName server, RegionInfo region, long timeout) throws IOException, InterruptedException {\n@@ -687,6 +688,9 @@ public static void closeRegionSilentlyAndWait(AsyncClusterConnection connection,\n     } catch (IOException e) {\n       LOG.warn(\"Exception when closing region: \" + region.getRegionNameAsString(), e);\n     }\n+    if (timeout < 0) {\n+      return;\n+    }\n     long expiration = timeout + System.currentTimeMillis();\n     while (System.currentTimeMillis() < expiration) {\n       try {",
                "deletions": 2
            },
            {
                "sha": "d6ae254a5a20d7b22583c62bb132d29742064220",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java",
                "status": "modified",
                "changes": 54,
                "additions": 41,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignmentManager.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -36,6 +36,7 @@\n import org.apache.hadoop.hbase.DoNotRetryIOException;\n import org.apache.hadoop.hbase.HBaseIOException;\n import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.PleaseHoldException;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.TableName;\n@@ -629,7 +630,8 @@ public TransitRegionStateProcedure createMoveRegionProcedure(RegionInfo regionIn\n       ServerName targetServer) throws HBaseIOException {\n     RegionStateNode regionNode = this.regionStates.getRegionStateNode(regionInfo);\n     if (regionNode == null) {\n-      throw new UnknownRegionException(\"No RegionState found for \" + regionInfo.getEncodedName());\n+      throw new UnknownRegionException(\"No RegionStateNode found for \" +\n+          regionInfo.getEncodedName() + \"(Closed/Deleted?)\");\n     }\n     TransitRegionStateProcedure proc;\n     regionNode.lock();\n@@ -944,7 +946,7 @@ private void updateRegionTransition(ServerName serverName, TransitionCode state,\n     if (regionNode == null) {\n       // the table/region is gone. maybe a delete, split, merge\n       throw new UnexpectedStateException(String.format(\n-        \"Server %s was trying to transition region %s to %s. but the region was removed.\",\n+        \"Server %s was trying to transition region %s to %s. but Region is not known.\",\n         serverName, regionInfo, state));\n     }\n     LOG.trace(\"Update region transition serverName={} region={} regionState={}\", serverName,\n@@ -966,7 +968,8 @@ private void updateRegionTransition(ServerName serverName, TransitionCode state,\n           state.equals(TransitionCode.CLOSED)) {\n           LOG.info(\"RegionServer {} {}\", state, regionNode.getRegionInfo().getEncodedName());\n         } else {\n-          LOG.warn(\"No matching procedure found for {} transition to {}\", regionNode, state);\n+          LOG.warn(\"No matching procedure found for {} transition on {} to {}\",\n+              serverName, regionNode, state);\n         }\n       }\n     } finally {\n@@ -1092,7 +1095,27 @@ public void reportOnlineRegions(ServerName serverName, Set<byte[]> regionNames)\n     checkOnlineRegionsReport(serverNode, regionNames);\n   }\n \n-  // just check and output possible inconsistency, without actually doing anything\n+  /**\n+   * Close <code>regionName</code> on <code>sn</code> silently and immediately without\n+   * using a Procedure or going via hbase:meta. For case where a RegionServer's hosting\n+   * of a Region is not aligned w/ the Master's accounting of Region state. This is for\n+   * cleaning up an error in accounting.\n+   */\n+  private void closeRegionSilently(ServerName sn, byte [] regionName) {\n+    try {\n+      RegionInfo ri = MetaTableAccessor.parseRegionInfoFromRegionName(regionName);\n+      // Pass -1 for timeout. Means do not wait.\n+      ServerManager.closeRegionSilentlyAndWait(this.master.getClusterConnection(), sn, ri, -1);\n+    } catch (Exception e) {\n+      LOG.error(\"Failed trying to close {} on {}\", Bytes.toStringBinary(regionName), sn, e);\n+    }\n+  }\n+\n+  /**\n+   * Check that what the RegionServer reports aligns with the Master's image.\n+   * If disagreement, we will tell the RegionServer to expediently close\n+   * a Region we do not think it should have.\n+   */\n   private void checkOnlineRegionsReport(ServerStateNode serverNode, Set<byte[]> regionNames) {\n     ServerName serverName = serverNode.getServerName();\n     for (byte[] regionName : regionNames) {\n@@ -1101,28 +1124,33 @@ private void checkOnlineRegionsReport(ServerStateNode serverNode, Set<byte[]> re\n       }\n       RegionStateNode regionNode = regionStates.getRegionStateNodeFromName(regionName);\n       if (regionNode == null) {\n-        LOG.warn(\"No region state node for {}, it should already be on {}\",\n-          Bytes.toStringBinary(regionName), serverName);\n+        String regionNameAsStr = Bytes.toStringBinary(regionName);\n+        LOG.warn(\"No RegionStateNode for {} but reported as up on {}; closing...\",\n+            regionNameAsStr, serverName);\n+        closeRegionSilently(serverNode.getServerName(), regionName);\n         continue;\n       }\n+      final long lag = 1000;\n       regionNode.lock();\n       try {\n         long diff = EnvironmentEdgeManager.currentTime() - regionNode.getLastUpdate();\n         if (regionNode.isInState(State.OPENING, State.OPEN)) {\n           // This is possible as a region server has just closed a region but the region server\n           // report is generated before the closing, but arrive after the closing. Make sure there\n           // is some elapsed time so less false alarms.\n-          if (!regionNode.getRegionLocation().equals(serverName) && diff > 1000) {\n-            LOG.warn(\"{} reported OPEN on server={} but state has otherwise\", regionNode,\n-              serverName);\n+          if (!regionNode.getRegionLocation().equals(serverName) && diff > lag) {\n+            LOG.warn(\"Reporting {} server does not match {} (time since last \" +\n+                    \"update={}ms); closing...\",\n+              serverName, regionNode, diff);\n+            closeRegionSilently(serverNode.getServerName(), regionName);\n           }\n         } else if (!regionNode.isInState(State.CLOSING, State.SPLITTING)) {\n           // So, we can get report that a region is CLOSED or SPLIT because a heartbeat\n           // came in at about same time as a region transition. Make sure there is some\n           // elapsed time so less false alarms.\n-          if (diff > 1000) {\n-            LOG.warn(\"{} reported an unexpected OPEN on {}; time since last update={}ms\",\n-              regionNode, serverName, diff);\n+          if (diff > lag) {\n+            LOG.warn(\"Reporting {} state does not match {} (time since last update={}ms)\",\n+              serverName, regionNode, diff);\n           }\n         }\n       } finally {",
                "deletions": 13
            },
            {
                "sha": "e4a18d0709894502486faa1faf59f84d6bfdca26",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStateNode.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -282,7 +282,7 @@ public String toString() {\n \n   public String toShortString() {\n     // rit= is the current Region-In-Transition State -- see State enum.\n-    return String.format(\"rit=%s, location=%s\", getState(), getRegionLocation());\n+    return String.format(\"state=%s, location=%s\", getState(), getRegionLocation());\n   }\n \n   public String toDescriptiveString() {",
                "deletions": 1
            },
            {
                "sha": "8d22c0e065fd1782775a64bbbf8ed0526df689bf",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -96,6 +96,9 @@ public int compare(final RegionState l, final RegionState r) {\n \n   public RegionStates() { }\n \n+  /**\n+   * Called on stop of AssignmentManager.\n+   */\n   public void clear() {\n     regionsMap.clear();\n     regionInTransition.clear();\n@@ -728,12 +731,15 @@ public ServerStateNode getOrCreateServer(final ServerName serverName) {\n     return serverMap.computeIfAbsent(serverName, key -> new ServerStateNode(key));\n   }\n \n+  /**\n+   * Called by SCP at end of successful processing.\n+   */\n   public void removeServer(final ServerName serverName) {\n     serverMap.remove(serverName);\n   }\n \n   /**\n-   * @return Pertinent ServerStateNode or NULL if none found.\n+   * @return Pertinent ServerStateNode or NULL if none found (Do not make modifications).\n    */\n   @VisibleForTesting\n   public ServerStateNode getServerNode(final ServerName serverName) {",
                "deletions": 1
            },
            {
                "sha": "f63de8bb1fd9c3f11850e27fe5a3e7cb6c8e6c3b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -248,7 +248,8 @@ private Flow confirmOpened(MasterProcedureEnv env, RegionStateNode regionNode)\n \n     if (retries > env.getAssignmentManager().getAssignRetryImmediatelyMaxAttempts()) {\n       // Throw exception to backoff and retry when failed open too many times\n-      throw new HBaseIOException(\"Failed to open region\");\n+      throw new HBaseIOException(\"Failed confirm OPEN of \" + regionNode +\n+          \" (remote log may yield more detail on why).\");\n     } else {\n       // Here we do not throw exception because we want to the region to be online ASAP\n       return Flow.HAS_MORE_STATE;",
                "deletions": 2
            },
            {
                "sha": "524fcd9bca6be2e58b3c18bdc15e8ce7eb8cf0f8",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/d75a7001b176bfb189aee83f89c834821b50e4a7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=d75a7001b176bfb189aee83f89c834821b50e4a7",
                "patch": "@@ -3170,7 +3170,7 @@ private void closeRegionIgnoreErrors(RegionInfo region, final boolean abort) {\n \n   /**\n    * Close asynchronously a region, can be called from the master or internally by the regionserver\n-   * when stopping. If called from the master, the region will update the znode status.\n+   * when stopping. If called from the master, the region will update the status.\n    *\n    * <p>\n    * If an opening was in progress, this method will cancel it, but will not start a new close. The\n@@ -3200,6 +3200,7 @@ protected boolean closeRegion(String encodedName, final boolean abort, final Ser\n       }\n     }\n \n+    // previous can come back 'null' if not in map.\n     final Boolean previous = this.regionsInTransitionInRS.putIfAbsent(Bytes.toBytes(encodedName),\n         Boolean.FALSE);\n \n@@ -3221,6 +3222,8 @@ protected boolean closeRegion(String encodedName, final boolean abort, final Ser\n         throw new NotServingRegionException(\"The region \" + encodedName +\n           \" was opening but not yet served. Opening is cancelled.\");\n       }\n+    } else if (previous == null) {\n+      LOG.info(\"Received CLOSE for {}\", encodedName);\n     } else if (Boolean.FALSE.equals(previous)) {\n       LOG.info(\"Received CLOSE for the region: \" + encodedName +\n         \", which we are already trying to CLOSE, but not completed yet\");",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21740 Fix NPE while shutting down RS",
        "commit": "https://github.com/apache/hbase/commit/7be71c0f558f77e0e41be3dad523a74bab1d3db2",
        "parent": "https://github.com/apache/hbase/commit/40e1d9174e1dd15ed95b03a4cf8944384b1aaa2c",
        "bug_id": "hbase_26",
        "file": [
            {
                "sha": "a441ceba356c5f2e6dd7c6a1566de7d192af205d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/7be71c0f558f77e0e41be3dad523a74bab1d3db2/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/7be71c0f558f77e0e41be3dad523a74bab1d3db2/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=7be71c0f558f77e0e41be3dad523a74bab1d3db2",
                "patch": "@@ -1976,7 +1976,9 @@ private void startServices() throws IOException {\n \n     Threads.setDaemonThreadRunning(this.walRoller.getThread(), getName() + \".logRoller\",\n     uncaughtExceptionHandler);\n-    this.cacheFlusher.start(uncaughtExceptionHandler);\n+    if (this.cacheFlusher != null) {\n+      this.cacheFlusher.start(uncaughtExceptionHandler);\n+    }\n     Threads.setDaemonThreadRunning(this.procedureResultReporter,\n       getName() + \".procedureResultReporter\", uncaughtExceptionHandler);\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-22376 master can fail to start w/NPE if lastflushedseqids file is empty\n\nSigned-off-by: Peter Somogyi <psomogyi@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/3641e7a97d1d6f460a8264fb989c821caf70bad5",
        "parent": "https://github.com/apache/hbase/commit/420fbba6aefa58bb6ff252ff0c33e9b4ccae64b0",
        "bug_id": "hbase_27",
        "file": [
            {
                "sha": "8eb8a481dabf1ed9906b1159fb4705c9db30b8dc",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/3641e7a97d1d6f460a8264fb989c821caf70bad5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/3641e7a97d1d6f460a8264fb989c821caf70bad5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=3641e7a97d1d6f460a8264fb989c821caf70bad5",
                "patch": "@@ -1007,7 +1007,7 @@ private void finishActiveMasterInitialization(MonitoredTask status) throws IOExc\n     try {\n       this.serverManager.loadLastFlushedSequenceIds();\n     } catch (IOException e) {\n-      LOG.debug(\"Failed to load last flushed sequence id of regions\"\n+      LOG.info(\"Failed to load last flushed sequence id of regions\"\n           + \" from file system\", e);\n     }\n     // Set ourselves as active Master now our claim has succeeded up in zk.",
                "deletions": 1
            },
            {
                "sha": "88edb79d7944c3ccbc016bb658b624949c73d1c1",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/3641e7a97d1d6f460a8264fb989c821caf70bad5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/3641e7a97d1d6f460a8264fb989c821caf70bad5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java?ref=3641e7a97d1d6f460a8264fb989c821caf70bad5",
                "patch": "@@ -1126,6 +1126,10 @@ public void loadLastFlushedSequenceIds() throws IOException {\n     try {\n       FlushedSequenceId flushedSequenceId =\n           FlushedSequenceId.parseDelimitedFrom(in);\n+      if (flushedSequenceId == null) {\n+        LOG.info(\".lastflushedseqids found at {} is empty\", lastFlushedSeqIdPath);\n+        return;\n+      }\n       for (FlushedRegionSequenceId flushedRegionSequenceId : flushedSequenceId\n           .getRegionSequenceIdList()) {\n         byte[] encodedRegionName = flushedRegionSequenceId",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21055 NullPointerException when balanceOverall() but server balance info is null\n\nSigned-off-by: huzheng <openinx@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/92fdc8dd51156d1b21424f227fa19105a50b425b",
        "parent": "https://github.com/apache/hbase/commit/e52039920112d2002832559b8c248582e8bade32",
        "bug_id": "hbase_28",
        "file": [
            {
                "sha": "89de13bb78d470cf4434ced0b2e721c76bfae387",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/92fdc8dd51156d1b21424f227fa19105a50b425b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/92fdc8dd51156d1b21424f227fa19105a50b425b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java?ref=92fdc8dd51156d1b21424f227fa19105a50b425b",
                "patch": "@@ -479,6 +479,9 @@ public void balanceOverall(List<RegionPlan> regionsToReturn,\n     for (int i = 0; i < serverLoadList.size(); i++) {\n       ServerAndLoad serverload = serverLoadList.get(i);\n       BalanceInfo balanceInfo = serverBalanceInfo.get(serverload.getServerName());\n+      if (balanceInfo == null) {\n+        continue;\n+      }\n       setLoad(serverLoadList, i, balanceInfo.getNumRegionsAdded());\n       if (balanceInfo.getHriList().size() + balanceInfo.getNumRegionsAdded() == max) {\n         RegionInfo hriToPlan;",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20350 NullPointerException in Scanner during close()",
        "commit": "https://github.com/apache/hbase/commit/281c29ff6070e08743ee96b0f7ce916bb1c3bba5",
        "parent": "https://github.com/apache/hbase/commit/199b392ec8fa1137876e2251bca566b52e426a04",
        "bug_id": "hbase_29",
        "file": [
            {
                "sha": "b0f42d7e1f31ec5dbf8b519c8abea96030fbc8f5",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java",
                "blob_url": "https://github.com/apache/hbase/blob/281c29ff6070e08743ee96b0f7ce916bb1c3bba5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java",
                "raw_url": "https://github.com/apache/hbase/raw/281c29ff6070e08743ee96b0f7ce916bb1c3bba5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java?ref=281c29ff6070e08743ee96b0f7ce916bb1c3bba5",
                "patch": "@@ -224,8 +224,8 @@ public void close() {\n       this.current.close();\n     }\n     if (this.heap != null) {\n-      KeyValueScanner scanner;\n-      while ((scanner = this.heap.poll()) != null) {\n+      // Order of closing the scanners shouldn't matter here, so simply iterate and close them.\n+      for (KeyValueScanner scanner : heap) {\n         scanner.close();\n       }\n     }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21494 NPE when loading RecoverStandByProcedure",
        "commit": "https://github.com/apache/hbase/commit/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
        "parent": "https://github.com/apache/hbase/commit/f555258e7abab1337ee4d39aaa1dafff72be287b",
        "bug_id": "hbase_30",
        "file": [
            {
                "sha": "e1d374090e0ef6e52e6dd93d20196ee6fbc32cb3",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "patch": "@@ -778,7 +778,6 @@ protected void initializeZKBasedSystemTrackers()\n     this.splitOrMergeTracker.start();\n \n     this.replicationPeerManager = ReplicationPeerManager.create(zooKeeper, conf);\n-    this.syncReplicationReplayWALManager = new SyncReplicationReplayWALManager(this);\n \n     this.drainingServerTracker = new DrainingServerTracker(zooKeeper, this, this.serverManager);\n     this.drainingServerTracker.start();\n@@ -949,7 +948,10 @@ private void finishActiveMasterInitialization(MonitoredTask status) throws IOExc\n     }\n \n     status.setStatus(\"Initialize ServerManager and schedule SCP for crash servers\");\n+    // The below two managers must be created before loading procedures, as they will be used during\n+    // loading.\n     this.serverManager = createServerManager(this);\n+    this.syncReplicationReplayWALManager = new SyncReplicationReplayWALManager(this);\n     createProcedureExecutor();\n     @SuppressWarnings(\"rawtypes\")\n     Map<Class<? extends Procedure>, List<Procedure<MasterProcedureEnv>>> procsByType =",
                "deletions": 1
            },
            {
                "sha": "ae624b145709c22f9a0088fed9a287ba1fc9d240",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "patch": "@@ -146,12 +146,12 @@ public SyncReplicationReplayWALManager(MasterServices services)\n     this.fs = services.getMasterFileSystem().getWALFileSystem();\n     this.walRootDir = services.getMasterFileSystem().getWALRootDir();\n     this.remoteWALDir = new Path(this.walRootDir, ReplicationUtils.REMOTE_WAL_DIR_NAME);\n-    MasterProcedureScheduler scheduler =\n-      services.getMasterProcedureExecutor().getEnvironment().getProcedureScheduler();\n     serverManager.registerListener(new ServerListener() {\n \n       @Override\n       public void serverAdded(ServerName serverName) {\n+        MasterProcedureScheduler scheduler =\n+          services.getMasterProcedureExecutor().getEnvironment().getProcedureScheduler();\n         for (UsedReplayWorkersForPeer usedWorkers : usedWorkersByPeer.values()) {\n           synchronized (usedWorkers) {\n             usedWorkers.wake(scheduler);",
                "deletions": 2
            },
            {
                "sha": "72aa32d22e4a799c3dc6f6b3861e00754638f286",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java",
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java",
                "status": "added",
                "changes": 127,
                "additions": 127,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestRegisterPeerWorkerWhenRestarting.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "patch": "@@ -0,0 +1,127 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.replication;\n+\n+import static org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyState.DISPATCH_WALS_VALUE;\n+import static org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyState.UNREGISTER_PEER_FROM_WORKER_STORAGE_VALUE;\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;\n+import org.apache.hadoop.hbase.replication.SyncReplicationState;\n+import org.apache.hadoop.hbase.replication.SyncReplicationTestBase;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;\n+import org.apache.zookeeper.KeeperException;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+/**\n+ * Testcase for HBASE-21494.\n+ */\n+@Category({ MasterTests.class, LargeTests.class })\n+public class TestRegisterPeerWorkerWhenRestarting extends SyncReplicationTestBase {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestRegisterPeerWorkerWhenRestarting.class);\n+\n+  private static volatile boolean FAIL = false;\n+\n+  public static final class HMasterForTest extends HMaster {\n+\n+    public HMasterForTest(Configuration conf) throws IOException, KeeperException {\n+      super(conf);\n+    }\n+\n+    @Override\n+    public void remoteProcedureCompleted(long procId) {\n+      if (FAIL && getMasterProcedureExecutor()\n+        .getProcedure(procId) instanceof SyncReplicationReplayWALRemoteProcedure) {\n+        throw new RuntimeException(\"Inject error\");\n+      }\n+      super.remoteProcedureCompleted(procId);\n+    }\n+  }\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    UTIL2.getConfiguration().setClass(HConstants.MASTER_IMPL, HMasterForTest.class, HMaster.class);\n+    SyncReplicationTestBase.setUp();\n+  }\n+\n+  @Test\n+  public void testRestart() throws Exception {\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.STANDBY);\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.ACTIVE);\n+\n+    UTIL1.getAdmin().disableReplicationPeer(PEER_ID);\n+    write(UTIL1, 0, 100);\n+    Thread.sleep(2000);\n+    // peer is disabled so no data have been replicated\n+    verifyNotReplicatedThroughRegion(UTIL2, 0, 100);\n+\n+    // transit the A to DA first to avoid too many error logs.\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.DOWNGRADE_ACTIVE);\n+    HMaster master = UTIL2.getHBaseCluster().getMaster();\n+    // make sure the transiting can not succeed\n+    FAIL = true;\n+    ProcedureExecutor<MasterProcedureEnv> procExec = master.getMasterProcedureExecutor();\n+    Thread t = new Thread() {\n+\n+      @Override\n+      public void run() {\n+        try {\n+          UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+            SyncReplicationState.DOWNGRADE_ACTIVE);\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      }\n+    };\n+    t.start();\n+    // wait until we are in the states where we need to register peer worker when restarting\n+    UTIL2.waitFor(60000,\n+      () -> procExec.getProcedures().stream().filter(p -> p instanceof RecoverStandbyProcedure)\n+        .map(p -> (RecoverStandbyProcedure) p)\n+        .anyMatch(p -> p.getCurrentStateId() == DISPATCH_WALS_VALUE ||\n+          p.getCurrentStateId() == UNREGISTER_PEER_FROM_WORKER_STORAGE_VALUE));\n+    // failover to another master\n+    MasterThread mt = UTIL2.getMiniHBaseCluster().getMasterThread();\n+    mt.getMaster().abort(\"for testing\");\n+    mt.join();\n+    FAIL = false;\n+    t.join();\n+    // make sure the new master can finish the transiting\n+    assertEquals(SyncReplicationState.DOWNGRADE_ACTIVE,\n+      UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID));\n+    verify(UTIL2, 0, 100);\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "9b73039c180c24887b58cac85c8a9b7c1cf42194",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java",
                "blob_url": "https://github.com/apache/hbase/blob/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java",
                "raw_url": "https://github.com/apache/hbase/raw/b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/replication/TestTransitPeerSyncReplicationStateProcedureRetry.java?ref=b329e6e3f271bc22ec4a6f4dd71a8e8b422db3d0",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.master.replication;\n \n+import static org.junit.Assert.assertEquals;\n+\n import java.io.IOException;\n import java.io.UncheckedIOException;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n@@ -90,5 +92,8 @@ public void run() {\n       .mapToLong(Procedure::getProcId).min().getAsLong();\n     MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId);\n     ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n+    assertEquals(SyncReplicationState.DOWNGRADE_ACTIVE,\n+      UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID));\n+    verify(UTIL2, 0, 100);\n   }\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21800: RegionServer aborted due to NPE from MetaTableMetrics coprocessor\n\nHave included code refactoring in MetaTableMetrics & LossyCounting",
        "commit": "https://github.com/apache/hbase/commit/abaeeace004caa73a585ba1120e21d162f4556fb",
        "parent": "https://github.com/apache/hbase/commit/6f16836c20802ff50ed3b0ab4bc4f1bfa791465e",
        "bug_id": "hbase_31",
        "file": [
            {
                "sha": "d08bae6762c030da0842ae84182bca0c0fbeb873",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java",
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java",
                "status": "modified",
                "changes": 78,
                "additions": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "patch": "@@ -75,50 +75,40 @@\n     @Override\n     public void preGetOp(ObserverContext<RegionCoprocessorEnvironment> e, Get get,\n         List<Cell> results) throws IOException {\n-      if (!active || !isMetaTableOp(e)) {\n-        return;\n-      }\n-      tableMetricRegisterAndMark(e, get);\n-      clientMetricRegisterAndMark(e);\n-      regionMetricRegisterAndMark(e, get);\n-      opMetricRegisterAndMark(e, get);\n-      opWithClientMetricRegisterAndMark(e, get);\n+      registerAndMarkMetrics(e, get);\n     }\n \n     @Override\n     public void prePut(ObserverContext<RegionCoprocessorEnvironment> e, Put put, WALEdit edit,\n         Durability durability) throws IOException {\n-      if (!active || !isMetaTableOp(e)) {\n-        return;\n-      }\n-      tableMetricRegisterAndMark(e, put);\n-      clientMetricRegisterAndMark(e);\n-      regionMetricRegisterAndMark(e, put);\n-      opMetricRegisterAndMark(e, put);\n-      opWithClientMetricRegisterAndMark(e, put);\n+      registerAndMarkMetrics(e, put);\n     }\n \n     @Override\n     public void preDelete(ObserverContext<RegionCoprocessorEnvironment> e, Delete delete,\n         WALEdit edit, Durability durability) throws IOException {\n+      registerAndMarkMetrics(e, delete);\n+    }\n+\n+    private void registerAndMarkMetrics(ObserverContext<RegionCoprocessorEnvironment> e, Row row){\n       if (!active || !isMetaTableOp(e)) {\n         return;\n       }\n-      tableMetricRegisterAndMark(e, delete);\n+      tableMetricRegisterAndMark(e, row);\n       clientMetricRegisterAndMark(e);\n-      regionMetricRegisterAndMark(e, delete);\n-      opMetricRegisterAndMark(e, delete);\n-      opWithClientMetricRegisterAndMark(e, delete);\n+      regionMetricRegisterAndMark(e, row);\n+      opMetricRegisterAndMark(e, row);\n+      opWithClientMetricRegisterAndMark(e, row);\n     }\n \n     private void markMeterIfPresent(String requestMeter) {\n       if (requestMeter.isEmpty()) {\n         return;\n       }\n-      Metric metric =\n-          requestsMap.get(requestMeter).isPresent() ? requestsMap.get(requestMeter).get() : null;\n-      if (metric != null) {\n-        ((Meter) metric).mark();\n+\n+      if (requestsMap.containsKey(requestMeter) && requestsMap.get(requestMeter).isPresent()) {\n+        Meter metric = (Meter) requestsMap.get(requestMeter).get();\n+        metric.mark();\n       }\n     }\n \n@@ -137,7 +127,7 @@ private void registerMeterIfNotPresent(ObserverContext<RegionCoprocessorEnvironm\n     /**\n      * Registers and counts lossyCount for Meters that kept by lossy counting.\n      * By using lossy count to maintain meters, at most 7 / e meters will be kept  (e is error rate)\n-     * e.g. when e is 0.02 by default, at most 50 Clients request metrics will be kept\n+     * e.g. when e is 0.02 by default, at most 350 Clients request metrics will be kept\n      *      also, all kept elements have frequency higher than e * N. (N is total count)\n      * @param e Region coprocessor environment\n      * @param requestMeter meter to be registered\n@@ -202,6 +192,7 @@ private boolean isMetaTableOp(ObserverContext<RegionCoprocessorEnvironment> e) {\n     }\n \n     private void clientMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e) {\n+      // Mark client metric\n       String clientIP = RpcServer.getRemoteIp() != null ? RpcServer.getRemoteIp().toString() : \"\";\n \n       String clientRequestMeter = clientRequestMeterName(clientIP);\n@@ -211,37 +202,43 @@ private void clientMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnviro\n \n     private void tableMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Row op) {\n-      // Mark the meta table meter whenever the coprocessor is called\n+      // Mark table metric\n       String tableName = getTableNameFromOp(op);\n       String tableRequestMeter = tableMeterName(tableName);\n-      registerMeterIfNotPresent(e, tableRequestMeter);\n-      markMeterIfPresent(tableRequestMeter);\n+      registerAndMarkMeterIfNotPresent(e, tableRequestMeter);\n     }\n \n     private void regionMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Row op) {\n-      // Mark the meta table meter whenever the coprocessor is called\n+      // Mark region metric\n       String regionId = getRegionIdFromOp(op);\n       String regionRequestMeter = regionMeterName(regionId);\n-      registerMeterIfNotPresent(e, regionRequestMeter);\n-      markMeterIfPresent(regionRequestMeter);\n+      registerAndMarkMeterIfNotPresent(e, regionRequestMeter);\n     }\n \n     private void opMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Row op) {\n+      // Mark access type [\"get\", \"put\", \"delete\"] metric\n       String opMeterName = opMeterName(op);\n-      registerMeterIfNotPresent(e, opMeterName);\n-      markMeterIfPresent(opMeterName);\n+      registerAndMarkMeterIfNotPresent(e, opMeterName);\n     }\n \n     private void opWithClientMetricRegisterAndMark(ObserverContext<RegionCoprocessorEnvironment> e,\n         Object op) {\n+      // // Mark client + access type metric\n       String opWithClientMeterName = opWithClientMeterName(op);\n-      registerMeterIfNotPresent(e, opWithClientMeterName);\n-      markMeterIfPresent(opWithClientMeterName);\n+      registerAndMarkMeterIfNotPresent(e, opWithClientMeterName);\n+    }\n+\n+    // Helper function to register and mark meter if not present\n+    private void registerAndMarkMeterIfNotPresent(ObserverContext<RegionCoprocessorEnvironment> e,\n+        String name) {\n+      registerMeterIfNotPresent(e, name);\n+      markMeterIfPresent(name);\n     }\n \n     private String opWithClientMeterName(Object op) {\n+      // Extract meter name containing the client IP\n       String clientIP = RpcServer.getRemoteIp() != null ? RpcServer.getRemoteIp().toString() : \"\";\n       if (clientIP.isEmpty()) {\n         return \"\";\n@@ -265,6 +262,7 @@ private String opWithClientMeterName(Object op) {\n     }\n \n     private String opMeterName(Object op) {\n+      // Extract meter name containing the access type\n       MetaTableOps ops = opsNameMap.get(op.getClass());\n       String opMeterName = \"\";\n       switch (ops) {\n@@ -284,17 +282,20 @@ private String opMeterName(Object op) {\n     }\n \n     private String tableMeterName(String tableName) {\n+      // Extract meter name containing the table name\n       return String.format(\"MetaTable_table_%s_request\", tableName);\n     }\n \n     private String clientRequestMeterName(String clientIP) {\n+      // Extract meter name containing the client IP\n       if (clientIP.isEmpty()) {\n         return \"\";\n       }\n       return String.format(\"MetaTable_client_%s_request\", clientIP);\n     }\n \n     private String regionMeterName(String regionId) {\n+      // Extract meter name containing the region ID\n       return String.format(\"MetaTable_region_%s_request\", regionId);\n     }\n   }\n@@ -306,30 +307,27 @@ private String regionMeterName(String regionId) {\n \n   @Override\n   public void start(CoprocessorEnvironment env) throws IOException {\n+    observer = new ExampleRegionObserverMeta();\n     if (env instanceof RegionCoprocessorEnvironment\n         && ((RegionCoprocessorEnvironment) env).getRegionInfo().getTable() != null\n         && ((RegionCoprocessorEnvironment) env).getRegionInfo().getTable()\n           .equals(TableName.META_TABLE_NAME)) {\n       regionCoprocessorEnv = (RegionCoprocessorEnvironment) env;\n-      observer = new ExampleRegionObserverMeta();\n       requestsMap = new ConcurrentHashMap<>();\n       clientMetricsLossyCounting = new LossyCounting();\n       // only be active mode when this region holds meta table.\n       active = true;\n-    } else {\n-      observer = new ExampleRegionObserverMeta();\n     }\n   }\n \n   @Override\n   public void stop(CoprocessorEnvironment env) throws IOException {\n     // since meta region can move around, clear stale metrics when stop.\n     if (requestsMap != null) {\n+      MetricRegistry registry = regionCoprocessorEnv.getMetricRegistryForRegionServer();\n       for (String meterName : requestsMap.keySet()) {\n-        MetricRegistry registry = regionCoprocessorEnv.getMetricRegistryForRegionServer();\n         registry.remove(meterName);\n       }\n     }\n   }\n-\n }",
                "deletions": 40
            },
            {
                "sha": "839bb90acf4cb765ab5f62d99df60ba104dde9b7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java",
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java",
                "status": "modified",
                "changes": 20,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/LossyCounting.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "patch": "@@ -24,7 +24,6 @@\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -61,27 +60,16 @@ public LossyCounting(double errorRate) {\n     this.bucketSize = (long) Math.ceil(1 / errorRate);\n     this.currentTerm = 1;\n     this.totalDataCount = 0;\n-    this.errorRate = errorRate;\n     this.data = new ConcurrentHashMap<>();\n     calculateCurrentTerm();\n   }\n \n   public LossyCounting() {\n-    Configuration conf = HBaseConfiguration.create();\n-    this.errorRate = conf.getDouble(HConstants.DEFAULT_LOSSY_COUNTING_ERROR_RATE, 0.02);\n-    this.bucketSize = (long) Math.ceil(1.0 / errorRate);\n-    this.currentTerm = 1;\n-    this.totalDataCount = 0;\n-    this.data = new ConcurrentHashMap<>();\n-    calculateCurrentTerm();\n+    this(HBaseConfiguration.create().getDouble(HConstants.DEFAULT_LOSSY_COUNTING_ERROR_RATE, 0.02));\n   }\n \n   public Set<String> addByOne(String key) {\n-    if(data.containsKey(key)) {\n-      data.put(key, data.get(key) +1);\n-    } else {\n-      data.put(key, 1);\n-    }\n+    data.put(key, data.getOrDefault(key, 0) + 1);\n     totalDataCount++;\n     calculateCurrentTerm();\n     Set<String> dataToBeSwept = new HashSet<>();\n@@ -105,7 +93,7 @@ public LossyCounting() {\n     for(String key : dataToBeSwept) {\n       data.remove(key);\n     }\n-    LOG.debug(String.format(\"Swept %d of elements.\", dataToBeSwept.size()));\n+    LOG.debug(String.format(\"Swept %d elements.\", dataToBeSwept.size()));\n     return dataToBeSwept;\n   }\n \n@@ -116,7 +104,7 @@ private void calculateCurrentTerm() {\n     this.currentTerm = (int) Math.ceil(1.0 * totalDataCount / bucketSize);\n   }\n \n-  public long getBuketSize(){\n+  public long getBucketSize(){\n     return bucketSize;\n   }\n ",
                "deletions": 16
            },
            {
                "sha": "bbbeb9e5273d6e474a41f455bf3ff98951f0cca9",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java",
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java",
                "status": "modified",
                "changes": 4,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMetaTableMetrics.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "patch": "@@ -222,10 +222,6 @@ public void test() throws IOException, InterruptedException {\n             jmxMetrics.stream().filter(metric -> metric.matches(putWithClientMetricNameRegex))\n                     .count();\n     assertEquals(5L, putWithClientMetricsCount);\n-\n-\n-\n-\n   }\n \n }",
                "deletions": 4
            },
            {
                "sha": "11758be7f5e1c765d130ff0ca84477b0cf93727b",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java",
                "blob_url": "https://github.com/apache/hbase/blob/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java",
                "raw_url": "https://github.com/apache/hbase/raw/abaeeace004caa73a585ba1120e21d162f4556fb/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestLossyCounting.java?ref=abaeeace004caa73a585ba1120e21d162f4556fb",
                "patch": "@@ -39,9 +39,9 @@\n   @Test\n   public void testBucketSize() {\n     LossyCounting lossyCounting = new LossyCounting(0.01);\n-    assertEquals(100L, lossyCounting.getBuketSize());\n+    assertEquals(100L, lossyCounting.getBucketSize());\n     LossyCounting lossyCounting2 = new LossyCounting();\n-    assertEquals(50L, lossyCounting2.getBuketSize());\n+    assertEquals(50L, lossyCounting2.getBucketSize());\n   }\n \n   @Test",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21422 NPE in TestMergeTableRegionsProcedure.testMergeWithoutPONR",
        "commit": "https://github.com/apache/hbase/commit/e7f6c2972dba2bc1eff8a5ae39893603508336ea",
        "parent": "https://github.com/apache/hbase/commit/ee55b558c0de0412b40ae65756b50ffb1bc49eee",
        "bug_id": "hbase_32",
        "file": [
            {
                "sha": "f0affd2b8aebcca828a4f1c2396833d634222718",
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "status": "modified",
                "changes": 14,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "patch": "@@ -92,29 +92,29 @@\n    * break PE having it fail at various junctures. When non-null, testing is set to an instance of\n    * the below internal {@link Testing} class with flags set for the particular test.\n    */\n-  Testing testing = null;\n+  volatile Testing testing = null;\n \n   /**\n    * Class with parameters describing how to fail/die when in testing-context.\n    */\n   public static class Testing {\n-    protected boolean killIfHasParent = true;\n-    protected boolean killIfSuspended = false;\n+    protected volatile boolean killIfHasParent = true;\n+    protected volatile boolean killIfSuspended = false;\n \n     /**\n      * Kill the PE BEFORE we store state to the WAL. Good for figuring out if a Procedure is\n      * persisting all the state it needs to recover after a crash.\n      */\n-    protected boolean killBeforeStoreUpdate = false;\n-    protected boolean toggleKillBeforeStoreUpdate = false;\n+    protected volatile boolean killBeforeStoreUpdate = false;\n+    protected volatile boolean toggleKillBeforeStoreUpdate = false;\n \n     /**\n      * Set when we want to fail AFTER state has been stored into the WAL. Rarely used. HBASE-20978\n      * is about a case where memory-state was being set after store to WAL where a crash could\n      * cause us to get stuck. This flag allows killing at what was a vulnerable time.\n      */\n-    protected boolean killAfterStoreUpdate = false;\n-    protected boolean toggleKillAfterStoreUpdate = false;\n+    protected volatile boolean killAfterStoreUpdate = false;\n+    protected volatile boolean toggleKillAfterStoreUpdate = false;\n \n     protected boolean shouldKillBeforeStoreUpdate() {\n       final boolean kill = this.killBeforeStoreUpdate;",
                "deletions": 7
            },
            {
                "sha": "452e08bc9c17d9a7b405fa1fb82068606e8e0c90",
                "filename": "hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "patch": "@@ -133,16 +133,15 @@ public static void initAndStartWorkers(ProcedureExecutor<?> procExecutor, int nu\n     if (actionBeforeStartWorker != null) {\n       actionBeforeStartWorker.call();\n     }\n+    if (avoidTestKillDuringRestart) {\n+      procExecutor.testing = testing;\n+    }\n     if (startWorkers) {\n       procExecutor.startWorkers();\n     }\n     if (startAction != null) {\n       startAction.call();\n     }\n-\n-    if (avoidTestKillDuringRestart) {\n-      procExecutor.testing = testing;\n-    }\n   }\n \n   public static void storeRestart(ProcedureStore procStore, ProcedureStore.ProcedureLoader loader)",
                "deletions": 4
            },
            {
                "sha": "6a8c4b3ae5f47c7dbb5b1a1c5497279314dbc29b",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java",
                "status": "modified",
                "changes": 41,
                "additions": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestMergeTableRegionsProcedure.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "patch": "@@ -275,36 +275,31 @@ public void testRollbackAndDoubleExecution() throws Exception {\n \n   @Test\n   public void testMergeWithoutPONR() throws Exception {\n-    try {\n-      final TableName tableName = TableName.valueOf(\"testMergeWithoutPONR\");\n-      final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    final TableName tableName = TableName.valueOf(\"testMergeWithoutPONR\");\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n \n-      List<RegionInfo> tableRegions = createTable(tableName);\n+    List<RegionInfo> tableRegions = createTable(tableName);\n \n-      ProcedureTestingUtility.waitNoProcedureRunning(procExec);\n-      ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+    ProcedureTestingUtility.waitNoProcedureRunning(procExec);\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n \n-      RegionInfo[] regionsToMerge = new RegionInfo[2];\n-      regionsToMerge[0] = tableRegions.get(0);\n-      regionsToMerge[1] = tableRegions.get(1);\n+    RegionInfo[] regionsToMerge = new RegionInfo[2];\n+    regionsToMerge[0] = tableRegions.get(0);\n+    regionsToMerge[1] = tableRegions.get(1);\n \n-      long procId = procExec.submitProcedure(\n-        new MergeTableRegionsProcedure(procExec.getEnvironment(), regionsToMerge, true));\n+    long procId = procExec.submitProcedure(\n+      new MergeTableRegionsProcedure(procExec.getEnvironment(), regionsToMerge, true));\n \n-      // Execute until step 9 of split procedure\n-      // NOTE: step 9 is after step MERGE_TABLE_REGIONS_UPDATE_META\n-      MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId, 9, false);\n+    // Execute until step 9 of split procedure\n+    // NOTE: step 9 is after step MERGE_TABLE_REGIONS_UPDATE_META\n+    MasterProcedureTestingUtility.testRecoveryAndDoubleExecution(procExec, procId, 9, false);\n \n-      // Unset Toggle Kill and make ProcExec work correctly\n-      ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n-      MasterProcedureTestingUtility.restartMasterProcedureExecutor(procExec);\n-      ProcedureTestingUtility.waitProcedure(procExec, procId);\n+    // Unset Toggle Kill and make ProcExec work correctly\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, false);\n+    MasterProcedureTestingUtility.restartMasterProcedureExecutor(procExec);\n+    ProcedureTestingUtility.waitProcedure(procExec, procId);\n \n-      assertRegionCount(tableName, initialRegionCount - 1);\n-    } catch (Throwable t) {\n-      LOG.error(\"error!\", t);\n-      throw t;\n-    }\n+    assertRegionCount(tableName, initialRegionCount - 1);\n   }\n \n   private List<RegionInfo> createTable(final TableName tableName) throws Exception {",
                "deletions": 23
            },
            {
                "sha": "98c39781432a653a9ca4a6efd8a17fe96d24e534",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureTestingUtility.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "patch": "@@ -110,8 +110,12 @@ public Void call() throws Exception {\n         @Override\n         public Void call() throws Exception {\n           AssignmentManager am = env.getAssignmentManager();\n-          am.joinCluster();\n-          master.setInitialized(true);\n+          try {\n+            am.joinCluster();\n+            master.setInitialized(true);\n+          } catch (Exception e) {\n+            LOG.warn(\"Failed to load meta\", e);\n+          }\n           return null;\n         }\n       });",
                "deletions": 2
            },
            {
                "sha": "0e4a84b5c7669b210d6af06f16adc66e2bc5eb42",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/e7f6c2972dba2bc1eff8a5ae39893603508336ea/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestServerCrashProcedure.java?ref=e7f6c2972dba2bc1eff8a5ae39893603508336ea",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.TableName;\n@@ -63,7 +64,8 @@\n   private void setupConf(Configuration conf) {\n     conf.setInt(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 1);\n     conf.set(\"hbase.balancer.tablesOnMaster\", \"none\");\n-    conf.setInt(\"hbase.client.retries.number\", 3);\n+    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 3);\n+    conf.setInt(HConstants.HBASE_CLIENT_SERVERSIDE_RETRIES_MULTIPLIER, 3);\n   }\n \n   @Before",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"HBASE-17094 Add a sitemap for hbase.apache.org\"\n\nIssue with doc generation... failing w/ NPE on sitemap generation.\n\nThis reverts commit 02690b429ff3ed2504b43a8c47d0242da5af0f5e.",
        "commit": "https://github.com/apache/hbase/commit/d152e9420905fa5810748c3b48bbcacf24e86283",
        "parent": "https://github.com/apache/hbase/commit/9af8d58d4abe145cef35c3d4abb75b2cb2510451",
        "bug_id": "hbase_33",
        "file": [
            {
                "sha": "5a3b9cf99a0fc39f3abc5fd5dea199f646aae576",
                "filename": "pom.xml",
                "blob_url": "https://github.com/apache/hbase/blob/d152e9420905fa5810748c3b48bbcacf24e86283/pom.xml",
                "raw_url": "https://github.com/apache/hbase/raw/d152e9420905fa5810748c3b48bbcacf24e86283/pom.xml",
                "status": "modified",
                "changes": 17,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/pom.xml?ref=d152e9420905fa5810748c3b48bbcacf24e86283",
                "patch": "@@ -2339,12 +2339,6 @@\n       <build>\n         <pluginManagement>\n           <plugins>\n-\t     <plugin>\n-              <groupId>com.github.s4u.plugins</groupId>\n-              <artifactId>sitemapxml-maven-plugin</artifactId>\n-              <version>1.0.0</version>\n-\t       <inherited>false</inherited>\n-            </plugin>\n             <plugin>\n               <groupId>org.codehaus.mojo</groupId>\n               <artifactId>findbugs-maven-plugin</artifactId>\n@@ -2403,17 +2397,6 @@\n       </properties>\n       <build>\n         <plugins>\n-          <plugin>\n-            <groupId>com.github.s4u.plugins</groupId>\n-            <artifactId>sitemapxml-maven-plugin</artifactId>\n-            <executions>\n-              <execution>\n-                <goals>\n-                  <goal>gen</goal>\n-                </goals>\n-              </execution>\n-            </executions>\n-          </plugin>\n           <plugin>\n             <groupId>org.apache.maven.plugins</groupId>\n             <artifactId>maven-antrun-plugin</artifactId>",
                "deletions": 17
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21592 quota.addGetResult(r) throw NPE\n\nSigned-off-by: huzheng <openinx@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/f78284685fc533230a0395d297ebacff32632396",
        "parent": "https://github.com/apache/hbase/commit/1971d02e725341fdee79b7ee2308a9870debe2f6",
        "bug_id": "hbase_34",
        "file": [
            {
                "sha": "f788a86f47b2602c2d8e8040a56d1848663644da",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=f78284685fc533230a0395d297ebacff32632396",
                "patch": "@@ -2571,7 +2571,8 @@ public GetResponse get(final RpcController controller, final GetRequest request)\n         }\n         builder.setResult(pbr);\n       }\n-      if (r != null) {\n+      //r.cells is null when an table.exists(get) call\n+      if (r != null && r.rawCells() != null) {\n         quota.addGetResult(r);\n       }\n       return builder.build();",
                "deletions": 1
            },
            {
                "sha": "c0694031489d738cb7be7f4d9f64c4ecff35dd34",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java",
                "blob_url": "https://github.com/apache/hbase/blob/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java",
                "raw_url": "https://github.com/apache/hbase/raw/f78284685fc533230a0395d297ebacff32632396/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java",
                "status": "modified",
                "changes": 17,
                "additions": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/quotas/TestQuotaThrottle.java?ref=f78284685fc533230a0395d297ebacff32632396",
                "patch": "@@ -553,6 +553,23 @@ public void testTableReadCapacityUnitThrottle() throws Exception {\n     triggerTableCacheRefresh(true, TABLE_NAMES[0]);\n   }\n \n+  @Test\n+  public void testTableExistsGetThrottle() throws Exception {\n+    final Admin admin = TEST_UTIL.getAdmin();\n+\n+    // Add throttle quota\n+    admin.setQuota(QuotaSettingsFactory.throttleTable(TABLE_NAMES[0],\n+        ThrottleType.REQUEST_NUMBER, 100, TimeUnit.MINUTES));\n+    triggerTableCacheRefresh(false, TABLE_NAMES[0]);\n+\n+    Table table = TEST_UTIL.getConnection().getTable(TABLE_NAMES[0]);\n+    // An exists call when having throttle quota\n+    table.exists(new Get(Bytes.toBytes(\"abc\")));\n+\n+    admin.setQuota(QuotaSettingsFactory.unthrottleTable(TABLE_NAMES[0]));\n+    triggerTableCacheRefresh(true, TABLE_NAMES[0]);\n+  }\n+\n   private int doPuts(int maxOps, final Table... tables) throws Exception {\n     return doPuts(maxOps, -1, tables);\n   }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21749 RS UI may throw NPE and make rs-status page inaccessible with multiwal and replication\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839",
        "parent": "https://github.com/apache/hbase/commit/35df6147eef0baf25e127df358f6e50bbf967e2c",
        "bug_id": "hbase_35",
        "file": [
            {
                "sha": "f1b6e766877fe499a59dd6d6fd5f84c1ab9350ee",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "blob_url": "https://github.com/apache/hbase/blob/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "raw_url": "https://github.com/apache/hbase/raw/f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "status": "modified",
                "changes": 15,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=f2820ea16fcadc9c3c4d6a8f8e2c42d52223c839",
                "patch": "@@ -329,11 +329,16 @@ private void tryStartNewShipper(String walGroupId, PriorityBlockingQueue<Path> q\n       replicationDelay =\n           ReplicationLoad.calculateReplicationDelay(ageOfLastShippedOp, lastTimeStamp, queueSize);\n       Path currentPath = shipper.getCurrentPath();\n-      try {\n-        fileSize = getFileSize(currentPath);\n-      } catch (IOException e) {\n-        LOG.warn(\"Ignore the exception as the file size of HLog only affects the web ui\", e);\n-        fileSize = -1;\n+      fileSize = -1;\n+      if (currentPath != null) {\n+        try {\n+          fileSize = getFileSize(currentPath);\n+        } catch (IOException e) {\n+          LOG.warn(\"Ignore the exception as the file size of HLog only affects the web ui\", e);\n+        }\n+      } else {\n+        currentPath = new Path(\"NO_LOGS_IN_QUEUE\");\n+        LOG.warn(\"No replication ongoing, waiting for new log\");\n       }\n       ReplicationStatus.ReplicationStatusBuilder statusBuilder = ReplicationStatus.newBuilder();\n       statusBuilder.withPeerId(this.getPeerId())",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21136 NPE in MetricsTableSourceImpl.updateFlushTime",
        "commit": "https://github.com/apache/hbase/commit/dc79029966c72f6c46add8c382e118308609cc81",
        "parent": "https://github.com/apache/hbase/commit/9c09efc0df4b2f60b359bad00fed27e7980cf92e",
        "bug_id": "hbase_36",
        "file": [
            {
                "sha": "5133a96db10ae9fe6a2ff35287baed826dade3cb",
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java",
                "status": "modified",
                "changes": 28,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableAggregateSourceImpl.java?ref=dc79029966c72f6c46add8c382e118308609cc81",
                "patch": "@@ -48,19 +48,15 @@ public MetricsTableAggregateSourceImpl(String metricsName,\n   }\n \n   private void register(MetricsTableSource source) {\n-    synchronized (this) {\n-      source.registerMetrics();\n-    }\n+    source.registerMetrics();\n   }\n \n   @Override\n   public void deleteTableSource(String table) {\n     try {\n-      synchronized (this) {\n-        MetricsTableSource source = tableSources.remove(table);\n-        if (source != null) {\n-          source.close();\n-        }\n+      MetricsTableSource source = tableSources.remove(table);\n+      if (source != null) {\n+        source.close();\n       }\n     } catch (Exception e) {\n       // Ignored. If this errors out it means that someone is double\n@@ -76,17 +72,13 @@ public MetricsTableSource getOrCreateTableSource(String table,\n     if (source != null) {\n       return source;\n     }\n-    source = CompatibilitySingletonFactory.getInstance(MetricsRegionServerSourceFactory.class)\n-      .createTable(table, wrapper);\n-    MetricsTableSource prev = tableSources.putIfAbsent(table, source);\n-\n-    if (prev != null) {\n-      return prev;\n-    } else {\n+    MetricsTableSource newSource = CompatibilitySingletonFactory\n+      .getInstance(MetricsRegionServerSourceFactory.class).createTable(table, wrapper);\n+    return tableSources.computeIfAbsent(table, k -> {\n       // register the new metrics now\n-      register(source);\n-    }\n-    return source;\n+      newSource.registerMetrics();\n+      return newSource;\n+    });\n   }\n \n   /**",
                "deletions": 18
            },
            {
                "sha": "3da16b88bcbc50ca4c3da3532f74f3159510f4c6",
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/dc79029966c72f6c46add8c382e118308609cc81/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java",
                "status": "modified",
                "changes": 33,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsTableSourceImpl.java?ref=dc79029966c72f6c46add8c382e118308609cc81",
                "patch": "@@ -15,21 +15,8 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-\n package org.apache.hadoop.hbase.regionserver;\n \n-import java.util.concurrent.atomic.AtomicBoolean;\n-\n-import org.apache.hadoop.hbase.TableName;\n-import org.apache.hadoop.hbase.metrics.Interns;\n-import org.apache.hadoop.metrics2.MetricsRecordBuilder;\n-import org.apache.hadoop.metrics2.MetricHistogram;\n-import org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry;\n-import org.apache.hadoop.metrics2.lib.MutableFastCounter;\n-import org.apache.yetus.audience.InterfaceAudience;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.COMPACTED_INPUT_BYTES;\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.COMPACTED_INPUT_BYTES_DESC;\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.COMPACTED_OUTPUT_BYTES;\n@@ -74,6 +61,17 @@\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.SPLIT_SUCCESS_DESC;\n import static org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.SPLIT_SUCCESS_KEY;\n \n+import java.util.concurrent.atomic.AtomicBoolean;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.metrics.Interns;\n+import org.apache.hadoop.metrics2.MetricHistogram;\n+import org.apache.hadoop.metrics2.MetricsRecordBuilder;\n+import org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry;\n+import org.apache.hadoop.metrics2.lib.MutableFastCounter;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n @InterfaceAudience.Private\n public class MetricsTableSourceImpl implements MetricsTableSource {\n \n@@ -123,7 +121,7 @@\n \n   public MetricsTableSourceImpl(String tblName,\n       MetricsTableAggregateSourceImpl aggregate, MetricsTableWrapperAggregate tblWrapperAgg) {\n-    LOG.debug(\"Creating new MetricsTableSourceImpl for table \");\n+    LOG.debug(\"Creating new MetricsTableSourceImpl for table '{}'\", tblName);\n     this.tableName = TableName.valueOf(tblName);\n     this.agg = aggregate;\n \n@@ -240,17 +238,11 @@ public int compareTo(MetricsTableSource source) {\n     if (!(source instanceof MetricsTableSourceImpl)) {\n       return -1;\n     }\n-\n     MetricsTableSourceImpl impl = (MetricsTableSourceImpl) source;\n-    if (impl == null) {\n-      return -1;\n-    }\n-\n     return Long.compare(hashCode, impl.hashCode);\n   }\n \n   void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n-\n     // If there is a close that started be double extra sure\n     // that we're not getting any locks and not putting data\n     // into the metrics that should be removed. So early out\n@@ -263,7 +255,6 @@ void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n     // This ensures that removes of the metrics\n     // can't happen while we are putting them back in.\n     synchronized (this) {\n-\n       // It's possible that a close happened between checking\n       // the closed variable and getting the lock.\n       if (closed.get()) {",
                "deletions": 21
            },
            {
                "sha": "71c3e71882f54f8ffbd51720ccab79de309e2b52",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java",
                "blob_url": "https://github.com/apache/hbase/blob/dc79029966c72f6c46add8c382e118308609cc81/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java",
                "raw_url": "https://github.com/apache/hbase/raw/dc79029966c72f6c46add8c382e118308609cc81/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java",
                "status": "modified",
                "changes": 44,
                "additions": 42,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsTableAggregate.java?ref=dc79029966c72f6c46add8c382e118308609cc81",
                "patch": "@@ -17,7 +17,14 @@\n  */\n package org.apache.hadoop.hbase.regionserver;\n \n+import static org.junit.Assert.assertTrue;\n+\n import java.io.IOException;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.CompatibilityFactory;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n@@ -29,15 +36,19 @@\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n-@Category({RegionServerTests.class, SmallTests.class})\n+@Category({ RegionServerTests.class, SmallTests.class })\n public class TestMetricsTableAggregate {\n \n   @ClassRule\n   public static final HBaseClassTestRule CLASS_RULE =\n     HBaseClassTestRule.forClass(TestMetricsTableAggregate.class);\n \n-  public static MetricsAssertHelper HELPER =\n+  private static final Logger LOG = LoggerFactory.getLogger(TestMetricsTableAggregate.class);\n+\n+  private static MetricsAssertHelper HELPER =\n     CompatibilityFactory.getInstance(MetricsAssertHelper.class);\n \n   private String tableName = \"testTableMetrics\";\n@@ -87,6 +98,7 @@ public void testRegionAndStoreMetrics() throws IOException {\n     HELPER.assertGauge(pre + \"averageRegionSize\", 88, agg);\n   }\n \n+  @Test\n   public void testFlush() {\n     rsm.updateFlush(tableName, 1, 2, 3);\n     HELPER.assertCounter(pre + \"flushTime_num_ops\", 1, agg);\n@@ -139,4 +151,32 @@ public void testCompaction() {\n     HELPER.assertCounter(pre + \"majorCompactedoutputBytes\", 500, agg);\n   }\n \n+  private void update(AtomicBoolean succ, int round, CyclicBarrier barrier) {\n+    try {\n+      for (int i = 0; i < round; i++) {\n+        String tn = tableName + \"-\" + i;\n+        barrier.await(10, TimeUnit.SECONDS);\n+        rsm.updateFlush(tn, 100, 1000, 500);\n+      }\n+    } catch (Exception e) {\n+      LOG.warn(\"Failed to update metrics\", e);\n+      succ.set(false);\n+    }\n+  }\n+\n+  @Test\n+  public void testConcurrentUpdate() throws InterruptedException {\n+    int threadNumber = 10;\n+    int round = 100;\n+    AtomicBoolean succ = new AtomicBoolean(true);\n+    CyclicBarrier barrier = new CyclicBarrier(threadNumber);\n+    Thread[] threads = IntStream.range(0, threadNumber)\n+      .mapToObj(i -> new Thread(() -> update(succ, round, barrier), \"Update-Worker-\" + i))\n+      .toArray(Thread[]::new);\n+    Stream.of(threads).forEach(Thread::start);\n+    for (Thread t : threads) {\n+      t.join();\n+    }\n+    assertTrue(succ.get());\n+  }\n }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20135 Fixed NullPointerException during reading bloom filter when upgraded from hbase-1 to hbase-2",
        "commit": "https://github.com/apache/hbase/commit/b8f999bf33678e6daa7ce8072ae40824c5ea25d1",
        "parent": "https://github.com/apache/hbase/commit/65bd0881e25981a41373ba70a5929dc15cf1c037",
        "bug_id": "hbase_37",
        "file": [
            {
                "sha": "3c74d114e1e8d80903c292786ce1c7fd8f2e4657",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "blob_url": "https://github.com/apache/hbase/blob/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "raw_url": "https://github.com/apache/hbase/raw/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "status": "modified",
                "changes": 12,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=b8f999bf33678e6daa7ce8072ae40824c5ea25d1",
                "patch": "@@ -38,6 +38,8 @@\n import org.apache.hadoop.hbase.shaded.protobuf.generated.HFileProtos;\n import org.apache.hadoop.hbase.util.Bytes;\n \n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * The {@link HFile} has a fixed trailer which contains offsets to other\n@@ -53,6 +55,8 @@\n  */\n @InterfaceAudience.Private\n public class FixedFileTrailer {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedFileTrailer.class);\n+\n   /**\n    * We store the comparator class name as a fixed-length field in the trailer.\n    */\n@@ -623,7 +627,13 @@ private String getHBase1CompatibleName(final String comparator) {\n   public static CellComparator createComparator(\n       String comparatorClassName) throws IOException {\n     try {\n-      return getComparatorClass(comparatorClassName).getDeclaredConstructor().newInstance();\n+\n+      Class<? extends CellComparator> comparatorClass = getComparatorClass(comparatorClassName);\n+      if(comparatorClass != null){\n+        return comparatorClass.getDeclaredConstructor().newInstance();\n+      }\n+      LOG.warn(\"No Comparator class for \" + comparatorClassName + \". Returning Null.\");\n+      return null;\n     } catch (Exception e) {\n       throw new IOException(\"Comparator class \" + comparatorClassName +\n         \" is not instantiable\", e);",
                "deletions": 1
            },
            {
                "sha": "d25ce4796d7ccbd9ac19e3f48b6bcba8525776a9",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java",
                "blob_url": "https://github.com/apache/hbase/blob/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java",
                "raw_url": "https://github.com/apache/hbase/raw/b8f999bf33678e6daa7ce8072ae40824c5ea25d1/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java",
                "status": "modified",
                "changes": 35,
                "additions": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java?ref=b8f999bf33678e6daa7ce8072ae40824c5ea25d1",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hbase.io.hfile;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n@@ -33,6 +34,7 @@\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.CellComparator;\n import org.apache.hadoop.hbase.CellComparatorImpl;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n@@ -43,8 +45,10 @@\n import org.apache.hadoop.hbase.util.Bytes;\n import org.junit.Before;\n import org.junit.ClassRule;\n+import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n+import org.junit.rules.ExpectedException;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n import org.junit.runners.Parameterized.Parameters;\n@@ -82,6 +86,9 @@ public TestFixedFileTrailer(int version) {\n     this.version = version;\n   }\n \n+  @Rule\n+  public ExpectedException expectedEx = ExpectedException.none();\n+\n   @Parameters\n   public static Collection<Object[]> getParameters() {\n     List<Object[]> versionsToTest = new ArrayList<>();\n@@ -108,6 +115,34 @@ public void testComparatorIsHBase1Compatible() {\n         pb.getComparatorClassName());\n   }\n \n+  @Test\n+  public void testCreateComparator() throws IOException {\n+    FixedFileTrailer t = new FixedFileTrailer(version, HFileReaderImpl.PBUF_TRAILER_MINOR_VERSION);\n+    try {\n+      assertEquals(CellComparatorImpl.class,\n+          t.createComparator(KeyValue.COMPARATOR.getLegacyKeyComparatorName()).getClass());\n+      assertEquals(CellComparatorImpl.class,\n+          t.createComparator(KeyValue.COMPARATOR.getClass().getName()).getClass());\n+      assertEquals(CellComparatorImpl.class,\n+          t.createComparator(CellComparator.class.getName()).getClass());\n+      assertEquals(CellComparatorImpl.MetaCellComparator.class,\n+          t.createComparator(KeyValue.META_COMPARATOR.getLegacyKeyComparatorName()).getClass());\n+      assertEquals(CellComparatorImpl.MetaCellComparator.class,\n+          t.createComparator(KeyValue.META_COMPARATOR.getClass().getName()).getClass());\n+      assertEquals(CellComparatorImpl.MetaCellComparator.class, t.createComparator(\n+          CellComparatorImpl.MetaCellComparator.META_COMPARATOR.getClass().getName()).getClass());\n+      assertNull(t.createComparator(Bytes.BYTES_RAWCOMPARATOR.getClass().getName()));\n+      assertNull(t.createComparator(\"org.apache.hadoop.hbase.KeyValue$RawBytesComparator\"));\n+    } catch (IOException e) {\n+      fail(\"Unexpected exception while testing FixedFileTrailer#createComparator()\");\n+    }\n+\n+    // Test an invalid comparatorClassName\n+    expectedEx.expect(IOException.class);\n+    t.createComparator(\"\");\n+\n+  }\n+\n   @Test\n   public void testTrailer() throws IOException {\n     FixedFileTrailer t = new FixedFileTrailer(version,",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21407 Resolve NPE in backup Master UI\n\nSigned-off-by: Michael Stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/25c964e9a325762191af23bcacfcdb64144f87ee",
        "parent": "https://github.com/apache/hbase/commit/e7f6c2972dba2bc1eff8a5ae39893603508336ea",
        "bug_id": "hbase_38",
        "file": [
            {
                "sha": "0df70998d11d54a82ee2029d77c1ae90e5da62b8",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/header.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/25c964e9a325762191af23bcacfcdb64144f87ee/hbase-server/src/main/resources/hbase-webapps/master/header.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/25c964e9a325762191af23bcacfcdb64144f87ee/hbase-server/src/main/resources/hbase-webapps/master/header.jsp",
                "status": "modified",
                "changes": 10,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/header.jsp?ref=25c964e9a325762191af23bcacfcdb64144f87ee",
                "patch": "@@ -18,7 +18,11 @@\n */\n --%>\n <%@ page contentType=\"text/html;charset=UTF-8\"\n-  import=\"org.apache.hadoop.hbase.HBaseConfiguration\"\n+    import=\"org.apache.hadoop.hbase.master.HMaster\"\n+    import=\"org.apache.hadoop.hbase.HBaseConfiguration\"\n+%>\n+<%\n+  HMaster master = (HMaster) getServletContext().getAttribute(HMaster.MASTER);\n %>\n <!DOCTYPE html>\n <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n@@ -52,7 +56,9 @@\n           <ul class=\"nav navbar-nav\">\n             <li><a href=\"/master-status\">Home</a></li>\n             <li><a href=\"/tablesDetailed.jsp\">Table Details</a></li>\n-            <li><a href=\"/procedures.jsp\">Procedures</a></li>\n+            <% if (master.isActiveMaster()){ %>\n+            <li><a href=\"/procedures.jsp\">Procedures &amp; Locks</a></li>\n+            <% }%>\n             <li><a href=\"/processMaster.jsp\">Process Metrics</a></li>\n             <li><a href=\"/logs/\">Local Logs</a></li>\n             <li><a href=\"/logLevel\">Log Level</a></li>",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-17351 Enforcer plugin fails with NullPointerException",
        "commit": "https://github.com/apache/hbase/commit/fb3c8bf6087b1d9e086a6becd1c0e518cb1f8936",
        "parent": "https://github.com/apache/hbase/commit/9c5b03acd72713f1a9956086ad0a6f4b389deaae",
        "bug_id": "hbase_39",
        "file": [
            {
                "sha": "019501c7e1ed5baec6f9409c69ca2164da0c28ca",
                "filename": "pom.xml",
                "blob_url": "https://github.com/apache/hbase/blob/fb3c8bf6087b1d9e086a6becd1c0e518cb1f8936/pom.xml",
                "raw_url": "https://github.com/apache/hbase/raw/fb3c8bf6087b1d9e086a6becd1c0e518cb1f8936/pom.xml",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/pom.xml?ref=fb3c8bf6087b1d9e086a6becd1c0e518cb1f8936",
                "patch": "@@ -812,7 +812,7 @@\n                   <pluginExecutionFilter>\n                     <groupId>org.apache.maven.plugins</groupId>\n                     <artifactId>maven-enforcer-plugin</artifactId>\n-                    <versionRange>[1.0.1,)</versionRange>\n+                    <version>${enforcer.version}</version>\n                     <goals>\n                       <goal>enforce</goal>\n                     </goals>\n@@ -971,6 +971,7 @@\n       <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-enforcer-plugin</artifactId>\n+        <version>${enforcer.version}</version>\n         <dependencies>\n           <dependency>\n             <groupId>org.codehaus.mojo</groupId>\n@@ -1517,6 +1518,8 @@\n     <argLine>${hbase-surefire.argLine}</argLine>\n     <jacoco.version>0.7.5.201505241946</jacoco.version>\n     <extra.enforcer.version>1.0-beta-6</extra.enforcer.version>\n+    <!--See HBASE-17351; we need to set version to 1.4. 1.4.1 fails with MENFORCER-248-->\n+    <enforcer.version>1.4</enforcer.version>\n     <!-- Location of test resources -->\n     <test.build.classes>${project.build.directory}/test-classes</test.build.classes>\n     <maven.build.timestamp.format>yyyy-MM-dd'T'HH:mm:ss'Z'</maven.build.timestamp.format>\n@@ -2314,6 +2317,7 @@\n           <plugin>\n             <groupId>org.apache.maven.plugins</groupId>\n             <artifactId>maven-enforcer-plugin</artifactId>\n+            <version>${enforcer.version}</version>\n             <configuration>\n               <rules>\n                 <enforceBytecodeVersion>",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20921 Possible NPE in ReopenTableRegionsProcedure",
        "commit": "https://github.com/apache/hbase/commit/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
        "parent": "https://github.com/apache/hbase/commit/d43e28dc8269d19596aaf801de8e63c8bbd8b68f",
        "bug_id": "hbase_40",
        "file": [
            {
                "sha": "9f012932c482f268fa72cc73dbd19d16433cd985",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "blob_url": "https://github.com/apache/hbase/blob/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "raw_url": "https://github.com/apache/hbase/raw/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/RegionStates.java?ref=80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
                "patch": "@@ -643,6 +643,12 @@ private HRegionLocation createRegionForReopen(RegionStateNode node) {\n    */\n   public HRegionLocation checkReopened(HRegionLocation oldLoc) {\n     RegionStateNode node = getRegionStateNode(oldLoc.getRegion());\n+    // HBASE-20921\n+    // if the oldLoc's state node does not exist, that means the region is\n+    // merged or split, no need to check it\n+    if (node == null) {\n+      return null;\n+    }\n     synchronized (node) {\n       if (oldLoc.getSeqNum() >= 0) {\n         // in OPEN state before",
                "deletions": 0
            },
            {
                "sha": "8f3aa22357624681b7f13fc9794665325bb1ca00",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.java?ref=80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
                "patch": "@@ -124,7 +124,7 @@ protected Flow executeFromState(MasterProcedureEnv env, ReopenTableRegionsState\n   @Override\n   protected void rollbackState(MasterProcedureEnv env, ReopenTableRegionsState state)\n       throws IOException, InterruptedException {\n-    throw new UnsupportedOperationException();\n+    throw new UnsupportedOperationException(\"unhandled state=\" + state);\n   }\n \n   @Override",
                "deletions": 1
            },
            {
                "sha": "16ad37315940351590d36ecd88b12970972459bc",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java",
                "blob_url": "https://github.com/apache/hbase/blob/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java",
                "raw_url": "https://github.com/apache/hbase/raw/80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java",
                "status": "added",
                "changes": 109,
                "additions": 109,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/assignment/TestModifyTableWhileMerging.java?ref=80b40a3b588d9d250c1f3fd0ce4ee50376fbe25e",
                "patch": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.assignment;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureConstants;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure;\n+import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;\n+\n+\n+@Category({MasterTests.class, MediumTests.class})\n+public class TestModifyTableWhileMerging {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestModifyTableWhileMerging.class);\n+\n+  private static final Logger LOG = LoggerFactory\n+      .getLogger(TestModifyTableWhileMerging.class);\n+\n+  protected static final HBaseTestingUtility UTIL = new HBaseTestingUtility();\n+  private static TableName TABLE_NAME = TableName.valueOf(\"test\");\n+  private static Admin admin;\n+  private static Table client;\n+  private static byte[] CF = Bytes.toBytes(\"cf\");\n+  private static byte[] SPLITKEY = Bytes.toBytes(\"bbbbbbb\");\n+\n+  @BeforeClass\n+  public static void setupCluster() throws Exception {\n+    //Set procedure executor thread to 1, making reproducing this issue of HBASE-20921 easier\n+    UTIL.getConfiguration().setInt(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 1);\n+    UTIL.startMiniCluster(1);\n+    admin = UTIL.getHBaseAdmin();\n+    byte[][] splitKeys = new byte[1][];\n+    splitKeys[0] = SPLITKEY;\n+    client = UTIL.createTable(TABLE_NAME, CF, splitKeys);\n+    UTIL.waitTableAvailable(TABLE_NAME);\n+  }\n+\n+  @AfterClass\n+  public static void cleanupTest() throws Exception {\n+    try {\n+      UTIL.shutdownMiniCluster();\n+    } catch (Exception e) {\n+      LOG.warn(\"failure shutting down cluster\", e);\n+    }\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    TableDescriptor tableDescriptor = client.getDescriptor();\n+    ProcedureExecutor<MasterProcedureEnv> executor = UTIL.getMiniHBaseCluster().getMaster()\n+        .getMasterProcedureExecutor();\n+    MasterProcedureEnv env = executor.getEnvironment();\n+    List<RegionInfo> regionInfos = admin.getRegions(TABLE_NAME);\n+    MergeTableRegionsProcedure mergeTableRegionsProcedure = new MergeTableRegionsProcedure(\n+      UTIL.getMiniHBaseCluster().getMaster().getMasterProcedureExecutor()\n+        .getEnvironment(), regionInfos.get(0), regionInfos.get(1));\n+    ModifyTableProcedure modifyTableProcedure = new ModifyTableProcedure(env, tableDescriptor);\n+    long procModify = executor.submitProcedure(modifyTableProcedure);\n+    UTIL.waitFor(30000, () -> executor.getProcedures().stream()\n+      .filter(p -> p instanceof ModifyTableProcedure)\n+      .map(p -> (ModifyTableProcedure) p)\n+      .anyMatch(p -> TABLE_NAME.equals(p.getTableName())));\n+    long proc = executor.submitProcedure(mergeTableRegionsProcedure);\n+    UTIL.waitFor(3000000, () -> UTIL.getMiniHBaseCluster().getMaster()\n+        .getMasterProcedureExecutor().isFinished(procModify));\n+    Assert.assertEquals(\"Modify Table procedure should success!\",\n+        ProcedureProtos.ProcedureState.SUCCESS, modifyTableProcedure.getState());\n+  }\n+\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21175 Partially initialized SnapshotHFileCleaner leads to NPE during TestHFileArchiving\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/7cdb52519236966a7cb6dff7fbd0609c87545f75",
        "parent": "https://github.com/apache/hbase/commit/e5ba79816a21036654a62e2f51bd2fbcb6b34ca0",
        "bug_id": "hbase_41",
        "file": [
            {
                "sha": "7ad6177764c2ddbaff105b3a3b97f4cb2000f126",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "status": "modified",
                "changes": 9,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java?ref=7cdb52519236966a7cb6dff7fbd0609c87545f75",
                "patch": "@@ -32,6 +32,8 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.Stoppable;\n import org.apache.hadoop.hbase.io.HFileLink;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.regionserver.StoreFileInfo;\n import org.apache.hadoop.hbase.util.StealJobQueue;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -44,6 +46,7 @@\n  */\n @InterfaceAudience.Private\n public class HFileCleaner extends CleanerChore<BaseHFileCleanerDelegate> {\n+  private MasterServices master;\n \n   public static final String MASTER_HFILE_CLEANER_PLUGINS = \"hbase.master.hfilecleaner.plugins\";\n \n@@ -496,4 +499,10 @@ public synchronized void cancel(boolean mayInterruptIfRunning) {\n       t.interrupt();\n     }\n   }\n+\n+  public void init(Map<String, Object> params) {\n+    if (params != null && params.containsKey(HMaster.MASTER)) {\n+      this.master = (MasterServices) params.get(HMaster.MASTER);\n+    }\n+  }\n }",
                "deletions": 0
            },
            {
                "sha": "2d32f9ecb420907e40198b1534e78b291d131db2",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java",
                "blob_url": "https://github.com/apache/hbase/blob/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java",
                "raw_url": "https://github.com/apache/hbase/raw/7cdb52519236966a7cb6dff7fbd0609c87545f75/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java",
                "status": "modified",
                "changes": 58,
                "additions": 41,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java?ref=7cdb52519236966a7cb6dff7fbd0609c87545f75",
                "patch": "@@ -19,26 +19,33 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.PathFilter;\n-import org.apache.hadoop.hbase.*;\n+import org.apache.hadoop.hbase.ChoreService;\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.Stoppable;\n+import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.cleaner.HFileCleaner;\n-import org.apache.hadoop.hbase.regionserver.CompactingMemStore;\n import org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy;\n import org.apache.hadoop.hbase.regionserver.HRegion;\n import org.apache.hadoop.hbase.regionserver.HRegionServer;\n-import org.apache.hadoop.hbase.regionserver.Region;\n import org.apache.hadoop.hbase.testclassification.MediumTests;\n import org.apache.hadoop.hbase.testclassification.MiscTests;\n import org.apache.hadoop.hbase.util.Bytes;\n@@ -177,10 +184,11 @@ public boolean accept(Path p) {\n   /**\n    * Test that the region directory is removed when we archive a region without store files, but\n    * still has hidden files.\n-   * @throws Exception\n+   * @throws IOException throws an IOException if there's problem creating a table\n+   *   or if there's an issue with accessing FileSystem.\n    */\n   @Test\n-  public void testDeleteRegionWithNoStoreFiles() throws Exception {\n+  public void testDeleteRegionWithNoStoreFiles() throws IOException {\n     final TableName tableName = TableName.valueOf(name.getMethodName());\n     UTIL.createTable(tableName, TEST_FAM);\n \n@@ -209,7 +217,7 @@ public void testDeleteRegionWithNoStoreFiles() throws Exception {\n     PathFilter nonHidden = new PathFilter() {\n       @Override\n       public boolean accept(Path file) {\n-        return dirFilter.accept(file) && !file.getName().toString().startsWith(\".\");\n+        return dirFilter.accept(file) && !file.getName().startsWith(\".\");\n       }\n     };\n     FileStatus[] storeDirs = FSUtils.listStatus(fs, regionDir, nonHidden);\n@@ -271,12 +279,14 @@ public void testArchiveOnTableDelete() throws Exception {\n     assertArchiveFiles(fs, storeFiles, 30000);\n   }\n \n-  private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long timeout) throws IOException {\n+  private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long timeout)\n+          throws IOException {\n     long end = System.currentTimeMillis() + timeout;\n     Path archiveDir = HFileArchiveUtil.getArchivePath(UTIL.getConfiguration());\n     List<String> archivedFiles = new ArrayList<>();\n \n-    // We have to ensure that the DeleteTableHandler is finished. HBaseAdmin.deleteXXX() can return before all files\n+    // We have to ensure that the DeleteTableHandler is finished. HBaseAdmin.deleteXXX()\n+    // can return before all files\n     // are archived. We should fix HBASE-5487 and fix synchronous operations from admin.\n     while (System.currentTimeMillis() < end) {\n       archivedFiles = getAllFileNames(fs, archiveDir);\n@@ -304,10 +314,11 @@ private void assertArchiveFiles(FileSystem fs, List<String> storeFiles, long tim\n \n   /**\n    * Test that the store files are archived when a column family is removed.\n-   * @throws Exception\n+   * @throws java.io.IOException if there's a problem creating a table.\n+   * @throws java.lang.InterruptedException problem getting a RegionServer.\n    */\n   @Test\n-  public void testArchiveOnTableFamilyDelete() throws Exception {\n+  public void testArchiveOnTableFamilyDelete() throws IOException, InterruptedException {\n     final TableName tableName = TableName.valueOf(name.getMethodName());\n     UTIL.createTable(tableName, new byte[][] {TEST_FAM, Bytes.toBytes(\"fam2\")});\n \n@@ -374,11 +385,10 @@ public void testCleaningRace() throws Exception {\n     Stoppable stoppable = new StoppableImplementation();\n \n     // The cleaner should be looping without long pauses to reproduce the race condition.\n-    HFileCleaner cleaner = new HFileCleaner(1, stoppable, conf, fs, archiveDir);\n-    assertFalse(\"cleaner should not be null\", cleaner == null);\n+    HFileCleaner cleaner = getHFileCleaner(stoppable, conf, fs, archiveDir);\n+    assertNotNull(\"cleaner should not be null\", cleaner);\n     try {\n       choreService.scheduleChore(cleaner);\n-\n       // Keep creating/archiving new files while the cleaner is running in the other thread\n       long startTime = System.currentTimeMillis();\n       for (long fid = 0; (System.currentTimeMillis() - startTime) < TEST_TIME; ++fid) {\n@@ -418,6 +428,16 @@ public void testCleaningRace() throws Exception {\n     }\n   }\n \n+  // Avoid passing a null master to CleanerChore, see HBASE-21175\n+  private HFileCleaner getHFileCleaner(Stoppable stoppable, Configuration conf,\n+        FileSystem fs, Path archiveDir) throws IOException {\n+    Map<String, Object> params = new HashMap<>();\n+    params.put(HMaster.MASTER, UTIL.getMiniHBaseCluster().getMaster());\n+    HFileCleaner cleaner = new HFileCleaner(1, stoppable, conf, fs, archiveDir);\n+    cleaner.init(params);\n+    return Objects.requireNonNull(cleaner);\n+  }\n+\n   private void clearArchiveDirectory() throws IOException {\n     UTIL.getTestFileSystem().delete(\n       new Path(UTIL.getDefaultRootDirPath(), HConstants.HFILE_ARCHIVE_DIRECTORY), true);\n@@ -428,9 +448,9 @@ private void clearArchiveDirectory() throws IOException {\n    * @param fs the file system to inspect\n    * @param archiveDir the directory in which to look\n    * @return a list of all files in the directory and sub-directories\n-   * @throws IOException\n+   * @throws java.io.IOException throws IOException in case FS is unavailable\n    */\n-  private List<String> getAllFileNames(final FileSystem fs, Path archiveDir) throws IOException {\n+  private List<String> getAllFileNames(final FileSystem fs, Path archiveDir) throws IOException  {\n     FileStatus[] files = FSUtils.listStatus(fs, archiveDir, new PathFilter() {\n       @Override\n       public boolean accept(Path p) {\n@@ -446,12 +466,16 @@ public boolean accept(Path p) {\n   /** Recursively lookup all the file names under the file[] array **/\n   private List<String> recurseOnFiles(FileSystem fs, FileStatus[] files, List<String> fileNames)\n       throws IOException {\n-    if (files == null || files.length == 0) return fileNames;\n+    if (files == null || files.length == 0) {\n+      return fileNames;\n+    }\n \n     for (FileStatus file : files) {\n       if (file.isDirectory()) {\n         recurseOnFiles(fs, FSUtils.listStatus(fs, file.getPath(), null), fileNames);\n-      } else fileNames.add(file.getPath().getName());\n+      } else {\n+        fileNames.add(file.getPath().getName());\n+      }\n     }\n     return fileNames;\n   }",
                "deletions": 17
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20569 NPE in RecoverStandbyProcedure.execute",
        "commit": "https://github.com/apache/hbase/commit/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
        "parent": "https://github.com/apache/hbase/commit/7448b045ccc96cea2876ebd39cb6b5ef7b73a207",
        "bug_id": "hbase_42",
        "file": [
            {
                "sha": "a062e9a8193369d35bf75c240287bf1db1a9bcfd",
                "filename": "hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "status": "modified",
                "changes": 26,
                "additions": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -486,22 +486,34 @@ message TransitPeerSyncReplicationStateStateData {\n \n enum RecoverStandbyState {\n   RENAME_SYNC_REPLICATION_WALS_DIR = 1;\n-  INIT_WORKERS = 2;\n-  DISPATCH_TASKS = 3;\n-  SNAPSHOT_SYNC_REPLICATION_WALS_DIR = 4;\n+  REGISTER_PEER_TO_WORKER_STORAGE = 2;\n+  DISPATCH_WALS = 3;\n+  UNREGISTER_PEER_FROM_WORKER_STORAGE = 4;\n+  SNAPSHOT_SYNC_REPLICATION_WALS_DIR = 5;\n+}\n+\n+enum SyncReplicationReplayWALState {\n+  ASSIGN_WORKER = 1;\n+  DISPATCH_WALS_TO_WORKER = 2;\n+  RELEASE_WORKER = 3;\n }\n \n message RecoverStandbyStateData {\n+  required bool serial  = 1;\n+}\n+\n+message SyncReplicationReplayWALStateData {\n   required string peer_id = 1;\n+  repeated string wal = 2;\n }\n \n-message ReplaySyncReplicationWALStateData {\n+message SyncReplicationReplayWALRemoteStateData {\n   required string peer_id = 1;\n-  required string wal = 2;\n-  optional ServerName target_server = 3;\n+  repeated string wal = 2;\n+  required ServerName target_server = 3;\n }\n \n message ReplaySyncReplicationWALParameter {\n   required string peer_id = 1;\n-  required string wal = 2;\n+  repeated string wal = 2;\n }",
                "deletions": 7
            },
            {
                "sha": "dc627523569b91f4285a18890dc45058ee33fc46",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -138,8 +138,8 @@\n import org.apache.hadoop.hbase.master.replication.DisablePeerProcedure;\n import org.apache.hadoop.hbase.master.replication.EnablePeerProcedure;\n import org.apache.hadoop.hbase.master.replication.RemovePeerProcedure;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n import org.apache.hadoop.hbase.master.replication.ReplicationPeerManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure;\n import org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure;\n import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;\n@@ -343,7 +343,7 @@ public void run() {\n   // manager of replication\n   private ReplicationPeerManager replicationPeerManager;\n \n-  private ReplaySyncReplicationWALManager replaySyncReplicationWALManager;\n+  private SyncReplicationReplayWALManager syncReplicationReplayWALManager;\n \n   // buffer for \"fatal error\" notices from region servers\n   // in the cluster. This is only used for assisting\n@@ -754,6 +754,7 @@ protected void initializeZKBasedSystemTrackers()\n     this.splitOrMergeTracker.start();\n \n     this.replicationPeerManager = ReplicationPeerManager.create(zooKeeper, conf);\n+    this.syncReplicationReplayWALManager = new SyncReplicationReplayWALManager(this);\n \n     this.drainingServerTracker = new DrainingServerTracker(zooKeeper, this, this.serverManager);\n     this.drainingServerTracker.start();\n@@ -852,7 +853,6 @@ private void finishActiveMasterInitialization(MonitoredTask status) throws IOExc\n     initializeMemStoreChunkCreator();\n     this.fileSystemManager = new MasterFileSystem(conf);\n     this.walManager = new MasterWalManager(this);\n-    this.replaySyncReplicationWALManager = new ReplaySyncReplicationWALManager(this);\n \n     // enable table descriptors cache\n     this.tableDescriptors.setCacheOn();\n@@ -3764,7 +3764,7 @@ public SnapshotQuotaObserverChore getSnapshotQuotaObserverChore() {\n   }\n \n   @Override\n-  public ReplaySyncReplicationWALManager getReplaySyncReplicationWALManager() {\n-    return this.replaySyncReplicationWALManager;\n+  public SyncReplicationReplayWALManager getSyncReplicationReplayWALManager() {\n+    return this.syncReplicationReplayWALManager;\n   }\n }",
                "deletions": 5
            },
            {
                "sha": "7b0c56a924ee6ad797a973557161d25bf0eb4486",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -38,8 +38,8 @@\n import org.apache.hadoop.hbase.master.locking.LockManager;\n import org.apache.hadoop.hbase.master.normalizer.RegionNormalizer;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n import org.apache.hadoop.hbase.master.replication.ReplicationPeerManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;\n import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;\n import org.apache.hadoop.hbase.procedure2.LockedResource;\n@@ -462,9 +462,9 @@ ReplicationPeerConfig getReplicationPeerConfig(String peerId) throws Replication\n   ReplicationPeerManager getReplicationPeerManager();\n \n   /**\n-   * Returns the {@link ReplaySyncReplicationWALManager}.\n+   * Returns the {@link SyncReplicationReplayWALManager}.\n    */\n-  ReplaySyncReplicationWALManager getReplaySyncReplicationWALManager();\n+  SyncReplicationReplayWALManager getSyncReplicationReplayWALManager();\n \n   /**\n    * Update the peerConfig for the specified peer",
                "deletions": 3
            },
            {
                "sha": "8a28b84b8162f0a06992c35137631d40271011fd",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -207,7 +207,8 @@ protected Procedure dequeue() {\n       // check if the next procedure is still a child.\n       // if not, remove the rq from the fairq and go back to the xlock state\n       Procedure<?> nextProc = rq.peek();\n-      if (nextProc != null && !Procedure.haveSameParent(nextProc, pollResult)) {\n+      if (nextProc != null && !Procedure.haveSameParent(nextProc, pollResult)\n+          && nextProc.getRootProcId() != pollResult.getRootProcId()) {\n         removeFromRunQueue(fairq, rq);\n       }\n     }",
                "deletions": 1
            },
            {
                "sha": "0195ab9c66ada42c0383f9b8d916873bd8b69254",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -24,7 +24,7 @@\n \n   enum PeerOperationType {\n     ADD, REMOVE, ENABLE, DISABLE, UPDATE_CONFIG, REFRESH, TRANSIT_SYNC_REPLICATION_STATE,\n-    RECOVER_STANDBY, REPLAY_SYNC_REPLICATION_WAL\n+    RECOVER_STANDBY, SYNC_REPLICATION_REPLAY_WAL, SYNC_REPLICATION_REPLAY_WAL_REMOTE\n   }\n \n   String getPeerId();",
                "deletions": 1
            },
            {
                "sha": "86d8e4341ab61328e797fb0db09630a799374f8f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/PeerQueue.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -50,6 +50,7 @@ public boolean requireExclusiveLock(Procedure<?> proc) {\n \n   private static boolean requirePeerExclusiveLock(PeerProcedureInterface proc) {\n     return proc.getPeerOperationType() != PeerOperationType.REFRESH\n-        && proc.getPeerOperationType() != PeerOperationType.REPLAY_SYNC_REPLICATION_WAL;\n+        && proc.getPeerOperationType() != PeerOperationType.SYNC_REPLICATION_REPLAY_WAL\n+        && proc.getPeerOperationType() != PeerOperationType.SYNC_REPLICATION_REPLAY_WAL_REMOTE;\n   }\n }",
                "deletions": 1
            },
            {
                "sha": "54947429d8b61f0861bfbb2719d4993ad99a4a9f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java",
                "status": "modified",
                "changes": 68,
                "additions": 52,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -18,60 +18,79 @@\n package org.apache.hadoop.hbase.master.replication;\n \n import java.io.IOException;\n+import java.util.Arrays;\n import java.util.List;\n \n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;\n import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyState;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RecoverStandbyStateData;\n \n @InterfaceAudience.Private\n public class RecoverStandbyProcedure extends AbstractPeerProcedure<RecoverStandbyState> {\n \n   private static final Logger LOG = LoggerFactory.getLogger(RecoverStandbyProcedure.class);\n \n+  private boolean serial;\n+\n   public RecoverStandbyProcedure() {\n   }\n \n-  public RecoverStandbyProcedure(String peerId) {\n+  public RecoverStandbyProcedure(String peerId, boolean serial) {\n     super(peerId);\n+    this.serial = serial;\n   }\n \n   @Override\n   protected Flow executeFromState(MasterProcedureEnv env, RecoverStandbyState state)\n       throws ProcedureSuspendedException, ProcedureYieldException, InterruptedException {\n-    ReplaySyncReplicationWALManager replaySyncReplicationWALManager =\n-        env.getMasterServices().getReplaySyncReplicationWALManager();\n+    SyncReplicationReplayWALManager syncReplicationReplayWALManager =\n+        env.getMasterServices().getSyncReplicationReplayWALManager();\n     switch (state) {\n       case RENAME_SYNC_REPLICATION_WALS_DIR:\n         try {\n-          replaySyncReplicationWALManager.renameToPeerReplayWALDir(peerId);\n+          syncReplicationReplayWALManager.renameToPeerReplayWALDir(peerId);\n         } catch (IOException e) {\n           LOG.warn(\"Failed to rename remote wal dir for peer id={}\", peerId, e);\n           setFailure(\"master-recover-standby\", e);\n           return Flow.NO_MORE_STATE;\n         }\n-        setNextState(RecoverStandbyState.INIT_WORKERS);\n+        setNextState(RecoverStandbyState.REGISTER_PEER_TO_WORKER_STORAGE);\n         return Flow.HAS_MORE_STATE;\n-      case INIT_WORKERS:\n-        replaySyncReplicationWALManager.initPeerWorkers(peerId);\n-        setNextState(RecoverStandbyState.DISPATCH_TASKS);\n+      case REGISTER_PEER_TO_WORKER_STORAGE:\n+        try {\n+          syncReplicationReplayWALManager.registerPeer(peerId);\n+        } catch (ReplicationException e) {\n+          LOG.warn(\"Failed to register peer to worker storage for peer id={}, retry\", peerId, e);\n+          throw new ProcedureYieldException();\n+        }\n+        setNextState(RecoverStandbyState.DISPATCH_WALS);\n         return Flow.HAS_MORE_STATE;\n-      case DISPATCH_TASKS:\n-        addChildProcedure(getReplayWALs(replaySyncReplicationWALManager).stream()\n-            .map(wal -> new ReplaySyncReplicationWALProcedure(peerId,\n-                replaySyncReplicationWALManager.removeWALRootPath(wal)))\n-            .toArray(ReplaySyncReplicationWALProcedure[]::new));\n+      case DISPATCH_WALS:\n+        dispathWals(syncReplicationReplayWALManager);\n+        setNextState(RecoverStandbyState.UNREGISTER_PEER_FROM_WORKER_STORAGE);\n+        return Flow.HAS_MORE_STATE;\n+      case UNREGISTER_PEER_FROM_WORKER_STORAGE:\n+        try {\n+          syncReplicationReplayWALManager.unregisterPeer(peerId);\n+        } catch (ReplicationException e) {\n+          LOG.warn(\"Failed to unregister peer from worker storage for peer id={}, retry\", peerId,\n+              e);\n+          throw new ProcedureYieldException();\n+        }\n         setNextState(RecoverStandbyState.SNAPSHOT_SYNC_REPLICATION_WALS_DIR);\n         return Flow.HAS_MORE_STATE;\n       case SNAPSHOT_SYNC_REPLICATION_WALS_DIR:\n         try {\n-          replaySyncReplicationWALManager.renameToPeerSnapshotWALDir(peerId);\n+          syncReplicationReplayWALManager.renameToPeerSnapshotWALDir(peerId);\n         } catch (IOException e) {\n           LOG.warn(\"Failed to cleanup replay wals dir for peer id={}, , retry\", peerId, e);\n           throw new ProcedureYieldException();\n@@ -82,10 +101,14 @@ protected Flow executeFromState(MasterProcedureEnv env, RecoverStandbyState stat\n     }\n   }\n \n-  private List<Path> getReplayWALs(ReplaySyncReplicationWALManager replaySyncReplicationWALManager)\n+  // TODO: dispatch wals by region server when serial is true and sort wals\n+  private void dispathWals(SyncReplicationReplayWALManager syncReplicationReplayWALManager)\n       throws ProcedureYieldException {\n     try {\n-      return replaySyncReplicationWALManager.getReplayWALsAndCleanUpUnusedFiles(peerId);\n+      List<Path> wals = syncReplicationReplayWALManager.getReplayWALsAndCleanUpUnusedFiles(peerId);\n+      addChildProcedure(wals.stream().map(wal -> new SyncReplicationReplayWALProcedure(peerId,\n+          Arrays.asList(syncReplicationReplayWALManager.removeWALRootPath(wal))))\n+          .toArray(SyncReplicationReplayWALProcedure[]::new));\n     } catch (IOException e) {\n       LOG.warn(\"Failed to get replay wals for peer id={}, , retry\", peerId, e);\n       throw new ProcedureYieldException();\n@@ -111,4 +134,17 @@ protected RecoverStandbyState getInitialState() {\n   public PeerOperationType getPeerOperationType() {\n     return PeerOperationType.RECOVER_STANDBY;\n   }\n+\n+  @Override\n+  protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException {\n+    super.serializeStateData(serializer);\n+    serializer.serialize(RecoverStandbyStateData.newBuilder().setSerial(serial).build());\n+  }\n+\n+  @Override\n+  protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {\n+    super.deserializeStateData(serializer);\n+    RecoverStandbyStateData data = serializer.deserialize(RecoverStandbyStateData.class);\n+    serial = data.getSerial();\n+  }\n }\n\\ No newline at end of file",
                "deletions": 16
            },
            {
                "sha": "4b77c8dba46ba0620d8b52967549bdf21ebebe9c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -67,11 +67,10 @@ protected void updatePeerStorage(MasterProcedureEnv env) throws ReplicationExcep\n   }\n \n   private void removeRemoteWALs(MasterProcedureEnv env) throws IOException {\n-    env.getMasterServices().getReplaySyncReplicationWALManager().removePeerRemoteWALs(peerId);\n+    env.getMasterServices().getSyncReplicationReplayWALManager().removePeerRemoteWALs(peerId);\n   }\n \n-  @Override\n-  protected void postPeerModification(MasterProcedureEnv env)\n+  @Override  protected void postPeerModification(MasterProcedureEnv env)\n       throws IOException, ReplicationException {\n     if (peerConfig.isSyncReplication()) {\n       removeRemoteWALs(env);",
                "deletions": 3
            },
            {
                "sha": "377c9f1e22a53a0e3af3bd4555da9032592c0c3b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java",
                "status": "renamed",
                "changes": 105,
                "additions": 77,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -17,23 +17,26 @@\n  */\n package org.apache.hadoop.hbase.master.replication;\n \n+import static org.apache.hadoop.hbase.replication.ReplicationUtils.REMOTE_WAL_REPLAY_SUFFIX;\n import static org.apache.hadoop.hbase.replication.ReplicationUtils.getPeerRemoteWALDir;\n import static org.apache.hadoop.hbase.replication.ReplicationUtils.getPeerReplayWALDir;\n import static org.apache.hadoop.hbase.replication.ReplicationUtils.getPeerSnapshotWALDir;\n \n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.LinkedBlockingQueue;\n-import java.util.concurrent.TimeUnit;\n+import java.util.Optional;\n+import java.util.Set;\n+\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n import org.apache.hadoop.hbase.replication.ReplicationUtils;\n import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.yetus.audience.InterfaceAudience;\n@@ -43,10 +46,10 @@\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n \n @InterfaceAudience.Private\n-public class ReplaySyncReplicationWALManager {\n+public class SyncReplicationReplayWALManager {\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(ReplaySyncReplicationWALManager.class);\n+      LoggerFactory.getLogger(SyncReplicationReplayWALManager.class);\n \n   private final MasterServices services;\n \n@@ -56,15 +59,67 @@\n \n   private final Path remoteWALDir;\n \n-  private final Map<String, BlockingQueue<ServerName>> availServers = new HashMap<>();\n+  private final ZKSyncReplicationReplayWALWorkerStorage workerStorage;\n+\n+  private final Map<String, Set<ServerName>> workers = new HashMap<>();\n \n-  public ReplaySyncReplicationWALManager(MasterServices services) {\n+  private final Object workerLock = new Object();\n+\n+  public SyncReplicationReplayWALManager(MasterServices services)\n+      throws IOException, ReplicationException {\n     this.services = services;\n     this.fs = services.getMasterFileSystem().getWALFileSystem();\n     this.walRootDir = services.getMasterFileSystem().getWALRootDir();\n     this.remoteWALDir = new Path(this.walRootDir, ReplicationUtils.REMOTE_WAL_DIR_NAME);\n+    this.workerStorage = new ZKSyncReplicationReplayWALWorkerStorage(services.getZooKeeper(),\n+        services.getConfiguration());\n+    checkReplayingWALDir();\n+  }\n+\n+  private void checkReplayingWALDir() throws IOException, ReplicationException {\n+    FileStatus[] files = fs.listStatus(remoteWALDir);\n+    for (FileStatus file : files) {\n+      String name = file.getPath().getName();\n+      if (name.endsWith(REMOTE_WAL_REPLAY_SUFFIX)) {\n+        String peerId = name.substring(0, name.length() - REMOTE_WAL_REPLAY_SUFFIX.length());\n+        workers.put(peerId, workerStorage.getPeerWorkers(peerId));\n+      }\n+    }\n+  }\n+\n+  public void registerPeer(String peerId) throws ReplicationException {\n+    workers.put(peerId, new HashSet<>());\n+    workerStorage.addPeer(peerId);\n+  }\n+\n+  public void unregisterPeer(String peerId) throws ReplicationException {\n+    workers.remove(peerId);\n+    workerStorage.removePeer(peerId);\n+  }\n+\n+  public ServerName getPeerWorker(String peerId) throws ReplicationException {\n+    Optional<ServerName> worker = Optional.empty();\n+    ServerName workerServer = null;\n+    synchronized (workerLock) {\n+      worker = services.getServerManager().getOnlineServers().keySet().stream()\n+          .filter(server -> !workers.get(peerId).contains(server)).findFirst();\n+      if (worker.isPresent()) {\n+        workerServer = worker.get();\n+        workers.get(peerId).add(workerServer);\n+      }\n+    }\n+    if (workerServer != null) {\n+      workerStorage.addPeerWorker(peerId, workerServer);\n+    }\n+    return workerServer;\n   }\n \n+  public void removePeerWorker(String peerId, ServerName worker) throws ReplicationException {\n+    synchronized (workerLock) {\n+      workers.get(peerId).remove(worker);\n+    }\n+    workerStorage.removePeerWorker(peerId, worker);\n+  }\n   public void createPeerRemoteWALDir(String peerId) throws IOException {\n     Path peerRemoteWALDir = getPeerRemoteWALDir(remoteWALDir, peerId);\n     if (!fs.exists(peerRemoteWALDir) && !fs.mkdirs(peerRemoteWALDir)) {\n@@ -77,23 +132,23 @@ private void rename(Path src, Path dst, String peerId) throws IOException {\n       deleteDir(dst, peerId);\n       if (!fs.rename(src, dst)) {\n         throw new IOException(\n-          \"Failed to rename dir from \" + src + \" to \" + dst + \" for peer id=\" + peerId);\n+            \"Failed to rename dir from \" + src + \" to \" + dst + \" for peer id=\" + peerId);\n       }\n       LOG.info(\"Renamed dir from {} to {} for peer id={}\", src, dst, peerId);\n     } else if (!fs.exists(dst)) {\n       throw new IOException(\n-        \"Want to rename from \" + src + \" to \" + dst + \", but they both do not exist\");\n+          \"Want to rename from \" + src + \" to \" + dst + \", but they both do not exist\");\n     }\n   }\n \n   public void renameToPeerReplayWALDir(String peerId) throws IOException {\n     rename(getPeerRemoteWALDir(remoteWALDir, peerId), getPeerReplayWALDir(remoteWALDir, peerId),\n-      peerId);\n+        peerId);\n   }\n \n   public void renameToPeerSnapshotWALDir(String peerId) throws IOException {\n     rename(getPeerReplayWALDir(remoteWALDir, peerId), getPeerSnapshotWALDir(remoteWALDir, peerId),\n-      peerId);\n+        peerId);\n   }\n \n   public List<Path> getReplayWALsAndCleanUpUnusedFiles(String peerId) throws IOException {\n@@ -103,7 +158,7 @@ public void renameToPeerSnapshotWALDir(String peerId) throws IOException {\n       Path src = status.getPath();\n       String srcName = src.getName();\n       String dstName =\n-        srcName.substring(0, srcName.length() - ReplicationUtils.RENAME_WAL_SUFFIX.length());\n+          srcName.substring(0, srcName.length() - ReplicationUtils.RENAME_WAL_SUFFIX.length());\n       FSUtils.renameFile(fs, src, new Path(src.getParent(), dstName));\n     }\n     List<Path> wals = new ArrayList<>();\n@@ -140,28 +195,22 @@ public void removePeerRemoteWALs(String peerId) throws IOException {\n     deleteDir(getPeerSnapshotWALDir(remoteWALDir, peerId), peerId);\n   }\n \n-  public void initPeerWorkers(String peerId) {\n-    BlockingQueue<ServerName> servers = new LinkedBlockingQueue<>();\n-    services.getServerManager().getOnlineServers().keySet()\n-        .forEach(server -> servers.offer(server));\n-    availServers.put(peerId, servers);\n-  }\n-\n-  public ServerName getAvailServer(String peerId, long timeout, TimeUnit unit)\n-      throws InterruptedException {\n-    return availServers.get(peerId).poll(timeout, unit);\n-  }\n-\n-  public void addAvailServer(String peerId, ServerName server) {\n-    availServers.get(peerId).offer(server);\n-  }\n-\n   public String removeWALRootPath(Path path) {\n     String pathStr = path.toString();\n     // remove the \"/\" too.\n     return pathStr.substring(walRootDir.toString().length() + 1);\n   }\n \n+  public void finishReplayWAL(String wal) throws IOException {\n+    Path walPath = new Path(walRootDir, wal);\n+    fs.truncate(walPath, 0);\n+  }\n+\n+  public boolean isReplayWALFinished(String wal) throws IOException {\n+    Path walPath = new Path(walRootDir, wal);\n+    return fs.getFileStatus(walPath).getLen() == 0;\n+  }\n+\n   @VisibleForTesting\n   public Path getRemoteWALDir() {\n     return remoteWALDir;",
                "deletions": 28,
                "previous_filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplaySyncReplicationWALManager.java"
            },
            {
                "sha": "26d6a3f78b44f424ae02369b1f8a24ff5cd04c62",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java",
                "status": "added",
                "changes": 164,
                "additions": 164,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -0,0 +1,164 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.replication;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n+import org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface;\n+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;\n+import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n+import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;\n+import org.apache.hadoop.hbase.procedure2.StateMachineProcedure;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SyncReplicationReplayWALState;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SyncReplicationReplayWALStateData;\n+\n+@InterfaceAudience.Private\n+public class SyncReplicationReplayWALProcedure\n+    extends StateMachineProcedure<MasterProcedureEnv, SyncReplicationReplayWALState>\n+    implements PeerProcedureInterface {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SyncReplicationReplayWALProcedure.class);\n+\n+  private String peerId;\n+\n+  private ServerName worker = null;\n+\n+  private List<String> wals;\n+\n+  public SyncReplicationReplayWALProcedure() {\n+  }\n+\n+  public SyncReplicationReplayWALProcedure(String peerId, List<String> wals) {\n+    this.peerId = peerId;\n+    this.wals = wals;\n+  }\n+\n+  @Override protected Flow executeFromState(MasterProcedureEnv env,\n+      SyncReplicationReplayWALState state)\n+      throws ProcedureSuspendedException, ProcedureYieldException, InterruptedException {\n+    SyncReplicationReplayWALManager syncReplicationReplayWALManager =\n+        env.getMasterServices().getSyncReplicationReplayWALManager();\n+    switch (state) {\n+      case ASSIGN_WORKER:\n+        try {\n+          worker = syncReplicationReplayWALManager.getPeerWorker(peerId);\n+        } catch (ReplicationException e) {\n+          LOG.info(\"Failed to get worker to replay wals {} for peer id={}, retry\", wals, peerId);\n+          throw new ProcedureYieldException();\n+        }\n+        if (worker == null) {\n+          LOG.info(\"No worker to replay wals {} for peer id={}, retry\", wals, peerId);\n+          setNextState(SyncReplicationReplayWALState.ASSIGN_WORKER);\n+        } else {\n+          setNextState(SyncReplicationReplayWALState.DISPATCH_WALS_TO_WORKER);\n+        }\n+        return Flow.HAS_MORE_STATE;\n+      case DISPATCH_WALS_TO_WORKER:\n+        addChildProcedure(new SyncReplicationReplayWALRemoteProcedure(peerId, wals, worker));\n+        setNextState(SyncReplicationReplayWALState.RELEASE_WORKER);\n+        return Flow.HAS_MORE_STATE;\n+      case RELEASE_WORKER:\n+        boolean finished = false;\n+        try {\n+          finished = syncReplicationReplayWALManager.isReplayWALFinished(wals.get(0));\n+        } catch (IOException e) {\n+          LOG.info(\"Failed to check whether replay wals {} finished for peer id={}\", wals, peerId);\n+          throw new ProcedureYieldException();\n+        }\n+        try {\n+          syncReplicationReplayWALManager.removePeerWorker(peerId, worker);\n+        } catch (ReplicationException e) {\n+          LOG.info(\"Failed to remove worker for peer id={}, retry\", peerId);\n+          throw new ProcedureYieldException();\n+        }\n+        if (!finished) {\n+          LOG.info(\"Failed to replay wals {} for peer id={}, retry\", wals, peerId);\n+          setNextState(SyncReplicationReplayWALState.ASSIGN_WORKER);\n+          return Flow.HAS_MORE_STATE;\n+        }\n+        return Flow.NO_MORE_STATE;\n+      default:\n+        throw new UnsupportedOperationException(\"unhandled state=\" + state);\n+    }\n+  }\n+\n+  @Override\n+  protected void rollbackState(MasterProcedureEnv env,\n+      SyncReplicationReplayWALState state)\n+      throws IOException, InterruptedException {\n+    if (state == getInitialState()) {\n+      return;\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  protected SyncReplicationReplayWALState getState(int state) {\n+    return SyncReplicationReplayWALState.forNumber(state);\n+  }\n+\n+  @Override\n+  protected int getStateId(\n+      SyncReplicationReplayWALState state) {\n+    return state.getNumber();\n+  }\n+\n+  @Override\n+  protected SyncReplicationReplayWALState getInitialState() {\n+    return SyncReplicationReplayWALState.ASSIGN_WORKER;\n+  }\n+\n+  @Override\n+  protected void serializeStateData(ProcedureStateSerializer serializer)\n+      throws IOException {\n+    SyncReplicationReplayWALStateData.Builder builder =\n+        SyncReplicationReplayWALStateData.newBuilder();\n+    builder.setPeerId(peerId);\n+    wals.stream().forEach(builder::addWal);\n+    serializer.serialize(builder.build());\n+  }\n+\n+  @Override\n+  protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {\n+    SyncReplicationReplayWALStateData data =\n+        serializer.deserialize(SyncReplicationReplayWALStateData.class);\n+    peerId = data.getPeerId();\n+    wals = new ArrayList<>();\n+    data.getWalList().forEach(wals::add);\n+  }\n+\n+  @Override\n+  public String getPeerId() {\n+    return peerId;\n+  }\n+\n+  @Override\n+  public PeerOperationType getPeerOperationType() {\n+    return PeerOperationType.SYNC_REPLICATION_REPLAY_WAL;\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "9f4f33088ad328e9ae9190d047c011ed0ac0ec4a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java",
                "status": "renamed",
                "changes": 113,
                "additions": 65,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -18,7 +18,8 @@\n package org.apache.hadoop.hbase.master.replication;\n \n import java.io.IOException;\n-import java.util.concurrent.TimeUnit;\n+import java.util.ArrayList;\n+import java.util.List;\n \n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n@@ -40,42 +41,45 @@\n \n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ReplaySyncReplicationWALParameter;\n-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ReplaySyncReplicationWALStateData;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SyncReplicationReplayWALRemoteStateData;\n \n @InterfaceAudience.Private\n-public class ReplaySyncReplicationWALProcedure extends Procedure<MasterProcedureEnv>\n+public class SyncReplicationReplayWALRemoteProcedure extends Procedure<MasterProcedureEnv>\n     implements RemoteProcedure<MasterProcedureEnv, ServerName>, PeerProcedureInterface {\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(ReplaySyncReplicationWALProcedure.class);\n-\n-  private static final long DEFAULT_WAIT_AVAILABLE_SERVER_TIMEOUT = 10000;\n+      LoggerFactory.getLogger(SyncReplicationReplayWALRemoteProcedure.class);\n \n   private String peerId;\n \n-  private ServerName targetServer = null;\n+  private ServerName targetServer;\n \n-  private String wal;\n+  private List<String> wals;\n \n   private boolean dispatched;\n \n   private ProcedureEvent<?> event;\n \n   private boolean succ;\n \n-  public ReplaySyncReplicationWALProcedure() {\n+  public SyncReplicationReplayWALRemoteProcedure() {\n   }\n \n-  public ReplaySyncReplicationWALProcedure(String peerId, String wal) {\n+  public SyncReplicationReplayWALRemoteProcedure(String peerId, List<String> wals,\n+      ServerName targetServer) {\n     this.peerId = peerId;\n-    this.wal = wal;\n+    this.wals = wals;\n+    this.targetServer = targetServer;\n   }\n \n   @Override\n   public RemoteOperation remoteCallBuild(MasterProcedureEnv env, ServerName remote) {\n+    ReplaySyncReplicationWALParameter.Builder builder =\n+        ReplaySyncReplicationWALParameter.newBuilder();\n+    builder.setPeerId(peerId);\n+    wals.stream().forEach(builder::addWal);\n     return new ServerOperation(this, getProcId(), ReplaySyncReplicationWALCallable.class,\n-        ReplaySyncReplicationWALParameter.newBuilder().setPeerId(peerId).setWal(wal).build()\n-            .toByteArray());\n+        builder.build().toByteArray());\n   }\n \n   @Override\n@@ -99,22 +103,47 @@ private void complete(MasterProcedureEnv env, Throwable error) {\n         getProcId());\n       return;\n     }\n-    ReplaySyncReplicationWALManager replaySyncReplicationWALManager =\n-        env.getMasterServices().getReplaySyncReplicationWALManager();\n     if (error != null) {\n-      LOG.warn(\"Replay sync replication wal {} on {} failed for peer id={}\", wal, targetServer,\n-        peerId, error);\n+      LOG.warn(\"Replay wals {} on {} failed for peer id={}\", wals, targetServer, peerId, error);\n       this.succ = false;\n     } else {\n-      LOG.warn(\"Replay sync replication wal {} on {} suceeded for peer id={}\", wal, targetServer,\n-        peerId);\n+      truncateWALs(env);\n+      LOG.info(\"Replay wals {} on {} succeed for peer id={}\", wals, targetServer, peerId);\n       this.succ = true;\n-      replaySyncReplicationWALManager.addAvailServer(peerId, targetServer);\n     }\n     event.wake(env.getProcedureScheduler());\n     event = null;\n   }\n \n+  /**\n+   * Only truncate wals one by one when task succeed. The parent procedure will check the first\n+   * wal length to know whether this task succeed.\n+   */\n+  private void truncateWALs(MasterProcedureEnv env) {\n+    String firstWal = wals.get(0);\n+    try {\n+      env.getMasterServices().getSyncReplicationReplayWALManager().finishReplayWAL(firstWal);\n+    } catch (IOException e) {\n+      // As it is idempotent to rerun this task. Just ignore this exception and return.\n+      LOG.warn(\"Failed to truncate wal {} for peer id={}\", firstWal, peerId, e);\n+      return;\n+    }\n+    for (int i = 1; i < wals.size(); i++) {\n+      String wal = wals.get(i);\n+      try {\n+        env.getMasterServices().getSyncReplicationReplayWALManager().finishReplayWAL(wal);\n+      } catch (IOException e1) {\n+        try {\n+          // retry\n+          env.getMasterServices().getSyncReplicationReplayWALManager().finishReplayWAL(wal);\n+        } catch (IOException e2) {\n+          // As the parent procedure only check the first wal length. Just ignore this exception.\n+          LOG.warn(\"Failed to truncate wal {} for peer id={}\", wal, peerId, e2);\n+        }\n+      }\n+    }\n+  }\n+\n   @Override\n   protected Procedure<MasterProcedureEnv>[] execute(MasterProcedureEnv env)\n       throws ProcedureYieldException, ProcedureSuspendedException, InterruptedException {\n@@ -126,25 +155,14 @@ private void complete(MasterProcedureEnv env, Throwable error) {\n       dispatched = false;\n     }\n \n-    // Try poll a available server\n-    if (targetServer == null) {\n-      targetServer = env.getMasterServices().getReplaySyncReplicationWALManager()\n-          .getAvailServer(peerId, DEFAULT_WAIT_AVAILABLE_SERVER_TIMEOUT, TimeUnit.MILLISECONDS);\n-      if (targetServer == null) {\n-        LOG.info(\"No available server to replay wal {} for peer id={}, retry\", wal, peerId);\n-        throw new ProcedureYieldException();\n-      }\n-    }\n-\n     // Dispatch task to target server\n     try {\n       env.getRemoteDispatcher().addOperationToNode(targetServer, this);\n     } catch (FailedRemoteDispatchException e) {\n-      LOG.info(\n-        \"Can not add remote operation for replay wal {} on {} for peer id={}, \" +\n-          \"this usually because the server is already dead, \" + \"retry\",\n-        wal, targetServer, peerId, e);\n-      targetServer = null;\n+      LOG.warn(\n+          \"Can not add remote operation for replay wals {} on {} for peer id={}, \"\n+              + \"this usually because the server is already dead, retry\",\n+          wals, targetServer, peerId);\n       throw new ProcedureYieldException();\n     }\n     dispatched = true;\n@@ -164,24 +182,23 @@ protected boolean abort(MasterProcedureEnv env) {\n   }\n \n   @Override\n-  protected void serializeStateData(ProcedureStateSerializer serializer) throws IOException {\n-    ReplaySyncReplicationWALStateData.Builder builder =\n-        ReplaySyncReplicationWALStateData.newBuilder().setPeerId(peerId).setWal(wal);\n-    if (targetServer != null) {\n-      builder.setTargetServer(ProtobufUtil.toServerName(targetServer));\n-    }\n+  protected void serializeStateData(ProcedureStateSerializer serializer)\n+      throws IOException {\n+    SyncReplicationReplayWALRemoteStateData.Builder builder =\n+        SyncReplicationReplayWALRemoteStateData.newBuilder().setPeerId(peerId)\n+            .setTargetServer(ProtobufUtil.toServerName(targetServer));\n+    wals.stream().forEach(builder::addWal);\n     serializer.serialize(builder.build());\n   }\n \n   @Override\n   protected void deserializeStateData(ProcedureStateSerializer serializer) throws IOException {\n-    ReplaySyncReplicationWALStateData data =\n-        serializer.deserialize(ReplaySyncReplicationWALStateData.class);\n+    SyncReplicationReplayWALRemoteStateData data =\n+        serializer.deserialize(SyncReplicationReplayWALRemoteStateData.class);\n     peerId = data.getPeerId();\n-    wal = data.getWal();\n-    if (data.hasTargetServer()) {\n-      targetServer = ProtobufUtil.toServerName(data.getTargetServer());\n-    }\n+    wals = new ArrayList<>();\n+    data.getWalList().forEach(wals::add);\n+    targetServer = ProtobufUtil.toServerName(data.getTargetServer());\n   }\n \n   @Override\n@@ -191,6 +208,6 @@ public String getPeerId() {\n \n   @Override\n   public PeerOperationType getPeerOperationType() {\n-    return PeerOperationType.REPLAY_SYNC_REPLICATION_WAL;\n+    return PeerOperationType.SYNC_REPLICATION_REPLAY_WAL_REMOTE;\n   }\n }\n\\ No newline at end of file",
                "deletions": 48,
                "previous_filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplaySyncReplicationWALProcedure.java"
            },
            {
                "sha": "c650974df8f05308cbd7b0e27770a1aefda82471",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -186,8 +186,8 @@ private void setNextStateAfterRefreshEnd() {\n     }\n   }\n \n-  private void replayRemoteWAL() {\n-    addChildProcedure(new RecoverStandbyProcedure(peerId));\n+  private void replayRemoteWAL(boolean serial) {\n+    addChildProcedure(new RecoverStandbyProcedure(peerId, serial));\n   }\n \n   @Override\n@@ -232,7 +232,7 @@ protected Flow executeFromState(MasterProcedureEnv env,\n         setNextStateAfterRefreshBegin();\n         return Flow.HAS_MORE_STATE;\n       case REPLAY_REMOTE_WAL_IN_PEER:\n-        replayRemoteWAL();\n+        replayRemoteWAL(env.getReplicationPeerManager().getPeerConfig(peerId).get().isSerial());\n         setNextState(\n           PeerSyncReplicationStateTransitionState.TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE);\n         return Flow.HAS_MORE_STATE;",
                "deletions": 3
            },
            {
                "sha": "5991cf048e30d479a7d261a05460c6c7430b3f97",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java",
                "status": "added",
                "changes": 108,
                "additions": 108,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ZKSyncReplicationReplayWALWorkerStorage.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -0,0 +1,108 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.replication;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n+import org.apache.hadoop.hbase.replication.ZKReplicationStorageBase;\n+import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n+import org.apache.hadoop.hbase.zookeeper.ZKWatcher;\n+import org.apache.hadoop.hbase.zookeeper.ZNodePaths;\n+import org.apache.yetus.audience.InterfaceAudience;\n+import org.apache.zookeeper.KeeperException;\n+\n+@InterfaceAudience.Private\n+public class ZKSyncReplicationReplayWALWorkerStorage extends ZKReplicationStorageBase {\n+\n+  public static final String WORKERS_ZNODE = \"zookeeper.znode.sync.replication.replaywal.workers\";\n+\n+  public static final String WORKERS_ZNODE_DEFAULT = \"replaywal-workers\";\n+\n+  /**\n+   * The name of the znode that contains a list of workers to replay wal.\n+   */\n+  private final String workersZNode;\n+\n+  public ZKSyncReplicationReplayWALWorkerStorage(ZKWatcher zookeeper, Configuration conf) {\n+    super(zookeeper, conf);\n+    String workersZNodeName = conf.get(WORKERS_ZNODE, WORKERS_ZNODE_DEFAULT);\n+    workersZNode = ZNodePaths.joinZNode(replicationZNode, workersZNodeName);\n+  }\n+\n+  private String getPeerNode(String peerId) {\n+    return ZNodePaths.joinZNode(workersZNode, peerId);\n+  }\n+\n+  public void addPeer(String peerId) throws ReplicationException {\n+    try {\n+      ZKUtil.createWithParents(zookeeper, getPeerNode(peerId));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\n+          \"Failed to add peer id=\" + peerId + \" to replaywal-workers storage\", e);\n+    }\n+  }\n+\n+  public void removePeer(String peerId) throws ReplicationException {\n+    try {\n+      ZKUtil.deleteNodeRecursively(zookeeper, getPeerNode(peerId));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\n+          \"Failed to remove peer id=\" + peerId + \" to replaywal-workers storage\", e);\n+    }\n+  }\n+\n+  private String getPeerWorkerNode(String peerId, ServerName worker) {\n+    return ZNodePaths.joinZNode(getPeerNode(peerId), worker.getServerName());\n+  }\n+\n+  public void addPeerWorker(String peerId, ServerName worker) throws ReplicationException {\n+    try {\n+      ZKUtil.createWithParents(zookeeper, getPeerWorkerNode(peerId, worker));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\"Failed to add worker=\" + worker + \" for peer id=\" + peerId,\n+          e);\n+    }\n+  }\n+\n+  public void removePeerWorker(String peerId, ServerName worker) throws ReplicationException {\n+    try {\n+      ZKUtil.deleteNode(zookeeper, getPeerWorkerNode(peerId, worker));\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\"Failed to remove worker=\" + worker + \" for peer id=\" + peerId,\n+          e);\n+    }\n+  }\n+\n+  public Set<ServerName> getPeerWorkers(String peerId) throws ReplicationException {\n+    try {\n+      List<String> children = ZKUtil.listChildrenNoWatch(zookeeper, getPeerNode(peerId));\n+      if (children == null) {\n+        return new HashSet<>();\n+      }\n+      return children.stream().map(ServerName::valueOf).collect(Collectors.toSet());\n+    } catch (KeeperException e) {\n+      throw new ReplicationException(\"Failed to list workers for peer id=\" + peerId, e);\n+    }\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "24963f1ad020198af6edc108655b4fccf6cb1aef",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java",
                "status": "modified",
                "changes": 46,
                "additions": 31,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -21,6 +21,8 @@\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.concurrent.locks.Lock;\n+\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -32,6 +34,7 @@\n import org.apache.hadoop.hbase.regionserver.HRegionServer;\n import org.apache.hadoop.hbase.regionserver.wal.WALUtil;\n import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.hbase.util.KeyLocker;\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.wal.WAL.Entry;\n import org.apache.hadoop.hbase.wal.WAL.Reader;\n@@ -68,31 +71,28 @@\n \n   private String peerId;\n \n-  private String wal;\n+  private List<String> wals = new ArrayList<>();\n \n   private Exception initError;\n \n   private long batchSize;\n \n+  private final KeyLocker<String> peersLock = new KeyLocker<>();\n+\n   @Override\n   public Void call() throws Exception {\n     if (initError != null) {\n       throw initError;\n     }\n-    LOG.info(\"Received a replay sync replication wal {} event, peerId={}\", wal, peerId);\n+    LOG.info(\"Received a replay sync replication wals {} event, peerId={}\", wals, peerId);\n     if (rs.getReplicationSinkService() != null) {\n-      try (Reader reader = getReader()) {\n-        List<Entry> entries = readWALEntries(reader);\n-        while (!entries.isEmpty()) {\n-          Pair<AdminProtos.ReplicateWALEntryRequest, CellScanner> pair = ReplicationProtbufUtil\n-              .buildReplicateWALEntryRequest(entries.toArray(new Entry[entries.size()]));\n-          ReplicateWALEntryRequest request = pair.getFirst();\n-          rs.getReplicationSinkService().replicateLogEntries(request.getEntryList(),\n-            pair.getSecond(), request.getReplicationClusterId(),\n-            request.getSourceBaseNamespaceDirPath(), request.getSourceHFileArchiveDirPath());\n-          // Read next entries.\n-          entries = readWALEntries(reader);\n+      Lock peerLock = peersLock.acquireLock(wals.get(0));\n+      try {\n+        for (String wal : wals) {\n+          replayWAL(wal);\n         }\n+      } finally {\n+        peerLock.unlock();\n       }\n     }\n     return null;\n@@ -107,7 +107,7 @@ public void init(byte[] parameter, HRegionServer rs) {\n       ReplaySyncReplicationWALParameter param =\n           ReplaySyncReplicationWALParameter.parseFrom(parameter);\n       this.peerId = param.getPeerId();\n-      this.wal = param.getWal();\n+      param.getWalList().forEach(this.wals::add);\n       this.batchSize = rs.getConfiguration().getLong(REPLAY_SYNC_REPLICATION_WAL_BATCH_SIZE,\n         DEFAULT_REPLAY_SYNC_REPLICATION_WAL_BATCH_SIZE);\n     } catch (InvalidProtocolBufferException e) {\n@@ -120,7 +120,23 @@ public EventType getEventType() {\n     return EventType.RS_REPLAY_SYNC_REPLICATION_WAL;\n   }\n \n-  private Reader getReader() throws IOException {\n+  private void replayWAL(String wal) throws IOException {\n+    try (Reader reader = getReader(wal)) {\n+      List<Entry> entries = readWALEntries(reader);\n+      while (!entries.isEmpty()) {\n+        Pair<AdminProtos.ReplicateWALEntryRequest, CellScanner> pair = ReplicationProtbufUtil\n+            .buildReplicateWALEntryRequest(entries.toArray(new Entry[entries.size()]));\n+        ReplicateWALEntryRequest request = pair.getFirst();\n+        rs.getReplicationSinkService().replicateLogEntries(request.getEntryList(),\n+            pair.getSecond(), request.getReplicationClusterId(),\n+            request.getSourceBaseNamespaceDirPath(), request.getSourceHFileArchiveDirPath());\n+        // Read next entries.\n+        entries = readWALEntries(reader);\n+      }\n+    }\n+  }\n+\n+  private Reader getReader(String wal) throws IOException {\n     Path path = new Path(rs.getWALRootDir(), wal);\n     long length = rs.getWALFileSystem().getFileStatus(path).getLen();\n     try {",
                "deletions": 15
            },
            {
                "sha": "ac20dbd0e2ba2096f8ca8de66e6658f742ea8cc3",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -41,8 +41,8 @@\n import org.apache.hadoop.hbase.master.locking.LockManager;\n import org.apache.hadoop.hbase.master.normalizer.RegionNormalizer;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n import org.apache.hadoop.hbase.master.replication.ReplicationPeerManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;\n import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;\n import org.apache.hadoop.hbase.procedure2.LockedResource;\n@@ -476,7 +476,7 @@ public long transitReplicationPeerSyncReplicationState(String peerId,\n   }\n \n   @Override\n-  public ReplaySyncReplicationWALManager getReplaySyncReplicationWALManager() {\n+  public SyncReplicationReplayWALManager getSyncReplicationReplayWALManager() {\n     return null;\n   }\n }\n\\ No newline at end of file",
                "deletions": 2
            },
            {
                "sha": "f765139099af3ceded1f50f0eac9690bb2faa873",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SyncReplicationTestBase.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -78,7 +78,7 @@\n \n   protected static Path REMOTE_WAL_DIR2;\n \n-  private static void initTestingUtility(HBaseTestingUtility util, String zkParent) {\n+  protected static void initTestingUtility(HBaseTestingUtility util, String zkParent) {\n     util.setZkCluster(ZK_UTIL.getZkCluster());\n     Configuration conf = util.getConfiguration();\n     conf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, zkParent);\n@@ -102,8 +102,8 @@ public static void setUp() throws Exception {\n     ZK_UTIL.startMiniZKCluster();\n     initTestingUtility(UTIL1, \"/cluster1\");\n     initTestingUtility(UTIL2, \"/cluster2\");\n-    UTIL1.startMiniCluster(3);\n-    UTIL2.startMiniCluster(3);\n+    UTIL1.startMiniCluster(2,3);\n+    UTIL2.startMiniCluster(2,3);\n     TableDescriptor td =\n       TableDescriptorBuilder.newBuilder(TABLE_NAME).setColumnFamily(ColumnFamilyDescriptorBuilder\n         .newBuilder(CF).setScope(HConstants.REPLICATION_SCOPE_GLOBAL).build()).build();",
                "deletions": 3
            },
            {
                "sha": "6265f5cce7c54f37fed17704a1778760333c61d4",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java",
                "status": "added",
                "changes": 88,
                "additions": 88,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillMaster.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -0,0 +1,88 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.master.MasterFileSystem;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestSyncReplicationStandbyKillMaster extends SyncReplicationTestBase {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestSyncReplicationStandbyKillMaster.class);\n+\n+  private final long SLEEP_TIME = 2000;\n+\n+  private final int COUNT = 1000;\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestSyncReplicationStandbyKillMaster.class);\n+\n+  @Test\n+  public void testStandbyKillMaster() throws Exception {\n+    MasterFileSystem mfs = UTIL2.getHBaseCluster().getMaster().getMasterFileSystem();\n+    Path remoteWALDir = getRemoteWALDir(mfs, PEER_ID);\n+    assertFalse(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.STANDBY);\n+    assertTrue(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.ACTIVE);\n+\n+    // Disable async replication and write data, then shutdown\n+    UTIL1.getAdmin().disableReplicationPeer(PEER_ID);\n+    write(UTIL1, 0, COUNT);\n+    UTIL1.shutdownMiniCluster();\n+\n+    Thread t = new Thread(() -> {\n+      try {\n+        Thread.sleep(SLEEP_TIME);\n+        UTIL2.getMiniHBaseCluster().getMaster().stop(\"Stop master for test\");\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to stop master\", e);\n+      }\n+    });\n+    t.start();\n+\n+    // Transit standby to DA to replay logs\n+    try {\n+      UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+          SyncReplicationState.DOWNGRADE_ACTIVE);\n+    } catch (Exception e) {\n+      LOG.error(\"Failed to transit standby cluster to \" + SyncReplicationState.DOWNGRADE_ACTIVE);\n+    }\n+\n+    while (UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID)\n+        != SyncReplicationState.DOWNGRADE_ACTIVE) {\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    verify(UTIL2, 0, COUNT);\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "3c9724f260ee164e49e8ab0262429c2316c103e6",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java",
                "status": "added",
                "changes": 119,
                "additions": 119,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationStandbyKillRS.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -0,0 +1,119 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.master.MasterFileSystem;\n+import org.apache.hadoop.hbase.master.ServerManager;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestSyncReplicationStandbyKillRS extends SyncReplicationTestBase {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestSyncReplicationStandbyKillRS.class);\n+\n+  private final long SLEEP_TIME = 1000;\n+\n+  private final int COUNT = 1000;\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestSyncReplicationStandbyKillRS.class);\n+\n+  @Test\n+  public void testStandbyKillRegionServer() throws Exception {\n+    MasterFileSystem mfs = UTIL2.getHBaseCluster().getMaster().getMasterFileSystem();\n+    Path remoteWALDir = getRemoteWALDir(mfs, PEER_ID);\n+    assertFalse(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.STANDBY);\n+    assertTrue(mfs.getWALFileSystem().exists(remoteWALDir));\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+        SyncReplicationState.ACTIVE);\n+\n+    // Disable async replication and write data, then shutdown\n+    UTIL1.getAdmin().disableReplicationPeer(PEER_ID);\n+    write(UTIL1, 0, COUNT);\n+    UTIL1.shutdownMiniCluster();\n+\n+    JVMClusterUtil.MasterThread activeMaster = UTIL2.getMiniHBaseCluster().getMasterThread();\n+    Thread t = new Thread(() -> {\n+      try {\n+        List<JVMClusterUtil.RegionServerThread> regionServers =\n+            UTIL2.getMiniHBaseCluster().getLiveRegionServerThreads();\n+        for (JVMClusterUtil.RegionServerThread rst : regionServers) {\n+          ServerName serverName = rst.getRegionServer().getServerName();\n+          rst.getRegionServer().stop(\"Stop RS for test\");\n+          waitForRSShutdownToStartAndFinish(activeMaster, serverName);\n+          JVMClusterUtil.RegionServerThread restarted =\n+              UTIL2.getMiniHBaseCluster().startRegionServer();\n+          restarted.waitForServerOnline();\n+        }\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to kill RS\", e);\n+      }\n+    });\n+    t.start();\n+\n+    // Transit standby to DA to replay logs\n+    try {\n+      UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+          SyncReplicationState.DOWNGRADE_ACTIVE);\n+    } catch (Exception e) {\n+      LOG.error(\"Failed to transit standby cluster to \" + SyncReplicationState.DOWNGRADE_ACTIVE);\n+    }\n+\n+    while (UTIL2.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID)\n+        != SyncReplicationState.DOWNGRADE_ACTIVE) {\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    verify(UTIL2, 0, COUNT);\n+  }\n+\n+  private void waitForRSShutdownToStartAndFinish(JVMClusterUtil.MasterThread activeMaster,\n+      ServerName serverName) throws InterruptedException {\n+    ServerManager sm = activeMaster.getMaster().getServerManager();\n+    // First wait for it to be in dead list\n+    while (!sm.getDeadServers().isDeadServer(serverName)) {\n+      LOG.debug(\"Waiting for [\" + serverName + \"] to be listed as dead in master\");\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    LOG.debug(\"Server [\" + serverName + \"] marked as dead, waiting for it to \" +\n+        \"finish dead processing\");\n+    while (sm.areDeadServersInProgress()) {\n+      LOG.debug(\"Server [\" + serverName + \"] still being processed, waiting\");\n+      Thread.sleep(SLEEP_TIME);\n+    }\n+    LOG.debug(\"Server [\" + serverName + \"] done with server shutdown processing\");\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "d01a0ac61ad260c8c926eafdbb012e0d5235eb6a",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/44ca13fe07dc5050a2bc98ccd3f65953f06aaef8/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/master/TestRecoverStandbyProcedure.java?ref=44ca13fe07dc5050a2bc98ccd3f65953f06aaef8",
                "patch": "@@ -40,7 +40,7 @@\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n import org.apache.hadoop.hbase.master.replication.RecoverStandbyProcedure;\n-import org.apache.hadoop.hbase.master.replication.ReplaySyncReplicationWALManager;\n+import org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager;\n import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;\n import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;\n import org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter;\n@@ -92,7 +92,7 @@\n \n   private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();\n \n-  private static ReplaySyncReplicationWALManager replaySyncReplicationWALManager;\n+  private static SyncReplicationReplayWALManager syncReplicationReplayWALManager;\n \n   private static ProcedureExecutor<MasterProcedureEnv> procExec;\n \n@@ -107,7 +107,7 @@ public static void setupCluster() throws Exception {\n     conf = UTIL.getConfiguration();\n     HMaster master = UTIL.getHBaseCluster().getMaster();\n     fs = master.getMasterFileSystem().getWALFileSystem();\n-    replaySyncReplicationWALManager = master.getReplaySyncReplicationWALManager();\n+    syncReplicationReplayWALManager = master.getSyncReplicationReplayWALManager();\n     procExec = master.getMasterProcedureExecutor();\n   }\n \n@@ -138,7 +138,7 @@ public void tearDownAfterTest() throws IOException {\n   @Test\n   public void testRecoverStandby() throws IOException, StreamLacksCapabilityException {\n     setupSyncReplicationWALs();\n-    long procId = procExec.submitProcedure(new RecoverStandbyProcedure(PEER_ID));\n+    long procId = procExec.submitProcedure(new RecoverStandbyProcedure(PEER_ID, false));\n     ProcedureTestingUtility.waitProcedure(procExec, procId);\n     ProcedureTestingUtility.assertProcNotFailed(procExec, procId);\n \n@@ -153,7 +153,7 @@ public void testRecoverStandby() throws IOException, StreamLacksCapabilityExcept\n \n   private void setupSyncReplicationWALs() throws IOException, StreamLacksCapabilityException {\n     Path peerRemoteWALDir = ReplicationUtils\n-      .getPeerRemoteWALDir(replaySyncReplicationWALManager.getRemoteWALDir(), PEER_ID);\n+      .getPeerRemoteWALDir(syncReplicationReplayWALManager.getRemoteWALDir(), PEER_ID);\n     if (!fs.exists(peerRemoteWALDir)) {\n       fs.mkdirs(peerRemoteWALDir);\n     }",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19980 NullPointerException when restoring a snapshot after splitting a region\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/d0f2d18ca73737764550b319f749a51c876cca39",
        "parent": "https://github.com/apache/hbase/commit/8d26736bc2b0c28efd5caa3be7d8c9037dba633a",
        "bug_id": "hbase_43",
        "file": [
            {
                "sha": "c4f0e258cb77e723bbb0c145555b093ad8eb404e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java",
                "blob_url": "https://github.com/apache/hbase/blob/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java",
                "raw_url": "https://github.com/apache/hbase/raw/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java",
                "status": "modified",
                "changes": 89,
                "additions": 53,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java?ref=d0f2d18ca73737764550b319f749a51c876cca39",
                "patch": "@@ -195,11 +195,33 @@ private RestoreMetaChanges restoreHdfsRegions(final ThreadPoolExecutor exec) thr\n     // this instance, by removing the regions already present in the restore dir.\n     Set<String> regionNames = new HashSet<>(regionManifests.keySet());\n \n+    List<RegionInfo> tableRegions = getTableRegions();\n+\n     RegionInfo mobRegion = MobUtils.getMobRegionInfo(snapshotManifest.getTableDescriptor()\n         .getTableName());\n+    if (tableRegions != null) {\n+      // restore the mob region in case\n+      if (regionNames.contains(mobRegion.getEncodedName())) {\n+        monitor.rethrowException();\n+        status.setStatus(\"Restoring mob region...\");\n+        List<RegionInfo> mobRegions = new ArrayList<>(1);\n+        mobRegions.add(mobRegion);\n+        restoreHdfsMobRegions(exec, regionManifests, mobRegions);\n+        regionNames.remove(mobRegion.getEncodedName());\n+        status.setStatus(\"Finished restoring mob region.\");\n+      }\n+    }\n+    if (regionNames.contains(mobRegion.getEncodedName())) {\n+      // add the mob region\n+      monitor.rethrowException();\n+      status.setStatus(\"Cloning mob region...\");\n+      cloneHdfsMobRegion(regionManifests, mobRegion);\n+      regionNames.remove(mobRegion.getEncodedName());\n+      status.setStatus(\"Finished cloning mob region.\");\n+    }\n+\n     // Identify which region are still available and which not.\n     // NOTE: we rely upon the region name as: \"table name, start key, end key\"\n-    List<RegionInfo> tableRegions = getTableRegions();\n     if (tableRegions != null) {\n       monitor.rethrowException();\n       for (RegionInfo regionInfo: tableRegions) {\n@@ -213,50 +235,40 @@ private RestoreMetaChanges restoreHdfsRegions(final ThreadPoolExecutor exec) thr\n           metaChanges.addRegionToRemove(regionInfo);\n         }\n       }\n-\n-      // Restore regions using the snapshot data\n-      monitor.rethrowException();\n-      status.setStatus(\"Restoring table regions...\");\n-      if (regionNames.contains(mobRegion.getEncodedName())) {\n-        // restore the mob region in case\n-        List<RegionInfo> mobRegions = new ArrayList<>(1);\n-        mobRegions.add(mobRegion);\n-        restoreHdfsMobRegions(exec, regionManifests, mobRegions);\n-        regionNames.remove(mobRegion.getEncodedName());\n-      }\n-      restoreHdfsRegions(exec, regionManifests, metaChanges.getRegionsToRestore());\n-      status.setStatus(\"Finished restoring all table regions.\");\n-\n-      // Remove regions from the current table\n-      monitor.rethrowException();\n-      status.setStatus(\"Starting to delete excess regions from table\");\n-      removeHdfsRegions(exec, metaChanges.getRegionsToRemove());\n-      status.setStatus(\"Finished deleting excess regions from table.\");\n     }\n \n     // Regions to Add: present in the snapshot but not in the current table\n+    List<RegionInfo> regionsToAdd = new ArrayList<>(regionNames.size());\n     if (regionNames.size() > 0) {\n-      List<RegionInfo> regionsToAdd = new ArrayList<>(regionNames.size());\n-\n       monitor.rethrowException();\n-      // add the mob region\n-      if (regionNames.contains(mobRegion.getEncodedName())) {\n-        cloneHdfsMobRegion(regionManifests, mobRegion);\n-        regionNames.remove(mobRegion.getEncodedName());\n-      }\n       for (String regionName: regionNames) {\n         LOG.info(\"region to add: \" + regionName);\n-        regionsToAdd.add(ProtobufUtil.toRegionInfo(regionManifests.get(regionName).getRegionInfo()));\n+        regionsToAdd.add(ProtobufUtil.toRegionInfo(regionManifests.get(regionName)\n+            .getRegionInfo()));\n       }\n-\n-      // Create new regions cloning from the snapshot\n-      monitor.rethrowException();\n-      status.setStatus(\"Cloning regions...\");\n-      RegionInfo[] clonedRegions = cloneHdfsRegions(exec, regionManifests, regionsToAdd);\n-      metaChanges.setNewRegions(clonedRegions);\n-      status.setStatus(\"Finished cloning regions.\");\n     }\n \n+    // Create new regions cloning from the snapshot\n+    // HBASE-19980: We need to call cloneHdfsRegions() before restoreHdfsRegions() because\n+    // regionsMap is constructed in cloneHdfsRegions() and it can be used in restoreHdfsRegions().\n+    monitor.rethrowException();\n+    status.setStatus(\"Cloning regions...\");\n+    RegionInfo[] clonedRegions = cloneHdfsRegions(exec, regionManifests, regionsToAdd);\n+    metaChanges.setNewRegions(clonedRegions);\n+    status.setStatus(\"Finished cloning regions.\");\n+\n+    // Restore regions using the snapshot data\n+    monitor.rethrowException();\n+    status.setStatus(\"Restoring table regions...\");\n+    restoreHdfsRegions(exec, regionManifests, metaChanges.getRegionsToRestore());\n+    status.setStatus(\"Finished restoring all table regions.\");\n+\n+    // Remove regions from the current table\n+    monitor.rethrowException();\n+    status.setStatus(\"Starting to delete excess regions from table\");\n+    removeHdfsRegions(exec, metaChanges.getRegionsToRemove());\n+    status.setStatus(\"Finished deleting excess regions from table.\");\n+\n     LOG.info(\"finishing restore table regions using snapshot=\" + snapshotDesc);\n \n     return metaChanges;\n@@ -742,11 +754,16 @@ private void restoreReferenceFile(final Path familyDir, final RegionInfo regionI\n \n     // Add the daughter region to the map\n     String regionName = Bytes.toString(regionsMap.get(regionInfo.getEncodedNameAsBytes()));\n+    if (regionName == null) {\n+      regionName = regionInfo.getEncodedName();\n+    }\n     LOG.debug(\"Restore reference \" + regionName + \" to \" + clonedRegionName);\n     synchronized (parentsMap) {\n       Pair<String, String> daughters = parentsMap.get(clonedRegionName);\n       if (daughters == null) {\n-        daughters = new Pair<>(regionName, null);\n+        // In case one side of the split is already compacted, regionName is put as both first and\n+        // second of Pair\n+        daughters = new Pair<>(regionName, regionName);\n         parentsMap.put(clonedRegionName, daughters);\n       } else if (!regionName.equals(daughters.getFirst())) {\n         daughters.setSecond(regionName);",
                "deletions": 36
            },
            {
                "sha": "eb8b20e124fbd24c01dd3c9997ed934fe4d0cc91",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/d0f2d18ca73737764550b319f749a51c876cca39/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java",
                "status": "modified",
                "changes": 20,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java?ref=d0f2d18ca73737764550b319f749a51c876cca39",
                "patch": "@@ -23,6 +23,7 @@\n \n import java.io.IOException;\n import java.util.HashSet;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -295,6 +296,25 @@ public void testCorruptedSnapshot() throws IOException, InterruptedException {\n     }\n   }\n \n+  @Test\n+  public void testRestoreSnapshotAfterSplittingRegions() throws IOException, InterruptedException {\n+    List<RegionInfo> regionInfos = admin.getRegions(tableName);\n+    RegionReplicaUtil.removeNonDefaultRegions(regionInfos);\n+\n+    // Split the first region\n+    splitRegion(regionInfos.get(0));\n+\n+    // Take a snapshot\n+    admin.snapshot(snapshotName1, tableName);\n+\n+    // Restore the snapshot\n+    admin.disableTable(tableName);\n+    admin.restoreSnapshot(snapshotName1);\n+    admin.enableTable(tableName);\n+\n+    verifyRowCount(TEST_UTIL, tableName, snapshot1Rows);\n+  }\n+\n   // ==========================================================================\n   //  Helpers\n   // ==========================================================================",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20678 NPE in ReplicationSourceManager#NodeFailoverWorker",
        "commit": "https://github.com/apache/hbase/commit/57c86717285d74f4604a277ee034878398568d81",
        "parent": "https://github.com/apache/hbase/commit/a45763df553edd006c2168df3a64f0f2cdf2366f",
        "bug_id": "hbase_44",
        "file": [
            {
                "sha": "cca8bfcab3da50d5d8440f2bc5d7ace8e18140a0",
                "filename": "hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "blob_url": "https://github.com/apache/hbase/blob/57c86717285d74f4604a277ee034878398568d81/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "raw_url": "https://github.com/apache/hbase/raw/57c86717285d74f4604a277ee034878398568d81/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
                "status": "modified",
                "changes": 9,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java?ref=57c86717285d74f4604a277ee034878398568d81",
                "patch": "@@ -393,10 +393,10 @@ public long getWALPosition(ServerName serverName, String queueId, String fileNam\n             \" failed when creating the node for \" + destServerName,\n           e);\n     }\n+    String newQueueId = queueId + \"-\" + sourceServerName;\n     try {\n       String oldQueueNode = getQueueNode(sourceServerName, queueId);\n       List<String> wals = ZKUtil.listChildrenNoWatch(zookeeper, oldQueueNode);\n-      String newQueueId = queueId + \"-\" + sourceServerName;\n       if (CollectionUtils.isEmpty(wals)) {\n         ZKUtil.deleteNodeFailSilent(zookeeper, oldQueueNode);\n         LOG.info(\"Removed empty {}/{}\", sourceServerName, queueId);\n@@ -427,11 +427,12 @@ public long getWALPosition(ServerName serverName, String queueId, String fileNam\n       return new Pair<>(newQueueId, logQueue);\n     } catch (NoNodeException | NodeExistsException | NotEmptyException | BadVersionException e) {\n       // Multi call failed; it looks like some other regionserver took away the logs.\n-      // These exceptions mean that zk tells us the request can not be execute so it is safe to just\n-      // return a null. For other types of exception should be thrown out to notify the upper layer.\n+      // These exceptions mean that zk tells us the request can not be execute. So return an empty\n+      // queue to tell the upper layer that claim nothing. For other types of exception should be\n+      // thrown out to notify the upper layer.\n       LOG.info(\"Claim queue queueId={} from {} to {} failed with {}, someone else took the log?\",\n           queueId,sourceServerName, destServerName, e.toString());\n-      return null;\n+      return new Pair<>(newQueueId, Collections.emptySortedSet());\n     } catch (KeeperException | InterruptedException e) {\n       throw new ReplicationException(\"Claim queue queueId=\" + queueId + \" from \" +\n         sourceServerName + \" to \" + destServerName + \" failed\", e);",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20670 NPE in HMaster#isInMaintenanceMode",
        "commit": "https://github.com/apache/hbase/commit/a45763df553edd006c2168df3a64f0f2cdf2366f",
        "parent": "https://github.com/apache/hbase/commit/832f67d483985bc6cf488bb8ecef32280fdee668",
        "bug_id": "hbase_45",
        "file": [
            {
                "sha": "0871482a5a0f79d1b278fe013f3191b5cc1bb8fc",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/a45763df553edd006c2168df3a64f0f2cdf2366f/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/a45763df553edd006c2168df3a64f0f2cdf2366f/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 25,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=a45763df553edd006c2168df3a64f0f2cdf2366f",
                "patch": "@@ -2822,7 +2822,10 @@ public boolean isInitialized() {\n    * @return true if master is in maintenanceMode\n    */\n   @Override\n-  public boolean isInMaintenanceMode() {\n+  public boolean isInMaintenanceMode() throws IOException {\n+    if (!isInitialized()) {\n+      throw new PleaseHoldException(\"Master is initializing\");\n+    }\n     return maintenanceModeTracker.isInMaintenanceMode();\n   }\n \n@@ -3359,7 +3362,11 @@ public void requestMobCompaction(TableName tableName,\n    * @return The state of the load balancer, or false if the load balancer isn't defined.\n    */\n   public boolean isBalancerOn() {\n-    if (null == loadBalancerTracker || isInMaintenanceMode()) {\n+    try {\n+      if (null == loadBalancerTracker || isInMaintenanceMode()) {\n+        return false;\n+      }\n+    } catch (IOException e) {\n       return false;\n     }\n     return loadBalancerTracker.isBalancerOn();\n@@ -3370,8 +3377,12 @@ public boolean isBalancerOn() {\n    * false is returned.\n    */\n   public boolean isNormalizerOn() {\n-    return (null == regionNormalizerTracker || isInMaintenanceMode()) ?\n-        false: regionNormalizerTracker.isNormalizerOn();\n+    try {\n+      return (null == regionNormalizerTracker || isInMaintenanceMode()) ?\n+          false: regionNormalizerTracker.isNormalizerOn();\n+    } catch (IOException e) {\n+      return false;\n+    }\n   }\n \n   /**\n@@ -3382,7 +3393,11 @@ public boolean isNormalizerOn() {\n    */\n   @Override\n   public boolean isSplitOrMergeEnabled(MasterSwitchType switchType) {\n-    if (null == splitOrMergeTracker || isInMaintenanceMode()) {\n+    try {\n+      if (null == splitOrMergeTracker || isInMaintenanceMode()) {\n+        return false;\n+      }\n+    } catch (IOException e) {\n       return false;\n     }\n     return splitOrMergeTracker.isSplitOrMergeEnabled(switchType);",
                "deletions": 5
            },
            {
                "sha": "d3202ce3c6d9aef6256af77e8d7992c48a78a284",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/a45763df553edd006c2168df3a64f0f2cdf2366f/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/a45763df553edd006c2168df3a64f0f2cdf2366f/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java?ref=a45763df553edd006c2168df3a64f0f2cdf2366f",
                "patch": "@@ -1491,7 +1491,11 @@ public IsInMaintenanceModeResponse isMasterInMaintenanceMode(\n       final RpcController controller,\n       final IsInMaintenanceModeRequest request) throws ServiceException {\n     IsInMaintenanceModeResponse.Builder response = IsInMaintenanceModeResponse.newBuilder();\n-    response.setInMaintenanceMode(master.isInMaintenanceMode());\n+    try {\n+      response.setInMaintenanceMode(master.isInMaintenanceMode());\n+    } catch (IOException e) {\n+      throw new ServiceException(e);\n+    }\n     return response.build();\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "3d2b9af51c2dd01cd05c792e2cb0c2e67ea0fd38",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/a45763df553edd006c2168df3a64f0f2cdf2366f/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/a45763df553edd006c2168df3a64f0f2cdf2366f/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java?ref=a45763df553edd006c2168df3a64f0f2cdf2366f",
                "patch": "@@ -352,8 +352,9 @@ long splitRegion(\n \n   /**\n    * @return true if master is in maintanceMode\n+   * @throws IOException if the inquiry failed due to an IO problem\n    */\n-  boolean isInMaintenanceMode();\n+  boolean isInMaintenanceMode() throws IOException;\n \n   /**\n    * Abort a procedure.",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21441 NPE if RS restarts between REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN and TRANSIT_PEER_NEW_SYNC_REPLICATION_STATE",
        "commit": "https://github.com/apache/hbase/commit/6d46b8d256bcd63349ea83e4a588b879a122854a",
        "parent": "https://github.com/apache/hbase/commit/86cbbdea9ec9bde73397619fceb3bac9625813a9",
        "bug_id": "hbase_46",
        "file": [
            {
                "sha": "799d9750edaf0b5ae0ccdc5a33bca038a8d00ba5",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "blob_url": "https://github.com/apache/hbase/blob/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "raw_url": "https://github.com/apache/hbase/raw/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "status": "modified",
                "changes": 11,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=6d46b8d256bcd63349ea83e4a588b879a122854a",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.hbase.replication.ReplicationStorageFactory;\n import org.apache.hadoop.hbase.replication.ReplicationTracker;\n import org.apache.hadoop.hbase.replication.ReplicationUtils;\n+import org.apache.hadoop.hbase.replication.SyncReplicationState;\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.wal.SyncReplicationWALProvider;\n import org.apache.hadoop.hbase.wal.WALProvider;\n@@ -137,6 +138,16 @@ public void initialize(Server server, FileSystem fs, Path logDir, Path oldLogDir\n         SyncReplicationWALProvider syncWALProvider = (SyncReplicationWALProvider) walProvider;\n         peerActionListener = syncWALProvider;\n         syncWALProvider.setPeerInfoProvider(syncReplicationPeerInfoProvider);\n+        // for sync replication state change, we need to reload the state twice, you can see the\n+        // code in PeerProcedureHandlerImpl, so here we need to go over the sync replication peers\n+        // to see if any of them are in the middle of the two refreshes, if so, we need to manually\n+        // repeat the action we have done in the first refresh, otherwise when the second refresh\n+        // comes we will be in trouble, such as NPE.\n+        replicationPeers.getAllPeerIds().stream().map(replicationPeers::getPeer)\n+            .filter(p -> p.getPeerConfig().isSyncReplication())\n+            .filter(p -> p.getNewSyncReplicationState() != SyncReplicationState.NONE)\n+            .forEach(p -> syncWALProvider.peerSyncReplicationStateChange(p.getId(),\n+              p.getSyncReplicationState(), p.getNewSyncReplicationState(), 0));\n       }\n     }\n     this.statsThreadPeriod =",
                "deletions": 0
            },
            {
                "sha": "86ad8c0c3f0b3baddf9d0459945188a2646e7840",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java",
                "blob_url": "https://github.com/apache/hbase/blob/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java",
                "raw_url": "https://github.com/apache/hbase/raw/6d46b8d256bcd63349ea83e4a588b879a122854a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java",
                "status": "added",
                "changes": 125,
                "additions": 125,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestSyncReplicationNewRSJoinBetweenRefreshes.java?ref=6d46b8d256bcd63349ea83e4a588b879a122854a",
                "patch": "@@ -0,0 +1,125 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.PeerSyncReplicationStateTransitionState.REOPEN_ALL_REGIONS_IN_PEER_VALUE;\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Optional;\n+import java.util.concurrent.CountDownLatch;\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.RegionServerObserver;\n+import org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+/**\n+ * Testcase for HBASE-21441.\n+ */\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestSyncReplicationNewRSJoinBetweenRefreshes extends SyncReplicationTestBase {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestSyncReplicationNewRSJoinBetweenRefreshes.class);\n+\n+  private static boolean HALT;\n+\n+  private static CountDownLatch ARRIVE;\n+\n+  private static CountDownLatch RESUME;\n+\n+  public static final class HaltCP implements RegionServerObserver, RegionServerCoprocessor {\n+\n+    @Override\n+    public Optional<RegionServerObserver> getRegionServerObserver() {\n+      return Optional.of(this);\n+    }\n+\n+    @Override\n+    public void postExecuteProcedures(ObserverContext<RegionServerCoprocessorEnvironment> ctx)\n+        throws IOException {\n+      synchronized (HaltCP.class) {\n+        if (!HALT) {\n+          return;\n+        }\n+        UTIL1.getMiniHBaseCluster().getMaster().getProcedures().stream()\n+          .filter(p -> p instanceof TransitPeerSyncReplicationStateProcedure)\n+          .filter(p -> !p.isFinished()).map(p -> (TransitPeerSyncReplicationStateProcedure) p)\n+          .findFirst().ifPresent(proc -> {\n+            // this is the next state of REFRESH_PEER_SYNC_REPLICATION_STATE_ON_RS_BEGIN_VALUE\n+            if (proc.getCurrentStateId() == REOPEN_ALL_REGIONS_IN_PEER_VALUE) {\n+              // tell the main thread to start a new region server\n+              ARRIVE.countDown();\n+              try {\n+                // wait for the region server to online\n+                RESUME.await();\n+              } catch (InterruptedException e) {\n+                throw new RuntimeException(e);\n+              }\n+              HALT = false;\n+            }\n+          });\n+      }\n+    }\n+  }\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    UTIL1.getConfiguration().setClass(CoprocessorHost.REGIONSERVER_COPROCESSOR_CONF_KEY,\n+      HaltCP.class, RegionServerObserver.class);\n+    SyncReplicationTestBase.setUp();\n+  }\n+\n+  @Test\n+  public void test() throws IOException, InterruptedException {\n+    UTIL2.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.STANDBY);\n+    UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+      SyncReplicationState.ACTIVE);\n+\n+    ARRIVE = new CountDownLatch(1);\n+    RESUME = new CountDownLatch(1);\n+    HALT = true;\n+    Thread t = new Thread(() -> {\n+      try {\n+        UTIL1.getAdmin().transitReplicationPeerSyncReplicationState(PEER_ID,\n+          SyncReplicationState.DOWNGRADE_ACTIVE);\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(e);\n+      }\n+    });\n+    t.start();\n+    ARRIVE.await();\n+    UTIL1.getMiniHBaseCluster().startRegionServer();\n+    RESUME.countDown();\n+    t.join();\n+    assertEquals(SyncReplicationState.DOWNGRADE_ACTIVE,\n+      UTIL1.getAdmin().getReplicationPeerSyncReplicationState(PEER_ID));\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21102 ServerCrashProcedure should select target server where no other replicas exist for the current region - addendum fixes NPE",
        "commit": "https://github.com/apache/hbase/commit/69431c75c16d8d863932815f0460322153a25dbb",
        "parent": "https://github.com/apache/hbase/commit/cebb725a9f4ab8dd9d3f306d21d53d6f56161c51",
        "bug_id": "hbase_47",
        "file": [
            {
                "sha": "6cca59fa7837a0c8b2677604ef5f5e30243d224b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/69431c75c16d8d863932815f0460322153a25dbb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/69431c75c16d8d863932815f0460322153a25dbb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=69431c75c16d8d863932815f0460322153a25dbb",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.master.RackManager;\n import org.apache.hadoop.hbase.master.RegionPlan;\n+import org.apache.hadoop.hbase.master.assignment.RegionStates;\n import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.Action.Type;\n import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;\n import org.apache.hbase.thirdparty.com.google.common.base.Joiner;\n@@ -1457,8 +1458,9 @@ public ServerName randomAssignment(RegionInfo regionInfo, List<ServerName> serve\n       // In the current set of regions even if one has region replica let us go with\n       // getting the entire snapshot\n       if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n-        if (!hasRegionReplica && this.services.getAssignmentManager().getRegionStates()\n-            .isReplicaAvailableForRegion(region)) {\n+        RegionStates states = this.services.getAssignmentManager().getRegionStates();\n+        if (!hasRegionReplica && states != null &&\n+            states.isReplicaAvailableForRegion(region)) {\n           hasRegionReplica = true;\n         }\n       }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20785 NPE getting metrics in PE testing scans",
        "commit": "https://github.com/apache/hbase/commit/ffe430237a44b1ebad31171bdf81ab7487ef5ce3",
        "parent": "https://github.com/apache/hbase/commit/952bb96c8a5c1f8a34237cab970ac41101bdd870",
        "bug_id": "hbase_48",
        "file": [
            {
                "sha": "1a72ececfcae0344534d92990be57657566fa6f6",
                "filename": "hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/ffe430237a44b1ebad31171bdf81ab7487ef5ce3/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/ffe430237a44b1ebad31171bdf81ab7487ef5ce3/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=ffe430237a44b1ebad31171bdf81ab7487ef5ce3",
                "patch": "@@ -1189,6 +1189,7 @@ void updateValueSize(final int valueSize) {\n     }\n \n     void updateScanMetrics(final ScanMetrics metrics) {\n+      if (metrics == null) return;\n       Map<String,Long> metricsMap = metrics.getMetricsMap();\n       Long rpcCalls = metricsMap.get(ScanMetrics.RPC_CALLS_METRIC_NAME);\n       if (rpcCalls != null) {",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-21204 NPE when scan raw DELETE_FAMILY_VERSION and codec is not set\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/cd161d976ef47b84e904f2d54bac65d2f3417c2a",
        "parent": "https://github.com/apache/hbase/commit/dc767c06d27c57e02d8963515317d54e89ddc718",
        "bug_id": "hbase_49",
        "file": [
            {
                "sha": "aada35329aafa97c63beae8c7c59b2131755b072",
                "filename": "hbase-protocol-shaded/src/main/protobuf/Cell.proto",
                "blob_url": "https://github.com/apache/hbase/blob/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol-shaded/src/main/protobuf/Cell.proto",
                "raw_url": "https://github.com/apache/hbase/raw/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol-shaded/src/main/protobuf/Cell.proto",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/Cell.proto?ref=cd161d976ef47b84e904f2d54bac65d2f3417c2a",
                "patch": "@@ -32,6 +32,7 @@ enum CellType {\n     PUT = 4;\n \n     DELETE = 8;\n+    DELETE_FAMILY_VERSION = 10;\n     DELETE_COLUMN = 12;\n     DELETE_FAMILY = 14;\n ",
                "deletions": 0
            },
            {
                "sha": "e518e658f63ba53ca9e9df58bc7f350377739a23",
                "filename": "hbase-protocol/src/main/protobuf/Cell.proto",
                "blob_url": "https://github.com/apache/hbase/blob/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol/src/main/protobuf/Cell.proto",
                "raw_url": "https://github.com/apache/hbase/raw/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-protocol/src/main/protobuf/Cell.proto",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol/src/main/protobuf/Cell.proto?ref=cd161d976ef47b84e904f2d54bac65d2f3417c2a",
                "patch": "@@ -32,6 +32,7 @@ enum CellType {\n     PUT = 4;\n \n     DELETE = 8;\n+    DELETE_FAMILY_VERSION = 10;\n     DELETE_COLUMN = 12;\n     DELETE_FAMILY = 14;\n ",
                "deletions": 0
            },
            {
                "sha": "5e8d107d4f58b7cd31764e1a08beca16b5930283",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java",
                "blob_url": "https://github.com/apache/hbase/blob/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java",
                "raw_url": "https://github.com/apache/hbase/raw/cd161d976ef47b84e904f2d54bac65d2f3417c2a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java",
                "status": "modified",
                "changes": 26,
                "additions": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java?ref=cd161d976ef47b84e904f2d54bac65d2f3417c2a",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.client;\n \n+import static org.apache.hadoop.hbase.HConstants.RPC_CODEC_CONF_KEY;\n+import static org.apache.hadoop.hbase.ipc.RpcClient.DEFAULT_CODEC_CLASS;\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -608,6 +610,30 @@ public void testGetRowOffset() throws Exception {\n        \"Testing offset + multiple CFs + maxResults\");\n   }\n \n+  @Test\n+  public void testScanRawDeleteFamilyVersion() throws Exception {\n+    TableName tableName = TableName.valueOf(name.getMethodName());\n+    TEST_UTIL.createTable(tableName, FAMILY);\n+    Configuration conf = new Configuration(TEST_UTIL.getConfiguration());\n+    conf.set(RPC_CODEC_CONF_KEY, \"\");\n+    conf.set(DEFAULT_CODEC_CLASS, \"\");\n+    try (Connection connection = ConnectionFactory.createConnection(conf);\n+        Table table = connection.getTable(tableName)) {\n+      Delete delete = new Delete(ROW);\n+      delete.addFamilyVersion(FAMILY, 0L);\n+      table.delete(delete);\n+      Scan scan = new Scan(ROW).setRaw(true);\n+      ResultScanner scanner = table.getScanner(scan);\n+      int count = 0;\n+      while (scanner.next() != null) {\n+        count++;\n+      }\n+      assertEquals(1, count);\n+    } finally {\n+      TEST_UTIL.deleteTable(tableName);\n+    }\n+  }\n+\n   /**\n    * Test from client side for scan while the region is reopened\n    * on the same region server.",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16359 NullPointerException in RSRpcServices.openRegion()",
        "commit": "https://github.com/apache/hbase/commit/5e23b3aad5fffbb564d793f1d722d152e39f628a",
        "parent": "https://github.com/apache/hbase/commit/c9f84e8137c451c797101146c9049f6dd440c662",
        "bug_id": "hbase_50",
        "file": [
            {
                "sha": "0de4a285705a053c1aaca08714caf5a5e9cb4c2a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/5e23b3aad5fffbb564d793f1d722d152e39f628a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/5e23b3aad5fffbb564d793f1d722d152e39f628a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=5e23b3aad5fffbb564d793f1d722d152e39f628a",
                "patch": "@@ -1741,6 +1741,9 @@ public OpenRegionResponse openRegion(final RpcController controller,\n           } else {\n             regionServer.updateRegionFavoredNodesMapping(region.getEncodedName(),\n               regionOpenInfo.getFavoredNodesList());\n+            if (htd == null) {\n+              throw new IOException(\"Missing table descriptor for \" + region.getEncodedName());\n+            }\n             if (htd.getPriority() >= HConstants.ADMIN_QOS || region.getTable().isSystemTable()) {\n               regionServer.service.submit(new OpenPriorityRegionHandler(\n                 regionServer, regionServer, region, htd, masterSystemTime));",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20531 RS may throw NPE when close meta regions in shutdown procedure.",
        "commit": "https://github.com/apache/hbase/commit/971f5350e81591e9360677f3617a399f453b6b96",
        "parent": "https://github.com/apache/hbase/commit/acd0d1e446c164d9c54bfb461b2d449c8d717c07",
        "bug_id": "hbase_51",
        "file": [
            {
                "sha": "188aef633bff2fecf03bc033e1f5d863dfed914c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/971f5350e81591e9360677f3617a399f453b6b96/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/971f5350e81591e9360677f3617a399f453b6b96/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 22,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=971f5350e81591e9360677f3617a399f453b6b96",
                "patch": "@@ -1052,15 +1052,6 @@ public void run() {\n     if (this.storefileRefresher != null) this.storefileRefresher.cancel(true);\n     sendShutdownInterrupt();\n \n-    // Stop the quota manager\n-    if (rsQuotaManager != null) {\n-      rsQuotaManager.stop();\n-    }\n-    if (rsSpaceQuotaManager != null) {\n-      rsSpaceQuotaManager.stop();\n-      rsSpaceQuotaManager = null;\n-    }\n-\n     // Stop the snapshot and other procedure handlers, forcefully killing all running tasks\n     if (rspmHost != null) {\n       rspmHost.stop(this.abortRequested || this.killed);\n@@ -1106,6 +1097,15 @@ public void run() {\n       LOG.info(\"stopping server \" + this.serverName + \"; all regions closed.\");\n     }\n \n+    // Stop the quota manager\n+    if (rsQuotaManager != null) {\n+      rsQuotaManager.stop();\n+    }\n+    if (rsSpaceQuotaManager != null) {\n+      rsSpaceQuotaManager.stop();\n+      rsSpaceQuotaManager = null;\n+    }\n+\n     //fsOk flag may be changed when closing regions throws exception.\n     if (this.fsOk) {\n       shutdownWAL(!abortRequested);\n@@ -3697,9 +3697,9 @@ public RegionServerSpaceQuotaManager getRegionServerSpaceQuotaManager() {\n \n   @Override\n   public boolean reportFileArchivalForQuotas(TableName tableName,\n-      Collection<Entry<String,Long>> archivedFiles) {\n+      Collection<Entry<String, Long>> archivedFiles) {\n     RegionServerStatusService.BlockingInterface rss = rssStub;\n-    if (rss == null) {\n+    if (rss == null || rsSpaceQuotaManager == null) {\n       // the current server could be stopping.\n       LOG.trace(\"Skipping file archival reporting to HMaster as stub is null\");\n       return false;",
                "deletions": 11
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20169 NPE when calling HBTU.shutdownMiniCluster (TestAssignmentManagerMetrics is flakey); AMENDMENT",
        "commit": "https://github.com/apache/hbase/commit/4cb444e77b41cdb733544770a471068256d65bbe",
        "parent": "https://github.com/apache/hbase/commit/c4b4023b60dd51a7e0ad83883afec569037e5329",
        "bug_id": "hbase_52",
        "file": [
            {
                "sha": "3a75d33dd261aeac09b1859c3347f2a4d40a6216",
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "blob_url": "https://github.com/apache/hbase/blob/4cb444e77b41cdb733544770a471068256d65bbe/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "raw_url": "https://github.com/apache/hbase/raw/4cb444e77b41cdb733544770a471068256d65bbe/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "status": "modified",
                "changes": 41,
                "additions": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java?ref=4cb444e77b41cdb733544770a471068256d65bbe",
                "patch": "@@ -264,9 +264,31 @@ protected void periodicExecute(final TEnvironment env) {\n   private final CopyOnWriteArrayList<ProcedureExecutorListener> listeners = new CopyOnWriteArrayList<>();\n \n   private Configuration conf;\n+\n+  /**\n+   * Created in the {@link #start(int, boolean)} method. Destroyed in {@link #join()} (FIX! Doing\n+   * resource handling rather than observing in a #join is unexpected).\n+   * Overridden when we do the ProcedureTestingUtility.testRecoveryAndDoubleExecution trickery\n+   * (Should be ok).\n+   */\n   private ThreadGroup threadGroup;\n+\n+  /**\n+   * Created in the {@link #start(int, boolean)} method. Terminated in {@link #join()} (FIX! Doing\n+   * resource handling rather than observing in a #join is unexpected).\n+   * Overridden when we do the ProcedureTestingUtility.testRecoveryAndDoubleExecution trickery\n+   * (Should be ok).\n+   */\n   private CopyOnWriteArrayList<WorkerThread> workerThreads;\n+\n+  /**\n+   * Created in the {@link #start(int, boolean)} method. Terminated in {@link #join()} (FIX! Doing\n+   * resource handling rather than observing in a #join is unexpected).\n+   * Overridden when we do the ProcedureTestingUtility.testRecoveryAndDoubleExecution trickery\n+   * (Should be ok).\n+   */\n   private TimeoutExecutorThread timeoutExecutor;\n+\n   private int corePoolSize;\n   private int maxPoolSize;\n \n@@ -299,6 +321,7 @@ public ProcedureExecutor(final Configuration conf, final TEnvironment environmen\n     this.conf = conf;\n     this.checkOwnerSet = conf.getBoolean(CHECK_OWNER_SET_CONF_KEY, DEFAULT_CHECK_OWNER_SET);\n     refreshConfiguration(conf);\n+\n   }\n \n   private void load(final boolean abortOnCorruption) throws IOException {\n@@ -510,11 +533,8 @@ public void start(int numThreads, boolean abortOnCorruption) throws IOException\n     LOG.info(\"Starting {} core workers (bigger of cpus/4 or 16) with max (burst) worker count={}\",\n         corePoolSize, maxPoolSize);\n \n-    // Create the Thread Group for the executors\n-    threadGroup = new ThreadGroup(\"PEWorkerGroup\");\n-\n-    // Create the timeout executor\n-    timeoutExecutor = new TimeoutExecutorThread(this, threadGroup);\n+    this.threadGroup = new ThreadGroup(\"PEWorkerGroup\");\n+    this.timeoutExecutor = new TimeoutExecutorThread(this, threadGroup);\n \n     // Create the workers\n     workerId.set(0);\n@@ -576,22 +596,21 @@ public void join() {\n \n     // stop the timeout executor\n     timeoutExecutor.awaitTermination();\n-    timeoutExecutor = null;\n \n     // stop the worker threads\n     for (WorkerThread worker: workerThreads) {\n       worker.awaitTermination();\n     }\n-    workerThreads = null;\n \n     // Destroy the Thread Group for the executors\n+    // TODO: Fix. #join is not place to destroy resources.\n     try {\n       threadGroup.destroy();\n     } catch (IllegalThreadStateException e) {\n-      LOG.error(\"ThreadGroup \" + threadGroup + \" contains running threads; \" + e.getMessage());\n-      threadGroup.list();\n-    } finally {\n-      threadGroup = null;\n+      LOG.error(\"ThreadGroup {} contains running threads; {}: See STDOUT\",\n+          this.threadGroup, e.getMessage());\n+      // This dumps list of threads on STDOUT.\n+      this.threadGroup.list();\n     }\n \n     // reset the in-memory state for testing",
                "deletions": 11
            },
            {
                "sha": "4c9d0e3c33d4d3ef91c9f32c31d2ce99be50e65f",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "blob_url": "https://github.com/apache/hbase/blob/4cb444e77b41cdb733544770a471068256d65bbe/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "raw_url": "https://github.com/apache/hbase/raw/4cb444e77b41cdb733544770a471068256d65bbe/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java?ref=4cb444e77b41cdb733544770a471068256d65bbe",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.hbase.client.Table;\n import org.apache.hadoop.hbase.client.TableDescriptor;\n import org.apache.hadoop.hbase.client.TableDescriptorBuilder;\n+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n import org.apache.hadoop.hbase.master.assignment.AssignmentManager;\n import org.apache.hadoop.hbase.test.MetricsAssertHelper;\n import org.apache.hadoop.hbase.testclassification.MasterTests;\n@@ -91,6 +92,8 @@ public static void startCluster() throws Exception {\n     // set a small interval for updating rit metrics\n     conf.setInt(AssignmentManager.RIT_CHORE_INTERVAL_MSEC_CONF_KEY, MSG_INTERVAL);\n \n+    // keep rs online so it can report the failed opens.\n+    conf.setBoolean(CoprocessorHost.ABORT_ON_ERROR_KEY, false);\n     TEST_UTIL.startMiniCluster(1);\n     CLUSTER = TEST_UTIL.getHBaseCluster();\n     MASTER = CLUSTER.getMaster();\n@@ -148,6 +151,9 @@ public void testRITAssignmentManagerMetrics() throws Exception {\n       }\n \n       // Sleep 3 seconds, wait for doMetrics chore catching up\n+      // the rit count consists of rit and failed opens. see RegionInTransitionStat#update\n+      // Waiting for the completion of rit makes the assert stable.\n+      TEST_UTIL.waitUntilNoRegionsInTransition();\n       Thread.sleep(MSG_INTERVAL * 3);\n       METRICS_HELPER.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_NAME, 1, amSource);\n       METRICS_HELPER.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_OVER_THRESHOLD_NAME, 1,",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20052 TestRegionOpen#testNonExistentRegionReplica fails due to NPE",
        "commit": "https://github.com/apache/hbase/commit/d68f697f39fc0e660eb85dc9ef67a757102169f2",
        "parent": "https://github.com/apache/hbase/commit/1fd2a276f6849107960acfab7a939aa2842fc9c5",
        "bug_id": "hbase_53",
        "file": [
            {
                "sha": "904060f8013db7d094debac5a6a0b913cf00e0b7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "blob_url": "https://github.com/apache/hbase/blob/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "raw_url": "https://github.com/apache/hbase/raw/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=d68f697f39fc0e660eb85dc9ef67a757102169f2",
                "patch": "@@ -986,12 +986,11 @@ public static HRegionFileSystem createRegionOnFileSystem(final Configuration con\n \n       // Write HRI to a file in case we need to recover hbase:meta\n       regionFs.writeRegionInfoOnFilesystem(false);\n-      return regionFs;\n     } else {\n       if (LOG.isDebugEnabled())\n         LOG.debug(\"Skipping creation of .regioninfo file for \" + regionInfo);\n     }\n-    return null;\n+    return regionFs;\n   }\n \n   /**",
                "deletions": 2
            },
            {
                "sha": "7190d840a7fac8d32f66fe868584a7e01c7d212f",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java",
                "blob_url": "https://github.com/apache/hbase/blob/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java",
                "raw_url": "https://github.com/apache/hbase/raw/d68f697f39fc0e660eb85dc9ef67a757102169f2/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java",
                "status": "modified",
                "changes": 2,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionOpen.java?ref=d68f697f39fc0e660eb85dc9ef67a757102169f2",
                "patch": "@@ -45,7 +45,6 @@\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.ClassRule;\n-import org.junit.Ignore;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n@@ -101,7 +100,6 @@ public void testPriorityRegionIsOpenedWithSeparateThreadPool() throws Exception\n     assertEquals(completed + 1, exec.getCompletedTaskCount());\n   }\n \n-  @Ignore // Needs rewrite since HBASE-19391 which returns null out of createRegionOnFileSystem\n   @Test\n   public void testNonExistentRegionReplica() throws Exception {\n     final TableName tableName = TableName.valueOf(name.getMethodName());",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-18309 (addendum) fix NPE in LogCleaner",
        "commit": "https://github.com/apache/hbase/commit/988ea870ed1760976d1596a56a75216b2d9d2762",
        "parent": "https://github.com/apache/hbase/commit/979767824d37df0e05002fa76402ff2b9e534d50",
        "bug_id": "hbase_54",
        "file": [
            {
                "sha": "44aafe20386d9a857b486996d3e7eb184f65b78e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/988ea870ed1760976d1596a56a75216b2d9d2762/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/988ea870ed1760976d1596a56a75216b2d9d2762/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java?ref=988ea870ed1760976d1596a56a75216b2d9d2762",
                "patch": "@@ -163,7 +163,9 @@ private void deleteFile() {\n         LOG.warn(\"Failed to clean oldwals with exception: \" + e);\n         succeed = false;\n       } finally {\n-        context.setResult(succeed);\n+        if (context != null) {\n+          context.setResult(succeed);\n+        }\n         if (interrupted) {\n           // Restore interrupt status\n           Thread.currentThread().interrupt();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19484 (addendum) NPE in ExtendedCell#write",
        "commit": "https://github.com/apache/hbase/commit/eebff56fe62c45feff9b6be02b0761535b900d63",
        "parent": "https://github.com/apache/hbase/commit/d28e126b2926d9277ea3496b68ae2ea5d45751e2",
        "bug_id": "hbase_55",
        "file": [
            {
                "sha": "36b07a81b7a945085b53bf5887bd81b4563bdf92",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCell.java",
                "blob_url": "https://github.com/apache/hbase/blob/eebff56fe62c45feff9b6be02b0761535b900d63/hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCell.java",
                "raw_url": "https://github.com/apache/hbase/raw/eebff56fe62c45feff9b6be02b0761535b900d63/hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCell.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/ExtendedCell.java?ref=eebff56fe62c45feff9b6be02b0761535b900d63",
                "patch": "@@ -53,8 +53,10 @@ default int write(OutputStream out, boolean withTags) throws IOException {\n     // Key\n     PrivateCellUtil.writeFlatKey(this, out);\n \n-    // Value\n-    out.write(getValueArray(), getValueOffset(), getValueLength());\n+    if (getValueLength() > 0) {\n+      // Value\n+      out.write(getValueArray(), getValueOffset(), getValueLength());\n+    }\n \n     // Tags length and tags byte array\n     if (withTags && getTagsLength() > 0) {",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15933 NullPointerException may be thrown from SimpleRegionNormalizer#getRegionSize()",
        "commit": "https://github.com/apache/hbase/commit/cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54",
        "parent": "https://github.com/apache/hbase/commit/a0f49c988419d48f6c655f46ac78f8199c643b50",
        "bug_id": "hbase_56",
        "file": [
            {
                "sha": "d209eb739468e0f0533c8ae37dc8381968bdea73",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java",
                "blob_url": "https://github.com/apache/hbase/blob/cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java",
                "raw_url": "https://github.com/apache/hbase/raw/cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.java?ref=cfe868d56eeb0367c2fcf4a18a1d06c57abb7e54",
                "patch": "@@ -140,7 +140,9 @@ public int compare(NormalizationPlan plan, NormalizationPlan plan2) {\n     for (int i = 0; i < tableRegions.size(); i++) {\n       HRegionInfo hri = tableRegions.get(i);\n       long regionSize = getRegionSize(hri);\n-      totalSizeMb += regionSize;\n+      if (regionSize > 0) {\n+        totalSizeMb += regionSize;\n+      }\n     }\n \n     double avgRegionSize = totalSizeMb / (double) tableRegions.size();\n@@ -204,6 +206,10 @@ private long getRegionSize(HRegionInfo hri) {\n       getRegionServerOfRegion(hri);\n     RegionLoad regionLoad = masterServices.getServerManager().getLoad(sn).\n       getRegionsLoad().get(hri.getRegionName());\n+    if (regionLoad == null) {\n+      LOG.debug(hri.getRegionNameAsString() + \" was not found in RegionsLoad\");\n+      return -1;\n+    }\n     return regionLoad.getStorefileSizeMB();\n   }\n }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20419 Fix potential NPE in ZKUtil#listChildrenAndWatchForNewChildren callers\n\nSigned-off-by: Yu Li <liyu@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/1339ff9666f449d8850236802c008a27307b7c61",
        "parent": "https://github.com/apache/hbase/commit/125767b44e93f1094b77a6cf8c2a5ca19b5cabd2",
        "bug_id": "hbase_57",
        "file": [
            {
                "sha": "659343c76bd8413e5b0579bdc2c6413428e1d0bf",
                "filename": "hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/1339ff9666f449d8850236802c008a27307b7c61/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/1339ff9666f449d8850236802c008a27307b7c61/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java?ref=1339ff9666f449d8850236802c008a27307b7c61",
                "patch": "@@ -354,7 +354,11 @@ public synchronized void removeServers(Set<Address> servers) throws IOException\n     //Overwrite any info stored by table, this takes precedence\n     try {\n       if(ZKUtil.checkExists(watcher, groupBasePath) != -1) {\n-        for(String znode: ZKUtil.listChildrenAndWatchForNewChildren(watcher, groupBasePath)) {\n+        List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(watcher, groupBasePath);\n+        if (children == null) {\n+          return RSGroupInfoList;\n+        }\n+        for(String znode: children) {\n           byte[] data = ZKUtil.getData(watcher, ZNodePaths.joinZNode(groupBasePath, znode));\n           if(data.length > 0) {\n             ProtobufUtil.expectPBMagicPrefix(data);",
                "deletions": 1
            },
            {
                "sha": "57d929da545934d2c28316af8755a99edc1aef51",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java",
                "blob_url": "https://github.com/apache/hbase/blob/1339ff9666f449d8850236802c008a27307b7c61/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java",
                "raw_url": "https://github.com/apache/hbase/raw/1339ff9666f449d8850236802c008a27307b7c61/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java?ref=1339ff9666f449d8850236802c008a27307b7c61",
                "patch": "@@ -135,8 +135,12 @@ private void watchForAbortedProcedures() {\n     LOG.debug(\"Checking for aborted procedures on node: '\" + zkController.getAbortZnode() + \"'\");\n     try {\n       // this is the list of the currently aborted procedues\n-      for (String node : ZKUtil.listChildrenAndWatchForNewChildren(zkController.getWatcher(),\n-        zkController.getAbortZnode())) {\n+      List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(zkController.getWatcher(),\n+                   zkController.getAbortZnode());\n+      if (children == null || children.isEmpty()) {\n+        return;\n+      }\n+      for (String node : children) {\n         String abortNode = ZNodePaths.joinZNode(zkController.getAbortZnode(), node);\n         abort(abortNode);\n       }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19061 update enforcer rules for NPE",
        "commit": "https://github.com/apache/hbase/commit/89d3b0b07f2ce7a84780e7088efaf9e3bce1ee5f",
        "parent": "https://github.com/apache/hbase/commit/64d164b86d32f6d6e987722bf223a809743f9f47",
        "bug_id": "hbase_58",
        "file": [
            {
                "sha": "8592d710b21d96c87741d402966f3172ecee0475",
                "filename": "hbase-shaded/hbase-shaded-check-invariants/pom.xml",
                "blob_url": "https://github.com/apache/hbase/blob/89d3b0b07f2ce7a84780e7088efaf9e3bce1ee5f/hbase-shaded/hbase-shaded-check-invariants/pom.xml",
                "raw_url": "https://github.com/apache/hbase/raw/89d3b0b07f2ce7a84780e7088efaf9e3bce1ee5f/hbase-shaded/hbase-shaded-check-invariants/pom.xml",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-shaded/hbase-shaded-check-invariants/pom.xml?ref=89d3b0b07f2ce7a84780e7088efaf9e3bce1ee5f",
                "patch": "@@ -76,7 +76,7 @@\n           <dependency>\n             <groupId>org.codehaus.mojo</groupId>\n             <artifactId>extra-enforcer-rules</artifactId>\n-            <version>1.0-beta-3</version>\n+            <version>1.0-beta-6</version>\n           </dependency>\n         </dependencies>\n         <executions>",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19445 PerformanceEvaluation NPE processing split policy option",
        "commit": "https://github.com/apache/hbase/commit/3e7b90ac6d808c171ea988d8d32ef998146713ac",
        "parent": "https://github.com/apache/hbase/commit/00750fe79acbb6a43daa62b9fcabbd1f5ce4cf6c",
        "bug_id": "hbase_59",
        "file": [
            {
                "sha": "8255573a2bc614f46904c4f3b4acf87abca08b82",
                "filename": "hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/3e7b90ac6d808c171ea988d8d32ef998146713ac/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/3e7b90ac6d808c171ea988d8d32ef998146713ac/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-mapreduce/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=3e7b90ac6d808c171ea988d8d32ef998146713ac",
                "patch": "@@ -406,7 +406,7 @@ protected static HTableDescriptor getTableDescriptor(TestOptions opts) {\n     if (opts.replicas != DEFAULT_OPTS.replicas) {\n       desc.setRegionReplication(opts.replicas);\n     }\n-    if (opts.splitPolicy != DEFAULT_OPTS.splitPolicy) {\n+    if (opts.splitPolicy != null && !opts.splitPolicy.equals(DEFAULT_OPTS.splitPolicy)) {\n       desc.setRegionSplitPolicyClassName(opts.splitPolicy);\n     }\n     return desc;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13520 NullPointerException in TagRewriteCell.(Josh Elser)",
        "commit": "https://github.com/apache/hbase/commit/2ba4c4eb9fe568b962f9d71de829531f51c5375b",
        "parent": "https://github.com/apache/hbase/commit/eb82b8b3098d6a9ac62aa50189f9d4b289f38472",
        "bug_id": "hbase_60",
        "file": [
            {
                "sha": "eb6d3bef82f8511270033e96a6c6f4c3b8f24d5d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/TagRewriteCell.java?ref=2ba4c4eb9fe568b962f9d71de829531f51c5375b",
                "patch": "@@ -41,6 +41,7 @@\n   public TagRewriteCell(Cell cell, byte[] tags) {\n     assert cell instanceof SettableSequenceId;\n     assert cell instanceof SettableTimestamp;\n+    assert tags != null;\n     this.cell = cell;\n     this.tags = tags;\n     // tag offset will be treated as 0 and length this.tags.length\n@@ -143,6 +144,10 @@ public int getTagsOffset() {\n \n   @Override\n   public int getTagsLength() {\n+    if (null == this.tags) {\n+      // Nulled out tags array optimization in constructor\n+      return 0;\n+    }\n     return this.tags.length;\n   }\n ",
                "deletions": 0
            },
            {
                "sha": "56cecf4ca4046f082d75aea6b11b5606a8f5cedf",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ba4c4eb9fe568b962f9d71de829531f51c5375b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java",
                "status": "added",
                "changes": 48,
                "additions": 48,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestTagRewriteCell.java?ref=2ba4c4eb9fe568b962f9d71de829531f51c5375b",
                "patch": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category(SmallTests.class)\n+public class TestTagRewriteCell {\n+\n+  @Test\n+  public void testHeapSize() {\n+    Cell originalCell = CellUtil.createCell(Bytes.toBytes(\"row\"), Bytes.toBytes(\"value\"));\n+    final int fakeTagArrayLength = 10;\n+    TagRewriteCell trCell = new TagRewriteCell(originalCell, new byte[fakeTagArrayLength]);\n+\n+    // Get the heapSize before the internal tags array in trCell are nuked\n+    long trCellHeapSize = trCell.heapSize();\n+\n+    // Make another TagRewriteCell with the original TagRewriteCell\n+    // This happens on systems with more than one RegionObserver/Coproc loaded (such as\n+    // VisibilityController and AccessController)\n+    TagRewriteCell trCell2 = new TagRewriteCell(trCell, new byte[fakeTagArrayLength]);\n+\n+    assertTrue(\"TagRewriteCell containing a TagRewriteCell's heapsize should be larger than a \" +\n+        \"single TagRewriteCell's heapsize\", trCellHeapSize < trCell2.heapSize());\n+    assertTrue(\"TagRewriteCell should have had nulled out tags array\", trCell.heapSize() <\n+        trCellHeapSize);\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19117 Avoid NPE occurring while active master dies",
        "commit": "https://github.com/apache/hbase/commit/22b07e91d73dfb846ce3fd1e6e270a794d161c1a",
        "parent": "https://github.com/apache/hbase/commit/ac6b998afe033cbb6a307d249c8e18bb97d54c9f",
        "bug_id": "hbase_61",
        "file": [
            {
                "sha": "c45b762b1cfcddad1efb2fdf6c1b6efbf762729a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/22b07e91d73dfb846ce3fd1e6e270a794d161c1a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/22b07e91d73dfb846ce3fd1e6e270a794d161c1a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 17,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=22b07e91d73dfb846ce3fd1e6e270a794d161c1a",
                "patch": "@@ -543,13 +543,16 @@ public void run() {\n     try {\n       super.run();\n     } finally {\n-      // If on way out, then we are no longer active master.\n-      this.clusterSchemaService.stopAsync();\n-      try {\n-        this.clusterSchemaService.awaitTerminated(getConfiguration().getInt(HBASE_MASTER_WAIT_ON_SERVICE_IN_SECONDS,\n-          DEFAULT_HBASE_MASTER_WAIT_ON_SERVICE_IN_SECONDS), TimeUnit.SECONDS);\n-      } catch (TimeoutException te) {\n-        LOG.warn(\"Failed shutdown of clusterSchemaService\", te);\n+      if (this.clusterSchemaService != null) {\n+        // If on way out, then we are no longer active master.\n+        this.clusterSchemaService.stopAsync();\n+        try {\n+          this.clusterSchemaService.awaitTerminated(\n+              getConfiguration().getInt(HBASE_MASTER_WAIT_ON_SERVICE_IN_SECONDS,\n+              DEFAULT_HBASE_MASTER_WAIT_ON_SERVICE_IN_SECONDS), TimeUnit.SECONDS);\n+        } catch (TimeoutException te) {\n+          LOG.warn(\"Failed shutdown of clusterSchemaService\", te);\n+        }\n       }\n       this.activeMaster = false;\n     }",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14885 NullPointerException in HMaster#normalizeRegions() due to missing TableDescriptor",
        "commit": "https://github.com/apache/hbase/commit/e73a9594c218ed969a2f5b0b356d7b8d0e1474c0",
        "parent": "https://github.com/apache/hbase/commit/efb5917f2247b4cc1951ba950a9b9ff1a4606c92",
        "bug_id": "hbase_62",
        "file": [
            {
                "sha": "46c7f79853a02b56140120733d1d4ba38297a126",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/e73a9594c218ed969a2f5b0b356d7b8d0e1474c0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/e73a9594c218ed969a2f5b0b356d7b8d0e1474c0/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 7,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=e73a9594c218ed969a2f5b0b356d7b8d0e1474c0",
                "patch": "@@ -69,6 +69,7 @@\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerLoad;\n import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.TableDescriptor;\n import org.apache.hadoop.hbase.TableDescriptors;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -1357,8 +1358,10 @@ public boolean normalizeRegions() throws IOException {\n           LOG.debug(\"Skipping normalizing \" + table + \" since its namespace has quota\");\n           continue;\n         }\n-        if (table.isSystemTable() || !getTableDescriptors().getDescriptor(table).\n-            getHTableDescriptor().isNormalizationEnabled()) {\n+        TableDescriptor tblDesc = getTableDescriptors().getDescriptor(table);\n+        if (table.isSystemTable() || (tblDesc != null &&\n+            tblDesc.getHTableDescriptor() != null &&\n+            !tblDesc.getHTableDescriptor().isNormalizationEnabled())) {\n           LOG.debug(\"Skipping normalization for table: \" + table + \", as it's either system\"\n             + \" table or doesn't have auto normalization turned on\");\n           continue;",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19094 NPE in RSGroupStartupWorker.waitForGroupTableOnline during master startup",
        "commit": "https://github.com/apache/hbase/commit/4dee4a854fece6516ba5006251898082c8fc161a",
        "parent": "https://github.com/apache/hbase/commit/15b32460f9484fc0c635172c81b6a6315283775a",
        "bug_id": "hbase_63",
        "file": [
            {
                "sha": "9520f5fbf8a74a9a6999662eb87ac0543609dde7",
                "filename": "hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/4dee4a854fece6516ba5006251898082c8fc161a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/4dee4a854fece6516ba5006251898082c8fc161a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.java?ref=4dee4a854fece6516ba5006251898082c8fc161a",
                "patch": "@@ -691,7 +691,9 @@ public boolean visitInternal(Result row) throws IOException {\n                     if(cell != null) {\n                       sn = ServerName.parseVersionedServerName(CellUtil.cloneValue(cell));\n                     }\n-                    if (tsm.isTableState(TableName.NAMESPACE_TABLE_NAME,\n+                    if (sn == null) {\n+                      nsFound.set(false);\n+                    } else if (tsm.isTableState(TableName.NAMESPACE_TABLE_NAME,\n                         TableState.State.ENABLED)) {\n                       try {\n                         ClientProtos.ClientService.BlockingInterface rs = conn.getClient(sn);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19424 Fix NPE in \"/metrics\" servlet.\n\nSigned-off-by: Apekshit Sharma <appy@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0",
        "parent": "https://github.com/apache/hbase/commit/2509a150c0792e914429264453510b9028250c29",
        "bug_id": "hbase_64",
        "file": [
            {
                "sha": "af72ab8c1e65f91310ddd479239dfdbeace4e9bf",
                "filename": "hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-http/src/main/java/org/apache/hadoop/hbase/http/HttpServer.java?ref=f29260cc4d27e86ea0948e9b9ddfed18a73ff7e0",
                "patch": "@@ -591,6 +591,9 @@ private static WebAppContext createWebAppContext(String name,\n     ctx.setContextPath(\"/\");\n     ctx.setWar(appDir + \"/\" + name);\n     ctx.getServletContext().setAttribute(CONF_CONTEXT_ATTRIBUTE, conf);\n+    // for org.apache.hadoop.metrics.MetricsServlet\n+    ctx.getServletContext().setAttribute(\n+      org.apache.hadoop.http.HttpServer2.CONF_CONTEXT_ATTRIBUTE, conf);\n     ctx.getServletContext().setAttribute(ADMINS_ACL, adminsAcl);\n     addNoCacheFilter(ctx);\n     return ctx;",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-11794 StripeStoreFlusher causes NullPointerException (jeongmin kim)",
        "commit": "https://github.com/apache/hbase/commit/368aee62af74fcae5a6a00253c0f6ea74a11b194",
        "parent": "https://github.com/apache/hbase/commit/f69a1945c641c3eaba6a519e7a65eb54ce71f628",
        "bug_id": "hbase_65",
        "file": [
            {
                "sha": "ae347997b7ddb7f22aeb0f0df5dc6c17a423ad17",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java",
                "blob_url": "https://github.com/apache/hbase/blob/368aee62af74fcae5a6a00253c0f6ea74a11b194/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java",
                "raw_url": "https://github.com/apache/hbase/raw/368aee62af74fcae5a6a00253c0f6ea74a11b194/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java?ref=368aee62af74fcae5a6a00253c0f6ea74a11b194",
                "patch": "@@ -21,6 +21,7 @@\n import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.List;\n \n import org.apache.commons.logging.Log;\n@@ -54,7 +55,7 @@ public StripeStoreFlusher(Configuration conf, Store store,\n   @Override\n   public List<Path> flushSnapshot(MemStoreSnapshot snapshot, long cacheFlushSeqNum,\n       MonitoredTask status) throws IOException {\n-    List<Path> result = null;\n+    List<Path> result = new ArrayList<Path>();\n     int cellsCount = snapshot.getCellsCount();\n     if (cellsCount == 0) return result; // don't flush if there are no entries\n \n@@ -83,9 +84,6 @@ public StripeStoreFlusher(Configuration conf, Store store,\n       }\n     } finally {\n       if (!success && (mw != null)) {\n-        if (result != null) {\n-          result.clear();\n-        }\n         for (Path leftoverFile : mw.abortWriters()) {\n           try {\n             store.getFileSystem().delete(leftoverFile, false);",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-18330 NPE in ReplicationZKLockCleanerChore",
        "commit": "https://github.com/apache/hbase/commit/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
        "parent": "https://github.com/apache/hbase/commit/5f54e28510fdbdc1a08688168f8df19904bcd975",
        "bug_id": "hbase_66",
        "file": [
            {
                "sha": "751e45441f8006bb8dd1f65ef81bf976147b429c",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeersZKImpl.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "patch": "@@ -520,6 +520,9 @@ private void checkQueuesDeleted(String peerId) throws ReplicationException {\n     if (queuesClient == null) return;\n     try {\n       List<String> replicators = queuesClient.getListOfReplicators();\n+      if (replicators == null || replicators.isEmpty()) {\n+        return;\n+      }\n       for (String replicator : replicators) {\n         List<String> queueIds = queuesClient.getAllQueues(replicator);\n         for (String queueId : queueIds) {",
                "deletions": 0
            },
            {
                "sha": "0115b6f812796382dad1af9fb7f299a150dfd95f",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueuesClientZKImpl.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "patch": "@@ -98,7 +98,7 @@ public void init() throws ReplicationException {\n     for (int retry = 0; ; retry++) {\n       int v0 = getQueuesZNodeCversion();\n       List<String> rss = getListOfReplicators();\n-      if (rss == null) {\n+      if (rss == null || rss.isEmpty()) {\n         LOG.debug(\"Didn't find any region server that replicates, won't prevent any deletions.\");\n         return ImmutableSet.of();\n       }",
                "deletions": 1
            },
            {
                "sha": "6d8962e9234b6fd1866319240cac3ba0caedd90d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationZKNodeCleaner.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "patch": "@@ -77,6 +77,9 @@ public ReplicationZKNodeCleaner(Configuration conf, ZooKeeperWatcher zkw, Aborta\n     Set<String> peerIds = new HashSet<>(this.replicationPeers.getAllPeerIds());\n     try {\n       List<String> replicators = this.queuesClient.getListOfReplicators();\n+      if (replicators == null || replicators.isEmpty()) {\n+        return undeletedQueues;\n+      }\n       for (String replicator : replicators) {\n         List<String> queueIds = this.queuesClient.getAllQueues(replicator);\n         for (String queueId : queueIds) {",
                "deletions": 0
            },
            {
                "sha": "4bda75be366d7f77854a9262521f9ab20dafbea1",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "blob_url": "https://github.com/apache/hbase/blob/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "raw_url": "https://github.com/apache/hbase/raw/01db60d65b9a2dff0ca001323cb77a6e4e8d6f48/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java?ref=01db60d65b9a2dff0ca001323cb77a6e4e8d6f48",
                "patch": "@@ -323,6 +323,9 @@ public String dumpQueues(ClusterConnection connection, ZooKeeperWatcher zkw, Set\n     // Loops each peer on each RS and dumps the queues\n     try {\n       List<String> regionservers = queuesClient.getListOfReplicators();\n+      if (regionservers == null || regionservers.isEmpty()) {\n+        return sb.toString();\n+      }\n       for (String regionserver : regionservers) {\n         List<String> queueIds = queuesClient.getAllQueues(regionserver);\n         replicationQueues.init(regionserver);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-18788 NPE when running TestSerialReplication (Fabrice Monnier)",
        "commit": "https://github.com/apache/hbase/commit/a4afa38f7a56aa6c36ee1af21fac8ce6d056a3db",
        "parent": "https://github.com/apache/hbase/commit/bb28a3b4660a64bb513ffe4399e72679fc2dcec5",
        "bug_id": "hbase_67",
        "file": [
            {
                "sha": "23e5a666b2bd9f2b731f5f2c22e2cb6e3f980d60",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/a4afa38f7a56aa6c36ee1af21fac8ce6d056a3db/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/a4afa38f7a56aa6c36ee1af21fac8ce6d056a3db/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/ReplicationMetaCleaner.java?ref=a4afa38f7a56aa6c36ee1af21fac8ce6d056a3db",
                "patch": "@@ -83,8 +83,12 @@ protected void chore() {\n \n       List<ReplicationPeerDescription> peers = admin.listReplicationPeers();\n       for (ReplicationPeerDescription peerDesc : peers) {\n-        for (Map.Entry<TableName, List<String>> map : peerDesc.getPeerConfig().getTableCFsMap()\n-            .entrySet()) {\n+        Map<TableName, List<String>> tableCFsMap = peerDesc.getPeerConfig().getTableCFsMap();\n+        if (tableCFsMap ==null) {\n+          continue;\n+        }\n+\n+        for (Map.Entry<TableName, List<String>> map : tableCFsMap.entrySet()) {\n           if (serialTables.containsKey(map.getKey().getNameAsString())) {\n             serialTables.get(map.getKey().getNameAsString()).add(peerDesc.getPeerId());\n             break;",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19593 Possible NPE if wal is closed during waledit append.(Rajeshabbu)",
        "commit": "https://github.com/apache/hbase/commit/c0598dcb10f2185164548ad8a1fdf1867d6a0b34",
        "parent": "https://github.com/apache/hbase/commit/448ba3a78f50df2ffac874c3768e9f50d52b15f6",
        "bug_id": "hbase_68",
        "file": [
            {
                "sha": "6401a8b957ef6473da0c93384c534e19c876126e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/c0598dcb10f2185164548ad8a1fdf1867d6a0b34/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/c0598dcb10f2185164548ad8a1fdf1867d6a0b34/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=c0598dcb10f2185164548ad8a1fdf1867d6a0b34",
                "patch": "@@ -7585,7 +7585,7 @@ private WriteEntry doWALAppend(WALEdit walEdit, Durability durability, List<UUID\n       }\n       writeEntry = walKey.getWriteEntry();\n     } catch (IOException ioe) {\n-      if (walKey != null) {\n+      if (walKey != null && walKey.getWriteEntry() != null) {\n         mvcc.complete(walKey.getWriteEntry());\n       }\n       throw ioe;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19870 Fix the NPE in ReadOnlyZKClient#run\n\nSigned-off-by: Chia-Ping Tsai <chia7712@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/221eb9576839ecc976120197a4845a9a1e371d9c",
        "parent": "https://github.com/apache/hbase/commit/f9480a56c74e7f9f42035f6e88c24fed08710e3c",
        "bug_id": "hbase_69",
        "file": [
            {
                "sha": "275fafbebad6e11840a0a7d8b75b8512c2e46166",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java",
                "status": "modified",
                "changes": 14,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ReadOnlyZKClient.java?ref=221eb9576839ecc976120197a4845a9a1e371d9c",
                "patch": "@@ -311,12 +311,14 @@ private void run() {\n       if (task == CLOSE) {\n         break;\n       }\n-      if (task == null && pendingRequests == 0) {\n-        LOG.debug(\n-          \"{} to {} no activities for {} ms, close active connection. \" +\n-            \"Will reconnect next time when there are new requests\",\n-          getId(), connectString, keepAliveTimeMs);\n-        closeZk();\n+      if (task == null) {\n+        if (pendingRequests == 0) {\n+          LOG.debug(\n+            \"{} to {} no activities for {} ms, close active connection. \" +\n+              \"Will reconnect next time when there are new requests\",\n+            getId(), connectString, keepAliveTimeMs);\n+          closeZk();\n+        }\n         continue;\n       }\n       if (!task.needZk()) {",
                "deletions": 6
            },
            {
                "sha": "a97a7c617464913b019755eccb416e1b1c49a620",
                "filename": "hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/221eb9576839ecc976120197a4845a9a1e371d9c/hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java",
                "status": "modified",
                "changes": 42,
                "additions": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-zookeeper/src/test/java/org/apache/hadoop/hbase/zookeeper/TestReadOnlyZKClient.java?ref=221eb9576839ecc976120197a4845a9a1e371d9c",
                "patch": "@@ -31,11 +31,15 @@\n import static org.mockito.ArgumentMatchers.anyBoolean;\n import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.doAnswer;\n-import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Exchanger;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ThreadLocalRandom;\n import org.apache.hadoop.conf.Configuration;\n@@ -45,20 +49,17 @@\n import org.apache.hadoop.hbase.Waiter.ExplainingPredicate;\n import org.apache.hadoop.hbase.testclassification.MediumTests;\n import org.apache.hadoop.hbase.testclassification.ZKTests;\n-import org.apache.zookeeper.AsyncCallback.StatCallback;\n+import org.apache.zookeeper.AsyncCallback;\n import org.apache.zookeeper.CreateMode;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.KeeperException.Code;\n import org.apache.zookeeper.ZooDefs;\n import org.apache.zookeeper.ZooKeeper;\n-import org.apache.zookeeper.data.Stat;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n-import org.mockito.invocation.InvocationOnMock;\n-import org.mockito.stubbing.Answer;\n \n @Category({ ZKTests.class, MediumTests.class })\n public class TestReadOnlyZKClient {\n@@ -165,25 +166,26 @@ public void testSessionExpire() throws Exception {\n \n   @Test\n   public void testNotCloseZkWhenPending() throws Exception {\n-    assertArrayEquals(DATA, RO_ZK.get(PATH).get());\n-    ZooKeeper mockedZK = spy(RO_ZK.zookeeper);\n-    CountDownLatch latch = new CountDownLatch(1);\n-    doAnswer(new Answer<Object>() {\n-\n-      @Override\n-      public Object answer(InvocationOnMock invocation) throws Throwable {\n-        latch.await();\n-        return invocation.callRealMethod();\n-      }\n-    }).when(mockedZK).exists(anyString(), anyBoolean(), any(StatCallback.class), any());\n+    ZooKeeper mockedZK = mock(ZooKeeper.class);\n+    Exchanger<AsyncCallback.DataCallback> exchanger = new Exchanger<>();\n+    doAnswer(i -> {\n+      exchanger.exchange(i.getArgument(2));\n+      return null;\n+    }).when(mockedZK).getData(anyString(), anyBoolean(),\n+      any(AsyncCallback.DataCallback.class), any());\n+    doAnswer(i -> null).when(mockedZK).close();\n+    when(mockedZK.getState()).thenReturn(ZooKeeper.States.CONNECTED);\n     RO_ZK.zookeeper = mockedZK;\n-    CompletableFuture<Stat> future = RO_ZK.exists(PATH);\n+    CompletableFuture<byte[]> future = RO_ZK.get(PATH);\n+    AsyncCallback.DataCallback callback = exchanger.exchange(null);\n     // 2 * keep alive time to ensure that we will not close the zk when there are pending requests\n     Thread.sleep(6000);\n     assertNotNull(RO_ZK.zookeeper);\n-    latch.countDown();\n-    assertEquals(CHILDREN, future.get().getNumChildren());\n+    verify(mockedZK, never()).close();\n+    callback.processResult(Code.OK.intValue(), PATH, null, DATA, null);\n+    assertArrayEquals(DATA, future.get());\n     // now we will close the idle connection.\n     waitForIdleConnectionClosed();\n+    verify(mockedZK, times(1)).close();\n   }\n }",
                "deletions": 20
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19756 Master NPE during completed failed proc eviction\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/ce50830a0af29e0ad2be24528629965923ef1cbf",
        "parent": "https://github.com/apache/hbase/commit/98cae45d2ba0854592956e8d7ed5976d3eaa5da6",
        "bug_id": "hbase_70",
        "file": [
            {
                "sha": "4a612998b307f2a691b03d56c1642b0780c62a64",
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "blob_url": "https://github.com/apache/hbase/blob/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "raw_url": "https://github.com/apache/hbase/raw/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java",
                "status": "modified",
                "changes": 20,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java?ref=ce50830a0af29e0ad2be24528629965923ef1cbf",
                "patch": "@@ -217,20 +217,22 @@ protected void periodicExecute(final TEnvironment env) {\n \n         // TODO: Select TTL based on Procedure type\n         if (retainer.isExpired(now, evictTtl, evictAckTtl)) {\n-          if (debugEnabled) {\n-            LOG.debug(\"Evict completed \" + proc);\n-          }\n-          batchIds[batchCount++] = entry.getKey();\n-          if (batchCount == batchIds.length) {\n-            store.delete(batchIds, 0, batchCount);\n-            batchCount = 0;\n+          // Failed procedures aren't persisted in WAL.\n+          if (!(proc instanceof FailedProcedure)) {\n+            batchIds[batchCount++] = entry.getKey();\n+            if (batchCount == batchIds.length) {\n+              store.delete(batchIds, 0, batchCount);\n+              batchCount = 0;\n+            }\n           }\n-          it.remove();\n-\n           final NonceKey nonceKey = proc.getNonceKey();\n           if (nonceKey != null) {\n             nonceKeysToProcIdsMap.remove(nonceKey);\n           }\n+          it.remove();\n+          if (debugEnabled) {\n+            LOG.debug(\"Evict completed \" + proc);\n+          }\n         }\n       }\n       if (batchCount > 0) {",
                "deletions": 9
            },
            {
                "sha": "740caea09d368d67849b7d279bd2b257e0395c87",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java",
                "blob_url": "https://github.com/apache/hbase/blob/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java",
                "raw_url": "https://github.com/apache/hbase/raw/ce50830a0af29e0ad2be24528629965923ef1cbf/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java",
                "status": "added",
                "changes": 110,
                "additions": 110,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java?ref=ce50830a0af29e0ad2be24528629965923ef1cbf",
                "patch": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hadoop.hbase.procedure;\n+\n+import static org.junit.Assert.fail;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.RegionInfo;\n+import org.apache.hadoop.hbase.client.TableDescriptor;\n+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessor;\n+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;\n+import org.apache.hadoop.hbase.coprocessor.MasterObserver;\n+import org.apache.hadoop.hbase.coprocessor.ObserverContext;\n+import org.apache.hadoop.hbase.procedure2.Procedure;\n+import org.apache.hadoop.hbase.security.AccessDeniedException;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+/**\n+ * Check if CompletedProcedureCleaner cleans up failed nonce procedures.\n+ */\n+@Category(MediumTests.class)\n+public class TestFailedProcCleanup {\n+  private static final Log LOG = LogFactory.getLog(TestFailedProcCleanup.class);\n+\n+  protected static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final TableName TABLE = TableName.valueOf(\"test\");\n+  private static final byte[] FAMILY = Bytes.toBytesBinary(\"f\");\n+  private static final int evictionDelay = 10 * 1000;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    Configuration conf = TEST_UTIL.getConfiguration();\n+    conf.setInt(\"hbase.procedure.cleaner.evict.ttl\", evictionDelay);\n+    conf.setInt(\"hbase.procedure.cleaner.evict.batch.size\", 1);\n+    conf.set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY, CreateFailObserver.class.getName());\n+    TEST_UTIL.startMiniCluster(3);\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    TEST_UTIL.cleanupTestDir();\n+    TEST_UTIL.cleanupDataTestDirOnTestFS();\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n+  @Test\n+  public void testFailCreateTable() throws Exception {\n+    try {\n+      TEST_UTIL.createTable(TABLE, FAMILY);\n+      fail(\"Table shouldn't be created\");\n+    } catch (AccessDeniedException e) {\n+      LOG.debug(\"Ignoring exception: \", e);\n+      Thread.sleep(evictionDelay * 3);\n+    }\n+    List<Procedure<?>> procedureInfos =\n+        TEST_UTIL.getMiniHBaseCluster().getMaster().getMasterProcedureExecutor().getProcedures();\n+    for (Procedure procedureInfo : procedureInfos) {\n+      if (procedureInfo.getProcName().equals(\"CreateTableProcedure\")\n+          && procedureInfo.getState() == ProcedureProtos.ProcedureState.ROLLEDBACK) {\n+        fail(\"Found procedure \" + procedureInfo + \" that hasn't been cleaned up\");\n+      }\n+    }\n+  }\n+\n+  public static class CreateFailObserver implements MasterCoprocessor, MasterObserver {\n+\n+    @Override\n+    public void preCreateTable(ObserverContext<MasterCoprocessorEnvironment> env,\n+        TableDescriptor desc, RegionInfo[] regions) throws IOException {\n+\n+      if (desc.getTableName().equals(TABLE)) {\n+        throw new AccessDeniedException(\"Don't allow creation of table\");\n+      }\n+    }\n+\n+    @Override\n+    public Optional<MasterObserver> getMasterObserver() {\n+      return Optional.of(this);\n+    }\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "guard against NPE while reading FileTrailer and HFileBlock\n\nguard against NPE from FSInputStream#seek\n\nSigned-off-by: Michael Stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/201c8382508da1266d11e04d3c7cbef42e0a256a",
        "parent": "https://github.com/apache/hbase/commit/35d7a0cd0798cabe7df5766fcc993512eca6c92e",
        "bug_id": "hbase_71",
        "file": [
            {
                "sha": "185423603628a26c1bd271dfaeddb4b81621ff86",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "blob_url": "https://github.com/apache/hbase/blob/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "raw_url": "https://github.com/apache/hbase/raw/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=201c8382508da1266d11e04d3c7cbef42e0a256a",
                "patch": "@@ -388,7 +388,8 @@ public static FixedFileTrailer readFromStream(FSDataInputStream istream,\n       bufferSize = (int) fileSize;\n     }\n \n-    istream.seek(seekPoint);\n+    HFileUtil.seekOnMultipleSources(istream, seekPoint);\n+\n     ByteBuffer buf = ByteBuffer.allocate(bufferSize);\n     istream.readFully(buf.array(), buf.arrayOffset(),\n         buf.arrayOffset() + buf.limit());",
                "deletions": 1
            },
            {
                "sha": "0b140b6e301eff851c9c78f30252f64fc4eced2e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "blob_url": "https://github.com/apache/hbase/blob/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "raw_url": "https://github.com/apache/hbase/raw/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java?ref=201c8382508da1266d11e04d3c7cbef42e0a256a",
                "patch": "@@ -1512,7 +1512,7 @@ protected int readAtOffset(FSDataInputStream istream, byte [] dest, int destOffs\n       if (!pread && streamLock.tryLock()) {\n         // Seek + read. Better for scanning.\n         try {\n-          istream.seek(fileOffset);\n+          HFileUtil.seekOnMultipleSources(istream, fileOffset);\n \n           long realOffset = istream.getPos();\n           if (realOffset != fileOffset) {",
                "deletions": 1
            },
            {
                "sha": "835450c2a9a8c8227ae4afe469bd0ec9d272dd49",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/201c8382508da1266d11e04d3c7cbef42e0a256a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "status": "added",
                "changes": 43,
                "additions": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java?ref=201c8382508da1266d11e04d3c7cbef42e0a256a",
                "patch": "@@ -0,0 +1,43 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.io.hfile;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+public class HFileUtil {\n+\n+  /** guards against NullPointer\n+   * utility which tries to seek on the DFSIS and will try an alternative source\n+   * if the FSDataInputStream throws an NPE HBASE-17501\n+   * @param istream\n+   * @param offset\n+   * @throws IOException\n+   */\n+  static public void seekOnMultipleSources(FSDataInputStream istream, long offset) throws IOException {\n+    try {\n+      // attempt to seek inside of current blockReader\n+      istream.seek(offset);\n+    } catch (NullPointerException e) {\n+      // retry the seek on an alternate copy of the data\n+      // this can occur if the blockReader on the DFSInputStream is null\n+      istream.seekToNewSource(offset);\n+    }\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-17803 Addendum fix NPE",
        "commit": "https://github.com/apache/hbase/commit/261aa9445c3c52e09c10d06168a77d11d0c9b4b4",
        "parent": "https://github.com/apache/hbase/commit/23abc90068f0ea75f09c3eecf6ef758f1aee9219",
        "bug_id": "hbase_72",
        "file": [
            {
                "sha": "40e50cfc60d91c7a21c660a0dcdd2af5ebdb254b",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/261aa9445c3c52e09c10d06168a77d11d0c9b4b4/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/261aa9445c3c52e09c10d06168a77d11d0c9b4b4/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=261aa9445c3c52e09c10d06168a77d11d0c9b4b4",
                "patch": "@@ -43,6 +43,7 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n+import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n@@ -325,7 +326,8 @@ static boolean checkTable(Admin admin, TestOptions opts) throws IOException {\n     // recreate the table when user has requested presplit or when existing\n     // {RegionSplitPolicy,replica count} does not match requested.\n     if ((exists && opts.presplitRegions != DEFAULT_OPTS.presplitRegions)\n-      || (!isReadCmd && desc != null && !desc.getRegionSplitPolicyClassName().equals(opts.splitPolicy))\n+      || (!isReadCmd && desc != null &&\n+          !StringUtils.equals(desc.getRegionSplitPolicyClassName(), opts.splitPolicy))\n       || (!isReadCmd && desc != null && desc.getRegionReplication() != opts.replicas)) {\n       needsDelete = true;\n       // wait, why did it delete my table?!?",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14525 Append and increment operation throws NullPointerException on non-existing column families.(Abhishek)",
        "commit": "https://github.com/apache/hbase/commit/79607bd9f821618e25baea53a625aec1d7985bd8",
        "parent": "https://github.com/apache/hbase/commit/a77f830198815db3927fc02646d2167c370a028f",
        "bug_id": "hbase_73",
        "file": [
            {
                "sha": "f8cbee216a33c2eef1d0debe4c252bdf17ecc838",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=79607bd9f821618e25baea53a625aec1d7985bd8",
                "patch": "@@ -6982,6 +6982,7 @@ public Result append(Append mutate, long nonceGroup, long nonce) throws IOExcept\n     Operation op = Operation.APPEND;\n     byte[] row = mutate.getRow();\n     checkRow(row, op.toString());\n+    checkFamilies(mutate.getFamilyCellMap().keySet());\n     boolean flush = false;\n     Durability durability = getEffectiveDurability(mutate.getDurability());\n     boolean writeToWAL = durability != Durability.SKIP_WAL;\n@@ -7224,6 +7225,7 @@ public Result increment(Increment mutation, long nonceGroup, long nonce)\n     Operation op = Operation.INCREMENT;\n     byte [] row = mutation.getRow();\n     checkRow(row, op.toString());\n+    checkFamilies(mutation.getFamilyCellMap().keySet());\n     boolean flush = false;\n     Durability durability = getEffectiveDurability(mutation.getDurability());\n     boolean writeToWAL = durability != Durability.SKIP_WAL;",
                "deletions": 0
            },
            {
                "sha": "49f36d6b1abc96a5be20df10c66ed6363153d6f8",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java",
                "blob_url": "https://github.com/apache/hbase/blob/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java",
                "raw_url": "https://github.com/apache/hbase/raw/79607bd9f821618e25baea53a625aec1d7985bd8/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java",
                "status": "modified",
                "changes": 37,
                "additions": 37,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java?ref=79607bd9f821618e25baea53a625aec1d7985bd8",
                "patch": "@@ -134,6 +134,43 @@ public void testAppend() throws IOException {\n     assertEquals(0, Bytes.compareTo(Bytes.toBytes(v2+v1), result.getValue(fam1, qual2)));\n   }\n \n+  @Test\n+  public void testAppendWithNonExistingFamily() throws IOException {\n+    initHRegion(tableName, name.getMethodName(), fam1);\n+    final String v1 = \"Value\";\n+    final Append a = new Append(row);\n+    a.add(fam1, qual1, Bytes.toBytes(v1));\n+    a.add(fam2, qual2, Bytes.toBytes(v1));\n+    Result result = null;\n+    try {\n+      result = region.append(a, HConstants.NO_NONCE, HConstants.NO_NONCE);\n+      fail(\"Append operation should fail with NoSuchColumnFamilyException.\");\n+    } catch (NoSuchColumnFamilyException e) {\n+      assertEquals(null, result);\n+    } catch (Exception e) {\n+      fail(\"Append operation should fail with NoSuchColumnFamilyException.\");\n+    }\n+  }\n+\n+  @Test\n+  public void testIncrementWithNonExistingFamily() throws IOException {\n+    initHRegion(tableName, name.getMethodName(), fam1);\n+    final Increment inc = new Increment(row);\n+    inc.addColumn(fam1, qual1, 1);\n+    inc.addColumn(fam2, qual2, 1);\n+    inc.setDurability(Durability.ASYNC_WAL);\n+    try {\n+      region.increment(inc, HConstants.NO_NONCE, HConstants.NO_NONCE);\n+    } catch (NoSuchColumnFamilyException e) {\n+      final Get g = new Get(row);\n+      final Result result = region.get(g);\n+      assertEquals(null, result.getValue(fam1, qual1));\n+      assertEquals(null, result.getValue(fam2, qual2));\n+    } catch (Exception e) {\n+      fail(\"Increment operation should fail with NoSuchColumnFamilyException.\");\n+    }\n+  }\n+\n   /**\n    * Test multi-threaded increments.\n    */",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13653 Uninitialized HRegionServer#walFactory may result in NullPointerException at region server startup\u200b",
        "commit": "https://github.com/apache/hbase/commit/7bdacf53f41d7e3c6c6698c5d20fdcebaa10a1bb",
        "parent": "https://github.com/apache/hbase/commit/5a2ca43fa16a95d8db67e5a3d8b48e4d3f3a9aeb",
        "bug_id": "hbase_74",
        "file": [
            {
                "sha": "00daacfa00e37f36bcc266b16dff6df8d2555a21",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/7bdacf53f41d7e3c6c6698c5d20fdcebaa10a1bb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/7bdacf53f41d7e3c6c6698c5d20fdcebaa10a1bb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=7bdacf53f41d7e3c6c6698c5d20fdcebaa10a1bb",
                "patch": "@@ -1508,6 +1508,7 @@ public WarmupRegionResponse warmupRegion(final RpcController controller,\n     WarmupRegionResponse response = WarmupRegionResponse.getDefaultInstance();\n \n     try {\n+      checkOpen();\n       String encodedName = region.getEncodedName();\n       byte[] encodedNameBytes = region.getEncodedNameAsBytes();\n       final Region onlineRegion = regionServer.getFromOnlineRegions(encodedName);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12337 Import tool fails with NullPointerException if clusterIds is not initialized",
        "commit": "https://github.com/apache/hbase/commit/0eb07609b66bec92d2f5479a1939f64ca3c8bdc0",
        "parent": "https://github.com/apache/hbase/commit/3f6b7346357219211f270fd9e13f91cd674c4a08",
        "bug_id": "hbase_75",
        "file": [
            {
                "sha": "15540c6671cabdabac2ed83211acc1c215e46d25",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java",
                "blob_url": "https://github.com/apache/hbase/blob/0eb07609b66bec92d2f5479a1939f64ca3c8bdc0/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java",
                "raw_url": "https://github.com/apache/hbase/raw/0eb07609b66bec92d2f5479a1939f64ca3c8bdc0/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java",
                "status": "modified",
                "changes": 8,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java?ref=0eb07609b66bec92d2f5479a1939f64ca3c8bdc0",
                "patch": "@@ -231,18 +231,26 @@ public void setup(Context context) {\n       }\n       // TODO: This is kind of ugly doing setup of ZKW just to read the clusterid.\n       ZooKeeperWatcher zkw = null;\n+      Exception ex = null;\n       try {\n         zkw = new ZooKeeperWatcher(conf, context.getTaskAttemptID().toString(), null);\n         clusterIds = Collections.singletonList(ZKClusterId.getUUIDForCluster(zkw));\n       } catch (ZooKeeperConnectionException e) {\n+        ex = e;\n         LOG.error(\"Problem connecting to ZooKeper during task setup\", e);\n       } catch (KeeperException e) {\n+        ex = e;\n         LOG.error(\"Problem reading ZooKeeper data during task setup\", e);\n       } catch (IOException e) {\n+        ex = e;\n         LOG.error(\"Problem setting up task\", e);\n       } finally {\n         if (zkw != null) zkw.close();\n       }\n+      if (clusterIds == null) {\n+        // exit early if setup fails\n+        throw new RuntimeException(ex);\n+      }\n     }\n   }\n ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16732 Avoid possible NPE in MetaTableLocator",
        "commit": "https://github.com/apache/hbase/commit/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c",
        "parent": "https://github.com/apache/hbase/commit/bf3c928b7499797735f71974992b68c9d876b97c",
        "bug_id": "hbase_76",
        "file": [
            {
                "sha": "7b64e0cb4a31a438c5c70db299938498a9359ca3",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java",
                "blob_url": "https://github.com/apache/hbase/blob/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java",
                "raw_url": "https://github.com/apache/hbase/raw/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java",
                "status": "modified",
                "changes": 13,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/MetaTableLocator.java?ref=3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c",
                "patch": "@@ -550,17 +550,20 @@ public void deleteMetaLocation(ZooKeeperWatcher zookeeper, int replicaId)\n       final long timeout, Configuration conf)\n           throws InterruptedException {\n     int numReplicasConfigured = 1;\n+\n+    List<ServerName> servers = new ArrayList<ServerName>();\n+    // Make the blocking call first so that we do the wait to know\n+    // the znodes are all in place or timeout.\n+    ServerName server = blockUntilAvailable(zkw, timeout);\n+    if (server == null) return null;\n+    servers.add(server);\n+\n     try {\n       List<String> metaReplicaNodes = zkw.getMetaReplicaNodes();\n       numReplicasConfigured = metaReplicaNodes.size();\n     } catch (KeeperException e) {\n       LOG.warn(\"Got ZK exception \" + e);\n     }\n-    List<ServerName> servers = new ArrayList<ServerName>(numReplicasConfigured);\n-    ServerName server = blockUntilAvailable(zkw, timeout);\n-    if (server == null) return null;\n-    servers.add(server);\n-\n     for (int replicaId = 1; replicaId < numReplicasConfigured; replicaId++) {\n       // return all replica locations for the meta\n       servers.add(getMetaRegionLocation(zkw, replicaId));",
                "deletions": 5
            },
            {
                "sha": "1f3afe42436c0d8a2062e35ec0f7ad090ef143d8",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java",
                "blob_url": "https://github.com/apache/hbase/blob/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java",
                "raw_url": "https://github.com/apache/hbase/raw/3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java",
                "status": "modified",
                "changes": 8,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java?ref=3757da643d43bf0eaf8a0bd4c30b56f24c95fb6c",
                "patch": "@@ -481,9 +481,11 @@ public boolean isDefaultMetaReplicaZnode(String node) {\n   public List<String> getMetaReplicaNodes() throws KeeperException {\n     List<String> childrenOfBaseNode = ZKUtil.listChildrenNoWatch(this, baseZNode);\n     List<String> metaReplicaNodes = new ArrayList<String>(2);\n-    String pattern = conf.get(\"zookeeper.znode.metaserver\",\"meta-region-server\");\n-    for (String child : childrenOfBaseNode) {\n-      if (child.startsWith(pattern)) metaReplicaNodes.add(child);\n+    if (childrenOfBaseNode != null) {\n+      String pattern = conf.get(\"zookeeper.znode.metaserver\",\"meta-region-server\");\n+      for (String child : childrenOfBaseNode) {\n+        if (child.startsWith(pattern)) metaReplicaNodes.add(child);\n+      }\n     }\n     return metaReplicaNodes;\n   }",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-18263 Resolve NPE in backup Master UI when accessing procedures.jsp\n\nSigned-off-by: tedyu <yuzhihong@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/96aca6b15392e9bdc611eee7e3273f424730cbd7",
        "parent": "https://github.com/apache/hbase/commit/d092008766c460de329d14d40e9cfd2377dcaf01",
        "bug_id": "hbase_77",
        "file": [
            {
                "sha": "58ae3eb07c00a9f8a4b4e8892a6e3b2c7ce82a68",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/96aca6b15392e9bdc611eee7e3273f424730cbd7/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/96aca6b15392e9bdc611eee7e3273f424730cbd7/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon?ref=96aca6b15392e9bdc611eee7e3273f424730cbd7",
                "patch": "@@ -125,7 +125,9 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n                 <ul class=\"nav navbar-nav\">\n                 <li class=\"active\"><a href=\"/\">Home</a></li>\n                 <li><a href=\"/tablesDetailed.jsp\">Table Details</a></li>\n+                <%if master.isActiveMaster() %>\n                 <li><a href=\"/procedures.jsp\">Procedures &amp; Locks</a></li>\n+                </%if>\n                 <li><a href=\"/logs/\">Local Logs</a></li>\n                 <li><a href=\"/logLevel\">Log Level</a></li>\n                 <li><a href=\"/dump\">Debug Dump</a></li>",
                "deletions": 0
            },
            {
                "sha": "e6bca57613a4c08c35fd0898d3aca3e9dd076a2c",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/tablesDetailed.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/96aca6b15392e9bdc611eee7e3273f424730cbd7/hbase-server/src/main/resources/hbase-webapps/master/tablesDetailed.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/96aca6b15392e9bdc611eee7e3273f424730cbd7/hbase-server/src/main/resources/hbase-webapps/master/tablesDetailed.jsp",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/tablesDetailed.jsp?ref=96aca6b15392e9bdc611eee7e3273f424730cbd7",
                "patch": "@@ -64,7 +64,9 @@\n               <ul class=\"nav navbar-nav\">\n                   <li class=\"active\"><a href=\"/master-status\">Home</a></li>\n                   <li><a href=\"/tablesDetailed.jsp\">Table Details</a></li>\n+                  <%if (master.isActiveMaster()) { %>\n                   <li><a href=\"/procedures.jsp\">Procedures &amp; Locks</a></li>\n+                  <% } %>\n                   <li><a href=\"/logs/\">Local Logs</a></li>\n                   <li><a href=\"/logLevel\">Log Level</a></li>\n                   <li><a href=\"/dump\">Debug Dump</a></li>",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16910 Avoid NPE when starting StochasticLoadBalancer (Guanghao Zhang)",
        "commit": "https://github.com/apache/hbase/commit/e2236396713cfc1cef61150a569e972c2faaca04",
        "parent": "https://github.com/apache/hbase/commit/0ae211eb399e5524196d89af8eac1941c8b61b60",
        "bug_id": "hbase_78",
        "file": [
            {
                "sha": "c42c46d108ee627e8a813cbcf44e613b5d8f3067",
                "filename": "hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "patch": "@@ -391,8 +391,8 @@ public void initialize() throws HBaseIOException {\n         HBASE_GROUP_LOADBALANCER_CLASS,\n         StochasticLoadBalancer.class, LoadBalancer.class);\n     internalBalancer = ReflectionUtils.newInstance(balancerKlass, config);\n-    internalBalancer.setClusterStatus(clusterStatus);\n     internalBalancer.setMasterServices(masterServices);\n+    internalBalancer.setClusterStatus(clusterStatus);\n     internalBalancer.setConf(config);\n     internalBalancer.initialize();\n   }",
                "deletions": 1
            },
            {
                "sha": "eac2fa22f8d61abefebae80ce9873f6876822169",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "patch": "@@ -742,8 +742,8 @@ private void finishActiveMasterInitialization(MonitoredTask status)\n     }\n \n     //initialize load balancer\n-    this.balancer.setClusterStatus(getClusterStatus());\n     this.balancer.setMasterServices(this);\n+    this.balancer.setClusterStatus(getClusterStatus());\n     this.balancer.initialize();\n \n     // Check if master is shutting down because of some issue",
                "deletions": 1
            },
            {
                "sha": "b02aac1aadd76aab8d03cd1ea370bb7c64e8ea81",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/e2236396713cfc1cef61150a569e972c2faaca04/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java?ref=e2236396713cfc1cef61150a569e972c2faaca04",
                "patch": "@@ -233,7 +233,7 @@ public synchronized void setClusterStatus(ClusterStatus st) {\n \n       updateMetricsSize(tablesCount * (functionsCount + 1)); // +1 for overall\n     } catch (Exception e) {\n-      LOG.error(\"failed to get the size of all tables, exception = \" + e.getMessage());\n+      LOG.error(\"failed to get the size of all tables\", e);\n     }\n   }\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16866 Avoid NPE in AsyncRequestFutureImpl#updateStats (ChiaPing Tsai)",
        "commit": "https://github.com/apache/hbase/commit/6c89c6251ff611c2f10bfd6f8c9f8a2d717dc71b",
        "parent": "https://github.com/apache/hbase/commit/bb6cc4d43e36fa1d558c6ece2c5c6d1ed414db0c",
        "bug_id": "hbase_79",
        "file": [
            {
                "sha": "6b6b99aa4f51ccb7fa4c1c9dc2d9616ebe02376a",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/6c89c6251ff611c2f10bfd6f8c9f8a2d717dc71b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/6c89c6251ff611c2f10bfd6f8c9f8a2d717dc71b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRequestFutureImpl.java?ref=6c89c6251ff611c2f10bfd6f8c9f8a2d717dc71b",
                "patch": "@@ -977,6 +977,11 @@ protected void updateStats(ServerName server, Map<byte[], MultiResponse.RegionRe\n     for (Map.Entry<byte[], MultiResponse.RegionResult> regionStats : results.entrySet()) {\n       byte[] regionName = regionStats.getKey();\n       ClientProtos.RegionLoadStats stat = regionStats.getValue().getStat();\n+      if (stat == null) {\n+        LOG.error(\"No ClientProtos.RegionLoadStats found for server=\" + server\n+          + \", region=\" + Bytes.toStringBinary(regionName));\n+        continue;\n+      }\n       RegionLoadStats regionLoadstats = ProtobufUtil.createRegionLoadStats(stat);\n       ResultStatsUtil.updateStats(asyncProcess.connection.getStatisticsTracker(), server,\n           regionName, regionLoadstats);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16855 Avoid NPE in MetricsConnection\u2019s construction (ChiaPing Tsai)",
        "commit": "https://github.com/apache/hbase/commit/c8e9a295c133ef9507a84ab9c70d18563e2c22ad",
        "parent": "https://github.com/apache/hbase/commit/278625312047a2100f4dbb2d2eaa4e2219d00e14",
        "bug_id": "hbase_80",
        "file": [
            {
                "sha": "36627bda40938193636c5292a2753f799f5e2f06",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "blob_url": "https://github.com/apache/hbase/blob/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "raw_url": "https://github.com/apache/hbase/raw/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "status": "modified",
                "changes": 31,
                "additions": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java?ref=c8e9a295c133ef9507a84ab9c70d18563e2c22ad",
                "patch": "@@ -298,23 +298,29 @@ public void updateRegionStats(ServerName serverName, byte[] regionName,\n   private final ConcurrentMap<String, Counter> cacheDroppingExceptions =\n     new ConcurrentHashMap<>(CAPACITY, LOAD_FACTOR, CONCURRENCY_LEVEL);\n \n-  public MetricsConnection(final ConnectionImplementation conn) {\n+  MetricsConnection(final ConnectionImplementation conn) {\n     this.scope = conn.toString();\n     this.registry = new MetricRegistry();\n-    final ThreadPoolExecutor batchPool = (ThreadPoolExecutor) conn.getCurrentBatchPool();\n-    final ThreadPoolExecutor metaPool = (ThreadPoolExecutor) conn.getCurrentMetaLookupPool();\n \n-    this.registry.register(name(this.getClass(), \"executorPoolActiveThreads\", scope),\n+    this.registry.register(getExecutorPoolName(),\n         new RatioGauge() {\n           @Override\n           protected Ratio getRatio() {\n+            ThreadPoolExecutor batchPool = (ThreadPoolExecutor) conn.getCurrentBatchPool();\n+            if (batchPool == null) {\n+              return Ratio.of(0, 0);\n+            }\n             return Ratio.of(batchPool.getActiveCount(), batchPool.getMaximumPoolSize());\n           }\n         });\n-    this.registry.register(name(this.getClass(), \"metaPoolActiveThreads\", scope),\n+    this.registry.register(getMetaPoolName(),\n         new RatioGauge() {\n           @Override\n           protected Ratio getRatio() {\n+            ThreadPoolExecutor metaPool = (ThreadPoolExecutor) conn.getCurrentMetaLookupPool();\n+            if (metaPool == null) {\n+              return Ratio.of(0, 0);\n+            }\n             return Ratio.of(metaPool.getActiveCount(), metaPool.getMaximumPoolSize());\n           }\n         });\n@@ -337,6 +343,21 @@ protected Ratio getRatio() {\n     this.reporter.start();\n   }\n \n+  @VisibleForTesting\n+  final String getExecutorPoolName() {\n+    return name(getClass(), \"executorPoolActiveThreads\", scope);\n+  }\n+\n+  @VisibleForTesting\n+  final String getMetaPoolName() {\n+    return name(getClass(), \"metaPoolActiveThreads\", scope);\n+  }\n+\n+  @VisibleForTesting\n+  MetricRegistry getMetricRegistry() {\n+    return registry;\n+  }\n+\n   public void shutdown() {\n     this.reporter.stop();\n   }",
                "deletions": 5
            },
            {
                "sha": "854ecc50221937dbae0a289af37cca42d7b9dd96",
                "filename": "hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java",
                "blob_url": "https://github.com/apache/hbase/blob/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java",
                "raw_url": "https://github.com/apache/hbase/raw/c8e9a295c133ef9507a84ab9c70d18563e2c22ad/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java",
                "status": "modified",
                "changes": 24,
                "additions": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestMetricsConnection.java?ref=c8e9a295c133ef9507a84ab9c70d18563e2c22ad",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.client;\n \n+import com.codahale.metrics.RatioGauge;\n+import com.codahale.metrics.RatioGauge.Ratio;\n import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;\n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.ClientService;\n@@ -32,24 +34,28 @@\n import org.apache.hadoop.hbase.testclassification.SmallTests;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.junit.AfterClass;\n-import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.mockito.Mockito;\n \n import java.io.IOException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import static org.junit.Assert.assertEquals;\n \n @Category({ClientTests.class, MetricsTests.class, SmallTests.class})\n public class TestMetricsConnection {\n \n   private static MetricsConnection METRICS;\n-\n+  private static final ExecutorService BATCH_POOL = Executors.newFixedThreadPool(2);\n   @BeforeClass\n   public static void beforeClass() {\n     ConnectionImplementation mocked = Mockito.mock(ConnectionImplementation.class);\n     Mockito.when(mocked.toString()).thenReturn(\"mocked-connection\");\n-    METRICS = new MetricsConnection(Mockito.mock(ConnectionImplementation.class));\n+    Mockito.when(mocked.getCurrentBatchPool()).thenReturn(BATCH_POOL);\n+    METRICS = new MetricsConnection(mocked);\n   }\n \n   @AfterClass\n@@ -112,9 +118,15 @@ public void testStaticMetrics() throws IOException {\n         METRICS.getTracker, METRICS.scanTracker, METRICS.multiTracker, METRICS.appendTracker,\n         METRICS.deleteTracker, METRICS.incrementTracker, METRICS.putTracker\n     }) {\n-      Assert.assertEquals(\"Failed to invoke callTimer on \" + t, loop, t.callTimer.getCount());\n-      Assert.assertEquals(\"Failed to invoke reqHist on \" + t, loop, t.reqHist.getCount());\n-      Assert.assertEquals(\"Failed to invoke respHist on \" + t, loop, t.respHist.getCount());\n+      assertEquals(\"Failed to invoke callTimer on \" + t, loop, t.callTimer.getCount());\n+      assertEquals(\"Failed to invoke reqHist on \" + t, loop, t.reqHist.getCount());\n+      assertEquals(\"Failed to invoke respHist on \" + t, loop, t.respHist.getCount());\n     }\n+    RatioGauge executorMetrics = (RatioGauge) METRICS.getMetricRegistry()\n+            .getMetrics().get(METRICS.getExecutorPoolName());\n+    RatioGauge metaMetrics = (RatioGauge) METRICS.getMetricRegistry()\n+            .getMetrics().get(METRICS.getMetaPoolName());\n+    assertEquals(Ratio.of(0, 3).getValue(), executorMetrics.getValue(), 0);\n+    assertEquals(Double.NaN, metaMetrics.getValue(), 0);\n   }\n }",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15999 NPE in MemstoreCompactor (Ram)",
        "commit": "https://github.com/apache/hbase/commit/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f",
        "parent": "https://github.com/apache/hbase/commit/158568e7806e461275406bc15856ba26e4660f4c",
        "bug_id": "hbase_81",
        "file": [
            {
                "sha": "ec5684dcda1fe49d6e5ff13eba43496244285e8d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/CompactingMemStore.java?ref=f19f1d9e99c8058f6ef5caa42c1b07a8ae1de53f",
                "patch": "@@ -253,10 +253,11 @@ protected void checkActiveSize() {\n       * in exclusive mode while this method (checkActiveSize) is invoked holding updatesLock\n       * in the shared mode. */\n       InMemoryFlushRunnable runnable = new InMemoryFlushRunnable();\n-      LOG.info(\"Dispatching the MemStore in-memory flush for store \" + store.getColumnFamilyName());\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\n+          \"Dispatching the MemStore in-memory flush for store \" + store.getColumnFamilyName());\n+      }\n       getPool().execute(runnable);\n-      // guard against queuing same old compactions over and over again\n-      inMemoryFlushInProgress.set(true);\n     }\n   }\n \n@@ -277,10 +278,9 @@ void flushInMemory() throws IOException {\n     }\n     // Phase II: Compact the pipeline\n     try {\n-      if (allowCompaction.get()) {\n+      if (allowCompaction.get() && inMemoryFlushInProgress.compareAndSet(false, true)) {\n         // setting the inMemoryFlushInProgress flag again for the case this method is invoked\n         // directly (only in tests) in the common path setting from true to true is idempotent\n-        inMemoryFlushInProgress.set(true);\n         // Speculative compaction execution, may be interrupted if flush is forced while\n         // compaction is in progress\n         compactor.startCompaction();",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-17450 TablePermission#equals throws NPE after namespace support was added (huzheng)",
        "commit": "https://github.com/apache/hbase/commit/cdee1a7034a5ec39a0d5435916050c3ae0ffa339",
        "parent": "https://github.com/apache/hbase/commit/7794c530bd8dea47e16ca5329270aecc46ea9c8f",
        "bug_id": "hbase_82",
        "file": [
            {
                "sha": "cf3f071a56a2dbb456a89a119f24ff7dc5648fab",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java",
                "blob_url": "https://github.com/apache/hbase/blob/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java",
                "raw_url": "https://github.com/apache/hbase/raw/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java?ref=cdee1a7034a5ec39a0d5435916050c3ae0ffa339",
                "patch": "@@ -314,7 +314,8 @@ public boolean equals(Object obj) {\n     }\n     TablePermission other = (TablePermission)obj;\n \n-    if (!(table.equals(other.getTableName()) &&\n+    if (!(((table == null && other.getTableName() == null) ||\n+           (table != null && table.equals(other.getTableName()))) &&\n         ((family == null && other.getFamily() == null) ||\n          Bytes.equals(family, other.getFamily())) &&\n         ((qualifier == null && other.getQualifier() == null) ||",
                "deletions": 1
            },
            {
                "sha": "1e525e2008f9371558e4e2d20764b670665541c3",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java",
                "blob_url": "https://github.com/apache/hbase/blob/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java",
                "raw_url": "https://github.com/apache/hbase/raw/cdee1a7034a5ec39a0d5435916050c3ae0ffa339/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java",
                "status": "modified",
                "changes": 11,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java?ref=cdee1a7034a5ec39a0d5435916050c3ae0ffa339",
                "patch": "@@ -83,6 +83,8 @@ public boolean isAborted() {\n     }\n   };\n \n+  private static String TEST_NAMESPACE = \"perms_test_ns\";\n+  private static String TEST_NAMESPACE2 = \"perms_test_ns2\";\n   private static TableName TEST_TABLE =\n       TableName.valueOf(\"perms_test\");\n   private static TableName TEST_TABLE2 =\n@@ -409,6 +411,15 @@ public void testEquals() throws Exception {\n     p2 = new TablePermission(TEST_TABLE, null);\n     assertFalse(p1.equals(p2));\n     assertFalse(p2.equals(p1));\n+\n+    p1 = new TablePermission(TEST_NAMESPACE, TablePermission.Action.READ);\n+    p2 = new TablePermission(TEST_NAMESPACE, TablePermission.Action.READ);\n+    assertEquals(p1, p2);\n+\n+    p1 = new TablePermission(TEST_NAMESPACE, TablePermission.Action.READ);\n+    p2 = new TablePermission(TEST_NAMESPACE2, TablePermission.Action.READ);\n+    assertFalse(p1.equals(p2));\n+    assertFalse(p2.equals(p1));\n   }\n \n   @Test",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "NPE in RpcServer causing intermittent UT failure of TestMasterReplication#testHFileCyclicReplication",
        "commit": "https://github.com/apache/hbase/commit/a33097e067b73be8e877b822afa90b89a0c7974f",
        "parent": "https://github.com/apache/hbase/commit/cb118c8de6ddb783e90c07912a5fbdd629eabf06",
        "bug_id": "hbase_83",
        "file": [
            {
                "sha": "73226aa4c762e39c13f61eb4c93868a40a4ebf10",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a33097e067b73be8e877b822afa90b89a0c7974f/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a33097e067b73be8e877b822afa90b89a0c7974f/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=a33097e067b73be8e877b822afa90b89a0c7974f",
                "patch": "@@ -2246,7 +2246,13 @@ public void setSecretManager(SecretManager<? extends TokenIdentifier> secretMana\n       // The above callBlockingMethod will always return a SE.  Strip the SE wrapper before\n       // putting it on the wire.  Its needed to adhere to the pb Service Interface but we don't\n       // need to pass it over the wire.\n-      if (e instanceof ServiceException) e = e.getCause();\n+      if (e instanceof ServiceException) {\n+        if (e.getCause() == null) {\n+          LOG.debug(\"Caught a ServiceException with null cause\", e);\n+        } else {\n+          e = e.getCause();\n+        }\n+      }\n \n       // increment the number of requests that were exceptions.\n       metrics.exception(e);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19310 Avoid an NPE IntegrationTestImportTsv when outside of the context of JUnit\n\nSigned-off-by: Michael Stack <stack@apache.org>\nSigned-off-by: Ted Yu <tedyu@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/b0b606429339aabe9fb964af6bf3c3129b3ac375",
        "parent": "https://github.com/apache/hbase/commit/548ebbc574021ca22ba92633678d4f0cec70be0d",
        "bug_id": "hbase_84",
        "file": [
            {
                "sha": "dfc54e0b338361af4f72686699950273e3899b6c",
                "filename": "hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java",
                "blob_url": "https://github.com/apache/hbase/blob/b0b606429339aabe9fb964af6bf3c3129b3ac375/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java",
                "raw_url": "https://github.com/apache/hbase/raw/b0b606429339aabe9fb964af6bf3c3129b3ac375/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java",
                "status": "modified",
                "changes": 12,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java?ref=b0b606429339aabe9fb964af6bf3c3129b3ac375",
                "patch": "@@ -185,13 +185,15 @@ protected static void validateDeletedPartitionsFile(Configuration conf) throws I\n \n   @Test\n   public void testGenerateAndLoad() throws Exception {\n+    generateAndLoad(TableName.valueOf(name.getMethodName()));\n+  }\n+\n+  void generateAndLoad(final TableName table) throws Exception {\n     LOG.info(\"Running test testGenerateAndLoad.\");\n-    final TableName table = TableName.valueOf(name.getMethodName());\n     String cf = \"d\";\n     Path hfiles = new Path(\n         util.getDataTestDirOnTestFS(table.getNameAsString()), \"hfiles\");\n \n-\n     Map<String, String> args = new HashMap<>();\n     args.put(ImportTsv.BULK_OUTPUT_CONF_KEY, hfiles.toString());\n     args.put(ImportTsv.COLUMNS_CONF_KEY,\n@@ -226,7 +228,11 @@ public int run(String[] args) throws Exception {\n     // adding more test methods? Don't forget to add them here... or consider doing what\n     // IntegrationTestsDriver does.\n     provisionCluster();\n-    testGenerateAndLoad();\n+    TableName tableName = TableName.valueOf(\"IntegrationTestImportTsv\");\n+    if (util.getAdmin().tableExists(tableName)) {\n+      util.deleteTable(tableName);\n+    }\n+    generateAndLoad(tableName);\n     releaseCluster();\n \n     return 0;",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"guard against NPE while reading FileTrailer and HFileBlock\"\n\nThis reverts commit 201c8382508da1266d11e04d3c7cbef42e0a256a.\n\nReverted because missing JIRA number. Fixing...",
        "commit": "https://github.com/apache/hbase/commit/9a4068dcf8caec644e6703ffa365a8649bbd336e",
        "parent": "https://github.com/apache/hbase/commit/edbd0e494d7abaf50319b7650e350d52b195fcc9",
        "bug_id": "hbase_85",
        "file": [
            {
                "sha": "7eac9c6eca6be97c4d380c5fe0e1d5a03b14e92f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "blob_url": "https://github.com/apache/hbase/blob/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "raw_url": "https://github.com/apache/hbase/raw/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=9a4068dcf8caec644e6703ffa365a8649bbd336e",
                "patch": "@@ -388,8 +388,7 @@ public static FixedFileTrailer readFromStream(FSDataInputStream istream,\n       bufferSize = (int) fileSize;\n     }\n \n-    HFileUtil.seekOnMultipleSources(istream, seekPoint);\n-\n+    istream.seek(seekPoint);\n     ByteBuffer buf = ByteBuffer.allocate(bufferSize);\n     istream.readFully(buf.array(), buf.arrayOffset(),\n         buf.arrayOffset() + buf.limit());",
                "deletions": 2
            },
            {
                "sha": "fba15baa0976a0952ed4375277ac1a090b9bc0f7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "blob_url": "https://github.com/apache/hbase/blob/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "raw_url": "https://github.com/apache/hbase/raw/9a4068dcf8caec644e6703ffa365a8649bbd336e/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java?ref=9a4068dcf8caec644e6703ffa365a8649bbd336e",
                "patch": "@@ -1512,7 +1512,7 @@ protected int readAtOffset(FSDataInputStream istream, byte [] dest, int destOffs\n       if (!pread && streamLock.tryLock()) {\n         // Seek + read. Better for scanning.\n         try {\n-          HFileUtil.seekOnMultipleSources(istream, fileOffset);\n+          istream.seek(fileOffset);\n \n           long realOffset = istream.getPos();\n           if (realOffset != fileOffset) {",
                "deletions": 1
            },
            {
                "sha": "835450c2a9a8c8227ae4afe469bd0ec9d272dd49",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/edbd0e494d7abaf50319b7650e350d52b195fcc9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/edbd0e494d7abaf50319b7650e350d52b195fcc9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "status": "removed",
                "changes": 43,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java?ref=edbd0e494d7abaf50319b7650e350d52b195fcc9",
                "patch": "@@ -1,43 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hbase.io.hfile;\n-\n-import java.io.IOException;\n-\n-import org.apache.hadoop.fs.FSDataInputStream;\n-\n-public class HFileUtil {\n-\n-  /** guards against NullPointer\n-   * utility which tries to seek on the DFSIS and will try an alternative source\n-   * if the FSDataInputStream throws an NPE HBASE-17501\n-   * @param istream\n-   * @param offset\n-   * @throws IOException\n-   */\n-  static public void seekOnMultipleSources(FSDataInputStream istream, long offset) throws IOException {\n-    try {\n-      // attempt to seek inside of current blockReader\n-      istream.seek(offset);\n-    } catch (NullPointerException e) {\n-      // retry the seek on an alternate copy of the data\n-      // this can occur if the blockReader on the DFSInputStream is null\n-      istream.seekToNewSource(offset);\n-    }\n-  }\n-}",
                "deletions": 43
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14909 NPE testing for RIT",
        "commit": "https://github.com/apache/hbase/commit/da0cc598feab995eed12527d90805dd627674035",
        "parent": "https://github.com/apache/hbase/commit/08f90f30b385bbf314f7fb014b810d86330d60b1",
        "bug_id": "hbase_86",
        "file": [
            {
                "sha": "006c3e71362f90732b2febbe947c9a28ec1858bd",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/da0cc598feab995eed12527d90805dd627674035/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/da0cc598feab995eed12527d90805dd627674035/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "status": "modified",
                "changes": 20,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=da0cc598feab995eed12527d90805dd627674035",
                "patch": "@@ -17,7 +17,10 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import javax.annotation.Nullable;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -44,6 +47,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import javax.annotation.Nullable;\n+\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -58,7 +63,6 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -84,6 +88,7 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n+import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -129,10 +134,6 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3767,8 +3768,11 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n-            .getAssignmentManager().getRegionStates();\n+        HMaster master = getMiniHBaseCluster().getMaster();\n+        if (master == null) return false;\n+        AssignmentManager am = master.getAssignmentManager();\n+        if (am == null) return false;\n+        final RegionStates regionStates = am.getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15369 Handle NPE in region.jsp (Samir Ahmic)",
        "commit": "https://github.com/apache/hbase/commit/3826894f890a850270053a25b53f07a007555711",
        "parent": "https://github.com/apache/hbase/commit/c93cffb95c0322fc4244fe78a584ff225bc105c9",
        "bug_id": "hbase_87",
        "file": [
            {
                "sha": "02f3d943d5075ab70a9b0ddb51d2687271258447",
                "filename": "hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/3826894f890a850270053a25b53f07a007555711/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/3826894f890a850270053a25b53f07a007555711/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "status": "modified",
                "changes": 18,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp?ref=3826894f890a850270053a25b53f07a007555711",
                "patch": "@@ -21,6 +21,7 @@\n   import=\"java.util.Collection\"\n   import=\"java.util.Date\"\n   import=\"java.util.List\"\n+  import=\"org.owasp.esapi.ESAPI\"\n   import=\"static org.apache.commons.lang.StringEscapeUtils.escapeXml\"\n   import=\"org.apache.hadoop.conf.Configuration\"\n   import=\"org.apache.hadoop.hbase.HTableDescriptor\"\n@@ -35,10 +36,14 @@\n   String regionName = request.getParameter(\"name\");\n   HRegionServer rs = (HRegionServer) getServletContext().getAttribute(HRegionServer.REGIONSERVER);\n   Configuration conf = rs.getConfiguration();\n-\n+  String displayName = null;\n   Region region = rs.getFromOnlineRegions(regionName);\n-  String displayName = HRegionInfo.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n+  if(region == null) {\n+    displayName= ESAPI.encoder().encodeForHTML(regionName) + \" does not exist\";\n+  } else {\n+    displayName = HRegionInfo.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n     rs.getConfiguration());\n+  }\n %>\n <!--[if IE]>\n <!DOCTYPE html>\n@@ -121,7 +126,14 @@\n          <p> <%= storeFiles.size() %> StoreFile(s) in set.</p>\n          </table>\n    <%  }\n-   }%>\n+   } else { %>\n+   <div class=\"container-fluid content\">\n+   <div class=\"row inner_header\">\n+   </div>\n+   <p><hr><p>\n+   <p>Go <a href=\"javascript:history.back()\">Back</a>\n+   </div>\n+  <% } %>\n </div>\n <script src=\"/static/js/jquery.min.js\" type=\"text/javascript\"></script>\n <script src=\"/static/js/bootstrap.min.js\" type=\"text/javascript\"></script>",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14501 NPE in replication with TDE",
        "commit": "https://github.com/apache/hbase/commit/2ff6d0fe4789857ab51685949711d755dedd459a",
        "parent": "https://github.com/apache/hbase/commit/6143b7694cc02e905b931de86462c6125ca8b3b6",
        "bug_id": "hbase_88",
        "file": [
            {
                "sha": "7534e9df6a2f690d57f5dd9e90feee26879e4144",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "status": "modified",
                "changes": 8,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "patch": "@@ -23,6 +23,7 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -2400,8 +2401,7 @@ public static KeyValue cloneAndAddTags(Cell c, List<Tag> newTags) {\n    * Create a KeyValue reading from the raw InputStream.\n    * Named <code>iscreate</code> so doesn't clash with {@link #create(DataInput)}\n    * @param in\n-   * @return Created KeyValue OR if we find a length of zero, we will return null which\n-   * can be useful marking a stream as done.\n+   * @return Created KeyValue or throws an exception\n    * @throws IOException\n    * {@link Deprecated} As of 1.2. Use {@link KeyValueUtil#iscreate(InputStream, boolean)} instead.\n    */\n@@ -2412,7 +2412,9 @@ public static KeyValue iscreate(final InputStream in) throws IOException {\n     while (bytesRead < intBytes.length) {\n       int n = in.read(intBytes, bytesRead, intBytes.length - bytesRead);\n       if (n < 0) {\n-        if (bytesRead == 0) return null; // EOF at start is ok\n+        if (bytesRead == 0) {\n+          throw new EOFException();\n+        }\n         throw new IOException(\"Failed read of int, read \" + bytesRead + \" bytes\");\n       }\n       bytesRead += n;",
                "deletions": 3
            },
            {
                "sha": "98e2205c9c75b49d91d7f4e6aa550719c16e63a3",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java",
                "status": "modified",
                "changes": 13,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "patch": "@@ -20,6 +20,7 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -187,7 +188,7 @@ public static void appendToByteBuffer(final ByteBuffer bb, final KeyValue kv,\n    * position to the start of the next KeyValue. Does not allocate a new array or copy data.\n    * @param bb\n    * @param includesMvccVersion\n-   * @param includesTags \n+   * @param includesTags\n    */\n   public static KeyValue nextShallowCopy(final ByteBuffer bb, final boolean includesMvccVersion,\n       boolean includesTags) {\n@@ -231,7 +232,7 @@ public static KeyValue previousKey(final KeyValue in) {\n     return createFirstOnRow(CellUtil.cloneRow(in), CellUtil.cloneFamily(in),\n       CellUtil.cloneQualifier(in), in.getTimestamp() - 1);\n   }\n-  \n+\n \n   /**\n    * Create a KeyValue for the specified row, family and qualifier that would be\n@@ -449,6 +450,7 @@ public static KeyValue ensureKeyValue(final Cell cell) {\n   @Deprecated\n   public static List<KeyValue> ensureKeyValues(List<Cell> cells) {\n     List<KeyValue> lazyList = Lists.transform(cells, new Function<Cell, KeyValue>() {\n+      @Override\n       public KeyValue apply(Cell arg0) {\n         return KeyValueUtil.ensureKeyValue(arg0);\n       }\n@@ -491,8 +493,9 @@ public static KeyValue iscreate(final InputStream in, boolean withTags) throws I\n     while (bytesRead < intBytes.length) {\n       int n = in.read(intBytes, bytesRead, intBytes.length - bytesRead);\n       if (n < 0) {\n-        if (bytesRead == 0)\n-          return null; // EOF at start is ok\n+        if (bytesRead == 0) {\n+          throw new EOFException();\n+        }\n         throw new IOException(\"Failed read of int, read \" + bytesRead + \" bytes\");\n       }\n       bytesRead += n;\n@@ -555,7 +558,7 @@ public static KeyValue create(final DataInput in) throws IOException {\n \n   /**\n    * Create a KeyValue reading <code>length</code> from <code>in</code>\n-   * \n+   *\n    * @param length\n    * @param in\n    * @return Created KeyValue OR if we find a length of zero, we will return",
                "deletions": 5
            },
            {
                "sha": "09dc37fd3674d64d89aaaf0470f18876cd0b2e80",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java",
                "status": "modified",
                "changes": 38,
                "additions": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/BaseDecoder.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "patch": "@@ -20,6 +20,9 @@\n import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n+import java.io.PushbackInputStream;\n+\n+import javax.annotation.Nonnull;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -32,27 +35,41 @@\n @InterfaceAudience.Private\n public abstract class BaseDecoder implements Codec.Decoder {\n   protected static final Log LOG = LogFactory.getLog(BaseDecoder.class);\n-  protected final InputStream in;\n-  private boolean hasNext = true;\n+\n+  protected final PBIS in;\n   private Cell current = null;\n \n+  protected static class PBIS extends PushbackInputStream {\n+    public PBIS(InputStream in, int size) {\n+      super(in, size);\n+    }\n+\n+    public void resetBuf(int size) {\n+      this.buf = new byte[size];\n+      this.pos = size;\n+    }\n+  }\n+\n   public BaseDecoder(final InputStream in) {\n-    this.in = in;\n+    this.in = new PBIS(in, 1);\n   }\n \n   @Override\n   public boolean advance() throws IOException {\n-    if (!this.hasNext) return this.hasNext;\n-    if (this.in.available() == 0) {\n-      this.hasNext = false;\n-      return this.hasNext;\n+    int firstByte = in.read();\n+    if (firstByte == -1) {\n+      return false;\n+    } else {\n+      in.unread(firstByte);\n     }\n+\n     try {\n       this.current = parseCell();\n     } catch (IOException ioEx) {\n+      in.resetBuf(1); // reset the buffer in case the underlying stream is read from upper layers\n       rethrowEofException(ioEx);\n     }\n-    return this.hasNext;\n+    return true;\n   }\n \n   private void rethrowEofException(IOException ioEx) throws IOException {\n@@ -72,9 +89,12 @@ private void rethrowEofException(IOException ioEx) throws IOException {\n   }\n \n   /**\n-   * @return extract a Cell\n+   * Extract a Cell.\n+   * @return a parsed Cell or throws an Exception. EOFException or a generic IOException maybe\n+   * thrown if EOF is reached prematurely. Does not return null.\n    * @throws IOException\n    */\n+  @Nonnull\n   protected abstract Cell parseCell() throws IOException;\n \n   @Override",
                "deletions": 9
            },
            {
                "sha": "666f440eea643ae3cf56f3b10352642a76a270aa",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodec.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "patch": "@@ -79,6 +79,7 @@ public CellDecoder(final InputStream in) {\n       super(in);\n     }\n \n+    @Override\n     protected Cell parseCell() throws IOException {\n       byte [] row = readByteArray(this.in);\n       byte [] family = readByteArray(in);",
                "deletions": 0
            },
            {
                "sha": "69181e5e94d6d82b8baca91823580f2263c7858e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java",
                "status": "modified",
                "changes": 9,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "patch": "@@ -19,7 +19,6 @@\n \n import java.io.ByteArrayInputStream;\n import java.io.ByteArrayOutputStream;\n-import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n@@ -84,12 +83,8 @@ protected Cell parseCell() throws IOException {\n         return super.parseCell();\n       }\n       int ivLength = 0;\n-      try {\n-        ivLength = StreamUtils.readRawVarint32(in);\n-      } catch (EOFException e) {\n-        // EOF at start is OK\n-        return null;\n-      }\n+\n+      ivLength = StreamUtils.readRawVarint32(in);\n \n       // TODO: An IV length of 0 could signify an unwrapped cell, when the\n       // encoder supports that just read the remainder in directly",
                "deletions": 7
            },
            {
                "sha": "3d9952324e5ef8bf560261746d43b8db0a7a814f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "blob_url": "https://github.com/apache/hbase/blob/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "raw_url": "https://github.com/apache/hbase/raw/2ff6d0fe4789857ab51685949711d755dedd459a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=2ff6d0fe4789857ab51685949711d755dedd459a",
                "patch": "@@ -855,9 +855,10 @@ private int countDistinctRowKeys(WALEdit edit) {\n       int distinctRowKeys = 1;\n       Cell lastCell = cells.get(0);\n       for (int i = 0; i < edit.size(); i++) {\n-        if (!CellUtil.matchingRow(cells.get(i), lastCell)) {\n+        if (!CellUtil.matchingRows(cells.get(i), lastCell)) {\n           distinctRowKeys++;\n         }\n+        lastCell = cells.get(i);\n       }\n       return distinctRowKeys;\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15524 Fix NPE in client-side metrics",
        "commit": "https://github.com/apache/hbase/commit/db3ba652f88083b0b1c57b4857f11fce7ae5b131",
        "parent": "https://github.com/apache/hbase/commit/fd5c0934b60664ecdde21a994910953339c7060d",
        "bug_id": "hbase_89",
        "file": [
            {
                "sha": "142e2a0647fb231f3b6f2a5ef67970b1e01db045",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "blob_url": "https://github.com/apache/hbase/blob/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "raw_url": "https://github.com/apache/hbase/raw/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "status": "modified",
                "changes": 26,
                "additions": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java?ref=db3ba652f88083b0b1c57b4857f11fce7ae5b131",
                "patch": "@@ -1190,9 +1190,15 @@ private void receiveGlobalFailure(\n         byte[] row = e.getValue().iterator().next().getAction().getRow();\n         // Do not use the exception for updating cache because it might be coming from\n         // any of the regions in the MultiAction.\n-        if (tableName != null) {\n-          connection.updateCachedLocations(tableName, regionName, row,\n+        try {\n+          if (tableName != null) {\n+            connection.updateCachedLocations(tableName, regionName, row,\n               ClientExceptionsUtil.isMetaClearingException(t) ? null : t, server);\n+          }\n+        } catch (Throwable ex) {\n+          // That should never happen, but if it did, we want to make sure\n+          // we still process errors\n+          LOG.error(\"Couldn't update cached region locations: \" + ex);\n         }\n         for (Action<Row> action : e.getValue()) {\n           Retry retry = manageError(\n@@ -1317,8 +1323,14 @@ private void receiveMultiAction(MultiAction<Row> multiAction,\n             // Register corresponding failures once per server/once per region.\n             if (!regionFailureRegistered) {\n               regionFailureRegistered = true;\n-              connection.updateCachedLocations(\n+              try {\n+                connection.updateCachedLocations(\n                   tableName, regionName, row.getRow(), result, server);\n+              } catch (Throwable ex) {\n+                // That should never happen, but if it did, we want to make sure\n+                // we still process errors\n+                LOG.error(\"Couldn't update cached region locations: \" + ex);\n+              }\n             }\n             if (failureCount == 0) {\n               errorsByServer.reportServerError(server);\n@@ -1372,8 +1384,14 @@ private void receiveMultiAction(MultiAction<Row> multiAction,\n           // for every possible exception that comes through, however.\n           connection.clearCaches(server);\n         } else {\n-          connection.updateCachedLocations(\n+          try {\n+            connection.updateCachedLocations(\n               tableName, region, actions.get(0).getAction().getRow(), throwable, server);\n+          } catch (Throwable ex) {\n+            // That should never happen, but if it did, we want to make sure\n+            // we still process errors\n+            LOG.error(\"Couldn't update cached region locations: \" + ex);\n+          }\n         }\n         failureCount += actions.size();\n ",
                "deletions": 4
            },
            {
                "sha": "53a33264cb41df0cf8337abc1402f7c52f2f63b5",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "blob_url": "https://github.com/apache/hbase/blob/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "raw_url": "https://github.com/apache/hbase/raw/db3ba652f88083b0b1c57b4857f11fce7ae5b131/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetricsConnection.java?ref=db3ba652f88083b0b1c57b4857f11fce7ae5b131",
                "patch": "@@ -63,6 +63,7 @@\n   private static final String MEMLOAD_BASE = \"memstoreLoad_\";\n   private static final String HEAP_BASE = \"heapOccupancy_\";\n   private static final String CACHE_BASE = \"cacheDroppingExceptions_\";\n+  private static final String UNKNOWN_EXCEPTION = \"UnknownException\";\n   private static final String CLIENT_SVC = ClientService.getDescriptor().getName();\n \n   /** A container class for collecting details about the RPC call as it percolates. */\n@@ -464,7 +465,8 @@ public void updateRpc(MethodDescriptor method, Message param, CallStats stats) {\n   }\n \n   public void incrCacheDroppingExceptions(Object exception) {\n-    getMetric(CACHE_BASE + exception.getClass().getSimpleName(),\n+    getMetric(CACHE_BASE +\n+      (exception == null? UNKNOWN_EXCEPTION : exception.getClass().getSimpleName()),\n       cacheDroppingExceptions, counterFactory).inc();\n   }\n }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13970 NPE during compaction in trunk",
        "commit": "https://github.com/apache/hbase/commit/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503",
        "parent": "https://github.com/apache/hbase/commit/272b025b25fed979da0e59ffd41615bbb9e105ea",
        "bug_id": "hbase_90",
        "file": [
            {
                "sha": "78a5cac659d152c7a67801e6a083d4af3af7150f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "blob_url": "https://github.com/apache/hbase/blob/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "raw_url": "https://github.com/apache/hbase/raw/28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java",
                "status": "modified",
                "changes": 22,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java?ref=28a035000f3cbf7c2b242d6dd9ba3bf84f4e2503",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -221,6 +222,24 @@ protected InternalScanner postCreateCoprocScanner(final CompactionRequest reques\n     return store.getCoprocessorHost().preCompact(store, scanner, scanType, request);\n   }\n \n+  /**\n+   * Used to prevent compaction name conflict when multiple compactions running parallel on the\n+   * same store.\n+   */\n+  private static final AtomicInteger NAME_COUNTER = new AtomicInteger(0);\n+\n+  private String generateCompactionName() {\n+    int counter;\n+    for (;;) {\n+      counter = NAME_COUNTER.get();\n+      int next = counter == Integer.MAX_VALUE ? 0 : counter + 1;\n+      if (NAME_COUNTER.compareAndSet(counter, next)) {\n+        break;\n+      }\n+    }\n+    return store.getRegionInfo().getRegionNameAsString() + \"#\"\n+        + store.getFamily().getNameAsString() + \"#\" + counter;\n+  }\n   /**\n    * Performs the compaction.\n    * @param scanner Where to read from.\n@@ -242,8 +261,7 @@ protected boolean performCompaction(InternalScanner scanner, CellSink writer,\n     if (LOG.isDebugEnabled()) {\n       lastMillis = EnvironmentEdgeManager.currentTime();\n     }\n-    String compactionName =\n-        store.getRegionInfo().getRegionNameAsString() + \"#\" + store.getFamily().getNameAsString();\n+    String compactionName = generateCompactionName();\n     long now = 0;\n     boolean hasMore;\n     ScannerContext scannerContext =",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12491 TableMapReduceUtil.findContainingJar() NPE",
        "commit": "https://github.com/apache/hbase/commit/b94b0e9ca76f5ab2fad648dae052dce85e578628",
        "parent": "https://github.com/apache/hbase/commit/555e78005df2b1f211e013acc5f743df9c7b80ab",
        "bug_id": "hbase_91",
        "file": [
            {
                "sha": "b5149415a5e6abbb9cdfaec75604bd2c6c6056b3",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/b94b0e9ca76f5ab2fad648dae052dce85e578628/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/b94b0e9ca76f5ab2fad648dae052dce85e578628/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "status": "modified",
                "changes": 35,
                "additions": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java?ref=b94b0e9ca76f5ab2fad648dae052dce85e578628",
                "patch": "@@ -900,25 +900,28 @@ private static void updateMap(String jar, Map<String, String> packagedClasses) t\n   private static String findContainingJar(Class<?> my_class, Map<String, String> packagedClasses)\n       throws IOException {\n     ClassLoader loader = my_class.getClassLoader();\n+\n     String class_file = my_class.getName().replaceAll(\"\\\\.\", \"/\") + \".class\";\n \n-    // first search the classpath\n-    for (Enumeration<URL> itr = loader.getResources(class_file); itr.hasMoreElements();) {\n-      URL url = itr.nextElement();\n-      if (\"jar\".equals(url.getProtocol())) {\n-        String toReturn = url.getPath();\n-        if (toReturn.startsWith(\"file:\")) {\n-          toReturn = toReturn.substring(\"file:\".length());\n+    if (loader != null) {\n+      // first search the classpath\n+      for (Enumeration<URL> itr = loader.getResources(class_file); itr.hasMoreElements();) {\n+        URL url = itr.nextElement();\n+        if (\"jar\".equals(url.getProtocol())) {\n+          String toReturn = url.getPath();\n+          if (toReturn.startsWith(\"file:\")) {\n+            toReturn = toReturn.substring(\"file:\".length());\n+          }\n+          // URLDecoder is a misnamed class, since it actually decodes\n+          // x-www-form-urlencoded MIME type rather than actual\n+          // URL encoding (which the file path has). Therefore it would\n+          // decode +s to ' 's which is incorrect (spaces are actually\n+          // either unencoded or encoded as \"%20\"). Replace +s first, so\n+          // that they are kept sacred during the decoding process.\n+          toReturn = toReturn.replaceAll(\"\\\\+\", \"%2B\");\n+          toReturn = URLDecoder.decode(toReturn, \"UTF-8\");\n+          return toReturn.replaceAll(\"!.*$\", \"\");\n         }\n-        // URLDecoder is a misnamed class, since it actually decodes\n-        // x-www-form-urlencoded MIME type rather than actual\n-        // URL encoding (which the file path has). Therefore it would\n-        // decode +s to ' 's which is incorrect (spaces are actually\n-        // either unencoded or encoded as \"%20\"). Replace +s first, so\n-        // that they are kept sacred during the decoding process.\n-        toReturn = toReturn.replaceAll(\"\\\\+\", \"%2B\");\n-        toReturn = URLDecoder.decode(toReturn, \"UTF-8\");\n-        return toReturn.replaceAll(\"!.*$\", \"\");\n       }\n     }\n ",
                "deletions": 16
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10705 CompactionRequest#toString() may throw NullPointerException (Rekha Joshi)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1584476 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/60cac497201c94e74e1a120030bc2822acc2fe7d",
        "parent": "https://github.com/apache/hbase/commit/62908378bc2c699d0ef156e5c07dbdf0c4883193",
        "bug_id": "hbase_92",
        "file": [
            {
                "sha": "5c9a2339c5f2d9d8637273c3a7620d0735628fcf",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java",
                "blob_url": "https://github.com/apache/hbase/blob/60cac497201c94e74e1a120030bc2822acc2fe7d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java",
                "raw_url": "https://github.com/apache/hbase/raw/60cac497201c94e74e1a120030bc2822acc2fe7d/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java",
                "status": "modified",
                "changes": 7,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java?ref=60cac497201c94e74e1a120030bc2822acc2fe7d",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.hbase.regionserver.Store;\n import org.apache.hadoop.hbase.regionserver.StoreFile;\n+import org.apache.hadoop.hbase.regionserver.StoreFile.Reader;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n import org.apache.hadoop.util.StringUtils;\n \n@@ -210,7 +211,8 @@ public boolean apply(StoreFile sf) {\n               }\n           }), new Function<StoreFile, String>() {\n             public String apply(StoreFile sf) {\n-              return StringUtils.humanReadableInt(sf.getReader().length());\n+              return StringUtils.humanReadableInt(\n+                (sf.getReader() == null) ? 0 : sf.getReader().length());\n             }\n           }));\n \n@@ -228,7 +230,8 @@ public String apply(StoreFile sf) {\n   private void recalculateSize() {\n     long sz = 0;\n     for (StoreFile sf : this.filesToCompact) {\n-      sz += sf.getReader().length();\n+      Reader r = sf.getReader();\n+      sz += r == null ? 0 : r.length();\n     }\n     this.totalSize = sz;\n   }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10839 NullPointerException in construction of RegionServer in Security Cluster\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1582140 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a87927acdcc022297fa2ac110f5b29fe8bcad2ea",
        "parent": "https://github.com/apache/hbase/commit/25932c793d5610f01970c9eaf3cc9bc3fe864f12",
        "bug_id": "hbase_93",
        "file": [
            {
                "sha": "46d4223cbcf65a806a2f2f90d97afe33f0273ee0",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/a87927acdcc022297fa2ac110f5b29fe8bcad2ea/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/a87927acdcc022297fa2ac110f5b29fe8bcad2ea/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 13,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=a87927acdcc022297fa2ac110f5b29fe8bcad2ea",
                "patch": "@@ -97,6 +97,7 @@\n import org.apache.hadoop.hbase.regionserver.RSRpcServices;\n import org.apache.hadoop.hbase.regionserver.RegionSplitPolicy;\n import org.apache.hadoop.hbase.replication.regionserver.Replication;\n+import org.apache.hadoop.hbase.security.UserProvider;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.CompressionTest;\n import org.apache.hadoop.hbase.util.FSUtils;\n@@ -272,6 +273,18 @@ public HMaster(final Configuration conf)\n     startActiveMasterManager();\n   }\n \n+  /**\n+   * For compatibility, if failed with regionserver credentials, try the master one\n+   */\n+  protected void login(UserProvider user, String host) throws IOException {\n+    try {\n+      super.login(user, host);\n+    } catch (IOException ie) {\n+      user.login(\"hbase.master.keytab.file\",\n+        \"hbase.master.kerberos.principal\", host);\n+    }\n+  }\n+\n   @VisibleForTesting\n   public MasterRpcServices getMasterRpcServices() {\n     return (MasterRpcServices)rpcServices;",
                "deletions": 0
            },
            {
                "sha": "329d50b73d8927df8c9e803a939c2edc8f863963",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a87927acdcc022297fa2ac110f5b29fe8bcad2ea/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a87927acdcc022297fa2ac110f5b29fe8bcad2ea/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 11,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=a87927acdcc022297fa2ac110f5b29fe8bcad2ea",
                "patch": "@@ -437,10 +437,9 @@ public HRegionServer(Configuration conf)\n     // login the zookeeper client principal (if using security)\n     ZKUtil.loginClient(this.conf, \"hbase.zookeeper.client.keytab.file\",\n       \"hbase.zookeeper.client.kerberos.principal\", hostName);\n-\n     // login the server principal (if using secure Hadoop)\n-    userProvider.login(\"hbase.regionserver.keytab.file\",\n-      \"hbase.regionserver.kerberos.principal\", hostName);\n+    login(userProvider, hostName);\n+\n     regionServerAccounting = new RegionServerAccounting();\n     cacheConfig = new CacheConfig(conf);\n     uncaughtExceptionHandler = new UncaughtExceptionHandler() {\n@@ -484,9 +483,15 @@ public void uncaughtException(Thread t, Throwable e) {\n       catalogTracker.start();\n     }\n \n+    rpcServices.start();\n     putUpWebUI();\n   }\n \n+  protected void login(UserProvider user, String host) throws IOException {\n+    user.login(\"hbase.regionserver.keytab.file\",\n+      \"hbase.regionserver.kerberos.principal\", host);\n+  }\n+\n   protected String getProcessName() {\n     return REGIONSERVER;\n   }",
                "deletions": 3
            },
            {
                "sha": "faf9561248a1ce57bdeee5687743e02f54044836",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/a87927acdcc022297fa2ac110f5b29fe8bcad2ea/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/a87927acdcc022297fa2ac110f5b29fe8bcad2ea/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=a87927acdcc022297fa2ac110f5b29fe8bcad2ea",
                "patch": "@@ -703,7 +703,6 @@ public RSRpcServices(HRegionServer rs) throws IOException {\n       initialIsa, // BindAddress is IP we got for this server.\n       rs.conf,\n       rpcSchedulerFactory.create(rs.conf, this));\n-    rpcServer.start();\n \n     scannerLeaseTimeoutPeriod = rs.conf.getInt(\n       HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD,\n@@ -759,6 +758,10 @@ PriorityFunction getPriority() {\n     return priority;\n   }\n \n+  void start() {\n+    rpcServer.start();\n+  }\n+\n   void stop() {\n     closeAllScanners();\n     rpcServer.stop();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-17879: Avoid NPE in snapshot.jsp when accessing without any request parameter\n\nSigned-off-by: Chia-Ping Tsai <chia7712@gmail.com>",
        "commit": "https://github.com/apache/hbase/commit/1848353fd60b2c51282552e9d0ad284be601cca5",
        "parent": "https://github.com/apache/hbase/commit/6edb8f82178f1da4b29dc4ee1a1d8ea8dd2484a4",
        "bug_id": "hbase_94",
        "file": [
            {
                "sha": "ad3ede5f572cd93c688eb2c2443c24c079ae2ce9",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/1848353fd60b2c51282552e9d0ad284be601cca5/hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/1848353fd60b2c51282552e9d0ad284be601cca5/hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp",
                "status": "modified",
                "changes": 20,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/snapshot.jsp?ref=1848353fd60b2c51282552e9d0ad284be601cca5",
                "patch": "@@ -36,14 +36,16 @@\n   SnapshotInfo.SnapshotStats stats = null;\n   TableName snapshotTable = null;\n   boolean tableExists = false;\n-  try (Admin admin = master.getConnection().getAdmin()) {\n-    for (SnapshotDescription snapshotDesc: admin.listSnapshots()) {\n-      if (snapshotName.equals(snapshotDesc.getName())) {\n-        snapshot = snapshotDesc;\n-        stats = SnapshotInfo.getSnapshotStats(conf, snapshot);\n-        snapshotTable = snapshot.getTableName();\n-        tableExists = admin.tableExists(snapshotTable);\n-        break;\n+  if(snapshotName != null) {\n+    try (Admin admin = master.getConnection().getAdmin()) {\n+      for (SnapshotDescription snapshotDesc: admin.listSnapshots()) {\n+        if (snapshotName.equals(snapshotDesc.getName())) {\n+          snapshot = snapshotDesc;\n+          stats = SnapshotInfo.getSnapshotStats(conf, snapshot);\n+          snapshotTable = snapshot.getTableName();\n+          tableExists = admin.tableExists(snapshotTable);\n+          break;\n+        }\n       }\n     }\n   }\n@@ -110,7 +112,7 @@\n   <div class=\"container-fluid content\">\n   <div class=\"row inner_header\">\n     <div class=\"page-header\">\n-      <h1>Snapshot \"<%= snapshotName %>\" does not exists</h1>\n+      <h1>Snapshot \"<%= snapshotName %>\" does not exist</h1>\n     </div>\n   </div>\n   <p>Go <a href=\"javascript:history.back()\">Back</a>, or wait for the redirect.",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12206 NPE in RSRpcServices",
        "commit": "https://github.com/apache/hbase/commit/7aa3a2d890359f41af773815ccd11b6b3640138b",
        "parent": "https://github.com/apache/hbase/commit/652b81ab1ea178dc5ea4d2b4a7675f69decad430",
        "bug_id": "hbase_95",
        "file": [
            {
                "sha": "804ff182b1bb3c51f4f7bed53b7b694dea62731e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/7aa3a2d890359f41af773815ccd11b6b3640138b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/7aa3a2d890359f41af773815ccd11b6b3640138b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "status": "modified",
                "changes": 13,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=7aa3a2d890359f41af773815ccd11b6b3640138b",
                "patch": "@@ -1953,11 +1953,14 @@ public ScanResponse scan(final RpcController controller, final ScanRequest reque\n         // If checkOpen failed, server not running or filesystem gone,\n         // cancel this lease; filesystem is gone or we're closing or something.\n         if (scannerName != null) {\n-          try {\n-            regionServer.leases.cancelLease(scannerName);\n-          } catch (LeaseException le) {\n-            LOG.info(\"Server shutting down and client tried to access missing scanner \" +\n-              scannerName);\n+          LOG.debug(\"Server shutting down and client tried to access missing scanner \"\n+            + scannerName);\n+          if (regionServer.leases != null) {\n+            try {\n+              regionServer.leases.cancelLease(scannerName);\n+            } catch (LeaseException le) {\n+              // No problem, ignore\n+            }\n           }\n         }\n         throw e;",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12184 ServerShutdownHandler throws NPE",
        "commit": "https://github.com/apache/hbase/commit/8c3697b0d8a5f9f5f2403bdf975f765a88da93d2",
        "parent": "https://github.com/apache/hbase/commit/062adcc1885ff58b8e9c55a870a34be8de7a685d",
        "bug_id": "hbase_96",
        "file": [
            {
                "sha": "1691c9df9dadbc24246b813d26ef9ddf69f7448c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/8c3697b0d8a5f9f5f2403bdf975f765a88da93d2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/8c3697b0d8a5f9f5f2403bdf975f765a88da93d2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java?ref=8c3697b0d8a5f9f5f2403bdf975f765a88da93d2",
                "patch": "@@ -188,7 +188,7 @@ public void process() throws IOException {\n             mfs.prepareLogReplay(serverName, hris);\n           } else {\n             LOG.info(\"Splitting logs for \" + serverName +\n-              \" before assignment; region count=\" + hris.size());\n+              \" before assignment; region count=\" + (hris == null ? 0 : hris.size()));\n             this.services.getMasterFileSystem().splitLog(serverName);\n           }\n           am.getRegionStates().logSplit(serverName);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12167 NPE in AssignmentManager",
        "commit": "https://github.com/apache/hbase/commit/5375ff07bcb6451e45c09f23f010a4d051968896",
        "parent": "https://github.com/apache/hbase/commit/d8a7b67d798ab5fec399d4a0b97a025d5bff531c",
        "bug_id": "hbase_97",
        "file": [
            {
                "sha": "77c32c439177ac460afbda679f7c911edf6fd83d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "patch": "@@ -1377,6 +1377,9 @@ public void assign(Map<HRegionInfo, ServerName> regions)\n     // Reuse existing assignment info\n     Map<ServerName, List<HRegionInfo>> bulkPlan =\n       balancer.retainAssignment(regions, servers);\n+    if (bulkPlan == null) {\n+      throw new IOException(\"Unable to determine a plan to assign region(s)\");\n+    }\n \n     assign(regions.size(), servers.size(),\n       \"retainAssignment=true\", bulkPlan);\n@@ -1404,8 +1407,11 @@ public void assign(List<HRegionInfo> regions)\n     // Generate a round-robin bulk assignment plan\n     Map<ServerName, List<HRegionInfo>> bulkPlan\n       = balancer.roundRobinAssignment(regions, servers);\n-    processFavoredNodes(regions);\n+    if (bulkPlan == null) {\n+      throw new IOException(\"Unable to determine a plan to assign region(s)\");\n+    }\n \n+    processFavoredNodes(regions);\n     assign(regions.size(), servers.size(),\n       \"round-robin=true\", bulkPlan);\n   }",
                "deletions": 1
            },
            {
                "sha": "eef0a09f3d0e54b1d239562e1d5ee8c7434f5b72",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 19,
                "additions": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "patch": "@@ -1079,15 +1079,30 @@ void move(final byte[] encodedRegionName,\n       final List<ServerName> destServers = this.serverManager.createDestinationServersList(\n         regionState.getServerName());\n       dest = balancer.randomAssignment(hri, destServers);\n+      if (dest == null) {\n+        LOG.debug(\"Unable to determine a plan to assign \" + hri);\n+        return;\n+      }\n     } else {\n       dest = ServerName.valueOf(Bytes.toString(destServerName));\n-      if (dest.equals(regionState.getServerName())) {\n+      if (dest.equals(serverName) && balancer instanceof BaseLoadBalancer\n+          && !((BaseLoadBalancer)balancer).shouldBeOnMaster(hri)) {\n+        // To avoid unnecessary region moving later by balancer. Don't put user\n+        // regions on master. Regions on master could be put on other region\n+        // server intentionally by test however.\n         LOG.debug(\"Skipping move of region \" + hri.getRegionNameAsString()\n-          + \" because region already assigned to the same server \" + dest + \".\");\n+          + \" to avoid unnecessary region moving later by load balancer,\"\n+          + \" because it should not be on master\");\n         return;\n       }\n     }\n \n+    if (dest.equals(regionState.getServerName())) {\n+      LOG.debug(\"Skipping move of region \" + hri.getRegionNameAsString()\n+        + \" because region already assigned to the same server \" + dest + \".\");\n+      return;\n+    }\n+\n     // Now we can do the move\n     RegionPlan rp = new RegionPlan(hri, regionState.getServerName(), dest);\n ",
                "deletions": 2
            },
            {
                "sha": "a5e110b6d0018255ba9e82e11e956d9693d14129",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/5375ff07bcb6451e45c09f23f010a4d051968896/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=5375ff07bcb6451e45c09f23f010a4d051968896",
                "patch": "@@ -858,7 +858,7 @@ protected void setSlop(Configuration conf) {\n    * Check if a region belongs to some small system table.\n    * If so, it may be expected to be put on the master regionserver.\n    */\n-  protected boolean shouldBeOnMaster(HRegionInfo region) {\n+  public boolean shouldBeOnMaster(HRegionInfo region) {\n     return tablesOnMaster.contains(region.getTable().getNameAsString());\n   }\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-11880 NPE in MasterStatusServlet",
        "commit": "https://github.com/apache/hbase/commit/842f4808db3a4951f43d958f7933e12ca9bd4830",
        "parent": "https://github.com/apache/hbase/commit/211c1e8ad439f2dfbbd2f07ce099c0f948e06df3",
        "bug_id": "hbase_98",
        "file": [
            {
                "sha": "714b5a8aede574f53bc0ce160f540b6c6347820b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=842f4808db3a4951f43d958f7933e12ca9bd4830",
                "patch": "@@ -480,8 +480,12 @@ private void finishActiveMasterInitialization(MonitoredTask status)\n     ZKClusterId.setClusterId(this.zooKeeper, fileSystemManager.getClusterId());\n     this.serverManager = createServerManager(this, this);\n \n-    metaTableLocator = new MetaTableLocator();\n-    shortCircuitConnection = createShortCircuitConnection();\n+    synchronized (this) {\n+      if (shortCircuitConnection == null) {\n+        shortCircuitConnection = createShortCircuitConnection();\n+        metaTableLocator = new MetaTableLocator();\n+      }\n+    }\n \n     // Invalidate all write locks held previously\n     this.tableLockManager.reapWriteLocks();",
                "deletions": 2
            },
            {
                "sha": "3059096380943f801394cf1a88e2e8aacfcbbf81",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java?ref=842f4808db3a4951f43d958f7933e12ca9bd4830",
                "patch": "@@ -27,8 +27,6 @@\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n \n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.ServerName;\n@@ -43,7 +41,6 @@\n  */\n @InterfaceAudience.Private\n public class MasterStatusServlet extends HttpServlet {\n-  private static final Log LOG = LogFactory.getLog(MasterStatusServlet.class);\n   private static final long serialVersionUID = 1L;\n \n   @Override\n@@ -88,7 +85,9 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n   }\n \n   private ServerName getMetaLocationOrNull(HMaster master) {\n-    return master.getMetaTableLocator().getMetaRegionLocation(master.getZooKeeper());\n+    MetaTableLocator metaTableLocator = master.getMetaTableLocator();\n+    return metaTableLocator == null ? null :\n+      metaTableLocator.getMetaRegionLocation(master.getZooKeeper());\n   }\n \n   private Map<String, Integer> getFragmentationInfo(",
                "deletions": 4
            },
            {
                "sha": "7947c4a6c55c6942df6642fe31efdbde958579b1",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/842f4808db3a4951f43d958f7933e12ca9bd4830/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=842f4808db3a4951f43d958f7933e12ca9bd4830",
                "patch": "@@ -637,8 +637,12 @@ private void initializeZooKeeper() throws IOException, InterruptedException {\n       this.abort(\"Failed to retrieve Cluster ID\",e);\n     }\n \n-    shortCircuitConnection = createShortCircuitConnection();\n-    metaTableLocator = new MetaTableLocator();\n+    synchronized (this) {\n+      if (shortCircuitConnection == null) {\n+        shortCircuitConnection = createShortCircuitConnection();\n+        metaTableLocator = new MetaTableLocator();\n+      }\n+    }\n \n     // watch for snapshots and other procedures\n     try {",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9952 Snapshot restore may fail due to NullPointerException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1540909 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6ec64a1dc089ca868ab06748c223ca2bf9c6deff",
        "parent": "https://github.com/apache/hbase/commit/26ddfe10b629dd300021a1a41e0c2118eb9da496",
        "bug_id": "hbase_99",
        "file": [
            {
                "sha": "2e970a169a5f1ec3a09d0fb5fea06e4c657247ce",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/6ec64a1dc089ca868ab06748c223ca2bf9c6deff/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/6ec64a1dc089ca868ab06748c223ca2bf9c6deff/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java?ref=6ec64a1dc089ca868ab06748c223ca2bf9c6deff",
                "patch": "@@ -157,7 +157,9 @@ protected void handleTableOperation(List<HRegionInfo> hris) throws IOException {\n       hris.clear();\n       if (metaChanges.hasRegionsToAdd()) hris.addAll(metaChanges.getRegionsToAdd());\n       MetaEditor.addRegionsToMeta(catalogTracker, hris);\n-      MetaEditor.overwriteRegions(catalogTracker, metaChanges.getRegionsToRestore());\n+      if (metaChanges.hasRegionsToRestore()) {\n+        MetaEditor.overwriteRegions(catalogTracker, metaChanges.getRegionsToRestore());\n+      }\n       metaChanges.updateMetaParentRegions(catalogTracker, hris);\n \n       // At this point the restore is complete. Next step is enabling the table.",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13314 Fix NPE in HMaster.getClusterStatus()",
        "commit": "https://github.com/apache/hbase/commit/80d230e1fbeac24d3dfdac8165e24f35ec26f988",
        "parent": "https://github.com/apache/hbase/commit/5459008ccdc83f09e6d8d7ec90144dc120e1a084",
        "bug_id": "hbase_100",
        "file": [
            {
                "sha": "75fa642bb5971bd7d0d6b9489096a4075d1a50e6",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java",
                "blob_url": "https://github.com/apache/hbase/blob/80d230e1fbeac24d3dfdac8165e24f35ec26f988/hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java",
                "raw_url": "https://github.com/apache/hbase/raw/80d230e1fbeac24d3dfdac8165e24f35ec26f988/hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java",
                "status": "modified",
                "changes": 67,
                "additions": 47,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java?ref=80d230e1fbeac24d3dfdac8165e24f35ec26f988",
                "patch": "@@ -118,21 +118,24 @@ public ClusterStatus(final String hbaseVersion, final String clusterid,\n    * @return the names of region servers on the dead list\n    */\n   public Collection<ServerName> getDeadServerNames() {\n+    if (deadServers == null) {\n+      return Collections.<ServerName>emptyList();\n+    }\n     return Collections.unmodifiableCollection(deadServers);\n   }\n \n   /**\n    * @return the number of region servers in the cluster\n    */\n   public int getServersSize() {\n-    return liveServers.size();\n+    return liveServers != null ? liveServers.size() : 0;\n   }\n \n   /**\n    * @return the number of dead region servers in the cluster\n    */\n   public int getDeadServers() {\n-    return deadServers.size();\n+    return deadServers != null ? deadServers.size() : 0;\n   }\n \n   /**\n@@ -148,8 +151,10 @@ public double getAverageLoad() {\n    */\n   public int getRegionsCount() {\n     int count = 0;\n-    for (Map.Entry<ServerName, ServerLoad> e: this.liveServers.entrySet()) {\n-      count += e.getValue().getNumberOfRegions();\n+    if (liveServers != null && !liveServers.isEmpty()) {\n+      for (Map.Entry<ServerName, ServerLoad> e: this.liveServers.entrySet()) {\n+        count += e.getValue().getNumberOfRegions();\n+      }\n     }\n     return count;\n   }\n@@ -159,8 +164,10 @@ public int getRegionsCount() {\n    */\n   public int getRequestsCount() {\n     int count = 0;\n-    for (Map.Entry<ServerName, ServerLoad> e: this.liveServers.entrySet()) {\n-      count += e.getValue().getNumberOfRequests();\n+    if (liveServers != null && !liveServers.isEmpty()) {\n+      for (Map.Entry<ServerName, ServerLoad> e: this.liveServers.entrySet()) {\n+        count += e.getValue().getNumberOfRequests();\n+      }\n     }\n     return count;\n   }\n@@ -222,6 +229,9 @@ public byte getVersion() {\n   }\n \n   public Collection<ServerName> getServers() {\n+    if (liveServers == null) {\n+      return Collections.<ServerName>emptyList();\n+    }\n     return Collections.unmodifiableCollection(this.liveServers.keySet());\n   }\n \n@@ -237,13 +247,16 @@ public ServerName getMaster() {\n    * @return the number of backup masters in the cluster\n    */\n   public int getBackupMastersSize() {\n-    return this.backupMasters.size();\n+    return backupMasters != null ? backupMasters.size() : 0;\n   }\n \n   /**\n    * @return the names of backup masters\n    */\n   public Collection<ServerName> getBackupMasters() {\n+    if (backupMasters == null) {\n+      return Collections.<ServerName>emptyList();\n+    }\n     return Collections.unmodifiableCollection(this.backupMasters);\n   }\n \n@@ -252,7 +265,7 @@ public int getBackupMastersSize() {\n    * @return Server's load or null if not found.\n    */\n   public ServerLoad getLoad(final ServerName sn) {\n-    return this.liveServers.get(sn);\n+    return liveServers != null ? liveServers.get(sn) : null;\n   }\n \n   @InterfaceAudience.Private\n@@ -303,27 +316,41 @@ public Boolean getBalancerOn() {\n   public String toString() {\n     StringBuilder sb = new StringBuilder(1024);\n     sb.append(\"Master: \" + master);\n-    sb.append(\"\\nNumber of backup masters: \" + backupMasters.size());\n-    for (ServerName serverName: backupMasters) {\n-      sb.append(\"\\n  \" + serverName);\n+\n+    int backupMastersSize = getBackupMastersSize();\n+    sb.append(\"\\nNumber of backup masters: \" + backupMastersSize);\n+    if (backupMastersSize > 0) {\n+      for (ServerName serverName: backupMasters) {\n+        sb.append(\"\\n  \" + serverName);\n+      }\n     }\n \n-    sb.append(\"\\nNumber of live region servers: \" + liveServers.size());\n-    for (ServerName serverName: liveServers.keySet()) {\n-      sb.append(\"\\n  \" + serverName.getServerName());\n+    int serversSize = getServersSize();\n+    sb.append(\"\\nNumber of live region servers: \" + serversSize);\n+    if (serversSize > 0) {\n+      for (ServerName serverName: liveServers.keySet()) {\n+        sb.append(\"\\n  \" + serverName.getServerName());\n+      }\n     }\n \n-    sb.append(\"\\nNumber of dead region servers: \" + deadServers.size());\n-    for (ServerName serverName: deadServers) {\n-      sb.append(\"\\n  \" + serverName);\n+    int deadServerSize = getDeadServers();\n+    sb.append(\"\\nNumber of dead region servers: \" + deadServerSize);\n+    if (deadServerSize > 0) {\n+      for (ServerName serverName: deadServers) {\n+        sb.append(\"\\n  \" + serverName);\n+      }\n     }\n \n     sb.append(\"\\nAverage load: \" + getAverageLoad());\n     sb.append(\"\\nNumber of requests: \" + getRequestsCount());\n     sb.append(\"\\nNumber of regions: \" + getRegionsCount());\n-    sb.append(\"\\nNumber of regions in transition: \" + intransition.size());\n-    for (RegionState state: intransition.values()) {\n-      sb.append(\"\\n  \" + state.toDescriptiveString());\n+\n+    int ritSize = (intransition != null) ? intransition.size() : 0;\n+    sb.append(\"\\nNumber of regions in transition: \" + ritSize);\n+    if (ritSize > 0) {\n+      for (RegionState state: intransition.values()) {\n+        sb.append(\"\\n  \" + state.toDescriptiveString());\n+      }\n     }\n     return sb.toString();\n   }",
                "deletions": 20
            },
            {
                "sha": "de9e9c6bd55963b2e789398a545bfcafdb86dfba",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/80d230e1fbeac24d3dfdac8165e24f35ec26f988/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/80d230e1fbeac24d3dfdac8165e24f35ec26f988/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 55,
                "additions": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=80d230e1fbeac24d3dfdac8165e24f35ec26f988",
                "patch": "@@ -1721,39 +1721,42 @@ public ClusterStatus getClusterStatus() throws InterruptedIOException {\n         this.zooKeeper.backupMasterAddressesZNode);\n     } catch (KeeperException e) {\n       LOG.warn(this.zooKeeper.prefix(\"Unable to list backup servers\"), e);\n-      backupMasterStrings = new ArrayList<String>(0);\n+      backupMasterStrings = null;\n     }\n-    List<ServerName> backupMasters = new ArrayList<ServerName>(\n-                                          backupMasterStrings.size());\n-    for (String s: backupMasterStrings) {\n-      try {\n-        byte [] bytes;\n+\n+    List<ServerName> backupMasters = null;\n+    if (backupMasterStrings != null && !backupMasterStrings.isEmpty()) {\n+      backupMasters = new ArrayList<ServerName>(backupMasterStrings.size());\n+      for (String s: backupMasterStrings) {\n         try {\n-          bytes = ZKUtil.getData(this.zooKeeper, ZKUtil.joinZNode(\n-              this.zooKeeper.backupMasterAddressesZNode, s));\n-        } catch (InterruptedException e) {\n-          throw new InterruptedIOException();\n-        }\n-        if (bytes != null) {\n-          ServerName sn;\n+          byte [] bytes;\n           try {\n-            sn = ServerName.parseFrom(bytes);\n-          } catch (DeserializationException e) {\n-            LOG.warn(\"Failed parse, skipping registering backup server\", e);\n-            continue;\n+            bytes = ZKUtil.getData(this.zooKeeper, ZKUtil.joinZNode(\n+                this.zooKeeper.backupMasterAddressesZNode, s));\n+          } catch (InterruptedException e) {\n+            throw new InterruptedIOException();\n+          }\n+          if (bytes != null) {\n+            ServerName sn;\n+            try {\n+              sn = ServerName.parseFrom(bytes);\n+            } catch (DeserializationException e) {\n+              LOG.warn(\"Failed parse, skipping registering backup server\", e);\n+              continue;\n+            }\n+            backupMasters.add(sn);\n           }\n-          backupMasters.add(sn);\n+        } catch (KeeperException e) {\n+          LOG.warn(this.zooKeeper.prefix(\"Unable to get information about \" +\n+                   \"backup servers\"), e);\n         }\n-      } catch (KeeperException e) {\n-        LOG.warn(this.zooKeeper.prefix(\"Unable to get information about \" +\n-                 \"backup servers\"), e);\n       }\n+      Collections.sort(backupMasters, new Comparator<ServerName>() {\n+        @Override\n+        public int compare(ServerName s1, ServerName s2) {\n+          return s1.getServerName().compareTo(s2.getServerName());\n+        }});\n     }\n-    Collections.sort(backupMasters, new Comparator<ServerName>() {\n-      @Override\n-      public int compare(ServerName s1, ServerName s2) {\n-        return s1.getServerName().compareTo(s2.getServerName());\n-      }});\n \n     String clusterId = fileSystemManager != null ?\n       fileSystemManager.getClusterId().toString() : null;",
                "deletions": 26
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9860 Intermittent TestHBaseFsck#testMissingRegionInfoQualifier failure due to NullPointerException\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1537371 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/94aa409410ef21f107e67c5c7fa5931570d8c941",
        "parent": "https://github.com/apache/hbase/commit/0077c822f719ea1d90350503f3d10a274631aebd",
        "bug_id": "hbase_101",
        "file": [
            {
                "sha": "2b200dbe374c545ef01c0f2f0b8b206a479f453b",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java",
                "blob_url": "https://github.com/apache/hbase/blob/94aa409410ef21f107e67c5c7fa5931570d8c941/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java",
                "raw_url": "https://github.com/apache/hbase/raw/94aa409410ef21f107e67c5c7fa5931570d8c941/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java?ref=94aa409410ef21f107e67c5c7fa5931570d8c941",
                "patch": "@@ -1888,7 +1888,8 @@ public void testMissingRegionInfoQualifier() throws Exception {\n \n         @Override\n         public boolean processRow(Result rowResult) throws IOException {\n-          if(!MetaScanner.getHRegionInfo(rowResult).getTable().isSystemTable()) {\n+          HRegionInfo hri = MetaScanner.getHRegionInfo(rowResult);\n+          if (hri != null && !hri.getTable().isSystemTable()) {\n             Delete delete = new Delete(rowResult.getRow());\n             delete.deleteColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n             deletes.add(delete);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13152 NPE in ExpiredMobFileCleanerChore (Jingcheng Du)",
        "commit": "https://github.com/apache/hbase/commit/d55405a7cee5747af294c0784e02dc41ef379521",
        "parent": "https://github.com/apache/hbase/commit/a8d0dfe71c9e508969ee88e4999d585e704b375d",
        "bug_id": "hbase_102",
        "file": [
            {
                "sha": "a9e977379ce9e47ee7ef4582ea80882c73a6fcdd",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java",
                "blob_url": "https://github.com/apache/hbase/blob/d55405a7cee5747af294c0784e02dc41ef379521/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java",
                "raw_url": "https://github.com/apache/hbase/raw/d55405a7cee5747af294c0784e02dc41ef379521/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java?ref=d55405a7cee5747af294c0784e02dc41ef379521",
                "patch": "@@ -53,6 +53,7 @@ public ExpiredMobFileCleanerChore(HMaster master) {\n     this.master = master;\n     this.tableLockManager = master.getTableLockManager();\n     cleaner = new ExpiredMobFileCleaner();\n+    cleaner.setConf(master.getConfiguration());\n   }\n \n   @Override",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16201 NPE in RpcServer causing intermittent UT failure of TestMasterReplication#testHFileCyclicReplication",
        "commit": "https://github.com/apache/hbase/commit/3c39cbd92c3f309c98ca01bfb70ca89bc046a228",
        "parent": "https://github.com/apache/hbase/commit/1578a045b6af6e8cd4c85d7b43397e204345a99d",
        "bug_id": "hbase_103",
        "file": [
            {
                "sha": "73226aa4c762e39c13f61eb4c93868a40a4ebf10",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/3c39cbd92c3f309c98ca01bfb70ca89bc046a228/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/3c39cbd92c3f309c98ca01bfb70ca89bc046a228/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=3c39cbd92c3f309c98ca01bfb70ca89bc046a228",
                "patch": "@@ -2246,7 +2246,13 @@ public void setSecretManager(SecretManager<? extends TokenIdentifier> secretMana\n       // The above callBlockingMethod will always return a SE.  Strip the SE wrapper before\n       // putting it on the wire.  Its needed to adhere to the pb Service Interface but we don't\n       // need to pass it over the wire.\n-      if (e instanceof ServiceException) e = e.getCause();\n+      if (e instanceof ServiceException) {\n+        if (e.getCause() == null) {\n+          LOG.debug(\"Caught a ServiceException with null cause\", e);\n+        } else {\n+          e = e.getCause();\n+        }\n+      }\n \n       // increment the number of requests that were exceptions.\n       metrics.exception(e);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"NPE in RpcServer causing intermittent UT failure of TestMasterReplication#testHFileCyclicReplication\"\n\nThis reverts commit a33097e067b73be8e877b822afa90b89a0c7974f.",
        "commit": "https://github.com/apache/hbase/commit/1578a045b6af6e8cd4c85d7b43397e204345a99d",
        "parent": "https://github.com/apache/hbase/commit/5e0d97e0e95434b03ba886279180a031ed997dfd",
        "bug_id": "hbase_104",
        "file": [
            {
                "sha": "ad88e8e5903dd298a0d5f6b869fdc2c4950cdd4c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/1578a045b6af6e8cd4c85d7b43397e204345a99d/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/1578a045b6af6e8cd4c85d7b43397e204345a99d/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 8,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=1578a045b6af6e8cd4c85d7b43397e204345a99d",
                "patch": "@@ -2246,13 +2246,7 @@ public void setSecretManager(SecretManager<? extends TokenIdentifier> secretMana\n       // The above callBlockingMethod will always return a SE.  Strip the SE wrapper before\n       // putting it on the wire.  Its needed to adhere to the pb Service Interface but we don't\n       // need to pass it over the wire.\n-      if (e instanceof ServiceException) {\n-        if (e.getCause() == null) {\n-          LOG.debug(\"Caught a ServiceException with null cause\", e);\n-        } else {\n-          e = e.getCause();\n-        }\n-      }\n+      if (e instanceof ServiceException) e = e.getCause();\n \n       // increment the number of requests that were exceptions.\n       metrics.exception(e);",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9281 user_permission command encounters NullPointerException (Ted Yu)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1516074 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/f8b1a13161635e8d7bdf2c80011cc212c13479f0",
        "parent": "https://github.com/apache/hbase/commit/a3c3c54bae4d8c13b37c5e516fdf07ebdcd01814",
        "bug_id": "hbase_105",
        "file": [
            {
                "sha": "4974eeb1c76a8d2564b0f456ced1fb46dfc79a84",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java",
                "blob_url": "https://github.com/apache/hbase/blob/f8b1a13161635e8d7bdf2c80011cc212c13479f0/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java",
                "raw_url": "https://github.com/apache/hbase/raw/f8b1a13161635e8d7bdf2c80011cc212c13479f0/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java?ref=f8b1a13161635e8d7bdf2c80011cc212c13479f0",
                "patch": "@@ -482,7 +482,7 @@ static boolean isAclTable(HTableDescriptor desc) {\n    */\n   static List<UserPermission> getUserTablePermissions(\n       Configuration conf, TableName tableName) throws IOException {\n-    return getUserPermissions(conf, tableName.getName());\n+    return getUserPermissions(conf, tableName == null ? null : tableName.getName());\n   }\n \n   static List<UserPermission> getUserNamespacePermissions(",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14662 Fix NPE in HFileOutputFormat2 (Heng Chen)",
        "commit": "https://github.com/apache/hbase/commit/81e2aba1f9bbe592d9d9284e701af5bedad3cd56",
        "parent": "https://github.com/apache/hbase/commit/5363f371bb7cd83902e8027b221fe7bf690f9eec",
        "bug_id": "hbase_106",
        "file": [
            {
                "sha": "a18377305f6d9feab9fcf2c30c1c86eb904284ea",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "blob_url": "https://github.com/apache/hbase/blob/81e2aba1f9bbe592d9d9284e701af5bedad3cd56/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "raw_url": "https://github.com/apache/hbase/raw/81e2aba1f9bbe592d9d9284e701af5bedad3cd56/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java",
                "status": "modified",
                "changes": 19,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat2.java?ref=81e2aba1f9bbe592d9d9284e701af5bedad3cd56",
                "patch": "@@ -210,15 +210,16 @@ public void write(ImmutableBytesWritable row, V cell)\n           if (conf.getBoolean(LOCALITY_SENSITIVE_CONF_KEY, DEFAULT_LOCALITY_SENSITIVE)) {\n             HRegionLocation loc = null;\n             String tableName = conf.get(OUTPUT_TABLE_NAME_CONF_KEY);\n-\n-            try (Connection connection = ConnectionFactory.createConnection(conf);\n-                   RegionLocator locator =\n-                     connection.getRegionLocator(TableName.valueOf(tableName))) {\n-              loc = locator.getRegionLocation(rowKey);\n-            } catch (Throwable e) {\n-              LOG.warn(\"there's something wrong when locating rowkey: \" +\n-                Bytes.toString(rowKey), e);\n-              loc = null;\n+            if (tableName != null) {\n+              try (Connection connection = ConnectionFactory.createConnection(conf);\n+                     RegionLocator locator =\n+                       connection.getRegionLocator(TableName.valueOf(tableName))) {\n+                loc = locator.getRegionLocation(rowKey);\n+              } catch (Throwable e) {\n+                LOG.warn(\"there's something wrong when locating rowkey: \" +\n+                  Bytes.toString(rowKey), e);\n+                loc = null;\n+              }\n             }\n \n             if (null == loc) {",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9048 HCM throws NullPointerException under load\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1507502 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/4d7c8a8457b75770b7c76ec37c21d173cf0edcae",
        "parent": "https://github.com/apache/hbase/commit/3812294456ff9a8672a7e2368d466a7119ff4959",
        "bug_id": "hbase_107",
        "file": [
            {
                "sha": "e0ccd154150b437ee02446f1e0fef58fb3bf4249",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/4d7c8a8457b75770b7c76ec37c21d173cf0edcae/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/4d7c8a8457b75770b7c76ec37c21d173cf0edcae/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=4d7c8a8457b75770b7c76ec37c21d173cf0edcae",
                "patch": "@@ -1085,7 +1085,9 @@ public void clearCaches(final ServerName serverName){\n         for (Map<byte[], HRegionLocation> tableLocations :\n             cachedRegionLocations.values()) {\n           for (Entry<byte[], HRegionLocation> e : tableLocations.entrySet()) {\n-            if (serverName.equals(e.getValue().getServerName())) {\n+            HRegionLocation value = e.getValue();\n+            if (value != null\n+                && serverName.equals(value.getServerName())) {\n               tableLocations.remove(e.getKey());\n               deletedSomething = true;\n             }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8794 DependentColumnFilter.toString() throws NullPointerException (Stefan Seelmann)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1498536 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f",
        "parent": "https://github.com/apache/hbase/commit/a7655cccf27d54c9409ff42768111b9ebecd55c1",
        "bug_id": "hbase_108",
        "file": [
            {
                "sha": "38a8bbe0f5f9a506363eed47e1f52fac4a1ca0cb",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java",
                "blob_url": "https://github.com/apache/hbase/blob/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java",
                "raw_url": "https://github.com/apache/hbase/raw/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java?ref=dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f",
                "patch": "@@ -285,6 +285,6 @@ public String toString() {\n         Bytes.toStringBinary(this.columnQualifier),\n         this.dropDependentColumn,\n         this.compareOp.name(),\n-        Bytes.toStringBinary(this.comparator.getValue()));\n+        this.comparator != null ? Bytes.toStringBinary(this.comparator.getValue()) : \"null\");\n   }\n }",
                "deletions": 1
            },
            {
                "sha": "277cff0e583ff3c0f3d78961fcd28919f2cb2f76",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java",
                "blob_url": "https://github.com/apache/hbase/blob/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java",
                "raw_url": "https://github.com/apache/hbase/raw/dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java",
                "status": "modified",
                "changes": 28,
                "additions": 28,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java?ref=dfc29dcabfd561f45ca4ab1b7bb15b1a5c7b815f",
                "patch": "@@ -39,6 +39,7 @@\n import org.junit.Before;\n import org.junit.Test;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n import org.junit.experimental.categories.Category;\n@@ -247,5 +248,32 @@ public void testFilterDropping() throws Exception {\n       assertEquals(\"check cell retention\", 2, accepted.size());\n   }\n \n+  /**\n+   * Test for HBASE-8794. Avoid NullPointerException in DependentColumnFilter.toString().\n+   */\n+  @Test\n+  public void testToStringWithNullComparator() {\n+    // Test constructor that implicitly sets a null comparator\n+    Filter filter = new DependentColumnFilter(FAMILIES[0], QUALIFIER);\n+    assertNotNull(filter.toString());\n+    assertTrue(\"check string contains 'null' as compatator is null\",\n+      filter.toString().contains(\"null\"));\n+\n+    // Test constructor with explicit null comparator\n+    filter = new DependentColumnFilter(FAMILIES[0], QUALIFIER, true, CompareOp.EQUAL, null);\n+    assertNotNull(filter.toString());\n+    assertTrue(\"check string contains 'null' as compatator is null\",\n+      filter.toString().contains(\"null\"));\n+  }\n+\n+  @Test\n+  public void testToStringWithNonNullComparator() {\n+    Filter filter =\n+        new DependentColumnFilter(FAMILIES[0], QUALIFIER, true, CompareOp.EQUAL,\n+            new BinaryComparator(MATCH_VAL));\n+    assertNotNull(filter.toString());\n+    assertTrue(\"check string contains comparator value\", filter.toString().contains(\"match\"));\n+  }\n+\n }\n ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8790 NullPointerException thrown when stopping regionserver (Liang Xie)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1497171 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/c5eea46314e21e09327218721e74ec0a803984f7",
        "parent": "https://github.com/apache/hbase/commit/0d98bb4d2b7cdf6681a241f23bf5cde915e0f0cb",
        "bug_id": "hbase_109",
        "file": [
            {
                "sha": "5b9e27b9f7b4d11ebc51407089105f286400166a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/c5eea46314e21e09327218721e74ec0a803984f7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/c5eea46314e21e09327218721e74ec0a803984f7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=c5eea46314e21e09327218721e74ec0a803984f7",
                "patch": "@@ -978,6 +978,10 @@ private boolean areAllUserRegionsOffline() {\n \n   void tryRegionServerReport(long reportStartTime, long reportEndTime)\n   throws IOException {\n+    if (this.rssStub == null) {\n+      // the current server is stopping.\n+      return;\n+    }\n     ClusterStatusProtos.ServerLoad sl = buildServerLoad(reportStartTime, reportEndTime);\n     try {\n       RegionServerReportRequest.Builder request = RegionServerReportRequest.newBuilder();",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-19939 Fixed NPE in tests TestSplitTableRegionProcedure#testSplitWithoutPONR() and testRecoveryAndDoubleExecution()\n\nValue of 'htd' is null as it is initialized in the constructor but when the object is deserialized its null. Got rid of member variable htd and made it local to method.",
        "commit": "https://github.com/apache/hbase/commit/3bb8daa60565ec2f7955352e52c2f6379176d8c6",
        "parent": "https://github.com/apache/hbase/commit/f5197979aaac7e36b6af36b86ea8dc8d7774fabe",
        "bug_id": "hbase_110",
        "file": [
            {
                "sha": "be0741d87305803f6448fd68ecfc8ff786682b83",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/3bb8daa60565ec2f7955352e52c2f6379176d8c6/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/3bb8daa60565ec2f7955352e52c2f6379176d8c6/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java",
                "status": "modified",
                "changes": 8,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java?ref=3bb8daa60565ec2f7955352e52c2f6379176d8c6",
                "patch": "@@ -94,7 +94,6 @@\n   private RegionInfo daughter_1_RI;\n   private RegionInfo daughter_2_RI;\n   private byte[] bestSplitRow;\n-  private TableDescriptor htd;\n   private RegionSplitPolicy splitPolicy;\n \n   public SplitTableRegionProcedure() {\n@@ -120,14 +119,14 @@ public SplitTableRegionProcedure(final MasterProcedureEnv env,\n         .setSplit(false)\n         .setRegionId(rid)\n         .build();\n-    this.htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n-    if(this.htd.getRegionSplitPolicyClassName() != null) {\n+    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n+    if(htd.getRegionSplitPolicyClassName() != null) {\n       // Since we don't have region reference here, creating the split policy instance without it.\n       // This can be used to invoke methods which don't require Region reference. This instantiation\n       // of a class on Master-side though it only makes sense on the RegionServer-side is\n       // for Phoenix Local Indexing. Refer HBASE-12583 for more information.\n       Class<? extends RegionSplitPolicy> clazz =\n-          RegionSplitPolicy.getSplitPolicyClass(this.htd, env.getMasterConfiguration());\n+          RegionSplitPolicy.getSplitPolicyClass(htd, env.getMasterConfiguration());\n       this.splitPolicy = ReflectionUtils.newInstance(clazz, env.getMasterConfiguration());\n     }\n   }\n@@ -611,6 +610,7 @@ public void createDaughterRegions(final MasterProcedureEnv env) throws IOExcepti\n       maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n     final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n \n+    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n     // Split each store file.\n     for (Map.Entry<String, Collection<StoreFileInfo>>e: files.entrySet()) {\n       byte [] familyName = Bytes.toBytes(e.getKey());",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8582 Possible NullPointerException in ZKInterProcessLockBase#visitLocks (Ted Yu)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1484643 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/634f63ef3c14adf3758dc1de0be83fb55350e7e1",
        "parent": "https://github.com/apache/hbase/commit/21735dde20c555198c5f29dd053252a2f4b377ac",
        "bug_id": "hbase_111",
        "file": [
            {
                "sha": "d06800ccd798b60d7548fd15672ccf9c53f00c8f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/634f63ef3c14adf3758dc1de0be83fb55350e7e1/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/634f63ef3c14adf3758dc1de0be83fb55350e7e1/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java?ref=634f63ef3c14adf3758dc1de0be83fb55350e7e1",
                "patch": "@@ -423,7 +423,7 @@ public void visitLocks(MetadataHandler handler) throws IOException {\n       LOG.error(\"Unexpected ZooKeeper error when listing children\", e);\n       throw new IOException(\"Unexpected ZooKeeper exception\", e);\n     }\n-    if (children.size() > 0) {\n+    if (children != null && children.size() > 0) {\n       for (String child : children) {\n         if (isChildOfSameType(child)) {\n           String znode = ZKUtil.joinZNode(parentLockNode, child);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8536 Possible NullPointerException in ZKInterProcessLockBase#reapExpiredLocks (Ted Yu)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1482429 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/90bd4de7cff6ae18ae924b5e790f03e39a274617",
        "parent": "https://github.com/apache/hbase/commit/8d3581dc7eb730e51906efb8bb8ffd9f17c0507d",
        "bug_id": "hbase_112",
        "file": [
            {
                "sha": "5a0fa641ddaf069358af6dd8a4b81dd0177b5091",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/90bd4de7cff6ae18ae924b5e790f03e39a274617/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/90bd4de7cff6ae18ae924b5e790f03e39a274617/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java?ref=90bd4de7cff6ae18ae924b5e790f03e39a274617",
                "patch": "@@ -386,6 +386,7 @@ public void reapExpiredLocks(long timeout) throws IOException {\n       LOG.error(\"Unexpected ZooKeeper error when listing children\", e);\n       throw new IOException(\"Unexpected ZooKeeper exception\", e);\n     }\n+    if (children == null) return;\n \n     KeeperException deferred = null;\n     Stat stat = new Stat();",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6685 Thrift DemoClient.pl got NullPointerException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1379007 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/3bbca25d3872fef17cf6b7766b89ac66fd5fd30c",
        "parent": "https://github.com/apache/hbase/commit/0930a60681befef7a05e669d87e83c114832c3fe",
        "bug_id": "hbase_113",
        "file": [
            {
                "sha": "17d92fad2f0a33b5ff7532e4adf71ce2f68e42e6",
                "filename": "examples/thrift/DemoClient.pl",
                "blob_url": "https://github.com/apache/hbase/blob/3bbca25d3872fef17cf6b7766b89ac66fd5fd30c/examples/thrift/DemoClient.pl",
                "raw_url": "https://github.com/apache/hbase/raw/3bbca25d3872fef17cf6b7766b89ac66fd5fd30c/examples/thrift/DemoClient.pl",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/examples/thrift/DemoClient.pl?ref=3bbca25d3872fef17cf6b7766b89ac66fd5fd30c",
                "patch": "@@ -141,7 +141,6 @@ ($)\n \n # this row name is valid utf8\n $key = \"foo\";\n-# $mutations = [ Hbase::Mutation->new ( column => \"entry:$key\", value => $valid ) ];\n # This is another way to use the Mutation class\n my $mutation = Hbase::Mutation->new ();\n $mutation->{column} = \"entry:$key\";\n@@ -151,7 +150,7 @@ ($)\n \n # non-utf8 is not allowed in row names\n eval {\n-\t$mutations = [ Hbase::Mutation->new ( column => \"entry:$key\", value => $invalid ) ];\n+\t$mutations = [ Hbase::Mutation->new ( { column => \"entry:$key\", value => $invalid } ) ];\n \t# this can throw a TApplicationException (HASH) error\n \t$client->mutateRow ($demo_table, $key, $mutations);\n \tdie (\"shouldn't get here!\");",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6394 verifyrep MR job map tasks throws NullPointerException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1361469 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/1b51219af609dba2144d61d17e28ab86a434105e",
        "parent": "https://github.com/apache/hbase/commit/3a687567044615e8cec144024ebc39c18f3fd530",
        "bug_id": "hbase_114",
        "file": [
            {
                "sha": "8fb0b8e370189e91e2aceefa041e0b6c4d1d2b1e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java",
                "blob_url": "https://github.com/apache/hbase/blob/1b51219af609dba2144d61d17e28ab86a434105e/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java",
                "raw_url": "https://github.com/apache/hbase/raw/1b51219af609dba2144d61d17e28ab86a434105e/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java?ref=1b51219af609dba2144d61d17e28ab86a434105e",
                "patch": "@@ -33,7 +33,6 @@\n import org.apache.hadoop.hbase.client.Result;\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n-import org.apache.hadoop.hbase.client.replication.ReplicationAdmin;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapreduce.TableInputFormat;\n import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;\n@@ -154,7 +153,10 @@ public Void connect(HConnection conn) throws IOException {\n     }\n \n     protected void cleanup(Context context) {\n-      replicatedScanner.close();\n+      if (replicatedScanner != null) {\n+        replicatedScanner.close();\n+        replicatedScanner = null;\n+      }\n     }\n   }\n ",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15884 NPE in StoreFileScanner#skipKVsNewerThanReadpoint during reverse scan (Sergey Soldatov)",
        "commit": "https://github.com/apache/hbase/commit/fa74baeb409778de71c8a92b115b46dc63f313a0",
        "parent": "https://github.com/apache/hbase/commit/39dc19236ecc5d7970cc2f43bbad6725826d778f",
        "bug_id": "hbase_115",
        "file": [
            {
                "sha": "4955ffea630c011cdb2f0b8ffd53818124a7139a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa74baeb409778de71c8a92b115b46dc63f313a0/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa74baeb409778de71c8a92b115b46dc63f313a0/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java?ref=fa74baeb409778de71c8a92b115b46dc63f313a0",
                "patch": "@@ -253,9 +253,9 @@ protected boolean skipKVsNewerThanReadpoint() throws IOException {\n     while(enforceMVCC\n         && cur != null\n         && (cur.getSequenceId() > readPt)) {\n-      hfs.next();\n+      boolean hasNext = hfs.next();\n       setCurrentCell(hfs.getCell());\n-      if (this.stopSkippingKVsIfNextRow\n+      if (hasNext && this.stopSkippingKVsIfNextRow\n           && getComparator().compareRows(cur, startKV) > 0) {\n         return false;\n       }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6021. NullPointerException when running LoadTestTool without specifying compression type\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1339909 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/512b75643f23efec8f3ff0d9d026c8b8b4934fd6",
        "parent": "https://github.com/apache/hbase/commit/a7083731c8178849af955e0205097774b300cdcc",
        "bug_id": "hbase_116",
        "file": [
            {
                "sha": "9fa7acd4bf9ff5fca6a2f0b55a20c711cdd3db9b",
                "filename": "src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/512b75643f23efec8f3ff0d9d026c8b8b4934fd6/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/512b75643f23efec8f3ff0d9d026c8b8b4934fd6/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java?ref=512b75643f23efec8f3ff0d9d026c8b8b4934fd6",
                "patch": "@@ -280,7 +280,7 @@ private void parseColumnFamilyOptions(CommandLine cmd) {\n     }\n \n     String compressStr = cmd.getOptionValue(OPT_COMPRESSION);\n-    compressAlgo = compressStr == null ? null :\n+    compressAlgo = compressStr == null ? Compression.Algorithm.NONE :\n         Compression.Algorithm.valueOf(compressStr);\n \n     String bloomStr = cmd.getOptionValue(OPT_BLOOM);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5759 HBaseClient throws NullPointerException when EOFException should be used.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1311899 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/7d1f7b7f37a6262c564531247e82570c9a3297a7",
        "parent": "https://github.com/apache/hbase/commit/f4ea4f4f07d0dc31b42122449cea9d8fd17ef4e8",
        "bug_id": "hbase_117",
        "file": [
            {
                "sha": "dc5d1c8ff9dd3576b7ca12bd997789219a302751",
                "filename": "src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/7d1f7b7f37a6262c564531247e82570c9a3297a7/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/7d1f7b7f37a6262c564531247e82570c9a3297a7/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=7d1f7b7f37a6262c564531247e82570c9a3297a7",
                "patch": "@@ -24,6 +24,7 @@\n import java.io.BufferedOutputStream;\n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n+import java.io.EOFException;\n import java.io.FilterInputStream;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -563,6 +564,11 @@ protected void receiveResponse() {\n \n         // Read the call id.\n         RpcResponse response = RpcResponse.parseDelimitedFrom(in);\n+        if (response == null) {\n+          // When the stream is closed, protobuf doesn't raise an EOFException,\n+          // instead, it returns a null message object. \n+          throw new EOFException();\n+        }\n         int id = response.getCallId();\n \n         if (LOG.isDebugEnabled())",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12209 NPE in HRegionServer#getLastSequenceId",
        "commit": "https://github.com/apache/hbase/commit/2c07372c2f923cc8421045915773d33611ce28d8",
        "parent": "https://github.com/apache/hbase/commit/922ced0d0850b678705140d9bd5f9bae17f06d4a",
        "bug_id": "hbase_118",
        "file": [
            {
                "sha": "7c1e805122b1abac41c3851f461f514498112eeb",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/2c07372c2f923cc8421045915773d33611ce28d8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/2c07372c2f923cc8421045915773d33611ce28d8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 16,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=2c07372c2f923cc8421045915773d33611ce28d8",
                "patch": "@@ -2110,11 +2110,23 @@ public long getLastSequenceId(byte[] region) {\n     try {\n       GetLastFlushedSequenceIdRequest req = RequestConverter\n           .buildGetLastFlushedSequenceIdRequest(region);\n-      lastFlushedSequenceId = rssStub.getLastFlushedSequenceId(null, req)\n+      RegionServerStatusService.BlockingInterface rss = rssStub;\n+      if (rss == null) { // Try to connect one more time\n+        createRegionServerStatusStub();\n+        rss = rssStub;\n+        if (rss == null) {\n+          // Still no luck, we tried\n+          LOG.warn(\"Unable to connect to the master to check \"\n+            + \"the last flushed sequence id\");\n+          return -1l;\n+        }\n+      }\n+      lastFlushedSequenceId = rss.getLastFlushedSequenceId(null, req)\n           .getLastFlushedSequenceId();\n     } catch (ServiceException e) {\n       lastFlushedSequenceId = -1l;\n-      LOG.warn(\"Unable to connect to the master to check \" + \"the last flushed sequence id\", e);\n+      LOG.warn(\"Unable to connect to the master to check \"\n+        + \"the last flushed sequence id\", e);\n     }\n     return lastFlushedSequenceId;\n   }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4112  Creating table may throw NullPointerException (Jinchao via Ted Yu)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1148138 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d757784cbf18ea7b050a5f2d8f40fc89feb4904c",
        "parent": "https://github.com/apache/hbase/commit/8f2fde07c79583b1398f48dd9a37b45c6a260d94",
        "bug_id": "hbase_119",
        "file": [
            {
                "sha": "80a9ae42d1fa216ae1e1c7ff8d56873b1939d818",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/d757784cbf18ea7b050a5f2d8f40fc89feb4904c/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/d757784cbf18ea7b050a5f2d8f40fc89feb4904c/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d757784cbf18ea7b050a5f2d8f40fc89feb4904c",
                "patch": "@@ -163,6 +163,7 @@ Release 0.91.0 - Unreleased\n    HBASE-4087  HBaseAdmin should perform validation of connection it holds\n    HBASE-4052  Enabling a table after master switch does not allow table scan,\n                throwing NotServingRegionException (ramkrishna via Ted Yu)\n+   HBASE-4112  Creating table may throw NullPointerException (Jinchao via Ted Yu)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "f151c778f197eda45e3a4678c1ae34f8d12d2d9a",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/d757784cbf18ea7b050a5f2d8f40fc89feb4904c/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/d757784cbf18ea7b050a5f2d8f40fc89feb4904c/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java?ref=d757784cbf18ea7b050a5f2d8f40fc89feb4904c",
                "patch": "@@ -360,9 +360,13 @@ public void createTable(final HTableDescriptor desc, byte [][] splitKeys)\n       MetaScannerVisitor visitor = new MetaScannerVisitor() {\n         @Override\n         public boolean processRow(Result rowResult) throws IOException {\n-          HRegionInfo info = Writables.getHRegionInfo(\n+          HRegionInfo info = Writables.getHRegionInfoOrNull(\n               rowResult.getValue(HConstants.CATALOG_FAMILY,\n                   HConstants.REGIONINFO_QUALIFIER));\n+          //If regioninfo is null, skip this row\n+          if (null == info) {\n+            return true;\n+          }\n           if (!(Bytes.equals(info.getTableName(), desc.getName()))) {\n             return false;\n           }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3408 AssignmentManager NullPointerException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1055248 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8620d2c6971807d96b6a14db3f5822ae1465f7ce",
        "parent": "https://github.com/apache/hbase/commit/46a56a74db2a423557584c3ed1c2027299204326",
        "bug_id": "hbase_120",
        "file": [
            {
                "sha": "fd626a18cc16c1df0ae6df75f4fd82098005e727",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/8620d2c6971807d96b6a14db3f5822ae1465f7ce/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/8620d2c6971807d96b6a14db3f5822ae1465f7ce/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8620d2c6971807d96b6a14db3f5822ae1465f7ce",
                "patch": "@@ -812,6 +812,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3383  [0.90RC1] bin/hbase script displays \"no such file\" warning on\n                target/cached_classpath.txt\n    HBASE-3344  Master aborts after RPC to server that was shutting down\n+   HBASE-3408  AssignmentManager NullPointerException\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "675810d8e37abfe1e64423d87c1705eaff374887",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/8620d2c6971807d96b6a14db3f5822ae1465f7ce/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/8620d2c6971807d96b6a14db3f5822ae1465f7ce/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8620d2c6971807d96b6a14db3f5822ae1465f7ce",
                "patch": "@@ -21,8 +21,8 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n-import java.io.IOException;\n import java.io.EOFException;\n+import java.io.IOException;\n import java.net.ConnectException;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -1005,8 +1005,9 @@ RegionPlan getRegionPlan(final RegionState state,\n     RegionPlan existingPlan = null;\n     synchronized (this.regionPlans) {\n       existingPlan = this.regionPlans.get(encodedName);\n-      if (existingPlan == null || forceNewPlan ||\n-          (existingPlan != null && existingPlan.getDestination().equals(serverToExclude))) {\n+      if (forceNewPlan || existingPlan == null \n+              || existingPlan.getDestination() == null \n+              || existingPlan.getDestination().equals(serverToExclude)) {\n         newPlan = true;\n         this.regionPlans.put(encodedName, randomPlan);\n       }",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2497  ProcessServerShutdown throws NullPointerException for offline regions\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@939123 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/9272667079e098bd76318089d995b1375d4b4fad",
        "parent": "https://github.com/apache/hbase/commit/369056d691eb5a863df9ca83b192bda63cb1d27a",
        "bug_id": "hbase_121",
        "file": [
            {
                "sha": "e095ed971fa574cbd1b333c54b8781b14737ffb7",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/9272667079e098bd76318089d995b1375d4b4fad/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/9272667079e098bd76318089d995b1375d4b4fad/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9272667079e098bd76318089d995b1375d4b4fad",
                "patch": "@@ -20,6 +20,8 @@ Release 0.21.0 - Unreleased\n    HBASE-2392  Upgrade to ZooKeeper 3.3.0\n    HBASE-2294  Enumerate ACID properties of HBase in a well defined spec\n                (Todd Lipcon via Stack)\n+   HBASE-2497  ProcessServerShutdown throws NullPointerException for offline\n+               regions\n \n   BUG FIXES\n    HBASE-1791  Timeout in IndexRecordWriter (Bradford Stephens via Andrew",
                "deletions": 0
            },
            {
                "sha": "e2a468dc881e794269a9055ce7fcc1f087926ac5",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/9272667079e098bd76318089d995b1375d4b4fad/core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/9272667079e098bd76318089d995b1375d4b4fad/core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java?ref=9272667079e098bd76318089d995b1375d4b4fad",
                "patch": "@@ -249,7 +249,7 @@ protected void scanRegion(final MetaRegion region) throws IOException {\n    * @param r\n    * @return Empty String or server address found in <code>r</code>\n    */\n-  private String getServerAddress(final Result r) {\n+  static String getServerAddress(final Result r) {\n     byte [] val = r.getValue(CATALOG_FAMILY, SERVER_QUALIFIER);\n     return val == null || val.length <= 0? \"\": Bytes.toString(val);\n   }\n@@ -258,7 +258,7 @@ private String getServerAddress(final Result r) {\n    * @param r\n    * @return Return 0L or server startcode found in <code>r</code>\n    */\n-  private long getStartCode(final Result r) {\n+  static long getStartCode(final Result r) {\n     byte [] val = r.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER);\n     return val == null || val.length <= 0? 0L: Bytes.toLong(val);\n   }",
                "deletions": 2
            },
            {
                "sha": "2bcfffe5923f2a6d640280e061534c62ce1526c8",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java",
                "blob_url": "https://github.com/apache/hbase/blob/9272667079e098bd76318089d995b1375d4b4fad/core/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java",
                "raw_url": "https://github.com/apache/hbase/raw/9272667079e098bd76318089d995b1375d4b4fad/core/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java?ref=9272667079e098bd76318089d995b1375d4b4fad",
                "patch": "@@ -140,10 +140,9 @@ protected void scanMetaRegion(HRegionInterface server, long scannerId,\n         // shutdown server but that would mean that we'd reassign regions that\n         // were already out being assigned, ones that were product of a split\n         // that happened while the shutdown was being processed.\n-        String serverAddress = \n-          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));\n-        long startCode =\n-          Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));\n+        String serverAddress = BaseScanner.getServerAddress(values);\n+        long startCode = BaseScanner.getStartCode(values);\n+\n         String serverName = null;\n         if (serverAddress != null && serverAddress.length() > 0) {\n           serverName = HServerInfo.getServerName(serverAddress, startCode);",
                "deletions": 4
            },
            {
                "sha": "60c1fd762ef2889b9636fefeeceb2551813027ac",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/master/TableOperation.java",
                "blob_url": "https://github.com/apache/hbase/blob/9272667079e098bd76318089d995b1375d4b4fad/core/src/main/java/org/apache/hadoop/hbase/master/TableOperation.java",
                "raw_url": "https://github.com/apache/hbase/raw/9272667079e098bd76318089d995b1375d4b4fad/core/src/main/java/org/apache/hadoop/hbase/master/TableOperation.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/master/TableOperation.java?ref=9272667079e098bd76318089d995b1375d4b4fad",
                "patch": "@@ -101,12 +101,10 @@ public Boolean call() throws IOException {\n                       Bytes.toStringBinary(values.getRow()));\n             continue;\n           }\n-          final String serverAddress =\n-            Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));\n+          final String serverAddress = BaseScanner.getServerAddress(values);\n           String serverName = null;\n           if (serverAddress != null && serverAddress.length() > 0) {\n-            long startCode =\n-              Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));\n+            long startCode = BaseScanner.getStartCode(values);\n             serverName = HServerInfo.getServerName(serverAddress, startCode);\n           }\n           if (Bytes.compareTo(info.getTableDesc().getName(), tableName) > 0) {",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1934 NullPointerException in ClientScanner\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@829703 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/e5f6c67f0dc3bff76db35d7a3984068e79f991bf",
        "parent": "https://github.com/apache/hbase/commit/a4c81cdd30f0cb1f86d82db9a332ce3d83d8a9e7",
        "bug_id": "hbase_122",
        "file": [
            {
                "sha": "2fa76165ac9823e66fd7cc5d9b0ad40f6c059db8",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/e5f6c67f0dc3bff76db35d7a3984068e79f991bf/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/e5f6c67f0dc3bff76db35d7a3984068e79f991bf/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=e5f6c67f0dc3bff76db35d7a3984068e79f991bf",
                "patch": "@@ -82,6 +82,7 @@ Release 0.21.0 - Unreleased\n    HBASE-1925  IllegalAccessError: Has not been initialized (getMaxSequenceId)\n    HBASE-1929  If hbase-default.xml is not in CP, zk session timeout is 10 secs!\n    HBASE-1927  Scanners not closed properly in certain circumstances\n+   HBASE-1934  NullPointerException in ClientScanner (Daniel Ploeg via Stack)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "7612208f3e004ee42d390ebeab27d3847fdae046",
                "filename": "src/java/org/apache/hadoop/hbase/client/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/e5f6c67f0dc3bff76db35d7a3984068e79f991bf/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/e5f6c67f0dc3bff76db35d7a3984068e79f991bf/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "status": "modified",
                "changes": 42,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HTable.java?ref=e5f6c67f0dc3bff76db35d7a3984068e79f991bf",
                "patch": "@@ -834,36 +834,6 @@ protected ScannerCallable getScannerCallable(byte [] localStartKey,\n       return s;\n     }\n \n-    /**\n-     * @param endKey\n-     * @return Returns true if the passed region endkey is judged beyond\n-     * filter.\n-     */\n-    private boolean filterSaysStop(final byte [] endKey) {\n-      if (scan.getStopRow().length > 0) {\n-        // there is a stop row, check to see if we are past it.\n-        byte [] stopRow = scan.getStopRow();\n-        int cmp = Bytes.compareTo(stopRow, 0, stopRow.length,\n-            endKey, 0, endKey.length);\n-        if (cmp <= 0) {\n-          // stopRow <= endKey (endKey is equals to or larger than stopRow)\n-          // This is a stop.\n-          return true;\n-        }\n-      }\n-\n-      if(!scan.hasFilter()) {\n-        return false;\n-      }\n-\n-      if (scan.getFilter() != null) {\n-        // Let the filter see current row.\n-        scan.getFilter().filterRowKey(endKey, 0, endKey.length);\n-        return scan.getFilter().filterAllRemaining();\n-      }\n-      return false; //unlikely.\n-    }\n-\n     public Result next() throws IOException {\n       // If the scanner is closed but there is some rows left in the cache,\n       // it will first empty it before returning null\n@@ -897,12 +867,14 @@ public Result next() throws IOException {\n             }\n             // Else, its signal from depths of ScannerCallable that we got an\n             // NSRE on a next and that we need to reset the scanner.\n-            this.scan.setStartRow(this.lastResult.getRow());\n-            // Clear region as flag to nextScanner to use this.scan.startRow.\n+            if (this.lastResult != null) {\n+              this.scan.setStartRow(this.lastResult.getRow());\n+              // Skip first row returned.  We already let it out on previous\n+              // invocation.\n+              skipFirst = true;\n+            }\n+            // Clear region\n             this.currentRegion = null;\n-            // Skip first row returned.  We already let it out on previous\n-            // invocation.\n-            skipFirst = true;\n             continue;\n           } catch (IOException e) {\n             if (e instanceof UnknownScannerException &&",
                "deletions": 35
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13923 Addendum fixes NPE in HRegionServer#buildServerLoad()",
        "commit": "https://github.com/apache/hbase/commit/db5dd1ebef007dfc9609e7fc87ef72224b8f0a53",
        "parent": "https://github.com/apache/hbase/commit/b5b5853043e20a719f1b7afb485ede68ddd823f0",
        "bug_id": "hbase_123",
        "file": [
            {
                "sha": "43c283669588ce47f1239d679cdcd009e0de02a7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/db5dd1ebef007dfc9609e7fc87ef72224b8f0a53/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/db5dd1ebef007dfc9609e7fc87ef72224b8f0a53/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 10,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=db5dd1ebef007dfc9609e7fc87ef72224b8f0a53",
                "patch": "@@ -1182,10 +1182,12 @@ protected void tryRegionServerReport(long reportStartTime, long reportEndTime)\n     RegionLoad.Builder regionLoadBldr = RegionLoad.newBuilder();\n     RegionSpecifier.Builder regionSpecifier = RegionSpecifier.newBuilder();\n     for (Region region : regions) {\n-      Set<String> regionCoprocessors = region.getCoprocessorHost().getCoprocessors();\n-      Iterator<String> iterator = regionCoprocessors.iterator();\n-      while (iterator.hasNext()) {\n-        serverLoad.addCoprocessors(coprocessorBuilder.setName(iterator.next()).build());\n+      if (region.getCoprocessorHost() != null) {\n+        Set<String> regionCoprocessors = region.getCoprocessorHost().getCoprocessors();\n+        Iterator<String> iterator = regionCoprocessors.iterator();\n+        while (iterator.hasNext()) {\n+          serverLoad.addCoprocessors(coprocessorBuilder.setName(iterator.next()).build());\n+        }\n       }\n       serverLoad.addRegionLoads(createRegionLoad(region, regionLoadBldr, regionSpecifier));\n       for (String coprocessor : getWAL(region.getRegionInfo()).getCoprocessorHost()",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-2338 Fix NullPointerException in master server.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@602226 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/612f446dbdd151a02700839705c5ebfd1fc33e2a",
        "parent": "https://github.com/apache/hbase/commit/bccf1dc26f0af20b0ec70d420990e7fd9e01c4d7",
        "bug_id": "hbase_124",
        "file": [
            {
                "sha": "af507998fcbeb2ce17577cf3a90faa65bf492f24",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "patch": "@@ -60,6 +60,7 @@ Trunk (unreleased changes)\n                (Bryan Duxbury via Stack)\n    HADOOP-2365 Result of HashFunction.hash() contains all identical values\n    HADOOP-2362 Leaking hdfs file handle on region split\n+   HADOOP-2338 Fix NullPointerException in master server.\n \n   IMPROVEMENTS\n    HADOOP-2401 Add convenience put method that takes writable",
                "deletions": 0
            },
            {
                "sha": "097eb39102ad5c68da1d56b53e922fd86af66f53",
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMaster.java",
                "status": "modified",
                "changes": 733,
                "additions": 383,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "patch": "@@ -31,8 +31,6 @@\n import java.util.Random;\n import java.util.Set;\n import java.util.SortedMap;\n-import java.util.Timer;\n-import java.util.TimerTask;\n import java.util.TreeMap;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ConcurrentHashMap;\n@@ -74,6 +72,7 @@\n   HMasterRegionInterface {\n   \n   static final Log LOG = LogFactory.getLog(HMaster.class.getName());\n+  static final Long ZERO_L = Long.valueOf(0L);\n \n   /** {@inheritDoc} */\n   public long getProtocolVersion(String protocol,\n@@ -93,6 +92,8 @@ public long getProtocolVersion(String protocol,\n   // started here in HMaster rather than have them have to know about the\n   // hosting class\n   volatile AtomicBoolean closed = new AtomicBoolean(true);\n+  volatile AtomicBoolean shutdownRequested = new AtomicBoolean(false);\n+  volatile AtomicInteger quiescedMetaServers = new AtomicInteger(0);\n   volatile boolean fsOk;\n   Path dir;\n   HBaseConfiguration conf;\n@@ -102,9 +103,9 @@ public long getProtocolVersion(String protocol,\n   int numRetries;\n   long maxRegionOpenTime;\n \n-  DelayQueue<ProcessServerShutdown> shutdownQueue =\n-    new DelayQueue<ProcessServerShutdown>();\n-  BlockingQueue<RegionServerOperation> msgQueue =\n+  DelayQueue<RegionServerOperation> delayedToDoQueue =\n+    new DelayQueue<RegionServerOperation>();\n+  BlockingQueue<RegionServerOperation> toDoQueue =\n     new LinkedBlockingQueue<RegionServerOperation>();\n \n   int leaseTimeout;\n@@ -424,8 +425,7 @@ protected void checkAssigned(final HRegionInfo info,\n           || killedRegions.contains(info.getRegionName()) // queued for offline\n           || regionsToDelete.contains(info.getRegionName())) { // queued for delete\n \n-        unassignedRegions.remove(info.getRegionName());\n-        assignAttempts.remove(info.getRegionName());\n+        unassignedRegions.remove(info);\n         return;\n       }\n       HServerInfo storedInfo = null;\n@@ -458,7 +458,7 @@ protected void checkAssigned(final HRegionInfo info,\n       if (!deadServer &&\n           ((storedInfo != null && storedInfo.getStartCode() != startCode) ||\n               (storedInfo == null &&\n-                  !unassignedRegions.containsKey(info.getRegionName()) &&\n+                  !unassignedRegions.containsKey(info) &&\n                   !pendingRegions.contains(info.getRegionName())\n               )\n           )\n@@ -495,8 +495,7 @@ protected void checkAssigned(final HRegionInfo info,\n           }\n         }\n         // Now get the region assigned\n-        unassignedRegions.put(info.getRegionName(), info);\n-        assignAttempts.put(info.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(info, ZERO_L);\n       }\n     }\n   }\n@@ -818,24 +817,18 @@ synchronized boolean waitForMetaRegionsOrClose() {\n     new ConcurrentHashMap<String, HServerLoad>();\n \n   /**\n-   * The 'unassignedRegions' table maps from a region name to a HRegionInfo \n-   * record, which includes the region's table, its id, and its start/end keys.\n+   * The 'unassignedRegions' table maps from a HRegionInfo to a timestamp that\n+   * indicates the last time we *tried* to assign the region to a RegionServer.\n+   * If the timestamp is out of date, then we can try to reassign it. \n    * \n    * We fill 'unassignedRecords' by scanning ROOT and META tables, learning the\n    * set of all known valid regions.\n    * \n    * <p>Items are removed from this list when a region server reports in that\n    * the region has been deployed.\n    */\n-  final SortedMap<Text, HRegionInfo> unassignedRegions =\n-    Collections.synchronizedSortedMap(new TreeMap<Text, HRegionInfo>());\n-\n-  /**\n-   * The 'assignAttempts' table maps from regions to a timestamp that indicates\n-   * the last time we *tried* to assign the region to a RegionServer. If the \n-   * timestamp is out of date, then we can try to reassign it.\n-   */\n-  final Map<Text, Long> assignAttempts = new ConcurrentHashMap<Text, Long>();\n+  final SortedMap<HRegionInfo, Long> unassignedRegions =\n+    Collections.synchronizedSortedMap(new TreeMap<HRegionInfo, Long>());\n \n   /**\n    * Regions that have been assigned, and the server has reported that it has\n@@ -978,10 +971,7 @@ public HMaster(Path dir, HServerAddress address, HBaseConfiguration conf)\n    */\n   void unassignRootRegion() {\n     this.rootRegionLocation.set(null);\n-    this.unassignedRegions.put(HRegionInfo.rootRegionInfo.getRegionName(),\n-        HRegionInfo.rootRegionInfo);\n-    this.assignAttempts.put(HRegionInfo.rootRegionInfo.getRegionName(),\n-        Long.valueOf(0L));\n+    this.unassignedRegions.put(HRegionInfo.rootRegionInfo, ZERO_L);\n   }\n \n   /**\n@@ -1030,7 +1020,11 @@ public Path getRootDir() {\n    * @return Location of the <code>-ROOT-</code> region.\n    */\n   public HServerAddress getRootRegionLocation() {\n-    return this.rootRegionLocation.get();\n+    HServerAddress rootServer = null;\n+    if (!shutdownRequested.get() && !closed.get()) {\n+      rootServer = this.rootRegionLocation.get();\n+    }\n+    return rootServer;\n   }\n   \n   /**\n@@ -1054,11 +1048,11 @@ public void run() {\n         if (rootRegionLocation.get() != null) {\n           // We can't process server shutdowns unless the root region is online \n \n-          op = this.shutdownQueue.poll();\n+          op = this.delayedToDoQueue.poll();\n         }\n         if (op == null ) {\n           try {\n-            op = msgQueue.poll(threadWakeFrequency, TimeUnit.MILLISECONDS);\n+            op = toDoQueue.poll(threadWakeFrequency, TimeUnit.MILLISECONDS);\n           } catch (InterruptedException e) {\n             // continue\n           }\n@@ -1077,7 +1071,7 @@ public void run() {\n             // for the missing meta region(s) to come back online, but since it\n             // is waiting, it cannot process the meta region online operation it\n             // is waiting for. So put this operation back on the queue for now.\n-            if (msgQueue.size() == 0) {\n+            if (toDoQueue.size() == 0) {\n               // The queue is currently empty so wait for a while to see if what\n               // we need comes in first\n               sleeper.sleep();\n@@ -1086,9 +1080,10 @@ public void run() {\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Put \" + op.toString() + \" back on queue\");\n               }\n-              msgQueue.put(op);\n+              toDoQueue.put(op);\n             } catch (InterruptedException e) {\n-              throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+              throw new RuntimeException(\n+                  \"Putting into toDoQueue was interrupted.\", e);\n             }\n           }\n         } catch (Exception ex) {\n@@ -1106,9 +1101,10 @@ public void run() {\n           }\n           LOG.warn(\"Processing pending operations: \" + op.toString(), ex);\n           try {\n-            msgQueue.put(op);\n+            toDoQueue.put(op);\n           } catch (InterruptedException e) {\n-            throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+            throw new RuntimeException(\n+                \"Putting into toDoQueue was interrupted.\", e);\n           }\n         }\n       }\n@@ -1255,7 +1251,7 @@ public MapWritable regionServerStartup(HServerInfo serverInfo)\n       if (root != null && root.equals(storedInfo.getServerAddress())) {\n         unassignRootRegion();\n       }\n-      shutdownQueue.put(new ProcessServerShutdown(storedInfo));\n+      delayedToDoQueue.put(new ProcessServerShutdown(storedInfo));\n     }\n \n     // record new server\n@@ -1302,48 +1298,70 @@ private long getServerLabel(final String s) {\n   throws IOException {\n     String serverName = serverInfo.getServerAddress().toString().trim();\n     long serverLabel = getServerLabel(serverName);\n-    if (msgs.length > 0 && msgs[0].getMsg() == HMsg.MSG_REPORT_EXITING) {\n-      synchronized (serversToServerInfo) {\n-        try {\n-          // HRegionServer is shutting down. Cancel the server's lease.\n-          // Note that canceling the server's lease takes care of updating\n-          // serversToServerInfo, etc.\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Region server \" + serverName +\n-            \": MSG_REPORT_EXITING -- cancelling lease\");\n-          }\n-\n-          if (cancelLease(serverName, serverLabel)) {\n-            // Only process the exit message if the server still has a lease.\n-            // Otherwise we could end up processing the server exit twice.\n-            LOG.info(\"Region server \" + serverName +\n-            \": MSG_REPORT_EXITING -- lease cancelled\");\n-            // Get all the regions the server was serving reassigned\n-            // (if we are not shutting down).\n-            if (!closed.get()) {\n-              for (int i = 1; i < msgs.length; i++) {\n-                HRegionInfo info = msgs[i].getRegionInfo();\n-                if (info.getTableDesc().getName().equals(ROOT_TABLE_NAME)) {\n-                  rootRegionLocation.set(null);\n-                } else if (info.getTableDesc().getName().equals(META_TABLE_NAME)) {\n-                  onlineMetaRegions.remove(info.getStartKey());\n-                }\n+//    if (LOG.isDebugEnabled()) {\n+//      LOG.debug(\"received heartbeat from \" + serverName);\n+//    }\n+    if (msgs.length > 0) {\n+      if (msgs[0].getMsg() == HMsg.MSG_REPORT_EXITING) {\n+        synchronized (serversToServerInfo) {\n+          try {\n+            // HRegionServer is shutting down. Cancel the server's lease.\n+            // Note that canceling the server's lease takes care of updating\n+            // serversToServerInfo, etc.\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Region server \" + serverName +\n+              \": MSG_REPORT_EXITING -- cancelling lease\");\n+            }\n+\n+            if (cancelLease(serverName, serverLabel)) {\n+              // Only process the exit message if the server still has a lease.\n+              // Otherwise we could end up processing the server exit twice.\n+              LOG.info(\"Region server \" + serverName +\n+              \": MSG_REPORT_EXITING -- lease cancelled\");\n+              // Get all the regions the server was serving reassigned\n+              // (if we are not shutting down).\n+              if (!closed.get()) {\n+                for (int i = 1; i < msgs.length; i++) {\n+                  HRegionInfo info = msgs[i].getRegionInfo();\n+                  if (info.isRootRegion()) {\n+                    rootRegionLocation.set(null);\n+                  } else if (info.isMetaTable()) {\n+                    onlineMetaRegions.remove(info.getStartKey());\n+                  }\n \n-                this.unassignedRegions.put(info.getRegionName(), info);\n-                this.assignAttempts.put(info.getRegionName(), Long.valueOf(0L));\n+                  this.unassignedRegions.put(info, ZERO_L);\n+                }\n               }\n             }\n-          }\n \n-          // We don't need to return anything to the server because it isn't\n-          // going to do any more work.\n-          return new HMsg[0];\n-        } finally {\n-          serversToServerInfo.notifyAll();\n+            // We don't need to return anything to the server because it isn't\n+            // going to do any more work.\n+            return new HMsg[0];\n+          } finally {\n+            serversToServerInfo.notifyAll();\n+          }\n+        }\n+      } else if (msgs[0].getMsg() == HMsg.MSG_REPORT_QUIESCED) {\n+        LOG.info(\"Region server \" + serverName + \" quiesced\");\n+        if(quiescedMetaServers.incrementAndGet() == serversToServerInfo.size()) {\n+          // If the only servers we know about are meta servers, then we can\n+          // proceed with shutdown\n+          LOG.info(\"All user tables quiesced. Proceeding with shutdown\");\n+          closed.set(true);\n+          synchronized(toDoQueue) {\n+            toDoQueue.clear();                         // Empty the queue\n+            delayedToDoQueue.clear();                  // Empty shut down queue\n+            toDoQueue.notifyAll();                     // Wake main thread\n+          }\n         }\n       }\n     }\n \n+    if (shutdownRequested.get() && !closed.get()) {\n+      // Tell the server to stop serving any user regions\n+      return new HMsg[]{new HMsg(HMsg.MSG_REGIONSERVER_QUIESCE)};\n+    }\n+\n     if (closed.get()) {\n       // Tell server to shut down if we are shutting down.  This should\n       // happen after check of MSG_REPORT_EXITING above, since region server\n@@ -1476,62 +1494,86 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n       switch (incomingMsgs[i].getMsg()) {\n \n       case HMsg.MSG_REPORT_PROCESS_OPEN:\n-        synchronized (this.assignAttempts) {\n+        synchronized (unassignedRegions) {\n           // Region server has acknowledged request to open region.\n-          // Extend region open time by 1/2 max region open time.\n-          assignAttempts.put(region.getRegionName(), \n-              Long.valueOf(assignAttempts.get(\n-                  region.getRegionName()).longValue() +\n-                  (this.maxRegionOpenTime / 2)));\n+          // Extend region open time by max region open time.\n+          unassignedRegions.put(region,\n+              System.currentTimeMillis() + this.maxRegionOpenTime);\n         }\n         break;\n         \n       case HMsg.MSG_REPORT_OPEN:\n-        HRegionInfo regionInfo = unassignedRegions.get(region.getRegionName());\n-\n-        if (regionInfo == null) {\n-\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"region server \" + info.getServerAddress().toString()\n-                + \" should not have opened region \" + region.getRegionName());\n+        boolean duplicateAssignment = false;\n+        synchronized (unassignedRegions) {\n+          if (unassignedRegions.remove(region) == null) {\n+            if (region.getRegionName().compareTo(\n+                HRegionInfo.rootRegionInfo.getRegionName()) == 0) {\n+              // Root region\n+              HServerAddress rootServer = rootRegionLocation.get();\n+              if (rootServer != null) {\n+                if (rootServer.toString().compareTo(serverName) == 0) {\n+                  // A duplicate open report from the correct server\n+                  break;\n+                }\n+                // We received an open report on the root region, but it is\n+                // assigned to a different server\n+                duplicateAssignment = true;\n+              }\n+            } else {\n+              // Not root region. If it is not a pending region, then we are\n+              // going to treat it as a duplicate assignment\n+              if (pendingRegions.contains(region.getRegionName())) {\n+                // A duplicate report from the correct server\n+                break;\n+              }\n+              // Although we can't tell for certain if this is a duplicate\n+              // report from the correct server, we are going to treat it\n+              // as such\n+              duplicateAssignment = true;\n+            }\n           }\n+          if (duplicateAssignment) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"region server \" + info.getServerAddress().toString()\n+                  + \" should not have opened region \" + region.getRegionName());\n+            }\n \n-          // This Region should not have been opened.\n-          // Ask the server to shut it down, but don't report it as closed.  \n-          // Otherwise the HMaster will think the Region was closed on purpose, \n-          // and then try to reopen it elsewhere; that's not what we want.\n+            // This Region should not have been opened.\n+            // Ask the server to shut it down, but don't report it as closed.  \n+            // Otherwise the HMaster will think the Region was closed on purpose, \n+            // and then try to reopen it elsewhere; that's not what we want.\n \n-          returnMsgs.add(new HMsg(HMsg.MSG_REGION_CLOSE_WITHOUT_REPORT, region)); \n+            returnMsgs.add(\n+                new HMsg(HMsg.MSG_REGION_CLOSE_WITHOUT_REPORT, region)); \n \n-        } else {\n-          LOG.info(info.getServerAddress().toString() + \" serving \" +\n-              region.getRegionName());\n-\n-          if (region.getRegionName().compareTo(\n-              HRegionInfo.rootRegionInfo.getRegionName()) == 0) {\n-            // Store the Root Region location (in memory)\n-            synchronized (rootRegionLocation) {\n-              this.rootRegionLocation.set(\n-                  new HServerAddress(info.getServerAddress()));\n-              this.rootRegionLocation.notifyAll();\n-            }\n           } else {\n-            // Note that the table has been assigned and is waiting for the meta\n-            // table to be updated.\n+            LOG.info(info.getServerAddress().toString() + \" serving \" +\n+                region.getRegionName());\n+\n+            if (region.getRegionName().compareTo(\n+                HRegionInfo.rootRegionInfo.getRegionName()) == 0) {\n+              // Store the Root Region location (in memory)\n+              synchronized (rootRegionLocation) {\n+                this.rootRegionLocation.set(\n+                    new HServerAddress(info.getServerAddress()));\n+                this.rootRegionLocation.notifyAll();\n+              }\n+            } else {\n+              // Note that the table has been assigned and is waiting for the\n+              // meta table to be updated.\n \n-            pendingRegions.add(region.getRegionName());\n+              pendingRegions.add(region.getRegionName());\n \n-            // Queue up an update to note the region location.\n+              // Queue up an update to note the region location.\n \n-            try {\n-              msgQueue.put(new ProcessRegionOpen(info, region));\n-            } catch (InterruptedException e) {\n-              throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n-            }\n-          } \n-          // Remove from unassigned list so we don't assign it to someone else\n-          this.unassignedRegions.remove(region.getRegionName());\n-          this.assignAttempts.remove(region.getRegionName());\n+              try {\n+                toDoQueue.put(new ProcessRegionOpen(info, region));\n+              } catch (InterruptedException e) {\n+                throw new RuntimeException(\n+                    \"Putting into toDoQueue was interrupted.\", e);\n+              }\n+            } \n+          }\n         }\n         break;\n \n@@ -1559,19 +1601,24 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n             deleteRegion = true;\n           }\n \n+          if (region.isMetaTable()) {\n+            // Region is part of the meta table. Remove it from onlineMetaRegions\n+            onlineMetaRegions.remove(region.getStartKey());\n+          }\n+\n           // NOTE: we cannot put the region into unassignedRegions as that\n           //       could create a race with the pending close if it gets \n           //       reassigned before the close is processed.\n \n-          unassignedRegions.remove(region.getRegionName());\n-          assignAttempts.remove(region.getRegionName());\n+          unassignedRegions.remove(region);\n \n           try {\n-            msgQueue.put(new ProcessRegionClose(region, reassignRegion,\n+            toDoQueue.put(new ProcessRegionClose(region, reassignRegion,\n                 deleteRegion));\n \n           } catch (InterruptedException e) {\n-            throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+            throw new RuntimeException(\n+                \"Putting into toDoQueue was interrupted.\", e);\n           }\n         }\n         break;\n@@ -1580,12 +1627,10 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n         // A region has split.\n \n         HRegionInfo newRegionA = incomingMsgs[++i].getRegionInfo();\n-        unassignedRegions.put(newRegionA.getRegionName(), newRegionA);\n-        assignAttempts.put(newRegionA.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(newRegionA, ZERO_L);\n \n         HRegionInfo newRegionB = incomingMsgs[++i].getRegionInfo();\n-        unassignedRegions.put(newRegionB.getRegionName(), newRegionB);\n-        assignAttempts.put(newRegionB.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(newRegionB, ZERO_L);\n \n         LOG.info(\"region \" + region.getRegionName() +\n             \" split. New regions are: \" + newRegionA.getRegionName() + \", \" +\n@@ -1631,15 +1676,22 @@ private boolean cancelLease(final String serverName, final long serverLabel) {\n   private void assignRegions(HServerInfo info, String serverName,\n       ArrayList<HMsg> returnMsgs) {\n     \n-    synchronized (this.assignAttempts) {\n+    synchronized (this.unassignedRegions) {\n       \n       // We need to hold a lock on assign attempts while we figure out what to\n       // do so that multiple threads do not execute this method in parallel\n       // resulting in assigning the same region to multiple servers.\n       \n       long now = System.currentTimeMillis();\n-      Set<Text> regionsToAssign = new HashSet<Text>();\n-      for (Map.Entry<Text, Long> e: this.assignAttempts.entrySet()) {\n+      Set<HRegionInfo> regionsToAssign = new HashSet<HRegionInfo>();\n+      for (Map.Entry<HRegionInfo, Long> e: this.unassignedRegions.entrySet()) {\n+        HRegionInfo i = e.getKey();\n+        if (numberOfMetaRegions.get() != onlineMetaRegions.size() &&\n+            !i.isMetaRegion()) {\n+          // Can't assign user regions until all meta regions have been assigned\n+          // and are on-line\n+          continue;\n+        }\n         long diff = now - e.getValue().longValue();\n         if (diff > this.maxRegionOpenTime) {\n           regionsToAssign.add(e.getKey());\n@@ -1720,11 +1772,10 @@ private void assignRegions(HServerInfo info, String serverName,\n         }\n \n         now = System.currentTimeMillis();\n-        for (Text regionName: regionsToAssign) {\n-          HRegionInfo regionInfo = this.unassignedRegions.get(regionName);\n-          LOG.info(\"assigning region \" + regionName + \" to server \" +\n-              serverName);\n-          this.assignAttempts.put(regionName, Long.valueOf(now));\n+        for (HRegionInfo regionInfo: regionsToAssign) {\n+          LOG.info(\"assigning region \" + regionInfo.getRegionName() +\n+              \" to server \" + serverName);\n+          this.unassignedRegions.put(regionInfo, Long.valueOf(now));\n           returnMsgs.add(new HMsg(HMsg.MSG_REGION_OPEN, regionInfo));\n           if (--nregions <= 0) {\n             break;\n@@ -1773,14 +1824,13 @@ private int regionsPerServer(final int nRegionsToAssign,\n    * @param serverName\n    * @param returnMsgs\n    */\n-  private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n+  private void assignRegionsToOneServer(final Set<HRegionInfo> regionsToAssign,\n       final String serverName, final ArrayList<HMsg> returnMsgs) {\n     long now = System.currentTimeMillis();\n-    for (Text regionName: regionsToAssign) {\n-      HRegionInfo regionInfo = this.unassignedRegions.get(regionName);\n-      LOG.info(\"assigning region \" + regionName + \" to the only server \" +\n-          serverName);\n-      this.assignAttempts.put(regionName, Long.valueOf(now));\n+    for (HRegionInfo regionInfo: regionsToAssign) {\n+      LOG.info(\"assigning region \" + regionInfo.getRegionName() +\n+          \" to the only server \" + serverName);\n+      this.unassignedRegions.put(regionInfo, Long.valueOf(now));\n       returnMsgs.add(new HMsg(HMsg.MSG_REGION_OPEN, regionInfo));\n     }\n   }\n@@ -1789,26 +1839,77 @@ private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n    * Some internal classes to manage msg-passing and region server operations\n    */\n \n-  private abstract class RegionServerOperation {\n-    RegionServerOperation() {}\n+  private abstract class RegionServerOperation implements Delayed {\n+    private long expire;\n \n-    abstract boolean process() throws IOException;\n+    protected RegionServerOperation() {\n+      // Set the future time at which we expect to be released from the\n+      // DelayQueue we're inserted in on lease expiration.\n+      this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n+    }\n+    \n+    /** {@inheritDoc} */\n+    public long getDelay(TimeUnit unit) {\n+      return unit.convert(this.expire - System.currentTimeMillis(),\n+        TimeUnit.MILLISECONDS);\n+    }\n+    \n+    /** {@inheritDoc} */\n+    public int compareTo(Delayed o) {\n+      return Long.valueOf(getDelay(TimeUnit.MILLISECONDS)\n+          - o.getDelay(TimeUnit.MILLISECONDS)).intValue();\n+    }\n+    \n+    protected void requeue() {\n+      this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n+      delayedToDoQueue.put(this);\n+    }\n+    \n+    protected boolean rootAvailable() {\n+      boolean available = true;\n+      if (rootRegionLocation.get() == null) {\n+        available = false;\n+        requeue();\n+      }\n+      return available;\n+    }\n+\n+    protected boolean metaTableAvailable() {\n+      boolean available = true;\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"numberOfMetaRegions: \" + numberOfMetaRegions.get() +\n+            \", onlineMetaRegions.size(): \" + onlineMetaRegions.size());\n+      }\n+      if (numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n+        // We can't proceed because not all of the meta regions are online.\n+        // We can't block either because that would prevent the meta region\n+        // online message from being processed. In order to prevent spinning\n+        // in the run queue, put this request on the delay queue to give\n+        // other threads the opportunity to get the meta regions on-line.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Requeuing because not all meta regions are online\");\n+        }\n+        available = false;\n+        requeue();\n+      }\n+      return available;\n+    }\n+    \n+    protected abstract boolean process() throws IOException;\n   }\n \n   /** \n    * Instantiated when a server's lease has expired, meaning it has crashed.\n    * The region server's log file needs to be split up for each region it was\n    * serving, and the regions need to get reassigned.\n    */\n-  private class ProcessServerShutdown extends RegionServerOperation\n-  implements Delayed {\n-    private long expire;\n+  private class ProcessServerShutdown extends RegionServerOperation {\n     private HServerAddress deadServer;\n     private String deadServerName;\n     private Path oldLogDir;\n-    private transient boolean logSplit;\n-    private transient boolean rootChecked;\n-    private transient boolean rootRescanned;\n+    private boolean logSplit;\n+    private boolean rootChecked;\n+    private boolean rootRescanned;\n \n     private class ToDoEntry {\n       boolean deleteRegion;\n@@ -1824,7 +1925,10 @@ private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n       }\n     }\n \n-    ProcessServerShutdown(HServerInfo serverInfo) {\n+    /**\n+     * @param serverInfo\n+     */\n+    public ProcessServerShutdown(HServerInfo serverInfo) {\n       super();\n       this.deadServer = serverInfo.getServerAddress();\n       this.deadServerName = this.deadServer.toString();\n@@ -1838,23 +1942,8 @@ private void assignRegionsToOneServer(final Set<Text> regionsToAssign,\n       dirName.append(\"_\");\n       dirName.append(deadServer.getPort());\n       this.oldLogDir = new Path(dir, dirName.toString());\n-      // Set the future time at which we expect to be released from the\n-      // DelayQueue we're inserted in on lease expiration.\n-      this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n     }\n \n-    /** {@inheritDoc} */\n-    public long getDelay(TimeUnit unit) {\n-      return unit.convert(this.expire - System.currentTimeMillis(),\n-        TimeUnit.MILLISECONDS);\n-    }\n-    \n-    /** {@inheritDoc} */\n-    public int compareTo(Delayed o) {\n-      return Long.valueOf(getDelay(TimeUnit.MILLISECONDS)\n-          - o.getDelay(TimeUnit.MILLISECONDS)).intValue();\n-    }\n-    \n     /** {@inheritDoc} */\n     @Override\n     public String toString() {\n@@ -1866,7 +1955,7 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n         Text regionName) throws IOException {\n \n       ArrayList<ToDoEntry> toDoList = new ArrayList<ToDoEntry>();\n-      TreeMap<Text, HRegionInfo> regions = new TreeMap<Text, HRegionInfo>();\n+      HashSet<HRegionInfo> regions = new HashSet<HRegionInfo>();\n \n       try {\n         while (true) {\n@@ -1958,8 +2047,7 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n             if (regionsToKill.containsKey(info.getRegionName())) {\n               regionsToKill.remove(info.getRegionName());\n               killList.put(deadServerName, regionsToKill);\n-              unassignedRegions.remove(info.getRegionName());\n-              assignAttempts.remove(info.getRegionName());\n+              unassignedRegions.remove(info);\n               synchronized (regionsToDelete) {\n                 if (regionsToDelete.contains(info.getRegionName())) {\n                   // Delete this region\n@@ -1974,7 +2062,7 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n \n           } else {\n             // Get region reassigned\n-            regions.put(info.getRegionName(), info);\n+            regions.add(info);\n \n             // If it was pending, remove.\n             // Otherwise will obstruct its getting reassigned.\n@@ -2008,16 +2096,13 @@ private void scanMetaRegion(HRegionInterface server, long scannerId,\n       }\n \n       // Get regions reassigned\n-      for (Map.Entry<Text, HRegionInfo> e: regions.entrySet()) {\n-        Text region = e.getKey();\n-        HRegionInfo regionInfo = e.getValue();\n-        unassignedRegions.put(region, regionInfo);\n-        assignAttempts.put(region, Long.valueOf(0L));\n+      for (HRegionInfo info: regions) {\n+        unassignedRegions.put(info, ZERO_L);\n       }\n     }\n \n     @Override\n-    boolean process() throws IOException {\n+    protected boolean process() throws IOException {\n       LOG.info(\"process shutdown of server \" + deadServer + \": logSplit: \" +\n           this.logSplit + \", rootChecked: \" + this.rootChecked +\n           \", rootRescanned: \" + this.rootRescanned + \", numberOfMetaRegions: \" +\n@@ -2040,30 +2125,12 @@ boolean process() throws IOException {\n       }\n \n       if (!rootChecked) {\n-        boolean rootRegionUnavailable = false;\n-        if (rootRegionLocation.get() == null) {\n-          rootRegionUnavailable = true;\n-\n-        } else if (deadServer.equals(rootRegionLocation.get())) {\n-          // We should never get here because whenever an object of this type\n-          // is created, a check is made to see if it is the root server.\n-          // and unassignRootRegion() is called then. However, in the\n-          // unlikely event that we do end up here, let's do the right thing.\n-          unassignRootRegion();\n-          rootRegionUnavailable = true;\n-        }\n-        if (rootRegionUnavailable) {\n-          // We can't do anything until the root region is on-line, put\n-          // us back on the delay queue. Reset the future time at which\n-          // we expect to be released from the DelayQueue we're inserted\n-          // in on lease expiration.\n-          this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n-          shutdownQueue.put(this);\n-          \n-          // Return true so run() does not put us back on the msgQueue\n+        if (!rootAvailable()) {\n+          // Return true so that worker does not put this request back on the\n+          // toDoQueue.\n+          // rootAvailable() has already put it on the delayedToDoQueue\n           return true;\n         }\n-        rootChecked = true;\n       }\n \n       if (!rootRescanned) {\n@@ -2114,27 +2181,14 @@ boolean process() throws IOException {\n         }\n         rootRescanned = true;\n       }\n-\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"numberOfMetaRegions: \" + numberOfMetaRegions.get() +\n-            \", onlineMetaRegions.size(): \" + onlineMetaRegions.size());\n-      }\n-      if (numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n-        // We can't proceed because not all of the meta regions are online.\n-        // We can't block either because that would prevent the meta region\n-        // online message from being processed. In order to prevent spinning\n-        // in the run queue, put this request on the delay queue to give\n-        // other threads the opportunity to get the meta regions on-line.\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\n-              \"Requeuing shutdown because not all meta regions are online\");\n-        }\n-        this.expire = System.currentTimeMillis() + leaseTimeout / 2;\n-        shutdownQueue.put(this);\n-        \n-        // Return true so run() does not put us back on the msgQueue\n+      \n+      if (!metaTableAvailable()) {\n+        // We can't proceed because not all meta regions are online.\n+        // metaAvailable() has put this request on the delayedToDoQueue\n+        // Return true so that worker does not put this on the toDoQueue\n         return true;\n       }\n+\n       for (int tries = 0; tries < numRetries; tries++) {\n         try {\n           if (closed.get()) {\n@@ -2180,34 +2234,96 @@ boolean process() throws IOException {\n     }\n   }\n \n+  /**\n+   * Abstract class that performs common operations for \n+   * @see #ProcessRegionClose and @see #ProcessRegionOpen\n+   */\n+  private abstract class ProcessRegionStatusChange\n+    extends RegionServerOperation {\n+\n+    protected final boolean isMetaTable;\n+    protected final HRegionInfo regionInfo;\n+    private MetaRegion metaRegion;\n+    protected Text metaRegionName;\n+    \n+    /**\n+     * @param regionInfo\n+     */\n+    public ProcessRegionStatusChange(HRegionInfo regionInfo) {\n+      super();\n+      this.regionInfo = regionInfo;\n+      this.isMetaTable = regionInfo.isMetaTable();\n+      this.metaRegion = null;\n+      this.metaRegionName = null;\n+    }\n+    \n+    protected boolean metaRegionAvailable() {\n+      boolean available = true;\n+      if (isMetaTable) {\n+        // This operation is for the meta table\n+        if (!rootAvailable()) {\n+          // But we can't proceed unless the root region is available\n+          available = false;\n+        }\n+      } else {\n+        if (!rootScanned || !metaTableAvailable()) {\n+          // The root region has not been scanned or the meta table is not\n+          // available so we can't proceed.\n+          // Put the operation on the delayedToDoQueue\n+          requeue();\n+          available = false;\n+        }\n+      }\n+      return available;\n+    }\n+    \n+    protected HRegionInterface getMetaServer() throws IOException {\n+      if (this.isMetaTable) {\n+        this.metaRegionName = HRegionInfo.rootRegionInfo.getRegionName();\n+      } else {\n+        if (this.metaRegion == null) {\n+          synchronized (onlineMetaRegions) {\n+            metaRegion = onlineMetaRegions.size() == 1 ? \n+                onlineMetaRegions.get(onlineMetaRegions.firstKey()) :\n+                  onlineMetaRegions.containsKey(regionInfo.getRegionName()) ?\n+                      onlineMetaRegions.get(regionInfo.getRegionName()) :\n+                        onlineMetaRegions.get(onlineMetaRegions.headMap(\n+                            regionInfo.getRegionName()).lastKey());\n+          }\n+          this.metaRegionName = metaRegion.getRegionName();\n+        }\n+      }\n+\n+      HServerAddress server = null;\n+      if (isMetaTable) {\n+        server = rootRegionLocation.get();\n+        \n+      } else {\n+        server = metaRegion.getServer();\n+      }\n+      return connection.getHRegionConnection(server);\n+    }\n+    \n+  }\n   /**\n    * ProcessRegionClose is instantiated when a region server reports that it\n    * has closed a region.\n    */\n-  private class ProcessRegionClose extends RegionServerOperation {\n-    private HRegionInfo regionInfo;\n+  private class ProcessRegionClose extends ProcessRegionStatusChange {\n     private boolean reassignRegion;\n     private boolean deleteRegion;\n-    private boolean rootRegion;\n \n-    ProcessRegionClose(HRegionInfo regionInfo, boolean reassignRegion,\n+    /**\n+     * @param regionInfo\n+     * @param reassignRegion\n+     * @param deleteRegion\n+     */\n+    public ProcessRegionClose(HRegionInfo regionInfo, boolean reassignRegion,\n         boolean deleteRegion) {\n \n-      super();\n-\n-      this.regionInfo = regionInfo;\n+      super(regionInfo);\n       this.reassignRegion = reassignRegion;\n       this.deleteRegion = deleteRegion;\n-\n-      // If the region closing down is a meta region then we need to update\n-      // the ROOT table\n-\n-      if (this.regionInfo.getTableDesc().getName().equals(META_TABLE_NAME)) {\n-        this.rootRegion = true;\n-\n-      } else {\n-        this.rootRegion = false;\n-      }\n     }\n \n     /** {@inheritDoc} */\n@@ -2217,7 +2333,7 @@ public String toString() {\n     }\n \n     @Override\n-    boolean process() throws IOException {\n+    protected boolean process() throws IOException {\n       for (int tries = 0; tries < numRetries; tries++) {\n         if (closed.get()) {\n           return true;\n@@ -2226,50 +2342,15 @@ boolean process() throws IOException {\n \n         // Mark the Region as unavailable in the appropriate meta table\n \n-        Text metaRegionName;\n-        HRegionInterface server;\n-        if (rootRegion) {\n-          if (rootRegionLocation.get() == null || !rootScanned) {\n-            // We can't proceed until the root region is online and has been\n-            // scanned\n-            return false;\n-          }\n-          metaRegionName = HRegionInfo.rootRegionInfo.getRegionName();\n-          server = connection.getHRegionConnection(rootRegionLocation.get());\n-          onlineMetaRegions.remove(regionInfo.getStartKey());\n-\n-        } else {\n-          if (!rootScanned ||\n-              numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n-            \n-            // We can't proceed because not all of the meta regions are online.\n-            // We can't block either because that would prevent the meta region\n-            // online message from being processed. So return false to have this\n-            // operation requeued.\n-            \n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Requeuing close because rootScanned=\" +\n-                  rootScanned + \", numberOfMetaRegions=\" +\n-                  numberOfMetaRegions.get() + \", onlineMetaRegions.size()=\" +\n-                  onlineMetaRegions.size());\n-            }\n-            return false;\n-          }\n-\n-          MetaRegion r = null;\n-          synchronized (onlineMetaRegions) {\n-            if (onlineMetaRegions.containsKey(regionInfo.getRegionName())) {\n-              r = onlineMetaRegions.get(regionInfo.getRegionName());\n-\n-            } else {\n-              r = onlineMetaRegions.get(onlineMetaRegions.headMap(\n-                  regionInfo.getRegionName()).lastKey());\n-            }\n-          }\n-          metaRegionName = r.getRegionName();\n-          server = connection.getHRegionConnection(r.getServer());\n+        if (!metaRegionAvailable()) {\n+          // We can't proceed unless the meta region we are going to update\n+          // is online. metaRegionAvailable() has put this operation on the\n+          // delayedToDoQueue, so return true so the operation is not put \n+          // back on the toDoQueue\n+          return true;\n         }\n \n+        HRegionInterface server = getMetaServer();\n         try {\n           BatchUpdate b = new BatchUpdate(rand.nextLong());\n           long lockid = b.startUpdate(regionInfo.getRegionName());\n@@ -2298,8 +2379,7 @@ boolean process() throws IOException {\n       if (reassignRegion) {\n         LOG.info(\"reassign region: \" + regionInfo.getRegionName());\n \n-        unassignedRegions.put(regionInfo.getRegionName(), regionInfo);\n-        assignAttempts.put(regionInfo.getRegionName(), Long.valueOf(0L));\n+        unassignedRegions.put(regionInfo, ZERO_L);\n \n       } else if (deleteRegion) {\n         try {\n@@ -2320,19 +2400,18 @@ boolean process() throws IOException {\n    * serving a region. This applies to all meta and user regions except the \n    * root region which is handled specially.\n    */\n-  private class ProcessRegionOpen extends RegionServerOperation {\n-    private final boolean rootRegion;\n-    private final HRegionInfo region;\n+  private class ProcessRegionOpen extends ProcessRegionStatusChange {\n     private final HServerAddress serverAddress;\n     private final byte [] startCode;\n \n-    ProcessRegionOpen(HServerInfo info, HRegionInfo region)\n+    /**\n+     * @param info\n+     * @param regionInfo\n+     * @throws IOException\n+     */\n+    public ProcessRegionOpen(HServerInfo info, HRegionInfo regionInfo)\n     throws IOException {\n-      // If true, the region which just came on-line is a META region.\n-      // We need to look in the ROOT region for its information.  Otherwise,\n-      // its just an ordinary region. Look for it in the META table.\n-      this.rootRegion = region.getTableDesc().getName().equals(META_TABLE_NAME);\n-      this.region = region;\n+      super(regionInfo);\n       this.serverAddress = info.getServerAddress();\n       this.startCode = Writables.longToBytes(info.getStartCode());\n     }\n@@ -2344,72 +2423,40 @@ public String toString() {\n     }\n \n     @Override\n-    boolean process() throws IOException {\n+    protected boolean process() throws IOException {\n       for (int tries = 0; tries < numRetries; tries++) {\n         if (closed.get()) {\n           return true;\n         }\n-        LOG.info(region.toString() + \" open on \" + \n+        LOG.info(regionInfo.toString() + \" open on \" + \n             this.serverAddress.toString());\n \n-        // Register the newly-available Region's location.\n-        Text metaRegionName;\n-        HRegionInterface server;\n-        if (this.rootRegion) {\n-          if (rootRegionLocation.get() == null || !rootScanned) {\n-            // We can't proceed until root region is online and scanned\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"root region: \" + \n-                ((rootRegionLocation.get() != null)?\n-                  rootRegionLocation.get().toString(): \"null\") +\n-                \", rootScanned: \" + rootScanned);\n-            }\n-            return false;\n-          }\n-          metaRegionName = HRegionInfo.rootRegionInfo.getRegionName();\n-          server = connection.getHRegionConnection(rootRegionLocation.get());\n-        } else {\n-          if (!rootScanned ||\n-              numberOfMetaRegions.get() != onlineMetaRegions.size()) {\n-            // We can't proceed because not all of the meta regions are online.\n-            // We can't block either because that would prevent the meta region\n-            // online message from being processed. So return false to have this\n-            // operation requeued.\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Requeuing open because rootScanned: \" +\n-                  rootScanned + \", numberOfMetaRegions: \" +\n-                  numberOfMetaRegions.get() + \", onlineMetaRegions.size(): \" +\n-                  onlineMetaRegions.size());\n-            }\n-            return false;\n-          }\n-\n-          MetaRegion r = null;\n-          synchronized (onlineMetaRegions) {\n-            r = onlineMetaRegions.containsKey(region.getRegionName()) ?\n-                onlineMetaRegions.get(region.getRegionName()) :\n-                  onlineMetaRegions.get(onlineMetaRegions.headMap(\n-                      region.getRegionName()).lastKey());\n-          }\n-          metaRegionName = r.getRegionName();\n-          server = connection.getHRegionConnection(r.getServer());\n+        if (!metaRegionAvailable()) {\n+          // We can't proceed unless the meta region we are going to update\n+          // is online. metaRegionAvailable() has put this operation on the\n+          // delayedToDoQueue, so return true so the operation is not put \n+          // back on the toDoQueue\n+          return true;\n         }\n+\n+        // Register the newly-available Region's location.\n         \n-        LOG.info(\"updating row \" + region.getRegionName() + \" in table \" +\n+        HRegionInterface server = getMetaServer();\n+        LOG.info(\"updating row \" + regionInfo.getRegionName() + \" in table \" +\n           metaRegionName + \" with startcode \" +\n           Writables.bytesToLong(this.startCode) + \" and server \"+\n           serverAddress.toString());\n         try {\n           BatchUpdate b = new BatchUpdate(rand.nextLong());\n-          long lockid = b.startUpdate(region.getRegionName());\n+          long lockid = b.startUpdate(regionInfo.getRegionName());\n           b.put(lockid, COL_SERVER,\n             Writables.stringToBytes(serverAddress.toString()));\n           b.put(lockid, COL_STARTCODE, startCode);\n           server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n-          if (region.getTableDesc().getName().equals(META_TABLE_NAME)) {\n+          if (isMetaTable) {\n             // It's a meta region.\n             MetaRegion m = new MetaRegion(this.serverAddress,\n-              this.region.getRegionName(), this.region.getStartKey());\n+              this.regionInfo.getRegionName(), this.regionInfo.getStartKey());\n             if (!initialMetaScanComplete) {\n               // Put it on the queue to be scanned for the first time.\n               try {\n@@ -2422,11 +2469,11 @@ boolean process() throws IOException {\n             } else {\n               // Add it to the online meta regions\n               LOG.debug(\"Adding to onlineMetaRegions: \" + m.toString());\n-              onlineMetaRegions.put(this.region.getStartKey(), m);\n+              onlineMetaRegions.put(this.regionInfo.getStartKey(), m);\n             }\n           }\n           // If updated successfully, remove from pending list.\n-          pendingRegions.remove(region.getRegionName());\n+          pendingRegions.remove(regionInfo.getRegionName());\n           break;\n         } catch (IOException e) {\n           if (tries == numRetries - 1) {\n@@ -2449,19 +2496,8 @@ public boolean isMasterRunning() {\n \n   /** {@inheritDoc} */\n   public void shutdown() {\n-    TimerTask tt = new TimerTask() {\n-      @Override\n-      public void run() {\n-        closed.set(true);\n-        synchronized(msgQueue) {\n-          msgQueue.clear();                         // Empty the queue\n-          shutdownQueue.clear();                    // Empty shut down queue\n-          msgQueue.notifyAll();                     // Wake main thread\n-        }\n-      }\n-    };\n-    Timer t = new Timer(getName() + \"-Shutdown\");\n-    t.schedule(tt, 10);\n+    LOG.info(\"Cluster shutdown requested. Starting to quiesce servers\");\n+    this.shutdownRequested.set(true);\n   }\n \n   /** {@inheritDoc} */\n@@ -2563,8 +2599,7 @@ private void createTable(final HRegionInfo newRegion) throws IOException {\n \n       // 5. Get it assigned to a server\n \n-      this.unassignedRegions.put(regionName, info);\n-      this.assignAttempts.put(regionName, Long.valueOf(0L));\n+      this.unassignedRegions.put(info, ZERO_L);\n \n     } finally {\n       tableInCreation.remove(newRegion.getTableDesc().getName());\n@@ -2838,14 +2873,12 @@ protected void postProcessMeta(MetaRegion m, HRegionInterface server)\n         }\n \n         if (online) {                         // Bring offline regions on-line\n-          if (!unassignedRegions.containsKey(i.getRegionName())) {\n-            unassignedRegions.put(i.getRegionName(), i);\n-            assignAttempts.put(i.getRegionName(), Long.valueOf(0L));\n+          if (!unassignedRegions.containsKey(i)) {\n+            unassignedRegions.put(i, ZERO_L);\n           }\n \n         } else {                              // Prevent region from getting assigned.\n-          unassignedRegions.remove(i.getRegionName());\n-          assignAttempts.remove(i.getRegionName());\n+          unassignedRegions.remove(i);\n         }\n       }\n \n@@ -3069,7 +3102,7 @@ public void leaseExpired() {\n       // here because the new server will start serving the root region before\n       // the ProcessServerShutdown operation has a chance to split the log file.\n       if (info != null) {\n-        shutdownQueue.put(new ProcessServerShutdown(info));\n+        delayedToDoQueue.put(new ProcessServerShutdown(info));\n       }\n     }\n   }",
                "deletions": 350
            },
            {
                "sha": "e9137ba962ae47ffcf2f302b69f2e2c564357999",
                "filename": "src/java/org/apache/hadoop/hbase/HMsg.java",
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMsg.java",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HMsg.java",
                "status": "modified",
                "changes": 16,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMsg.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "patch": "@@ -48,6 +48,9 @@\n \n   /** Stop serving the specified region and don't report back that it's closed */\n   public static final byte MSG_REGION_CLOSE_WITHOUT_REPORT = 6;\n+  \n+  /** Stop serving user regions */\n+  public static final byte MSG_REGIONSERVER_QUIESCE = 7;\n \n   // Messages sent from the region server to the master\n   \n@@ -72,9 +75,12 @@\n    * region server is shutting down\n    * \n    * note that this message is followed by MSG_REPORT_CLOSE messages for each\n-   * region the region server was serving.\n+   * region the region server was serving, unless it was told to quiesce.\n    */\n   public static final byte MSG_REPORT_EXITING = 104;\n+  \n+  /** region server has closed all user regions but is still serving meta regions */\n+  public static final byte MSG_REPORT_QUIESCED = 105;\n \n   byte msg;\n   HRegionInfo info;\n@@ -148,6 +154,10 @@ public String toString() {\n       message.append(\"MSG_REGION_CLOSE_WITHOUT_REPORT : \");\n       break;\n       \n+    case MSG_REGIONSERVER_QUIESCE:\n+      message.append(\"MSG_REGIONSERVER_QUIESCE : \");\n+      break;\n+      \n     case MSG_REPORT_PROCESS_OPEN:\n       message.append(\"MSG_REPORT_PROCESS_OPEN : \");\n       break;\n@@ -168,6 +178,10 @@ public String toString() {\n       message.append(\"MSG_REPORT_EXITING : \");\n       break;\n       \n+    case MSG_REPORT_QUIESCED:\n+      message.append(\"MSG_REPORT_QUIESCED : \");\n+      break;\n+      \n     default:\n       message.append(\"unknown message code (\");\n       message.append(msg);",
                "deletions": 1
            },
            {
                "sha": "cb881c7a31b4471668cb7bd6b8825b239480eb60",
                "filename": "src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "status": "modified",
                "changes": 15,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionInfo.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "patch": "@@ -193,6 +193,21 @@ public HTableDescriptor getTableDesc(){\n     return tableDesc;\n   }\n   \n+  /** @return true if this is the root region */\n+  public boolean isRootRegion() {\n+    return this.tableDesc.isRootRegion();\n+  }\n+  \n+  /** @return true if this is the meta table */\n+  public boolean isMetaTable() {\n+    return this.tableDesc.isMetaTable();\n+  }\n+\n+  /** @return true if this region is a meta region */\n+  public boolean isMetaRegion() {\n+    return this.tableDesc.isMetaRegion();\n+  }\n+  \n   /**\n    * @return True if has been split and has daughters.\n    */",
                "deletions": 0
            },
            {
                "sha": "94c0ab3ee1e3e764a5627633be6acbaff83518bc",
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "status": "modified",
                "changes": 92,
                "additions": 82,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "patch": "@@ -81,6 +81,8 @@\n   // Chore threads need to know about the hosting class.\n   protected final AtomicBoolean stopRequested = new AtomicBoolean(false);\n   \n+  protected final AtomicBoolean quiesced = new AtomicBoolean(false);\n+  \n   // Go down hard.  Used if file system becomes unavailable and also in\n   // debugging and unit tests.\n   protected volatile boolean abortRequested;\n@@ -652,6 +654,7 @@ public HRegionServer(HServerAddress address, HBaseConfiguration conf)\n    * load/unload instructions.\n    */\n   public void run() {\n+    boolean quiesceRequested = false;\n     try {\n       init(reportForDuty());\n       long lastMsg = 0;\n@@ -682,16 +685,24 @@ public void run() {\n               HMsg msgs[] =\n                 this.hbaseMaster.regionServerReport(serverInfo, outboundArray);\n               lastMsg = System.currentTimeMillis();\n+              \n+              if (this.quiesced.get() && onlineRegions.size() == 0) {\n+                // We've just told the master we're exiting because we aren't\n+                // serving any regions. So set the stop bit and exit.\n+                LOG.info(\"Server quiesced and not serving any regions. \" +\n+                    \"Starting shutdown\");\n+                stopRequested.set(true);\n+                continue;\n+              }\n+              \n               // Queue up the HMaster's instruction stream for processing\n               boolean restart = false;\n               for(int i = 0; i < msgs.length && !stopRequested.get() &&\n                   !restart; i++) {\n                 switch(msgs[i].getMsg()) {\n                 \n                 case HMsg.MSG_CALL_SERVER_STARTUP:\n-                  if (LOG.isDebugEnabled()) {\n-                    LOG.debug(\"Got call server startup message\");\n-                  }\n+                  LOG.info(\"Got call server startup message\");\n                   // We the MSG_CALL_SERVER_STARTUP on startup but we can also\n                   // get it when the master is panicing because for instance\n                   // the HDFS has been yanked out from under it.  Be wary of\n@@ -725,11 +736,22 @@ public void run() {\n                   break;\n \n                 case HMsg.MSG_REGIONSERVER_STOP:\n-                  if (LOG.isDebugEnabled()) {\n-                    LOG.debug(\"Got regionserver stop message\");\n-                  }\n+                  LOG.info(\"Got regionserver stop message\");\n                   stopRequested.set(true);\n                   break;\n+                  \n+                case HMsg.MSG_REGIONSERVER_QUIESCE:\n+                  if (!quiesceRequested) {\n+                    LOG.info(\"Got quiesce server message\");\n+                    try {\n+                      toDo.put(new ToDoEntry(msgs[i]));\n+                    } catch (InterruptedException e) {\n+                      throw new RuntimeException(\"Putting into msgQueue was \" +\n+                        \"interrupted.\", e);\n+                    }\n+                    quiesceRequested = true;\n+                  }\n+                  break;\n \n                 default:\n                   if (fsOk) {\n@@ -1101,6 +1123,10 @@ public void run() {\n           try {\n             LOG.info(e.msg.toString());\n             switch(e.msg.getMsg()) {\n+            \n+            case HMsg.MSG_REGIONSERVER_QUIESCE:\n+              closeUserRegions();\n+              break;\n \n             case HMsg.MSG_REGION_OPEN:\n               // Open a region\n@@ -1149,12 +1175,19 @@ public void run() {\n     }\n   }\n   \n-  void openRegion(final HRegionInfo regionInfo) throws IOException {\n+  void openRegion(final HRegionInfo regionInfo) {\n     HRegion region = onlineRegions.get(regionInfo.getRegionName());\n     if(region == null) {\n-      region = new HRegion(new Path(this.conf.get(HConstants.HBASE_DIR)),\n-        this.log, FileSystem.get(conf), conf, regionInfo, null,\n-        this.cacheFlusher);\n+      try {\n+        region = new HRegion(new Path(this.conf.get(HConstants.HBASE_DIR)),\n+            this.log, FileSystem.get(conf), conf, regionInfo, null,\n+            this.cacheFlusher);\n+        \n+      } catch (IOException e) {\n+        LOG.error(\"error opening region \" + regionInfo.getRegionName(), e);\n+        reportClose(region);\n+        return;\n+      }\n       this.lock.writeLock().lock();\n       try {\n         this.log.setSequenceNumber(region.getMinSequenceId());\n@@ -1208,6 +1241,45 @@ void closeRegion(final HRegionInfo hri, final boolean reportWhenCompleted)\n     return regionsToClose;\n   }\n \n+  /** Called as the first stage of cluster shutdown. */\n+  void closeUserRegions() {\n+    ArrayList<HRegion> regionsToClose = new ArrayList<HRegion>();\n+    this.lock.writeLock().lock();\n+    try {\n+      synchronized (onlineRegions) {\n+        for (Iterator<Map.Entry<Text, HRegion>> i =\n+          onlineRegions.entrySet().iterator();\n+        i.hasNext();) {\n+          Map.Entry<Text, HRegion> e = i.next();\n+          HRegion r = e.getValue();\n+          if (!r.getRegionInfo().isMetaRegion()) {\n+            regionsToClose.add(r);\n+            i.remove();\n+          }\n+        }\n+      }\n+    } finally {\n+      this.lock.writeLock().unlock();\n+    }\n+    for(HRegion region: regionsToClose) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"closing region \" + region.getRegionName());\n+      }\n+      try {\n+        region.close(false);\n+      } catch (IOException e) {\n+        LOG.error(\"error closing region \" + region.getRegionName(),\n+          RemoteExceptionHandler.checkIOException(e));\n+      }\n+    }\n+    this.quiesced.set(true);\n+    if (onlineRegions.size() == 0) {\n+      outboundMsgs.add(new HMsg(HMsg.MSG_REPORT_EXITING));\n+    } else {\n+      outboundMsgs.add(new HMsg(HMsg.MSG_REPORT_QUIESCED));\n+    }\n+  }\n+\n   //\n   // HRegionInterface\n   //",
                "deletions": 10
            },
            {
                "sha": "03a97dc76f81bff3746ce80293db6545ee39a609",
                "filename": "src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "blob_url": "https://github.com/apache/hbase/blob/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "raw_url": "https://github.com/apache/hbase/raw/612f446dbdd151a02700839705c5ebfd1fc33e2a/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "status": "modified",
                "changes": 30,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTableDescriptor.java?ref=612f446dbdd151a02700839705c5ebfd1fc33e2a",
                "patch": "@@ -52,7 +52,8 @@\n             HColumnDescriptor.CompressionType.NONE, false, Integer.MAX_VALUE,\n             null));\n   \n-\n+  private boolean rootregion;\n+  private boolean metaregion;\n   private Text name;\n   // TODO: Does this need to be a treemap?  Can it be a HashMap?\n   private final TreeMap<Text, HColumnDescriptor> families;\n@@ -69,6 +70,8 @@\n \n   /** Used to construct the table descriptors for root and meta tables */\n   private HTableDescriptor(Text name, HColumnDescriptor family) {\n+    rootregion = name.equals(HConstants.ROOT_TABLE_NAME);\n+    this.metaregion = true;\n     this.name = new Text(name);\n     this.families = new TreeMap<Text, HColumnDescriptor>();\n     families.put(family.getName(), family);\n@@ -92,13 +95,30 @@ public HTableDescriptor() {\n    * <code>[a-zA-Z_0-9]\n    */\n   public HTableDescriptor(String name) {\n+    this();\n     Matcher m = LEGAL_TABLE_NAME.matcher(name);\n     if (m == null || !m.matches()) {\n       throw new IllegalArgumentException(\n           \"Table names can only contain 'word characters': i.e. [a-zA-Z_0-9\");\n     }\n-    this.name = new Text(name);\n-    this.families = new TreeMap<Text, HColumnDescriptor>();\n+    this.name.set(name);\n+    this.rootregion = false;\n+    this.metaregion = false;\n+  }\n+  \n+  /** @return true if this is the root region */\n+  public boolean isRootRegion() {\n+    return rootregion;\n+  }\n+  \n+  /** @return true if table is the meta table */\n+  public boolean isMetaTable() {\n+    return metaregion && !rootregion;\n+  }\n+  \n+  /** @return true if this is a meta region (part of the root or meta tables) */\n+  public boolean isMetaRegion() {\n+    return metaregion;\n   }\n \n   /** @return name of table */\n@@ -165,6 +185,8 @@ public int hashCode() {\n \n   /** {@inheritDoc} */\n   public void write(DataOutput out) throws IOException {\n+    out.writeBoolean(rootregion);\n+    out.writeBoolean(metaregion);\n     name.write(out);\n     out.writeInt(families.size());\n     for(Iterator<HColumnDescriptor> it = families.values().iterator();\n@@ -175,6 +197,8 @@ public void write(DataOutput out) throws IOException {\n \n   /** {@inheritDoc} */\n   public void readFields(DataInput in) throws IOException {\n+    this.rootregion = in.readBoolean();\n+    this.metaregion = in.readBoolean();\n     this.name.readFields(in);\n     int numCols = in.readInt();\n     families.clear();",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1888 NullPointerException in HMemcacheScanner (reprise)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@575986 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/20a6c00b5cafd152c84a5d35ab66420805d75e15",
        "parent": "https://github.com/apache/hbase/commit/b271048e2f86b9d186be6b71c504c0dda92c0966",
        "bug_id": "hbase_125",
        "file": [
            {
                "sha": "cf5f737b16174b200a3066b83790b8304d5a0fb1",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "patch": "@@ -47,7 +47,7 @@ Trunk (unreleased changes)\n     HADOOP-1847 Many HBase tests do not fail well. (phase 2)\n     HADOOP-1870 Once file system failure has been detected, don't check it again\n                 and get on with shutting down the hbase cluster.\n-    HADOOP-1888 NullPointerException in HMemcacheScanner\n+    HADOOP-1888 NullPointerException in HMemcacheScanner (reprise)\n     HADOOP-1903 Possible data loss if Exception happens between snapshot and flush\n                 to disk.\n ",
                "deletions": 1
            },
            {
                "sha": "852582733d737031bd820e79d119adf0fc8bcddc",
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "status": "modified",
                "changes": 94,
                "additions": 53,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -43,10 +44,10 @@\n   // Note that since these structures are always accessed with a lock held,\n   // no additional synchronization is required.\n   \n-  TreeMap<HStoreKey, byte []> memcache = new TreeMap<HStoreKey, byte []>();\n-  final ArrayList<TreeMap<HStoreKey, byte []>> history =\n-    new ArrayList<TreeMap<HStoreKey, byte []>>();\n-  TreeMap<HStoreKey, byte []> snapshot = null;\n+  volatile SortedMap<HStoreKey, byte []> memcache;\n+  List<SortedMap<HStoreKey, byte []>> history =\n+    Collections.synchronizedList(new ArrayList<SortedMap<HStoreKey, byte []>>());\n+  volatile SortedMap<HStoreKey, byte []> snapshot = null;\n \n   final HLocking lock = new HLocking();\n   \n@@ -62,14 +63,16 @@\n    */\n   public HMemcache() {\n     super();\n+    memcache  =\n+      Collections.synchronizedSortedMap(new TreeMap<HStoreKey, byte []>());\n   }\n \n   /** represents the state of the memcache at a specified point in time */\n   static class Snapshot {\n-    final TreeMap<HStoreKey, byte []> memcacheSnapshot;\n+    final SortedMap<HStoreKey, byte []> memcacheSnapshot;\n     final long sequenceId;\n     \n-    Snapshot(final TreeMap<HStoreKey, byte[]> memcache, final Long i) {\n+    Snapshot(final SortedMap<HStoreKey, byte[]> memcache, final Long i) {\n       super();\n       this.memcacheSnapshot = memcache;\n       this.sequenceId = i.longValue();\n@@ -103,8 +106,11 @@ Snapshot snapshotMemcacheForLog(HLog log) throws IOException {\n         new Snapshot(memcache, Long.valueOf(log.startCacheFlush()));\n       // From here on, any failure is catastrophic requiring replay of hlog\n       this.snapshot = memcache;\n-      history.add(memcache);\n-      memcache = new TreeMap<HStoreKey, byte []>();\n+      synchronized (history) {\n+        history.add(memcache);\n+      }\n+      memcache =\n+        Collections.synchronizedSortedMap(new TreeMap<HStoreKey, byte []>());\n       // Reset size of this memcache.\n       this.size.set(0);\n       return retval;\n@@ -126,14 +132,8 @@ public void deleteSnapshot() throws IOException {\n       if(snapshot == null) {\n         throw new IOException(\"Snapshot not present!\");\n       }\n-      for (Iterator<TreeMap<HStoreKey, byte []>> it = history.iterator(); \n-        it.hasNext(); ) {\n-        \n-        TreeMap<HStoreKey, byte []> cur = it.next();\n-        if (snapshot == cur) {\n-          it.remove();\n-          break;\n-        }\n+      synchronized (history) {\n+        history.remove(snapshot);\n       }\n       this.snapshot = null;\n     } finally {\n@@ -182,12 +182,14 @@ public long getSize() {\n     this.lock.obtainReadLock();\n     try {\n       ArrayList<byte []> results = get(memcache, key, numVersions);\n-      for (int i = history.size() - 1; i >= 0; i--) {\n-        if (numVersions > 0 && results.size() >= numVersions) {\n-          break;\n+      synchronized (history) {\n+        for (int i = history.size() - 1; i >= 0; i--) {\n+          if (numVersions > 0 && results.size() >= numVersions) {\n+            break;\n+          }\n+          results.addAll(results.size(),\n+              get(history.get(i), key, numVersions - results.size()));\n         }\n-        results.addAll(results.size(),\n-            get(history.get(i), key, numVersions - results.size()));\n       }\n       return (results.size() == 0) ? null :\n         ImmutableBytesWritable.toArray(results);\n@@ -210,9 +212,11 @@ public long getSize() {\n     this.lock.obtainReadLock();\n     try {\n       internalGetFull(memcache, key, results);\n-      for (int i = history.size() - 1; i >= 0; i--) {\n-        TreeMap<HStoreKey, byte []> cur = history.get(i);\n-        internalGetFull(cur, key, results);\n+      synchronized (history) {\n+        for (int i = history.size() - 1; i >= 0; i--) {\n+          SortedMap<HStoreKey, byte []> cur = history.get(i);\n+          internalGetFull(cur, key, results);\n+        }\n       }\n       return results;\n       \n@@ -221,7 +225,7 @@ public long getSize() {\n     }\n   }\n   \n-  void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key, \n+  void internalGetFull(SortedMap<HStoreKey, byte []> map, HStoreKey key, \n       TreeMap<Text, byte []> results) {\n     SortedMap<HStoreKey, byte []> tailMap = map.tailMap(key);\n     for (Map.Entry<HStoreKey, byte []> es: tailMap.entrySet()) {\n@@ -252,7 +256,7 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n    * @return Ordered list of items found in passed <code>map</code>.  If no\n    * matching values, returns an empty list (does not return null).\n    */\n-  ArrayList<byte []> get(final TreeMap<HStoreKey, byte []> map,\n+  ArrayList<byte []> get(final SortedMap<HStoreKey, byte []> map,\n       final HStoreKey key, final int numVersions) {\n     ArrayList<byte []> result = new ArrayList<byte []>();\n     // TODO: If get is of a particular version -- numVersions == 1 -- we\n@@ -289,10 +293,12 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n     this.lock.obtainReadLock();\n     try {\n       List<HStoreKey> results = getKeys(this.memcache, origin, versions);\n-      for (int i = history.size() - 1; i >= 0; i--) {\n-        results.addAll(results.size(), getKeys(history.get(i), origin,\n-            versions == HConstants.ALL_VERSIONS ? versions :\n-              (versions - results.size())));\n+      synchronized (history) {\n+        for (int i = history.size() - 1; i >= 0; i--) {\n+          results.addAll(results.size(), getKeys(history.get(i), origin,\n+              versions == HConstants.ALL_VERSIONS ? versions :\n+                (versions - results.size())));\n+        }\n       }\n       return results;\n     } finally {\n@@ -308,7 +314,7 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n    * equal or older timestamp.  If no keys, returns an empty List. Does not\n    * return null.\n    */\n-  private List<HStoreKey> getKeys(final TreeMap<HStoreKey, byte []> map,\n+  private List<HStoreKey> getKeys(final SortedMap<HStoreKey, byte []> map,\n       final HStoreKey origin, final int versions) {\n     List<HStoreKey> result = new ArrayList<HStoreKey>();\n     SortedMap<HStoreKey, byte []> tailMap = map.tailMap(origin);\n@@ -360,7 +366,7 @@ HInternalScannerInterface getScanner(long timestamp,\n   //////////////////////////////////////////////////////////////////////////////\n \n   class HMemcacheScanner extends HAbstractScanner {\n-    final TreeMap<HStoreKey, byte []> backingMaps[];\n+    SortedMap<HStoreKey, byte []> backingMaps[];\n     final Iterator<HStoreKey> keyIterators[];\n \n     @SuppressWarnings(\"unchecked\")\n@@ -370,14 +376,16 @@ HInternalScannerInterface getScanner(long timestamp,\n       super(timestamp, targetCols);\n       lock.obtainReadLock();\n       try {\n-        this.backingMaps = new TreeMap[history.size() + 1];\n+        synchronized (history) {\n+          this.backingMaps = new SortedMap[history.size() + 1];\n \n-        // Note that since we iterate through the backing maps from 0 to n, we\n-        // need to put the memcache first, the newest history second, ..., etc.\n+          // Note that since we iterate through the backing maps from 0 to n, we\n+          // need to put the memcache first, the newest history second, ..., etc.\n \n-        backingMaps[0] = memcache;\n-        for (int i = history.size() - 1; i > 0; i--) {\n-          backingMaps[i + 1] = history.get(i);\n+          backingMaps[0] = memcache;\n+          for (int i = history.size() - 1; i >= 0; i--) {\n+            backingMaps[i + 1] = history.get(i);\n+          }\n         }\n \n         this.keyIterators = new Iterator[backingMaps.length];\n@@ -388,9 +396,13 @@ HInternalScannerInterface getScanner(long timestamp,\n         \n         HStoreKey firstKey = new HStoreKey(firstRow);\n         for (int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = firstRow.getLength() != 0 ?\n-              backingMaps[i].tailMap(firstKey).keySet().iterator() :\n-                backingMaps[i].keySet().iterator();\n+          if (firstRow != null && firstRow.getLength() != 0) {\n+            keyIterators[i] =\n+              backingMaps[i].tailMap(firstKey).keySet().iterator();\n+            \n+          } else {\n+            keyIterators[i] = backingMaps[i].keySet().iterator();\n+          }\n \n           while (getNext(i)) {\n             if (!findFirstRow(i, firstRow)) {",
                "deletions": 41
            },
            {
                "sha": "69911669920ecb8added94603be0e31ff5924035",
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HRegion.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "patch": "@@ -1615,6 +1615,7 @@ public boolean isMultipleMatchScanner() {\n       return multipleMatchers;\n     }\n \n+    /** {@inheritDoc} */\n     public boolean next(HStoreKey key, SortedMap<Text, byte[]> results)\n     throws IOException {\n       // Filtered flag is set by filters.  If a cell has been 'filtered out'",
                "deletions": 0
            },
            {
                "sha": "cd88bf4e9d49ae6c42c2a88c2d6cdf1535e7b731",
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/java/org/apache/hadoop/hbase/HStore.java",
                "status": "modified",
                "changes": 38,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "patch": "@@ -31,6 +31,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n+import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.Vector;\n import java.util.Map.Entry;\n@@ -439,13 +440,13 @@ private void flushBloomFilter() throws IOException {\n    * @param logCacheFlushId flush sequence number\n    * @throws IOException\n    */\n-  void flushCache(final TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCache(final SortedMap<HStoreKey, byte []> inputCache,\n     final long logCacheFlushId)\n   throws IOException {\n     flushCacheHelper(inputCache, logCacheFlushId, true);\n   }\n   \n-  void flushCacheHelper(TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCacheHelper(SortedMap<HStoreKey, byte []> inputCache,\n       long logCacheFlushId, boolean addToAvailableMaps)\n   throws IOException {\n     synchronized(flushLock) {\n@@ -1123,7 +1124,7 @@ void getFull(HStoreKey key, TreeMap<Text, byte []> results)\n    * @param key\n    * @param numVersions Number of versions to fetch.  Must be > 0.\n    * @param memcache Checked for deletions\n-   * @return\n+   * @return values for the specified versions\n    * @throws IOException\n    */\n   byte [][] get(HStoreKey key, int numVersions, final HMemcache memcache)\n@@ -1171,10 +1172,11 @@ void getFull(HStoreKey key, TreeMap<Text, byte []> results)\n               break;\n             }\n           }\n-          while ((readval = new ImmutableBytesWritable()) != null &&\n+          for (readval = new ImmutableBytesWritable();\n               map.next(readkey, readval) &&\n               readkey.matchesRowCol(key) &&\n-              !hasEnoughVersions(numVersions, results)) {\n+              !hasEnoughVersions(numVersions, results);\n+              readval = new ImmutableBytesWritable()) {\n             if (!isDeleted(readkey, readval.get(), memcache, deletes)) {\n               results.add(readval.get());\n             }\n@@ -1212,10 +1214,11 @@ private boolean hasEnoughVersions(final int numVersions,\n    * @throws IOException\n    */\n   List<HStoreKey> getKeys(final HStoreKey origin, List<HStoreKey> allKeys,\n-      final int versions)\n-  throws IOException {\n-    if (allKeys == null) {\n-      allKeys = new ArrayList<HStoreKey>();\n+      final int versions) throws IOException {\n+    \n+    List<HStoreKey> keys = allKeys;\n+    if (keys == null) {\n+      keys = new ArrayList<HStoreKey>();\n     }\n     // This code below is very close to the body of the get method.\n     this.lock.obtainReadLock();\n@@ -1238,23 +1241,24 @@ private boolean hasEnoughVersions(final int numVersions,\n             continue;\n           }\n           if (!isDeleted(readkey, readval.get(), null, null) &&\n-              !allKeys.contains(readkey)) {\n-            allKeys.add(new HStoreKey(readkey));\n+              !keys.contains(readkey)) {\n+            keys.add(new HStoreKey(readkey));\n           }\n-          while ((readval = new ImmutableBytesWritable()) != null &&\n+          for (readval = new ImmutableBytesWritable();\n               map.next(readkey, readval) &&\n-              readkey.matchesRowCol(origin)) {\n+              readkey.matchesRowCol(origin);\n+              readval = new ImmutableBytesWritable()) {\n             if (!isDeleted(readkey, readval.get(), null, null) &&\n-                !allKeys.contains(readkey)) {\n-              allKeys.add(new HStoreKey(readkey));\n-              if (versions != ALL_VERSIONS && allKeys.size() >= versions) {\n+                !keys.contains(readkey)) {\n+              keys.add(new HStoreKey(readkey));\n+              if (versions != ALL_VERSIONS && keys.size() >= versions) {\n                 break;\n               }\n             }\n           }\n         }\n       }\n-      return allKeys;\n+      return keys;\n     } finally {\n       this.lock.releaseReadLock();\n     }",
                "deletions": 17
            },
            {
                "sha": "093d3594eb0f76953c038cbde3ee7863298367cf",
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "status": "modified",
                "changes": 28,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "patch": "@@ -50,7 +50,8 @@\n   private FileSystem fs;\n   private Path parentdir;\n   private MasterThread masterThread = null;\n-  ArrayList<RegionServerThread> regionThreads;\n+  ArrayList<RegionServerThread> regionThreads =\n+    new ArrayList<RegionServerThread>();\n   private boolean deleteOnExit = true;\n \n   /**\n@@ -125,7 +126,7 @@ private void init(final int nRegionNodes) throws IOException {\n       this.parentdir = new Path(conf.get(HBASE_DIR, DEFAULT_HBASE_DIR));\n       fs.mkdirs(parentdir);\n       this.masterThread = startMaster(this.conf);\n-      this.regionThreads = startRegionServers(this.conf, nRegionNodes);\n+      this.regionThreads.addAll(startRegionServers(this.conf, nRegionNodes));\n     } catch(IOException e) {\n       shutdown();\n       throw e;\n@@ -357,17 +358,15 @@ public static void shutdown(final MasterThread masterThread,\n     if(masterThread != null) {\n       masterThread.getMaster().shutdown();\n     }\n-    if (regionServerThreads != null) {\n-      synchronized(regionServerThreads) {\n-        if (regionServerThreads != null) {\n-          for(Thread t: regionServerThreads) {\n-            if (t.isAlive()) {\n-              try {\n-                t.join();\n-              } catch (InterruptedException e) {\n-                // continue\n-              }\n-            }\n+    // regionServerThreads can never be null because they are initialized when\n+    // the class is constructed.\n+    synchronized(regionServerThreads) {\n+      for(Thread t: regionServerThreads) {\n+        if (t.isAlive()) {\n+          try {\n+            t.join();\n+          } catch (InterruptedException e) {\n+            // continue\n           }\n         }\n       }\n@@ -381,8 +380,7 @@ public static void shutdown(final MasterThread masterThread,\n     }\n     LOG.info(\"Shutdown \" +\n       ((masterThread != null)? masterThread.getName(): \"0 masters\") + \" \" +\n-      ((regionServerThreads == null)? 0: regionServerThreads.size()) +\n-      \" region server(s)\");\n+      regionServerThreads.size() + \" region server(s)\");\n   }\n   \n   void shutdown() {",
                "deletions": 15
            },
            {
                "sha": "68c8338ef81fa13578e93c1cc3244cd445e4c27c",
                "filename": "src/test/org/apache/hadoop/hbase/TestHMemcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/TestHMemcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/20a6c00b5cafd152c84a5d35ab66420805d75e15/src/test/org/apache/hadoop/hbase/TestHMemcache.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHMemcache.java?ref=20a6c00b5cafd152c84a5d35ab66420805d75e15",
                "patch": "@@ -22,6 +22,7 @@\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n import java.util.Map;\n+import java.util.SortedMap;\n import java.util.TreeMap;\n \n import junit.framework.TestCase;\n@@ -97,7 +98,7 @@ private Snapshot runSnapshot(final HMemcache hmc, final HLog log)\n     \n     // Save off old state.\n     int oldHistorySize = hmc.history.size();\n-    TreeMap<HStoreKey, byte []> oldMemcache = hmc.memcache;\n+    SortedMap<HStoreKey, byte []> oldMemcache = hmc.memcache;\n     // Run snapshot.\n     Snapshot s = hmc.snapshotMemcacheForLog(log);\n     // Make some assertions about what just happened.",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1888 NullPointerException in HMemcacheScanner\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@575791 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/1596f8aa10e3af2549c406a90a04ab85447830ea",
        "parent": "https://github.com/apache/hbase/commit/d660cfc5269085dcc5da75ab2f8fab098a31f177",
        "bug_id": "hbase_126",
        "file": [
            {
                "sha": "e47f49259ce3a027625f62f85bcc59b3f6300e61",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "patch": "@@ -46,6 +46,7 @@ Trunk (unreleased changes)\n     HADOOP-1847 Many HBase tests do not fail well. (phase 2)\n     HADOOP-1870 Once file system failure has been detected, don't check it again\n                 and get on with shutting down the hbase cluster.\n+    HADOOP-1888 NullPointerException in HMemcacheScanner\n \n   IMPROVEMENTS\n     HADOOP-1737 Make HColumnDescriptor data publically members settable",
                "deletions": 0
            },
            {
                "sha": "71f53173e46074ff2826ca694f383fe11d10fb63",
                "filename": "src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "patch": "@@ -28,7 +28,7 @@\n   /** constructor */\n   public HBaseConfiguration() {\n     super();\n-    addDefaultResource(\"hbase-default.xml\");\n-    addDefaultResource(\"hbase-site.xml\");\n+    addResource(\"hbase-default.xml\");\n+    addResource(\"hbase-site.xml\");\n   }\n }",
                "deletions": 2
            },
            {
                "sha": "5d7aa06a4446d12d4ede629cdd83ff30e0d46d11",
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMaster.java",
                "status": "modified",
                "changes": 7,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "patch": "@@ -897,7 +897,7 @@ public HMaster(Path dir, HServerAddress address, Configuration conf)\n       \n     } catch (IOException e) {\n       LOG.fatal(\"Not starting HMaster because:\", e);\n-      return;\n+      throw e;\n     }\n \n     this.threadWakeFrequency = conf.getLong(THREAD_WAKE_FREQUENCY, 10 * 1000);\n@@ -1145,6 +1145,11 @@ public void run() {\n    * by remote region servers have expired.\n    */\n   private void letRegionServersShutdown() {\n+    if (!fsOk) {\n+      // Forget waiting for the region servers if the file system has gone\n+      // away. Just exit as quickly as possible.\n+      return;\n+    }\n     synchronized (serversToServerInfo) {\n       while (this.serversToServerInfo.size() > 0) {\n         LOG.info(\"Waiting on following regionserver(s) to go down (or \" +",
                "deletions": 1
            },
            {
                "sha": "b029e987b92c48bb322b7473f71bcaec5b84eddc",
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "status": "modified",
                "changes": 70,
                "additions": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "patch": "@@ -26,7 +26,6 @@\n import java.util.Map;\n import java.util.SortedMap;\n import java.util.TreeMap;\n-import java.util.Vector;\n import java.util.concurrent.atomic.AtomicLong;\n \n import org.apache.commons.logging.Log;\n@@ -40,10 +39,13 @@\n  */\n public class HMemcache {\n   static final Log LOG = LogFactory.getLog(HMemcache.class);\n-  TreeMap<HStoreKey, byte []> memcache =\n-    new TreeMap<HStoreKey, byte []>();\n-  final Vector<TreeMap<HStoreKey, byte []>> history\n-    = new Vector<TreeMap<HStoreKey, byte []>>();\n+  \n+  // Note that since these structures are always accessed with a lock held,\n+  // no additional synchronization is required.\n+  \n+  TreeMap<HStoreKey, byte []> memcache = new TreeMap<HStoreKey, byte []>();\n+  final ArrayList<TreeMap<HStoreKey, byte []>> history =\n+    new ArrayList<TreeMap<HStoreKey, byte []>>();\n   TreeMap<HStoreKey, byte []> snapshot = null;\n \n   final HLocking lock = new HLocking();\n@@ -124,7 +126,8 @@ public void deleteSnapshot() throws IOException {\n         throw new IOException(\"Snapshot not present!\");\n       }\n       for (Iterator<TreeMap<HStoreKey, byte []>> it = history.iterator(); \n-          it.hasNext();) {\n+        it.hasNext(); ) {\n+        \n         TreeMap<HStoreKey, byte []> cur = it.next();\n         if (snapshot == cur) {\n           it.remove();\n@@ -183,10 +186,11 @@ public long getSize() {\n           break;\n         }\n         results.addAll(results.size(),\n-          get(history.elementAt(i), key, numVersions - results.size()));\n+            get(history.get(i), key, numVersions - results.size()));\n       }\n-      return (results.size() == 0)?\n-        null: ImmutableBytesWritable.toArray(results);\n+      return (results.size() == 0) ? null :\n+        ImmutableBytesWritable.toArray(results);\n+      \n     } finally {\n       this.lock.releaseReadLock();\n     }\n@@ -205,8 +209,8 @@ public long getSize() {\n     this.lock.obtainReadLock();\n     try {\n       internalGetFull(memcache, key, results);\n-      for (int i = history.size()-1; i >= 0; i--) {\n-        TreeMap<HStoreKey, byte []> cur = history.elementAt(i);\n+      for (int i = history.size() - 1; i >= 0; i--) {\n+        TreeMap<HStoreKey, byte []> cur = history.get(i);\n         internalGetFull(cur, key, results);\n       }\n       return results;\n@@ -285,9 +289,9 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n     try {\n       List<HStoreKey> results = getKeys(this.memcache, origin, versions);\n       for (int i = history.size() - 1; i >= 0; i--) {\n-        results.addAll(results.size(), getKeys(history.elementAt(i), origin,\n-          versions == HConstants.ALL_VERSIONS? versions:\n-           (results != null? versions - results.size(): versions)));\n+        results.addAll(results.size(), getKeys(history.get(i), origin,\n+            versions == HConstants.ALL_VERSIONS ? versions :\n+              (versions - results.size())));\n       }\n       return results;\n     } finally {\n@@ -345,9 +349,8 @@ boolean isDeleted(final byte [] value) {\n    * Return a scanner over the keys in the HMemcache\n    */\n   HInternalScannerInterface getScanner(long timestamp,\n-      Text targetCols[], Text firstRow)\n-  throws IOException {  \n-    return new HMemcacheScanner(timestamp, targetCols, firstRow);\n+      Text targetCols[], Text firstRow) throws IOException {  \n+      return new HMemcacheScanner(timestamp, targetCols, firstRow);\n   }\n \n   //////////////////////////////////////////////////////////////////////////////\n@@ -361,35 +364,38 @@ HInternalScannerInterface getScanner(long timestamp,\n \n     @SuppressWarnings(\"unchecked\")\n     HMemcacheScanner(final long timestamp, final Text targetCols[],\n-        final Text firstRow)\n-    throws IOException {\n+        final Text firstRow) throws IOException {\n+\n       super(timestamp, targetCols);\n       lock.obtainReadLock();\n       try {\n         this.backingMaps = new TreeMap[history.size() + 1];\n-        \n-        //NOTE: Since we iterate through the backing maps from 0 to n, we need\n-        // to put the memcache first, the newest history second, ..., etc.\n+\n+        // Note that since we iterate through the backing maps from 0 to n, we\n+        // need to put the memcache first, the newest history second, ..., etc.\n+\n         backingMaps[0] = memcache;\n-        for(int i = history.size() - 1; i > 0; i--) {\n-          backingMaps[i] = history.elementAt(i);\n+        for (int i = history.size() - 1; i > 0; i--) {\n+          backingMaps[i + 1] = history.get(i);\n         }\n-      \n+\n         this.keyIterators = new Iterator[backingMaps.length];\n         this.keys = new HStoreKey[backingMaps.length];\n         this.vals = new byte[backingMaps.length][];\n \n         // Generate list of iterators\n+        \n         HStoreKey firstKey = new HStoreKey(firstRow);\n-        for(int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = (/*firstRow != null &&*/ firstRow.getLength() != 0)?\n-            backingMaps[i].tailMap(firstKey).keySet().iterator():\n-            backingMaps[i].keySet().iterator();\n-          while(getNext(i)) {\n-            if(! findFirstRow(i, firstRow)) {\n+        for (int i = 0; i < backingMaps.length; i++) {\n+          keyIterators[i] = firstRow.getLength() != 0 ?\n+              backingMaps[i].tailMap(firstKey).keySet().iterator() :\n+                backingMaps[i].keySet().iterator();\n+\n+          while (getNext(i)) {\n+            if (!findFirstRow(i, firstRow)) {\n               continue;\n             }\n-            if(columnMatch(i)) {\n+            if (columnMatch(i)) {\n               break;\n             }\n           }",
                "deletions": 32
            },
            {
                "sha": "c3fd41d059d95e35f7a9c6b1e80c5388713808ee",
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "status": "modified",
                "changes": 42,
                "additions": 25,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "patch": "@@ -676,8 +676,10 @@ public void run() {\n                 if (LOG.isDebugEnabled()) {\n                   LOG.debug(\"Got call server startup message\");\n                 }\n-                closeAllRegions();\n-                restart = true;\n+                if (fsOk) {\n+                  closeAllRegions();\n+                  restart = true;\n+                }\n                 break;\n \n               case HMsg.MSG_REGIONSERVER_STOP:\n@@ -689,10 +691,12 @@ public void run() {\n                 break;\n \n               default:\n-                try {\n-                  toDo.put(new ToDoEntry(msgs[i]));\n-                } catch (InterruptedException e) {\n-                  throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+                if (fsOk) {\n+                  try {\n+                    toDo.put(new ToDoEntry(msgs[i]));\n+                  } catch (InterruptedException e) {\n+                    throw new RuntimeException(\"Putting into msgQueue was interrupted.\", e);\n+                  }\n                 }\n               }\n             }\n@@ -747,20 +751,24 @@ public void run() {\n     }\n \n     if (abortRequested) {\n-      try {\n-        log.close();\n-        LOG.info(\"On abort, closed hlog\");\n-      } catch (IOException e) {\n-        if (e instanceof RemoteException) {\n-          try {\n-            e = RemoteExceptionHandler.decodeRemoteException((RemoteException) e);\n-          } catch (IOException ex) {\n-            e = ex;\n+      if (fsOk) {\n+        // Only try to clean up if the file system is available\n+\n+        try {\n+          log.close();\n+          LOG.info(\"On abort, closed hlog\");\n+        } catch (IOException e) {\n+          if (e instanceof RemoteException) {\n+            try {\n+              e = RemoteExceptionHandler.decodeRemoteException((RemoteException) e);\n+            } catch (IOException ex) {\n+              e = ex;\n+            }\n           }\n+          LOG.error(\"Unable to close log in abort\", e);\n         }\n-        LOG.error(\"Unable to close log in abort\", e);\n+        closeAllRegions(); // Don't leave any open file handles\n       }\n-      closeAllRegions(); // Don't leave any open file handles\n       LOG.info(\"aborting server at: \" +\n         serverInfo.getServerAddress().toString());\n     } else {",
                "deletions": 17
            },
            {
                "sha": "88779c9fbea97f17ab971e320363e9121e719089",
                "filename": "src/test/org/apache/hadoop/hbase/TestDFSAbort.java",
                "blob_url": "https://github.com/apache/hbase/blob/1596f8aa10e3af2549c406a90a04ab85447830ea/src/test/org/apache/hadoop/hbase/TestDFSAbort.java",
                "raw_url": "https://github.com/apache/hbase/raw/1596f8aa10e3af2549c406a90a04ab85447830ea/src/test/org/apache/hadoop/hbase/TestDFSAbort.java",
                "status": "modified",
                "changes": 1,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestDFSAbort.java?ref=1596f8aa10e3af2549c406a90a04ab85447830ea",
                "patch": "@@ -32,7 +32,6 @@ public TestDFSAbort() {\n     super();\n     conf.setInt(\"ipc.client.timeout\", 5000);            // reduce ipc client timeout\n     conf.setInt(\"ipc.client.connect.max.retries\", 5);   // and number of retries\n-    conf.setInt(\"hbase.client.retries.number\", 5);      // reduce HBase retries\n     Logger.getRootLogger().setLevel(Level.WARN);\n     Logger.getLogger(this.getClass().getPackage().getName()).setLevel(Level.DEBUG);\n   }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-11458 NPEs if RegionServer cannot initialize",
        "commit": "https://github.com/apache/hbase/commit/b47a3f6924e22a1ae3651c866382dd55c8103025",
        "parent": "https://github.com/apache/hbase/commit/36d84a15158057f52c62b14d3bc4fa223f2693e6",
        "bug_id": "hbase_127",
        "file": [
            {
                "sha": "fb457ed59afdf250560aee50289642e86ddca181",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/b47a3f6924e22a1ae3651c866382dd55c8103025/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/b47a3f6924e22a1ae3651c866382dd55c8103025/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 36,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=b47a3f6924e22a1ae3651c866382dd55c8103025",
                "patch": "@@ -825,7 +825,9 @@ public void run() {\n       cacheConfig.getBlockCache().shutdown();\n     }\n \n-    movedRegionsCleaner.stop(\"Region Server stopping\");\n+    if (movedRegionsCleaner != null) {\n+      movedRegionsCleaner.stop(\"Region Server stopping\");\n+    }\n \n     // Send interrupts to wake up threads if sleeping so they notice shutdown.\n     // TODO: Should we check they are alive? If OOME could have exited already\n@@ -845,7 +847,9 @@ public void run() {\n     }\n \n     // Stop the snapshot and other procedure handlers, forcefully killing all running tasks\n-    rspmHost.stop(this.abortRequested || this.killed);\n+    if (rspmHost != null) {\n+      rspmHost.stop(this.abortRequested || this.killed);\n+    }\n \n     if (this.killed) {\n       // Just skip out w/o closing regions.  Used when testing.\n@@ -897,8 +901,12 @@ public void run() {\n     if (this.rssStub != null) {\n       this.rssStub = null;\n     }\n-    this.rpcClient.stop();\n-    this.leases.close();\n+    if (this.rpcClient != null) {\n+      this.rpcClient.stop();\n+    }\n+    if (this.leases != null) {\n+      this.leases.close();\n+    }\n     if (this.pauseMonitor != null) {\n       this.pauseMonitor.stop();\n     }\n@@ -907,7 +915,9 @@ public void run() {\n       stopServiceThreads();\n     }\n \n-    this.rpcServices.stop();\n+    if (this.rpcServices != null) {\n+      this.rpcServices.stop();\n+    }\n \n     try {\n       deleteMyEphemeralNode();\n@@ -918,7 +928,9 @@ public void run() {\n     //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n     ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n \n-    this.zooKeeper.close();\n+    if (this.zooKeeper != null) {\n+      this.zooKeeper.close();\n+    }\n     LOG.info(\"stopping server \" + this.serverName +\n       \"; zookeeper connection closed.\");\n \n@@ -1852,9 +1864,15 @@ protected void stopServiceThreads() {\n     if (this.nonceManagerChore != null) {\n       Threads.shutdown(this.nonceManagerChore.getThread());\n     }\n-    Threads.shutdown(this.compactionChecker.getThread());\n-    Threads.shutdown(this.periodicFlusher.getThread());\n-    this.cacheFlusher.join();\n+    if (this.compactionChecker != null) {\n+      Threads.shutdown(this.compactionChecker.getThread());\n+    }\n+    if (this.periodicFlusher != null) {\n+      Threads.shutdown(this.periodicFlusher.getThread());\n+    }\n+    if (this.cacheFlusher != null) {\n+      this.cacheFlusher.join();\n+    }\n     if (this.healthCheckChore != null) {\n       Threads.shutdown(this.healthCheckChore.getThread());\n     }",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13855 Race in multi threaded PartitionedMobCompactor causes NPE. (Jingcheng)",
        "commit": "https://github.com/apache/hbase/commit/faefb9073f388663df91d0ef2db24b00d6512519",
        "parent": "https://github.com/apache/hbase/commit/26893aa451215ef0395b7df16f129414b7b86c86",
        "bug_id": "hbase_128",
        "file": [
            {
                "sha": "6c2ff01dd84426f3b63d0b4aac8cdc5e48deef12",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "blob_url": "https://github.com/apache/hbase/blob/faefb9073f388663df91d0ef2db24b00d6512519/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "raw_url": "https://github.com/apache/hbase/raw/faefb9073f388663df91d0ef2db24b00d6512519/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java",
                "status": "modified",
                "changes": 37,
                "additions": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/PartitionedMobCompactor.java?ref=faefb9073f388663df91d0ef2db24b00d6512519",
                "patch": "@@ -211,14 +211,22 @@ protected PartitionedMobCompactionRequest select(List<FileStatus> candidates,\n     }\n     List<Path> newDelPaths = compactDelFiles(request, delFilePaths);\n     List<StoreFile> newDelFiles = new ArrayList<StoreFile>();\n-    for (Path newDelPath : newDelPaths) {\n-      StoreFile sf = new StoreFile(fs, newDelPath, conf, compactionCacheConfig, BloomType.NONE);\n-      newDelFiles.add(sf);\n+    List<Path> paths = null;\n+    try {\n+      for (Path newDelPath : newDelPaths) {\n+        StoreFile sf = new StoreFile(fs, newDelPath, conf, compactionCacheConfig, BloomType.NONE);\n+        // pre-create reader of a del file to avoid race condition when opening the reader in each\n+        // partition.\n+        sf.createReader();\n+        newDelFiles.add(sf);\n+      }\n+      LOG.info(\"After merging, there are \" + newDelFiles.size() + \" del files\");\n+      // compact the mob files by partitions.\n+      paths = compactMobFiles(request, newDelFiles);\n+      LOG.info(\"After compaction, there are \" + paths.size() + \" mob files\");\n+    } finally {\n+      closeStoreFileReaders(newDelFiles);\n     }\n-    LOG.info(\"After merging, there are \" + newDelFiles.size() + \" del files\");\n-    // compact the mob files by partitions.\n-    List<Path> paths = compactMobFiles(request, newDelFiles);\n-    LOG.info(\"After compaction, there are \" + paths.size() + \" mob files\");\n     // archive the del files if all the mob files are selected.\n     if (request.type == CompactionType.ALL_FILES && !newDelPaths.isEmpty()) {\n       LOG.info(\"After a mob compaction with all files selected, archiving the del files \"\n@@ -336,6 +344,20 @@ protected PartitionedMobCompactionRequest select(List<FileStatus> candidates,\n     return newFiles;\n   }\n \n+  /**\n+   * Closes the readers of store files.\n+   * @param storeFiles The store files to be closed.\n+   */\n+  private void closeStoreFileReaders(List<StoreFile> storeFiles) {\n+    for (StoreFile storeFile : storeFiles) {\n+      try {\n+        storeFile.closeReader(true);\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed to close the reader on store file \" + storeFile.getPath(), e);\n+      }\n+    }\n+  }\n+\n   /**\n    * Compacts a partition of selected small mob files and all the del files in a batch.\n    * @param request The compaction request.\n@@ -415,6 +437,7 @@ private void compactMobFilesInBatch(PartitionedMobCompactionRequest request,\n     }\n     // archive the old mob files, do not archive the del files.\n     try {\n+      closeStoreFileReaders(mobFilesToCompact);\n       MobUtils\n         .removeMobFiles(conf, fs, tableName, mobTableDir, column.getName(), mobFilesToCompact);\n     } catch (IOException e) {",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15311 Prevent NPE in BlockCacheViewTmpl.\n\nSigned-off-by: stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/75c57a04ddad2d7cf3435df1eba13541775319fb",
        "parent": "https://github.com/apache/hbase/commit/40c55915e7a45a639adb7f7a370a04f38058ac26",
        "bug_id": "hbase_129",
        "file": [
            {
                "sha": "c6d7a619d0fe7eb27d498ca109be4f6e621ab5c8",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/75c57a04ddad2d7cf3435df1eba13541775319fb/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/75c57a04ddad2d7cf3435df1eba13541775319fb/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl.jamon",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl.jamon?ref=75c57a04ddad2d7cf3435df1eba13541775319fb",
                "patch": "@@ -44,7 +44,7 @@ org.apache.hadoop.util.StringUtils;\n   if (bcn.equals(\"L1\")) {\n     bc = bcs == null || bcs.length == 0? bc: bcs[0];\n   } else {\n-    if (bcs.length < 2) {\n+    if (bcs == null || bcs.length < 2) {\n       System.out.println(\"There is no L2 block cache\");\n       return;\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-NPE when running TestSplitLogManager (Andrey Stepachev and Zhangduo)",
        "commit": "https://github.com/apache/hbase/commit/70ecf18817ef219389a9e024ff21ffb99b6615d9",
        "parent": "https://github.com/apache/hbase/commit/dad2474f08d201d09989e36f5cf1c25d3fa4acee",
        "bug_id": "hbase_130",
        "file": [
            {
                "sha": "c93ecf6b639ffc45bf4c9ca015667e948df3bde3",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/70ecf18817ef219389a9e024ff21ffb99b6615d9/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/70ecf18817ef219389a9e024ff21ffb99b6615d9/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java?ref=70ecf18817ef219389a9e024ff21ffb99b6615d9",
                "patch": "@@ -149,8 +149,8 @@ public SplitLogManager(Server server, Configuration conf, Stoppable stopper,\n       Set<String> failedDeletions = Collections.synchronizedSet(new HashSet<String>());\n       SplitLogManagerDetails details =\n           new SplitLogManagerDetails(tasks, master, failedDeletions, serverName);\n-      coordination.init();\n       coordination.setDetails(details);\n+      coordination.init();\n       // Determine recovery mode\n     }\n     this.unassignedTimeout =",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-17357 FIX PerformanceEvaluation parameters parsing triggers NPE.\n\ncheck command name is not null, if null print usage and exit\n\nSigned-off-by: Michael Stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/c74cf12925b810b7a59c5b639834508f00054053",
        "parent": "https://github.com/apache/hbase/commit/79018056f542cde5850b1d1fc2fe248f0007fd66",
        "bug_id": "hbase_131",
        "file": [
            {
                "sha": "7f1c6408c550319b449c4c78b58845919952b284",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/c74cf12925b810b7a59c5b639834508f00054053/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/c74cf12925b810b7a59c5b639834508f00054053/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=c74cf12925b810b7a59c5b639834508f00054053",
                "patch": "@@ -2223,6 +2223,12 @@ public int run(String[] args) throws Exception {\n         throw new IllegalArgumentException(\"Number of clients must be > 0\");\n       }\n \n+      // cmdName should not be null, print help and exit\n+      if (opts.cmdName == null) {\n+        printUsage();\n+        return errCode;\n+      }\n+\n       Class<? extends Test> cmdClass = determineCommandClass(opts.cmdName);\n       if (cmdClass != null) {\n         runTest(cmdClass, opts);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12767 Fix a StoreFileScanner NPE in reverse scan flow",
        "commit": "https://github.com/apache/hbase/commit/c24810eebe1f52bbe42bd0f4e0bf512425295aad",
        "parent": "https://github.com/apache/hbase/commit/02b03326506d5ffeb191a61a598763a99380e2a0",
        "bug_id": "hbase_132",
        "file": [
            {
                "sha": "05c996f50523653a92681413461db8c4d4d8fcc8",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "status": "modified",
                "changes": 12,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=c24810eebe1f52bbe42bd0f4e0bf512425295aad",
                "patch": "@@ -304,10 +304,18 @@ public boolean seekBefore(Cell key) throws IOException {\n           // The equals sign isn't strictly necessary just here to be consistent\n           // with seekTo\n           if (getComparator().compareOnlyKeyPortion(key, splitCell) >= 0) {\n-            return this.delegate.seekBefore(splitCell);\n+            boolean ret = this.delegate.seekBefore(splitCell);\n+            if (ret) {\n+              atEnd = false;\n+            }\n+            return ret;\n           }\n         }\n-        return this.delegate.seekBefore(key);\n+        boolean ret = this.delegate.seekBefore(key);\n+        if (ret) {\n+          atEnd = false;\n+        }\n+        return ret;\n       }\n     };\n   }",
                "deletions": 2
            },
            {
                "sha": "992a978acd038a767481ca81287678f0b483d0cb",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/c24810eebe1f52bbe42bd0f4e0bf512425295aad/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "status": "modified",
                "changes": 102,
                "additions": 102,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=c24810eebe1f52bbe42bd0f4e0bf512425295aad",
                "patch": "@@ -5009,6 +5009,7 @@ private void checkOneCell(Cell kv, byte[] cf, int rowIdx, int colIdx, long ts) {\n         Bytes.toString(CellUtil.cloneValue(kv)));\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStore_SingleCF_Normal()\n       throws IOException {\n     byte[] rowC = Bytes.toBytes(\"rowC\");\n@@ -5064,6 +5065,7 @@ public void testReverseScanner_FromMemStore_SingleCF_Normal()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStore_SingleCF_LargerKey()\n       throws IOException {\n     byte[] rowC = Bytes.toBytes(\"rowC\");\n@@ -5120,6 +5122,7 @@ public void testReverseScanner_FromMemStore_SingleCF_LargerKey()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStore_SingleCF_FullScan()\n       throws IOException {\n     byte[] rowC = Bytes.toBytes(\"rowC\");\n@@ -5173,6 +5176,7 @@ public void testReverseScanner_FromMemStore_SingleCF_FullScan()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_moreRowsMayExistAfter() throws IOException {\n     // case for \"INCLUDE_AND_SEEK_NEXT_ROW & SEEK_NEXT_ROW\" endless loop\n     byte[] rowA = Bytes.toBytes(\"rowA\");\n@@ -5250,6 +5254,7 @@ public void testReverseScanner_moreRowsMayExistAfter() throws IOException {\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_smaller_blocksize() throws IOException {\n     // case to ensure no conflict with HFile index optimization\n     byte[] rowA = Bytes.toBytes(\"rowA\");\n@@ -5329,6 +5334,7 @@ public void testReverseScanner_smaller_blocksize() throws IOException {\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs1()\n       throws IOException {\n     byte[] row0 = Bytes.toBytes(\"row0\"); // 1 kv\n@@ -5489,6 +5495,7 @@ public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs1()\n     }\n   }\n \n+  @Test (timeout=60000)\n   public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs2()\n       throws IOException {\n     byte[] row1 = Bytes.toBytes(\"row1\");\n@@ -5562,6 +5569,101 @@ public void testReverseScanner_FromMemStoreAndHFiles_MultiCFs2()\n     }\n   }\n \n+  @Test (timeout=60000)\n+  public void testSplitRegionWithReverseScan() throws IOException {\n+    byte [] tableName = Bytes.toBytes(\"testSplitRegionWithReverseScan\");\n+    byte [] qualifier = Bytes.toBytes(\"qualifier\");\n+    Configuration hc = initSplit();\n+    int numRows = 3;\n+    byte [][] families = {fam1};\n+\n+    //Setting up region\n+    String method = this.getName();\n+    this.region = initHRegion(tableName, method, hc, families);\n+\n+    //Put data in region\n+    int startRow = 100;\n+    putData(startRow, numRows, qualifier, families);\n+    int splitRow = startRow + numRows;\n+    putData(splitRow, numRows, qualifier, families);\n+    int endRow = splitRow + numRows;\n+    region.flushcache();\n+\n+    HRegion [] regions = null;\n+    try {\n+      regions = splitRegion(region, Bytes.toBytes(\"\" + splitRow));\n+      //Opening the regions returned.\n+      for (int i = 0; i < regions.length; i++) {\n+        regions[i] = HRegion.openHRegion(regions[i], null);\n+      }\n+      //Verifying that the region has been split\n+      assertEquals(2, regions.length);\n+\n+      //Verifying that all data is still there and that data is in the right\n+      //place\n+      verifyData(regions[0], startRow, numRows, qualifier, families);\n+      verifyData(regions[1], splitRow, numRows, qualifier, families);\n+\n+      //fire the reverse scan1:  top range, and larger than the last row\n+      Scan scan = new Scan(Bytes.toBytes(String.valueOf(startRow + 10 * numRows)));\n+      scan.setReversed(true);\n+      InternalScanner scanner = regions[1].getScanner(scan);\n+      List<Cell> currRow = new ArrayList<Cell>();\n+      boolean more = false;\n+      int verify = startRow + 2 * numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, startRow + numRows - 1);\n+      scanner.close();\n+      //fire the reverse scan2:  top range, and equals to the last row\n+      scan = new Scan(Bytes.toBytes(String.valueOf(startRow + 2 * numRows - 1)));\n+      scan.setReversed(true);\n+      scanner = regions[1].getScanner(scan);\n+      verify = startRow + 2 * numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, startRow + numRows - 1);\n+      scanner.close();\n+      //fire the reverse scan3:  bottom range, and larger than the last row\n+      scan = new Scan(Bytes.toBytes(String.valueOf(startRow + numRows)));\n+      scan.setReversed(true);\n+      scanner = regions[0].getScanner(scan);\n+      verify = startRow + numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, 99);\n+      scanner.close();\n+      //fire the reverse scan4:  bottom range, and equals to the last row\n+      scan = new Scan(Bytes.toBytes(String.valueOf(startRow + numRows - 1)));\n+      scan.setReversed(true);\n+      scanner = regions[0].getScanner(scan);\n+      verify = startRow + numRows - 1;\n+      do {\n+        more = scanner.next(currRow);\n+        assertEquals(Bytes.toString(currRow.get(0).getRow()), verify + \"\");\n+        verify--;\n+        currRow.clear();\n+      } while(more);\n+      assertEquals(verify, startRow - 1);\n+      scanner.close();\n+    } finally {\n+      HRegion.closeHRegion(this.region);\n+      this.region = null;\n+    }\n+  }\n+\n   @Test\n   public void testWriteRequestsCounter() throws IOException {\n     byte[] fam = Bytes.toBytes(\"info\");",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12692 NPE from SnapshotManager#stop (Ashish Singhi)",
        "commit": "https://github.com/apache/hbase/commit/3a652ba41c86aa566ea5377234611888da5a8bd8",
        "parent": "https://github.com/apache/hbase/commit/afb753ecc3a94a5824a510121aa186948fb317df",
        "bug_id": "hbase_133",
        "file": [
            {
                "sha": "b7a891d01bab67bb82d92c1e9931b386a5628ff8",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/3a652ba41c86aa566ea5377234611888da5a8bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/3a652ba41c86aa566ea5377234611888da5a8bd8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java?ref=3a652ba41c86aa566ea5377234611888da5a8bd8",
                "patch": "@@ -927,7 +927,9 @@ public void stop(String why) {\n       restoreHandler.cancel(why);\n     }\n     try {\n-      coordinator.close();\n+      if (coordinator != null) {\n+        coordinator.close();\n+      }\n     } catch (IOException e) {\n       LOG.error(\"stop ProcedureCoordinator error\", e);\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-15114 NPE when IPC server ByteBuffer reservoir is turned off",
        "commit": "https://github.com/apache/hbase/commit/2d2fdd5a9fd122a76fc71db549b4e10985395dd2",
        "parent": "https://github.com/apache/hbase/commit/76bce7732633a0881ff7cb0dfd2f56f13a98303a",
        "bug_id": "hbase_134",
        "file": [
            {
                "sha": "cdc97bfd118cea217fb7a60bbbbc598ee87541a0",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/2d2fdd5a9fd122a76fc71db549b4e10985395dd2/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/2d2fdd5a9fd122a76fc71db549b4e10985395dd2/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=2d2fdd5a9fd122a76fc71db549b4e10985395dd2",
                "patch": "@@ -349,7 +349,7 @@\n      * cleanup.\n      */\n     void done() {\n-      if (this.cellBlock != null) {\n+      if (this.cellBlock != null && reservoir != null) {\n         // Return buffer to reservoir now we are done with it.\n         reservoir.putBuffer(this.cellBlock);\n         this.cellBlock = null;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"HBASE-14909 NPE testing for RIT\"\n\nThis reverts commit da0cc598feab995eed12527d90805dd627674035.",
        "commit": "https://github.com/apache/hbase/commit/157a60f1b396ab9adc7f934a15352f2dbc5493a9",
        "parent": "https://github.com/apache/hbase/commit/35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
        "bug_id": "hbase_135",
        "file": [
            {
                "sha": "5bb25dbdc0efebb03d279ffa916bd87c35b5f3b9",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/157a60f1b396ab9adc7f934a15352f2dbc5493a9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/157a60f1b396ab9adc7f934a15352f2dbc5493a9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "status": "modified",
                "changes": 20,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=157a60f1b396ab9adc7f934a15352f2dbc5493a9",
                "patch": "@@ -17,10 +17,7 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n+import javax.annotation.Nullable;\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -47,8 +44,6 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n-import javax.annotation.Nullable;\n-\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -63,6 +58,7 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -88,7 +84,6 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n-import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -134,6 +129,10 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3768,11 +3767,8 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        HMaster master = getMiniHBaseCluster().getMaster();\n-        if (master == null) return false;\n-        AssignmentManager am = master.getAssignmentManager();\n-        if (am == null) return false;\n-        final RegionStates regionStates = am.getRegionStates();\n+        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n+            .getAssignmentManager().getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "deletions": 12
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14798 NPE reporting server load causes regionserver abort; causes TestAcidGuarantee to fail",
        "commit": "https://github.com/apache/hbase/commit/43506320a1bb6ca2193162edfb5dee21fffc08a9",
        "parent": "https://github.com/apache/hbase/commit/1fa7b71cf82cc30757ecf5d2a8e0cfba654ed469",
        "bug_id": "hbase_136",
        "file": [
            {
                "sha": "9f38b9eb00dc344a71054484d8df76c182975b96",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java?ref=43506320a1bb6ca2193162edfb5dee21fffc08a9",
                "patch": "@@ -70,6 +70,7 @@ public void loadFiles(List<StoreFile> storeFiles) {\n \n   @Override\n   public final Collection<StoreFile> getStorefiles() {\n+    // TODO: I can return a null list of StoreFiles? That'll mess up clients. St.Ack 20151111\n     return storefiles;\n   }\n ",
                "deletions": 0
            },
            {
                "sha": "994270b447590dd17e4d8e7909a107f08de4512c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/43506320a1bb6ca2193162edfb5dee21fffc08a9/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 53,
                "additions": 33,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=43506320a1bb6ca2193162edfb5dee21fffc08a9",
                "patch": "@@ -960,32 +960,34 @@ private void initializeWarmup(final CancelableProgressable reporter) throws IOEx\n     initializeStores(reporter, status);\n   }\n \n-  private void writeRegionOpenMarker(WAL wal, long openSeqId) throws IOException {\n-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n+  /**\n+   * @return Map of StoreFiles by column family\n+   */\n+  private NavigableMap<byte[], List<Path>> getStoreFiles() {\n+    NavigableMap<byte[], List<Path>> allStoreFiles =\n+      new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n     for (Store store: getStores()) {\n-      ArrayList<Path> storeFileNames = new ArrayList<Path>();\n-      for (StoreFile storeFile: store.getStorefiles()) {\n+      Collection<StoreFile> storeFiles = store.getStorefiles();\n+      if (storeFiles == null) continue;\n+      List<Path> storeFileNames = new ArrayList<Path>();\n+      for (StoreFile storeFile: storeFiles) {\n         storeFileNames.add(storeFile.getPath());\n       }\n-      storeFiles.put(store.getFamily().getName(), storeFileNames);\n+      allStoreFiles.put(store.getFamily().getName(), storeFileNames);\n     }\n+    return allStoreFiles;\n+  }\n \n+  private void writeRegionOpenMarker(WAL wal, long openSeqId) throws IOException {\n+    Map<byte[], List<Path>> storeFiles = getStoreFiles();\n     RegionEventDescriptor regionOpenDesc = ProtobufUtil.toRegionEventDescriptor(\n       RegionEventDescriptor.EventType.REGION_OPEN, getRegionInfo(), openSeqId,\n       getRegionServerServices().getServerName(), storeFiles);\n     WALUtil.writeRegionEventMarker(wal, getTableDesc(), getRegionInfo(), regionOpenDesc, mvcc);\n   }\n \n   private void writeRegionCloseMarker(WAL wal) throws IOException {\n-    Map<byte[], List<Path>> storeFiles = new TreeMap<byte[], List<Path>>(Bytes.BYTES_COMPARATOR);\n-    for (Store store: getStores()) {\n-      ArrayList<Path> storeFileNames = new ArrayList<Path>();\n-      for (StoreFile storeFile: store.getStorefiles()) {\n-        storeFileNames.add(storeFile.getPath());\n-      }\n-      storeFiles.put(store.getFamily().getName(), storeFileNames);\n-    }\n-\n+    Map<byte[], List<Path>> storeFiles = getStoreFiles();\n     RegionEventDescriptor regionEventDesc = ProtobufUtil.toRegionEventDescriptor(\n       RegionEventDescriptor.EventType.REGION_CLOSE, getRegionInfo(), mvcc.getReadPoint(),\n       getRegionServerServices().getServerName(), storeFiles);\n@@ -1016,7 +1018,9 @@ public HDFSBlocksDistribution getHDFSBlocksDistribution() {\n       new HDFSBlocksDistribution();\n     synchronized (this.stores) {\n       for (Store store : this.stores.values()) {\n-        for (StoreFile sf : store.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = store.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile sf : storeFiles) {\n           HDFSBlocksDistribution storeFileBlocksDistribution =\n             sf.getHDFSBlockDistribution();\n           hdfsBlocksDistribution.add(storeFileBlocksDistribution);\n@@ -1059,7 +1063,6 @@ public static HDFSBlocksDistribution computeHDFSBlocksDistribution(final Configu\n     for (HColumnDescriptor family: tableDescriptor.getFamilies()) {\n       Collection<StoreFileInfo> storeFiles = regionFs.getStoreFiles(family.getNameAsString());\n       if (storeFiles == null) continue;\n-\n       for (StoreFileInfo storeFileInfo : storeFiles) {\n         try {\n           hdfsBlocksDistribution.add(storeFileInfo.computeHDFSBlocksDistribution(fs));\n@@ -1639,10 +1642,16 @@ public long getEarliestFlushTimeForAllStores() {\n   public long getOldestHfileTs(boolean majorCompactioOnly) throws IOException {\n     long result = Long.MAX_VALUE;\n     for (Store store : getStores()) {\n-      for (StoreFile file : store.getStorefiles()) {\n-        HFile.Reader reader = file.getReader().getHFileReader();\n+      Collection<StoreFile> storeFiles = store.getStorefiles();\n+      if (storeFiles == null) continue;\n+      for (StoreFile file : storeFiles) {\n+        StoreFile.Reader sfReader = file.getReader();\n+        if (sfReader == null) continue;\n+        HFile.Reader reader = sfReader.getHFileReader();\n+        if (reader == null) continue;\n         if (majorCompactioOnly) {\n           byte[] val = reader.loadFileInfo().get(StoreFile.MAJOR_COMPACTION_KEY);\n+          if (val == null) continue;\n           if (val == null || !Bytes.toBoolean(val)) {\n             continue;\n           }\n@@ -4898,7 +4907,9 @@ private void logRegionFiles() {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(getRegionInfo().getEncodedName() + \" : Store files for region: \");\n       for (Store s : stores.values()) {\n-        for (StoreFile sf : s.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = s.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile sf : storeFiles) {\n           LOG.trace(getRegionInfo().getEncodedName() + \" : \" + sf);\n         }\n       }\n@@ -5006,7 +5017,9 @@ private Store getStore(Cell cell) {\n           throw new IllegalArgumentException(\"No column family : \" +\n               new String(column) + \" available\");\n         }\n-        for (StoreFile storeFile: store.getStorefiles()) {\n+        Collection<StoreFile> storeFiles = store.getStorefiles();\n+        if (storeFiles == null) continue;\n+        for (StoreFile storeFile: storeFiles) {\n           storeFileNames.add(storeFile.getPath().toString());\n         }\n ",
                "deletions": 20
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14278 Fix NPE that is showing up since HBASE-14274 went in",
        "commit": "https://github.com/apache/hbase/commit/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
        "parent": "https://github.com/apache/hbase/commit/7b08f4c8be60582cd02ba31161be214c9c9d40f9",
        "bug_id": "hbase_137",
        "file": [
            {
                "sha": "1835f6b6dbd88fd6ada7d7cc06625c4fbba74c8b",
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java",
                "status": "modified",
                "changes": 18,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionAggregateSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "patch": "@@ -23,6 +23,8 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.TimeUnit;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.metrics.BaseSourceImpl;\n import org.apache.hadoop.metrics2.MetricsCollector;\n@@ -35,6 +37,8 @@\n public class MetricsRegionAggregateSourceImpl extends BaseSourceImpl\n     implements MetricsRegionAggregateSource {\n \n+  private static final Log LOG = LogFactory.getLog(MetricsRegionAggregateSourceImpl.class);\n+\n   private final MetricsExecutorImpl executor = new MetricsExecutorImpl();\n \n   private final Set<MetricsRegionSource> regionSources =\n@@ -54,7 +58,7 @@ public MetricsRegionAggregateSourceImpl(String metricsName,\n     // Every few mins clean the JMX cache.\n     executor.getExecutor().scheduleWithFixedDelay(new Runnable() {\n       public void run() {\n-        JmxCacheBuster.clearJmxCache(true);\n+        JmxCacheBuster.clearJmxCache();\n       }\n     }, 5, 5, TimeUnit.MINUTES);\n   }\n@@ -67,12 +71,20 @@ public void register(MetricsRegionSource source) {\n \n   @Override\n   public void deregister(MetricsRegionSource toRemove) {\n-    regionSources.remove(toRemove);\n+    try {\n+      regionSources.remove(toRemove);\n+    } catch (Exception e) {\n+      // Ignored. If this errors out it means that someone is double\n+      // closing the region source and the region is already nulled out.\n+      LOG.info(\n+          \"Error trying to remove \" + toRemove + \" from \" + this.getClass().getSimpleName(),\n+          e);\n+    }\n     clearCache();\n   }\n \n   private synchronized void clearCache() {\n-    JmxCacheBuster.clearJmxCache(true);\n+    JmxCacheBuster.clearJmxCache();\n   }\n \n   /**",
                "deletions": 3
            },
            {
                "sha": "a4891da1a9b6d41bdcdb3dc38f67985cebc91c47",
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "patch": "@@ -68,28 +68,28 @@ public MetricsRegionServerSourceImpl(String metricsName,\n     this.rsWrap = rsWrap;\n \n     putHisto = getMetricsRegistry().newHistogram(MUTATE_KEY);\n-    slowPut = getMetricsRegistry().newCounter(SLOW_MUTATE_KEY, SLOW_MUTATE_DESC, 0l);\n+    slowPut = getMetricsRegistry().newCounter(SLOW_MUTATE_KEY, SLOW_MUTATE_DESC, 0L);\n \n     deleteHisto = getMetricsRegistry().newHistogram(DELETE_KEY);\n-    slowDelete = getMetricsRegistry().newCounter(SLOW_DELETE_KEY, SLOW_DELETE_DESC, 0l);\n+    slowDelete = getMetricsRegistry().newCounter(SLOW_DELETE_KEY, SLOW_DELETE_DESC, 0L);\n \n     getHisto = getMetricsRegistry().newHistogram(GET_KEY);\n-    slowGet = getMetricsRegistry().newCounter(SLOW_GET_KEY, SLOW_GET_DESC, 0l);\n+    slowGet = getMetricsRegistry().newCounter(SLOW_GET_KEY, SLOW_GET_DESC, 0L);\n \n     incrementHisto = getMetricsRegistry().newHistogram(INCREMENT_KEY);\n     slowIncrement = getMetricsRegistry().newCounter(SLOW_INCREMENT_KEY, SLOW_INCREMENT_DESC, 0L);\n \n     appendHisto = getMetricsRegistry().newHistogram(APPEND_KEY);\n-    slowAppend = getMetricsRegistry().newCounter(SLOW_APPEND_KEY, SLOW_APPEND_DESC, 0l);\n+    slowAppend = getMetricsRegistry().newCounter(SLOW_APPEND_KEY, SLOW_APPEND_DESC, 0L);\n     \n     replayHisto = getMetricsRegistry().newHistogram(REPLAY_KEY);\n     scanNextHisto = getMetricsRegistry().newHistogram(SCAN_NEXT_KEY);\n \n     splitTimeHisto = getMetricsRegistry().newHistogram(SPLIT_KEY);\n     flushTimeHisto = getMetricsRegistry().newHistogram(FLUSH_KEY);\n \n-    splitRequest = getMetricsRegistry().newCounter(SPLIT_REQUEST_KEY, SPLIT_REQUEST_DESC, 0l);\n-    splitSuccess = getMetricsRegistry().newCounter(SPLIT_SUCCESS_KEY, SPLIT_SUCCESS_DESC, 0l);\n+    splitRequest = getMetricsRegistry().newCounter(SPLIT_REQUEST_KEY, SPLIT_REQUEST_DESC, 0L);\n+    splitSuccess = getMetricsRegistry().newCounter(SPLIT_SUCCESS_KEY, SPLIT_SUCCESS_DESC, 0L);\n   }\n \n   @Override",
                "deletions": 6
            },
            {
                "sha": "0ecf2b2a5743649df27b2141ca42c011b18efb7c",
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java",
                "status": "modified",
                "changes": 50,
                "additions": 31,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "patch": "@@ -20,16 +20,12 @@\n \n import java.util.Map;\n import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReadWriteLock;\n-import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.math.stat.descriptive.DescriptiveStatistics;\n import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.metrics2.MetricsRecordBuilder;\n-import org.apache.hadoop.metrics2.impl.JmxCacheBuster;\n import org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry;\n import org.apache.hadoop.metrics2.lib.Interns;\n import org.apache.hadoop.metrics2.lib.MutableCounterLong;\n@@ -65,6 +61,7 @@\n   private final MutableCounterLong regionAppend;\n   private final MutableHistogram regionGet;\n   private final MutableHistogram regionScanNext;\n+  private final int hashCode;\n \n   public MetricsRegionSourceImpl(MetricsRegionWrapper regionWrapper,\n                                  MetricsRegionAggregateSourceImpl aggregate) {\n@@ -101,6 +98,8 @@ public MetricsRegionSourceImpl(MetricsRegionWrapper regionWrapper,\n \n     regionScanNextKey = regionNamePrefix + MetricsRegionServerSource.SCAN_NEXT_KEY;\n     regionScanNext = registry.newHistogram(regionScanNextKey);\n+\n+    hashCode = regionWrapper.getRegionHashCode();\n   }\n \n   @Override\n@@ -173,12 +172,16 @@ public MetricsRegionAggregateSource getAggregateSource() {\n \n   @Override\n   public int compareTo(MetricsRegionSource source) {\n-    if (!(source instanceof MetricsRegionSourceImpl))\n+    if (!(source instanceof MetricsRegionSourceImpl)) {\n       return -1;\n+    }\n \n     MetricsRegionSourceImpl impl = (MetricsRegionSourceImpl) source;\n-    return this.regionWrapper.getRegionName()\n-        .compareTo(impl.regionWrapper.getRegionName());\n+    if (impl == null) {\n+      return -1;\n+    }\n+\n+    return Long.compare(hashCode, impl.hashCode);\n   }\n \n   void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n@@ -203,31 +206,40 @@ void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n       }\n \n       mrb.addGauge(\n-          Interns.info(regionNamePrefix + MetricsRegionServerSource.STORE_COUNT,\n+          Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.STORE_COUNT,\n               MetricsRegionServerSource.STORE_COUNT_DESC),\n           this.regionWrapper.getNumStores());\n-      mrb.addGauge(Interns.info(regionNamePrefix + MetricsRegionServerSource.STOREFILE_COUNT,\n+      mrb.addGauge(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.STOREFILE_COUNT,\n               MetricsRegionServerSource.STOREFILE_COUNT_DESC),\n           this.regionWrapper.getNumStoreFiles());\n-      mrb.addGauge(Interns.info(regionNamePrefix + MetricsRegionServerSource.MEMSTORE_SIZE,\n+      mrb.addGauge(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.MEMSTORE_SIZE,\n               MetricsRegionServerSource.MEMSTORE_SIZE_DESC),\n           this.regionWrapper.getMemstoreSize());\n-      mrb.addGauge(Interns.info(regionNamePrefix + MetricsRegionServerSource.STOREFILE_SIZE,\n+      mrb.addGauge(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.STOREFILE_SIZE,\n               MetricsRegionServerSource.STOREFILE_SIZE_DESC),\n           this.regionWrapper.getStoreFileSize());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionSource.COMPACTIONS_COMPLETED_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionSource.COMPACTIONS_COMPLETED_COUNT,\n               MetricsRegionSource.COMPACTIONS_COMPLETED_DESC),\n           this.regionWrapper.getNumCompactionsCompleted());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionSource.NUM_BYTES_COMPACTED_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionSource.NUM_BYTES_COMPACTED_COUNT,\n               MetricsRegionSource.NUM_BYTES_COMPACTED_DESC),\n           this.regionWrapper.getNumBytesCompacted());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionSource.NUM_FILES_COMPACTED_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionSource.NUM_FILES_COMPACTED_COUNT,\n               MetricsRegionSource.NUM_FILES_COMPACTED_DESC),\n           this.regionWrapper.getNumFilesCompacted());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionServerSource.READ_REQUEST_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.READ_REQUEST_COUNT,\n               MetricsRegionServerSource.READ_REQUEST_COUNT_DESC),\n           this.regionWrapper.getReadRequestCount());\n-      mrb.addCounter(Interns.info(regionNamePrefix + MetricsRegionServerSource.WRITE_REQUEST_COUNT,\n+      mrb.addCounter(Interns.info(\n+              regionNamePrefix + MetricsRegionServerSource.WRITE_REQUEST_COUNT,\n               MetricsRegionServerSource.WRITE_REQUEST_COUNT_DESC),\n           this.regionWrapper.getWriteRequestCount());\n \n@@ -265,12 +277,12 @@ void snapshot(MetricsRecordBuilder mrb, boolean ignored) {\n \n   @Override\n   public int hashCode() {\n-    return regionWrapper.getRegionHashCode();\n+    return hashCode;\n   }\n \n   @Override\n   public boolean equals(Object obj) {\n-    if (obj == this) return true;\n-    return obj instanceof MetricsRegionSourceImpl && compareTo((MetricsRegionSourceImpl) obj) == 0;\n+    return obj == this ||\n+        (obj instanceof MetricsRegionSourceImpl && compareTo((MetricsRegionSourceImpl) obj) == 0);\n   }\n }",
                "deletions": 19
            },
            {
                "sha": "95734ba4e6f95c420a1cbb34a80b08c911b0c209",
                "filename": "hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java",
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java",
                "status": "modified",
                "changes": 13,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/metrics2/impl/JmxCacheBuster.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "patch": "@@ -49,24 +49,18 @@ private JmxCacheBuster() {\n   /**\n    * For JMX to forget about all previously exported metrics.\n    */\n-\n   public static void clearJmxCache() {\n-    clearJmxCache(false);\n-  }\n-\n-  public static synchronized void clearJmxCache(boolean force) {\n     //If there are more then 100 ms before the executor will run then everything should be merged.\n     ScheduledFuture future = fut.get();\n-    if (!force &&\n-        (future == null || (!future.isDone() && future.getDelay(TimeUnit.MILLISECONDS) > 100))) {\n+    if ((future == null || (!future.isDone() && future.getDelay(TimeUnit.MILLISECONDS) > 100))) {\n       // BAIL OUT\n       return;\n     }\n     future = executor.getExecutor().schedule(new JmxCacheBusterRunnable(), 5, TimeUnit.SECONDS);\n     fut.set(future);\n   }\n \n-  static class JmxCacheBusterRunnable implements Runnable {\n+  final static class JmxCacheBusterRunnable implements Runnable {\n     @Override\n     public void run() {\n       if (LOG.isTraceEnabled()) {\n@@ -78,6 +72,9 @@ public void run() {\n       try {\n         if (DefaultMetricsSystem.instance() != null) {\n           DefaultMetricsSystem.instance().stop();\n+          // Sleep some time so that the rest of the hadoop metrics\n+          // system knows that things are done\n+          Thread.sleep(500);\n           DefaultMetricsSystem.instance().start();\n         }\n       }  catch (Exception exception)  {",
                "deletions": 8
            },
            {
                "sha": "9d60a8ff2c8dabae9506ef16dece50c90ddc198d",
                "filename": "hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java",
                "status": "modified",
                "changes": 8,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-hadoop2-compat/src/test/java/org/apache/hadoop/hbase/regionserver/TestMetricsRegionSourceImpl.java?ref=c1ac4bb8601f88eb3fe246eb62c3f40e95faf93d",
                "patch": "@@ -42,8 +42,10 @@ public void testCompareToHashCodeEquals() throws Exception {\n     assertEquals(one.hashCode(), oneClone.hashCode());\n     assertNotEquals(one, two);\n \n-    assertTrue( one.compareTo(two) < 0);\n-    assertTrue( two.compareTo(one) > 0);\n+    assertTrue( one.compareTo(two) != 0);\n+    assertTrue( two.compareTo(one) != 0);\n+    assertTrue( two.compareTo(one) != one.compareTo(two));\n+    assertTrue( two.compareTo(two) == 0);\n   }\n \n \n@@ -59,8 +61,6 @@ public void testNoGetRegionServerMetricsSourceImpl() throws Exception {\n     private String regionName;\n \n     public RegionWrapperStub(String regionName) {\n-\n-\n       this.regionName = regionName;\n     }\n ",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14291 NPE On StochasticLoadBalancer Balance Involving RS With No Regions",
        "commit": "https://github.com/apache/hbase/commit/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2",
        "parent": "https://github.com/apache/hbase/commit/902cd172f8a1ace4ad9fb35d4dfb7700fc10de21",
        "bug_id": "hbase_138",
        "file": [
            {
                "sha": "962b241663fc320e92b62926faaea81d8435d4d7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=e95cf8fdb4c78458e3058198e13c6d3cbd85a0e2",
                "patch": "@@ -860,7 +860,13 @@ int getLeastLoadedTopServerForRegion(int region) {\n         int leastLoadedServerIndex = -1;\n         int load = Integer.MAX_VALUE;\n         for (ServerName sn : topLocalServers) {\n-          int index = serversToIndex.get(sn);\n+          if (!serversToIndex.containsKey(sn.getHostAndPort())) {\n+            continue;\n+          }\n+          int index = serversToIndex.get(sn.getHostAndPort());\n+          if (regionsPerServer[index] == null) {\n+            continue;\n+          }\n           int tempLoad = regionsPerServer[index].length;\n           if (tempLoad <= load) {\n             leastLoadedServerIndex = index;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14050 NPE in org.apache.hadoop.hbase.ipc.RpcServer.readAndProcess",
        "commit": "https://github.com/apache/hbase/commit/a249989b93881b40a0919dc2fbd98530e05b6cf2",
        "parent": "https://github.com/apache/hbase/commit/257df19cc339a5418036efb3f3c4e7b400614c30",
        "bug_id": "hbase_139",
        "file": [
            {
                "sha": "9d33785d31502e001c6cbc1d8efc1be26907c3b9",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a249989b93881b40a0919dc2fbd98530e05b6cf2/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a249989b93881b40a0919dc2fbd98530e05b6cf2/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 1,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=a249989b93881b40a0919dc2fbd98530e05b6cf2",
                "patch": "@@ -1898,7 +1898,6 @@ private boolean authorizeConnection() throws IOException {\n     protected synchronized void close() {\n       disposeSasl();\n       data = null;\n-      this.dataLengthBuffer = null;\n       if (!channel.isOpen())\n         return;\n       try {socket.shutdownOutput();} catch(Exception ignored) {} // FindBugs DE_MIGHT_IGNORE",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13777 Table fragmentation display triggers NPE on master status page",
        "commit": "https://github.com/apache/hbase/commit/91a5090365204b6f9daf2b679a259a69989a49f1",
        "parent": "https://github.com/apache/hbase/commit/bb0d64b77818bcad4589a5e6409fe9fad2e4a905",
        "bug_id": "hbase_140",
        "file": [
            {
                "sha": "0cc675803f2efbce77bc52059e85a438b2cc1cd0",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/91a5090365204b6f9daf2b679a259a69989a49f1/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/91a5090365204b6f9daf2b679a259a69989a49f1/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "status": "modified",
                "changes": 13,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon?ref=91a5090365204b6f9daf2b679a259a69989a49f1",
                "patch": "@@ -215,7 +215,7 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n         <section>\n             <& BackupMasterStatusTmpl; master = master &>\n         </section>\n-\t</%if>    \n+\t</%if>\n \n \n         <section>\n@@ -367,7 +367,7 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n \n <%def userTables>\n <%java>\n-   HTableDescriptor[] tables = null; \n+   HTableDescriptor[] tables = null;\n    try (Admin admin = master.getConnection().getAdmin()) {\n      tables = master.isInitialized() ? admin.listTables() : null;\n    }\n@@ -389,9 +389,10 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n     </tr>\n     <%for HTableDescriptor htDesc : tables%>\n     <%java>\n+      TableName tableName = htDesc.getTableName();\n       Map<RegionState.State, List<HRegionInfo>> tableRegions =\n           master.getAssignmentManager().getRegionStates()\n-            .getRegionByStateOfTable(htDesc.getTableName());\n+            .getRegionByStateOfTable(tableName);\n       int openRegionsCount = tableRegions.get(RegionState.State.OPEN).size();\n       int offlineRegionsCount = tableRegions.get(RegionState.State.OFFLINE).size();\n       int splitRegionsCount = tableRegions.get(RegionState.State.SPLIT).size();\n@@ -407,10 +408,10 @@ AssignmentManager assignmentManager = master.getAssignmentManager();\n                      - splitRegionsCount;\n     </%java>\n     <tr>\n-        <td><% htDesc.getTableName().getNamespaceAsString() %></td>\n-        <td><a href=table.jsp?name=<% htDesc.getTableName().getNameAsString() %>><% htDesc.getTableName().getQualifierAsString() %></a> </td>\n+        <td><% tableName.getNamespaceAsString() %></td>\n+        <td><a href=table.jsp?name=<% tableName.getNameAsString() %>><% tableName.getQualifierAsString() %></a> </td>\n         <%if (frags != null) %>\n-            <td align=\"center\"><% frags.get(htDesc.getTableName().getNameAsString()) != null ? frags.get(htDesc.getTableName().getQualifierAsString()).intValue() + \"%\" : \"n/a\" %></td>\n+            <td align=\"center\"><% frags.get(tableName.getNameAsString()) != null ? frags.get(tableName.getNameAsString()).intValue() + \"%\" : \"n/a\" %></td>\n         </%if>\n         <td><% openRegionsCount %></td>\n         <td><% offlineRegionsCount %></td>",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12229 NullPointerException in SnapshotTestingUtils\n\n* Implemented the waitUntilAllRegionsOnline in HBaseTestingUtility#createTable\n* Add waitFor around #isTableAvailable call that previously didn't do anything\n* Remove unused byte[] hex\n\nSigned-off-by: stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/7e995b6496a44c057337535363aee778f52318b9",
        "parent": "https://github.com/apache/hbase/commit/349a56ae2c88e62917ba8923283ad4ca0c83841c",
        "bug_id": "hbase_141",
        "file": [
            {
                "sha": "450e805028ef63d24dc67f8468ed5cc840c68871",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "status": "modified",
                "changes": 15,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=7e995b6496a44c057337535363aee778f52318b9",
                "patch": "@@ -1206,6 +1206,21 @@ public HTable createTable(HTableDescriptor htd, byte[][] families, Configuration\n     return new HTable(c, htd.getTableName());\n   }\n \n+  /**\n+   * Create a table.\n+   * @param htd\n+   * @param splitRows\n+   * @return An HTable instance for the created table.\n+   * @throws IOException\n+   */\n+  public HTable createTable(HTableDescriptor htd, byte[][] splitRows)\n+      throws IOException {\n+    getHBaseAdmin().createTable(htd, splitRows);\n+    // HBaseAdmin only waits for regions to appear in hbase:meta we should wait until they are assigned\n+    waitUntilAllRegionsAssigned(htd.getTableName());\n+    return new HTable(getConfiguration(), htd.getTableName());\n+  }\n+\n   /**\n    * Create a table.\n    * @param tableName",
                "deletions": 0
            },
            {
                "sha": "f0070d8dbc4c5cb6b554de9c6867baad41502a79",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java",
                "blob_url": "https://github.com/apache/hbase/blob/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java",
                "raw_url": "https://github.com/apache/hbase/raw/7e995b6496a44c057337535363aee778f52318b9/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java",
                "status": "modified",
                "changes": 17,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java?ref=7e995b6496a44c057337535363aee778f52318b9",
                "patch": "@@ -45,8 +45,10 @@\n import org.apache.hadoop.hbase.TableDescriptor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotEnabledException;\n+import org.apache.hadoop.hbase.Waiter;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.Durability;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Put;\n import org.apache.hadoop.hbase.client.Table;\n@@ -633,27 +635,32 @@ public static void waitForTableToBeOnline(final HBaseTestingUtility util,\n     for (HRegion region : onlineRegions) {\n       region.waitForFlushesAndCompactions();\n     }\n-    util.getHBaseAdmin().isTableAvailable(tableName);\n+    // Wait up to 60 seconds for a table to be available.\n+    final HBaseAdmin hBaseAdmin = util.getHBaseAdmin();\n+    util.waitFor(60000, new Waiter.Predicate<IOException>() {\n+      @Override\n+      public boolean evaluate() throws IOException {\n+        return hBaseAdmin.isTableAvailable(tableName);\n+      }\n+    });\n   }\n \n   public static void createTable(final HBaseTestingUtility util, final TableName tableName,\n       int regionReplication, final byte[]... families) throws IOException, InterruptedException {\n     HTableDescriptor htd = new HTableDescriptor(tableName);\n     htd.setRegionReplication(regionReplication);\n-    for (byte[] family: families) {\n+    for (byte[] family : families) {\n       HColumnDescriptor hcd = new HColumnDescriptor(family);\n       htd.addFamily(hcd);\n     }\n     byte[][] splitKeys = getSplitKeys();\n-    util.getHBaseAdmin().createTable(htd, splitKeys);\n-    waitForTableToBeOnline(util, tableName);\n+    util.createTable(htd, splitKeys);\n     assertEquals((splitKeys.length + 1) * regionReplication,\n         util.getHBaseAdmin().getTableRegions(tableName).size());\n   }\n \n   public static byte[][] getSplitKeys() {\n     byte[][] splitKeys = new byte[KEYS.length-2][];\n-    byte[] hex = Bytes.toBytes(\"123456789abcde\");\n     for (int i = 0; i < splitKeys.length; ++i) {\n       splitKeys[i] = new byte[] { KEYS[i+1] };\n     }",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10630 NullPointerException in ConnectionManager$HConnectionImplementation.locateRegionInMeta() due to missing region info\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1572761 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/c997c3311e39b218d197c9119bcc42793356fa04",
        "parent": "https://github.com/apache/hbase/commit/a98f52953a0d8fdde2eb37110436967c7121d52c",
        "bug_id": "hbase_142",
        "file": [
            {
                "sha": "9a4c8e624563d4b94731b022791ef89136d3b6a8",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/c997c3311e39b218d197c9119bcc42793356fa04/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/c997c3311e39b218d197c9119bcc42793356fa04/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionManager.java?ref=c997c3311e39b218d197c9119bcc42793356fa04",
                "patch": "@@ -1065,6 +1065,10 @@ private RegionLocations locateRegionInMeta(TableName tableName, byte[] row,\n \n           // convert the row result into the HRegionLocation we need!\n           RegionLocations locations = MetaReader.getRegionLocations(regionInfoRow);\n+          if (locations == null || locations.getRegionLocation() == null) {\n+            throw new IOException(\"HRegionInfo was null in \" +\n+              tableName + \", row=\" + regionInfoRow);\n+          }\n           HRegionInfo regionInfo = locations.getRegionLocation().getRegionInfo();\n           if (regionInfo == null) {\n             throw new IOException(\"HRegionInfo was null or empty in \" +",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12966 NPE in HMaster while recovering tables in Enabling state; ADDENDUM",
        "commit": "https://github.com/apache/hbase/commit/2f2faa68d1b158f3d44b1ea4f2ef832705933f3f",
        "parent": "https://github.com/apache/hbase/commit/eddd5739a14ceb5cfc9b9c7d2e357eea96bd9703",
        "bug_id": "hbase_143",
        "file": [
            {
                "sha": "d3d623944857e222ce81e773cfb866114bb59756",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/2f2faa68d1b158f3d44b1ea4f2ef832705933f3f/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/2f2faa68d1b158f3d44b1ea4f2ef832705933f3f/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java?ref=2f2faa68d1b158f3d44b1ea4f2ef832705933f3f",
                "patch": "@@ -85,6 +85,7 @@ public void testEnableTableWithNoRegionServers() throws Exception {\n     // disable once more\n     admin.disableTable(tableName);\n \n+    TEST_UTIL.waitUntilNoRegionsInTransition(60000);\n     // now stop region servers\n     JVMClusterUtil.RegionServerThread rs = cluster.getRegionServerThreads().get(0);\n     rs.getRegionServer().stop(\"stop\");",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12966 NPE in HMaster while recovering tables in Enabling state (Andrey Stepachev)",
        "commit": "https://github.com/apache/hbase/commit/8aeb3acaf959e2905191fd6c92fa56300f7d3597",
        "parent": "https://github.com/apache/hbase/commit/4f472062a436726f686608023126dd8d5cc9c9bc",
        "bug_id": "hbase_144",
        "file": [
            {
                "sha": "c4969be90659a87315b51ae5489032c909533def",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "status": "modified",
                "changes": 35,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java?ref=8aeb3acaf959e2905191fd6c92fa56300f7d3597",
                "patch": "@@ -26,14 +26,14 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.hbase.classification.InterfaceAudience;\n-import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n-import org.apache.hadoop.hbase.MetaTableAccessor;\n+import org.apache.hadoop.hbase.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.client.TableState;\n import org.apache.hadoop.hbase.executor.EventHandler;\n import org.apache.hadoop.hbase.executor.EventType;\n@@ -207,19 +207,26 @@ private void handleEnableTable() throws IOException,\n     List<ServerName> onlineServers = serverManager.createDestinationServersList();\n     Map<ServerName, List<HRegionInfo>> bulkPlan =\n         this.assignmentManager.getBalancer().retainAssignment(regionsToAssign, onlineServers);\n-    LOG.info(\"Bulk assigning \" + regionsCount + \" region(s) across \" + bulkPlan.size()\n-      + \" server(s), retainAssignment=true\");\n+    if (bulkPlan != null) {\n+      LOG.info(\"Bulk assigning \" + regionsCount + \" region(s) across \" + bulkPlan.size()\n+          + \" server(s), retainAssignment=true\");\n \n-    BulkAssigner ba = new GeneralBulkAssigner(this.server, bulkPlan, this.assignmentManager, true);\n-    try {\n-      if (ba.bulkAssign()) {\n-        done = true;\n+      BulkAssigner ba =\n+          new GeneralBulkAssigner(this.server, bulkPlan, this.assignmentManager, true);\n+      try {\n+        if (ba.bulkAssign()) {\n+          done = true;\n+        }\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"Enable operation was interrupted when enabling table '\"\n+            + this.tableName + \"'\");\n+        // Preserve the interrupt.\n+        Thread.currentThread().interrupt();\n       }\n-    } catch (InterruptedException e) {\n-      LOG.warn(\"Enable operation was interrupted when enabling table '\"\n-        + this.tableName + \"'\");\n-      // Preserve the interrupt.\n-      Thread.currentThread().interrupt();\n+    } else {\n+      LOG.info(\"Balancer was unable to find suitable servers for table \" + tableName\n+          + \", leaving unassigned\");\n+      done = true;\n     }\n     if (done) {\n       // Flip the table to enabled.",
                "deletions": 14
            },
            {
                "sha": "f5f2cd056df54c620a9f3aecf2bc882fc1fcd932",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/8aeb3acaf959e2905191fd6c92fa56300f7d3597/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java",
                "status": "added",
                "changes": 101,
                "additions": 101,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/handler/TestEnableTableHandler.java?ref=8aeb3acaf959e2905191fd6c92fa56300f7d3597",
                "patch": "@@ -0,0 +1,101 @@\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.handler;\n+\n+import java.util.List;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.MiniHBaseCluster;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.MediumTests;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.util.JVMClusterUtil;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+@Category({ MasterTests.class, MediumTests.class })\n+public class TestEnableTableHandler {\n+  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n+  private static final Log LOG = LogFactory.getLog(TestEnableTableHandler.class);\n+  private static final byte[] FAMILYNAME = Bytes.toBytes(\"fam\");\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    TEST_UTIL.startMiniCluster(1);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    TEST_UTIL.shutdownMiniCluster();\n+  }\n+\n+  @Test(timeout = 300000)\n+  public void testEnableTableWithNoRegionServers() throws Exception {\n+    final TableName tableName = TableName.valueOf(\"testEnableTableWithNoRegionServers\");\n+    final MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();\n+    final HMaster m = cluster.getMaster();\n+    final HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();\n+    final HTableDescriptor desc = new HTableDescriptor(tableName);\n+    desc.addFamily(new HColumnDescriptor(FAMILYNAME));\n+    admin.createTable(desc);\n+    admin.disableTable(tableName);\n+    TEST_UTIL.waitTableDisabled(tableName.getName());\n+\n+    admin.enableTable(tableName);\n+    TEST_UTIL.waitTableEnabled(tableName);\n+\n+    // disable once more\n+    admin.disableTable(tableName);\n+\n+    // now stop region servers\n+    JVMClusterUtil.RegionServerThread rs = cluster.getRegionServerThreads().get(0);\n+    rs.getRegionServer().stop(\"stop\");\n+    cluster.waitForRegionServerToStop(rs.getRegionServer().getServerName(), 10000);\n+\n+    EnableTableHandler handler =\n+        new EnableTableHandler(m, tableName, m.getAssignmentManager(), m.getTableLockManager(),\n+            true);\n+    handler.prepare();\n+    handler.process();\n+\n+    assertTrue(admin.isTableEnabled(tableName));\n+\n+    JVMClusterUtil.RegionServerThread rs2 = cluster.startRegionServer();\n+    m.getAssignmentManager().assign(admin.getTableRegions(tableName));\n+    TEST_UTIL.waitUntilAllRegionsAssigned(tableName);\n+    List<HRegionInfo> onlineRegions = admin.getOnlineRegions(\n+        rs2.getRegionServer().getServerName());\n+    assertEquals(1, onlineRegions.size());\n+    assertEquals(tableName, onlineRegions.get(0).getTable());\n+  }\n+\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4907 NPE of MobUtils.hasMobColumns in Build failed in Jenkins: HBase-Trunk_matrix \u00bb latest1.8,Hadoop #513 (Jingcheng Du)",
        "commit": "https://github.com/apache/hbase/commit/03e4712f0ca08d57586b3fc4d93cf02c999515d8",
        "parent": "https://github.com/apache/hbase/commit/8b3d1f144408e4a7a014c5ac46418c9e91b9b0db",
        "bug_id": "hbase_145",
        "file": [
            {
                "sha": "1e86254a061de41e730c62f0b5bf28a69db51462",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "status": "modified",
                "changes": 23,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java?ref=03e4712f0ca08d57586b3fc4d93cf02c999515d8",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HRegionInfo;\n-import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -345,19 +344,13 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n       LOG.debug(\"Table '\" + tableName + \"' archived!\");\n     }\n \n-    // Archive the mob data if there is a mob-enabled column\n-    HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(tableName);\n-    boolean hasMob = MobUtils.hasMobColumns(htd);\n-    Path mobTableDir = null;\n-    if (hasMob) {\n-      // Archive mob data\n-      mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n-              tableName);\n-      Path regionDir =\n-              new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n-      if (fs.exists(regionDir)) {\n-        HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n-      }\n+    // Archive mob data\n+    Path mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n+            tableName);\n+    Path regionDir =\n+            new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n+    if (fs.exists(regionDir)) {\n+      HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n     }\n \n     // Delete table directory from FS (temp directory)\n@@ -366,7 +359,7 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n     }\n \n     // Delete the table directory where the mob files are saved\n-    if (hasMob && mobTableDir != null && fs.exists(mobTableDir)) {\n+    if (mobTableDir != null && fs.exists(mobTableDir)) {\n       if (!fs.delete(mobTableDir, true)) {\n         throw new IOException(\"Couldn't delete mob dir \" + mobTableDir);\n       }",
                "deletions": 15
            },
            {
                "sha": "676a3f4dec947a85be7f243cf086f23b976c233e",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/03e4712f0ca08d57586b3fc4d93cf02c999515d8/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "status": "modified",
                "changes": 58,
                "additions": 34,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java?ref=03e4712f0ca08d57586b3fc4d93cf02c999515d8",
                "patch": "@@ -195,32 +195,15 @@ public void testRecoveryAndDoubleExecution() throws Exception {\n   @Test(timeout=90000)\n   public void testRollbackAndDoubleExecution() throws Exception {\n     final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecution\");\n+    testRollbackAndDoubleExecution(MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\"));\n+  }\n \n-    // create the table\n-    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n-    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n-\n-    // Start the Create procedure && kill the executor\n-    final byte[][] splitKeys = new byte[][] {\n-      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n-    };\n+  @Test(timeout=90000)\n+  public void testRollbackAndDoubleExecutionOnMobTable() throws Exception {\n+    final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecutionOnMobTable\");\n     HTableDescriptor htd = MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\");\n-    htd.setRegionReplication(3);\n-    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n-    long procId = procExec.submitProcedure(\n-      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n-\n-    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n-    //       so you have to look at this test at least once when you add a new step.\n-    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n-        procExec, procId, 4, CreateTableState.values());\n-\n-    MasterProcedureTestingUtility.validateTableDeletion(\n-      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n-\n-    // are we able to create the table after a rollback?\n-    resetProcExecutorTestingKillFlag();\n-    testSimpleCreate(tableName, splitKeys);\n+    htd.getFamily(Bytes.toBytes(\"f1\")).setMobEnabled(true);\n+    testRollbackAndDoubleExecution(htd);\n   }\n \n   @Test(timeout=90000)\n@@ -282,4 +265,31 @@ protected void rollbackState(final MasterProcedureEnv env, final CreateTableStat\n       }\n     }\n   }\n+\n+  private void testRollbackAndDoubleExecution(HTableDescriptor htd) throws Exception {\n+    // create the table\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+\n+    // Start the Create procedure && kill the executor\n+    final byte[][] splitKeys = new byte[][] {\n+      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n+    };\n+    htd.setRegionReplication(3);\n+    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n+    long procId = procExec.submitProcedure(\n+      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n+\n+    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n+    //       so you have to look at this test at least once when you add a new step.\n+    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n+        procExec, procId, 4, CreateTableState.values());\n+    TableName tableName = htd.getTableName();\n+    MasterProcedureTestingUtility.validateTableDeletion(\n+      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n+\n+    // are we able to create the table after a rollback?\n+    resetProcExecutorTestingKillFlag();\n+    testSimpleCreate(tableName, splitKeys);\n+  }\n }",
                "deletions": 24
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-17501 Revert \"Revert \"guard against NPE while reading FileTrailer and HFileBlock\"\"\n\nThis reverts commit 9a4068dcf8caec644e6703ffa365a8649bbd336e.\n\nThis is a revert of a revert -- i.e. a restore -- just so I can add the\nJIRA issue to the commit message.",
        "commit": "https://github.com/apache/hbase/commit/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
        "parent": "https://github.com/apache/hbase/commit/9a4068dcf8caec644e6703ffa365a8649bbd336e",
        "bug_id": "hbase_146",
        "file": [
            {
                "sha": "185423603628a26c1bd271dfaeddb4b81621ff86",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "blob_url": "https://github.com/apache/hbase/blob/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "raw_url": "https://github.com/apache/hbase/raw/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java?ref=6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
                "patch": "@@ -388,7 +388,8 @@ public static FixedFileTrailer readFromStream(FSDataInputStream istream,\n       bufferSize = (int) fileSize;\n     }\n \n-    istream.seek(seekPoint);\n+    HFileUtil.seekOnMultipleSources(istream, seekPoint);\n+\n     ByteBuffer buf = ByteBuffer.allocate(bufferSize);\n     istream.readFully(buf.array(), buf.arrayOffset(),\n         buf.arrayOffset() + buf.limit());",
                "deletions": 1
            },
            {
                "sha": "0b140b6e301eff851c9c78f30252f64fc4eced2e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "blob_url": "https://github.com/apache/hbase/blob/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "raw_url": "https://github.com/apache/hbase/raw/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java?ref=6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
                "patch": "@@ -1512,7 +1512,7 @@ protected int readAtOffset(FSDataInputStream istream, byte [] dest, int destOffs\n       if (!pread && streamLock.tryLock()) {\n         // Seek + read. Better for scanning.\n         try {\n-          istream.seek(fileOffset);\n+          HFileUtil.seekOnMultipleSources(istream, fileOffset);\n \n           long realOffset = istream.getPos();\n           if (realOffset != fileOffset) {",
                "deletions": 1
            },
            {
                "sha": "835450c2a9a8c8227ae4afe469bd0ec9d272dd49",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java",
                "status": "added",
                "changes": 43,
                "additions": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileUtil.java?ref=6fb44f7eb8ded5496d2348af21a9d5ca0dd39ab4",
                "patch": "@@ -0,0 +1,43 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.io.hfile;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+public class HFileUtil {\n+\n+  /** guards against NullPointer\n+   * utility which tries to seek on the DFSIS and will try an alternative source\n+   * if the FSDataInputStream throws an NPE HBASE-17501\n+   * @param istream\n+   * @param offset\n+   * @throws IOException\n+   */\n+  static public void seekOnMultipleSources(FSDataInputStream istream, long offset) throws IOException {\n+    try {\n+      // attempt to seek inside of current blockReader\n+      istream.seek(offset);\n+    } catch (NullPointerException e) {\n+      // retry the seek on an alternate copy of the data\n+      // this can occur if the blockReader on the DFSInputStream is null\n+      istream.seekToNewSource(offset);\n+    }\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12696 Possible NPE in HRegionFileSystem#splitStoreFile when skipStoreFileRangeCheck in splitPolicy return true(Rajeshbabu)",
        "commit": "https://github.com/apache/hbase/commit/96c6b9815ddbc9f2589655df4ad2381af04ac9f8",
        "parent": "https://github.com/apache/hbase/commit/110c5f593057366509b8480e396cc750d5fd782b",
        "bug_id": "hbase_147",
        "file": [
            {
                "sha": "0751634263ab838e9df321af31e671060abdaa7d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "blob_url": "https://github.com/apache/hbase/blob/96c6b9815ddbc9f2589655df4ad2381af04ac9f8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "raw_url": "https://github.com/apache/hbase/raw/96c6b9815ddbc9f2589655df4ad2381af04ac9f8/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=96c6b9815ddbc9f2589655df4ad2381af04ac9f8",
                "patch": "@@ -601,7 +601,7 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName, final StoreF\n       }\n     }\n \n-    f.getReader().close(true);\n+    f.closeReader(true);\n \n     Path splitDir = new Path(getSplitsDir(hri), familyName);\n     // A reference to the bottom half of the hsf store file.",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12535 NPE in WALFactory under contention for getInstance()\n\nSigned-off-by: stack <stack@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/db2b6421ff52608f56522684461a4de3f491744f",
        "parent": "https://github.com/apache/hbase/commit/05ced20a347213c31e46c528e53b93c675f85891",
        "bug_id": "hbase_148",
        "file": [
            {
                "sha": "3500bd0ff885a139682e4ff1105b0c2845e66844",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java",
                "blob_url": "https://github.com/apache/hbase/blob/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java",
                "raw_url": "https://github.com/apache/hbase/raw/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALFactory.java?ref=db2b6421ff52608f56522684461a4de3f491744f",
                "patch": "@@ -187,7 +187,11 @@ public void close() throws IOException {\n     if (null != metaProvider) {\n       metaProvider.close();\n     }\n-    provider.close();\n+    // close is called on a WALFactory with null provider in the case of contention handling\n+    // within the getInstance method.\n+    if (null != provider) {\n+      provider.close();\n+    }\n   }\n \n   /**",
                "deletions": 1
            },
            {
                "sha": "47b001aa73d9274eddc14d23761e9942df4231c5",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java",
                "blob_url": "https://github.com/apache/hbase/blob/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java",
                "raw_url": "https://github.com/apache/hbase/raw/db2b6421ff52608f56522684461a4de3f491744f/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestWALFactory.java?ref=db2b6421ff52608f56522684461a4de3f491744f",
                "patch": "@@ -152,6 +152,11 @@ public static void tearDownAfterClass() throws Exception {\n     TEST_UTIL.shutdownMiniCluster();\n   }\n \n+  @Test\n+  public void canCloseSingleton() throws IOException {\n+    WALFactory.getInstance(conf).close();\n+  }\n+\n   /**\n    * Just write multiple logs then split.  Before fix for HADOOP-2283, this\n    * would fail.",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10108 NullPointerException thrown while using Canary with '-regionserver' option (Takeshi Miao)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1549683 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/08bd55aab2c12b136e2b75941f0c4819bab2acef",
        "parent": "https://github.com/apache/hbase/commit/0e8a141437a6645deb85a730bc9adfb757bb27b8",
        "bug_id": "hbase_149",
        "file": [
            {
                "sha": "eb536a3a6c9ffdb33f4e6558ce150862daa9b911",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java",
                "blob_url": "https://github.com/apache/hbase/blob/08bd55aab2c12b136e2b75941f0c4819bab2acef/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java",
                "raw_url": "https://github.com/apache/hbase/raw/08bd55aab2c12b136e2b75941f0c4819bab2acef/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java?ref=08bd55aab2c12b136e2b75941f0c4819bab2acef",
                "patch": "@@ -568,6 +568,8 @@ private boolean checkNoTableNames() {\n         return false;\n       }\n \n+      if (this.targets == null || this.targets.length == 0) return true;\n+\n       for (String target : this.targets) {\n         for (TableName tableName : tableNames) {\n           if (target.equals(tableName.getNameAsString())) {",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14366 NPE in case visibility expression is not present in labels table during importtsv run. (Bhupendra)",
        "commit": "https://github.com/apache/hbase/commit/4969879df5f6d1d9a2774236cdee1710694ddf6a",
        "parent": "https://github.com/apache/hbase/commit/2e593a9d3801a42751244ab4478650a581437875",
        "bug_id": "hbase_150",
        "file": [
            {
                "sha": "782d1637d915296b052a24d4f517c07b28463401",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java",
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityConstants.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "patch": "@@ -58,4 +58,7 @@\n   public static final String OPEN_PARAN = \"(\";\n   public static final String CLOSED_PARAN = \")\";\n \n+  /** Label ordinal value for invalid labels */\n+  public static final int NON_EXIST_LABEL_ORDINAL = 0;\n+\n }",
                "deletions": 0
            },
            {
                "sha": "597a7641e822b940b18630425ca96485a86b4b10",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java",
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/DefaultVisibilityExpressionResolver.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.client.Table;\n import org.apache.hadoop.hbase.security.visibility.Authorizations;\n+import org.apache.hadoop.hbase.security.visibility.VisibilityConstants;\n import org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider;\n import org.apache.hadoop.hbase.security.visibility.VisibilityUtils;\n import org.apache.hadoop.hbase.util.Bytes;\n@@ -124,7 +125,12 @@ public void init() {\n     VisibilityLabelOrdinalProvider provider = new VisibilityLabelOrdinalProvider() {\n       @Override\n       public int getLabelOrdinal(String label) {\n-        return labels.get(label);\n+        Integer ordinal = null;\n+        ordinal = labels.get(label);\n+        if (ordinal != null) {\n+          return ordinal.intValue();\n+        }\n+        return VisibilityConstants.NON_EXIST_LABEL_ORDINAL;\n       }\n \n       @Override",
                "deletions": 1
            },
            {
                "sha": "8fe5cc7407683b136d7ccb05793fdfaf705b94dc",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java",
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java",
                "status": "modified",
                "changes": 17,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TextSortReducer.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "patch": "@@ -24,16 +24,17 @@\n import java.util.Set;\n import java.util.TreeSet;\n \n-import org.apache.hadoop.hbase.classification.InterfaceAudience;\n-import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.CellComparator;\n import org.apache.hadoop.hbase.KeyValue;\n import org.apache.hadoop.hbase.KeyValueUtil;\n import org.apache.hadoop.hbase.Tag;\n import org.apache.hadoop.hbase.TagType;\n+import org.apache.hadoop.hbase.classification.InterfaceAudience;\n+import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.security.visibility.InvalidLabelException;\n import org.apache.hadoop.hbase.util.Base64;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.Text;\n@@ -186,21 +187,15 @@ protected void reduce(\n             kvs.add(kv);\n             curSize += kv.heapSize();\n           }\n-        } catch (ImportTsv.TsvParser.BadTsvLineException badLine) {\n+        } catch (ImportTsv.TsvParser.BadTsvLineException | IllegalArgumentException\n+            | InvalidLabelException badLine) {\n           if (skipBadLines) {\n             System.err.println(\"Bad line.\" + badLine.getMessage());\n             incrementBadLineCount(1);\n             continue;\n           }\n           throw new IOException(badLine);\n-        } catch (IllegalArgumentException e) {\n-          if (skipBadLines) {\n-            System.err.println(\"Bad line.\" + e.getMessage());\n-            incrementBadLineCount(1);\n-            continue;\n-          } \n-          throw new IOException(e);\n-        } \n+        }\n       }\n       context.setStatus(\"Read \" + kvs.size() + \" entries of \" + kvs.getClass()\n           + \"(\" + StringUtils.humanReadableInt(curSize) + \")\");",
                "deletions": 11
            },
            {
                "sha": "98dc25eaaf08a8d680f068bf10060a47cfa1a81d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java",
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java",
                "status": "modified",
                "changes": 8,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "patch": "@@ -21,17 +21,18 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.hbase.classification.InterfaceAudience;\n-import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.KeyValue;\n import org.apache.hadoop.hbase.Tag;\n import org.apache.hadoop.hbase.TagType;\n+import org.apache.hadoop.hbase.classification.InterfaceAudience;\n+import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Put;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser.BadTsvLineException;\n import org.apache.hadoop.hbase.security.visibility.CellVisibility;\n+import org.apache.hadoop.hbase.security.visibility.InvalidLabelException;\n import org.apache.hadoop.hbase.util.Base64;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.io.LongWritable;\n@@ -182,7 +183,8 @@ public void map(LongWritable offset, Text value,\n         populatePut(lineBytes, parsed, put, i);\n       }\n       context.write(rowKey, put);\n-    } catch (ImportTsv.TsvParser.BadTsvLineException|IllegalArgumentException badLine) {\n+    } catch (ImportTsv.TsvParser.BadTsvLineException | IllegalArgumentException\n+        | InvalidLabelException badLine) {\n       if (logBadLines) {\n         System.err.println(value);\n       }",
                "deletions": 3
            },
            {
                "sha": "0948520b0c5d7ba4ab0bee22e9ab663273e0ed0d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java",
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "patch": "@@ -49,7 +49,6 @@\n public class VisibilityLabelsCache implements VisibilityLabelOrdinalProvider {\n \n   private static final Log LOG = LogFactory.getLog(VisibilityLabelsCache.class);\n-  private static final int NON_EXIST_LABEL_ORDINAL = 0;\n   private static final List<String> EMPTY_LIST = Collections.emptyList();\n   private static final Set<Integer> EMPTY_SET = Collections.emptySet();\n   private static VisibilityLabelsCache instance;\n@@ -175,7 +174,7 @@ public int getLabelOrdinal(String label) {\n       return ordinal.intValue();\n     }\n     // 0 denotes not available\n-    return NON_EXIST_LABEL_ORDINAL;\n+    return VisibilityConstants.NON_EXIST_LABEL_ORDINAL;\n   }\n \n   /**",
                "deletions": 2
            },
            {
                "sha": "3aec6694d76d832177a66534c90e9cf364ddaaef",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java",
                "blob_url": "https://github.com/apache/hbase/blob/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java",
                "raw_url": "https://github.com/apache/hbase/raw/4969879df5f6d1d9a2774236cdee1710694ddf6a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java",
                "status": "modified",
                "changes": 95,
                "additions": 86,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTSVWithVisibilityLabels.java?ref=4969879df5f6d1d9a2774236cdee1710694ddf6a",
                "patch": "@@ -42,18 +42,16 @@\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.TableName;\n-import org.apache.hadoop.hbase.client.Admin;\n-import org.apache.hadoop.hbase.testclassification.LargeTests;\n-import org.apache.hadoop.hbase.testclassification.MapReduceTests;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Delete;\n-import org.apache.hadoop.hbase.client.HBaseAdmin;\n-import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Result;\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n import org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.VisibilityLabelsResponse;\n import org.apache.hadoop.hbase.security.User;\n import org.apache.hadoop.hbase.security.visibility.Authorizations;\n@@ -64,6 +62,8 @@\n import org.apache.hadoop.hbase.security.visibility.VisibilityConstants;\n import org.apache.hadoop.hbase.security.visibility.VisibilityController;\n import org.apache.hadoop.hbase.security.visibility.VisibilityUtils;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.MapReduceTests;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.mapred.Utils.OutputFileUtils.OutputFilesFilter;\n import org.apache.hadoop.util.Tool;\n@@ -274,6 +274,50 @@ public void testMRWithOutputFormat() throws Exception {\n     util.deleteTable(tableName);\n   }\n \n+  @Test\n+  public void testBulkOutputWithInvalidLabels() throws Exception {\n+    TableName tableName = TableName.valueOf(\"test-\" + UUID.randomUUID());\n+    Path hfiles = new Path(util.getDataTestDirOnTestFS(tableName.getNameAsString()), \"hfiles\");\n+    // Prepare the arguments required for the test.\n+    String[] args =\n+        new String[] { \"-D\" + ImportTsv.BULK_OUTPUT_CONF_KEY + \"=\" + hfiles.toString(),\n+            \"-D\" + ImportTsv.COLUMNS_CONF_KEY + \"=HBASE_ROW_KEY,FAM:A,FAM:B,HBASE_CELL_VISIBILITY\",\n+            \"-D\" + ImportTsv.SEPARATOR_CONF_KEY + \"=\\u001b\", tableName.getNameAsString() };\n+\n+    // 2 Data rows, one with valid label and one with invalid label\n+    String data =\n+        \"KEY\\u001bVALUE1\\u001bVALUE2\\u001bprivate\\nKEY1\\u001bVALUE1\\u001bVALUE2\\u001binvalid\\n\";\n+    util.createTable(tableName, FAMILY);\n+    doMROnTableTest(util, FAMILY, data, args, 1, 2);\n+    util.deleteTable(tableName);\n+  }\n+\n+  @Test\n+  public void testBulkOutputWithTsvImporterTextMapperWithInvalidLabels() throws Exception {\n+    TableName tableName = TableName.valueOf(\"test-\" + UUID.randomUUID());\n+    Path hfiles = new Path(util.getDataTestDirOnTestFS(tableName.getNameAsString()), \"hfiles\");\n+    // Prepare the arguments required for the test.\n+    String[] args =\n+        new String[] {\n+            \"-D\" + ImportTsv.MAPPER_CONF_KEY\n+                + \"=org.apache.hadoop.hbase.mapreduce.TsvImporterTextMapper\",\n+            \"-D\" + ImportTsv.BULK_OUTPUT_CONF_KEY + \"=\" + hfiles.toString(),\n+            \"-D\" + ImportTsv.COLUMNS_CONF_KEY + \"=HBASE_ROW_KEY,FAM:A,FAM:B,HBASE_CELL_VISIBILITY\",\n+            \"-D\" + ImportTsv.SEPARATOR_CONF_KEY + \"=\\u001b\", tableName.getNameAsString() };\n+\n+    // 2 Data rows, one with valid label and one with invalid label\n+    String data =\n+        \"KEY\\u001bVALUE1\\u001bVALUE2\\u001bprivate\\nKEY1\\u001bVALUE1\\u001bVALUE2\\u001binvalid\\n\";\n+    util.createTable(tableName, FAMILY);\n+    doMROnTableTest(util, FAMILY, data, args, 1, 2);\n+    util.deleteTable(tableName);\n+  }\n+\n+  protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, String data,\n+      String[] args, int valueMultiplier) throws Exception {\n+    return doMROnTableTest(util, family, data, args, valueMultiplier, -1);\n+  }\n+\n   /**\n    * Run an ImportTsv job and perform basic validation on the results. Returns\n    * the ImportTsv <code>Tool</code> instance so that other tests can inspect it\n@@ -282,10 +326,13 @@ public void testMRWithOutputFormat() throws Exception {\n    *\n    * @param args\n    *          Any arguments to pass BEFORE inputFile path is appended.\n+   *\n+   * @param expectedKVCount Expected KV count. pass -1 to skip the kvcount check\n+   *\n    * @return The Tool instance used to run the test.\n    */\n   protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, String data,\n-      String[] args, int valueMultiplier) throws Exception {\n+      String[] args, int valueMultiplier,int expectedKVCount) throws Exception {\n     TableName table = TableName.valueOf(args[args.length - 1]);\n     Configuration conf = new Configuration(util.getConfiguration());\n \n@@ -328,7 +375,7 @@ protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, S\n     }\n     LOG.debug(\"validating the table \" + createdHFiles);\n     if (createdHFiles)\n-     validateHFiles(fs, outputPath, family);\n+     validateHFiles(fs, outputPath, family,expectedKVCount);\n     else\n       validateTable(conf, table, family, valueMultiplier);\n \n@@ -342,14 +389,15 @@ protected static Tool doMROnTableTest(HBaseTestingUtility util, String family, S\n   /**\n    * Confirm ImportTsv via HFiles on fs.\n    */\n-  private static void validateHFiles(FileSystem fs, String outputPath, String family)\n-      throws IOException {\n+  private static void validateHFiles(FileSystem fs, String outputPath, String family,\n+      int expectedKVCount) throws IOException {\n \n     // validate number and content of output columns\n     LOG.debug(\"Validating HFiles.\");\n     Set<String> configFamilies = new HashSet<String>();\n     configFamilies.add(family);\n     Set<String> foundFamilies = new HashSet<String>();\n+    int actualKVCount = 0;\n     for (FileStatus cfStatus : fs.listStatus(new Path(outputPath), new OutputFilesFilter())) {\n       LOG.debug(\"The output path has files\");\n       String[] elements = cfStatus.getPath().toString().split(Path.SEPARATOR);\n@@ -361,8 +409,16 @@ private static void validateHFiles(FileSystem fs, String outputPath, String fami\n       for (FileStatus hfile : fs.listStatus(cfStatus.getPath())) {\n         assertTrue(String.format(\"HFile %s appears to contain no data.\", hfile.getPath()),\n             hfile.getLen() > 0);\n+        if (expectedKVCount > -1) {\n+          actualKVCount += getKVCountFromHfile(fs, hfile.getPath());\n+        }\n       }\n     }\n+    if (expectedKVCount > -1) {\n+      assertTrue(String.format(\n+        \"KV count in output hfile=<%d> doesn't match with expected KV count=<%d>\", actualKVCount,\n+        expectedKVCount), actualKVCount == expectedKVCount);\n+    }\n   }\n \n   /**\n@@ -412,4 +468,25 @@ private static void validateTable(Configuration conf, TableName tableName, Strin\n     assertTrue(verified);\n   }\n \n+  /**\n+   * Method returns the total KVs in given hfile\n+   * @param fs File System\n+   * @param p HFile path\n+   * @return KV count in the given hfile\n+   * @throws IOException\n+   */\n+  private static int getKVCountFromHfile(FileSystem fs, Path p) throws IOException {\n+    Configuration conf = util.getConfiguration();\n+    HFile.Reader reader = HFile.createReader(fs, p, new CacheConfig(conf), conf);\n+    reader.loadFileInfo();\n+    HFileScanner scanner = reader.getScanner(false, false);\n+    scanner.seekTo();\n+    int count = 0;\n+    do {\n+      count++;\n+    } while (scanner.next());\n+    reader.close();\n+    return count;\n+  }\n+\n }",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8374 NullPointerException when launching the balancer due to unknown region location (Ted Yu)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1469954 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a2af3a76a22bf4e8e07396606927b58c6da7c771",
        "parent": "https://github.com/apache/hbase/commit/e946c2871f1c1dea9b767845f1c529682c6ecd5b",
        "bug_id": "hbase_151",
        "file": [
            {
                "sha": "94149a5dbb5d128786b44e277bc3a6666e161003",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java",
                "status": "modified",
                "changes": 9,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java?ref=a2af3a76a22bf4e8e07396606927b58c6da7c771",
                "patch": "@@ -105,10 +105,15 @@ protected Cluster(Map<ServerName, List<HRegionInfo>> clusterState,  Map<String,\n       regionLocations = new int[numRegions][];\n \n       int tableIndex = 0, serverIndex = 0, regionIndex = 0, regionPerServerIndex = 0;\n+      // populate serversToIndex first\n       for (Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {\n         servers[serverIndex] = entry.getKey();\n         regionsPerServer[serverIndex] = new int[entry.getValue().size()];\n         serversToIndex.put(servers[serverIndex], Integer.valueOf(serverIndex));\n+        serverIndex++;\n+      }\n+      serverIndex = 0;\n+      for (Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {\n         regionPerServerIndex = 0;\n         for (HRegionInfo region : entry.getValue()) {\n           byte[] tableName = region.getTableName();\n@@ -142,7 +147,9 @@ protected Cluster(Map<ServerName, List<HRegionInfo>> clusterState,  Map<String,\n             List<ServerName> loc = regionFinder.getTopBlockLocations(region);\n             regionLocations[regionIndex] = new int[loc.size()];\n             for (int i=0; i < loc.size(); i++) {\n-              regionLocations[regionIndex][i] = serversToIndex.get(loc.get(i));\n+              regionLocations[regionIndex][i] =\n+                  loc.get(i) == null ? -1 :\n+                    (serversToIndex.get(loc.get(i)) == null ? -1 : serversToIndex.get(loc.get(i)));\n             }\n           }\n ",
                "deletions": 1
            },
            {
                "sha": "330e9b18ad7720ed0f9a9b4190c2a359ee067c3b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a2af3a76a22bf4e8e07396606927b58c6da7c771/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java?ref=a2af3a76a22bf4e8e07396606927b58c6da7c771",
                "patch": "@@ -505,7 +505,7 @@ protected double computeCost(Cluster cluster) {\n \n       int index = -1;\n       for (int j = 0; j < regionLocations.length; j++) {\n-        if (regionLocations[j] == serverIndex) {\n+        if (regionLocations[j] >= 0 && regionLocations[j] == serverIndex) {\n           index = j;\n           break;\n         }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6607 NullPointerException when accessing master web ui while master is initializing\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1375673 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/280c6124b5b92dae2149f7565c59157ad2b41640",
        "parent": "https://github.com/apache/hbase/commit/94f8d617ddaf989fe071ec712ac077611da69e6f",
        "bug_id": "hbase_152",
        "file": [
            {
                "sha": "60fbbf86a20ae45dc8a9a7f1cc0273b3a4b76e75",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/280c6124b5b92dae2149f7565c59157ad2b41640/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/280c6124b5b92dae2149f7565c59157ad2b41640/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon?ref=280c6124b5b92dae2149f7565c59157ad2b41640",
                "patch": "@@ -120,7 +120,7 @@ org.apache.hadoop.hbase.HBaseConfiguration;\n           for details.\n           </div>\n         </%if>\n-        <%if !catalogJanitorEnabled %>\n+        <%if master.isInitialized() && !catalogJanitorEnabled %>\n           <div class=\"alert alert-error\">\n           Please note that your cluster is running with the CatalogJanitor disabled. It can be\n           re-enabled from the hbase shell by running the command 'catalogjanitor_switch true'",
                "deletions": 1
            },
            {
                "sha": "02a249b5eda9eeb44d183d9022306d96c9fe370a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/280c6124b5b92dae2149f7565c59157ad2b41640/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/280c6124b5b92dae2149f7565c59157ad2b41640/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=280c6124b5b92dae2149f7565c59157ad2b41640",
                "patch": "@@ -1207,8 +1207,8 @@ public EnableCatalogJanitorResponse enableCatalogJanitor(RpcController c,\n   @Override\n   public IsCatalogJanitorEnabledResponse isCatalogJanitorEnabled(RpcController c,\n       IsCatalogJanitorEnabledRequest req) throws ServiceException {\n-    return IsCatalogJanitorEnabledResponse.newBuilder().\n-        setValue(catalogJanitorChore.getEnabled()).build();\n+    boolean isEnabled = catalogJanitorChore != null ? catalogJanitorChore.getEnabled() : false;\n+    return IsCatalogJanitorEnabledResponse.newBuilder().setValue(isEnabled).build();\n   }\n \n   /**",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6049 Serializing 'List' containing null elements will cause NullPointerException in HbaseObjectWritable.writeObject()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1344363 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9",
        "parent": "https://github.com/apache/hbase/commit/20bd3f02d03743429e28a64e9d2095ad72a690d1",
        "bug_id": "hbase_153",
        "file": [
            {
                "sha": "e19af32ba644fab789bd598e0a15863d89b0d747",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java",
                "blob_url": "https://github.com/apache/hbase/blob/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java",
                "raw_url": "https://github.com/apache/hbase/raw/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java?ref=2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9",
                "patch": "@@ -477,8 +477,9 @@ public static void writeObject(DataOutput out, Object instance,\n       int length = list.size();\n       out.writeInt(length);\n       for (int i = 0; i < length; i++) {\n-        writeObject(out, list.get(i),\n-                  list.get(i).getClass(), conf);\n+        Object elem = list.get(i);\n+        writeObject(out, elem,\n+                  elem == null ? Writable.class : elem.getClass(), conf);\n       }\n     } else if (declClass == String.class) {   // String\n       Text.writeString(out, (String)instanceObj);",
                "deletions": 2
            },
            {
                "sha": "2e666668d63613858fb76e6fa18ea8b16e961046",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java",
                "blob_url": "https://github.com/apache/hbase/blob/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java",
                "raw_url": "https://github.com/apache/hbase/raw/2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java",
                "status": "modified",
                "changes": 8,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java?ref=2bbc95823ca1e8aca4004cb4ac3bd53400cf73d9",
                "patch": "@@ -198,6 +198,14 @@ public void testReadObjectDataInputConfiguration() throws IOException {\n     obj = doType(conf, list, List.class);\n     assertTrue(obj instanceof List);\n     Assert.assertArrayEquals(list.toArray(), ((List)obj).toArray() );\n+    //List.class with null values\n+    List<String> listWithNulls = new ArrayList<String>();\n+    listWithNulls.add(\"hello\");\n+    listWithNulls.add(\"world\");\n+    listWithNulls.add(null);\n+    obj = doType(conf, listWithNulls, List.class);\n+    assertTrue(obj instanceof List);\n+    Assert.assertArrayEquals(listWithNulls.toArray(), ((List)obj).toArray() );\n     //ArrayList.class\n     ArrayList<String> arr = new ArrayList<String>();\n     arr.add(\"hello\");",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4273  java.lang.NullPointerException when a table is being disabled and\n               HMaster restarts (Ming Ma)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1164347 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fa5f75ab0e799f839b8eacc8615f7142b9254110",
        "parent": "https://github.com/apache/hbase/commit/5af490cb6bed065dee0d0e8c2984f2090919fcdf",
        "bug_id": "hbase_154",
        "file": [
            {
                "sha": "fe83fc650d46cc04b53f8cbb628b70b4eb75637c",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -231,6 +231,8 @@ Release 0.91.0 - Unreleased\n    HBASE-4310  SlabCache metrics bugfix (Li Pi)\n    HBASE-4283  HBaseAdmin never recovers from restarted cluster (Lars Hofhansl)\n    HBASE-4315  RPC logging too verbose (todd)\n+   HBASE-4273  java.lang.NullPointerException when a table is being disabled and\n+               HMaster restarts (Ming Ma)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "d0392d52f6e3c6b7f305b1858d6dedc8b4a13bd5",
                "filename": "src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java",
                "status": "added",
                "changes": 50,
                "additions": 50,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -0,0 +1,50 @@\n+/**\n+ * Copyright 2011 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * Thrown if a table should be enabled but is not\n+ */\n+public class TableNotEnabledException extends IOException {\n+  private static final long serialVersionUID = 262144L;\n+  /** default constructor */\n+  public TableNotEnabledException() {\n+    super();\n+  }\n+\n+  /**\n+   * Constructor\n+   * @param s message\n+   */\n+  public TableNotEnabledException(String s) {\n+    super(s);\n+  }\n+\n+  /**\n+   * @param tableName Name of table that is not enabled\n+   */\n+  public TableNotEnabledException(byte[] tableName) {\n+    this(Bytes.toString(tableName));\n+  }\n+}\n\\ No newline at end of file",
                "deletions": 0
            },
            {
                "sha": "f17036e1eeb47f358b4560c01cc101c0e999ccde",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "status": "modified",
                "changes": 8,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -565,8 +565,12 @@ public void enableTable(final String tableName)\n   /**\n    * Enable a table.  May timeout.  Use {@link #enableTableAsync(byte[])}\n    * and {@link #isTableEnabled(byte[])} instead.\n+   * The table has to be in disabled state for it to be enabled.\n    * @param tableName name of the table\n    * @throws IOException if a remote or network exception occurs\n+   * There could be couple types of IOException\n+   * TableNotFoundException means the table doesn't exist.\n+   * TableNotDisabledException means the table isn't in disabled state.\n    * @see #isTableEnabled(byte[])\n    * @see #disableTable(byte[])\n    * @see #enableTableAsync(byte[])\n@@ -706,8 +710,12 @@ public void disableTable(final String tableName)\n    * Disable table and wait on completion.  May timeout eventually.  Use\n    * {@link #disableTableAsync(byte[])} and {@link #isTableDisabled(String)}\n    * instead.\n+   * The table has to be in enabled state for it to be disabled.\n    * @param tableName\n    * @throws IOException\n+   * There could be couple types of IOException\n+   * TableNotFoundException means the table doesn't exist.\n+   * TableNotEnabledException means the table isn't in enabled state.\n    */\n   public void disableTable(final byte [] tableName)\n   throws IOException {",
                "deletions": 0
            },
            {
                "sha": "698c7d3518cef2dd4db136c5b0111f41aaa7b2bc",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 24,
                "additions": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -1830,12 +1830,20 @@ boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regi\n       ServerName regionLocation = region.getSecond();\n       String tableName = regionInfo.getTableNameAsString();\n       if (regionLocation == null) {\n-        // Region not being served, add to region map with no assignment\n-        // If this needs to be assigned out, it will also be in ZK as RIT\n-        // add if the table is not in disabled and enabling state\n-        if (false == checkIfRegionBelongsToDisabled(regionInfo)\n-            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {\n-          regions.put(regionInfo, regionLocation);\n+        // regionLocation could be null if createTable didn't finish properly.\n+        // When createTable is in progress, HMaster restarts.\n+        // Some regions have been added to .META., but have not been assigned.\n+        // When this happens, the region's table must be in ENABLING state.\n+        // It can't be in ENABLED state as that is set when all regions are\n+        // assigned.\n+        // It can't be in DISABLING state, because DISABLING state transitions\n+        // from ENABLED state when application calls disableTable.\n+        // It can't be in DISABLED state, because DISABLED states transitions\n+        // from DISABLING state.\n+        if (false == checkIfRegionsBelongsToEnabling(regionInfo)) {\n+          LOG.warn(\"Region \" + regionInfo.getEncodedName() +\n+            \" has null regionLocation.\" + \" But its table \" + tableName +\n+            \" isn't in ENABLING state.\");\n         }\n         addTheTablesInPartialState(disablingTables, enablingTables, regionInfo,\n             tableName);\n@@ -1901,7 +1909,7 @@ private boolean recoverTableInDisablingState(Set<String> disablingTables)\n             + \" is in DISABLING state.  Hence recovering by moving the table\"\n             + \" to DISABLED state.\");\n         new DisableTableHandler(this.master, tableName.getBytes(),\n-            catalogTracker, this).process();\n+            catalogTracker, this, true).process();\n       }\n     }\n     return isWatcherCreated;\n@@ -1931,7 +1939,7 @@ private void recoverTableInEnablingState(Set<String> enablingTables,\n             + \" is in ENABLING state.  Hence recovering by moving the table\"\n             + \" to ENABLED state.\");\n         new EnableTableHandler(this.master, tableName.getBytes(),\n-            catalogTracker, this).process();\n+            catalogTracker, this, true).process();\n       }\n     }\n   }",
                "deletions": 8
            },
            {
                "sha": "de93dc546aa7759ed32ba134e14dc3b55207bd38",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -1029,7 +1029,7 @@ public void enableTable(final byte [] tableName) throws IOException {\n       cpHost.preEnableTable(tableName);\n     }\n     this.executorService.submit(new EnableTableHandler(this, tableName,\n-      catalogTracker, assignmentManager));\n+      catalogTracker, assignmentManager, false));\n     if (cpHost != null) {\n       cpHost.postEnableTable(tableName);\n     }\n@@ -1040,7 +1040,7 @@ public void disableTable(final byte [] tableName) throws IOException {\n       cpHost.preDisableTable(tableName);\n     }\n     this.executorService.submit(new DisableTableHandler(this, tableName,\n-      catalogTracker, assignmentManager));\n+      catalogTracker, assignmentManager, false));\n     if (cpHost != null) {\n       cpHost.postDisableTable(tableName);\n     }",
                "deletions": 2
            },
            {
                "sha": "5af0690207e24c78813981d1f2b15f260d56917b",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java",
                "status": "modified",
                "changes": 41,
                "additions": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableNotEnabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.catalog.CatalogTracker;\n import org.apache.hadoop.hbase.catalog.MetaReader;\n@@ -46,8 +47,9 @@\n   private final AssignmentManager assignmentManager;\n \n   public DisableTableHandler(Server server, byte [] tableName,\n-      CatalogTracker catalogTracker, AssignmentManager assignmentManager)\n-  throws TableNotFoundException, IOException {\n+      CatalogTracker catalogTracker, AssignmentManager assignmentManager,\n+      boolean skipTableStateCheck)\n+  throws TableNotFoundException, TableNotEnabledException, IOException {\n     super(server, EventType.C_M_DISABLE_TABLE);\n     this.tableName = tableName;\n     this.tableNameStr = Bytes.toString(this.tableName);\n@@ -57,7 +59,25 @@ public DisableTableHandler(Server server, byte [] tableName,\n     //       part of old master rewrite, schema to zk to check for table\n     //       existence and such\n     if (!MetaReader.tableExists(catalogTracker, this.tableNameStr)) {\n-      throw new TableNotFoundException(Bytes.toString(tableName));\n+      throw new TableNotFoundException(this.tableNameStr);\n+    }\n+\n+    // There could be multiple client requests trying to disable or enable\n+    // the table at the same time. Ensure only the first request is honored\n+    // After that, no other requests can be accepted until the table reaches\n+    // DISABLED or ENABLED.\n+    if (!skipTableStateCheck)\n+    {\n+      try {\n+        if (!this.assignmentManager.getZKTable().checkEnabledAndSetDisablingTable\n+          (this.tableNameStr)) {\n+          LOG.info(\"Table \" + tableNameStr + \" isn't enabled; skipping disable\");\n+          throw new TableNotEnabledException(this.tableNameStr);\n+        }\n+      } catch (KeeperException e) {\n+        throw new IOException(\"Unable to ensure that the table will be\" +\n+          \" disabling because of a ZooKeeper issue\", e);\n+      }\n     }\n   }\n \n@@ -67,9 +87,10 @@ public String toString() {\n     if(server != null && server.getServerName() != null) {\n       name = server.getServerName().toString();\n     }\n-    return getClass().getSimpleName() + \"-\" + name + \"-\" + getSeqid() + \"-\" + tableNameStr;\n+    return getClass().getSimpleName() + \"-\" + name + \"-\" + getSeqid() + \"-\" +\n+      tableNameStr;\n   }\n-  \n+\n   @Override\n   public void process() {\n     try {\n@@ -83,18 +104,14 @@ public void process() {\n   }\n \n   private void handleDisableTable() throws IOException, KeeperException {\n-    if (this.assignmentManager.getZKTable().isDisabledTable(this.tableNameStr)) {\n-      LOG.info(\"Table \" + tableNameStr + \" already disabled; skipping disable\");\n-      return;\n-    }\n     // Set table disabling flag up in zk.\n     this.assignmentManager.getZKTable().setDisablingTable(this.tableNameStr);\n     boolean done = false;\n     while (true) {\n       // Get list of online regions that are of this table.  Regions that are\n       // already closed will not be included in this list; i.e. the returned\n-      // list is not ALL regions in a table, its all online regions according to\n-      // the in-memory state on this master.\n+      // list is not ALL regions in a table, its all online regions according\n+      // to the in-memory state on this master.\n       final List<HRegionInfo> regions =\n         this.assignmentManager.getRegionsOfTable(tableName);\n       if (regions.size() == 0) {\n@@ -159,4 +176,4 @@ protected boolean waitUntilDone(long timeout)\n       return regions != null && regions.isEmpty();\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 12
            },
            {
                "sha": "b6dc1c04eb9b5d18ac5f4566a04a4f27ffd68137",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java",
                "status": "modified",
                "changes": 36,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.TableNotDisabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.catalog.CatalogTracker;\n import org.apache.hadoop.hbase.catalog.MetaReader;\n@@ -47,8 +48,9 @@\n   private final CatalogTracker ct;\n \n   public EnableTableHandler(Server server, byte [] tableName,\n-      CatalogTracker catalogTracker, AssignmentManager assignmentManager)\n-  throws TableNotFoundException, IOException {\n+      CatalogTracker catalogTracker, AssignmentManager assignmentManager,\n+      boolean skipTableStateCheck)\n+  throws TableNotFoundException, TableNotDisabledException, IOException {\n     super(server, EventType.C_M_ENABLE_TABLE);\n     this.tableName = tableName;\n     this.tableNameStr = Bytes.toString(tableName);\n@@ -58,6 +60,24 @@ public EnableTableHandler(Server server, byte [] tableName,\n     if (!MetaReader.tableExists(catalogTracker, this.tableNameStr)) {\n       throw new TableNotFoundException(Bytes.toString(tableName));\n     }\n+\n+    // There could be multiple client requests trying to disable or enable\n+    // the table at the same time. Ensure only the first request is honored\n+    // After that, no other requests can be accepted until the table reaches\n+    // DISABLED or ENABLED.\n+    if (!skipTableStateCheck)\n+    {\n+      try {\n+        if (!this.assignmentManager.getZKTable().checkDisabledAndSetEnablingTable\n+          (this.tableNameStr)) {\n+          LOG.info(\"Table \" + tableNameStr + \" isn't disabled; skipping enable\");\n+          throw new TableNotDisabledException(this.tableNameStr);\n+        }\n+      } catch (KeeperException e) {\n+        throw new IOException(\"Unable to ensure that the table will be\" +\n+          \" enabling because of a ZooKeeper issue\", e);\n+      }\n+    }\n   }\n \n   @Override\n@@ -83,10 +103,6 @@ public void process() {\n   }\n \n   private void handleEnableTable() throws IOException, KeeperException {\n-    if (this.assignmentManager.getZKTable().isEnabledTable(this.tableNameStr)) {\n-      LOG.info(\"Table \" + tableNameStr + \" is already enabled; skipping enable\");\n-      return;\n-    }\n     // I could check table is disabling and if so, not enable but require\n     // that user first finish disabling but that might be obnoxious.\n \n@@ -121,7 +137,8 @@ private void handleEnableTable() throws IOException, KeeperException {\n       }\n     }\n     // Flip the table to enabled.\n-    if (done) this.assignmentManager.getZKTable().setEnabledTable(this.tableNameStr);\n+    if (done) this.assignmentManager.getZKTable().setEnabledTable(\n+      this.tableNameStr);\n     LOG.info(\"Enabled table is done=\" + done);\n   }\n \n@@ -131,7 +148,8 @@ private void handleEnableTable() throws IOException, KeeperException {\n    * been onlined; i.e. List of regions that need onlining.\n    * @throws IOException\n    */\n-  private List<HRegionInfo> regionsToAssign(final List<HRegionInfo> regionsInMeta)\n+  private List<HRegionInfo> regionsToAssign(\n+    final List<HRegionInfo> regionsInMeta)\n   throws IOException {\n     final List<HRegionInfo> onlineRegions =\n       this.assignmentManager.getRegionsOfTable(tableName);\n@@ -186,4 +204,4 @@ private boolean isDone(final List<HRegionInfo> regions) {\n       return regions != null && regions.size() >= this.countOfRegionsInTable;\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 9
            },
            {
                "sha": "4930f66987562ee99ae7a4c1f2377c13a3ff425b",
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java",
                "status": "modified",
                "changes": 38,
                "additions": 37,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -189,6 +189,42 @@ public boolean checkAndSetEnablingTable(final String tableName)\n     }\n   }\n \n+  /**\n+   * Sets the specified table as ENABLING in zookeeper atomically\n+   * If the table isn't in DISABLED state, no operation is performed\n+   * @param tableName\n+   * @return if the operation succeeds or not\n+   * @throws KeeperException unexpected zookeeper exception\n+   */\n+  public boolean checkDisabledAndSetEnablingTable(final String tableName)\n+    throws KeeperException {\n+    synchronized (this.cache) {\n+      if (!isDisabledTable(tableName)) {\n+        return false;\n+      }\n+      setTableState(tableName, TableState.ENABLING);\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Sets the specified table as DISABLING in zookeeper atomically\n+   * If the table isn't in ENABLED state, no operation is performed\n+   * @param tableName\n+   * @return if the operation succeeds or not\n+   * @throws KeeperException unexpected zookeeper exception\n+   */\n+  public boolean checkEnabledAndSetDisablingTable(final String tableName)\n+    throws KeeperException {\n+    synchronized (this.cache) {\n+      if (!isEnabledTable(tableName)) {\n+        return false;\n+      }\n+      setTableState(tableName, TableState.DISABLING);\n+      return true;\n+    }\n+  }\n+\n   private void setTableState(final String tableName, final TableState state)\n   throws KeeperException {\n     String znode = ZKUtil.joinZNode(this.watcher.tableZNode, tableName);\n@@ -366,4 +402,4 @@ public void setEnabledTable(final String tableName)\n     }\n     return disabledTables;\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 1
            },
            {
                "sha": "23b91e12d0fe63077c2246c696debaeb04bb24aa",
                "filename": "src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java",
                "status": "modified",
                "changes": 1,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/avro/TestAvroServer.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -171,7 +171,6 @@ public void testFamilyAdminAndMetadata() throws Exception {\n     impl.deleteFamily(tableAname, familyAname);\n     assertEquals(impl.describeTable(tableAname).families.size(), 0);\n \n-    impl.disableTable(tableAname);\n     impl.deleteTable(tableAname);\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "c6c0d2baabfb3d36d71fa9444f1a79a0eb7cd4fc",
                "filename": "src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java",
                "status": "modified",
                "changes": 32,
                "additions": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -1,5 +1,5 @@\n /**\n- * Copyright 2009 The Apache Software Foundation\n+ * Copyright 2011 The Apache Software Foundation\n  *\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n@@ -46,6 +46,7 @@\n import org.apache.hadoop.hbase.NotServingRegionException;\n import org.apache.hadoop.hbase.TableExistsException;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n+import org.apache.hadoop.hbase.TableNotEnabledException;\n import org.apache.hadoop.hbase.TableNotFoundException;\n import org.apache.hadoop.hbase.executor.EventHandler;\n import org.apache.hadoop.hbase.executor.EventHandler.EventType;\n@@ -916,12 +917,37 @@ public void testTableNames() throws IOException {\n    * @throws IOException\n    */\n   @Test (expected=TableExistsException.class)\n-  public void testTableNotFoundExceptionWithATable() throws IOException {\n-    final byte [] name = Bytes.toBytes(\"testTableNotFoundExceptionWithATable\");\n+  public void testTableExistsExceptionWithATable() throws IOException {\n+    final byte [] name = Bytes.toBytes(\"testTableExistsExceptionWithATable\");\n     TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n     TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n   }\n \n+  /**\n+   * Can't disable a table if the table isn't in enabled state\n+   * @throws IOException\n+   */\n+  @Test (expected=TableNotEnabledException.class)\n+  public void testTableNotEnabledExceptionWithATable() throws IOException {\n+    final byte [] name = Bytes.toBytes(\n+      \"testTableNotEnabledExceptionWithATable\");\n+    TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n+    this.admin.disableTable(name);\n+    this.admin.disableTable(name);\n+  }\n+\n+  /**\n+   * Can't enable a table if the table isn't in disabled state\n+   * @throws IOException\n+   */\n+  @Test (expected=TableNotDisabledException.class)\n+  public void testTableNotDisabledExceptionWithATable() throws IOException {\n+    final byte [] name = Bytes.toBytes(\n+      \"testTableNotDisabledExceptionWithATable\");\n+    TEST_UTIL.createTable(name, HConstants.CATALOG_FAMILY);\n+    this.admin.enableTable(name);\n+  }\n+\n   /**\n    * For HADOOP-2579\n    * @throws IOException",
                "deletions": 3
            },
            {
                "sha": "14d24bab53f36b2076dbee7ece579f9199bf57df",
                "filename": "src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa5f75ab0e799f839b8eacc8615f7142b9254110/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java",
                "status": "modified",
                "changes": 6,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java?ref=fa5f75ab0e799f839b8eacc8615f7142b9254110",
                "patch": "@@ -104,9 +104,6 @@ public void testTableCreateAndDeleteXML() throws IOException, JAXBException {\n     response = client.put(schemaPath, Constants.MIMETYPE_XML, toXML(model));\n     assertEquals(response.getCode(), 403);\n \n-    // make sure HBase concurs, and wait for the table to come online\n-    admin.enableTable(TABLE1);\n-\n     // retrieve the schema and validate it\n     response = client.get(schemaPath, Constants.MIMETYPE_XML);\n     assertEquals(response.getCode(), 200);\n@@ -145,9 +142,6 @@ public void testTableCreateAndDeletePB() throws IOException, JAXBException {\n       model.createProtobufOutput());\n     assertEquals(response.getCode(), 403);\n \n-    // make sure HBase concurs, and wait for the table to come online\n-    admin.enableTable(TABLE2);\n-\n     // retrieve the schema and validate it\n     response = client.get(schemaPath, Constants.MIMETYPE_PROTOBUF);\n     assertEquals(response.getCode(), 200);",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2077 NullPointerException with an open scanner that expired causing an immediate region server shutdown -- part 2\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1138180 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/791659b27efdbfc3b9d6785f026856e1ea8047aa",
        "parent": "https://github.com/apache/hbase/commit/34d2fa085f8119c6d48043a54d2d3c4a22654299",
        "bug_id": "hbase_155",
        "file": [
            {
                "sha": "d2acf053e50656ca32a3e0a81c3b4691e066dc83",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "patch": "@@ -343,6 +343,8 @@ Release 0.90.4 - Unreleased\n                (Li Pi)\n    HBASE-3988  Infinite loop for secondary master (Liyin Tang)\n    HBASE-3995  HBASE-3946 broke TestMasterFailover\n+   HBASE-2077  NullPointerException with an open scanner that expired causing\n+               an immediate region server shutdown -- part 2.\n \n \n   IMPROVEMENT",
                "deletions": 0
            },
            {
                "sha": "a6f375729968bf069b53f4d3defb818378eea5f7",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "patch": "@@ -1783,7 +1783,7 @@ private void processDeadServers(\n         Result result = region.getSecond();\n         // If region was in transition (was in zk) force it offline for reassign\n         try {\n-          // Process with existing RS shutdown code  \n+          // Process with existing RS shutdown code\n           boolean assign =\n             ServerShutdownHandler.processDeadRegion(regionInfo, result, this,\n             this.catalogTracker);",
                "deletions": 1
            },
            {
                "sha": "bbfdcbe8fdcdfd06c056d200cc660171cbb040c8",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 42,
                "additions": 24,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "patch": "@@ -1943,26 +1943,27 @@ public Result next(final long scannerId) throws IOException {\n   }\n \n   public Result[] next(final long scannerId, int nbRows) throws IOException {\n+    String scannerName = String.valueOf(scannerId);\n+    InternalScanner s = this.scanners.get(scannerName);\n+    if (s == null) throw new UnknownScannerException(\"Name: \" + scannerName);\n     try {\n-      String scannerName = String.valueOf(scannerId);\n-      InternalScanner s = this.scanners.get(scannerName);\n-      if (s == null) {\n-        throw new UnknownScannerException(\"Name: \" + scannerName);\n-      }\n+      checkOpen();\n+    } catch (IOException e) {\n+      // If checkOpen failed, server not running or filesystem gone,\n+      // cancel this lease; filesystem is gone or we're closing or something.\n       try {\n-        checkOpen();\n-      } catch (IOException e) {\n-        // If checkOpen failed, server not running or filesystem gone,\n-        // cancel this lease; filesystem is gone or we're closing or something.\n-        try {\n-          this.leases.cancelLease(scannerName);\n-        } catch (LeaseException le) {\n-          LOG.info(\"Server shutting down and client tried to access missing scanner \" +\n-            scannerName);\n-        }\n-        throw e;\n+        this.leases.cancelLease(scannerName);\n+      } catch (LeaseException le) {\n+        LOG.info(\"Server shutting down and client tried to access missing scanner \" +\n+          scannerName);\n       }\n-      this.leases.renewLease(scannerName);\n+      throw e;\n+    }\n+    Leases.Lease lease = null;\n+    try {\n+      // Remove lease while its being processed in server; protects against case\n+      // where processing of request takes > lease expiration time.\n+      lease = this.leases.removeLease(scannerName);\n       List<Result> results = new ArrayList<Result>(nbRows);\n       long currentScanResultSize = 0;\n       List<KeyValue> values = new ArrayList<KeyValue>();\n@@ -2024,10 +2025,15 @@ public Result next(final long scannerId) throws IOException {\n           : results.toArray(new Result[0]);\n     } catch (Throwable t) {\n       if (t instanceof NotServingRegionException) {\n-        String scannerName = String.valueOf(scannerId);\n         this.scanners.remove(scannerName);\n       }\n       throw convertThrowableToIOE(cleanup(t));\n+    } finally {\n+      // We're done. On way out readd the above removed lease.  Adding resets\n+      // expiration time on lease.\n+      if (this.scanners.containsKey(scannerName)) {\n+        if (lease != null) this.leases.addLease(lease);\n+      }\n     }\n   }\n ",
                "deletions": 18
            },
            {
                "sha": "8756efb222abc4a1619c99e846a251a4d779a196",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java",
                "blob_url": "https://github.com/apache/hbase/blob/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java",
                "raw_url": "https://github.com/apache/hbase/raw/791659b27efdbfc3b9d6785f026856e1ea8047aa/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java",
                "status": "modified",
                "changes": 57,
                "additions": 37,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java?ref=791659b27efdbfc3b9d6785f026856e1ea8047aa",
                "patch": "@@ -140,16 +140,24 @@ public void close() {\n    */\n   public void createLease(String leaseName, final LeaseListener listener)\n   throws LeaseStillHeldException {\n-    if (stopRequested) {\n+    addLease(new Lease(leaseName, listener));\n+  }\n+\n+  /**\n+   * Inserts lease.  Resets expiration before insertion.\n+   * @param lease\n+   * @throws LeaseStillHeldException\n+   */\n+  public void addLease(final Lease lease) throws LeaseStillHeldException {\n+    if (this.stopRequested) {\n       return;\n     }\n-    Lease lease = new Lease(leaseName, listener,\n-        System.currentTimeMillis() + leasePeriod);\n+    lease.setExpirationTime(System.currentTimeMillis() + this.leasePeriod);\n     synchronized (leaseQueue) {\n-      if (leases.containsKey(leaseName)) {\n-        throw new LeaseStillHeldException(leaseName);\n+      if (leases.containsKey(lease.getLeaseName())) {\n+        throw new LeaseStillHeldException(lease.getLeaseName());\n       }\n-      leases.put(leaseName, lease);\n+      leases.put(lease.getLeaseName(), lease);\n       leaseQueue.add(lease);\n     }\n   }\n@@ -189,7 +197,7 @@ public void renewLease(final String leaseName) throws LeaseException {\n       // in a corrupt leaseQueue.\n       if (lease == null || !leaseQueue.remove(lease)) {\n         throw new LeaseException(\"lease '\" + leaseName +\n-                \"' does not exist or has already expired\");\n+        \"' does not exist or has already expired\");\n       }\n       lease.setExpirationTime(System.currentTimeMillis() + leasePeriod);\n       leaseQueue.add(lease);\n@@ -198,26 +206,44 @@ public void renewLease(final String leaseName) throws LeaseException {\n \n   /**\n    * Client explicitly cancels a lease.\n-   *\n    * @param leaseName name of lease\n    * @throws LeaseException\n    */\n   public void cancelLease(final String leaseName) throws LeaseException {\n+    removeLease(leaseName);\n+  }\n+\n+  /**\n+   * Remove named lease.\n+   * Lease is removed from the list of leases and removed from the delay queue.\n+   * Lease can be resinserted using {@link #addLease(Lease)}\n+   *\n+   * @param leaseName name of lease\n+   * @throws LeaseException\n+   * @return Removed lease\n+   */\n+  Lease removeLease(final String leaseName) throws LeaseException {\n+    Lease lease =  null;\n     synchronized (leaseQueue) {\n-      Lease lease = leases.remove(leaseName);\n+      lease = leases.remove(leaseName);\n       if (lease == null) {\n         throw new LeaseException(\"lease '\" + leaseName + \"' does not exist\");\n       }\n       leaseQueue.remove(lease);\n     }\n+    return lease;\n   }\n \n   /** This class tracks a single Lease. */\n-  private static class Lease implements Delayed {\n+  static class Lease implements Delayed {\n     private final String leaseName;\n     private final LeaseListener listener;\n     private long expirationTime;\n \n+    Lease(final String leaseName, LeaseListener listener) {\n+      this(leaseName, listener, 0);\n+    }\n+\n     Lease(final String leaseName, LeaseListener listener, long expirationTime) {\n       this.leaseName = leaseName;\n       this.listener = listener;\n@@ -269,14 +295,5 @@ public int compareTo(Delayed o) {\n     public void setExpirationTime(long expirationTime) {\n       this.expirationTime = expirationTime;\n     }\n-\n-    /**\n-     * Get the expiration time for that lease\n-     * @return expiration time\n-     */\n-    public long getExpirationTime() {\n-      return this.expirationTime;\n-    }\n-\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 20
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3987 Fix a NullPointerException on a failure to load Bloom filter data\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1136689 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6f09e358e04f0f228623efcc81f09526b634c430",
        "parent": "https://github.com/apache/hbase/commit/ebb63ef44d42bde034e8a2603ebace7d930e210b",
        "bug_id": "hbase_156",
        "file": [
            {
                "sha": "385d33407af94b2597bfee9964ee955d703ffaa9",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/6f09e358e04f0f228623efcc81f09526b634c430/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/6f09e358e04f0f228623efcc81f09526b634c430/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=6f09e358e04f0f228623efcc81f09526b634c430",
                "patch": "@@ -335,6 +335,9 @@ Release 0.90.4 - Unreleased\n                instead of localhost. (Li Pi)\n    HBASE-3985  Same Region could be picked out twice in LoadBalance\n                (Jieshan Bean)\n+   HBASE-3987  Fix a NullPointerException on a failure to load Bloom filter data\n+               (Mikhail Bautin)\n+\n \n   IMPROVEMENT\n    HBASE-3882  hbase-config.sh needs to be updated so it can auto-detects the",
                "deletions": 0
            },
            {
                "sha": "b60002076e2c832d5d0a0f4699e9981096210b99",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/6f09e358e04f0f228623efcc81f09526b634c430/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/6f09e358e04f0f228623efcc81f09526b634c430/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "status": "modified",
                "changes": 9,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java?ref=6f09e358e04f0f228623efcc81f09526b634c430",
                "patch": "@@ -983,7 +983,8 @@ private boolean passesTimerangeFilter(Scan scan) {\n     }\n \n     private boolean passesBloomFilter(Scan scan, final SortedSet<byte[]> columns) {\n-      if (this.bloomFilter == null || !scan.isGetScan()) {\n+      BloomFilter bm = this.bloomFilter;\n+      if (bm == null || !scan.isGetScan()) {\n         return true;\n       }\n       byte[] row = scan.getStartRow();\n@@ -1011,11 +1012,11 @@ private boolean passesBloomFilter(Scan scan, final SortedSet<byte[]> columns) {\n             // columns, a file might be skipped if using row+col Bloom filter.\n             // In order to ensure this file is included an additional check is\n             // required looking only for a row bloom.\n-            return this.bloomFilter.contains(key, bloom) ||\n-                this.bloomFilter.contains(row, bloom);\n+            return bm.contains(key, bloom) ||\n+                bm.contains(row, bloom);\n           }\n           else {\n-            return this.bloomFilter.contains(key, bloom);\n+            return bm.contains(key, bloom);\n           }\n         }\n       } catch (IOException e) {",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1888 KeyValue methods throw NullPointerException instead of IllegalArgumentException during parameter sanity check\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1043218 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/afac14fcf052219063b338b18db936bd69f22233",
        "parent": "https://github.com/apache/hbase/commit/584e9b5c26e78668d3ccf02d7dc65e5fd97871dd",
        "bug_id": "hbase_157",
        "file": [
            {
                "sha": "4ab8cf58ef0d7932f1b9b18264e061fdf687eae7",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/afac14fcf052219063b338b18db936bd69f22233/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/afac14fcf052219063b338b18db936bd69f22233/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=afac14fcf052219063b338b18db936bd69f22233",
                "patch": "@@ -17,6 +17,8 @@ Release 0.91.0 - Unreleased\n                the HBase shell (Igor Ranitovic via Stack)\n    HBASE-3317  Javadoc and Throws Declaration for Bytes.incrementBytes() is\n                Wrong (Ed Kohlwey via Stack)\n+   HBASE-1888  KeyValue methods throw NullPointerException instead of\n+               IllegalArgumentException during parameter sanity check\n \n   IMPROVEMENTS\n    HBASE-2001  Coprocessors: Colocate user code with regions (Mingjie Lai via",
                "deletions": 0
            },
            {
                "sha": "91221effb73f6120ecd05a03bbb2cdde21b550eb",
                "filename": "src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "blob_url": "https://github.com/apache/hbase/blob/afac14fcf052219063b338b18db936bd69f22233/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "raw_url": "https://github.com/apache/hbase/raw/afac14fcf052219063b338b18db936bd69f22233/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "status": "modified",
                "changes": 40,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=afac14fcf052219063b338b18db936bd69f22233",
                "patch": "@@ -36,35 +36,35 @@\n import org.apache.hadoop.io.Writable;\n \n /**\n- * An HBase Key/Value.\n+ * An HBase Key/Value.  This is the fundamental HBase Type.\n  *\n  * <p>If being used client-side, the primary methods to access individual fields\n  * are {@link #getRow()}, {@link #getFamily()}, {@link #getQualifier()},\n  * {@link #getTimestamp()}, and {@link #getValue()}.  These methods allocate new\n- * byte arrays and return copies so they should be avoided server-side.\n+ * byte arrays and return copies. Avoid their use server-side.\n  *\n- * <p>Instances of this class are immutable.  They are not\n- * comparable but Comparators are provided.  Comparators change with context,\n- * whether user table or a catalog table comparison context.  Its\n- * important that you use the appropriate comparator comparing rows in\n- * particular.  There are Comparators for KeyValue instances and then for\n- * just the Key portion of a KeyValue used mostly in {@link HFile}.\n+ * <p>Instances of this class are immutable.  They do not implement Comparable\n+ * but Comparators are provided.  Comparators change with context,\n+ * whether user table or a catalog table comparison.  Its critical you use the\n+ * appropriate comparator.  There are Comparators for KeyValue instances and\n+ * then for just the Key portion of a KeyValue used mostly by {@link HFile}.\n  *\n- * <p>KeyValue wraps a byte array and has offset and length for passed array\n- * at where to start interpreting the content as a KeyValue blob.  The KeyValue\n- * blob format inside the byte array is:\n+ * <p>KeyValue wraps a byte array and takes offsets and lengths into passed\n+ * array at where to start interpreting the content as KeyValue.  The KeyValue\n+ * format inside a byte array is:\n  * <code>&lt;keylength> &lt;valuelength> &lt;key> &lt;value></code>\n- * Key is decomposed as:\n+ * Key is further decomposed as:\n  * <code>&lt;rowlength> &lt;row> &lt;columnfamilylength> &lt;columnfamily> &lt;columnqualifier> &lt;timestamp> &lt;keytype></code>\n- * Rowlength maximum is Short.MAX_SIZE, column family length maximum is\n- * Byte.MAX_SIZE, and column qualifier + key length must be < Integer.MAX_SIZE.\n- * The column does not contain the family/qualifier delimiter.\n- *\n- * <p>TODO: Group Key-only comparators and operations into a Key class, just\n- * for neatness sake, if can figure what to call it.\n+ * The <code>rowlength</code> maximum is <code>Short.MAX_SIZE</code>,\n+ * column family length maximum is\n+ * <code>Byte.MAX_SIZE</code>, and column qualifier + key length must\n+ * be < <code>Integer.MAX_SIZE</code>.\n+ * The column does not contain the family/qualifier delimiter, {@link #COLUMN_FAMILY_DELIMITER}\n  */\n public class KeyValue implements Writable, HeapSize {\n   static final Log LOG = LogFactory.getLog(KeyValue.class);\n+  // TODO: Group Key-only comparators and operations into a Key class, just\n+  // for neatness sake, if can figure what to call it.\n \n   /**\n    * Colon character in UTF-8\n@@ -1293,7 +1293,7 @@ static int getRequiredDelimiterInReverse(final byte [] b,\n   public static int getDelimiter(final byte [] b, int offset, final int length,\n       final int delimiter) {\n     if (b == null) {\n-      throw new NullPointerException();\n+      throw new IllegalArgumentException(\"Passed buffer is null\");\n     }\n     int result = -1;\n     for (int i = offset; i < length + offset; i++) {\n@@ -1314,7 +1314,7 @@ public static int getDelimiter(final byte [] b, int offset, final int length,\n   public static int getDelimiterInReverse(final byte [] b, final int offset,\n       final int length, final int delimiter) {\n     if (b == null) {\n-      throw new NullPointerException();\n+      throw new IllegalArgumentException(\"Passed buffer is null\");\n     }\n     int result = -1;\n     for (int i = (offset + length) - 1; i >= offset; i--) {",
                "deletions": 20
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2077  NullPointerException with an open scanner that expired causing\n            an immediate region server shutdown (JD's fix)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@916070 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6c100b6de3489bfb321df472394348d1fabe9c7b",
        "parent": "https://github.com/apache/hbase/commit/961e65362eee6fc49248df4a2ef5ed75a45f57dd",
        "bug_id": "hbase_158",
        "file": [
            {
                "sha": "d1c643c795edb8405c18ec93eb80537aa36b71a2",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/Leases.java",
                "blob_url": "https://github.com/apache/hbase/blob/6c100b6de3489bfb321df472394348d1fabe9c7b/core/src/main/java/org/apache/hadoop/hbase/Leases.java",
                "raw_url": "https://github.com/apache/hbase/raw/6c100b6de3489bfb321df472394348d1fabe9c7b/core/src/main/java/org/apache/hadoop/hbase/Leases.java",
                "status": "modified",
                "changes": 28,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/Leases.java?ref=6c100b6de3489bfb321df472394348d1fabe9c7b",
                "patch": "@@ -90,14 +90,19 @@ public void run() {\n       if (lease == null) {\n         continue;\n       }\n-      // A lease expired.  Run the expired code before removing from queue\n-      // since its presence in queue is used to see if lease exists still.\n-      if (lease.getListener() == null) {\n-        LOG.error(\"lease listener is null for lease \" + lease.getLeaseName());\n-      } else {\n-        lease.getListener().leaseExpired();\n-      }\n       synchronized (leaseQueue) {\n+        if (lease.getExpirationTime() <= System.currentTimeMillis()) {\n+          leaseQueue.add(lease);\n+          continue;\n+        }\n+        // A lease expired.  Run the expired code before removing from queue\n+        // since its presence in queue is used to see if lease exists still.\n+        if (lease.getListener() == null) {\n+          LOG.error(\"lease listener is null for lease \" + lease.getLeaseName());\n+        } else {\n+          lease.getListener().leaseExpired();\n+        }\n+\n         leases.remove(lease.getLeaseName());\n       }\n     }\n@@ -268,5 +273,14 @@ public int compareTo(Delayed o) {\n     public void setExpirationTime(long expirationTime) {\n       this.expirationTime = expirationTime;\n     }\n+\n+    /**\n+     * Get the expiration time for that lease\n+     * @return expiration time\n+     */\n+    public long getExpirationTime() {\n+      return this.expirationTime;\n+    }\n+\n   }\n }",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2077  NullPointerException with an open scanner that expired causing \n            an immediate region server shutdown (Sam Pullara via JD)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@894553 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/58039a7bc2882c334c4e673354e5cb1bce255f99",
        "parent": "https://github.com/apache/hbase/commit/68021818558d028f23b2a81503ce6d181a1fc3b0",
        "bug_id": "hbase_159",
        "file": [
            {
                "sha": "bfa3e4e345d532c7866a58b87288b695381e646a",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/58039a7bc2882c334c4e673354e5cb1bce255f99/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/58039a7bc2882c334c4e673354e5cb1bce255f99/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=58039a7bc2882c334c4e673354e5cb1bce255f99",
                "patch": "@@ -142,6 +142,8 @@ Release 0.21.0 - Unreleased\n    HBASE-2026  NPE in StoreScanner on compaction\n    HBASE-2072  fs.automatic.close isn't passed to FileSystem\n    HBASE-2075  Master requires HDFS superuser privileges due to waitOnSafeMode\n+   HBASE-2077  NullPointerException with an open scanner that expired causing \n+               an immediate region server shutdown (Sam Pullara via JD)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "64f9bab8c6460cfc4dec8eb5d9c5da276deeea61",
                "filename": "src/java/org/apache/hadoop/hbase/Leases.java",
                "blob_url": "https://github.com/apache/hbase/blob/58039a7bc2882c334c4e673354e5cb1bce255f99/src/java/org/apache/hadoop/hbase/Leases.java",
                "raw_url": "https://github.com/apache/hbase/raw/58039a7bc2882c334c4e673354e5cb1bce255f99/src/java/org/apache/hadoop/hbase/Leases.java",
                "status": "modified",
                "changes": 8,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/Leases.java?ref=58039a7bc2882c334c4e673354e5cb1bce255f99",
                "patch": "@@ -183,11 +183,13 @@ public String getName() {\n   public void renewLease(final String leaseName) throws LeaseException {\n     synchronized (leaseQueue) {\n       Lease lease = leases.get(leaseName);\n-      if (lease == null) {\n+      // We need to check to see if the remove is successful as the poll in the run()\n+      // method could have completed between the get and the remove which will result\n+      // in a corrupt leaseQueue.\n+      if (lease == null || !leaseQueue.remove(lease)) {\n         throw new LeaseException(\"lease '\" + leaseName +\n-            \"' does not exist\");\n+                \"' does not exist or has already expired\");\n       }\n-      leaseQueue.remove(lease);\n       lease.setExpirationTime(System.currentTimeMillis() + leasePeriod);\n       leaseQueue.add(lease);\n     }",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-12811 [AccessController] NPE while scanning a table with user not having READ permission on the namespace (Ashish)",
        "commit": "https://github.com/apache/hbase/commit/4ea490b63aca6c4c6ce949a0f7aae543ebbee4fb",
        "parent": "https://github.com/apache/hbase/commit/ee32eebeab38be4e171c6aaf362aff9a584a37f3",
        "bug_id": "hbase_160",
        "file": [
            {
                "sha": "6ca40e670b3d2eb5e65e38e8e1bee4c87cd25047",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/4ea490b63aca6c4c6ce949a0f7aae543ebbee4fb/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/4ea490b63aca6c4c6ce949a0f7aae543ebbee4fb/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java?ref=4ea490b63aca6c4c6ce949a0f7aae543ebbee4fb",
                "patch": "@@ -508,8 +508,8 @@ public boolean authorizeGroup(String groupName, TableName table, byte[] family,\n     }\n     if (table == null) table = AccessControlLists.ACL_TABLE_NAME;\n     // Namespace authorization supercedes table level\n-    if (authorize(getNamespacePermissions(table.getNamespaceAsString()).getGroup(groupName),\n-        table, family, action)) {\n+    String namespace = table.getNamespaceAsString();\n+    if (authorize(getNamespacePermissions(namespace).getGroup(groupName), namespace, action)) {\n       return true;\n     }\n     // Check table level",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"HBASE-14902 Revert some of the stringency recently introduced by checkstyle tightening\"\nThis seems to cause a NPE when generating report.... reverting.\n\nThis reverts commit 2c0394f078158ef668e75b74f589a7da59ff9e0e.",
        "commit": "https://github.com/apache/hbase/commit/5e08e2ceb6af231eebeff9f46e18d654a7b89338",
        "parent": "https://github.com/apache/hbase/commit/93e200d52b29d35ad5a98eed9eea05783960f6b2",
        "bug_id": "hbase_161",
        "file": [
            {
                "sha": "6095d997a96b8ae8ad5c2ede21703e977dd7ef5f",
                "filename": "hbase-checkstyle/src/main/resources/hbase/checkstyle.xml",
                "blob_url": "https://github.com/apache/hbase/blob/5e08e2ceb6af231eebeff9f46e18d654a7b89338/hbase-checkstyle/src/main/resources/hbase/checkstyle.xml",
                "raw_url": "https://github.com/apache/hbase/raw/5e08e2ceb6af231eebeff9f46e18d654a7b89338/hbase-checkstyle/src/main/resources/hbase/checkstyle.xml",
                "status": "modified",
                "changes": 4,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-checkstyle/src/main/resources/hbase/checkstyle.xml?ref=5e08e2ceb6af231eebeff9f46e18d654a7b89338",
                "patch": "@@ -41,9 +41,7 @@\n     http://checkstyle.sourceforge.net/config_blocks.html -->\n     <module name=\"EmptyBlock\"/>\n     <module name=\"LeftCurly\"/>\n-    <module name=\"NeedBraces\">\n-      <property name=\"allowSingleLineStatement\" value=\"true\"/>\n-    </module>\n+    <module name=\"NeedBraces\"/>\n \n     <!-- Class Design Checks\n     http://checkstyle.sourceforge.net/config_design.html -->",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Amend HBASE-14771 RpcServer#getRemoteAddress always returns null\n\nThis change has been reported to cause Phoenix's PhoenixIndexRpcSchedulerTest to fail\nwith a NPE",
        "commit": "https://github.com/apache/hbase/commit/79588240c60bf422fb9d9a74e0edd8b47a66b9f9",
        "parent": "https://github.com/apache/hbase/commit/2d2fdd5a9fd122a76fc71db549b4e10985395dd2",
        "bug_id": "hbase_162",
        "file": [
            {
                "sha": "bd3342ba15b426272510ef7bec71add55209d8f0",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/79588240c60bf422fb9d9a74e0edd8b47a66b9f9/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/79588240c60bf422fb9d9a74e0edd8b47a66b9f9/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=79588240c60bf422fb9d9a74e0edd8b47a66b9f9",
                "patch": "@@ -2603,7 +2603,7 @@ static MonitoredRPCHandler getStatus() {\n    */\n   public static InetAddress getRemoteIp() {\n     Call call = CurCall.get();\n-    if (call != null && call.connection.socket != null) {\n+    if (call != null && call.connection != null && call.connection.socket != null) {\n       return call.connection.socket.getInetAddress();\n     }\n     return null;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"Revert \"HBASE-14909 NPE testing for RIT\"\"\nReverted the wrong patch... putting it back (revert of a revert)\n\nThis reverts commit 157a60f1b396ab9adc7f934a15352f2dbc5493a9.",
        "commit": "https://github.com/apache/hbase/commit/a4a44587b3f2236750569f5c032b283dc77942f6",
        "parent": "https://github.com/apache/hbase/commit/157a60f1b396ab9adc7f934a15352f2dbc5493a9",
        "bug_id": "hbase_163",
        "file": [
            {
                "sha": "006c3e71362f90732b2febbe947c9a28ec1858bd",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/a4a44587b3f2236750569f5c032b283dc77942f6/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/a4a44587b3f2236750569f5c032b283dc77942f6/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "status": "modified",
                "changes": 20,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=a4a44587b3f2236750569f5c032b283dc77942f6",
                "patch": "@@ -17,7 +17,10 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import javax.annotation.Nullable;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n@@ -44,6 +47,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import javax.annotation.Nullable;\n+\n import org.apache.commons.lang.RandomStringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -58,7 +63,6 @@\n import org.apache.hadoop.hbase.classification.InterfaceStability;\n import org.apache.hadoop.hbase.client.Admin;\n import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.ClusterConnection;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.ConnectionFactory;\n import org.apache.hadoop.hbase.client.Consistency;\n@@ -84,6 +88,7 @@\n import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;\n+import org.apache.hadoop.hbase.master.AssignmentManager;\n import org.apache.hadoop.hbase.master.HMaster;\n import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.master.ServerManager;\n@@ -129,10 +134,6 @@\n import org.apache.zookeeper.ZooKeeper;\n import org.apache.zookeeper.ZooKeeper.States;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n /**\n  * Facility for testing HBase. Replacement for\n  * old HBaseTestCase and HBaseClusterTestCase functionality.\n@@ -3767,8 +3768,11 @@ public String explainFailure() throws IOException {\n \n       @Override\n       public boolean evaluate() throws IOException {\n-        final RegionStates regionStates = getMiniHBaseCluster().getMaster()\n-            .getAssignmentManager().getRegionStates();\n+        HMaster master = getMiniHBaseCluster().getMaster();\n+        if (master == null) return false;\n+        AssignmentManager am = master.getAssignmentManager();\n+        if (am == null) return false;\n+        final RegionStates regionStates = am.getRegionStates();\n         return !regionStates.isRegionsInTransition();\n       }\n     };",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-13892 NPE in ClientScanner on null results array\n\nSigned-off-by: Andrew Purtell <apurtell@apache.org>\nAmending-Author: Andrew Purtell <apurtell@apache.org>",
        "commit": "https://github.com/apache/hbase/commit/8cef99e5062d889a748c8442595a0e0644e11458",
        "parent": "https://github.com/apache/hbase/commit/9d3422ed16004da1b0f9a874a98bd140b46b7a6f",
        "bug_id": "hbase_164",
        "file": [
            {
                "sha": "c013a4d89b93efbafb35e5abc17c5402565c6097",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/8cef99e5062d889a748c8442595a0e0644e11458/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/8cef99e5062d889a748c8442595a0e0644e11458/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java?ref=8cef99e5062d889a748c8442595a0e0644e11458",
                "patch": "@@ -586,7 +586,8 @@ public int getCacheCount() {\n     // the caller will receive a result back where the number of cells in the result is less than\n     // the batch size even though it may not be the last group of cells for that row.\n     if (allowPartials || isBatchSet) {\n-      addResultsToList(resultsToAddToCache, resultsFromServer, 0, resultsFromServer.length);\n+      addResultsToList(resultsToAddToCache, resultsFromServer, 0,\n+          (null == resultsFromServer ? 0 : resultsFromServer.length));\n       return resultsToAddToCache;\n     }\n ",
                "deletions": 1
            },
            {
                "sha": "027a3482beb573223936b9a8a7ab32e017e3eece",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java",
                "blob_url": "https://github.com/apache/hbase/blob/8cef99e5062d889a748c8442595a0e0644e11458/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java",
                "raw_url": "https://github.com/apache/hbase/raw/8cef99e5062d889a748c8442595a0e0644e11458/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java",
                "status": "modified",
                "changes": 16,
                "additions": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java?ref=8cef99e5062d889a748c8442595a0e0644e11458",
                "patch": "@@ -73,6 +73,8 @@\n import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;\n import org.apache.hadoop.hbase.filter.Filter;\n import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.InclusiveStopFilter;\n import org.apache.hadoop.hbase.filter.KeyOnlyFilter;\n import org.apache.hadoop.hbase.filter.LongComparator;\n import org.apache.hadoop.hbase.filter.PrefixFilter;\n@@ -6320,4 +6322,18 @@ public void testGetStartEndKeysWithRegionReplicas() throws IOException {\n       }\n     }\n   }\n+\n+  @Test\n+  public void testFilterAllRecords() throws IOException {\n+    Scan scan = new Scan();\n+    scan.setBatch(1);\n+    scan.setCaching(1);\n+    // Filter out any records\n+    scan.setFilter(new FilterList(new FirstKeyOnlyFilter(), new InclusiveStopFilter(new byte[0])));\n+    try (Table table = TEST_UTIL.getConnection().getTable(TableName.NAMESPACE_TABLE_NAME)) {\n+      try (ResultScanner s = table.getScanner(scan)) {\n+        assertNull(s.next());\n+      }\n+    }\n+  }\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7530  [replication] Work around HDFS-4380 else we get NPEs\nHBASE-7531  [replication] NPE in SequenceFileLogReader because\n            ReplicationSource doesn't nullify the reader\nHBASE-7534  [replication] TestReplication.queueFailover can fail\n            because HBaseTestingUtility.createMultiRegions is dangerous\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1431768 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
        "parent": "https://github.com/apache/hbase/commit/2eb9249ac233008ff4e280a1f5a20bbb43a97ab2",
        "bug_id": "hbase_165",
        "file": [
            {
                "sha": "f18c265c28d19a9010cc45471362b3529eb07f1e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "blob_url": "https://github.com/apache/hbase/blob/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "raw_url": "https://github.com/apache/hbase/raw/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
                "patch": "@@ -553,6 +553,7 @@ protected boolean openReader(int sleepMultiplier) {\n       }\n     } catch (IOException ioe) {\n       LOG.warn(peerClusterZnode + \" Got: \", ioe);\n+      this.reader = null;\n       // TODO Need a better way to determinate if a file is really gone but\n       // TODO without scanning all logs dir\n       if (sleepMultiplier == this.maxRetriesMultiplier) {",
                "deletions": 0
            },
            {
                "sha": "c35012396d0cb0d501e126c8447914781c28907b",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "blob_url": "https://github.com/apache/hbase/blob/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "raw_url": "https://github.com/apache/hbase/raw/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java",
                "status": "modified",
                "changes": 12,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java?ref=72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
                "patch": "@@ -1260,6 +1260,18 @@ public int createMultiRegions(HTable table, byte[] columnFamily)\n     Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\")\n   };\n \n+  public static final byte[][] KEYS_FOR_HBA_CREATE_TABLE = {\n+      Bytes.toBytes(\"bbb\"),\n+      Bytes.toBytes(\"ccc\"), Bytes.toBytes(\"ddd\"), Bytes.toBytes(\"eee\"),\n+      Bytes.toBytes(\"fff\"), Bytes.toBytes(\"ggg\"), Bytes.toBytes(\"hhh\"),\n+      Bytes.toBytes(\"iii\"), Bytes.toBytes(\"jjj\"), Bytes.toBytes(\"kkk\"),\n+      Bytes.toBytes(\"lll\"), Bytes.toBytes(\"mmm\"), Bytes.toBytes(\"nnn\"),\n+      Bytes.toBytes(\"ooo\"), Bytes.toBytes(\"ppp\"), Bytes.toBytes(\"qqq\"),\n+      Bytes.toBytes(\"rrr\"), Bytes.toBytes(\"sss\"), Bytes.toBytes(\"ttt\"),\n+      Bytes.toBytes(\"uuu\"), Bytes.toBytes(\"vvv\"), Bytes.toBytes(\"www\"),\n+      Bytes.toBytes(\"xxx\"), Bytes.toBytes(\"yyy\"), Bytes.toBytes(\"zzz\")\n+  };\n+\n   /**\n    * Creates many regions names \"aaa\" to \"zzz\".\n    * @param c Configuration to use.",
                "deletions": 0
            },
            {
                "sha": "07ebc61c5c5225915ed3683e58c98d5aedb90238",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java",
                "blob_url": "https://github.com/apache/hbase/blob/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java",
                "raw_url": "https://github.com/apache/hbase/raw/72bf55b22449896bbb2fc8b7b983e372d49a3dd9/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java",
                "status": "modified",
                "changes": 9,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java?ref=72bf55b22449896bbb2fc8b7b983e372d49a3dd9",
                "patch": "@@ -91,9 +91,8 @@\n   @BeforeClass\n   public static void setUpBeforeClass() throws Exception {\n     conf1.set(HConstants.ZOOKEEPER_ZNODE_PARENT, \"/1\");\n-    // smaller block size and capacity to trigger more operations\n-    // and test them\n-    conf1.setInt(\"hbase.regionserver.hlog.blocksize\", 1024*20);\n+    // smaller log roll size to trigger more events\n+    conf1.setFloat(\"hbase.regionserver.logroll.multiplier\", 0.0003f);\n     conf1.setInt(\"replication.source.size.capacity\", 1024);\n     conf1.setLong(\"replication.source.sleepforretries\", 100);\n     conf1.setInt(\"hbase.regionserver.maxlogs\", 10);\n@@ -142,7 +141,7 @@ public static void setUpBeforeClass() throws Exception {\n     table.addFamily(fam);\n     HBaseAdmin admin1 = new HBaseAdmin(conf1);\n     HBaseAdmin admin2 = new HBaseAdmin(conf2);\n-    admin1.createTable(table);\n+    admin1.createTable(table, HBaseTestingUtility.KEYS_FOR_HBA_CREATE_TABLE);\n     admin2.createTable(table);\n     htable1 = new HTable(conf1, tableName);\n     htable1.setWriteBufferSize(1024);\n@@ -716,8 +715,6 @@ public void testVerifyRepJob() throws Exception {\n    */\n   @Test(timeout=300000)\n   public void queueFailover() throws Exception {\n-    utility1.createMultiRegions(htable1, famName);\n-\n     // killing the RS with .META. can result into failed puts until we solve\n     // IO fencing\n     int rsToKill1 =",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10517 NPE in MetaCache.clearCache()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1567827 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/998cd1642f4ae3819f18061468755bf992257a52",
        "parent": "https://github.com/apache/hbase/commit/481a116e26faea7472328420469b6aa72ce378f1",
        "bug_id": "hbase_166",
        "file": [
            {
                "sha": "10a48ae28ac57acf57271d308a75467b865f302c",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "blob_url": "https://github.com/apache/hbase/blob/998cd1642f4ae3819f18061468755bf992257a52/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "raw_url": "https://github.com/apache/hbase/raw/998cd1642f4ae3819f18061468755bf992257a52/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaCache.java?ref=998cd1642f4ae3819f18061468755bf992257a52",
                "patch": "@@ -329,6 +329,9 @@ public void clearCache(final HRegionLocation location) {\n     TableName tableName = location.getRegionInfo().getTable();\n     ConcurrentMap<byte[], RegionLocations> tableLocations = getTableLocations(tableName);\n     RegionLocations rll = tableLocations.get(location.getRegionInfo().getStartKey());\n+    if (rll == null) {\n+      return;\n+    }\n     RegionLocations updatedLocations = rll.remove(location);\n     if (updatedLocations.isEmpty()) {\n       tableLocations.remove(location.getRegionInfo().getStartKey(), rll);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-20169 NPE when calling HBTU.shutdownMiniCluster\n\nAdds a prepare step to RecoverMetaProcedure in which we test for\ncluster up and master being up. If not up, we fail the run.\n\nModified hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java\nModified hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java\n Minor log cleanup.\n\nModified hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java\n Add pepare step.\n\nModified hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java\n Debug for the failing test....\n\nAdded hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java\n Test the prepare step goes down if master or cluster are down.",
        "commit": "https://github.com/apache/hbase/commit/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
        "parent": "https://github.com/apache/hbase/commit/74c28bdf4448c25612df71bb3bebf0420eadd719",
        "bug_id": "hbase_167",
        "file": [
            {
                "sha": "0e3a4cd65479476bb331adcd75bad812a0ffd66b",
                "filename": "hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -74,7 +74,8 @@ protected final int getCycles() {\n    */\n   private int previousState;\n \n-  protected enum Flow {\n+  @VisibleForTesting\n+  public enum Flow {\n     HAS_MORE_STATE,\n     NO_MORE_STATE,\n   }",
                "deletions": 1
            },
            {
                "sha": "fa6fa757a80b2ceec55439ec8d61f42c529f4778",
                "filename": "hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -307,6 +307,7 @@ enum ServerCrashState {\n }\n \n enum RecoverMetaState {\n+  RECOVER_META_PREPARE = 0;\n   RECOVER_META_SPLIT_LOGS = 1;\n   RECOVER_META_ASSIGN_REGIONS = 2;\n }",
                "deletions": 0
            },
            {
                "sha": "b85b56963b69e8ee0e69db0ec3e7fd82accff12a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -235,12 +235,12 @@ protected void consumerLoop(BlockingQueue<HFileDeleteTask> queue) {\n           break;\n         }\n         if (task != null) {\n-          LOG.debug(\"Removing: {} from archive\", task.filePath);\n+          LOG.debug(\"Removing {}\", task.filePath);\n           boolean succeed;\n           try {\n             succeed = this.fs.delete(task.filePath, false);\n           } catch (IOException e) {\n-            LOG.warn(\"Failed to delete file {}\", task.filePath, e);\n+            LOG.warn(\"Failed to delete {}\", task.filePath, e);\n             succeed = false;\n           }\n           task.setResult(succeed);\n@@ -250,7 +250,7 @@ protected void consumerLoop(BlockingQueue<HFileDeleteTask> queue) {\n         }\n       }\n     } finally {\n-      LOG.debug(\"Exit thread: {}\", Thread.currentThread());\n+      LOG.debug(\"Exit {}\", Thread.currentThread());\n     }\n   }\n ",
                "deletions": 3
            },
            {
                "sha": "2234a1bb8983d916712d6df64b2a2db29869c755",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java",
                "status": "modified",
                "changes": 17,
                "additions": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -104,6 +104,21 @@ protected Flow executeFromState(MasterProcedureEnv env,\n \n     try {\n       switch (state) {\n+        case RECOVER_META_PREPARE:\n+          // If Master is going down or cluster is up, skip this assign by returning NO_MORE_STATE\n+          if (!master.isClusterUp()) {\n+            String msg = \"Cluster not up! Skipping hbase:meta assign.\";\n+            LOG.warn(msg);\n+            return Flow.NO_MORE_STATE;\n+          }\n+          if (master.isStopping() || master.isStopped()) {\n+            String msg = \"Master stopping=\" + master.isStopping() + \", stopped=\" +\n+                master.isStopped() + \"; skipping hbase:meta assign.\";\n+            LOG.warn(msg);\n+            return Flow.NO_MORE_STATE;\n+          }\n+          setNextState(RecoverMetaState.RECOVER_META_SPLIT_LOGS);\n+          break;\n         case RECOVER_META_SPLIT_LOGS:\n           LOG.info(\"Start \" + this);\n           if (shouldSplitWal) {\n@@ -202,7 +217,7 @@ protected int getStateId(MasterProcedureProtos.RecoverMetaState recoverMetaState\n \n   @Override\n   protected MasterProcedureProtos.RecoverMetaState getInitialState() {\n-    return RecoverMetaState.RECOVER_META_SPLIT_LOGS;\n+    return RecoverMetaState.RECOVER_META_PREPARE;\n   }\n \n   @Override",
                "deletions": 1
            },
            {
                "sha": "5577be47ad25444ec983a524e78373264bd47d61",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ChunkCreator.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -424,7 +424,7 @@ private void logStats() {\n         long created = chunkCount.get();\n         long reused = reusedChunkCount.sum();\n         long total = created + reused;\n-        LOG.debug(\"{} Stats (chunk size={}): current pool size={}, created chunk count={}, \" +\n+        LOG.debug(\"{} stats (chunk size={}): current pool size={}, created chunk count={}, \" +\n                 \"reused chunk count={}, reuseRatio={}\", label, chunkSize, reclaimedChunks.size(),\n             created, reused,\n             (total == 0? \"0\": StringUtils.formatPercent((float)reused/(float)total,2)));",
                "deletions": 1
            },
            {
                "sha": "87f6fa4e25fae42c8c7f0a5e4c75da99567c1c7c",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManagerMetrics.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -98,6 +98,7 @@ public static void startCluster() throws Exception {\n \n   @AfterClass\n   public static void after() throws Exception {\n+    LOG.info(\"AFTER {} <= IS THIS NULL?\", TEST_UTIL);\n     TEST_UTIL.shutdownMiniCluster();\n   }\n ",
                "deletions": 0
            },
            {
                "sha": "dc939f50f00e01b79e8ac058e10608cdea475352",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java",
                "status": "added",
                "changes": 109,
                "additions": 109,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestRecoverMetaProcedure.java?ref=acbdb86bb4e1eaa496bb76b8a6dc77c15a3141c1",
                "patch": "@@ -0,0 +1,109 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.master.procedure;\n+\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.master.MasterServices;\n+import org.apache.hadoop.hbase.master.assignment.MockMasterServices;\n+import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n+import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;\n+import org.apache.hadoop.hbase.procedure2.StateMachineProcedure;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;\n+import org.apache.hadoop.hbase.testclassification.MasterTests;\n+import org.apache.hadoop.hbase.testclassification.SmallTests;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.mockito.Mockito;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+@Category({MasterTests.class, SmallTests.class})\n+public class TestRecoverMetaProcedure {\n+  private static final Logger LOG = LoggerFactory.getLogger(TestRecoverMetaProcedure.class);\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+      HBaseClassTestRule.forClass(TestRecoverMetaProcedure.class);\n+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();\n+\n+  /**\n+   * Test the new prepare step.\n+   * Here we test that our Mock is faking out the precedure well-enough for it to progress past the\n+   * first prepare stage.\n+   */\n+  @Test\n+  public void testPrepare() throws ProcedureSuspendedException, ProcedureYieldException,\n+      InterruptedException, IOException {\n+    RecoverMetaProcedure rmp = new RecoverMetaProcedure();\n+    MasterProcedureEnv env = Mockito.mock(MasterProcedureEnv.class);\n+    MasterServices masterServices =\n+        new MockMasterServices(UTIL.getConfiguration(), null);\n+    Mockito.when(env.getMasterServices()).thenReturn(masterServices);\n+    assertEquals(StateMachineProcedure.Flow.HAS_MORE_STATE,\n+        rmp.executeFromState(env, rmp.getInitialState()));\n+    int stateId = rmp.getCurrentStateId();\n+    assertEquals(MasterProcedureProtos.RecoverMetaState.RECOVER_META_SPLIT_LOGS_VALUE,\n+        rmp.getCurrentStateId());\n+  }\n+\n+  /**\n+   * Test the new prepare step.\n+   * If Master is stopping, procedure should skip the assign by returning NO_MORE_STATE\n+   */\n+  @Test\n+  public void testPrepareWithMasterStopping() throws ProcedureSuspendedException,\n+      ProcedureYieldException, InterruptedException, IOException {\n+    RecoverMetaProcedure rmp = new RecoverMetaProcedure();\n+    MasterProcedureEnv env = Mockito.mock(MasterProcedureEnv.class);\n+    MasterServices masterServices = new MockMasterServices(UTIL.getConfiguration(), null) {\n+      @Override\n+      public boolean isStopping() {\n+        return true;\n+      }\n+    };\n+    Mockito.when(env.getMasterServices()).thenReturn(masterServices);\n+    assertEquals(StateMachineProcedure.Flow.NO_MORE_STATE,\n+        rmp.executeFromState(env, rmp.getInitialState()));\n+  }\n+\n+  /**\n+   * Test the new prepare step.\n+   * If cluster is down, procedure should skip the assign by returning NO_MORE_STATE\n+   */\n+  @Test\n+  public void testPrepareWithNoCluster() throws ProcedureSuspendedException,\n+      ProcedureYieldException, InterruptedException, IOException {\n+    RecoverMetaProcedure rmp = new RecoverMetaProcedure();\n+    MasterProcedureEnv env = Mockito.mock(MasterProcedureEnv.class);\n+    MasterServices masterServices = new MockMasterServices(UTIL.getConfiguration(), null) {\n+      @Override\n+      public boolean isClusterUp() {\n+        return false;\n+      }\n+    };\n+    Mockito.when(env.getMasterServices()).thenReturn(masterServices);\n+    assertEquals(StateMachineProcedure.Flow.NO_MORE_STATE,\n+        rmp.executeFromState(env, rmp.getInitialState()));\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10853 NPE in RSRpcServices.get on trunk\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1582553 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a0415141f9a9453f8db4a88deb267f994a6c1420",
        "parent": "https://github.com/apache/hbase/commit/325fcaf4e111ae6a90868611c5bbe0b8649a5ef1",
        "bug_id": "hbase_168",
        "file": [
            {
                "sha": "c30b520cbf87bf52d5ddc1a128619ceea714b3ab",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "blob_url": "https://github.com/apache/hbase/blob/a0415141f9a9453f8db4a88deb267f994a6c1420/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "raw_url": "https://github.com/apache/hbase/raw/a0415141f9a9453f8db4a88deb267f994a6c1420/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java",
                "status": "modified",
                "changes": 52,
                "additions": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java?ref=a0415141f9a9453f8db4a88deb267f994a6c1420",
                "patch": "@@ -404,8 +404,10 @@ private Result append(final HRegion region, final MutationProto m,\n         region.getCoprocessorHost().postAppend(append, r);\n       }\n     }\n-    regionServer.metricsRegionServer.updateAppend(\n-      EnvironmentEdgeManager.currentTimeMillis() - before);\n+    if (regionServer.metricsRegionServer != null) {\n+      regionServer.metricsRegionServer.updateAppend(\n+        EnvironmentEdgeManager.currentTimeMillis() - before);\n+    }\n     return r;\n   }\n \n@@ -438,8 +440,10 @@ private Result increment(final HRegion region, final MutationProto mutation,\n         r = region.getCoprocessorHost().postIncrement(increment, r);\n       }\n     }\n-    regionServer.metricsRegionServer.updateIncrement(\n-      EnvironmentEdgeManager.currentTimeMillis() - before);\n+    if (regionServer.metricsRegionServer != null) {\n+      regionServer.metricsRegionServer.updateIncrement(\n+        EnvironmentEdgeManager.currentTimeMillis() - before);\n+    }\n     return r;\n   }\n \n@@ -609,12 +613,14 @@ private void doBatchOp(final RegionActionResult.Builder builder, final HRegion r\n         builder.addResultOrException(getResultOrException(ie, mutations.get(i).getIndex()));\n       }\n     }\n-    long after = EnvironmentEdgeManager.currentTimeMillis();\n-    if (batchContainsPuts) {\n-      regionServer.metricsRegionServer.updatePut(after - before);\n-    }\n-    if (batchContainsDelete) {\n-      regionServer.metricsRegionServer.updateDelete(after - before);\n+    if (regionServer.metricsRegionServer != null) {\n+      long after = EnvironmentEdgeManager.currentTimeMillis();\n+      if (batchContainsPuts) {\n+        regionServer.metricsRegionServer.updatePut(after - before);\n+      }\n+      if (batchContainsDelete) {\n+        regionServer.metricsRegionServer.updateDelete(after - before);\n+      }\n     }\n   }\n \n@@ -649,12 +655,14 @@ private void doBatchOp(final RegionActionResult.Builder builder, final HRegion r\n       }\n       return region.batchReplay(mArray);\n     } finally {\n-      long after = EnvironmentEdgeManager.currentTimeMillis();\n-      if (batchContainsPuts) {\n-        regionServer.metricsRegionServer.updatePut(after - before);\n-      }\n-      if (batchContainsDelete) {\n-        regionServer.metricsRegionServer.updateDelete(after - before);\n+      if (regionServer.metricsRegionServer != null) {\n+        long after = EnvironmentEdgeManager.currentTimeMillis();\n+          if (batchContainsPuts) {\n+          regionServer.metricsRegionServer.updatePut(after - before);\n+        }\n+        if (batchContainsDelete) {\n+          regionServer.metricsRegionServer.updateDelete(after - before);\n+        }\n       }\n     }\n   }\n@@ -1334,8 +1342,10 @@ public ReplicateWALEntryResponse replay(final RpcController controller,\n     } catch (IOException ie) {\n       throw new ServiceException(ie);\n     } finally {\n-      regionServer.metricsRegionServer.updateReplay(\n-        EnvironmentEdgeManager.currentTimeMillis() - before);\n+      if (regionServer.metricsRegionServer != null) {\n+        regionServer.metricsRegionServer.updateReplay(\n+          EnvironmentEdgeManager.currentTimeMillis() - before);\n+      }\n     }\n   }\n \n@@ -1574,8 +1584,10 @@ public GetResponse get(final RpcController controller,\n     } catch (IOException ie) {\n       throw new ServiceException(ie);\n     } finally {\n-      regionServer.metricsRegionServer.updateGet(\n-        EnvironmentEdgeManager.currentTimeMillis() - before);\n+      if (regionServer.metricsRegionServer != null) {\n+        regionServer.metricsRegionServer.updateGet(\n+          EnvironmentEdgeManager.currentTimeMillis() - before);\n+      }\n     }\n   }\n ",
                "deletions": 20
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10739 RS web UI NPE if master shuts down sooner\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1577415 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
        "parent": "https://github.com/apache/hbase/commit/1160171e645da5e18391523f76d2c673ba98a97f",
        "bug_id": "hbase_169",
        "file": [
            {
                "sha": "e6c86136ceaeaf1723d40d6ee57ce3513f2a6293",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -32,7 +32,7 @@ if (master.isActiveMaster()) {\n   ClusterStatus status = master.getClusterStatus();\n   masters = status.getBackupMasters();\n } else{\n-  ServerName sn = master.getMasterAddressManager().getMasterAddress();\n+  ServerName sn = master.getMasterAddressTracker().getMasterAddress();\n   assert sn != null : \"Failed to retreive master's ServerName!\";\n   masters = Collections.singletonList(sn);\n }",
                "deletions": 1
            },
            {
                "sha": "1efb0b011e5b9e0f3e683211428cae6e60affb15",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon",
                "status": "modified",
                "changes": 10,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -24,13 +24,12 @@ String format = \"html\";\n <%import>\n java.util.*;\n org.apache.hadoop.hbase.regionserver.HRegionServer;\n-org.apache.hadoop.hbase.util.Bytes;\n org.apache.hadoop.hbase.HRegionInfo;\n org.apache.hadoop.hbase.ServerName;\n org.apache.hadoop.hbase.HBaseConfiguration;\n org.apache.hadoop.hbase.protobuf.ProtobufUtil;\n org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ServerInfo;\n-org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;\n+org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;\n </%import>\n <%if format.equals(\"json\") %>\n   <& ../common/TaskMonitorTmpl; filter = filter; format = \"json\" &>\n@@ -41,6 +40,9 @@ org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;\n   ServerName serverName = ProtobufUtil.toServerName(serverInfo.getServerName());\n   List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(regionServer);\n   int masterInfoPort = regionServer.getConfiguration().getInt(\"hbase.master.info.port\", 16010);\n+  MasterAddressTracker masterAddressTracker = regionServer.getMasterAddressTracker();\n+  ServerName masterServerName = masterAddressTracker == null ? null\n+    : masterAddressTracker.getMasterAddress();\n </%java>\n <!--[if IE]>\n <!DOCTYPE html>\n@@ -146,9 +148,11 @@ org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;\n             <td>\n                 <%if (masterInfoPort < 0) %>\n                 No hbase.master.info.port found\n+                <%elseif masterServerName == null %>\n+                No master found\n                 <%else>\n                 <%java>\n-                String host = regionServer.getMasterAddressManager().getMasterAddress().getHostname() + \":\" + masterInfoPort;\n+                String host = masterServerName.getHostname() + \":\" + masterInfoPort;\n                 String url = \"//\" + host + \"/\";\n                 </%java>\n                 <a href=\"<% url %>\"><% host %></a>",
                "deletions": 3
            },
            {
                "sha": "a3f873f4d638622e0da7c9f5be781cac1cebc14b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -294,8 +294,8 @@\n   private DrainingServerTracker drainingServerTracker;\n   // Tracker for load balancer state\n   private LoadBalancerTracker loadBalancerTracker;\n-  // master address manager and watcher\n-  private MasterAddressTracker masterAddressManager;\n+  // master address tracker\n+  private MasterAddressTracker masterAddressTracker;\n \n   // RPC server for the HMaster\n   private final RpcServerInterface rpcServer;\n@@ -586,8 +586,8 @@ public void run() {\n     startupStatus.setDescription(\"Master startup\");\n     masterStartTime = System.currentTimeMillis();\n     try {\n-      this.masterAddressManager = new MasterAddressTracker(getZooKeeperWatcher(), this);\n-      this.masterAddressManager.start();\n+      this.masterAddressTracker = new MasterAddressTracker(getZooKeeperWatcher(), this);\n+      this.masterAddressTracker.start();\n \n       // Put up info server.\n       int port = this.conf.getInt(\"hbase.master.info.port\", HConstants.DEFAULT_MASTER_INFOPORT);\n@@ -1164,8 +1164,8 @@ public ActiveMasterManager getActiveMasterManager() {\n     return this.activeMasterManager;\n   }\n \n-  public MasterAddressTracker getMasterAddressManager() {\n-    return this.masterAddressManager;\n+  public MasterAddressTracker getMasterAddressTracker() {\n+    return this.masterAddressTracker;\n   }\n \n   /*",
                "deletions": 6
            },
            {
                "sha": "73bf86ba5bcb7b8527eaab70437dfbfec1b291bb",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 18,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -429,8 +429,8 @@\n   // zookeeper connection and watcher\n   private ZooKeeperWatcher zooKeeper;\n \n-  // master address manager and watcher\n-  private MasterAddressTracker masterAddressManager;\n+  // master address tracker\n+  private MasterAddressTracker masterAddressTracker;\n \n   // Cluster Status Tracker\n   private ClusterStatusTracker clusterStatusTracker;\n@@ -723,12 +723,12 @@ private void initializeZooKeeper() throws IOException, InterruptedException {\n     this.zooKeeper = new ZooKeeperWatcher(conf, REGIONSERVER + \":\" +\n       this.isa.getPort(), this);\n \n-    // Create the master address manager, register with zk, and start it.  Then\n+    // Create the master address tracker, register with zk, and start it.  Then\n     // block until a master is available.  No point in starting up if no master\n     // running.\n-    this.masterAddressManager = new MasterAddressTracker(this.zooKeeper, this);\n-    this.masterAddressManager.start();\n-    blockAndCheckIfStopped(this.masterAddressManager);\n+    this.masterAddressTracker = new MasterAddressTracker(this.zooKeeper, this);\n+    this.masterAddressTracker.start();\n+    blockAndCheckIfStopped(this.masterAddressTracker);\n \n     // Wait on cluster being up.  Master will set this flag up in zookeeper\n     // when ready.\n@@ -1588,8 +1588,8 @@ public MetricsRegionServer getMetrics() {\n   /**\n    * @return Master address tracker instance.\n    */\n-  public MasterAddressTracker getMasterAddressManager() {\n-    return this.masterAddressManager;\n+  public MasterAddressTracker getMasterAddressTracker() {\n+    return this.masterAddressTracker;\n   }\n \n   /*\n@@ -1960,7 +1960,7 @@ ReplicationSinkService getReplicationSinkService() {\n     boolean interrupted = false;\n     try {\n       while (keepLooping() && master == null) {\n-        sn = this.masterAddressManager.getMasterAddress(refresh);\n+        sn = this.masterAddressTracker.getMasterAddress(refresh);\n         if (sn == null) {\n           if (!keepLooping()) {\n             // give up with no connection.",
                "deletions": 9
            },
            {
                "sha": "64ae85983385b3cc62c81e37e4dded1e53743cb7",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -53,7 +53,6 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       tmpl.setFormat(req.getParameter(\"format\"));\n     if (req.getParameter(\"filter\") != null)\n       tmpl.setFilter(req.getParameter(\"filter\"));\n-    if (hrs != null) tmpl.render(resp.getWriter(), hrs);\n+    tmpl.render(resp.getWriter(), hrs);\n   }\n-\n }",
                "deletions": 2
            },
            {
                "sha": "e2b4f0cea8aac97326ba802866d42492680c6dc6",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -388,7 +388,7 @@ public void testCreateSilentIsReallySilent() throws InterruptedException,\n     // Assumes the  root of the ZooKeeper space is writable as it creates a node\n     // wherever the cluster home is defined.\n     ZooKeeperWatcher zk2 = new ZooKeeperWatcher(TEST_UTIL.getConfiguration(),\n-      \"testMasterAddressManagerFromZK\", null);\n+      \"testCreateSilentIsReallySilent\", null);\n \n     // Save the previous ACL\n     Stat s =  null;",
                "deletions": 1
            },
            {
                "sha": "b351d944c0ec21e30eebfbbfad17dc8be699c597",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -98,7 +98,7 @@ public void setupBasicMocks() {\n \n     // Fake MasterAddressTracker\n     MasterAddressTracker tracker = Mockito.mock(MasterAddressTracker.class);\n-    Mockito.doReturn(tracker).when(master).getMasterAddressManager();\n+    Mockito.doReturn(tracker).when(master).getMasterAddressTracker();\n     Mockito.doReturn(FAKE_HOST).when(tracker).getMasterAddress();\n \n     // Mock admin",
                "deletions": 1
            },
            {
                "sha": "82b8673742a4c34f7e35d9bf6523cdd92d91b524",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -58,7 +58,7 @@\n import org.apache.hadoop.hbase.Waiter;\n import org.apache.hadoop.hbase.master.SplitLogManager.Task;\n import org.apache.hadoop.hbase.master.SplitLogManager.TaskBatch;\n-import org.apache.hadoop.hbase.regionserver.TestMasterAddressManager.NodeCreationListener;\n+import org.apache.hadoop.hbase.regionserver.TestMasterAddressTracker.NodeCreationListener;\n import org.apache.hadoop.hbase.zookeeper.ZKSplitLog;\n import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;",
                "deletions": 1
            },
            {
                "sha": "cb5b556820c33d4a75a654c74c49ddf04874f763",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java",
                "status": "renamed",
                "changes": 20,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressTracker.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -36,8 +36,8 @@\n import org.junit.experimental.categories.Category;\n \n @Category(MediumTests.class)\n-public class TestMasterAddressManager {\n-  private static final Log LOG = LogFactory.getLog(TestMasterAddressManager.class);\n+public class TestMasterAddressTracker {\n+  private static final Log LOG = LogFactory.getLog(TestMasterAddressTracker.class);\n \n   private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();\n \n@@ -56,17 +56,17 @@ public static void tearDownAfterClass() throws Exception {\n    * @throws Exception\n    */\n   @Test\n-  public void testMasterAddressManagerFromZK() throws Exception {\n+  public void testMasterAddressTrackerFromZK() throws Exception {\n \n     ZooKeeperWatcher zk = new ZooKeeperWatcher(TEST_UTIL.getConfiguration(),\n-        \"testMasterAddressManagerFromZK\", null);\n+        \"testMasterAddressTrackerFromZK\", null);\n     ZKUtil.createAndFailSilent(zk, zk.baseZNode);\n \n     // Should not have a master yet\n-    MasterAddressTracker addressManager = new MasterAddressTracker(zk, null);\n-    addressManager.start();\n-    assertFalse(addressManager.hasMaster());\n-    zk.registerListener(addressManager);\n+    MasterAddressTracker addressTracker = new MasterAddressTracker(zk, null);\n+    addressTracker.start();\n+    assertFalse(addressTracker.hasMaster());\n+    zk.registerListener(addressTracker);\n \n     // Use a listener to capture when the node is actually created\n     NodeCreationListener listener = new NodeCreationListener(zk, zk.getMasterAddressZNode());\n@@ -83,8 +83,8 @@ public void testMasterAddressManagerFromZK() throws Exception {\n     LOG.info(\"Waiting for master address manager to be notified\");\n     listener.waitForCreation();\n     LOG.info(\"Master node created\");\n-    assertTrue(addressManager.hasMaster());\n-    ServerName pulledAddress = addressManager.getMasterAddress();\n+    assertTrue(addressTracker.hasMaster());\n+    ServerName pulledAddress = addressTracker.getMasterAddress();\n     assertTrue(pulledAddress.equals(sn));\n \n   }",
                "deletions": 10,
                "previous_filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressManager.java"
            },
            {
                "sha": "15e55bbbfb3735efd5197b15ee18fdd17e510014",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/6b2437de2ccc19fa383442f81b9096c81b2c8bb5/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java?ref=6b2437de2ccc19fa383442f81b9096c81b2c8bb5",
                "patch": "@@ -78,7 +78,7 @@ public void setupBasicMocks() throws IOException, ServiceException {\n     // Fake MasterAddressTracker\n     MasterAddressTracker mat = Mockito.mock(MasterAddressTracker.class);\n     Mockito.doReturn(fakeMasterAddress).when(mat).getMasterAddress();\n-    Mockito.doReturn(mat).when(rs).getMasterAddressManager();\n+    Mockito.doReturn(mat).when(rs).getMasterAddressTracker();\n \n     MetricsRegionServer rms = Mockito.mock(MetricsRegionServer.class);\n     Mockito.doReturn(new MetricsRegionServerWrapperStub()).when(rms).getRegionServerWrapper();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9294 NPE in /rs-status during RS shutdown\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1576953 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/748aaefb12dee675adc89a7fbf08790f311525fb",
        "parent": "https://github.com/apache/hbase/commit/d8ce8d050636dd7432dab8698098bb7ec3c10215",
        "bug_id": "hbase_170",
        "file": [
            {
                "sha": "f94f53f412bae5bcba8709191fb6d0b61cca3c24",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/748aaefb12dee675adc89a7fbf08790f311525fb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/748aaefb12dee675adc89a7fbf08790f311525fb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java?ref=748aaefb12dee675adc89a7fbf08790f311525fb",
                "patch": "@@ -53,7 +53,7 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       tmpl.setFormat(req.getParameter(\"format\"));\n     if (req.getParameter(\"filter\") != null)\n       tmpl.setFilter(req.getParameter(\"filter\"));\n-    tmpl.render(resp.getWriter(), hrs);\n+    if (hrs != null) tmpl.render(resp.getWriter(), hrs);\n   }\n \n }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10231 Potential NPE in HBaseFsck#checkMetaRegion()\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1553286 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/f23ae335a46d6e6f460da812371dad12ed86ea47",
        "parent": "https://github.com/apache/hbase/commit/1fc969846cb264ccfec3be1ce53427957a08f5b8",
        "bug_id": "hbase_171",
        "file": [
            {
                "sha": "3c8b2add83fcd3d63ceb45b78be8482043397adf",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "blob_url": "https://github.com/apache/hbase/blob/f23ae335a46d6e6f460da812371dad12ed86ea47/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "raw_url": "https://github.com/apache/hbase/raw/f23ae335a46d6e6f460da812371dad12ed86ea47/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java?ref=f23ae335a46d6e6f460da812371dad12ed86ea47",
                "patch": "@@ -2704,6 +2704,11 @@ boolean checkMetaRegion() throws IOException, KeeperException, InterruptedExcept\n         errors\n             .reportError(ERROR_CODE.MULTI_META_REGION, \"hbase:meta is found on more than one region.\");\n         if (shouldFixAssignments()) {\n+          if (metaHbckInfo == null) {\n+            errors.print(\n+              \"Unable to fix problem with hbase:meta due to hbase:meta region info missing\");\n+            return false;\n+          }\n           errors.print(\"Trying to fix a problem with hbase:meta..\");\n           setShouldRerun();\n           // try fix it (treat is a dupe assignment)",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9796 npe in RegionServerCallable\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1533296 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/c476b4bd87ecacda7dde52c768c7fc1fe599bd5d",
        "parent": "https://github.com/apache/hbase/commit/25c871b71c7ea20eb571e4f5e196a1e9fc02cafa",
        "bug_id": "hbase_172",
        "file": [
            {
                "sha": "22e659e16a75fb3f0d0f203bf369cdc737d2a045",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCallable.java",
                "blob_url": "https://github.com/apache/hbase/blob/c476b4bd87ecacda7dde52c768c7fc1fe599bd5d/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCallable.java",
                "raw_url": "https://github.com/apache/hbase/raw/c476b4bd87ecacda7dde52c768c7fc1fe599bd5d/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCallable.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RegionServerCallable.java?ref=c476b4bd87ecacda7dde52c768c7fc1fe599bd5d",
                "patch": "@@ -117,7 +117,7 @@ public void throwable(Throwable t, boolean retrying) {\n       // if thrown these exceptions, we clear all the cache entries that\n       // map to that slow/dead server; otherwise, let cache miss and ask\n       // hbase:meta again to find the new location\n-      getConnection().clearCaches(location.getServerName());\n+      if (this.location != null) getConnection().clearCaches(location.getServerName());\n     } else if (t instanceof RegionMovedException) {\n       getConnection().updateCachedLocations(tableName, row, t, location);\n     } else if (t instanceof NotServingRegionException && !retrying) {\n@@ -136,7 +136,7 @@ public String getExceptionMessageAdditionalDetail() {\n   public long sleep(long pause, int tries) {\n     // Tries hasn't been bumped up yet so we use \"tries + 1\" to get right pause time\n     long sleep = ConnectionUtils.getPauseTime(pause, tries + 1);\n-    if (sleep < MIN_WAIT_DEAD_SERVER \n+    if (sleep < MIN_WAIT_DEAD_SERVER\n         && (location == null || getConnection().isDeadServer(location.getServerName()))) {\n       sleep = ConnectionUtils.addJitter(MIN_WAIT_DEAD_SERVER, 0.10f);\n     }\n@@ -152,4 +152,4 @@ public HRegionInfo getHRegionInfo() {\n     }\n     return this.location.getRegionInfo();\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9519 fix NPE in EncodedScannerV2.getFirstKeyInBlock()\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1525953 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/29909882ea7781203d638f4bcba1d81bfb95d895",
        "parent": "https://github.com/apache/hbase/commit/3c986c4c674abd9912dda1e0342e887e0e968eb0",
        "bug_id": "hbase_173",
        "file": [
            {
                "sha": "3e478be721cc7b0bf09c860bb338acb96be8cdb4",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java",
                "blob_url": "https://github.com/apache/hbase/blob/29909882ea7781203d638f4bcba1d81bfb95d895/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java",
                "raw_url": "https://github.com/apache/hbase/raw/29909882ea7781203d638f4bcba1d81bfb95d895/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java",
                "status": "modified",
                "changes": 18,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java?ref=29909882ea7781203d638f4bcba1d81bfb95d895",
                "patch": "@@ -992,17 +992,18 @@ protected void updateCurrentBlock(HFileBlock newBlock) {\n             \"EncodedScanner works only on encoded data blocks\");\n       }\n \n-      short dataBlockEncoderId = block.getDataBlockEncodingId();\n+      updateDataBlockEncoder(block);\n+      seeker.setCurrentBuffer(getEncodedBuffer(newBlock));\n+      blockFetches++;\n+    }\n+\n+    private void updateDataBlockEncoder(HFileBlock curBlock) {\n+      short dataBlockEncoderId = curBlock.getDataBlockEncodingId();\n       if (dataBlockEncoder == null ||\n-          !DataBlockEncoding.isCorrectEncoder(dataBlockEncoder,\n-              dataBlockEncoderId)) {\n-        DataBlockEncoder encoder =\n-            DataBlockEncoding.getDataBlockEncoderById(dataBlockEncoderId);\n+          !DataBlockEncoding.isCorrectEncoder(dataBlockEncoder, dataBlockEncoderId)) {\n+        DataBlockEncoder encoder = DataBlockEncoding.getDataBlockEncoderById(dataBlockEncoderId);\n         setDataBlockEncoder(encoder);\n       }\n-\n-      seeker.setCurrentBuffer(getEncodedBuffer(newBlock));\n-      blockFetches++;\n     }\n \n     private ByteBuffer getEncodedBuffer(HFileBlock newBlock) {\n@@ -1097,6 +1098,7 @@ private void assertValidSeek() {\n \n     @Override\n     protected ByteBuffer getFirstKeyInBlock(HFileBlock curBlock) {\n+      updateDataBlockEncoder(curBlock);\n       return dataBlockEncoder.getFirstKeyInBlock(getEncodedBuffer(curBlock));\n     }\n ",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9554 TestOfflineMetaRebuildOverlap#testMetaRebuildOverlapFail fails due to NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1523870 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852",
        "parent": "https://github.com/apache/hbase/commit/80ab9fe15c9751ddb54039e97c96d84c4e1c841c",
        "bug_id": "hbase_174",
        "file": [
            {
                "sha": "2b72fd3cbff33e1672e025db20e43bb781a29c91",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/8fd9db6d6c7a3b702e752b7c4aa18f48215c0852/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8fd9db6d6c7a3b702e752b7c4aa18f48215c0852",
                "patch": "@@ -2298,7 +2298,7 @@ public void unassign(HRegionInfo region, boolean force, ServerName dest) {\n         // Region is not in transition.\n         // We can unassign it only if it's not SPLIT/MERGED.\n         state = regionStates.getRegionState(encodedName);\n-        if (state.isMerged() || state.isSplit()) {\n+        if (state != null && (state.isMerged() || state.isSplit())) {\n           LOG.info(\"Attempting to unassign \" + state + \", ignored\");\n           return;\n         }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9552. NPE while updating RegionServers with the favored nodes assignments.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1523849 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a61c9beeeb6e37190490dae8226284d10127ca30",
        "parent": "https://github.com/apache/hbase/commit/c26a69f23ec41455b3b06b3f2b83fbe1251fa323",
        "bug_id": "hbase_175",
        "file": [
            {
                "sha": "2335b4f1f08eff5f0f41daf583713a346531fd22",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a61c9beeeb6e37190490dae8226284d10127ca30/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a61c9beeeb6e37190490dae8226284d10127ca30/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementMaintainer.java?ref=a61c9beeeb6e37190490dae8226284d10127ca30",
                "patch": "@@ -685,9 +685,9 @@ private void updateAssignmentPlanToRegionServers(FavoredNodesPlan plan)\n             }\n             // Update the single server update\n             singleServerPlan.updateAssignmentPlan(region, favoredServerList);\n-          }\n-          regionUpdateInfos.add(\n+            regionUpdateInfos.add(\n               new Pair<HRegionInfo, List<ServerName>>(region, favoredServerList));\n+          }\n         }\n         if (singleServerPlan != null) {\n           // Update the current region server with its updated favored nodes",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9486 NPE in HTable.close() with AsyncProcess\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1522852 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/1d376ebc0d117b607e254a3942b0fb56b71cd167",
        "parent": "https://github.com/apache/hbase/commit/b862303a415426a75a3cd0f8bf297b05fbe36f7b",
        "bug_id": "hbase_176",
        "file": [
            {
                "sha": "0023917242000a22fc5845711897ecd8e1cdea9b",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "blob_url": "https://github.com/apache/hbase/blob/1d376ebc0d117b607e254a3942b0fb56b71cd167/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "raw_url": "https://github.com/apache/hbase/raw/1d376ebc0d117b607e254a3942b0fb56b71cd167/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java",
                "status": "modified",
                "changes": 31,
                "additions": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java?ref=1d376ebc0d117b607e254a3942b0fb56b71cd167",
                "patch": "@@ -145,23 +145,27 @@\n   }\n \n   private static class BatchErrors {\n-    private List<Throwable> throwables = new ArrayList<Throwable>();\n-    private List<Row> actions = new ArrayList<Row>();\n-    private List<String> addresses = new ArrayList<String>();\n+    private final List<Throwable> throwables = new ArrayList<Throwable>();\n+    private final List<Row> actions = new ArrayList<Row>();\n+    private final List<String> addresses = new ArrayList<String>();\n+\n+    public synchronized void add(Throwable ex, Row row, HRegionLocation location) {\n+      if (row == null){\n+        throw new IllegalArgumentException(\"row cannot be null. location=\" + location);\n+      }\n \n-    public void add(Throwable ex, Row row, HRegionLocation location) {\n       throwables.add(ex);\n       actions.add(row);\n       addresses.add(location != null ? location.getHostnamePort() : \"null location\");\n     }\n \n-    private RetriesExhaustedWithDetailsException makeException() {\n+    private synchronized RetriesExhaustedWithDetailsException makeException() {\n       return new RetriesExhaustedWithDetailsException(\n           new ArrayList<Throwable>(throwables),\n           new ArrayList<Row>(actions), new ArrayList<String>(addresses));\n     }\n \n-    public void clear() {\n+    public synchronized void clear() {\n       throwables.clear();\n       actions.clear();\n       addresses.clear();\n@@ -171,6 +175,10 @@ public void clear() {\n   public AsyncProcess(HConnection hc, TableName tableName, ExecutorService pool,\n       AsyncProcessCallback<CResult> callback, Configuration conf,\n       RpcRetryingCallerFactory rpcCaller) {\n+    if (hc == null){\n+      throw new IllegalArgumentException(\"HConnection cannot be null.\");\n+    }\n+\n     this.hConnection = hc;\n     this.tableName = tableName;\n     this.pool = pool;\n@@ -283,6 +291,10 @@ private void addAction(HRegionLocation loc, Action<Row> action, Map<HRegionLocat\n   private HRegionLocation findDestLocation(Row row, int numAttempt,\n                                            int posInList, boolean force,\n                                            Map<String, Boolean> regionStatus) {\n+    if (row == null){\n+      throw new IllegalArgumentException(\"row cannot be null\");\n+    }\n+\n     HRegionLocation loc = null;\n     IOException locationException = null;\n     try {\n@@ -608,10 +620,11 @@ private void receiveMultiAction(List<Action<Row>> initialActions,\n           ConnectionUtils.getPauseTime(pause, numAttempt));\n       if (numAttempt > 3 && LOG.isDebugEnabled()) {\n         // We use this value to have some logs when we have multiple failures, but not too many\n-        //  logs as errors are to be expected wehn region moves, split and so on\n+        //  logs, as errors are to be expected when a region moves, splits and so on\n         LOG.debug(\"Attempt #\" + numAttempt + \"/\" + numTries + \" failed \" + failureCount +\n-          \" ops , resubmitting \" + toReplay.size() + \", \" + location + \", last exception was: \" +\n-          throwable.getMessage() + \", sleeping \" + backOffTime + \"ms\");\n+            \" ops , resubmitting \" + toReplay.size() + \", \" + location + \", last exception was: \" +\n+            (throwable == null ? \"null\" : throwable.getMessage()) +\n+            \", sleeping \" + backOffTime + \"ms\");\n       }\n       try {\n         Thread.sleep(backOffTime);",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9498 NPE in HBaseAdmin if master not running\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1521902 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6bcccde06ad58f6c8d2a9a09a3c80a8642db73eb",
        "parent": "https://github.com/apache/hbase/commit/8e4d074c127529dc497b96d82e905e2c42b923a5",
        "bug_id": "hbase_177",
        "file": [
            {
                "sha": "e368ca27043ac385e4b3a454fe8fdb9622ee32f0",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/6bcccde06ad58f6c8d2a9a09a3c80a8642db73eb/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/6bcccde06ad58f6c8d2a9a09a3c80a8642db73eb/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java?ref=6bcccde06ad58f6c8d2a9a09a3c80a8642db73eb",
                "patch": "@@ -3062,7 +3062,8 @@ public void prepare(boolean reload) throws IOException {\n \n     @Override\n     public void close() throws IOException {\n-      this.masterAdmin.close();\n+      // The above prepare could fail but this would still be called though masterAdmin is null\n+      if (this.masterAdmin != null) this.masterAdmin.close();\n     }\n   }\n \n@@ -3083,7 +3084,8 @@ public void prepare(boolean reload) throws IOException {\n \n     @Override\n     public void close() throws IOException {\n-      this.masterMonitor.close();\n+      // The above prepare could fail but this would still be called though masterMonitor is null\n+      if (this.masterMonitor != null) this.masterMonitor.close();\n     }\n   }\n ",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9302 NPE when granting permission on table (Ted Yu)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1516593 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fa5e087ae9b9bb4fe96fb273100f1f38d31f1091",
        "parent": "https://github.com/apache/hbase/commit/af208097a5915d4e99f2e19dbdd4e72af415828c",
        "bug_id": "hbase_178",
        "file": [
            {
                "sha": "6b30ff01417cc005a907d438d9cf57c81499620d",
                "filename": "hbase-server/src/main/ruby/hbase/security.rb",
                "blob_url": "https://github.com/apache/hbase/blob/fa5e087ae9b9bb4fe96fb273100f1f38d31f1091/hbase-server/src/main/ruby/hbase/security.rb",
                "raw_url": "https://github.com/apache/hbase/raw/fa5e087ae9b9bb4fe96fb273100f1f38d31f1091/hbase-server/src/main/ruby/hbase/security.rb",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/ruby/hbase/security.rb?ref=fa5e087ae9b9bb4fe96fb273100f1f38d31f1091",
                "patch": "@@ -53,14 +53,14 @@ def grant(user, permissions, table_name=nil, family=nil, qualifier=nil)\n         end\n \n         if (table_name != nil)\n+          tablebytes=table_name.to_java_bytes\n           #check if the tablename passed is actually a namespace\n           if (isNamespace?(table_name))\n             # Namespace should exist first.\n             namespace_name = table_name[1...table_name.length]\n             raise(ArgumentError, \"Can't find a namespace: #{namespace_name}\") unless namespace_exists?(namespace_name)\n \n             #We pass the namespace name along with \"@\" so that we can differentiate a namespace from a table.\n-            tablebytes=table_name.to_java_bytes\n             # invoke cp endpoint to perform access controlse\n             org.apache.hadoop.hbase.protobuf.ProtobufUtil.grant(\n               protocol, user, tablebytes, perm.getActions())",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9038  Compaction WALEdit gives NPEs with Replication enabled\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1508255 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/45968ad1b9ab4056e24ccf8e5ce248144bcfb729",
        "parent": "https://github.com/apache/hbase/commit/fd991fe9089afe84a45fa2720d3cd09801cd059f",
        "bug_id": "hbase_179",
        "file": [
            {
                "sha": "8692390abab9202de3e54de7064393cdc1327791",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "blob_url": "https://github.com/apache/hbase/blob/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "raw_url": "https://github.com/apache/hbase/raw/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "status": "modified",
                "changes": 14,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=45968ad1b9ab4056e24ccf8e5ce248144bcfb729",
                "patch": "@@ -222,11 +222,25 @@ public void visitLogEntryBeforeWrite(HRegionInfo info, HLogKey logKey,\n   @Override\n   public void visitLogEntryBeforeWrite(HTableDescriptor htd, HLogKey logKey,\n                                        WALEdit logEdit) {\n+    scopeWALEdits(htd, logKey, logEdit);\n+  }\n+\n+  /**\n+   * Utility method used to set the correct scopes on each log key. Doesn't set a scope on keys\n+   * from compaction WAL edits and if the scope is local.\n+   * @param htd Descriptor used to find the scope to use\n+   * @param logKey Key that may get scoped according to its edits\n+   * @param logEdit Edits used to lookup the scopes\n+   */\n+  public static void scopeWALEdits(HTableDescriptor htd, HLogKey logKey,\n+                                   WALEdit logEdit) {\n     NavigableMap<byte[], Integer> scopes =\n         new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);\n     byte[] family;\n     for (KeyValue kv : logEdit.getKeyValues()) {\n       family = kv.getFamily();\n+      if (kv.matchingFamily(WALEdit.METAFAMILY)) continue;\n+\n       int scope = htd.getFamily(family).getScope();\n       if (scope != REPLICATION_SCOPE_LOCAL &&\n           !scopes.containsKey(family)) {",
                "deletions": 0
            },
            {
                "sha": "922102240e3f96e342c8c17b53729ae5288cf920",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "blob_url": "https://github.com/apache/hbase/blob/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "raw_url": "https://github.com/apache/hbase/raw/45968ad1b9ab4056e24ccf8e5ce248144bcfb729/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "status": "modified",
                "changes": 17,
                "additions": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java?ref=45968ad1b9ab4056e24ccf8e5ce248144bcfb729",
                "patch": "@@ -24,6 +24,10 @@\n import org.apache.hadoop.hbase.LargeTests;\n import org.apache.hadoop.hbase.client.*;\n import org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication;\n+import org.apache.hadoop.hbase.protobuf.generated.WALProtos;\n+import org.apache.hadoop.hbase.regionserver.wal.HLogKey;\n+import org.apache.hadoop.hbase.regionserver.wal.WALEdit;\n+import org.apache.hadoop.hbase.replication.regionserver.Replication;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n import org.apache.hadoop.hbase.util.JVMClusterUtil;\n@@ -461,4 +465,17 @@ public void testVerifyRepJob() throws Exception {\n         findCounter(VerifyReplication.Verifier.Counters.BADROWS).getValue());\n   }\n \n+  /**\n+   * Test for HBASE-9038, Replication.scopeWALEdits would NPE if it wasn't filtering out\n+   * the compaction WALEdit\n+   * @throws Exception\n+   */\n+  @Test(timeout=300000)\n+  public void testCompactionWALEdits() throws Exception {\n+    WALProtos.CompactionDescriptor compactionDescriptor =\n+        WALProtos.CompactionDescriptor.getDefaultInstance();\n+    WALEdit edit = WALEdit.createCompaction(compactionDescriptor);\n+    Replication.scopeWALEdits(htable1.getTableDescriptor(), new HLogKey(), edit);\n+  }\n+\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9044 Merging regions throws NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1507500 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/3812294456ff9a8672a7e2368d466a7119ff4959",
        "parent": "https://github.com/apache/hbase/commit/ff5fdb671f0e0d9fc76b6ead1800188569fb2348",
        "bug_id": "hbase_180",
        "file": [
            {
                "sha": "fffc213491586bdd160e161d7915e80c9db4654c",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.hbase.exceptions.FailedLogCloseException;\n import org.apache.hadoop.hbase.exceptions.HBaseSnapshotException;\n import org.apache.hadoop.hbase.exceptions.MasterNotRunningException;\n+import org.apache.hadoop.hbase.exceptions.MergeRegionException;\n import org.apache.hadoop.hbase.exceptions.NotServingRegionException;\n import org.apache.hadoop.hbase.exceptions.RegionException;\n import org.apache.hadoop.hbase.exceptions.RestoreSnapshotException;\n@@ -1710,6 +1711,9 @@ public void mergeRegions(final byte[] encodedNameOfRegionA,\n       if (ioe instanceof UnknownRegionException) {\n         throw (UnknownRegionException) ioe;\n       }\n+      if (ioe instanceof MergeRegionException) {\n+        throw (MergeRegionException) ioe;\n+      }\n       LOG.error(\"Unexpected exception: \" + se\n           + \" from calling HMaster.dispatchMergingRegions\");\n     } catch (DeserializationException de) {\n@@ -2740,4 +2744,4 @@ public long sleep(long pause, int tries) {\n   public CoprocessorRpcChannel coprocessorService() {\n     return new MasterCoprocessorRpcChannel(connection);\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 1
            },
            {
                "sha": "d4fe3d06325bfd4a8dcb0f8e801070e3b0b702c9",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java",
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java",
                "status": "added",
                "changes": 44,
                "additions": 44,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/MergeRegionException.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.exceptions;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+\n+/**\n+ * Thrown when something is wrong in trying to merge two regions.\n+ */\n+@InterfaceAudience.Public\n+@InterfaceStability.Stable\n+public class MergeRegionException extends RegionException {\n+\n+  private static final long serialVersionUID = 4970899110066124122L;\n+\n+  /** default constructor */\n+  public MergeRegionException() {\n+    super();\n+  }\n+\n+  /**\n+   * Constructor\n+   * @param s message\n+   */\n+  public MergeRegionException(String s) {\n+    super(s);\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "5c491735c9d319c23448daa1823c037a5cc7ff81",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 29,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "patch": "@@ -69,16 +69,17 @@\n import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;\n import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hbase.exceptions.MasterNotRunningException;\n+import org.apache.hadoop.hbase.exceptions.MergeRegionException;\n import org.apache.hadoop.hbase.exceptions.PleaseHoldException;\n import org.apache.hadoop.hbase.exceptions.TableNotDisabledException;\n import org.apache.hadoop.hbase.exceptions.TableNotFoundException;\n import org.apache.hadoop.hbase.exceptions.UnknownProtocolException;\n import org.apache.hadoop.hbase.exceptions.UnknownRegionException;\n import org.apache.hadoop.hbase.executor.ExecutorService;\n import org.apache.hadoop.hbase.executor.ExecutorType;\n-import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.RpcServer;\n import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;\n+import org.apache.hadoop.hbase.ipc.RpcServerInterface;\n import org.apache.hadoop.hbase.ipc.ServerRpcController;\n import org.apache.hadoop.hbase.ipc.SimpleRpcScheduler;\n import org.apache.hadoop.hbase.master.balancer.BalancerChore;\n@@ -1525,16 +1526,28 @@ public DispatchMergingRegionsResponse dispatchMergingRegions(\n               : encodedNameOfRegionB)));\n     }\n \n-    if (!forcible && !HRegionInfo.areAdjacent(regionStateA.getRegion(),\n-            regionStateB.getRegion())) {\n-      throw new ServiceException(\"Unable to merge not adjacent regions \"\n-          + regionStateA.getRegion().getRegionNameAsString() + \", \"\n-          + regionStateB.getRegion().getRegionNameAsString()\n-          + \" where forcible = \" + forcible);\n+    if (!regionStateA.isOpened() || !regionStateB.isOpened()) {\n+      throw new ServiceException(new MergeRegionException(\n+        \"Unable to merge regions not online \" + regionStateA + \", \" + regionStateB));\n+    }\n+\n+    HRegionInfo regionInfoA = regionStateA.getRegion();\n+    HRegionInfo regionInfoB = regionStateB.getRegion();\n+    if (regionInfoA.compareTo(regionInfoB) == 0) {\n+      throw new ServiceException(new MergeRegionException(\n+        \"Unable to merge a region to itself \" + regionInfoA + \", \" + regionInfoB));\n+    }\n+\n+    if (!forcible && !HRegionInfo.areAdjacent(regionInfoA, regionInfoB)) {\n+      throw new ServiceException(new MergeRegionException(\n+        \"Unable to merge not adjacent regions \"\n+          + regionInfoA.getRegionNameAsString() + \", \"\n+          + regionInfoB.getRegionNameAsString()\n+          + \" where forcible = \" + forcible));\n     }\n \n     try {\n-      dispatchMergingRegions(regionStateA.getRegion(), regionStateB.getRegion(), forcible);\n+      dispatchMergingRegions(regionInfoA, regionInfoB, forcible);\n     } catch (IOException ioe) {\n       throw new ServiceException(ioe);\n     }",
                "deletions": 8
            },
            {
                "sha": "0a8c6f84cdc6b454fdd8b563b0ca69bc872133d3",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java",
                "status": "modified",
                "changes": 23,
                "additions": 17,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DispatchMergingRegionHandler.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "patch": "@@ -20,12 +20,14 @@\n \n import java.io.IOException;\n import java.io.InterruptedIOException;\n+import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.RegionLoad;\n+import org.apache.hadoop.hbase.ServerLoad;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.exceptions.RegionOpeningException;\n import org.apache.hadoop.hbase.executor.EventHandler;\n@@ -34,6 +36,7 @@\n import org.apache.hadoop.hbase.master.MasterServices;\n import org.apache.hadoop.hbase.master.RegionPlan;\n import org.apache.hadoop.hbase.master.RegionStates;\n+import org.apache.hadoop.hbase.master.ServerManager;\n import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;\n \n /**\n@@ -98,12 +101,8 @@ public void process() throws IOException {\n       // Move region_b to region a's location, switch region_a and region_b if\n       // region_a's load lower than region_b's, so we will always move lower\n       // load region\n-      RegionLoad loadOfRegionA = masterServices.getServerManager()\n-          .getLoad(region_a_location).getRegionsLoad()\n-          .get(region_a.getRegionName());\n-      RegionLoad loadOfRegionB = masterServices.getServerManager()\n-          .getLoad(region_b_location).getRegionsLoad()\n-          .get(region_b.getRegionName());\n+      RegionLoad loadOfRegionA = getRegionLoad(region_a_location, region_a);\n+      RegionLoad loadOfRegionB = getRegionLoad(region_b_location, region_b);\n       if (loadOfRegionA != null && loadOfRegionB != null\n           && loadOfRegionA.getRequestsCount() < loadOfRegionB\n               .getRequestsCount()) {\n@@ -174,4 +173,16 @@ public void process() throws IOException {\n           + (EnvironmentEdgeManager.currentTimeMillis() - startTime) + \"ms\");\n     }\n   }\n+\n+  private RegionLoad getRegionLoad(ServerName sn, HRegionInfo hri) {\n+    ServerManager serverManager =  masterServices.getServerManager();\n+    ServerLoad load = serverManager.getLoad(sn);\n+    if (load != null) {\n+      Map<byte[], RegionLoad> regionsLoad = load.getRegionsLoad();\n+      if (regionsLoad != null) {\n+        return regionsLoad.get(hri.getRegionName());\n+      }\n+    }\n+    return null;\n+  }\n }\n\\ No newline at end of file",
                "deletions": 6
            },
            {
                "sha": "49d77873052ed44cce4ad5847b04ee0f9e7bdeb3",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/3812294456ff9a8672a7e2368d466a7119ff4959/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java",
                "status": "modified",
                "changes": 56,
                "additions": 56,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionMergeTransactionOnCluster.java?ref=3812294456ff9a8672a7e2368d466a7119ff4959",
                "patch": "@@ -21,6 +21,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.IOException;\n import java.util.List;\n@@ -43,7 +44,10 @@\n import org.apache.hadoop.hbase.client.Result;\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.exceptions.MergeRegionException;\n+import org.apache.hadoop.hbase.exceptions.UnknownRegionException;\n import org.apache.hadoop.hbase.master.HMaster;\n+import org.apache.hadoop.hbase.master.RegionStates;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Pair;\n import org.junit.AfterClass;\n@@ -190,6 +194,58 @@ public void testCleanMergeReference() throws Exception {\n     }\n   }\n \n+  /**\n+   * This test tests 1, merging region not online;\n+   * 2, merging same two regions; 3, merging unknown regions.\n+   * They are in one test case so that we don't have to create\n+   * many tables, and these tests are simple.\n+   */\n+  @Test\n+  public void testMerge() throws Exception {\n+    LOG.info(\"Starting testMerge\");\n+    final byte[] tableName = Bytes.toBytes(\"testMerge\");\n+\n+    try {\n+      // Create table and load data.\n+      HTable table = createTableAndLoadData(master, tableName);\n+      RegionStates regionStates = master.getAssignmentManager().getRegionStates();\n+      List<HRegionInfo> regions = regionStates.getRegionsOfTable(tableName);\n+      // Fake offline one region\n+      HRegionInfo a = regions.get(0);\n+      HRegionInfo b = regions.get(1);\n+      regionStates.regionOffline(a);\n+      try {\n+        // Merge offline region. Region a is offline here\n+        admin.mergeRegions(a.getEncodedNameAsBytes(), b.getEncodedNameAsBytes(), false);\n+        fail(\"Offline regions should not be able to merge\");\n+      } catch (IOException ie) {\n+        assertTrue(\"Exception should mention regions not online\",\n+          ie.getMessage().contains(\"regions not online\")\n+            && ie instanceof MergeRegionException);\n+      }\n+      try {\n+        // Merge the same region: b and b.\n+        admin.mergeRegions(b.getEncodedNameAsBytes(), b.getEncodedNameAsBytes(), true);\n+        fail(\"A region should not be able to merge with itself, even forcifully\");\n+      } catch (IOException ie) {\n+        assertTrue(\"Exception should mention regions not online\",\n+          ie.getMessage().contains(\"region to itself\")\n+            && ie instanceof MergeRegionException);\n+      }\n+      try {\n+        // Merge unknown regions\n+        admin.mergeRegions(Bytes.toBytes(\"-f1\"), Bytes.toBytes(\"-f2\"), true);\n+        fail(\"Unknown region could not be merged\");\n+      } catch (IOException ie) {\n+        assertTrue(\"UnknownRegionException should be thrown\",\n+          ie instanceof UnknownRegionException);\n+      }\n+      table.close();\n+    } finally {\n+      TEST_UTIL.deleteTable(tableName);\n+    }\n+  }\n+\n   private void mergeRegionsAndVerifyRegionNum(HMaster master, byte[] tablename,\n       int regionAnum, int regionBnum, int expectedRegionNum) throws Exception {\n     requestMergeRegion(master, tablename, regionAnum, regionBnum);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8380 NPE in HBaseClient.readResponse\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1471271 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/929776752e52e97d397930807b38f95c9bfc3c1b",
        "parent": "https://github.com/apache/hbase/commit/374052f071f314b22eb37394c24407377beb5de7",
        "bug_id": "hbase_181",
        "file": [
            {
                "sha": "be60417f26db45f6e3daae1d0c3bf21389062e95",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/929776752e52e97d397930807b38f95c9bfc3c1b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/929776752e52e97d397930807b38f95c9bfc3c1b/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "status": "modified",
                "changes": 26,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=929776752e52e97d397930807b38f95c9bfc3c1b",
                "patch": "@@ -392,7 +392,7 @@ protected Connection createConnection(ConnectionId remoteId, final Codec codec,\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Use \" + authMethod + \" authentication for protocol \"\n-            + protocol.getSimpleName());\n+            + (protocol == null ? \"null\" : protocol.getSimpleName()));\n       }\n       reloginMaxBackoff = conf.getInt(\"hbase.security.relogin.maxbackoff\", 5000);\n       this.remoteId = remoteId;\n@@ -811,7 +811,7 @@ protected synchronized void setupIOstreams()\n                 ticket = ticket.getRealUser();\n               }\n             }\n-            boolean continueSasl = false;\n+            boolean continueSasl;\n             try {\n               if (ticket == null) {\n                 throw new NullPointerException(\"ticket is null\");\n@@ -855,7 +855,7 @@ public Boolean run() throws IOException {\n         }\n       } catch (Throwable t) {\n         failedServers.addToFailedServers(remoteId.address);\n-        IOException e = null;\n+        IOException e;\n         if (t instanceof IOException) {\n           e = (IOException)t;\n           markClosed(e);\n@@ -1007,14 +1007,16 @@ protected void readResponse() {\n             if (call != null) call.setException(re);\n           }\n         } else {\n-          Message rpcResponseType;\n-          try {\n-            // TODO: Why pb engine pollution in here in this class?  FIX.\n-            rpcResponseType =\n-              ProtobufRpcClientEngine.Invoker.getReturnProtoType(\n-                reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));\n-          } catch (Exception e) {\n-            throw new RuntimeException(e); //local exception\n+          Message rpcResponseType = null;\n+          if (call != null){\n+            try {\n+              // TODO: Why pb engine pollution in here in this class?  FIX.\n+              rpcResponseType =\n+                ProtobufRpcClientEngine.Invoker.getReturnProtoType(\n+                  reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));\n+            } catch (Exception e) {\n+              throw new RuntimeException(e); //local exception\n+            }\n           }\n           Message value = null;\n           if (rpcResponseType != null) {\n@@ -1474,4 +1476,4 @@ public int hashCode() {\n       return hashcode;\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 12
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-14907 NPE of MobUtils.hasMobColumns in Build failed in Jenkins: HBase-Trunk_matrix \u00bb latest1.8,Hadoop #513 (Jingcheng Du)\nPut back HBASE-14907 but with right JIRA number (by doing a revert of a revert)\n\nThis reverts commit 35a7b56e530f3e4a12f1968df5aee9d3b63815bb.",
        "commit": "https://github.com/apache/hbase/commit/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92",
        "parent": "https://github.com/apache/hbase/commit/a4a44587b3f2236750569f5c032b283dc77942f6",
        "bug_id": "hbase_182",
        "file": [
            {
                "sha": "1e86254a061de41e730c62f0b5bf28a69db51462",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "status": "modified",
                "changes": 23,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java?ref=c6b8e6f1ac4aebb996d793b1cae0a95dd343db92",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HRegionInfo;\n-import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -345,19 +344,13 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n       LOG.debug(\"Table '\" + tableName + \"' archived!\");\n     }\n \n-    // Archive the mob data if there is a mob-enabled column\n-    HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(tableName);\n-    boolean hasMob = MobUtils.hasMobColumns(htd);\n-    Path mobTableDir = null;\n-    if (hasMob) {\n-      // Archive mob data\n-      mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n-              tableName);\n-      Path regionDir =\n-              new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n-      if (fs.exists(regionDir)) {\n-        HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n-      }\n+    // Archive mob data\n+    Path mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n+            tableName);\n+    Path regionDir =\n+            new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n+    if (fs.exists(regionDir)) {\n+      HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n     }\n \n     // Delete table directory from FS (temp directory)\n@@ -366,7 +359,7 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n     }\n \n     // Delete the table directory where the mob files are saved\n-    if (hasMob && mobTableDir != null && fs.exists(mobTableDir)) {\n+    if (mobTableDir != null && fs.exists(mobTableDir)) {\n       if (!fs.delete(mobTableDir, true)) {\n         throw new IOException(\"Couldn't delete mob dir \" + mobTableDir);\n       }",
                "deletions": 15
            },
            {
                "sha": "676a3f4dec947a85be7f243cf086f23b976c233e",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/c6b8e6f1ac4aebb996d793b1cae0a95dd343db92/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "status": "modified",
                "changes": 58,
                "additions": 34,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java?ref=c6b8e6f1ac4aebb996d793b1cae0a95dd343db92",
                "patch": "@@ -195,32 +195,15 @@ public void testRecoveryAndDoubleExecution() throws Exception {\n   @Test(timeout=90000)\n   public void testRollbackAndDoubleExecution() throws Exception {\n     final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecution\");\n+    testRollbackAndDoubleExecution(MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\"));\n+  }\n \n-    // create the table\n-    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n-    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n-\n-    // Start the Create procedure && kill the executor\n-    final byte[][] splitKeys = new byte[][] {\n-      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n-    };\n+  @Test(timeout=90000)\n+  public void testRollbackAndDoubleExecutionOnMobTable() throws Exception {\n+    final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecutionOnMobTable\");\n     HTableDescriptor htd = MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\");\n-    htd.setRegionReplication(3);\n-    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n-    long procId = procExec.submitProcedure(\n-      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n-\n-    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n-    //       so you have to look at this test at least once when you add a new step.\n-    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n-        procExec, procId, 4, CreateTableState.values());\n-\n-    MasterProcedureTestingUtility.validateTableDeletion(\n-      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n-\n-    // are we able to create the table after a rollback?\n-    resetProcExecutorTestingKillFlag();\n-    testSimpleCreate(tableName, splitKeys);\n+    htd.getFamily(Bytes.toBytes(\"f1\")).setMobEnabled(true);\n+    testRollbackAndDoubleExecution(htd);\n   }\n \n   @Test(timeout=90000)\n@@ -282,4 +265,31 @@ protected void rollbackState(final MasterProcedureEnv env, final CreateTableStat\n       }\n     }\n   }\n+\n+  private void testRollbackAndDoubleExecution(HTableDescriptor htd) throws Exception {\n+    // create the table\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+\n+    // Start the Create procedure && kill the executor\n+    final byte[][] splitKeys = new byte[][] {\n+      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n+    };\n+    htd.setRegionReplication(3);\n+    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n+    long procId = procExec.submitProcedure(\n+      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n+\n+    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n+    //       so you have to look at this test at least once when you add a new step.\n+    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n+        procExec, procId, 4, CreateTableState.values());\n+    TableName tableName = htd.getTableName();\n+    MasterProcedureTestingUtility.validateTableDeletion(\n+      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n+\n+    // are we able to create the table after a rollback?\n+    resetProcExecutorTestingKillFlag();\n+    testSimpleCreate(tableName, splitKeys);\n+  }\n }",
                "deletions": 24
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Revert \"HBASE-4907 NPE of MobUtils.hasMobColumns in Build failed in Jenkins: HBase-Trunk_matrix \u00bb latest1.8,Hadoop #513 (Jingcheng Du)\"\nRevert because I pitched this against the wrong JIRA number!\n\nThis reverts commit 03e4712f0ca08d57586b3fc4d93cf02c999515d8.",
        "commit": "https://github.com/apache/hbase/commit/35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
        "parent": "https://github.com/apache/hbase/commit/26dd0d17f81627d3688f28bba1a293513ff5d702",
        "bug_id": "hbase_183",
        "file": [
            {
                "sha": "4b95fa89d74499704a1b845f628638ea4caa6f07",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java",
                "status": "modified",
                "changes": 23,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java?ref=35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.MetaTableAccessor;\n import org.apache.hadoop.hbase.TableName;\n import org.apache.hadoop.hbase.TableNotDisabledException;\n@@ -344,13 +345,19 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n       LOG.debug(\"Table '\" + tableName + \"' archived!\");\n     }\n \n-    // Archive mob data\n-    Path mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n-            tableName);\n-    Path regionDir =\n-            new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n-    if (fs.exists(regionDir)) {\n-      HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n+    // Archive the mob data if there is a mob-enabled column\n+    HTableDescriptor htd = env.getMasterServices().getTableDescriptors().get(tableName);\n+    boolean hasMob = MobUtils.hasMobColumns(htd);\n+    Path mobTableDir = null;\n+    if (hasMob) {\n+      // Archive mob data\n+      mobTableDir = FSUtils.getTableDir(new Path(mfs.getRootDir(), MobConstants.MOB_DIR_NAME),\n+              tableName);\n+      Path regionDir =\n+              new Path(mobTableDir, MobUtils.getMobRegionInfo(tableName).getEncodedName());\n+      if (fs.exists(regionDir)) {\n+        HFileArchiver.archiveRegion(fs, mfs.getRootDir(), mobTableDir, regionDir);\n+      }\n     }\n \n     // Delete table directory from FS (temp directory)\n@@ -359,7 +366,7 @@ protected static void deleteFromFs(final MasterProcedureEnv env,\n     }\n \n     // Delete the table directory where the mob files are saved\n-    if (mobTableDir != null && fs.exists(mobTableDir)) {\n+    if (hasMob && mobTableDir != null && fs.exists(mobTableDir)) {\n       if (!fs.delete(mobTableDir, true)) {\n         throw new IOException(\"Couldn't delete mob dir \" + mobTableDir);\n       }",
                "deletions": 8
            },
            {
                "sha": "0aad5fa81cf04fe505ccd311662cd2f2937fbb7f",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "blob_url": "https://github.com/apache/hbase/blob/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "raw_url": "https://github.com/apache/hbase/raw/35a7b56e530f3e4a12f1968df5aee9d3b63815bb/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java",
                "status": "modified",
                "changes": 58,
                "additions": 24,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestCreateTableProcedure.java?ref=35a7b56e530f3e4a12f1968df5aee9d3b63815bb",
                "patch": "@@ -195,15 +195,32 @@ public void testRecoveryAndDoubleExecution() throws Exception {\n   @Test(timeout=90000)\n   public void testRollbackAndDoubleExecution() throws Exception {\n     final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecution\");\n-    testRollbackAndDoubleExecution(MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\"));\n-  }\n \n-  @Test(timeout=90000)\n-  public void testRollbackAndDoubleExecutionOnMobTable() throws Exception {\n-    final TableName tableName = TableName.valueOf(\"testRollbackAndDoubleExecutionOnMobTable\");\n+    // create the table\n+    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n+    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n+\n+    // Start the Create procedure && kill the executor\n+    final byte[][] splitKeys = new byte[][] {\n+      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n+    };\n     HTableDescriptor htd = MasterProcedureTestingUtility.createHTD(tableName, \"f1\", \"f2\");\n-    htd.getFamily(Bytes.toBytes(\"f1\")).setMobEnabled(true);\n-    testRollbackAndDoubleExecution(htd);\n+    htd.setRegionReplication(3);\n+    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n+    long procId = procExec.submitProcedure(\n+      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n+\n+    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n+    //       so you have to look at this test at least once when you add a new step.\n+    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n+        procExec, procId, 4, CreateTableState.values());\n+\n+    MasterProcedureTestingUtility.validateTableDeletion(\n+      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n+\n+    // are we able to create the table after a rollback?\n+    resetProcExecutorTestingKillFlag();\n+    testSimpleCreate(tableName, splitKeys);\n   }\n \n   @Test(timeout=90000)\n@@ -265,31 +282,4 @@ protected void rollbackState(final MasterProcedureEnv env, final CreateTableStat\n       }\n     }\n   }\n-\n-  private void testRollbackAndDoubleExecution(HTableDescriptor htd) throws Exception {\n-    // create the table\n-    final ProcedureExecutor<MasterProcedureEnv> procExec = getMasterProcedureExecutor();\n-    ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);\n-\n-    // Start the Create procedure && kill the executor\n-    final byte[][] splitKeys = new byte[][] {\n-      Bytes.toBytes(\"a\"), Bytes.toBytes(\"b\"), Bytes.toBytes(\"c\")\n-    };\n-    htd.setRegionReplication(3);\n-    HRegionInfo[] regions = ModifyRegionUtils.createHRegionInfos(htd, splitKeys);\n-    long procId = procExec.submitProcedure(\n-      new CreateTableProcedure(procExec.getEnvironment(), htd, regions), nonceGroup, nonce);\n-\n-    // NOTE: the 4 (number of CreateTableState steps) is hardcoded,\n-    //       so you have to look at this test at least once when you add a new step.\n-    MasterProcedureTestingUtility.testRollbackAndDoubleExecution(\n-        procExec, procId, 4, CreateTableState.values());\n-    TableName tableName = htd.getTableName();\n-    MasterProcedureTestingUtility.validateTableDeletion(\n-      UTIL.getHBaseCluster().getMaster(), tableName, regions, \"f1\", \"f2\");\n-\n-    // are we able to create the table after a rollback?\n-    resetProcExecutorTestingKillFlag();\n-    testSimpleCreate(tableName, splitKeys);\n-  }\n }",
                "deletions": 34
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7933 NPE in TableLockManager\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1450554 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/4d356e24e18a9217b55739ec333ec9b80c9f25ec",
        "parent": "https://github.com/apache/hbase/commit/60859badfbba005d36ad58c7296740a53409f2a7",
        "bug_id": "hbase_184",
        "file": [
            {
                "sha": "5981c77ef269399b8ca2c3b74c9adf60b493ec06",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/4d356e24e18a9217b55739ec333ec9b80c9f25ec/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/4d356e24e18a9217b55739ec333ec9b80c9f25ec/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java",
                "status": "modified",
                "changes": 21,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/lock/ZKInterProcessLockBase.java?ref=4d356e24e18a9217b55739ec333ec9b80c9f25ec",
                "patch": "@@ -143,11 +143,6 @@ protected ZKInterProcessLockBase(ZooKeeperWatcher zkWatcher,\n     this.fullyQualifiedZNode = ZKUtil.joinZNode(parentLockNode, childNode);\n     this.metadata = metadata;\n     this.handler = handler;\n-    try {\n-      ZKUtil.createWithParents(zkWatcher, parentLockNode);\n-    } catch (KeeperException ex) {\n-      LOG.warn(\"Failed to create znode:\" + parentLockNode, ex);\n-    }\n   }\n \n   /**\n@@ -167,7 +162,12 @@ public boolean tryAcquire(long timeoutMs)\n     boolean hasTimeout = timeoutMs != -1;\n     long waitUntilMs =\n         hasTimeout ?EnvironmentEdgeManager.currentTimeMillis() + timeoutMs : -1;\n-    String createdZNode = createLockZNode();\n+    String createdZNode;\n+    try {\n+      createdZNode = createLockZNode();\n+    } catch (KeeperException ex) {\n+      throw new IOException(\"Failed to create znode: \" + fullyQualifiedZNode, ex);\n+    }\n     while (true) {\n       List<String> children;\n       try {\n@@ -221,13 +221,14 @@ public boolean tryAcquire(long timeoutMs)\n     return true;\n   }\n \n-  private String createLockZNode() {\n+  private String createLockZNode() throws KeeperException {\n     try {\n       return ZKUtil.createNodeIfNotExistsNoWatch(zkWatcher, fullyQualifiedZNode,\n           metadata, CreateMode.EPHEMERAL_SEQUENTIAL);\n-    } catch (KeeperException ex) {\n-      LOG.warn(\"Failed to create znode: \" + fullyQualifiedZNode, ex);\n-      return null;\n+    } catch (KeeperException.NoNodeException nne) {\n+      //create parents, retry\n+      ZKUtil.createWithParents(zkWatcher, parentLockNode);\n+      return createLockZNode();\n     }\n   }\n ",
                "deletions": 10
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7616 NPE in ZKProcedure.nodeCreated\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-7290@1445855 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6a254004094d53e1302b27c98de9962d978c4692",
        "parent": "https://github.com/apache/hbase/commit/bc37ac15538ffa6504bd8d7f9b28fc2cbe4d4ccf",
        "bug_id": "hbase_185",
        "file": [
            {
                "sha": "2e9e931a3a85b20cac430af9589dd63638255411",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinatorRpcs.java",
                "blob_url": "https://github.com/apache/hbase/blob/6a254004094d53e1302b27c98de9962d978c4692/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinatorRpcs.java",
                "raw_url": "https://github.com/apache/hbase/raw/6a254004094d53e1302b27c98de9962d978c4692/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinatorRpcs.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinatorRpcs.java?ref=6a254004094d53e1302b27c98de9962d978c4692",
                "patch": "@@ -39,7 +39,7 @@\n @InterfaceAudience.Public\n @InterfaceStability.Evolving\n public class ZKProcedureCoordinatorRpcs implements ProcedureCoordinatorRpcs {\n-  public static final Log LOG = LogFactory.getLog(ZKProcedureUtil.class);\n+  public static final Log LOG = LogFactory.getLog(ZKProcedureCoordinatorRpcs.class);\n   private ZKProcedureUtil zkProc = null;\n   protected ProcedureCoordinator coordinator = null;  // if started this should be non-null\n \n@@ -165,20 +165,20 @@ final public boolean start(final ProcedureCoordinator listener) {\n       this.zkProc = new ZKProcedureUtil(watcher, procedureType, coordName) {\n         @Override\n         public void nodeCreated(String path) {\n-          if (!zkProc.isInProcedurePath(path)) return;\n+          if (!isInProcedurePath(path)) return;\n           LOG.debug(\"Node created: \" + path);\n           logZKTree(this.baseZNode);\n-          if (zkProc.isAcquiredPathNode(path)) {\n+          if (isAcquiredPathNode(path)) {\n             // node wasn't present when we created the watch so zk event triggers acquire\n             listener.memberAcquiredBarrier(ZKUtil.getNodeName(ZKUtil.getParent(path)), ZKUtil.getNodeName(path));\n           }\n-          if (zkProc.isReachedPathNode(path)) {\n+          if (isReachedPathNode(path)) {\n             // node wasn't present when we created the watch so zk event triggers the finished barrier.\n \n             // TODO Nothing enforces that acquire and reached znodes from showing up in the wrong order.\n             listener.memberFinishedBarrier(ZKUtil.getNodeName(ZKUtil.getParent(path)), ZKUtil.getNodeName(path));\n           }\n-          if (zkProc.isAbortPathNode(path)) {\n+          if (isAbortPathNode(path)) {\n             abort(path);\n           }\n         }",
                "deletions": 5
            },
            {
                "sha": "d4df8d01880526d0df5cf4e1aa8fc99ba2a8404b",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java",
                "blob_url": "https://github.com/apache/hbase/blob/6a254004094d53e1302b27c98de9962d978c4692/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java",
                "raw_url": "https://github.com/apache/hbase/raw/6a254004094d53e1302b27c98de9962d978c4692/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java",
                "status": "modified",
                "changes": 48,
                "additions": 25,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java?ref=6a254004094d53e1302b27c98de9962d978c4692",
                "patch": "@@ -74,29 +74,31 @@ public ZKProcedureMemberRpcs(ZooKeeperWatcher watcher,\n     this.zkController = new ZKProcedureUtil(watcher, procType, memberName) {\n       @Override\n       public void nodeCreated(String path) {\n-        if (path.startsWith(this.baseZNode)) {\n-          LOG.info(\"Received created event:\" + path);\n-          // if it is a simple start/end/abort then we just rewatch the node\n-          if (path.equals(this.acquiredZnode)) {\n-            waitForNewProcedures();\n-            return;\n-          } else if (path.equals(this.abortZnode)) {\n-            watchForAbortedProcedures();\n-            return;\n-          }\n-          String parent = ZKUtil.getParent(path);\n-          // if its the end barrier, the procedure can be completed\n-          if (parent.equals(this.reachedZnode)) {\n-            receivedReachedGlobalBarrier(path);\n-            return;\n-          } else if (parent.equals(this.abortZnode)) {\n-            abort(path);\n-            return;\n-          } else if (parent.equals(this.acquiredZnode)) {\n-            startNewSubprocedure(path);\n-          } else {\n-            LOG.debug(\"Ignoring created notification for node:\" + path);\n-          }\n+        if (!isInProcedurePath(path)) {\n+          return;\n+        }\n+\n+        LOG.info(\"Received created event:\" + path);\n+        // if it is a simple start/end/abort then we just rewatch the node\n+        if (isAcquiredNode(path)) {\n+          waitForNewProcedures();\n+          return;\n+        } else if (isAbortNode(path)) {\n+          watchForAbortedProcedures();\n+          return;\n+        }\n+        String parent = ZKUtil.getParent(path);\n+        // if its the end barrier, the procedure can be completed\n+        if (isReachedNode(parent)) {\n+          receivedReachedGlobalBarrier(path);\n+          return;\n+        } else if (isAbortNode(parent)) {\n+          abort(path);\n+          return;\n+        } else if (isAcquiredNode(parent)) {\n+          startNewSubprocedure(path);\n+        } else {\n+          LOG.debug(\"Ignoring created notification for node:\" + path);\n         }\n       }\n ",
                "deletions": 23
            },
            {
                "sha": "5740b89a42b260afc0a6e773eacaec7d08d56311",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/6a254004094d53e1302b27c98de9962d978c4692/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/6a254004094d53e1302b27c98de9962d978c4692/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java",
                "status": "modified",
                "changes": 31,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java?ref=6a254004094d53e1302b27c98de9962d978c4692",
                "patch": "@@ -179,28 +179,51 @@ public ZooKeeperWatcher getWatcher() {\n    *\n    * @return true if starts with baseZnode\n    */\n-  public boolean isInProcedurePath(String path) {\n+  boolean isInProcedurePath(String path) {\n     return path.startsWith(baseZNode);\n   }\n \n+  /**\n+   * Is this the exact procedure barrier acquired znode\n+   */\n+  boolean isAcquiredNode(String path) {\n+    return path.equals(acquiredZnode);\n+  }\n+\n+  \n   /**\n    * Is this in the procedure barrier acquired znode path\n    */\n-  public boolean isAcquiredPathNode(String path) {\n+  boolean isAcquiredPathNode(String path) {\n     return path.startsWith(this.acquiredZnode) && !path.equals(acquiredZnode);\n   }\n \n+  /**\n+   * Is this the exact procedure barrier reached znode\n+   */\n+  boolean isReachedNode(String path) {\n+    return path.equals(reachedZnode);\n+  }\n+\n   /**\n    * Is this in the procedure barrier reached znode path\n    */\n-  public boolean isReachedPathNode(String path) {\n+  boolean isReachedPathNode(String path) {\n     return path.startsWith(this.reachedZnode) && !path.equals(reachedZnode);\n   }\n \n+\n+  /**\n+   * Is this in the procedure barrier abort znode path\n+   */\n+  boolean isAbortNode(String path) {\n+    return path.equals(abortZnode);\n+  }\n+\n   /**\n    * Is this in the procedure barrier abort znode path\n    */\n-    public boolean isAbortPathNode(String path) {\n+  public boolean isAbortPathNode(String path) {\n     return path.startsWith(this.abortZnode) && !path.equals(abortZnode);\n   }\n ",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7459 NPE in HMaster TestLocalHBaseCluster\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-7290@1445820 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/65d9c0d6d2c61e17ff45129fb8255d92336f41f7",
        "parent": "https://github.com/apache/hbase/commit/8d117a6117e1b944c6ab4f4aab5da37d23d5fa1b",
        "bug_id": "hbase_186",
        "file": [
            {
                "sha": "c66d4473be719f6ed3d8338ecca2997dd5e5cd1e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d9c0d6d2c61e17ff45129fb8255d92336f41f7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d9c0d6d2c61e17ff45129fb8255d92336f41f7/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=65d9c0d6d2c61e17ff45129fb8255d92336f41f7",
                "patch": "@@ -2044,7 +2044,9 @@ public MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer() {\n   }\n \n   public void shutdown() throws IOException {\n-    spanReceiverHost.closeReceivers();\n+    if (spanReceiverHost != null) { \n+      spanReceiverHost.closeReceivers();\n+    }\n     if (cpHost != null) {\n       cpHost.preShutdown();\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7513 HDFSBlocksDistribution shouldn't send NPEs when something goes wrong\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1430560 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/087f1df0e2a538f7627aaaa94e6e92215545dce4",
        "parent": "https://github.com/apache/hbase/commit/f77e5b5bff0d7718890d2f2b44f65dacf26ac4d1",
        "bug_id": "hbase_187",
        "file": [
            {
                "sha": "11619d3683e3a90af9cf257a2ec9434f1b8634a1",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java",
                "blob_url": "https://github.com/apache/hbase/blob/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java",
                "raw_url": "https://github.com/apache/hbase/raw/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java",
                "status": "modified",
                "changes": 11,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java?ref=087f1df0e2a538f7627aaaa94e6e92215545dce4",
                "patch": "@@ -30,7 +30,9 @@\n \n \n /**\n- * Data structure to describe the distribution of HDFS blocks amount hosts\n+ * Data structure to describe the distribution of HDFS blocks amount hosts.\n+ *\n+ * Adding erroneous data will be ignored silently.\n  */\n @InterfaceAudience.Private\n public class HDFSBlocksDistribution {\n@@ -122,8 +124,10 @@ public synchronized String toString() {\n    */\n   public void addHostsAndBlockWeight(String[] hosts, long weight) {\n     if (hosts == null || hosts.length == 0) {\n-      throw new NullPointerException(\"empty hosts\");\n+      // erroneous data\n+      return;\n     }\n+\n     addUniqueWeight(weight);\n     for (String hostname : hosts) {\n       addHostAndBlockWeight(hostname, weight);\n@@ -146,7 +150,8 @@ private void addUniqueWeight(long weight) {\n    */\n   private void addHostAndBlockWeight(String host, long weight) {\n     if (host == null) {\n-      throw new NullPointerException(\"Passed hostname is null\");\n+      // erroneous data\n+      return;\n     }\n \n     HostAndWeight hostAndWeight = this.hostAndWeights.get(host);",
                "deletions": 3
            },
            {
                "sha": "ea694067c7821d1bec532681e388628d70c8086c",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java",
                "blob_url": "https://github.com/apache/hbase/blob/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java",
                "raw_url": "https://github.com/apache/hbase/raw/087f1df0e2a538f7627aaaa94e6e92215545dce4/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java",
                "status": "added",
                "changes": 69,
                "additions": 69,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java?ref=087f1df0e2a538f7627aaaa94e6e92215545dce4",
                "patch": "@@ -0,0 +1,69 @@\n+/**\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static junit.framework.Assert.assertEquals;\n+\n+@Category(SmallTests.class)\n+public class TestHDFSBlocksDistribution {\n+  @Test\n+  public void testAddHostsAndBlockWeight() throws Exception {\n+    HDFSBlocksDistribution distribution = new HDFSBlocksDistribution();\n+    distribution.addHostsAndBlockWeight(null, 100);\n+    assertEquals(\"Expecting no hosts weights\", 0, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[0], 100);\n+    assertEquals(\"Expecting no hosts weights\", 0, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[] {\"test\"}, 101);\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[] {\"test\"}, 202);\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    assertEquals(\"test host should have weight 303\", 303,\n+        distribution.getHostAndWeights().get(\"test\").getWeight());\n+    distribution.addHostsAndBlockWeight(new String[] {\"testTwo\"}, 222);\n+    assertEquals(\"Should be two hosts\", 2, distribution.getHostAndWeights().size());\n+    assertEquals(\"Total weight should be 525\", 525, distribution.getUniqueBlocksTotalWeight());\n+  }\n+\n+  public class MockHDFSBlocksDistribution extends HDFSBlocksDistribution {\n+    public Map<String,HostAndWeight> getHostAndWeights() {\n+      HashMap<String, HostAndWeight> map = new HashMap<String, HostAndWeight>();\n+      map.put(\"test\", new HostAndWeight(null, 100));\n+      return map;\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testAdd() throws Exception {\n+    HDFSBlocksDistribution distribution = new HDFSBlocksDistribution();\n+    distribution.add(new MockHDFSBlocksDistribution());\n+    assertEquals(\"Expecting no hosts weights\", 0, distribution.getHostAndWeights().size());\n+    distribution.addHostsAndBlockWeight(new String[]{\"test\"}, 10);\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    distribution.add(new MockHDFSBlocksDistribution());\n+    assertEquals(\"Should be one host\", 1, distribution.getHostAndWeights().size());\n+    assertEquals(\"Total weight should be 10\", 10, distribution.getUniqueBlocksTotalWeight());\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7459 NPE in HMaster TestLocalHBaseCluster\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1426789 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/21e5b5200d1c27834b8048a384c5a1b126ccb8ea",
        "parent": "https://github.com/apache/hbase/commit/9e249881ef6a49879511be24e11d717738085dd5",
        "bug_id": "hbase_188",
        "file": [
            {
                "sha": "e25bddefe13953287ec8de0277ab2966586832cb",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/21e5b5200d1c27834b8048a384c5a1b126ccb8ea/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/21e5b5200d1c27834b8048a384c5a1b126ccb8ea/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=21e5b5200d1c27834b8048a384c5a1b126ccb8ea",
                "patch": "@@ -2050,7 +2050,9 @@ public MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer() {\n   }\n \n   public void shutdown() throws IOException {\n-    spanReceiverHost.closeReceivers();\n+    if (spanReceiverHost != null) { \n+      spanReceiverHost.closeReceivers();\n+    }\n     if (cpHost != null) {\n       cpHost.preShutdown();\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7355. NPE in ClusterStatus PB conversion\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1422076 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/5dbff1427d29bd6df0f78c8a2b29143b029d046e",
        "parent": "https://github.com/apache/hbase/commit/5c5c60f8820791cac506ca78d0872c363df286a5",
        "bug_id": "hbase_189",
        "file": [
            {
                "sha": "d9dfac038e8e2941f7ff87972182cbdc71ba2121",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java",
                "blob_url": "https://github.com/apache/hbase/blob/5dbff1427d29bd6df0f78c8a2b29143b029d046e/hbase-server/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java",
                "raw_url": "https://github.com/apache/hbase/raw/5dbff1427d29bd6df0f78c8a2b29143b029d046e/hbase-server/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java?ref=5dbff1427d29bd6df0f78c8a2b29143b029d046e",
                "patch": "@@ -337,7 +337,11 @@ public static ClusterStatus convert(ClusterStatusProtos.ClusterStatus proto) {\n       RegionState value = RegionState.convert(region.getRegionState());\n       rit.put(key,value);\n     }\n-    final String[] masterCoprocessors = proto.getMasterCoprocessorsList().toArray(new String[0]);\n+    final int numMasterCoprocessors = proto.getMasterCoprocessorsCount();\n+    final String[] masterCoprocessors = new String[numMasterCoprocessors];\n+    for (int i = 0; i < numMasterCoprocessors; i++) {\n+      masterCoprocessors[i] = proto.getMasterCoprocessors(i).getName();\n+    }\n     return new ClusterStatus(proto.getHbaseVersion().getVersion(),\n       ClusterId.convert(proto.getClusterId()).toString(),servers,deadServers,\n       ProtobufUtil.toServerName(proto.getMaster()),backupMasters,rit,masterCoprocessors,",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5633 NPE reading ZK config in HBase\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1304924 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/9cfe7cfa3c58e189256a83d764067612832b3daf",
        "parent": "https://github.com/apache/hbase/commit/accf8ee8621010ce76169a99ce0daee76469c0af",
        "bug_id": "hbase_190",
        "file": [
            {
                "sha": "07041b5dccbb4cfc370f2e8c6ab614dfc065890e",
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "blob_url": "https://github.com/apache/hbase/blob/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "raw_url": "https://github.com/apache/hbase/raw/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=9cfe7cfa3c58e189256a83d764067612832b3daf",
                "patch": "@@ -82,6 +82,9 @@\n   /** Cluster is fully-distributed */\n   public static final String CLUSTER_IS_DISTRIBUTED = \"true\";\n \n+  /** Default value for cluster distributed mode */  \n+  public static final String DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n+\n   /** default host address */\n   public static final String DEFAULT_HOST = \"0.0.0.0\";\n ",
                "deletions": 0
            },
            {
                "sha": "b6fef32ed5e359d9b81acd326988a2cb1d54c203",
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "blob_url": "https://github.com/apache/hbase/blob/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "raw_url": "https://github.com/apache/hbase/raw/9cfe7cfa3c58e189256a83d764067612832b3daf/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java?ref=9cfe7cfa3c58e189256a83d764067612832b3daf",
                "patch": "@@ -163,7 +163,8 @@ public static Properties parseZooCfg(Configuration conf,\n       }\n       // Special case for 'hbase.cluster.distributed' property being 'true'\n       if (key.startsWith(\"server.\")) {\n-        if (conf.get(HConstants.CLUSTER_DISTRIBUTED).equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n+        if (conf.get(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED).\n+              equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n             && value.startsWith(HConstants.LOCALHOST)) {\n           String msg = \"The server in zoo.cfg cannot be set to localhost \" +\n               \"in a fully-distributed setup because it won't be reachable. \" +",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4890 fix possible NPE in HConnectionManager\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1298272 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/0f7285a81a75b14be937e411f949081d465d1b92",
        "parent": "https://github.com/apache/hbase/commit/edc6696bc5a611a20fb1e713eb00b48d2cb7dd87",
        "bug_id": "hbase_191",
        "file": [
            {
                "sha": "2602461c58e6278b6c1b41abf622c685f402def9",
                "filename": "src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/0f7285a81a75b14be937e411f949081d465d1b92/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/0f7285a81a75b14be937e411f949081d465d1b92/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "status": "modified",
                "changes": 21,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=0f7285a81a75b14be937e411f949081d465d1b92",
                "patch": "@@ -676,7 +676,17 @@ protected void cleanupCalls(long rpcTimeout) {\n         Call c = itor.next().getValue();\n         long waitTime = System.currentTimeMillis() - c.getStartTime();\n         if (waitTime >= rpcTimeout) {\n-          c.setException(closeException); // local exception\n+          if (this.closeException == null) {\n+            // There may be no exception in the case that there are many calls\n+            // being multiplexed over this connection and these are succeeding\n+            // fine while this Call object is taking a long time to finish\n+            // over on the server; e.g. I just asked the regionserver to bulk\n+            // open 3k regions or its a big fat multiput into a heavily-loaded\n+            // server (Perhaps this only happens at the extremes?)\n+            this.closeException = new CallTimeoutException(\"Call id=\" + c.id +\n+              \", waitTime=\" + waitTime + \", rpcTimetout=\" + rpcTimeout);\n+          }\n+          c.setException(this.closeException);\n           synchronized (c) {\n             c.notifyAll();\n           }\n@@ -705,6 +715,15 @@ protected void cleanupCalls(long rpcTimeout) {\n     }\n   }\n \n+  /**\n+   * Client-side call timeout\n+   */\n+  public static class CallTimeoutException extends IOException {\n+    public CallTimeoutException(final String msg) {\n+      super(msg);\n+    }\n+  }\n+\n   /** Call implementation used for parallel calls. */\n   protected class ParallelCall extends Call {\n     private final ParallelResults results;",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5424 HTable meet NPE when call getRegionInfo() -- REVERT\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1292646 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/240d6bc52bb2276ebe60754e93c8344958e1d2b9",
        "parent": "https://github.com/apache/hbase/commit/6a80cf5bf9cb75b4124b2918fd0bfaf77486ec15",
        "bug_id": "hbase_192",
        "file": [
            {
                "sha": "29b8004b943232f5752e21bc0c62e930284b2cc8",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/240d6bc52bb2276ebe60754e93c8344958e1d2b9/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/240d6bc52bb2276ebe60754e93c8344958e1d2b9/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "status": "modified",
                "changes": 26,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HTable.java?ref=240d6bc52bb2276ebe60754e93c8344958e1d2b9",
                "patch": "@@ -449,26 +449,24 @@ public boolean processRow(Result rowResult) throws IOException {\n \n     MetaScannerVisitor visitor = new MetaScannerVisitor() {\n       public boolean processRow(Result rowResult) throws IOException {\n-        HRegionInfo info = Writables.getHRegionInfoOrNull(\n+        HRegionInfo info = Writables.getHRegionInfo(\n             rowResult.getValue(HConstants.CATALOG_FAMILY,\n                 HConstants.REGIONINFO_QUALIFIER));\n \n-        if (info != null) {\n-          if (!(Bytes.equals(info.getTableName(), getTableName()))) {\n-            return false;\n-          }\n+        if (!(Bytes.equals(info.getTableName(), getTableName()))) {\n+          return false;\n+        }\n \n-          HServerAddress server = new HServerAddress();\n-          byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY,\n+        HServerAddress server = new HServerAddress();\n+        byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY,\n             HConstants.SERVER_QUALIFIER);\n-          if (value != null && value.length > 0) {\n-            String hostAndPort = Bytes.toString(value);\n-            server = new HServerAddress(Addressing.createInetSocketAddressFromHostAndPortStr(hostAndPort));\n-          }\n+        if (value != null && value.length > 0) {\n+          String hostAndPort = Bytes.toString(value);\n+          server = new HServerAddress(Addressing.createInetSocketAddressFromHostAndPortStr(hostAndPort));\n+        }\n \n-          if (!(info.isOffline() || info.isSplit())) {\n-            regionMap.put(new UnmodifyableHRegionInfo(info), server);\n-          }\n+        if (!(info.isOffline() || info.isSplit())) {\n+          regionMap.put(new UnmodifyableHRegionInfo(info), server);\n         }\n         return true;\n       }",
                "deletions": 14
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5424 HTable meet NPE when call getRegionInfo()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1292645 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6a80cf5bf9cb75b4124b2918fd0bfaf77486ec15",
        "parent": "https://github.com/apache/hbase/commit/c3575d2eb57ad08466c260748c1be0ea09468f98",
        "bug_id": "hbase_193",
        "file": [
            {
                "sha": "5086640795b0259d7f5d8c056be8b3ae5cce248a",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/6a80cf5bf9cb75b4124b2918fd0bfaf77486ec15/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/6a80cf5bf9cb75b4124b2918fd0bfaf77486ec15/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "status": "modified",
                "changes": 26,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HTable.java?ref=6a80cf5bf9cb75b4124b2918fd0bfaf77486ec15",
                "patch": "@@ -449,24 +449,26 @@ public boolean processRow(Result rowResult) throws IOException {\n \n     MetaScannerVisitor visitor = new MetaScannerVisitor() {\n       public boolean processRow(Result rowResult) throws IOException {\n-        HRegionInfo info = Writables.getHRegionInfo(\n+        HRegionInfo info = Writables.getHRegionInfoOrNull(\n             rowResult.getValue(HConstants.CATALOG_FAMILY,\n                 HConstants.REGIONINFO_QUALIFIER));\n \n-        if (!(Bytes.equals(info.getTableName(), getTableName()))) {\n-          return false;\n-        }\n+        if (info != null) {\n+          if (!(Bytes.equals(info.getTableName(), getTableName()))) {\n+            return false;\n+          }\n \n-        HServerAddress server = new HServerAddress();\n-        byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY,\n+          HServerAddress server = new HServerAddress();\n+          byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY,\n             HConstants.SERVER_QUALIFIER);\n-        if (value != null && value.length > 0) {\n-          String hostAndPort = Bytes.toString(value);\n-          server = new HServerAddress(Addressing.createInetSocketAddressFromHostAndPortStr(hostAndPort));\n-        }\n+          if (value != null && value.length > 0) {\n+            String hostAndPort = Bytes.toString(value);\n+            server = new HServerAddress(Addressing.createInetSocketAddressFromHostAndPortStr(hostAndPort));\n+          }\n \n-        if (!(info.isOffline() || info.isSplit())) {\n-          regionMap.put(new UnmodifyableHRegionInfo(info), server);\n+          if (!(info.isOffline() || info.isSplit())) {\n+            regionMap.put(new UnmodifyableHRegionInfo(info), server);\n+          }\n         }\n         return true;\n       }",
                "deletions": 12
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5279 NPE in Master after upgrading to 0.92.0\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1245767 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/5994c5014382bb09d040c0f9e9e0aae13d025e8b",
        "parent": "https://github.com/apache/hbase/commit/7160ecd133e687887091c5b62e498db960f4b6ac",
        "bug_id": "hbase_194",
        "file": [
            {
                "sha": "77a121b33a40b228fb2040f57e61a71b170b8000",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "patch": "@@ -118,6 +118,8 @@ public boolean visit(Result r) throws IOException {\n         Pair<HRegionInfo, ServerName> region = parseCatalogResult(r);\n         if (region == null) return true;\n         HRegionInfo hri = region.getFirst();\n+        if (hri  == null) return true;\n+        if (hri.getTableNameAsString() == null) return true;\n         if (disabledTables.contains(\n             hri.getTableNameAsString())) return true;\n         // Are we to include split parents in the list?",
                "deletions": 0
            },
            {
                "sha": "6748e5c87260cc06ccf9b8c14e45258555b5f798",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "patch": "@@ -2254,6 +2254,7 @@ boolean waitUntilNoRegionsInTransition(final long timeout, Set<HRegionInfo> regi\n       if (region == null) continue;\n       HRegionInfo regionInfo = region.getFirst();\n       ServerName regionLocation = region.getSecond();\n+      if (regionInfo == null) continue;\n       String tableName = regionInfo.getTableNameAsString();\n       if (regionLocation == null) {\n         // regionLocation could be null if createTable didn't finish properly.",
                "deletions": 0
            },
            {
                "sha": "4c5b1dcad734d1117a8736f8e11d8d29291f7960",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/5994c5014382bb09d040c0f9e9e0aae13d025e8b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=5994c5014382bb09d040c0f9e9e0aae13d025e8b",
                "patch": "@@ -4123,6 +4123,8 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n        }\n     }\n \n+    Scan scan = new Scan(get);\n+\n     RegionScanner scanner = null;\n     try {\n       scanner = getScanner(scan);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4945  NPE in HRegion.bulkLoadHFiles (Andrew P and Lars H)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1210212 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8fe805ce290bfd9ceffc900509b36af4928bdf34",
        "parent": "https://github.com/apache/hbase/commit/f547de07c84137d865ac45518d6824f8577f0cab",
        "bug_id": "hbase_195",
        "file": [
            {
                "sha": "4ff8cbecae4d2615ad948e26f0c60013a7029f66",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/8fe805ce290bfd9ceffc900509b36af4928bdf34/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/8fe805ce290bfd9ceffc900509b36af4928bdf34/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8fe805ce290bfd9ceffc900509b36af4928bdf34",
                "patch": "@@ -450,6 +450,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4877  TestHCM failing sporadically on jenkins and always for me on an\n                ubuntu machine\n    HBASE-4878  Master crash when splitting hlog may cause data loss (Chunhui Shen)\n+   HBASE-4945  NPE in HRegion.bulkLoadHFiles (Andrew P and Lars H)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "deletions": 0
            },
            {
                "sha": "e26e213a628728a20bd018ef98f2e9c9b1e5c21b",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/8fe805ce290bfd9ceffc900509b36af4928bdf34/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/8fe805ce290bfd9ceffc900509b36af4928bdf34/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 21,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=8fe805ce290bfd9ceffc900509b36af4928bdf34",
                "patch": "@@ -2977,20 +2977,19 @@ public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths)\n               \"No such column family \" + Bytes.toStringBinary(familyName));\n           ioes.add(ioe);\n           failures.add(p);\n-        }\n-\n-        try {\n-          store.assertBulkLoadHFileOk(new Path(path));\n-        } catch (WrongRegionException wre) {\n-          // recoverable (file doesn't fit in region)\n-          failures.add(p);\n-        } catch (IOException ioe) {\n-          // unrecoverable (hdfs problem)\n-          ioes.add(ioe);\n+        } else {\n+          try {\n+            store.assertBulkLoadHFileOk(new Path(path));\n+          } catch (WrongRegionException wre) {\n+            // recoverable (file doesn't fit in region)\n+            failures.add(p);\n+          } catch (IOException ioe) {\n+            // unrecoverable (hdfs problem)\n+            ioes.add(ioe);\n+          }\n         }\n       }\n \n-\n       // validation failed, bail out before doing anything permanent.\n       if (failures.size() != 0) {\n         StringBuilder list = new StringBuilder();",
                "deletions": 11
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4725 NPE in AM#updateTimers\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1197815 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
        "parent": "https://github.com/apache/hbase/commit/bb0c9a11d8843344f287ac5919e4bea3f34fb240",
        "bug_id": "hbase_196",
        "file": [
            {
                "sha": "352113449e879f44dd5781b44af30c9dd2be3e61",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
                "patch": "@@ -449,6 +449,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4719  HBase script assumes pre-Hadoop 0.21 layout of jar files\n                (Roman Shposhnik)\n    HBASE-4553  The update of .tableinfo is not atomic; we remove then rename\n+   HBASE-4725  NPE in AM#updateTimers\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "deletions": 0
            },
            {
                "sha": "b5a2cc730e60dd8c7eca9b7a93ac9705e11a9f5f",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/56e19a72dcd13edd23b3f9c5a3696acdb5243dc7/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=56e19a72dcd13edd23b3f9c5a3696acdb5243dc7",
                "patch": "@@ -61,7 +61,6 @@\n import org.apache.hadoop.hbase.executor.ExecutorService;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n-import org.apache.hadoop.hbase.master.AssignmentManager.RegionState;\n import org.apache.hadoop.hbase.master.handler.ClosedRegionHandler;\n import org.apache.hadoop.hbase.master.handler.DisableTableHandler;\n import org.apache.hadoop.hbase.master.handler.EnableTableHandler;\n@@ -1056,6 +1055,7 @@ private void updateTimers(final ServerName sn) {\n       copy.putAll(this.regionPlans);\n     }\n     for (Map.Entry<String, RegionPlan> e: copy.entrySet()) {\n+      if (e.getValue() == null || e.getValue().getDestination() == null) continue;\n       if (!e.getValue().getDestination().equals(sn)) continue;\n       RegionState rs = null;\n       synchronized (this.regionsInTransition) {",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4578 NPE when altering a table that has moving regions\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1188439 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/95a8d2bfc4c761cf3306c082ebe4d33a107fbaae",
        "parent": "https://github.com/apache/hbase/commit/b31527ee356962ce7c1a29ad751dd376f338ff76",
        "bug_id": "hbase_197",
        "file": [
            {
                "sha": "5714d945e0915d1231d53810d63a8ff7983d1cbf",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/95a8d2bfc4c761cf3306c082ebe4d33a107fbaae/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/95a8d2bfc4c761cf3306c082ebe4d33a107fbaae/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=95a8d2bfc4c761cf3306c082ebe4d33a107fbaae",
                "patch": "@@ -394,6 +394,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4647  RAT finds about 40 files missing licenses\n    HBASE-4642  Add Apache License Header\n    HBASE-4591  TTL for old HLogs should be calculated from last modification time.\n+   HBASE-4578  NPE when altering a table that has moving regions (gaojinchao)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "deletions": 0
            },
            {
                "sha": "59fc758a1bb9e2e79acac4f379460ff8598bafc1",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/95a8d2bfc4c761cf3306c082ebe4d33a107fbaae/src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/95a8d2bfc4c761cf3306c082ebe4d33a107fbaae/src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java",
                "status": "modified",
                "changes": 16,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java?ref=95a8d2bfc4c761cf3306c082ebe4d33a107fbaae",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.hbase.master.handler;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.NavigableMap;\n@@ -86,7 +87,6 @@ public void process() {\n       if (eventType.isOnlineSchemaChangeSupported() && this.masterServices.\n           getAssignmentManager().getZKTable().\n           isEnabledTable(Bytes.toString(tableName))) {\n-        this.masterServices.getAssignmentManager().setRegionsToReopen(hris);\n         if (reOpenAllRegions(hris)) {\n           LOG.info(\"Completed table operation \" + eventType + \" on table \" +\n               Bytes.toString(tableName));\n@@ -108,17 +108,27 @@ public boolean reOpenAllRegions(List<HRegionInfo> regions) throws IOException {\n     TreeMap<ServerName, List<HRegionInfo>> serverToRegions = Maps\n         .newTreeMap();\n     NavigableMap<HRegionInfo, ServerName> hriHserverMapping = table.getRegionLocations();\n-\n+    List<HRegionInfo> reRegions = new ArrayList<HRegionInfo>();\n     for (HRegionInfo hri : regions) {\n       ServerName rsLocation = hriHserverMapping.get(hri);\n+\n+      // Skip the offlined split parent region\n+      // See HBASE-4578 for more information.\n+      if (null == rsLocation) {\n+        LOG.info(\"Skip \" + hri);\n+        continue;\n+      }\n       if (!serverToRegions.containsKey(rsLocation)) {\n         LinkedList<HRegionInfo> hriList = Lists.newLinkedList();\n         serverToRegions.put(rsLocation, hriList);\n       }\n+      reRegions.add(hri);\n       serverToRegions.get(rsLocation).add(hri);\n     }\n-    LOG.info(\"Reopening \" + regions.size() + \" regions on \"\n+    \n+    LOG.info(\"Reopening \" + reRegions.size() + \" regions on \"\n         + serverToRegions.size() + \" region servers.\");\n+    this.masterServices.getAssignmentManager().setRegionsToReopen(reRegions);\n     BulkReOpen bulkReopen = new BulkReOpen(this.server, serverToRegions,\n         this.masterServices.getAssignmentManager());\n     while (true) {",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4386  Fix a potential NPE in TaskMonitor\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1179479 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
        "parent": "https://github.com/apache/hbase/commit/ead9159ecd6b82b27dafce0ee3891d69444c1704",
        "bug_id": "hbase_198",
        "file": [
            {
                "sha": "295513df4de5e8663e2a93162d828c662e387163",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
                "patch": "@@ -339,6 +339,7 @@ Release 0.92.0 - Unreleased\n    HBASE-4494  AvroServer:: get fails with NPE on a non-existent row\n                (Kay Kay)\n    HBASE-4481  TestMergeTool failed in 0.92 build 20\n+   HBASE-4386  Fix a potential NPE in TaskMonitor (todd)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "deletions": 0
            },
            {
                "sha": "fc9c8301e470eda48495bcb8cd64954c32e3a56c",
                "filename": "src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "blob_url": "https://github.com/apache/hbase/blob/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "raw_url": "https://github.com/apache/hbase/raw/3d156a5a24699b3fd3c76863c99aaf79f7c865c2/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java",
                "status": "modified",
                "changes": 10,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java?ref=3d156a5a24699b3fd3c76863c99aaf79f7c865c2",
                "patch": "@@ -72,7 +72,9 @@ public MonitoredTask createStatus(String description) {\n         new Class<?>[] { MonitoredTask.class },\n         new PassthroughInvocationHandler<MonitoredTask>(stat));\n     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);\n-    tasks.add(pair);\n+    synchronized (this) {\n+      tasks.add(pair);\n+    }\n     return proxy;\n   }\n \n@@ -84,7 +86,9 @@ public MonitoredRPCHandler createRPCStatus(String description) {\n         new Class<?>[] { MonitoredRPCHandler.class },\n         new PassthroughInvocationHandler<MonitoredRPCHandler>(stat));\n     TaskAndWeakRefPair pair = new TaskAndWeakRefPair(stat, proxy);\n-    tasks.add(pair);\n+    synchronized (this) {\n+      tasks.add(pair);\n+    }\n     return proxy;\n   }\n \n@@ -142,7 +146,7 @@ private boolean canPurge(MonitoredTask stat) {\n   public void dumpAsText(PrintWriter out) {\n     long now = System.currentTimeMillis();\n     \n-    List<MonitoredTask> tasks = TaskMonitor.get().getTasks();\n+    List<MonitoredTask> tasks = getTasks();\n     for (MonitoredTask task : tasks) {\n       out.println(\"Task: \" + task.getDescription());\n       out.println(\"Status: \" + task.getState() + \":\" + task.getStatus());",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4088 npes in server shutdown\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1145855 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/148627fe9c029405b393a0bbcf5c16cec5cd48a9",
        "parent": "https://github.com/apache/hbase/commit/1f3a4fefb89d4fa108932270de25e4668e901451",
        "bug_id": "hbase_199",
        "file": [
            {
                "sha": "0b9ba405e28078bdb4eda9b536acfe6155b76241",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/148627fe9c029405b393a0bbcf5c16cec5cd48a9/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/148627fe9c029405b393a0bbcf5c16cec5cd48a9/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=148627fe9c029405b393a0bbcf5c16cec5cd48a9",
                "patch": "@@ -407,6 +407,7 @@ Release 0.90.4 - Unreleased\n                are hosted by the HRegionServer (Akash Ashok)\n    HBASE-4033  The shutdown RegionServer could be added to\n                AssignmentManager.servers again (Jieshan Bean)\n+   HBASE-4088  npes in server shutdown\n \n   IMPROVEMENT\n    HBASE-3882  hbase-config.sh needs to be updated so it can auto-detects the",
                "deletions": 0
            },
            {
                "sha": "2116f960a08f8df5981cf007f3b38f222d1f32a3",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/148627fe9c029405b393a0bbcf5c16cec5cd48a9/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/148627fe9c029405b393a0bbcf5c16cec5cd48a9/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java?ref=148627fe9c029405b393a0bbcf5c16cec5cd48a9",
                "patch": "@@ -186,7 +186,8 @@ public void process() throws IOException {\n       }\n     }\n \n-    LOG.info(\"Reassigning \" + hris.size() + \" region(s) that \" + serverName +\n+    LOG.info(\"Reassigning \" + (hris == null? 0: hris.size()) +\n+      \" region(s) that \" + serverName +\n       \" was carrying (skipping \" + regionsInTransition.size() +\n       \" regions(s) that are already in transition)\");\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4045  [replication] NPE in ReplicationSource when ZK is gone\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1141313 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/1d0975f581e26040e4d7022763dbadabd70e405a",
        "parent": "https://github.com/apache/hbase/commit/f0120d5a47dc59b32ede92454a96c214380f2d19",
        "bug_id": "hbase_200",
        "file": [
            {
                "sha": "21c07374889289340be60de18b440978189e037a",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/1d0975f581e26040e4d7022763dbadabd70e405a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/1d0975f581e26040e4d7022763dbadabd70e405a/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=1d0975f581e26040e4d7022763dbadabd70e405a",
                "patch": "@@ -381,6 +381,7 @@ Release 0.90.4 - Unreleased\n                (Vandana Ayyalasomayajula via Ted Yu)\n    HBASE-3984  CT.verifyRegionLocation isn't doing a very good check,\n                can delay cluster recovery\n+   HBASE-4045  [replication] NPE in ReplicationSource when ZK is gone\n \n   IMPROVEMENT\n    HBASE-3882  hbase-config.sh needs to be updated so it can auto-detects the",
                "deletions": 0
            },
            {
                "sha": "b6843b91269e7031a994108551ecef58b756d6ed",
                "filename": "src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "blob_url": "https://github.com/apache/hbase/blob/1d0975f581e26040e4d7022763dbadabd70e405a/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "raw_url": "https://github.com/apache/hbase/raw/1d0975f581e26040e4d7022763dbadabd70e405a/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "status": "modified",
                "changes": 8,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java?ref=1d0975f581e26040e4d7022763dbadabd70e405a",
                "patch": "@@ -219,16 +219,16 @@ private void connectExistingPeers() throws IOException, KeeperException {\n   /**\n    * Get the list of all the region servers from the specified peer\n    * @param zkw zk connection to use\n-   * @return list of region server addresses\n+   * @return list of region server addresses or an empty list if the slave\n+   * is unavailable\n    */\n   private List<ServerName> fetchSlavesAddresses(ZooKeeperWatcher zkw) {\n-    List<ServerName> rss = null;\n     try {\n-      rss = listChildrenAndGetAsServerNames(zkw, zkw.rsZNode);\n+      return listChildrenAndGetAsServerNames(zkw, zkw.rsZNode);\n     } catch (KeeperException e) {\n       LOG.warn(\"Cannot get peer's region server addresses\", e);\n+      return new ArrayList<ServerName>(0);\n     }\n-    return rss;\n   }\n \n   /**",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3889  NPE in Distributed Log Splitting\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1136659 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
        "parent": "https://github.com/apache/hbase/commit/0f15fcac46b9ffb520ca81e736f855bde2731e6f",
        "bug_id": "hbase_201",
        "file": [
            {
                "sha": "d481c9dec001d2d34ee0c1295bd9988685009164",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
                "patch": "@@ -127,6 +127,7 @@ Release 0.91.0 - Unreleased\n    HBASE-3983  list command in shell seems broken\n    HBASE-3793  HBASE-3468 Broke checkAndPut with null value (Ming Ma)\n    HBASE-3995  HBASE-3946 broke TestMasterFailover\n+   HBASE-3889  NPE in Distributed Log Splitting (Anirudh Todi)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "843873e8fa58dd51a56fb76945ff685d4141b8f7",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java",
                "blob_url": "https://github.com/apache/hbase/blob/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java",
                "raw_url": "https://github.com/apache/hbase/raw/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java",
                "status": "modified",
                "changes": 10,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java?ref=bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
                "patch": "@@ -136,6 +136,7 @@ public Status exec(String filename, CancelableProgressable p) {\n \n   @Override\n   public void run() {\n+   try {\n     LOG.info(\"SplitLogWorker \" + this.serverName + \" starting\");\n     this.watcher.registerListener(this);\n     int res;\n@@ -162,8 +163,13 @@ public void run() {\n     }\n \n     taskLoop();\n-\n-    LOG.info(\"SplitLogWorker \" + this.serverName + \" exiting\");\n+   } catch (Throwable t) {\n+\t   // only a logical error can cause here. Printing it out \n+\t   // to make debugging easier\n+\t   LOG.error(\"unexpected error \", t);\n+   } finally {\n+\t   LOG.info(\"SplitLogWorker \" + this.serverName + \" exiting\");\n+   }\n   }\n \n   /**",
                "deletions": 2
            },
            {
                "sha": "de28418efea0a7db8cc0a0ad709cc31f70236956",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java",
                "blob_url": "https://github.com/apache/hbase/blob/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java",
                "raw_url": "https://github.com/apache/hbase/raw/bd7c8e327f3e39fb2c10b5b3d192c374f35efb83/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java",
                "status": "modified",
                "changes": 32,
                "additions": 30,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java?ref=bd7c8e327f3e39fb2c10b5b3d192c374f35efb83",
                "patch": "@@ -152,7 +152,8 @@ public void tearDown() throws Exception {\n    * @throws IOException\n    * @see https://issues.apache.org/jira/browse/HBASE-3020\n    */\n-  @Test public void testRecoveredEditsPathForMeta() throws IOException {\n+  @Test \n+  public void testRecoveredEditsPathForMeta() throws IOException {\n     FileSystem fs = FileSystem.get(TEST_UTIL.getConfiguration());\n     byte [] encoded = HRegionInfo.FIRST_META_REGIONINFO.getEncodedNameAsBytes();\n     Path tdir = new Path(hbaseDir, Bytes.toString(HConstants.META_TABLE_NAME));\n@@ -952,7 +953,35 @@ public void testSplitLogFileWithOneRegion() throws IOException {\n \n     assertEquals(true, logsAreEqual(originalLog, splitLog));\n   }\n+  \n+  @Test\n+  public void testSplitLogFileDeletedRegionDir()\n+  throws IOException {\n+\tLOG.info(\"testSplitLogFileDeletedRegionDir\");\n+\tfinal String REGION = \"region__1\";\n+    regions.removeAll(regions);\n+    regions.add(REGION);\n+\n+\n+    generateHLogs(1, 10, -1);\n+    FileStatus logfile = fs.listStatus(hlogDir)[0];\n+    fs.initialize(fs.getUri(), conf);\n+    \n+    Path regiondir = new Path(tabledir, REGION);\n+    LOG.info(\"Region directory is\" + regiondir);\n+    fs.delete(regiondir, true);\n+    \n+    HLogSplitter.splitLogFileToTemp(hbaseDir, \"tmpdir\", logfile, fs,\n+        conf, reporter);\n+    HLogSplitter.moveRecoveredEditsFromTemp(\"tmpdir\", hbaseDir, oldLogDir,\n+        logfile.getPath().toString(), conf);\n+    \n+    assertTrue(!fs.exists(regiondir));\n+    assertTrue(true);\n+  }\n \n+  \n+  \n   @Test\n   public void testSplitLogFileEmpty() throws IOException {\n     LOG.info(\"testSplitLogFileEmpty\");\n@@ -1009,7 +1038,6 @@ public void testSplitLogFileFirstLineCorruptionLog()\n     assertEquals(1, fs.listStatus(corruptDir).length);\n   }\n \n-\n   private void flushToConsole(String s) {\n     System.out.println(s);\n     System.out.flush();",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3912 [Stargate] Columns not handle by Scan; fix NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1126556 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/60d7a81833369262e9b21af31d88ed9c81398f42",
        "parent": "https://github.com/apache/hbase/commit/981f414cd3b6ef7c1b846e139977c06ff781802f",
        "bug_id": "hbase_202",
        "file": [
            {
                "sha": "7768612dceb9633478b047eec78c784e2a384092",
                "filename": "src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "blob_url": "https://github.com/apache/hbase/blob/60d7a81833369262e9b21af31d88ed9c81398f42/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "raw_url": "https://github.com/apache/hbase/raw/60d7a81833369262e9b21af31d88ed9c81398f42/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java?ref=60d7a81833369262e9b21af31d88ed9c81398f42",
                "patch": "@@ -356,8 +356,12 @@ public static ScannerModel fromScan(Scan scan) throws Exception {\n     Map<byte [], NavigableSet<byte []>> families = scan.getFamilyMap();\n     if (families != null) {\n       for (Map.Entry<byte [], NavigableSet<byte []>> entry : families.entrySet()) {\n-        for (byte[] qualifier : entry.getValue()) {\n-          model.addColumn(Bytes.add(entry.getKey(), COLUMN_DIVIDER, qualifier));\n+        if (entry.getValue() != null) {\n+          for (byte[] qualifier: entry.getValue()) {\n+            model.addColumn(Bytes.add(entry.getKey(), COLUMN_DIVIDER, qualifier));\n+          }\n+        } else {\n+          model.addColumn(entry.getKey());\n         }\n       }\n     }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3790  Fix NPE in ExecResult.write() with null return value\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1092854 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a195d0af7eeb54d12588aa3b51baa053a830c70e",
        "parent": "https://github.com/apache/hbase/commit/cac85a9833bd640ff63fa0b9dcb322fe20e00313",
        "bug_id": "hbase_203",
        "file": [
            {
                "sha": "714f4b075c24d031edfc6ac456132c66600dde3e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/a195d0af7eeb54d12588aa3b51baa053a830c70e/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/a195d0af7eeb54d12588aa3b51baa053a830c70e/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=a195d0af7eeb54d12588aa3b51baa053a830c70e",
                "patch": "@@ -75,6 +75,7 @@ Release 0.91.0 - Unreleased\n    HBASE-3598  Broken formatting in LRU stats output (Erik Onnen)\n    HBASE-3758  Delete triggers pre/postScannerOpen upcalls of RegionObserver\n                (Mingjie Lai via garyh)\n+   HBASE-3790  Fix NPE in ExecResult.write() with null return value\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "eb107ed0a77bb0eb12c87e173c784f28278398ff",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java",
                "blob_url": "https://github.com/apache/hbase/blob/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java",
                "raw_url": "https://github.com/apache/hbase/raw/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java?ref=a195d0af7eeb54d12588aa3b51baa053a830c70e",
                "patch": "@@ -72,7 +72,7 @@ public Object getValue() {\n   public void write(DataOutput out) throws IOException {\n     Bytes.writeByteArray(out, regionName);\n     HbaseObjectWritable.writeObject(out, value,\n-        value.getClass(), null);\n+        value != null ? value.getClass() : valueType, null);\n     Class<?> alternativeSerializationClass;\n     if(value instanceof Writable){\n       alternativeSerializationClass = Writable.class;",
                "deletions": 1
            },
            {
                "sha": "f522bdb229ef1f729de5a11f3a2dafae27575191",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "blob_url": "https://github.com/apache/hbase/blob/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "raw_url": "https://github.com/apache/hbase/raw/a195d0af7eeb54d12588aa3b51baa053a830c70e/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "status": "modified",
                "changes": 32,
                "additions": 25,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java?ref=a195d0af7eeb54d12588aa3b51baa053a830c70e",
                "patch": "@@ -1,5 +1,5 @@\n /*\n- * Copyright 2010 The Apache Software Foundation\n+ * Copyright 2011 The Apache Software Foundation\n  *\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n@@ -19,10 +19,7 @@\n  */\n package org.apache.hadoop.hbase.regionserver;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n \n import java.io.IOException;\n import java.util.List;\n@@ -83,6 +80,8 @@ public int incrementCount(int diff) {\n     public String hello(String name) {\n       if (name == null) {\n         return \"Who are you?\";\n+      } else if (\"nobody\".equals(name)) {\n+        return null;\n       }\n       return \"Hello, \"+name;\n     }\n@@ -154,6 +153,8 @@ public void testSingleProxy() throws Exception {\n     assertEquals(\"Invalid custom protocol response\", \"Hello, George\", result);\n     result = pinger.hello(null);\n     assertEquals(\"Should handle NULL parameter\", \"Who are you?\", result);\n+    result = pinger.hello(\"nobody\");\n+    assertNull(result);\n     int cnt = pinger.getPingCount();\n     assertTrue(\"Count should be incremented\", cnt > 0);\n     int newcnt = pinger.incrementCount(5);\n@@ -297,6 +298,23 @@ public String call(PingProtocol instance) {\n     verifyRegionResults(table, results, \"Who are you?\", ROW_C);\n   }\n \n+  @Test\n+  public void testNullReturn() throws Throwable {\n+    HTable table = new HTable(util.getConfiguration(), TEST_TABLE);\n+\n+    Map<byte[],String> results = table.coprocessorExec(PingProtocol.class,\n+        ROW_A, ROW_C,\n+        new Batch.Call<PingProtocol,String>(){\n+          public String call(PingProtocol instance) {\n+            return instance.hello(\"nobody\");\n+          }\n+        });\n+\n+    verifyRegionResults(table, results, null, ROW_A);\n+    verifyRegionResults(table, results, null, ROW_B);\n+    verifyRegionResults(table, results, null, ROW_C);\n+  }\n+\n   private void verifyRegionResults(HTable table,\n       Map<byte[],String> results, byte[] row) throws Exception {\n     verifyRegionResults(table, results, \"pong\", row);\n@@ -307,9 +325,9 @@ private void verifyRegionResults(HTable table,\n   throws Exception {\n     HRegionLocation loc = table.getRegionLocation(row);\n     byte[] region = loc.getRegionInfo().getRegionName();\n-    assertNotNull(\"Results should contain region \" +\n+    assertTrue(\"Results should contain region \" +\n         Bytes.toStringBinary(region)+\" for row '\"+Bytes.toStringBinary(row)+\"'\",\n-        results.get(region));\n+        results.containsKey(region));\n     assertEquals(\"Invalid result for row '\"+Bytes.toStringBinary(row)+\"'\",\n         expected, results.get(region));\n   }",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3728 NPE in HTablePool.closeTablePool\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1088692 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/44309ae90dfc011f3d49cbc94284c715fcba5416",
        "parent": "https://github.com/apache/hbase/commit/1e6b64f3af4e8684be366204695c03fa38ee07b8",
        "bug_id": "hbase_204",
        "file": [
            {
                "sha": "77573caafc426d7a361254a42ccd35be06745494",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/44309ae90dfc011f3d49cbc94284c715fcba5416/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/44309ae90dfc011f3d49cbc94284c715fcba5416/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=44309ae90dfc011f3d49cbc94284c715fcba5416",
                "patch": "@@ -60,6 +60,7 @@ Release 0.91.0 - Unreleased\n                (Ted Yu via Stack)\n    HBASE-3238  HBase needs to have the CREATE permission on the parent of its\n                ZooKeeper parent znode (Alex Newman via Stack)\n+   HBASE-3728  NPE in HTablePool.closeTablePool (Ted Yu via Stack)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "a1c1d87ac307bb1fe43ecf079c372728a59759c2",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HTablePool.java",
                "blob_url": "https://github.com/apache/hbase/blob/44309ae90dfc011f3d49cbc94284c715fcba5416/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java",
                "raw_url": "https://github.com/apache/hbase/raw/44309ae90dfc011f3d49cbc94284c715fcba5416/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java",
                "status": "modified",
                "changes": 10,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java?ref=44309ae90dfc011f3d49cbc94284c715fcba5416",
                "patch": "@@ -137,10 +137,12 @@ protected HTableInterface createHTable(String tableName) {\n    */\n   public void closeTablePool(final String tableName)  {\n     Queue<HTableInterface> queue = tables.get(tableName);\n-    HTableInterface table = queue.poll();\n-    while (table != null) {\n-      this.tableFactory.releaseHTableInterface(table);\n-      table = queue.poll();\n+    if (queue != null) {\n+      HTableInterface table = queue.poll();\n+      while (table != null) {\n+        this.tableFactory.releaseHTableInterface(table);\n+        table = queue.poll();\n+      }\n     }\n     HConnectionManager.deleteConnection(this.config, true);\n   }",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3702  Fix NPE in Exec method parameter serialization\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085818 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
        "parent": "https://github.com/apache/hbase/commit/561deab8fc7b46e65c96743dac285dce67f52323",
        "bug_id": "hbase_205",
        "file": [
            {
                "sha": "2f062db150fe04bfcffaddb0224be67a7da2cd87",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
                "patch": "@@ -50,6 +50,7 @@ Release 0.91.0 - Unreleased\n                when HRegionInterface#get is invoked (Mingjie Lai via\n                Andrew Purtell)\n    HBASE-3688  Setters of class HTableDescriptor do not work properly\n+   HBASE-3702  Fix NPE in Exec method parameter serialization\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "504bd77980564077fd31eeea73f331e89720deb0",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java",
                "blob_url": "https://github.com/apache/hbase/blob/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java",
                "raw_url": "https://github.com/apache/hbase/raw/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java?ref=70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
                "patch": "@@ -88,8 +88,9 @@ public void write(DataOutput out) throws IOException {\n     out.writeUTF(this.methodName);\n     out.writeInt(parameterClasses.length);\n     for (int i = 0; i < parameterClasses.length; i++) {\n-      HbaseObjectWritable.writeObject(out, parameters[i], parameters[i].getClass(),\n-                                 conf);\n+      HbaseObjectWritable.writeObject(out, parameters[i],\n+          parameters[i] != null ? parameters[i].getClass() : parameterClasses[i],\n+          conf);\n       out.writeUTF(parameterClasses[i].getName());\n     }\n     // fields for Exec",
                "deletions": 2
            },
            {
                "sha": "3ca03aa27a3fd889ed2e1e482c9f86a432fe1275",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "blob_url": "https://github.com/apache/hbase/blob/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "raw_url": "https://github.com/apache/hbase/raw/70379d17b3cbcdfb83ba14b4ebd115e03b85b226/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java",
                "status": "modified",
                "changes": 22,
                "additions": 22,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java?ref=70379d17b3cbcdfb83ba14b4ebd115e03b85b226",
                "patch": "@@ -81,6 +81,9 @@ public int incrementCount(int diff) {\n \n     @Override\n     public String hello(String name) {\n+      if (name == null) {\n+        return \"Who are you?\";\n+      }\n       return \"Hello, \"+name;\n     }\n \n@@ -149,6 +152,8 @@ public void testSingleProxy() throws Exception {\n     assertEquals(\"Invalid custom protocol response\", \"pong\", result);\n     result = pinger.hello(\"George\");\n     assertEquals(\"Invalid custom protocol response\", \"Hello, George\", result);\n+    result = pinger.hello(null);\n+    assertEquals(\"Should handle NULL parameter\", \"Who are you?\", result);\n     int cnt = pinger.getPingCount();\n     assertTrue(\"Count should be incremented\", cnt > 0);\n     int newcnt = pinger.incrementCount(5);\n@@ -275,6 +280,23 @@ public String call(PingProtocol instance) {\n     verifyRegionResults(table, results, \"Hello, pong\", ROW_C);\n   }\n \n+  @Test\n+  public void testNullCall() throws Throwable {\n+    HTable table = new HTable(util.getConfiguration(), TEST_TABLE);\n+\n+    Map<byte[],String> results = table.coprocessorExec(PingProtocol.class,\n+        ROW_A, ROW_C,\n+        new Batch.Call<PingProtocol,String>() {\n+          public String call(PingProtocol instance) {\n+            return instance.hello(null);\n+          }\n+        });\n+\n+    verifyRegionResults(table, results, \"Who are you?\", ROW_A);\n+    verifyRegionResults(table, results, \"Who are you?\", ROW_B);\n+    verifyRegionResults(table, results, \"Who are you?\", ROW_C);\n+  }\n+\n   private void verifyRegionResults(HTable table,\n       Map<byte[],String> results, byte[] row) throws Exception {\n     verifyRegionResults(table, results, \"pong\", row);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3627 NPE in EventHandler when region already reassigned\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085076 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/89794648e6e8a2588e0748fb1b6770e37816f008",
        "parent": "https://github.com/apache/hbase/commit/cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
        "bug_id": "hbase_206",
        "file": [
            {
                "sha": "d1607cd275aa8f08acf623cf4d00a300c64e2ca0",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/89794648e6e8a2588e0748fb1b6770e37816f008/src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/89794648e6e8a2588e0748fb1b6770e37816f008/src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java",
                "status": "added",
                "changes": 227,
                "additions": 227,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java?ref=89794648e6e8a2588e0748fb1b6770e37816f008",
                "patch": "@@ -0,0 +1,227 @@\n+/**\n+ * Copyright 2011 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.regionserver.handler;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HServerInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.ZooKeeperConnectionException;\n+import org.apache.hadoop.hbase.catalog.CatalogTracker;\n+import org.apache.hadoop.hbase.ipc.HBaseRpcMetrics;\n+import org.apache.hadoop.hbase.regionserver.CompactionRequestor;\n+import org.apache.hadoop.hbase.regionserver.FlushRequester;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.regionserver.RegionServerServices;\n+import org.apache.hadoop.hbase.regionserver.wal.HLog;\n+import org.apache.hadoop.hbase.zookeeper.ZKAssign;\n+import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.KeeperException.NodeExistsException;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Test of the {@link OpenRegionHandler}.\n+ */\n+public class TestOpenRegionHandler {\n+  private static final Log LOG = LogFactory.getLog(TestOpenRegionHandler.class);\n+  private final static HBaseTestingUtility HTU = new HBaseTestingUtility();\n+\n+  @BeforeClass public static void before() throws Exception {\n+    HTU.startMiniZKCluster();\n+  }\n+\n+  @AfterClass public static void after() throws IOException {\n+    HTU.shutdownMiniZKCluster();\n+  }\n+\n+  /**\n+   * Basic mock Server\n+   */\n+  static class MockServer implements Server {\n+    boolean stopped = false;\n+    final static String NAME = \"MockServer\";\n+    final ZooKeeperWatcher zk;\n+\n+    MockServer() throws ZooKeeperConnectionException, IOException {\n+      this.zk =  new ZooKeeperWatcher(HTU.getConfiguration(), NAME, this);\n+    }\n+\n+    @Override\n+    public void abort(String why, Throwable e) {\n+      LOG.fatal(\"Abort why=\" + why, e);\n+      this.stopped = true;\n+    }\n+\n+    @Override\n+    public void stop(String why) {\n+      LOG.debug(\"Stop why=\" + why);\n+      this.stopped = true;\n+    }\n+\n+    @Override\n+    public boolean isStopped() {\n+      return this.stopped;\n+    }\n+\n+    @Override\n+    public Configuration getConfiguration() {\n+      return HTU.getConfiguration();\n+    }\n+\n+    @Override\n+    public ZooKeeperWatcher getZooKeeper() {\n+      return this.zk;\n+    }\n+\n+    @Override\n+    public CatalogTracker getCatalogTracker() {\n+      // TODO Auto-generated method stub\n+      return null;\n+    }\n+\n+    @Override\n+    public String getServerName() {\n+      return NAME;\n+    }\n+  }\n+\n+  /**\n+   * Basic mock region server services.\n+   */\n+  static class MockRegionServerServices implements RegionServerServices {\n+    final Map<String, HRegion> regions = new HashMap<String, HRegion>();\n+    boolean stopping = false;\n+\n+    @Override\n+    public boolean removeFromOnlineRegions(String encodedRegionName) {\n+      return this.regions.remove(encodedRegionName) != null;\n+    }\n+    \n+    @Override\n+    public HRegion getFromOnlineRegions(String encodedRegionName) {\n+      return this.regions.get(encodedRegionName);\n+    }\n+    \n+    @Override\n+    public void addToOnlineRegions(HRegion r) {\n+      this.regions.put(r.getRegionInfo().getEncodedName(), r);\n+    }\n+    \n+    @Override\n+    public void postOpenDeployTasks(HRegion r, CatalogTracker ct, boolean daughter)\n+        throws KeeperException, IOException {\n+    }\n+    \n+    @Override\n+    public boolean isStopping() {\n+      return this.stopping;\n+    }\n+    \n+    @Override\n+    public HLog getWAL() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public HServerInfo getServerInfo() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public HBaseRpcMetrics getRpcMetrics() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public FlushRequester getFlushRequester() {\n+      return null;\n+    }\n+    \n+    @Override\n+    public CompactionRequestor getCompactionRequester() {\n+      return null;\n+    }\n+\n+    @Override\n+    public CatalogTracker getCatalogTracker() {\n+      return null;\n+    }\n+\n+    @Override\n+    public ZooKeeperWatcher getZooKeeperWatcher() {\n+      return null;\n+    }\n+  };\n+\n+  /**\n+   * Test the openregionhandler can deal with its znode being yanked out from\n+   * under it.\n+   * @see <a href=\"https://issues.apache.org/jira/browse/HBASE-3627\">HBASE-3627</a>\n+   * @throws IOException\n+   * @throws NodeExistsException\n+   * @throws KeeperException\n+   */\n+  @Test public void testOpenRegionHandlerYankingRegionFromUnderIt()\n+  throws IOException, NodeExistsException, KeeperException {\n+    final Server server = new MockServer();\n+    final RegionServerServices rss = new MockRegionServerServices();\n+\n+    HTableDescriptor htd =\n+      new HTableDescriptor(\"testOpenRegionHandlerYankingRegionFromUnderIt\");\n+    final HRegionInfo hri =\n+      new HRegionInfo(htd, HConstants.EMPTY_END_ROW, HConstants.EMPTY_END_ROW);\n+    OpenRegionHandler handler = new OpenRegionHandler(server, rss, hri) {\n+      HRegion openRegion() {\n+        // Open region first, then remove znode as though it'd been hijacked.\n+        HRegion region = super.openRegion();\n+        // Don't actually open region BUT remove the znode as though it'd\n+        // been hijacked on us.\n+        ZooKeeperWatcher zkw = this.server.getZooKeeper();\n+        String node = ZKAssign.getNodeName(zkw, hri.getEncodedName());\n+        try {\n+          ZKUtil.deleteNodeFailSilent(zkw, node);\n+        } catch (KeeperException e) {\n+          throw new RuntimeException(\"Ugh failed delete of \" + node, e);\n+        }\n+        return region;\n+      }\n+    };\n+    // Call process without first creating OFFLINE region in zk, see if\n+    // exception or just quiet return (expected).\n+    handler.process();\n+    ZKAssign.createNodeOffline(server.getZooKeeper(), hri, server.getServerName());\n+    // Call process again but this time yank the zk znode out from under it\n+    // post OPENING; again will expect it to come back w/o NPE or exception.\n+    handler.process();\n+  }\n+}\n\\ No newline at end of file",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3627 NPE in EventHandler when region already reassigned\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1085075 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
        "parent": "https://github.com/apache/hbase/commit/237bc82af2992a82d0f71648f68311c4dab25f3e",
        "bug_id": "hbase_207",
        "file": [
            {
                "sha": "5beea27f13e9ef0c15545e94f587b59ab32f0cc3",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -168,6 +168,7 @@ Release 0.90.2 - Unreleased\n    HBASE-3617  NoRouteToHostException during balancing will cause Master abort\n                (Ted Yu via Stack)\n    HBASE-3668  CatalogTracker.waitForMeta can wait forever and totally stall a RS\n+   HBASE-3627  NPE in EventHandler when region already reassigned\n \n   IMPROVEMENTS\n    HBASE-3542  MultiGet methods in Thrift",
                "deletions": 0
            },
            {
                "sha": "be311797c81a8894d9280cad6b77c618bedff118",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -23,6 +23,7 @@\n import java.io.IOException;\n import java.net.ConnectException;\n import java.net.SocketTimeoutException;\n+import java.net.SocketException;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.commons.logging.Log;\n@@ -390,8 +391,11 @@ private HRegionInterface getCachedConnection(HServerAddress address)\n         throw e;\n       }\n     } catch (SocketTimeoutException e) {\n-      // We were passed the wrong address.  Return 'protocol' == null.\n+      // Return 'protocol' == null.\n       LOG.debug(\"Timed out connecting to \" + address);\n+    } catch (SocketException e) {\n+      // Return 'protocol' == null.\n+      LOG.debug(\"Exception connecting to \" + address);\n     } catch (IOException ioe) {\n       Throwable cause = ioe.getCause();\n       if (cause != null && cause instanceof EOFException) {",
                "deletions": 1
            },
            {
                "sha": "6e22cf5e44997f7b258d4666a6c3d78f262d135f",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "status": "modified",
                "changes": 11,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -308,14 +308,9 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n     } catch (java.net.SocketTimeoutException e) {\n       // Treat this exception + message as unavailable catalog table. Catch it\n       // and fall through to return a null\n-    } catch (java.net.ConnectException e) {\n-      if (e.getMessage() != null &&\n-          e.getMessage().contains(\"Connection refused\")) {\n-        // Treat this exception + message as unavailable catalog table. Catch it\n-        // and fall through to return a null\n-      } else {\n-        throw e;\n-      }\n+    } catch (java.net.SocketException e) {\n+      // Treat this exception + message as unavailable catalog table. Catch it\n+      // and fall through to return a null\n     } catch (RemoteException re) {\n       IOException ioe = re.unwrapRemoteException();\n       if (ioe instanceof NotServingRegionException) {",
                "deletions": 8
            },
            {
                "sha": "e9b2af27b6a8c96656079194719260f8ca06b037",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -1770,6 +1770,10 @@ protected void chore() {\n                   Stat stat = new Stat();\n                   RegionTransitionData data = ZKAssign.getDataNoWatch(watcher,\n                       node, stat);\n+                  if (data == null) {\n+                    LOG.warn(\"Data is null, node \" + node + \" no longer exists\");\n+                    break;\n+                  }\n                   if (data.getEventType() == EventType.RS_ZK_REGION_OPENED) {\n                     LOG.debug(\"Region has transitioned to OPENED, allowing \" +\n                         \"watched event handlers to process\");",
                "deletions": 0
            },
            {
                "sha": "441b48419e4b85f91a73e3b3eb54c0d96268a41b",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -87,7 +87,11 @@ public void process() throws IOException {\n \n     // If fails, just return.  Someone stole the region from under us.\n     // Calling transitionZookeeperOfflineToOpening initalizes this.version.\n-    if (!transitionZookeeperOfflineToOpening(encodedName)) return;\n+    if (!transitionZookeeperOfflineToOpening(encodedName)) {\n+      LOG.warn(\"Region was hijacked? It no longer exists, encodedName=\" +\n+        encodedName);\n+      return;\n+    }\n \n     // Open region.  After a successful open, failures in subsequent processing\n     // needs to do a close as part of cleanup.\n@@ -254,7 +258,7 @@ private boolean transitionToOpened(final HRegion r) throws IOException {\n   /**\n    * @return Instance of HRegion if successful open else null.\n    */\n-  private HRegion openRegion() {\n+  HRegion openRegion() {\n     HRegion region = null;\n     try {\n       // Instantiate the region.  This also periodically tickles our zk OPENING",
                "deletions": 2
            },
            {
                "sha": "34e17b60638fc698d4bbfe1e49a09312719cd4b9",
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java",
                "status": "modified",
                "changes": 14,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -396,7 +396,8 @@ public static boolean deleteNode(ZooKeeperWatcher zkw, String regionName,\n     zkw.sync(node);\n     Stat stat = new Stat();\n     byte [] bytes = ZKUtil.getDataNoWatch(zkw, node, stat);\n-    if(bytes == null) {\n+    if (bytes == null) {\n+      // If it came back null, node does not exist.\n       throw KeeperException.create(Code.NONODE);\n     }\n     RegionTransitionData data = RegionTransitionData.fromBytes(bytes);\n@@ -674,8 +675,11 @@ public static int transitionNode(ZooKeeperWatcher zkw, HRegionInfo region,\n \n     // Read existing data of the node\n     Stat stat = new Stat();\n-    byte [] existingBytes =\n-      ZKUtil.getDataNoWatch(zkw, node, stat);\n+    byte [] existingBytes = ZKUtil.getDataNoWatch(zkw, node, stat);\n+    if (existingBytes == null) {\n+      // Node no longer exists.  Return -1. It means unsuccessful transition.\n+      return -1;\n+    }\n     RegionTransitionData existingData =\n       RegionTransitionData.fromBytes(existingBytes);\n \n@@ -762,7 +766,7 @@ public static RegionTransitionData getData(ZooKeeperWatcher zkw,\n    * @param zkw zk reference\n    * @param pathOrRegionName fully-specified path or region name\n    * @param stat object to store node info into on getData call\n-   * @return data for the unassigned node\n+   * @return data for the unassigned node or null if node does not exist\n    * @throws KeeperException if unexpected zookeeper exception\n    */\n   public static RegionTransitionData getDataNoWatch(ZooKeeperWatcher zkw,\n@@ -771,7 +775,7 @@ public static RegionTransitionData getDataNoWatch(ZooKeeperWatcher zkw,\n     String node = pathOrRegionName.startsWith(\"/\") ?\n         pathOrRegionName : getNodeName(zkw, pathOrRegionName);\n     byte [] data = ZKUtil.getDataNoWatch(zkw, node, stat);\n-    if(data == null) {\n+    if (data == null) {\n       return null;\n     }\n     return RegionTransitionData.fromBytes(data);",
                "deletions": 5
            },
            {
                "sha": "08748f810272dbcc582ed381d1979fa33b238f08",
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/cf3284dfb9b7b081193eb35297e2cf5920aa97e3/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java?ref=cf3284dfb9b7b081193eb35297e2cf5920aa97e3",
                "patch": "@@ -576,7 +576,7 @@ public static int getNumberOfChildren(ZooKeeperWatcher zkw, String znode)\n    * @param zkw zk reference\n    * @param znode path of node\n    * @param stat node status to set if node exists\n-   * @return data of the specified znode, or null if does not exist\n+   * @return data of the specified znode, or null if node does not exist\n    * @throws KeeperException if unexpected zookeeper exception\n    */\n   public static byte [] getDataNoWatch(ZooKeeperWatcher zkw, String znode,",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3613  NPE in MemStoreFlusher\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1081986 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/f3134976f41988256f83ea168bc52f564a1c28a7",
        "parent": "https://github.com/apache/hbase/commit/27c1c071af5a526a2947c0e0111f8fb27d94ec6c",
        "bug_id": "hbase_208",
        "file": [
            {
                "sha": "32596efc8e9efd3ba40713928b6e3fd06fa50b0f",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/f3134976f41988256f83ea168bc52f564a1c28a7/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/f3134976f41988256f83ea168bc52f564a1c28a7/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=f3134976f41988256f83ea168bc52f564a1c28a7",
                "patch": "@@ -165,6 +165,7 @@ Release 0.90.2 - Unreleased\n                bloomfilter (Liyin Tang via Stack)\n    HBASE-3639  FSUtils.getRootDir should qualify path\n    HBASE-3648  [replication] failover is sloppy with znodes\n+   HBASE-3613  NPE in MemStoreFlusher\n \n   IMPROVEMENTS\n    HBASE-3542  MultiGet methods in Thrift",
                "deletions": 0
            },
            {
                "sha": "099ff4beacaf6b7a0a7a2294f641af2aeb8426fd",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java",
                "blob_url": "https://github.com/apache/hbase/blob/f3134976f41988256f83ea168bc52f564a1c28a7/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java",
                "raw_url": "https://github.com/apache/hbase/raw/f3134976f41988256f83ea168bc52f564a1c28a7/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java",
                "status": "modified",
                "changes": 14,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java?ref=f3134976f41988256f83ea168bc52f564a1c28a7",
                "patch": "@@ -159,19 +159,21 @@ private boolean flushOneForGlobalPressure() {\n     boolean flushedOne = false;\n     while (!flushedOne) {\n       // Find the biggest region that doesn't have too many storefiles\n+      // (might be null!)\n       HRegion bestFlushableRegion = getBiggestMemstoreRegion(\n           regionsBySize, excludedRegionNames, true);\n       // Find the biggest region, total, even if it might have too many flushes.\n       HRegion bestAnyRegion = getBiggestMemstoreRegion(\n           regionsBySize, excludedRegionNames, false);\n \n       if (bestAnyRegion == null) {\n-        LOG.fatal(\"Above memory mark but there are no flushable regions!\");\n+        LOG.error(\"Above memory mark but there are no flushable regions!\");\n         return false;\n       }\n \n       HRegion regionToFlush;\n-      if (bestAnyRegion.memstoreSize.get() > 2 * bestFlushableRegion.memstoreSize.get()) {\n+      if (bestFlushableRegion != null &&\n+\t  bestAnyRegion.memstoreSize.get() > 2 * bestFlushableRegion.memstoreSize.get()) {\n         // Even if it's not supposed to be flushed, pick a region if it's more than twice\n         // as big as the best flushable one - otherwise when we're under pressure we make\n         // lots of little flushes and cause lots of compactions, etc, which just makes\n@@ -183,9 +185,13 @@ private boolean flushOneForGlobalPressure() {\n             \" vs best flushable region's \" +\n             StringUtils.humanReadableInt(bestFlushableRegion.memstoreSize.get()) +\n             \". Choosing the bigger.\");\n-        regionToFlush = bestAnyRegion;\n+\tregionToFlush = bestAnyRegion;\n       } else {\n-        regionToFlush = bestFlushableRegion;\n+\t  if (bestFlushableRegion == null) {\n+\t      regionToFlush = bestAnyRegion;\n+\t  } else {\n+\t      regionToFlush = bestFlushableRegion;\n+\t  }\n       }\n \n       Preconditions.checkState(regionToFlush.memstoreSize.get() > 0);",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3524  NPE from CompactionChecker\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1070033 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/7901f641d58c1304f8fcf10ed45c0ee52bc566cf",
        "parent": "https://github.com/apache/hbase/commit/5311df528e2a6cd4a521c83560a908d3eb006890",
        "bug_id": "hbase_209",
        "file": [
            {
                "sha": "21236c4ad7391f24cae7f690448f412f7426e71e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/7901f641d58c1304f8fcf10ed45c0ee52bc566cf/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/7901f641d58c1304f8fcf10ed45c0ee52bc566cf/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=7901f641d58c1304f8fcf10ed45c0ee52bc566cf",
                "patch": "@@ -108,6 +108,7 @@ Release 0.90.1 - Unreleased\n   \n   BUG FIXES\n    HBASE-3483  Memstore lower limit should trigger asynchronous flushes\n+   HBASE-3524  NPE from CompactionChecker\n \n   IMPROVEMENTS\n    HBASE-3508  LruBlockCache statistics thread should have a name",
                "deletions": 0
            },
            {
                "sha": "a7e2720f9e98949c4dc79f6f4023f0ea59cb65b5",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "blob_url": "https://github.com/apache/hbase/blob/7901f641d58c1304f8fcf10ed45c0ee52bc566cf/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "raw_url": "https://github.com/apache/hbase/raw/7901f641d58c1304f8fcf10ed45c0ee52bc566cf/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=7901f641d58c1304f8fcf10ed45c0ee52bc566cf",
                "patch": "@@ -788,7 +788,10 @@ private boolean isMajorCompaction(final List<StoreFile> filesToCompact) throws I\n       if (filesToCompact.size() == 1) {\n         // Single file\n         StoreFile sf = filesToCompact.get(0);\n-        long oldest = now - sf.getReader().timeRangeTracker.minimumTimestamp;\n+        long oldest =\n+            (sf.getReader().timeRangeTracker == null) ?\n+                Long.MIN_VALUE :\n+                now - sf.getReader().timeRangeTracker.minimumTimestamp;\n         if (sf.isMajorCompaction() &&\n             (this.ttl == HConstants.FOREVER || oldest < this.ttl)) {\n           if (LOG.isDebugEnabled()) {",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3492  NPE while splitting table with empty column family store\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1066785 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d7a63983fc079da951290c3a0212e8fe03c35417",
        "parent": "https://github.com/apache/hbase/commit/a3a2ce921508da46e3e8db9550cf281683fadb6c",
        "bug_id": "hbase_210",
        "file": [
            {
                "sha": "6eeabc601b5fa5a7de11406ba59999f1882efbe7",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/d7a63983fc079da951290c3a0212e8fe03c35417/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/d7a63983fc079da951290c3a0212e8fe03c35417/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d7a63983fc079da951290c3a0212e8fe03c35417",
                "patch": "@@ -38,6 +38,7 @@ Release 0.91.0 - Unreleased\n                row are the same\n    HBASE-3416  For intra-row scanning, the update readers notification resets\n                the query matcher and can lead to incorrect behavior\n+   HBASE-3492  NPE while splitting table with empty column family store\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "e13cf59375f59c7fe2191fe3e58dfaf03cbf6784",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "blob_url": "https://github.com/apache/hbase/blob/d7a63983fc079da951290c3a0212e8fe03c35417/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "raw_url": "https://github.com/apache/hbase/raw/d7a63983fc079da951290c3a0212e8fe03c35417/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "status": "modified",
                "changes": 25,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=d7a63983fc079da951290c3a0212e8fe03c35417",
                "patch": "@@ -725,17 +725,17 @@ private boolean hasReferences(Collection<StoreFile> files) {\n    * @param dir\n    * @throws IOException\n    */\n-  private static long getLowestTimestamp(FileSystem fs, \n+  private static long getLowestTimestamp(FileSystem fs,\n       final List<StoreFile> candidates) throws IOException {\n     long minTs = Long.MAX_VALUE;\n     if (candidates.isEmpty()) {\n-      return minTs; \n+      return minTs;\n     }\n     Path[] p = new Path[candidates.size()];\n     for (int i = 0; i < candidates.size(); ++i) {\n       p[i] = candidates.get(i).getPath();\n     }\n-    \n+\n     FileStatus[] stats = fs.listStatus(p);\n     if (stats == null || stats.length == 0) {\n       return minTs;\n@@ -756,13 +756,13 @@ boolean isMajorCompaction() throws IOException {\n         return false;\n       }\n     }\n-    \n+\n     List<StoreFile> candidates = new ArrayList<StoreFile>(this.storefiles);\n \n     // exclude files above the max compaction threshold\n     // except: save all references. we MUST compact them\n     int pos = 0;\n-    while (pos < candidates.size() && \n+    while (pos < candidates.size() &&\n            candidates.get(pos).getReader().length() > this.maxCompactSize &&\n            !candidates.get(pos).isReference()) ++pos;\n     candidates.subList(0, pos).clear();\n@@ -868,7 +868,7 @@ long getNextMajorCompactTime() {\n       // do not compact old files above a configurable threshold\n       // save all references. we MUST compact them\n       int pos = 0;\n-      while (pos < filesToCompact.size() && \n+      while (pos < filesToCompact.size() &&\n              filesToCompact.get(pos).getReader().length() > maxCompactSize &&\n              !filesToCompact.get(pos).isReference()) ++pos;\n       filesToCompact.subList(0, pos).clear();\n@@ -878,7 +878,7 @@ long getNextMajorCompactTime() {\n       LOG.debug(this.storeNameStr + \": no store files to compact\");\n       return filesToCompact;\n     }\n-    \n+\n     // major compact on user action or age (caveat: we have too many files)\n     boolean majorcompaction = (forcemajor || isMajorCompaction(filesToCompact))\n       && filesToCompact.size() < this.maxFilesToCompact;\n@@ -891,7 +891,7 @@ long getNextMajorCompactTime() {\n       int start = 0;\n       double r = this.compactRatio;\n \n-      /* TODO: add sorting + unit test back in when HBASE-2856 is fixed \n+      /* TODO: add sorting + unit test back in when HBASE-2856 is fixed\n       // Sort files by size to correct when normal skew is altered by bulk load.\n       Collections.sort(filesToCompact, StoreFile.Comparators.FILE_SIZE);\n        */\n@@ -1320,10 +1320,11 @@ StoreSize checkSplit(final boolean force) {\n     this.lock.readLock().lock();\n     try {\n       // sanity checks\n-      if (!force) {\n-        if (storeSize < this.desiredMaxFileSize || this.storefiles.isEmpty()) {\n-          return null;\n-        }\n+      if (this.storefiles.isEmpty()) {\n+        return null;\n+      }\n+      if (!force && storeSize < this.desiredMaxFileSize) {\n+        return null;\n       }\n \n       if (this.region.getRegionInfo().isMetaRegion()) {",
                "deletions": 12
            },
            {
                "sha": "988dfe1c1674b43e2b6c200f4af9b0a7d02141ec",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/d7a63983fc079da951290c3a0212e8fe03c35417/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/d7a63983fc079da951290c3a0212e8fe03c35417/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java",
                "status": "modified",
                "changes": 13,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java?ref=d7a63983fc079da951290c3a0212e8fe03c35417",
                "patch": "@@ -61,7 +61,7 @@\n import com.google.common.base.Joiner;\n \n /**\n- * Test class fosr the Store\n+ * Test class for the Store\n  */\n public class TestStore extends TestCase {\n   public static final Log LOG = LogFactory.getLog(TestStore.class);\n@@ -630,4 +630,15 @@ public void testMultipleTimestamps() throws IOException {\n     result = HBaseTestingUtility.getFromStoreFile(store, get);\n     assertTrue(result.size()==0);\n   }\n+\n+  /**\n+   * Test for HBASE-3492 - Test split on empty colfam (no store files).\n+   *\n+   * @throws IOException When the IO operations fail.\n+   */\n+  public void testSplitWithEmptyColFam() throws IOException {\n+    init(this.getName());\n+    assertNull(store.checkSplit(false));\n+    assertNull(store.checkSplit(true));\n+  }\n }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3388 NPE processRegionInTransition(AssignmentManager.java:264) doing rolling-restart.sh\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1052058 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/1758553f45e4a38e493f0cb5dee37e483c33905a",
        "parent": "https://github.com/apache/hbase/commit/bfe27f5764b6a9b1b23599e140b6c699bba572ed",
        "bug_id": "hbase_211",
        "file": [
            {
                "sha": "030ae6e4bed5625f79d3c5276e0bb2828ef839df",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/1758553f45e4a38e493f0cb5dee37e483c33905a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/1758553f45e4a38e493f0cb5dee37e483c33905a/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=1758553f45e4a38e493f0cb5dee37e483c33905a",
                "patch": "@@ -803,6 +803,8 @@ Release 0.90.0 - Unreleased\n    HBASE-3343  Server not shutting down after losing log lease\n    HBASE-3381  Interrupt of a region open comes across as a successful open\n    HBASE-3386  NPE in TableRecordReaderImpl.restart\n+   HBASE-3388  NPE processRegionInTransition(AssignmentManager.java:264)\n+               doing rolling-restart.sh\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "2b345fb2ff5007351702ceee493a4f9442240453",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/1758553f45e4a38e493f0cb5dee37e483c33905a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/1758553f45e4a38e493f0cb5dee37e483c33905a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 9,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=1758553f45e4a38e493f0cb5dee37e483c33905a",
                "patch": "@@ -261,8 +261,13 @@ boolean processRegionInTransition(final String encodedRegionName,\n   throws KeeperException, IOException {\n     RegionTransitionData data = ZKAssign.getData(watcher, encodedRegionName);\n     if (data == null) return false;\n-    HRegionInfo hri = (regionInfo != null)? regionInfo:\n-      MetaReader.getRegion(catalogTracker, data.getRegionName()).getFirst();\n+    HRegionInfo hri = regionInfo;\n+    if (hri == null) {\n+      Pair<HRegionInfo, HServerAddress> p =\n+        MetaReader.getRegion(catalogTracker, data.getRegionName());\n+      if (p == null) return false;\n+      hri = p.getFirst();\n+    }\n     processRegionsInTransition(data, hri);\n     return true;\n   }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3386  NPE in TableRecordReaderImpl.restart\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1052051 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/bfe27f5764b6a9b1b23599e140b6c699bba572ed",
        "parent": "https://github.com/apache/hbase/commit/94cc5237b0576d9f36f51b7142ef913cc032d9bc",
        "bug_id": "hbase_212",
        "file": [
            {
                "sha": "f73e9a409d5fcda41688c031400a385842c7105e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/bfe27f5764b6a9b1b23599e140b6c699bba572ed/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/bfe27f5764b6a9b1b23599e140b6c699bba572ed/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=bfe27f5764b6a9b1b23599e140b6c699bba572ed",
                "patch": "@@ -802,6 +802,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3374  Our jruby jar has *GPL jars in it; fix\n    HBASE-3343  Server not shutting down after losing log lease\n    HBASE-3381  Interrupt of a region open comes across as a successful open\n+   HBASE-3386  NPE in TableRecordReaderImpl.restart\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "c813c490a365c2445725af3c8122cd5dc7b329ca",
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/bfe27f5764b6a9b1b23599e140b6c699bba572ed/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/bfe27f5764b6a9b1b23599e140b6c699bba572ed/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=bfe27f5764b6a9b1b23599e140b6c699bba572ed",
                "patch": "@@ -94,6 +94,11 @@\n   public RecordReader<ImmutableBytesWritable, Result> createRecordReader(\n       InputSplit split, TaskAttemptContext context)\n   throws IOException {\n+    if (table == null) {\n+      throw new IOException(\"Cannot create a record reader because of a\" +\n+          \" previous error. Please look at the previous logs lines from\" +\n+          \" the task's full log for more details.\");\n+    }\n     TableSplit tSplit = (TableSplit) split;\n     TableRecordReader trr = this.tableRecordReader;\n     // if no table record reader was provided use default",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Fix possible NPE in assign\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1042885 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f",
        "parent": "https://github.com/apache/hbase/commit/d547429a1442e48d9a3f69faaf9298b817a29967",
        "bug_id": "hbase_213",
        "file": [
            {
                "sha": "c3fd667a1411dcfb4625507ca4b7fbfc0537efd6",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/ea3c445e2d73b2abcfb29f0e09c379551f86ee8f/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=ea3c445e2d73b2abcfb29f0e09c379551f86ee8f",
                "patch": "@@ -956,7 +956,7 @@ RegionPlan getRegionPlan(final RegionState state,\n     synchronized (this.regionPlans) {\n       existingPlan = this.regionPlans.get(encodedName);\n       if (existingPlan == null || forceNewPlan ||\n-          existingPlan.getDestination().equals(serverToExclude)) {\n+          (existingPlan != null && existingPlan.getDestination().equals(serverToExclude))) {\n         newPlan = true;\n         this.regionPlans.put(encodedName, randomPlan);\n       }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3224  NPE in KeyValue.compare when compacting (forgot a file)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1035129 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d8c6ed3be515198920394e48f01d643bccec6bf2",
        "parent": "https://github.com/apache/hbase/commit/069fecaeb7d128979a338e9cbbb4ad4a34726125",
        "bug_id": "hbase_214",
        "file": [
            {
                "sha": "47f4c3910573f2baf8497df28351a180dfd9994f",
                "filename": "src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/d8c6ed3be515198920394e48f01d643bccec6bf2/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/d8c6ed3be515198920394e48f01d643bccec6bf2/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java",
                "status": "added",
                "changes": 141,
                "additions": 141,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java?ref=d8c6ed3be515198920394e48f01d643bccec6bf2",
                "patch": "@@ -0,0 +1,141 @@\n+/*\n+ * Copyright 2010 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+\n+public class TestHalfStoreFileReader {\n+\n+  /**\n+   * Test the scanner and reseek of a half hfile scanner. The scanner API\n+   * demands that seekTo and reseekTo() only return < 0 if the key lies\n+   * before the start of the file (with no position on the scanner). Returning\n+   * 0 if perfect match (rare), and return > 1 if we got an imperfect match.\n+   *\n+   * The latter case being the most common, we should generally be returning 1,\n+   * and if we do, there may or may not be a 'next' in the scanner/file.\n+   *\n+   * A bug in the half file scanner was returning -1 at the end of the bottom\n+   * half, and that was causing the infrastructure above to go null causing NPEs\n+   * and other problems.  This test reproduces that failure, and also tests\n+   * both the bottom and top of the file while we are at it.\n+   *\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testHalfScanAndReseek() throws IOException {\n+    HBaseTestingUtility test_util = new HBaseTestingUtility();\n+    String root_dir = HBaseTestingUtility.getTestDir(\"TestHalfStoreFile\").toString();\n+    Path p = new Path(root_dir, \"test\");\n+\n+    FileSystem fs = FileSystem.get(test_util.getConfiguration());\n+\n+    HFile.Writer w = new HFile.Writer(fs, p, 1024, \"none\", KeyValue.KEY_COMPARATOR);\n+\n+    // write some things.\n+    List<KeyValue> items = genSomeKeys();\n+    for (KeyValue kv : items) {\n+      w.append(kv);\n+    }\n+    w.close();\n+\n+    HFile.Reader r = new HFile.Reader(fs, p, null, false);\n+    r.loadFileInfo();\n+    byte [] midkey = r.midkey();\n+    KeyValue midKV = KeyValue.createKeyValueFromKey(midkey);\n+    midkey = midKV.getRow();\n+\n+    //System.out.println(\"midkey: \" + midKV + \" or: \" + Bytes.toStringBinary(midkey));\n+\n+    Reference bottom = new Reference(midkey, Reference.Range.bottom);\n+    doTestOfScanAndReseek(p, fs, bottom);\n+\n+    Reference top = new Reference(midkey, Reference.Range.top);\n+    doTestOfScanAndReseek(p, fs, top);\n+  }\n+\n+  private void doTestOfScanAndReseek(Path p, FileSystem fs, Reference bottom)\n+      throws IOException {\n+    final HalfStoreFileReader halfreader =\n+        new HalfStoreFileReader(fs, p, null, bottom);\n+    halfreader.loadFileInfo();\n+    final HFileScanner scanner = halfreader.getScanner(false, false);\n+\n+    scanner.seekTo();\n+    KeyValue curr;\n+    do {\n+      curr = scanner.getKeyValue();\n+      KeyValue reseekKv =\n+          getLastOnCol(curr);\n+      int ret = scanner.reseekTo(reseekKv.getKey());\n+      assertTrue(\"reseek to returned: \" + ret, ret > 0);\n+      //System.out.println(curr + \": \" + ret);\n+    } while (scanner.next());\n+\n+    int ret = scanner.reseekTo(getLastOnCol(curr).getKey());\n+    //System.out.println(\"Last reseek: \" + ret);\n+    assertTrue( ret > 0 );\n+  }\n+\n+  private KeyValue getLastOnCol(KeyValue curr) {\n+    return KeyValue.createLastOnRow(\n+        curr.getBuffer(), curr.getRowOffset(), curr.getRowLength(),\n+        curr.getBuffer(), curr.getFamilyOffset(), curr.getFamilyLength(),\n+        curr.getBuffer(), curr.getQualifierOffset(), curr.getQualifierLength());\n+  }\n+\n+  static final int SIZE = 1000;\n+\n+  static byte[] _b(String s) {\n+    return Bytes.toBytes(s);\n+  }\n+\n+  List<KeyValue> genSomeKeys() {\n+    List<KeyValue> ret = new ArrayList<KeyValue>(SIZE);\n+    for (int i = 0 ; i < SIZE; i++) {\n+      KeyValue kv =\n+          new KeyValue(\n+              _b(String.format(\"row_%04d\", i)),\n+              _b(\"family\"),\n+              _b(\"qualifier\"),\n+              1000, // timestamp\n+              _b(\"value\"));\n+      ret.add(kv);\n+    }\n+    return ret;\n+  }\n+\n+\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3224  NPE in KeyValue$KVComparator.compare when compacting\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1034229 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/433f7a3ff6d7e18b44202cc950478cfac49daae0",
        "parent": "https://github.com/apache/hbase/commit/b5086781068232ace42d4e9687e2eb351a52cced",
        "bug_id": "hbase_215",
        "file": [
            {
                "sha": "b0d353359652589ca164f65a58ab90f0e1320941",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "patch": "@@ -677,6 +677,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3219  Split parents are reassigned on restart and on disable/enable\n    HBASE-3222  Regionserver region listing in UI is no longer ordered\n    HBASE-3221  Race between splitting and disabling\n+   HBASE-3224  NPE in KeyValue$KVComparator.compare when compacting\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "056520122e0afbed171032d8f770dd249ccc8949",
                "filename": "src/docbkx/book.xml",
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/docbkx/book.xml",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/docbkx/book.xml",
                "status": "modified",
                "changes": 9,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/docbkx/book.xml?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "patch": "@@ -508,6 +508,15 @@ index e70ebc6..96f8c27 100644\n       </section>\n \n       <section xml:id=\"recommended_configurations\"><title>Recommended Configuations</title>\n+      <section xml:id=\"big_memory\">\n+        <title>Configuration for large memory machines</title>\n+        <para>\n+          HBase ships with a reasonable configuration that will work on nearly all\n+          machine types that people might want to test with. If you have larger\n+          machines you might the following configuration options helpful.\n+        </para>\n+\n+      </section>\n       <section xml:id=\"lzo\">\n       <title>LZO compression</title>\n       <para>You should consider enabling LZO compression.  Its",
                "deletions": 0
            },
            {
                "sha": "40be64977075fbbe5856a09d6afd68a383876859",
                "filename": "src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/433f7a3ff6d7e18b44202cc950478cfac49daae0/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java?ref=433f7a3ff6d7e18b44202cc950478cfac49daae0",
                "patch": "@@ -229,6 +229,10 @@ public int reseekTo(byte[] key, int offset, int length)\n             return 1;\n           }\n         }\n+        if (atEnd) {\n+          // skip the 'reseek' and just return 1.\n+          return 1;\n+        }\n         return delegate.reseekTo(key, offset, length);\n       }\n ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3205  TableRecordReaderImpl.restart NPEs when first next is restarted\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1032761 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a68b2c6645aa92988c7d4feed3119a385be4f519",
        "parent": "https://github.com/apache/hbase/commit/160395c9f793dfa6458bde8bbb2973986e2fa3ff",
        "bug_id": "hbase_216",
        "file": [
            {
                "sha": "5f1b3c4fa161d10929743d252e7d708497611c5a",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/a68b2c6645aa92988c7d4feed3119a385be4f519/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/a68b2c6645aa92988c7d4feed3119a385be4f519/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=a68b2c6645aa92988c7d4feed3119a385be4f519",
                "patch": "@@ -657,6 +657,7 @@ Release 0.90.0 - Unreleased\n                and it'll hold up regionserver shutdown\n    HBASE-3204  Reenable deferred log flush\n    HBASE-3195  [rest] Fix TestTransform breakage on Hudson\n+   HBASE-3205  TableRecordReaderImpl.restart NPEs when first next is restarted\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "dbb11e858be2020c1262b451059b8b799f7b7e51",
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java",
                "blob_url": "https://github.com/apache/hbase/blob/a68b2c6645aa92988c7d4feed3119a385be4f519/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java",
                "raw_url": "https://github.com/apache/hbase/raw/a68b2c6645aa92988c7d4feed3119a385be4f519/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java",
                "status": "modified",
                "changes": 7,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java?ref=a68b2c6645aa92988c7d4feed3119a385be4f519",
                "patch": "@@ -132,6 +132,13 @@ public boolean nextKeyValue() throws IOException, InterruptedException {\n       value = this.scanner.next();\n     } catch (IOException e) {\n       LOG.debug(\"recovered from \" + StringUtils.stringifyException(e));\n+      if (lastRow == null) {\n+        LOG.warn(\"We are restarting the first next() invocation,\" +\n+            \" if your mapper's restarted a few other times like this\" +\n+            \" then you should consider killing this job and investigate\" +\n+            \" why it's taking so long.\");\n+        lastRow = scan.getStartRow();\n+      }\n       restart(lastRow);\n       scanner.next();    // skip presumed already mapped row\n       value = scanner.next();",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3151 NPE when trying to read regioninfo from .META.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1030293 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/34db492b4627298cde804a565f076ce1bb3fa2c6",
        "parent": "https://github.com/apache/hbase/commit/c767ca49d13d3a296cc4a1fa34e3003db87626bd",
        "bug_id": "hbase_217",
        "file": [
            {
                "sha": "381471c096018a191c3088661917b9c543b6759c",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "patch": "@@ -642,6 +642,7 @@ Release 0.21.0 - Unreleased\n    HBASE-3185  User-triggered compactions are triggering splits!\n    HBASE-1932  Encourage use of 'lzo' compression... add the wiki page to\n                getting started\n+   HBASE-3151  NPE when trying to read regioninfo from .META.\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "53bc9bef78aacb06d814ce126e57652abcf26d41",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "patch": "@@ -205,7 +205,7 @@ public static void deleteDaughterReferenceInParent(CatalogTracker catalogTracker\n     catalogTracker.waitForMetaServerConnectionDefault().\n       delete(CatalogTracker.META_REGION, delete);\n     LOG.info(\"Deleted daughter \" + daughter.getRegionNameAsString() +\n-      \" \" + Bytes.toString(qualifier) + \" from parent \" +\n+      \" \" + Bytes.toString(qualifier) + \" reference in parent \" +\n       parent.getRegionNameAsString());\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "8556595745dba52ef4743547fef1c34c318602e3",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "status": "modified",
                "changes": 11,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "patch": "@@ -136,6 +136,7 @@ private static boolean isMetaRegion(final byte [] regionName) {\n       public boolean visit(Result r) throws IOException {\n         if (r ==  null || r.isEmpty()) return true;\n         Pair<HRegionInfo,HServerAddress> region = metaRowToRegionPair(r);\n+        if (region == null) return true;\n         regions.put(region.getFirst(), region.getSecond());\n         return true;\n       }\n@@ -296,13 +297,16 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n   /**\n    * @param data A .META. table row.\n    * @return A pair of the regioninfo and the server address from <code>data</code>\n-   * (or null for server address if no address set in .META.).\n+   * or null for server address if no address set in .META. or null for a result\n+   * if no HRegionInfo found.\n    * @throws IOException\n    */\n   public static Pair<HRegionInfo, HServerAddress> metaRowToRegionPair(\n       Result data) throws IOException {\n-    HRegionInfo info = Writables.getHRegionInfo(\n-      data.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER));\n+    byte [] bytes =\n+      data.getValue(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);\n+    if (bytes == null) return null;\n+    HRegionInfo info = Writables.getHRegionInfo(bytes);\n     final byte[] value = data.getValue(HConstants.CATALOG_FAMILY,\n       HConstants.SERVER_QUALIFIER);\n     if (value != null && value.length > 0) {\n@@ -463,6 +467,7 @@ public static boolean tableExists(CatalogTracker catalogTracker,\n       while((data = metaServer.next(scannerid)) != null) {\n         if (data != null && data.size() > 0) {\n           Pair<HRegionInfo, HServerAddress> region = metaRowToRegionPair(data);\n+          if (region == null) continue;\n           if (region.getFirst().getTableDesc().getNameAsString().equals(\n               tableName)) {\n             regions.add(region);",
                "deletions": 3
            },
            {
                "sha": "03ac47df1d277faeb43b1e90f243afafe688972a",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "status": "modified",
                "changes": 46,
                "additions": 38,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "patch": "@@ -24,6 +24,8 @@\n import java.util.ArrayList;\n import java.util.List;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n@@ -40,6 +42,7 @@\n  * minor releases.\n  */\n public class MetaScanner {\n+  private static final Log LOG = LogFactory.getLog(MetaScanner.class);\n   /**\n    * Scans the meta table and calls a visitor on each RowResult and uses a empty\n    * start row value as table name.\n@@ -93,7 +96,8 @@ public static void metaScan(Configuration configuration,\n     // if row is not null, we want to use the startKey of the row's region as\n     // the startRow for the meta scan.\n     byte[] startRow;\n-    if (row != null) {\n+    // row could be non-null but empty -- the HConstants.EMPTY_START_ROW\n+    if (row != null && row.length > 0) {\n       // Scan starting at a particular row in a particular table\n       assert tableName != null;\n       byte[] searchRow =\n@@ -118,7 +122,9 @@ public static void metaScan(Configuration configuration,\n       byte[] rowBefore = regionInfo.getStartKey();\n       startRow = HRegionInfo.createRegionName(tableName, rowBefore,\n           HConstants.ZEROES, false);\n-    } else if (tableName == null || tableName.length == 0) {\n+    } else if (row == null ||\n+        (row != null && Bytes.equals(HConstants.EMPTY_START_ROW, row)) ||\n+        (tableName == null || tableName.length == 0)) {\n       // Full META scan\n       startRow = HConstants.EMPTY_START_ROW;\n     } else {\n@@ -133,8 +139,12 @@ public static void metaScan(Configuration configuration,\n         configuration.getInt(\"hbase.meta.scanner.caching\", 100));\n     do {\n       final Scan scan = new Scan(startRow).addFamily(HConstants.CATALOG_FAMILY);\n-      callable = new ScannerCallable(connection, HConstants.META_TABLE_NAME,\n-          scan);\n+      byte [] metaTableName = Bytes.equals(tableName, HConstants.ROOT_TABLE_NAME)?\n+        HConstants.ROOT_TABLE_NAME: HConstants.META_TABLE_NAME;\n+      LOG.debug(\"Scanning \" + Bytes.toString(metaTableName) +\n+        \" starting at row=\" + Bytes.toString(startRow) + \" for max=\" +\n+        rowUpperLimit + \" rows\");\n+      callable = new ScannerCallable(connection, metaTableName, scan);\n       // Open scanner\n       connection.getRegionServerWithRetries(callable);\n \n@@ -172,10 +182,24 @@ public static void metaScan(Configuration configuration,\n \n   /**\n    * Lists all of the regions currently in META.\n-   * @return\n+   * @param conf\n+   * @return List of all user-space regions.\n    * @throws IOException\n    */\n   public static List<HRegionInfo> listAllRegions(Configuration conf)\n+  throws IOException {\n+    return listAllRegions(conf, true);\n+  }\n+\n+  /**\n+   * Lists all of the regions currently in META.\n+   * @param conf\n+   * @param offlined True if we are to include offlined regions, false and we'll\n+   * leave out offlined regions from returned list.\n+   * @return List of all user-space regions.\n+   * @throws IOException\n+   */\n+  public static List<HRegionInfo> listAllRegions(Configuration conf, final boolean offlined)\n   throws IOException {\n     final List<HRegionInfo> regions = new ArrayList<HRegionInfo>();\n     MetaScannerVisitor visitor =\n@@ -185,9 +209,15 @@ public boolean processRow(Result result) throws IOException {\n           if (result == null || result.isEmpty()) {\n             return true;\n           }\n-          HRegionInfo regionInfo = Writables.getHRegionInfo(\n-              result.getValue(HConstants.CATALOG_FAMILY,\n-                  HConstants.REGIONINFO_QUALIFIER));\n+          byte [] bytes = result.getValue(HConstants.CATALOG_FAMILY,\n+            HConstants.REGIONINFO_QUALIFIER);\n+          if (bytes == null) {\n+            LOG.warn(\"Null REGIONINFO_QUALIFIER: \" + result);\n+            return true;\n+          }\n+          HRegionInfo regionInfo = Writables.getHRegionInfo(bytes);\n+          // If region offline AND we are not to include offlined regions, return.\n+          if (regionInfo.isOffline() && !offlined) return true;\n           regions.add(regionInfo);\n           return true;\n         }",
                "deletions": 8
            },
            {
                "sha": "d383e2bf28463d866f33b25c044a2cc0c81aaa9d",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "patch": "@@ -519,9 +519,6 @@ public void regionOnline(HRegionInfo regionInfo, HServerInfo serverInfo) {\n         this.regionsInTransition.remove(regionInfo.getEncodedName());\n       if (rs != null) {\n         this.regionsInTransition.notifyAll();\n-      } else {\n-        LOG.warn(\"Asked online a region that was not in \" +\n-          \"regionsInTransition: \" + rs);\n       }\n     }\n     synchronized (this.regions) {\n@@ -597,7 +594,10 @@ public void setOffline(HRegionInfo regionInfo) {\n       HServerInfo serverInfo = this.regions.remove(regionInfo);\n       if (serverInfo != null) {\n         List<HRegionInfo> serverRegions = this.servers.get(serverInfo);\n-        serverRegions.remove(regionInfo);\n+        if (!serverRegions.remove(regionInfo)) {\n+          LOG.warn(\"Asked offline a region that was not on expected server: \" +\n+            regionInfo + \", \" + serverInfo.getServerName());\n+        }\n       } else {\n         LOG.warn(\"Asked offline a region that was not online: \" + regionInfo);\n       }\n@@ -1104,9 +1104,9 @@ public void assignAllUserRegions() throws IOException {\n     // First experiment at synchronous assignment\n     // Simpler because just wait for no regions in transition\n \n-    // Scan META for all user regions\n+    // Scan META for all user regions; do not include offlined regions in list.\n     List<HRegionInfo> allRegions =\n-      MetaScanner.listAllRegions(master.getConfiguration());\n+      MetaScanner.listAllRegions(master.getConfiguration(), false);\n     if (allRegions == null || allRegions.isEmpty()) return;\n \n     // Get all available servers",
                "deletions": 6
            },
            {
                "sha": "9fbc376f41dd8b1d72b4c972f27402558491fb05",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "blob_url": "https://github.com/apache/hbase/blob/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "raw_url": "https://github.com/apache/hbase/raw/34db492b4627298cde804a565f076ce1bb3fa2c6/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java?ref=34db492b4627298cde804a565f076ce1bb3fa2c6",
                "patch": "@@ -98,6 +98,7 @@ public boolean visit(Result r) throws IOException {\n         if (r == null || r.isEmpty()) return true;\n         count.incrementAndGet();\n         HRegionInfo info = getHRegionInfo(r);\n+        if (info == null) return true; // Keep scanning\n         if (info.isSplitParent()) splitParents.put(info, r);\n         // Returning true means \"keep scanning\"\n         return true;\n@@ -157,7 +158,7 @@ boolean cleanParent(final HRegionInfo parent,\n     boolean hasReferencesB =\n       checkDaughter(parent, rowContent, HConstants.SPLITB_QUALIFIER);\n     if (!hasReferencesA && !hasReferencesB) {\n-      LOG.info(\"Deleting region \" + parent.getRegionNameAsString() +\n+      LOG.debug(\"Deleting region \" + parent.getRegionNameAsString() +\n         \" because daughter splits no longer hold references\");\n       FileSystem fs = this.services.getMasterFileSystem().getFileSystem();\n       Path rootdir = this.services.getMasterFileSystem().getRootDir();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Fix NPE in TestCatalogTracker up on hudson; removed toString on HConnection\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1006344 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d3f3ccee7ad3d9b61bd83debcf190159b1990060",
        "parent": "https://github.com/apache/hbase/commit/9d3bb109db6e8376b3bd47ca305c0e723ed06b39",
        "bug_id": "hbase_218",
        "file": [
            {
                "sha": "7c5947652884845e236859a8c648d5d351680bde",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "blob_url": "https://github.com/apache/hbase/blob/d3f3ccee7ad3d9b61bd83debcf190159b1990060/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "raw_url": "https://github.com/apache/hbase/raw/d3f3ccee7ad3d9b61bd83debcf190159b1990060/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java?ref=d3f3ccee7ad3d9b61bd83debcf190159b1990060",
                "patch": "@@ -129,15 +129,15 @@ public CatalogTracker(final ZooKeeperWatcher zk, final HConnection connection,\n   public void start() throws IOException, InterruptedException {\n     this.rootRegionTracker.start();\n     this.metaNodeTracker.start();\n+    LOG.debug(\"Starting catalog tracker \" + this);\n   }\n \n   /**\n    * Stop working.\n    * Interrupts any ongoing waits.\n    */\n   public void stop() {\n-    LOG.debug(\"Stopping catalog tracker \" + this.connection.toString() +\n-      \"; will interrupt blocked waits on root and meta\");\n+    LOG.debug(\"Stopping catalog tracker \" + this);\n     this.stopped = true;\n     this.rootRegionTracker.stop();\n     this.metaNodeTracker.stop();",
                "deletions": 2
            },
            {
                "sha": "fbffd0bccc008280cf4e99e3a788bc3669ff53e1",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/d3f3ccee7ad3d9b61bd83debcf190159b1990060/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/d3f3ccee7ad3d9b61bd83debcf190159b1990060/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 7,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=d3f3ccee7ad3d9b61bd83debcf190159b1990060",
                "patch": "@@ -216,7 +216,6 @@ static boolean isRegionCached(Configuration conf,\n       new ConcurrentHashMap<String, HRegionInterface>();\n \n     private final RootRegionTracker rootRegionTracker;\n-    private final String identifier;\n \n     /**\n      * Map of table to table {@link HRegionLocation}s.  The table key is made\n@@ -262,7 +261,6 @@ public HConnectionImplementation(Configuration conf)\n \n       // initialize zookeeper and master address manager\n       this.zooKeeper = getZooKeeperWatcher();\n-      this.identifier = this.zooKeeper.toString();\n       masterAddressTracker = new MasterAddressTracker(this.zooKeeper, this);\n       zooKeeper.registerListener(masterAddressTracker);\n       masterAddressTracker.start();\n@@ -278,11 +276,6 @@ public Configuration getConfiguration() {\n       return this.conf;\n     }\n \n-    @Override\n-    public String toString() {\n-      return this.identifier;\n-    }\n-\n     private long getPauseTime(int tries) {\n       int ntries = tries;\n       if (ntries >= HConstants.RETRY_BACKOFF.length) {",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3024 NPE processing server crash in MetaEditor.addDaughter\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@999672 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/af3bf8cad03d83404ad1e9b120899dd588689287",
        "parent": "https://github.com/apache/hbase/commit/5fa3d69f666585bbd9733d58e95292a0116029a5",
        "bug_id": "hbase_219",
        "file": [
            {
                "sha": "6000ec1fb37ec8d905b58d0a3d4d85e288b9efc5",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/af3bf8cad03d83404ad1e9b120899dd588689287/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/af3bf8cad03d83404ad1e9b120899dd588689287/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=af3bf8cad03d83404ad1e9b120899dd588689287",
                "patch": "@@ -533,6 +533,7 @@ Release 0.21.0 - Unreleased\n    HBASE-3018  Bulk assignment on startup runs serially through the cluster\n                servers assigning in bulk to one at a time\n    HBASE-3023  NPE processing server crash in MetaReader. getServerUserRegions\n+   HBASE-3024  NPE processing server crash in MetaEditor.addDaughter\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "fbc6bbacfc0bbbf9782e518945b490518de95b93",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "blob_url": "https://github.com/apache/hbase/blob/af3bf8cad03d83404ad1e9b120899dd588689287/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "raw_url": "https://github.com/apache/hbase/raw/af3bf8cad03d83404ad1e9b120899dd588689287/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java?ref=af3bf8cad03d83404ad1e9b120899dd588689287",
                "patch": "@@ -100,9 +100,9 @@ public static void addDaughter(final CatalogTracker catalogTracker,\n     if (serverInfo != null) addLocation(put, serverInfo);\n     server.put(catalogRegionName, put);\n     LOG.info(\"Added daughter \" + regionInfo.getRegionNameAsString() +\n-      \" in region \" + Bytes.toString(catalogRegionName) + \" with \" +\n-      \"server=\" + serverInfo.getHostnamePort() + \", \" +\n-      \"startcode=\" + serverInfo.getStartCode());\n+      \" in region \" + Bytes.toString(catalogRegionName) +\n+      (serverInfo == null?\n+        \", serverInfo=null\": \", serverInfo=\" + serverInfo.getServerName()));\n   }\n \n   /**",
                "deletions": 3
            },
            {
                "sha": "ccd4d36cfa7c4d9a21fb7d47e07d0252a8c5f7ee",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/af3bf8cad03d83404ad1e9b120899dd588689287/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/af3bf8cad03d83404ad1e9b120899dd588689287/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java?ref=af3bf8cad03d83404ad1e9b120899dd588689287",
                "patch": "@@ -152,7 +152,7 @@ void fixupDaughter(final NavigableMap<HRegionInfo, Result> hris,\n     if (bytes == null || bytes.length <= 0) return;\n     HRegionInfo hri = Writables.getHRegionInfo(bytes);\n     if (!hris.containsKey(hri)) {\n-      LOG.info(\"Fixup; missing daughter \" + hri.getEncodedNameAsBytes());\n+      LOG.info(\"Fixup; missing daughter \" + hri.getEncodedName());\n       MetaEditor.addDaughter(this.server.getCatalogTracker(), hri, null);\n       this.services.getAssignmentManager().assign(hri);\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3023 NPE processing server crash in MetaReader. getServerUserRegions\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@999664 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/5fa3d69f666585bbd9733d58e95292a0116029a5",
        "parent": "https://github.com/apache/hbase/commit/ffedf2c58efcc50debfcd8c7b4700f5b0f90e818",
        "bug_id": "hbase_220",
        "file": [
            {
                "sha": "1bb7b55fd0d6f59d42cfe2c83d9af5d7915eccd3",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/5fa3d69f666585bbd9733d58e95292a0116029a5/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/5fa3d69f666585bbd9733d58e95292a0116029a5/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=5fa3d69f666585bbd9733d58e95292a0116029a5",
                "patch": "@@ -532,6 +532,7 @@ Release 0.21.0 - Unreleased\n                the region\n    HBASE-3018  Bulk assignment on startup runs serially through the cluster\n                servers assigning in bulk to one at a time\n+   HBASE-3023  NPE processing server crash in MetaReader. getServerUserRegions\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "21a075ac5e53341f87ffc51b667729bf62eb38ff",
                "filename": "src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/5fa3d69f666585bbd9733d58e95292a0116029a5/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/5fa3d69f666585bbd9733d58e95292a0116029a5/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=5fa3d69f666585bbd9733d58e95292a0116029a5",
                "patch": "@@ -232,7 +232,8 @@ private static HServerAddress readLocation(HRegionInterface metaServer,\n \n   /**\n    * @param data A .META. table row.\n-   * @return A pair of the regioninfo and the server address from <code>data</code>.\n+   * @return A pair of the regioninfo and the server address from <code>data</code>\n+   * (or null for server address if no address set in .META.).\n    * @throws IOException\n    */\n   public static Pair<HRegionInfo, HServerAddress> metaRowToRegionPair(\n@@ -410,7 +411,10 @@ public static boolean tableExists(CatalogTracker catalogTracker,\n       while((result = metaServer.next(scannerid)) != null) {\n         if (result != null && result.size() > 0) {\n           Pair<HRegionInfo, HServerAddress> pair = metaRowToRegionPair(result);\n-          if (!pair.getSecond().equals(hsi.getServerAddress())) continue;\n+          if (pair.getSecond() == null ||\n+              !pair.getSecond().equals(hsi.getServerAddress())) {\n+            continue;\n+          }\n           hris.put(pair.getFirst(), result);\n         }\n       }",
                "deletions": 2
            },
            {
                "sha": "ab3516790df196651aeea4007532092dc76ece40",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/5fa3d69f666585bbd9733d58e95292a0116029a5/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/5fa3d69f666585bbd9733d58e95292a0116029a5/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java?ref=5fa3d69f666585bbd9733d58e95292a0116029a5",
                "patch": "@@ -111,7 +111,7 @@ protected void chore() {\n       String deadServersList = deadservers.toString();\n       LOG.info(\"regionservers=\" + numServers +\n         \", averageload=\" + StringUtils.limitDecimalTo2(averageLoad) +\n-        ((numDeadServers > 0)?  (\"deadservers=\" + deadServersList): \"\"));\n+        ((numDeadServers > 0)?  (\", deadservers=\" + deadServersList): \"\"));\n     }\n   }\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2986 multi writable can npe causing client hang\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@997353 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/787d807b37739139a598bb66fb4bc47e1a76b392",
        "parent": "https://github.com/apache/hbase/commit/762e374861d0a8e7a0f6710f05890877f34b4b3a",
        "bug_id": "hbase_221",
        "file": [
            {
                "sha": "c7edbf0bb78913239422233626b85be46008a9a4",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/787d807b37739139a598bb66fb4bc47e1a76b392/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/787d807b37739139a598bb66fb4bc47e1a76b392/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=787d807b37739139a598bb66fb4bc47e1a76b392",
                "patch": "@@ -520,6 +520,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2978  LoadBalancer IndexOutOfBoundsException\n    HBASE-2983  TestHLog unit test is mis-comparing an assertion\n                (Alex Newman via Todd Lipcon)\n+   HBASE-2986  multi writable can npe causing client hang\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "7efc7fc037db4d1b40ed363f2b10892517556faa",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/787d807b37739139a598bb66fb4bc47e1a76b392/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/787d807b37739139a598bb66fb4bc47e1a76b392/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=787d807b37739139a598bb66fb4bc47e1a76b392",
                "patch": "@@ -1128,9 +1128,8 @@ public void processBatch(List<Row> list,\n               for (Entry<byte[], List<Pair<Integer,Result>>> e : resp.getResults().entrySet()) {\n                 byte[] regionName = e.getKey();\n                 List<Pair<Integer, Result>> regionResults = e.getValue();\n-                for (int i = 0; i < regionResults.size(); i++) {\n-                  Pair<Integer, Result> regionResult = regionResults.get(i);\n-                  if (regionResult.getSecond() == null) {\n+                for (Pair<Integer, Result> regionResult : regionResults) {\n+                  if (regionResult == null) {\n                     // failed\n                     LOG.debug(\"Failures for region: \" + Bytes.toStringBinary(regionName) + \", removing from cache\");\n                   } else {",
                "deletions": 3
            },
            {
                "sha": "91bd04b0bf0945a2eee5aefc7fee2471460913cb",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java",
                "blob_url": "https://github.com/apache/hbase/blob/787d807b37739139a598bb66fb4bc47e1a76b392/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java",
                "raw_url": "https://github.com/apache/hbase/raw/787d807b37739139a598bb66fb4bc47e1a76b392/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java",
                "status": "modified",
                "changes": 18,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java?ref=787d807b37739139a598bb66fb4bc47e1a76b392",
                "patch": "@@ -61,7 +61,7 @@ public int size() {\n \n   /**\n    * Add the pair to the container, grouped by the regionName\n-   * \n+   *\n    * @param regionName\n    * @param r\n    *          First item in the pair is the original index of the Action\n@@ -89,8 +89,12 @@ public void write(DataOutput out) throws IOException {\n       List<Pair<Integer, Result>> lst = e.getValue();\n       out.writeInt(lst.size());\n       for (Pair<Integer, Result> r : lst) {\n-        out.writeInt(r.getFirst());\n-        HbaseObjectWritable.writeObject(out, r.getSecond(), Result.class, null);\n+        if (r == null) {\n+          out.writeInt(-1); // Cant have index -1; on other side we recognize -1 as 'null'\n+        } else {\n+          out.writeInt(r.getFirst()); // Can this can npe!?!\n+          HbaseObjectWritable.writeObject(out, r.getSecond(), Result.class, null);\n+        }\n       }\n     }\n   }\n@@ -106,8 +110,12 @@ public void readFields(DataInput in) throws IOException {\n           listSize);\n       for (int j = 0; j < listSize; j++) {\n         Integer idx = in.readInt();\n-        Result r = (Result) HbaseObjectWritable.readObject(in, null);\n-        lst.add(new Pair<Integer, Result>(idx, r));\n+        if (idx == -1) {\n+          lst.add(null); \n+        } else {\n+          Result r = (Result) HbaseObjectWritable.readObject(in, null);\n+          lst.add(new Pair<Integer, Result>(idx, r));\n+        }\n       }\n       results.put(key, lst);\n     }",
                "deletions": 5
            },
            {
                "sha": "423e912159be90d08345487d8243f6381a41e971",
                "filename": "src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/787d807b37739139a598bb66fb4bc47e1a76b392/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/787d807b37739139a598bb66fb4bc47e1a76b392/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java?ref=787d807b37739139a598bb66fb4bc47e1a76b392",
                "patch": "@@ -961,7 +961,7 @@ public void run() {\n             throw e;\n           }\n         } catch (Exception e) {\n-          LOG.info(getName() + \" caught: \" +\n+          LOG.warn(getName() + \" caught: \" +\n                    StringUtils.stringifyException(e));\n         }\n       }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2973 NPE in LogCleaner\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@995281 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/2210cf908b2c4b69f20960f93a0809b23dcd21b1",
        "parent": "https://github.com/apache/hbase/commit/e144e8f32dfd9f36036e0402c43eb93956ba82ca",
        "bug_id": "hbase_222",
        "file": [
            {
                "sha": "f0b6f8d4955558eb306d2a16650c991d545cc008",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/2210cf908b2c4b69f20960f93a0809b23dcd21b1/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/2210cf908b2c4b69f20960f93a0809b23dcd21b1/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2210cf908b2c4b69f20960f93a0809b23dcd21b1",
                "patch": "@@ -513,6 +513,7 @@ Release 0.21.0 - Unreleased\n                written out to SequenceFile\n    HBASE-2969  missing sync in HTablePool.getTable()\n                (Guilherme Mauro Germoglio Barbosa via Stack)\n+   HBASE-2973  NPE in LogCleaner\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "bacd64da43887f9117c8e001722945d9bbe50ccc",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/2210cf908b2c4b69f20960f93a0809b23dcd21b1/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/2210cf908b2c4b69f20960f93a0809b23dcd21b1/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java?ref=2210cf908b2c4b69f20960f93a0809b23dcd21b1",
                "patch": "@@ -123,7 +123,8 @@ public void addLogCleaner(LogCleanerDelegate logCleaner) {\n   @Override\n   protected void chore() {\n     try {\n-      FileStatus[] files = this.fs.listStatus(this.oldLogDir);\n+      FileStatus [] files = this.fs.listStatus(this.oldLogDir);\n+      if (files == null) return;\n       int nbDeletedLog = 0;\n       FILE: for (FileStatus file : files) {\n         Path filePath = file.getPath();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2909  SoftValueSortedMap is broken, can generate NPEs\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@985383 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d80de85a69a6aec4c39463e5440ebd1137d15959",
        "parent": "https://github.com/apache/hbase/commit/b3baa8aca7b76cefcbac0742483ad9d6c1d746e9",
        "bug_id": "hbase_223",
        "file": [
            {
                "sha": "5fa60c33b323020d32fbe8c5b0892044790ec0ae",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/d80de85a69a6aec4c39463e5440ebd1137d15959/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/d80de85a69a6aec4c39463e5440ebd1137d15959/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d80de85a69a6aec4c39463e5440ebd1137d15959",
                "patch": "@@ -475,6 +475,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2905  NPE when inserting mass data via REST interface (Sandy Yin via\n                Andrew Purtell)\n    HBASE-2908  Wrong order of null-check [in TIF] (Libor Dener via Stack)\n+   HBASE-2909  SoftValueSortedMap is broken, can generate NPEs\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "f81bc9810596b072b6e084aafcf193722002366a",
                "filename": "src/main/java/org/apache/hadoop/hbase/util/SoftValue.java",
                "blob_url": "https://github.com/apache/hbase/blob/b3baa8aca7b76cefcbac0742483ad9d6c1d746e9/src/main/java/org/apache/hadoop/hbase/util/SoftValue.java",
                "raw_url": "https://github.com/apache/hbase/raw/b3baa8aca7b76cefcbac0742483ad9d6c1d746e9/src/main/java/org/apache/hadoop/hbase/util/SoftValue.java",
                "status": "removed",
                "changes": 49,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/SoftValue.java?ref=b3baa8aca7b76cefcbac0742483ad9d6c1d746e9",
                "patch": "@@ -1,49 +0,0 @@\n-/**\n- * Copyright 2010 The Apache Software Foundation\n- *\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hbase.util;\n-\n-import java.lang.ref.ReferenceQueue;\n-import java.lang.ref.SoftReference;\n-import java.util.Map;\n-\n-/**\n- * A SoftReference derivative so that we can track down what keys to remove.\n- */\n-class SoftValue<K, V> extends SoftReference<V> implements Map.Entry<K, V> {\n-  private final K key;\n-\n-  @SuppressWarnings(\"unchecked\")\n-  SoftValue(K key, V value, ReferenceQueue queue) {\n-    super(value, queue);\n-    this.key = key;\n-  }\n-\n-  public K getKey() {\n-    return this.key;\n-  }\n-\n-  public V getValue() {\n-    return get();\n-  }\n-\n-  public V setValue(V value) {\n-    throw new RuntimeException(\"Not implemented\");\n-  }\n-}\n\\ No newline at end of file",
                "deletions": 49
            },
            {
                "sha": "6294a52227520f4551eda2ba7e72a5965adef6d7",
                "filename": "src/main/java/org/apache/hadoop/hbase/util/SoftValueMap.java",
                "blob_url": "https://github.com/apache/hbase/blob/b3baa8aca7b76cefcbac0742483ad9d6c1d746e9/src/main/java/org/apache/hadoop/hbase/util/SoftValueMap.java",
                "raw_url": "https://github.com/apache/hbase/raw/b3baa8aca7b76cefcbac0742483ad9d6c1d746e9/src/main/java/org/apache/hadoop/hbase/util/SoftValueMap.java",
                "status": "removed",
                "changes": 146,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/SoftValueMap.java?ref=b3baa8aca7b76cefcbac0742483ad9d6c1d746e9",
                "patch": "@@ -1,146 +0,0 @@\n-/**\n- * Copyright 2010 The Apache Software Foundation\n- *\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hbase.util;\n-\n-import java.lang.ref.ReferenceQueue;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-\n-/**\n- * A Map that uses Soft Reference values internally. Use as a simple cache.\n- *\n- * @param <K> key class\n- * @param <V> value class\n- */\n-public class SoftValueMap<K,V> implements Map<K,V> {\n-  private final Map<K, SoftValue<K,V>> internalMap =\n-    new HashMap<K, SoftValue<K,V>>();\n-  private final ReferenceQueue<?> rq;\n-\n-  public SoftValueMap() {\n-    this(new ReferenceQueue());\n-  }\n-\n-  public SoftValueMap(final ReferenceQueue<?> rq) {\n-    this.rq = rq;\n-  }\n-\n-  /**\n-   * Checks soft references and cleans any that have been placed on\n-   * ReferenceQueue.\n-   * @return How many references cleared.\n-   */\n-  @SuppressWarnings({\"unchecked\"})\n-  public int checkReferences() {\n-    int i = 0;\n-    for (Object obj; (obj = this.rq.poll()) != null;) {\n-      i++;\n-      this.internalMap.remove(((SoftValue<K,V>)obj).getKey());\n-    }\n-    return i;\n-  }\n-\n-  public V put(K key, V value) {\n-    checkReferences();\n-    SoftValue<K,V> oldValue = this.internalMap.put(key,\n-      new SoftValue<K,V>(key, value, this.rq));\n-    return oldValue == null ? null : oldValue.get();\n-  }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  public void putAll(Map map) {\n-    throw new RuntimeException(\"Not implemented\");\n-  }\n-\n-  @SuppressWarnings({\"SuspiciousMethodCalls\"})\n-  public V get(Object key) {\n-    checkReferences();\n-    SoftValue<K,V> value = this.internalMap.get(key);\n-    if (value == null) {\n-      return null;\n-    }\n-    if (value.get() == null) {\n-      this.internalMap.remove(key);\n-      return null;\n-    }\n-    return value.get();\n-  }\n-\n-  public V remove(Object key) {\n-    checkReferences();\n-    SoftValue<K,V> value = this.internalMap.remove(key);\n-    return value == null ? null : value.get();\n-  }\n-\n-  public boolean containsKey(Object key) {\n-    checkReferences();\n-    return this.internalMap.containsKey(key);\n-  }\n-\n-  public boolean containsValue(Object value) {\n-/*    checkReferences();\n-    return internalMap.containsValue(value);*/\n-    throw new UnsupportedOperationException(\"Don't support containsValue!\");\n-  }\n-\n-  public boolean isEmpty() {\n-    checkReferences();\n-    return this.internalMap.isEmpty();\n-  }\n-\n-  public int size() {\n-    checkReferences();\n-    return this.internalMap.size();\n-  }\n-\n-  public void clear() {\n-    checkReferences();\n-    this.internalMap.clear();\n-  }\n-\n-  public Set<K> keySet() {\n-    checkReferences();\n-    return this.internalMap.keySet();\n-  }\n-\n-  public Set<Map.Entry<K,V>> entrySet() {\n-    checkReferences();\n-    Set<Map.Entry<K, SoftValue<K,V>>> entries = this.internalMap.entrySet();\n-    Set<Map.Entry<K, V>> real_entries = new HashSet<Map.Entry<K,V>>();\n-    for(Map.Entry<K, SoftValue<K,V>> entry : entries) {\n-      real_entries.add(entry.getValue());\n-    }\n-    return real_entries;\n-  }\n-\n-  public Collection<V> values() {\n-    checkReferences();\n-    Collection<SoftValue<K,V>> softValues = this.internalMap.values();\n-    ArrayList<V> hardValues = new ArrayList<V>();\n-    for(SoftValue<K,V> softValue : softValues) {\n-      hardValues.add(softValue.get());\n-    }\n-    return hardValues;\n-  }\n-}\n\\ No newline at end of file",
                "deletions": 146
            },
            {
                "sha": "4d1a5524c36c0e25efa170b726c8301afffad343",
                "filename": "src/main/java/org/apache/hadoop/hbase/util/SoftValueSortedMap.java",
                "blob_url": "https://github.com/apache/hbase/blob/d80de85a69a6aec4c39463e5440ebd1137d15959/src/main/java/org/apache/hadoop/hbase/util/SoftValueSortedMap.java",
                "raw_url": "https://github.com/apache/hbase/raw/d80de85a69a6aec4c39463e5440ebd1137d15959/src/main/java/org/apache/hadoop/hbase/util/SoftValueSortedMap.java",
                "status": "modified",
                "changes": 20,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/SoftValueSortedMap.java?ref=d80de85a69a6aec4c39463e5440ebd1137d15959",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.hbase.util;\n \n import java.lang.ref.ReferenceQueue;\n+import java.lang.ref.SoftReference;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Comparator;\n@@ -72,7 +73,7 @@ private int checkReferences() {\n     for (Object obj; (obj = this.rq.poll()) != null;) {\n       i++;\n       //noinspection unchecked\n-      this.internalMap.remove(((SoftValue<K,V>)obj).getKey());\n+      this.internalMap.remove(((SoftValue<K,V>)obj).key);\n     }\n     return i;\n   }\n@@ -171,13 +172,7 @@ public synchronized Comparator comparator() {\n   }\n \n   public synchronized Set<Map.Entry<K,V>> entrySet() {\n-    checkReferences();\n-    Set<Map.Entry<K, SoftValue<K,V>>> entries = this.internalMap.entrySet();\n-    Set<Map.Entry<K, V>> real_entries = new TreeSet<Map.Entry<K,V>>();\n-    for(Map.Entry<K, SoftValue<K,V>> entry : entries) {\n-      real_entries.add(entry.getValue());\n-    }\n-    return real_entries;\n+    throw new RuntimeException(\"Not implemented\");\n   }\n \n   public synchronized Collection<V> values() {\n@@ -189,4 +184,13 @@ public synchronized Comparator comparator() {\n     }\n     return hardValues;\n   }\n+\n+  private static class SoftValue<K,V> extends SoftReference<V> {\n+    final K key;\n+\n+    SoftValue(K key, V value, ReferenceQueue q) {\n+      super(value, q);\n+      this.key = key;\n+    }\n+  }\n }",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2905 NPE when inserting mass data via REST interface\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@983699 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a95f1ed4f6a25872567d4b027a8e2c3e44572c01",
        "parent": "https://github.com/apache/hbase/commit/af09891840737cacb1ac1f4917736225dc8691b4",
        "bug_id": "hbase_224",
        "file": [
            {
                "sha": "0869f6a8bd5d5fbbbdb0f96c309e3abb84c76bda",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/a95f1ed4f6a25872567d4b027a8e2c3e44572c01/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/a95f1ed4f6a25872567d4b027a8e2c3e44572c01/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=a95f1ed4f6a25872567d4b027a8e2c3e44572c01",
                "patch": "@@ -472,6 +472,8 @@ Release 0.21.0 - Unreleased\n    HBASE-2823  Entire Row Deletes not stored in Row+Col Bloom\n                (Alexander Georgiev via Stack)\n    HBASE-2897  RowResultGenerator should handle NoSuchColumnFamilyException\n+   HBASE-2905  NPE when inserting mass data via REST interface (Sandy Yin via\n+               Andrew Purtell)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "092c69575b4d23fb050d9162bee9942ea94b7e49",
                "filename": "src/main/java/org/apache/hadoop/hbase/rest/provider/producer/PlainTextMessageBodyProducer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a95f1ed4f6a25872567d4b027a8e2c3e44572c01/src/main/java/org/apache/hadoop/hbase/rest/provider/producer/PlainTextMessageBodyProducer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a95f1ed4f6a25872567d4b027a8e2c3e44572c01/src/main/java/org/apache/hadoop/hbase/rest/provider/producer/PlainTextMessageBodyProducer.java",
                "status": "modified",
                "changes": 14,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/rest/provider/producer/PlainTextMessageBodyProducer.java?ref=a95f1ed4f6a25872567d4b027a8e2c3e44572c01",
                "patch": "@@ -24,8 +24,6 @@\n import java.io.OutputStream;\n import java.lang.annotation.Annotation;\n import java.lang.reflect.Type;\n-import java.util.Map;\n-import java.util.WeakHashMap;\n \n import javax.ws.rs.Produces;\n import javax.ws.rs.WebApplicationException;\n@@ -47,7 +45,7 @@\n public class PlainTextMessageBodyProducer \n   implements MessageBodyWriter<Object> {\n \n-  private Map<Object, byte[]> buffer = new WeakHashMap<Object, byte[]>();\n+  private ThreadLocal<byte[]> buffer = new ThreadLocal<byte[]>();\n \n   @Override\n   public boolean isWriteable(Class<?> arg0, Type arg1, Annotation[] arg2,\n@@ -58,16 +56,18 @@ public boolean isWriteable(Class<?> arg0, Type arg1, Annotation[] arg2,\n \t@Override\n \tpublic long getSize(Object object, Class<?> type, Type genericType,\n \t\t\tAnnotation[] annotations, MediaType mediaType) {\n-\t  byte[] bytes = object.toString().getBytes(); \n-\t  buffer.put(object, bytes);\n-\t\treturn bytes.length;\n+    byte[] bytes = object.toString().getBytes(); \n+\t  buffer.set(bytes);\n+    return bytes.length;\n \t}\n \n \t@Override\n \tpublic void writeTo(Object object, Class<?> type, Type genericType,\n \t\t\tAnnotation[] annotations, MediaType mediaType,\n \t\t\tMultivaluedMap<String, Object> httpHeaders, OutputStream outStream)\n \t\t\tthrows IOException, WebApplicationException {\n-\t\toutStream.write(buffer.remove(object));\n+    byte[] bytes = buffer.get();\n+\t\toutStream.write(bytes);\n+    buffer.remove();\n \t}\t\n }",
                "deletions": 7
            },
            {
                "sha": "a1b4b70831adca82437331165cff19bf9f23d8b6",
                "filename": "src/main/java/org/apache/hadoop/hbase/rest/provider/producer/ProtobufMessageBodyProducer.java",
                "blob_url": "https://github.com/apache/hbase/blob/a95f1ed4f6a25872567d4b027a8e2c3e44572c01/src/main/java/org/apache/hadoop/hbase/rest/provider/producer/ProtobufMessageBodyProducer.java",
                "raw_url": "https://github.com/apache/hbase/raw/a95f1ed4f6a25872567d4b027a8e2c3e44572c01/src/main/java/org/apache/hadoop/hbase/rest/provider/producer/ProtobufMessageBodyProducer.java",
                "status": "modified",
                "changes": 10,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/rest/provider/producer/ProtobufMessageBodyProducer.java?ref=a95f1ed4f6a25872567d4b027a8e2c3e44572c01",
                "patch": "@@ -25,8 +25,6 @@\n import java.io.OutputStream;\n import java.lang.annotation.Annotation;\n import java.lang.reflect.Type;\n-import java.util.Map;\n-import java.util.WeakHashMap;\n \n import javax.ws.rs.Produces;\n import javax.ws.rs.WebApplicationException;\n@@ -49,7 +47,7 @@\n public class ProtobufMessageBodyProducer\n   implements MessageBodyWriter<ProtobufMessageHandler> {\n \n-  private Map<Object, byte[]> buffer = new WeakHashMap<Object, byte[]>();\n+  private ThreadLocal<byte[]> buffer = new ThreadLocal<byte[]>();\n \n \t@Override\n \tpublic boolean isWriteable(Class<?> type, Type genericType, \n@@ -67,14 +65,16 @@ public long getSize(ProtobufMessageHandler m, Class<?> type, Type genericType,\n \t    return -1;\n \t  }\n \t  byte[] bytes = baos.toByteArray();\n-\t  buffer.put(m, bytes);\n+\t  buffer.set(bytes);\n \t  return bytes.length;\n \t}\n \n \tpublic void writeTo(ProtobufMessageHandler m, Class<?> type, Type genericType,\n \t    Annotation[] annotations, MediaType mediaType, \n \t    MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) \n \t    throws IOException, WebApplicationException {\n-\t  entityStream.write(buffer.remove(m));\n+    byte[] bytes = buffer.get();\n+\t  entityStream.write(bytes);\n+    buffer.remove();\n \t}\n }",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2852  Bloom filter NPE (pranav via jgray)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@979491 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/90908553e987654fb891f6315787639f01fc8b95",
        "parent": "https://github.com/apache/hbase/commit/55e8b201e6d7320ea56a8d03399e3e548693cf2f",
        "bug_id": "hbase_225",
        "file": [
            {
                "sha": "49b0ee641a36fa0cd150c14191b7fa2355cabaf9",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/90908553e987654fb891f6315787639f01fc8b95/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/90908553e987654fb891f6315787639f01fc8b95/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=90908553e987654fb891f6315787639f01fc8b95",
                "patch": "@@ -456,6 +456,7 @@ Release 0.21.0 - Unreleased\n                that are in offline state in meta after a split\n    HBASE-2815  not able to run the test suite in background because TestShell\n                gets suspended on tty output (Alexey Kovyrin via Stack)\n+   HBASE-2852  Bloom filter NPE (pranav via jgray)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "757a50c44f95f47f569330df22ca16851f6e0a27",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/90908553e987654fb891f6315787639f01fc8b95/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/90908553e987654fb891f6315787639f01fc8b95/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java?ref=90908553e987654fb891f6315787639f01fc8b95",
                "patch": "@@ -930,7 +930,7 @@ private boolean passesBloomFilter(Scan scan, final SortedSet<byte[]> columns) {\n           key = row;\n           break;\n         case ROWCOL:\n-          if (columns.size() == 1) {\n+          if (columns != null && columns.size() == 1) {\n             byte[] col = columns.first();\n             key = Bytes.add(row, col);\n             break;",
                "deletions": 1
            },
            {
                "sha": "98bd3e5d8fab4b7ea7324c4790269ab75e1130ea",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/90908553e987654fb891f6315787639f01fc8b95/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/90908553e987654fb891f6315787639f01fc8b95/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "status": "modified",
                "changes": 65,
                "additions": 65,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=90908553e987654fb891f6315787639f01fc8b95",
                "patch": "@@ -2658,7 +2658,52 @@ public void testIndexesScanWithOneDeletedRow() throws IOException {\n \n   }\n \n+  //////////////////////////////////////////////////////////////////////////////\n+  // Bloom filter test\n+  //////////////////////////////////////////////////////////////////////////////\n+\n+  public void testAllColumnsWithBloomFilter() throws IOException {\n+    byte [] TABLE = Bytes.toBytes(\"testAllColumnsWithBloomFilter\");\n+    byte [] FAMILY = Bytes.toBytes(\"family\");\n+\n+    //Create table\n+    HColumnDescriptor hcd = new HColumnDescriptor(FAMILY, Integer.MAX_VALUE,\n+        HColumnDescriptor.DEFAULT_COMPRESSION,\n+        HColumnDescriptor.DEFAULT_IN_MEMORY,\n+        HColumnDescriptor.DEFAULT_BLOCKCACHE,\n+        Integer.MAX_VALUE, HColumnDescriptor.DEFAULT_TTL,\n+        \"rowcol\",\n+        HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);\n+    HTableDescriptor htd = new HTableDescriptor(TABLE);\n+    htd.addFamily(hcd);\n+    HRegionInfo info = new HRegionInfo(htd, null, null, false);\n+    Path path = new Path(DIR + \"testAllColumnsWithBloomFilter\");\n+    region = HRegion.createHRegion(info, path, conf);\n+\n+    // For row:0, col:0: insert versions 1 through 5.\n+    byte row[] = Bytes.toBytes(\"row:\" + 0);\n+    byte column[] = Bytes.toBytes(\"column:\" + 0);\n+    Put put = new Put(row);\n+    for (long idx = 1; idx <= 4; idx++) {\n+      put.add(FAMILY, column, idx, Bytes.toBytes(\"value-version-\" + idx));\n+    }\n+    region.put(put);\n+\n+    //Flush\n+    region.flushcache();\n \n+    //Get rows\n+    Get get = new Get(row);\n+    get.setMaxVersions();\n+    KeyValue[] kvs = region.get(get, null).raw();\n+\n+    //Check if rows are correct\n+    assertEquals(4, kvs.length);\n+    checkOneCell(kvs[0], FAMILY, 0, 0, 4);\n+    checkOneCell(kvs[1], FAMILY, 0, 0, 3);\n+    checkOneCell(kvs[2], FAMILY, 0, 0, 2);\n+    checkOneCell(kvs[3], FAMILY, 0, 0, 1);\n+  }\n \n   private void putData(int startRow, int numRows, byte [] qf,\n       byte [] ...families)\n@@ -2784,4 +2829,24 @@ private void initHRegion (byte [] tableName, String callingMethod,\n     Path path = new Path(DIR + callingMethod);\n     region = HRegion.createHRegion(info, path, conf);\n   }\n+\n+  /**\n+   * Assert that the passed in KeyValue has expected contents for the\n+   * specified row, column & timestamp.\n+   */\n+  private void checkOneCell(KeyValue kv, byte[] cf,\n+                             int rowIdx, int colIdx, long ts) {\n+    String ctx = \"rowIdx=\" + rowIdx + \"; colIdx=\" + colIdx + \"; ts=\" + ts;\n+    assertEquals(\"Row mismatch which checking: \" + ctx,\n+                 \"row:\"+ rowIdx, Bytes.toString(kv.getRow()));\n+    assertEquals(\"ColumnFamily mismatch while checking: \" + ctx,\n+                 Bytes.toString(cf), Bytes.toString(kv.getFamily()));\n+    assertEquals(\"Column qualifier mismatch while checking: \" + ctx,\n+                 \"column:\" + colIdx, Bytes.toString(kv.getQualifier()));\n+    assertEquals(\"Timestamp mismatch while checking: \" + ctx,\n+                 ts, kv.getTimestamp());\n+    assertEquals(\"Value mismatch while checking: \" + ctx,\n+                 \"value-version-\" + ts, Bytes.toString(kv.getValue()));\n+  }\n+\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2797  Another NPE in ReadWriteConsistencyControl\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@961549 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/b5a5aa63814b93b567410867b6824f3b33124c4e",
        "parent": "https://github.com/apache/hbase/commit/c2c41f0439fe2c8c72f78949494ecfdcd9de63ae",
        "bug_id": "hbase_226",
        "file": [
            {
                "sha": "7ef4321545145fb8b1eb1dabe76fb8d53402ac15",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/b5a5aa63814b93b567410867b6824f3b33124c4e/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/b5a5aa63814b93b567410867b6824f3b33124c4e/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=b5a5aa63814b93b567410867b6824f3b33124c4e",
                "patch": "@@ -429,6 +429,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2806  DNS hiccups cause uncaught NPE in HServerAddress#getBindAddress\n                (Benoit Sigoure via Stack)\n    HBASE-2806  (small compile fix via jgray)\n+   HBASE-2797  Another NPE in ReadWriteConsistencyControl\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "91eafd062760f240392eca91db77849a2ee92b0e",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/b5a5aa63814b93b567410867b6824f3b33124c4e/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/b5a5aa63814b93b567410867b6824f3b33124c4e/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "status": "modified",
                "changes": 8,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=b5a5aa63814b93b567410867b6824f3b33124c4e",
                "patch": "@@ -169,15 +169,9 @@\n   }\n \n   public synchronized KeyValue peek() {\n-    try {\n-      checkReseek();\n-    } catch (IOException e) {\n-      throw new RuntimeException(\"IOE conversion\", e);\n-    }\n     if (this.heap == null) {\n-      return null;\n+      return this.lastTop;\n     }\n-\n     return this.heap.peek();\n   }\n ",
                "deletions": 7
            },
            {
                "sha": "1b5fb25ee7036cc0c42b4f76f7f52e70ad357f03",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/b5a5aa63814b93b567410867b6824f3b33124c4e/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/b5a5aa63814b93b567410867b6824f3b33124c4e/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java?ref=b5a5aa63814b93b567410867b6824f3b33124c4e",
                "patch": "@@ -452,6 +452,8 @@ public void testScannerReseekDoesntNPE() throws Exception {\n     scan.updateReaders();\n \n     scan.updateReaders();\n+\n+    scan.peek();\n   }\n \n ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2806 DNS hiccups cause uncaught NPE in HServerAddress#getBindAddress\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@960650 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/2993d97c97a69906b6f790bbc0608848f26d5b48",
        "parent": "https://github.com/apache/hbase/commit/03933720fa438d575c7ccbfe67421b3fb0d90a0b",
        "bug_id": "hbase_227",
        "file": [
            {
                "sha": "38be4c9c141565df75f2a27f0c11ba7184ecf2a2",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/2993d97c97a69906b6f790bbc0608848f26d5b48/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/2993d97c97a69906b6f790bbc0608848f26d5b48/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2993d97c97a69906b6f790bbc0608848f26d5b48",
                "patch": "@@ -426,6 +426,8 @@ Release 0.21.0 - Unreleased\n    HBASE-2707  Can't recover from a dead ROOT server if any exceptions happens\n                during log splitting\n    HBASE-2501  Refactor StoreFile Code\n+   HBASE-2806  DNS hiccups cause uncaught NPE in HServerAddress#getBindAddress\n+               (Benoit Sigoure via Stack)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "0fff4476feea9103bb72a346bb3794e8864748d6",
                "filename": "src/main/java/org/apache/hadoop/hbase/HServerAddress.java",
                "blob_url": "https://github.com/apache/hbase/blob/2993d97c97a69906b6f790bbc0608848f26d5b48/src/main/java/org/apache/hadoop/hbase/HServerAddress.java",
                "raw_url": "https://github.com/apache/hbase/raw/2993d97c97a69906b6f790bbc0608848f26d5b48/src/main/java/org/apache/hadoop/hbase/HServerAddress.java",
                "status": "modified",
                "changes": 61,
                "additions": 39,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HServerAddress.java?ref=2993d97c97a69906b6f790bbc0608848f26d5b48",
                "patch": "@@ -19,12 +19,14 @@\n  */\n package org.apache.hadoop.hbase;\n \n-import org.apache.hadoop.io.*;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.io.WritableComparable;\n \n import java.io.DataInput;\n import java.io.DataOutput;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.net.InetAddress;\n \n /**\n  * HServerAddress is a \"label\" for a HBase server made of host and port number.\n@@ -39,28 +41,29 @@ public HServerAddress() {\n   }\n \n   /**\n-   * Construct a HServerAddress from an InetSocketAddress\n+   * Construct an instance from an {@link InetSocketAddress}.\n    * @param address InetSocketAddress of server\n    */\n   public HServerAddress(InetSocketAddress address) {\n     this.address = address;\n     this.stringValue = address.getAddress().getHostAddress() + \":\" +\n       address.getPort();\n+    checkBindAddressCanBeResolved();\n   }\n \n   /**\n    * @param hostAndPort Hostname and port formatted as <code>&lt;hostname> ':' &lt;port></code>\n    */\n   public HServerAddress(String hostAndPort) {\n     int colonIndex = hostAndPort.lastIndexOf(':');\n-    if(colonIndex < 0) {\n+    if (colonIndex < 0) {\n       throw new IllegalArgumentException(\"Not a host:port pair: \" + hostAndPort);\n     }\n     String host = hostAndPort.substring(0, colonIndex);\n-    int port =\n-      Integer.valueOf(hostAndPort.substring(colonIndex + 1)).intValue();\n+    int port = Integer.parseInt(hostAndPort.substring(colonIndex + 1));\n     this.address = new InetSocketAddress(host, port);\n     this.stringValue = hostAndPort;\n+    checkBindAddressCanBeResolved();\n   }\n \n   /**\n@@ -70,46 +73,61 @@ public HServerAddress(String hostAndPort) {\n   public HServerAddress(String bindAddress, int port) {\n     this.address = new InetSocketAddress(bindAddress, port);\n     this.stringValue = bindAddress + \":\" + port;\n+    checkBindAddressCanBeResolved();\n   }\n \n   /**\n-   * Copy-constructor\n-   *\n+   * Copy-constructor.\n    * @param other HServerAddress to copy from\n    */\n   public HServerAddress(HServerAddress other) {\n     String bindAddress = other.getBindAddress();\n     int port = other.getPort();\n     this.address = new InetSocketAddress(bindAddress, port);\n-    stringValue = bindAddress + \":\" + port;\n+    stringValue = other.stringValue;\n+    checkBindAddressCanBeResolved();\n   }\n \n   /** @return Bind address */\n   public String getBindAddress() {\n-    return this.address.getAddress().getHostAddress();\n+    final InetAddress addr = address.getAddress();\n+    if (addr != null) {\n+      return addr.getHostAddress();\n+    } else {\n+      LogFactory.getLog(HServerAddress.class).error(\"Could not resolve the\"\n+          + \" DNS name of \" + stringValue);\n+      return null;\n+    }\n+  }\n+\n+  private checkBindAddressCanBeResolved() {\n+    if (getBindAddress() == null) {\n+      throw new IllegalArgumentException(\"Could not resolve the\"\n+          + \" DNS name of \" + stringValue);\n+    }\n   }\n \n   /** @return Port number */\n   public int getPort() {\n-    return this.address.getPort();\n+    return address.getPort();\n   }\n \n   /** @return Hostname */\n   public String getHostname() {\n-    return this.address.getHostName();\n+    return address.getHostName();\n   }\n \n   /** @return The InetSocketAddress */\n   public InetSocketAddress getInetSocketAddress() {\n-    return this.address;\n+    return address;\n   }\n \n   /**\n    * @return String formatted as <code>&lt;bind address> ':' &lt;port></code>\n    */\n   @Override\n   public String toString() {\n-    return (this.stringValue == null ? \"\" : this.stringValue);\n+    return stringValue == null ? \"\" : stringValue;\n   }\n \n   @Override\n@@ -123,13 +141,13 @@ public boolean equals(Object o) {\n     if (getClass() != o.getClass()) {\n       return false;\n     }\n-    return this.compareTo((HServerAddress)o) == 0;\n+    return compareTo((HServerAddress) o) == 0;\n   }\n \n   @Override\n   public int hashCode() {\n-    int result = this.address.hashCode();\n-    result ^= this.stringValue.hashCode();\n+    int result = address.hashCode();\n+    result ^= stringValue.hashCode();\n     return result;\n   }\n \n@@ -141,21 +159,20 @@ public void readFields(DataInput in) throws IOException {\n     String bindAddress = in.readUTF();\n     int port = in.readInt();\n \n-    if(bindAddress == null || bindAddress.length() == 0) {\n+    if (bindAddress == null || bindAddress.length() == 0) {\n       address = null;\n       stringValue = null;\n-\n     } else {\n       address = new InetSocketAddress(bindAddress, port);\n       stringValue = bindAddress + \":\" + port;\n+      checkBindAddressCanBeResolved();\n     }\n   }\n \n   public void write(DataOutput out) throws IOException {\n     if (address == null) {\n       out.writeUTF(\"\");\n       out.writeInt(0);\n-\n     } else {\n       out.writeUTF(address.getAddress().getHostAddress());\n       out.writeInt(address.getPort());\n@@ -170,7 +187,7 @@ public int compareTo(HServerAddress o) {\n     // Addresses as Strings may not compare though address is for the one\n     // server with only difference being that one address has hostname\n     // resolved whereas other only has IP.\n-    if (this.address.equals(o.address)) return 0;\n-    return this.toString().compareTo(o.toString());\n+    if (address.equals(o.address)) return 0;\n+    return toString().compareTo(o.toString());\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 22
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2740 NPE in ReadWriteConsistencyControl (build fix)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@955800 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/98a6a203c0630e63ba497211308240aebb708c67",
        "parent": "https://github.com/apache/hbase/commit/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
        "bug_id": "hbase_228",
        "file": [
            {
                "sha": "695bffbe9635decbba908edb31d53147dfd70b8b",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/98a6a203c0630e63ba497211308240aebb708c67/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/98a6a203c0630e63ba497211308240aebb708c67/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "status": "modified",
                "changes": 16,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java?ref=98a6a203c0630e63ba497211308240aebb708c67",
                "patch": "@@ -256,7 +256,7 @@ public void testDeleteVersionsMixedAndMultipleVersionReturn() throws IOException\n         KeyValueTestUtil.create(\"R2\", \"cf\", \"z\", now, KeyValue.Type.Put, \"dont-care\")\n     };\n     List<KeyValueScanner> scanners = scanFixture(kvs1, kvs2);\n-    \n+\n     Scan scanSpec = new Scan(Bytes.toBytes(\"R1\")).setMaxVersions(2);\n     StoreScanner scan =\n       new StoreScanner(scanSpec, CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n@@ -378,9 +378,7 @@ public void testDeleteColumn() throws IOException {\n     };\n \n   public void testSkipColumn() throws IOException {\n-    KeyValueScanner [] scanners = new KeyValueScanner[] {\n-        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n-    };\n+    List<KeyValueScanner> scanners = scanFixture(kvs);\n     StoreScanner scan =\n       new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n           getCols(\"a\", \"d\"), scanners);\n@@ -441,9 +439,7 @@ public void testWildCardTtlScan() throws IOException {\n   }\n \n   public void testScannerReseekDoesntNPE() throws Exception {\n-    KeyValueScanner [] scanners = new KeyValueScanner[] {\n-        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n-    };\n+    List<KeyValueScanner> scanners = scanFixture(kvs);\n     StoreScanner scan =\n         new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n             getCols(\"a\", \"d\"), scanners);\n@@ -457,8 +453,8 @@ public void testScannerReseekDoesntNPE() throws Exception {\n \n     scan.updateReaders();\n   }\n-    \n-  \n+\n+\n   /**\n    * TODO this fails, since we don't handle deletions, etc, in peek\n    */\n@@ -472,6 +468,6 @@ public void SKIP_testPeek() throws Exception {\n     StoreScanner scan =\n       new StoreScanner(scanSpec, CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n           getCols(\"a\"), scanners);\n-    assertNull(scan.peek());    \n+    assertNull(scan.peek());\n   }\n }",
                "deletions": 10
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2740  NPE in ReadWriteConsistencyControl\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@955784 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
        "parent": "https://github.com/apache/hbase/commit/7aedf3a36f8adcd50efb37bf1eb4a67cfdf6f67b",
        "bug_id": "hbase_229",
        "file": [
            {
                "sha": "0902c1f2957c99a01cdb7453750c38cce3447f34",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
                "patch": "@@ -394,6 +394,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2738  TestTimeRangeMapRed updated now that we keep multiple cells with\n                same timestamp in MemStore\n    HBASE-2725  Shutdown hook management is gone in trunk; restore\n+   HBASE-2740  NPE in ReadWriteConsistencyControl\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "7680a222cb5a9164ade205e222582ea2cf92e67b",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "status": "modified",
                "changes": 11,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
                "patch": "@@ -31,7 +31,7 @@\n import java.util.NavigableSet;\n \n /**\n- * Scanner scans both the memstore and the HStore. Coaleace KeyValue stream\n+ * Scanner scans both the memstore and the HStore. Coalesce KeyValue stream\n  * into List<KeyValue> for a single row.\n  */\n class StoreScanner implements KeyValueScanner, InternalScanner, ChangedReadersObserver {\n@@ -194,6 +194,8 @@ public synchronized void close() {\n       this.store.deleteChangedReaderObserver(this);\n     if (this.heap != null)\n       this.heap.close();\n+    this.heap = null; // CLOSED!\n+    this.lastTop = null; // If both are null, we are closed.\n   }\n \n   public synchronized boolean seek(KeyValue key) throws IOException {\n@@ -298,6 +300,13 @@ public synchronized boolean next(List<KeyValue> outResult) throws IOException {\n   public synchronized void updateReaders() throws IOException {\n     if (this.closing) return;\n \n+    // All public synchronized API calls will call 'checkReseek' which will cause\n+    // the scanner stack to reseek if this.heap==null && this.lastTop != null.\n+    // But if two calls to updateReaders() happen without a 'next' or 'peek' then we\n+    // will end up calling this.peek() which would cause a reseek in the middle of a updateReaders\n+    // which is NOT what we want, not to mention could cause an NPE. So we early out here.\n+    if (this.heap == null) return;\n+\n     // this could be null.\n     this.lastTop = this.peek();\n ",
                "deletions": 1
            },
            {
                "sha": "af95a5c2b263cf092bcdd3c08d0f41594a833b17",
                "filename": "src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java",
                "status": "modified",
                "changes": 34,
                "additions": 28,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java?ref=0beea3751b66efbc5cc87cf2b6c52ca9b3b4778c",
                "patch": "@@ -109,7 +109,6 @@ public void testScanTimeRange() throws IOException {\n     results = new ArrayList<KeyValue>();\n     assertEquals(true, scan.next(results));\n     assertEquals(3, results.size());\n-\n   }\n \n   public void testScanSameTimestamp() throws IOException {\n@@ -285,6 +284,7 @@ public void testWildCardOneVersionScan() throws IOException {\n     assertEquals(kvs[0], results.get(0));\n     assertEquals(kvs[1], results.get(1));\n   }\n+\n   public void testWildCardScannerUnderDeletes() throws IOException {\n     KeyValue [] kvs = new KeyValue [] {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 2, KeyValue.Type.Put, \"dont-care\"), // inc\n@@ -317,6 +317,7 @@ public void testWildCardScannerUnderDeletes() throws IOException {\n     assertEquals(kvs[6], results.get(3));\n     assertEquals(kvs[7], results.get(4));\n   }\n+\n   public void testDeleteFamily() throws IOException {\n     KeyValue [] kvs = new KeyValue[] {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 100, KeyValue.Type.DeleteFamily, \"dont-care\"),\n@@ -363,8 +364,7 @@ public void testDeleteColumn() throws IOException {\n     assertEquals(kvs[3], results.get(0));\n   }\n \n-  public void testSkipColumn() throws IOException {\n-    KeyValue [] kvs = new KeyValue[] {\n+  private static final  KeyValue [] kvs = new KeyValue[] {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 11, KeyValue.Type.Put, \"dont-care\"),\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"b\", 11, KeyValue.Type.Put, \"dont-care\"),\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"c\", 11, KeyValue.Type.Put, \"dont-care\"),\n@@ -376,7 +376,11 @@ public void testSkipColumn() throws IOException {\n         KeyValueTestUtil.create(\"R1\", \"cf\", \"i\", 11, KeyValue.Type.Put, \"dont-care\"),\n         KeyValueTestUtil.create(\"R2\", \"cf\", \"a\", 11, KeyValue.Type.Put, \"dont-care\"),\n     };\n-    List<KeyValueScanner> scanners = scanFixture(kvs);\n+\n+  public void testSkipColumn() throws IOException {\n+    KeyValueScanner [] scanners = new KeyValueScanner[] {\n+        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n+    };\n     StoreScanner scan =\n       new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n           getCols(\"a\", \"d\"), scanners);\n@@ -395,9 +399,9 @@ public void testSkipColumn() throws IOException {\n     results.clear();\n     assertEquals(false, scan.next(results));\n   }\n-  \n+\n   /*\n-   * Test expiration of KeyValues in combination with a configured TTL for \n+   * Test expiration of KeyValues in combination with a configured TTL for\n    * a column family (as should be triggered in a major compaction).\n    */\n   public void testWildCardTtlScan() throws IOException {\n@@ -435,6 +439,24 @@ public void testWildCardTtlScan() throws IOException {\n \n     assertEquals(false, scanner.next(results));\n   }\n+\n+  public void testScannerReseekDoesntNPE() throws Exception {\n+    KeyValueScanner [] scanners = new KeyValueScanner[] {\n+        new KeyValueScanFixture(KeyValue.COMPARATOR, kvs)\n+    };\n+    StoreScanner scan =\n+        new StoreScanner(new Scan(), CF, Long.MAX_VALUE, KeyValue.COMPARATOR,\n+            getCols(\"a\", \"d\"), scanners);\n+\n+\n+    // Previously a updateReaders twice in a row would cause an NPE.  In test this would also\n+    // normally cause an NPE because scan.store is null.  So as long as we get through these\n+    // two calls we are good and the bug was quashed.\n+\n+    scan.updateReaders();\n+\n+    scan.updateReaders();\n+  }\n     \n   \n   /**",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2614 killing server in TestMasterTransitions causes NPEs and test deadlock\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@951652 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
        "parent": "https://github.com/apache/hbase/commit/9cacbb074cb070d7565e8215df12771a56265d31",
        "bug_id": "hbase_230",
        "file": [
            {
                "sha": "ab2232bdbc131b00fa5e8b5a8b9ea2b56526178b",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -373,6 +373,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2657  TestTableResource is broken in trunk\n    HBASE-2662  TestScannerResource.testScannerResource broke in trunk\n    HBASE-2667  TestHLog.testSplit failing in trunk\n+   HBASE-2614  killing server in TestMasterTransitions causes NPEs and test deadlock\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "c3935accd4900434d7dbae90e43cfc4cd5e77dad",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -589,6 +589,7 @@ public void interruptAndStop() {\n     synchronized(scannerLock){\n       if (isAlive()) {\n         super.interrupt();\n+        LOG.info(\"Interrupted\");\n       }\n     }\n   }",
                "deletions": 0
            },
            {
                "sha": "5946eee5a65901b22187ce672639e6aac8eb9354",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -447,6 +447,9 @@ public void run() {\n           if (this.serverManager.numServers() == 0) {\n             startShutdown();\n             break;\n+          } else {\n+            LOG.debug(\"Waiting on \" +\n+              this.serverManager.getServersToServerInfo().keySet().toString());\n           }\n         }\n         final HServerAddress root = this.regionManager.getRootRegionLocation();",
                "deletions": 0
            },
            {
                "sha": "71597afcb476b335fe76ff0eee1da36dc6c23f2f",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "status": "modified",
                "changes": 16,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/RegionManager.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -592,17 +592,8 @@ public boolean metaRegionsInTransition() {\n    * regions can shut down.\n    */\n   public void stopScanners() {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"telling root scanner to stop\");\n-    }\n-    rootScannerThread.interruptAndStop();\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"telling meta scanner to stop\");\n-    }\n-    metaScannerThread.interruptAndStop();\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"meta and root scanners notified\");\n-    }\n+    this.rootScannerThread.interruptAndStop();\n+    this.metaScannerThread.interruptAndStop();\n   }\n \n   /** Stop the region assigner */\n@@ -1152,7 +1143,8 @@ public HServerAddress getRootRegionLocation() {\n    */\n   public void waitForRootRegionLocation() {\n     synchronized (rootRegionLocation) {\n-      while (!master.isClosed() && rootRegionLocation.get() == null) {\n+      while (!master.getShutdownRequested().get() &&\n+          !master.isClosed() && rootRegionLocation.get() == null) {\n         // rootRegionLocation will be filled in when we get an 'open region'\n         // regionServerReport message from the HRegionServer that has been\n         // allocated the ROOT region below.",
                "deletions": 12
            },
            {
                "sha": "9909f2c10a72c580b2974d37d5bb6c7c24345c0d",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -1121,7 +1121,9 @@ public void stop() {\n   public void abort() {\n     this.abortRequested = true;\n     this.reservedSpace.clear();\n-    LOG.info(\"Dump of metrics: \" + this.metrics.toString());\n+    if (this.metrics != null) {\n+      LOG.info(\"Dump of metrics: \" + this.metrics.toString());\n+    }\n     stop();\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "280b91da4a97bead9e81390d9bf0bf510f769c0c",
                "filename": "src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java",
                "status": "modified",
                "changes": 7,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -55,7 +55,12 @@ public HRegionServer getRegionServer() {\n      * to be used.\n      */\n     public void waitForServerOnline() {\n-      while (!regionServer.isOnline()) {\n+      // The server is marked online after the init method completes inside of\n+      // the HRS#run method.  HRS#init can fail for whatever region.  In those\n+      // cases, we'll jump out of the run without setting online flag.  Check\n+      // stopRequested so we don't wait here a flag that will never be flipped.\n+      while (!this.regionServer.isOnline() &&\n+          !this.regionServer.isStopRequested()) {\n         try {\n           Thread.sleep(1000);\n         } catch (InterruptedException e) {",
                "deletions": 1
            },
            {
                "sha": "30333cde654dfb17f27b92db8d83f6df049f9a4c",
                "filename": "src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java",
                "blob_url": "https://github.com/apache/hbase/blob/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java",
                "raw_url": "https://github.com/apache/hbase/raw/64c6a071d7d2ea1dd1827b0241f30615c0e8f082/src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java?ref=64c6a071d7d2ea1dd1827b0241f30615c0e8f082",
                "patch": "@@ -74,6 +74,10 @@\n    */\n   @BeforeClass public static void beforeAllTests() throws Exception {\n     TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n+    // Parcel out the regions, don't give them out in big lumps.  We've only\n+    // a few in this test.  Let a couple of cycles pass is more realistic and\n+    // gives stuff a chance to work.\n+    TEST_UTIL.getConfiguration().setInt(\"hbase.regions.percheckin\", 2);\n     // Start a cluster of two regionservers.\n     TEST_UTIL.startMiniCluster(2);\n     // Create a table of three families.  This will assign a region.",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2509 NPEs in various places, HRegion.get, HRS.close\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@944533 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3",
        "parent": "https://github.com/apache/hbase/commit/f65e61c7abe27fd718f1f4abb25fe59a20249aa6",
        "bug_id": "hbase_231",
        "file": [
            {
                "sha": "a385ce1f767696da4854e33bfd0863ac04da053b",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 10,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3",
                "patch": "@@ -1921,7 +1921,8 @@ public Path getBaseDir() {\n    * It is used to combine scanners from multiple Stores (aka column families).\n    */\n   class RegionScanner implements InternalScanner {\n-    private KeyValueHeap storeHeap = null;\n+    // Package local for testability\n+    KeyValueHeap storeHeap = null;\n     private final byte [] stopRow;\n     private Filter filter;\n     private List<KeyValue> results = new ArrayList<KeyValue>();\n@@ -2091,8 +2092,11 @@ private boolean hasResults() {\n     }\n \n     public synchronized void close() {\n-      storeHeap.close();\n-      this.filterClosed = true;\n+      if (storeHeap != null) {\n+        storeHeap.close();\n+        storeHeap = null;\n+      }\n+\t  this.filterClosed = true;\n     }\n \n     /**",
                "deletions": 3
            },
            {
                "sha": "482caa900a1ae8dfc806de7f52bf630bbf42f72f",
                "filename": "core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3/core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "status": "modified",
                "changes": 108,
                "additions": 98,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=ef4d353ab50e1fe8f1ee9f51e5f5fe942306a7e3",
                "patch": "@@ -41,11 +41,9 @@\n import org.apache.hadoop.hbase.filter.Filter;\n import org.apache.hadoop.hbase.filter.FilterList;\n import org.apache.hadoop.hbase.filter.PrefixFilter;\n-import org.apache.hadoop.hbase.filter.RowFilter;\n import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;\n import org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.junit.Assert;\n \n import java.io.IOException;\n import java.util.ArrayList;\n@@ -54,6 +52,9 @@\n import java.util.List;\n import java.util.Map;\n import java.util.TreeMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n \n /**\n  * Basic stand-alone testing of HRegion.\n@@ -92,6 +93,93 @@ protected void setUp() throws Exception {\n   // /tmp/testtable\n   //////////////////////////////////////////////////////////////////////////////\n \n+  public void testGetWhileRegionClose() throws IOException {\n+    HBaseConfiguration hc = initSplit();\n+    int numRows = 100;\n+    byte [][] families = {fam1, fam2, fam3};\n+    \n+    //Setting up region\n+    String method = this.getName();\n+    initHRegion(tableName, method, hc, families);\n+\n+    // Put data in region\n+    final int startRow = 100;\n+    putData(startRow, numRows, qual1, families);\n+    putData(startRow, numRows, qual2, families);\n+    putData(startRow, numRows, qual3, families);\n+    // this.region.flushcache();\n+    final AtomicBoolean done = new AtomicBoolean(false);\n+    final AtomicInteger gets = new AtomicInteger(0);\n+    GetTillDoneOrException [] threads = new GetTillDoneOrException[10];\n+    try {\n+      // Set ten threads running concurrently getting from the region.\n+      for (int i = 0; i < threads.length / 2; i++) {\n+        threads[i] = new GetTillDoneOrException(i, Bytes.toBytes(\"\" + startRow),\n+          done, gets);\n+        threads[i].setDaemon(true);\n+        threads[i].start();\n+      }\n+      // Artificially make the condition by setting closing flag explicitly.\n+      // I can't make the issue happen with a call to region.close().\n+      this.region.closing.set(true);\n+      for (int i = threads.length / 2; i < threads.length; i++) {\n+        threads[i] = new GetTillDoneOrException(i, Bytes.toBytes(\"\" + startRow),\n+          done, gets);\n+        threads[i].setDaemon(true);\n+        threads[i].start();\n+      }\n+    } finally {\n+      if (this.region != null) {\n+        this.region.close();\n+        this.region.getLog().closeAndDelete();\n+      }\n+    }\n+    done.set(true);\n+    for (GetTillDoneOrException t: threads) {\n+      try {\n+        t.join();\n+      } catch (InterruptedException e) {\n+        e.printStackTrace();\n+      }\n+      if (t.e != null) {\n+        LOG.info(\"Exception=\" + t.e);\n+        assertFalse(\"Found a NPE in \" + t.getName(),\n+          t.e instanceof NullPointerException);\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Thread that does get on single row until 'done' flag is flipped.  If an\n+   * exception causes us to fail, it records it.\n+   */\n+  class GetTillDoneOrException extends Thread {\n+    private final Get g;\n+    private final AtomicBoolean done;\n+    private final AtomicInteger count;\n+    private Exception e;\n+\n+    GetTillDoneOrException(final int i, final byte[] r, final AtomicBoolean d,\n+        final AtomicInteger c) {\n+      super(\"getter.\" + i);\n+      this.g = new Get(r);\n+      this.done = d;\n+      this.count = c;\n+    }\n+\n+    @Override\n+    public void run() {\n+      while (!this.done.get()) {\n+        try {\n+          assertTrue(region.get(g, null).size() > 0);\n+          this.count.incrementAndGet();\n+        } catch (Exception e) {\n+          this.e = e;\n+          break;\n+        }\n+      }\n+    }\n+  }\n \n   /*\n    * An involved filter test.  Has multiple column families and deletes in mix.\n@@ -1034,13 +1122,13 @@ public void testGetScanner_WithNoFamilies() throws IOException {\n     scan.addFamily(fam4);\n     is = (RegionScanner) region.getScanner(scan);\n     is.initHeap(); // i dont like this test\n-    assertEquals(1, ((RegionScanner)is).getStoreHeap().getHeap().size());\n-\n+    assertEquals(1, ((RegionScanner)is).storeHeap.getHeap().size());\n+    \n     scan = new Scan();\n     is = (RegionScanner) region.getScanner(scan);\n     is.initHeap();\n     assertEquals(families.length -1, \n-        ((RegionScanner)is).getStoreHeap().getHeap().size());\n+        ((RegionScanner)is).storeHeap.getHeap().size());\n   }\n \n   public void testRegionScanner_Next() throws IOException {\n@@ -1921,7 +2009,7 @@ public void testFlushCacheWhileScanning() throws IOException, InterruptedExcepti\n         if (!toggle) {\n           flushThread.flush();\n         }\n-        Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n+        assertEquals(\"i=\" + i, expectedCount, res.size());\n         toggle = !toggle;\n       }\n     }\n@@ -1944,7 +2032,7 @@ public void done() {\n \n     public void checkNoError() {\n       if (error != null) {\n-        Assert.assertNull(error);\n+        assertNull(error);\n       }\n     }\n \n@@ -2082,7 +2170,7 @@ public void done() {\n \n     public void checkNoError() {\n       if (error != null) {\n-        Assert.assertNull(error);\n+        assertNull(error);\n       }\n     }\n \n@@ -2175,7 +2263,7 @@ public void testWritesWhileGetting()\n       boolean previousEmpty = result == null || result.isEmpty();\n       result = region.get(get, null);\n       if (!result.isEmpty() || !previousEmpty || i > compactInterval) {\n-        Assert.assertEquals(\"i=\" + i, expectedCount, result.size());\n+        assertEquals(\"i=\" + i, expectedCount, result.size());\n         // TODO this was removed, now what dangit?!\n         // search looking for the qualifier in question?\n         long timestamp = 0;\n@@ -2185,7 +2273,7 @@ public void testWritesWhileGetting()\n             timestamp = kv.getTimestamp();\n           }\n         }\n-        Assert.assertTrue(timestamp >= prevTimestamp);\n+        assertTrue(timestamp >= prevTimestamp);\n         prevTimestamp = timestamp;\n \n         byte [] gotValue = null;",
                "deletions": 10
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2398 NPE in HLog.append when calling writer.getLength\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@930129 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/4503dbe136d193ca5cde96fb1476d6b901b67606",
        "parent": "https://github.com/apache/hbase/commit/4cbfa4b5941bdd4baa7ccdf7b0e3c358d7c45eea",
        "bug_id": "hbase_232",
        "file": [
            {
                "sha": "fb3701a29710d8f5198a91343ed4989ce0e750e0",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/4503dbe136d193ca5cde96fb1476d6b901b67606/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/4503dbe136d193ca5cde96fb1476d6b901b67606/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=4503dbe136d193ca5cde96fb1476d6b901b67606",
                "patch": "@@ -261,6 +261,8 @@ Release 0.21.0 - Unreleased\n                different address/startcode than SCAN\"\n    HBASE-2361  WALEdit broke replication scope\n    HBASE-2365  Double-assignment around split\n+   HBASE-2398  NPE in HLog.append when calling writer.getLength\n+               (Kannan Muthukkaruppan via Stack)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "19c9b5437fbbf183e534f39457f2cefa20ae8aab",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java",
                "blob_url": "https://github.com/apache/hbase/blob/4503dbe136d193ca5cde96fb1476d6b901b67606/core/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java",
                "raw_url": "https://github.com/apache/hbase/raw/4503dbe136d193ca5cde96fb1476d6b901b67606/core/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java",
                "status": "modified",
                "changes": 15,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java?ref=4503dbe136d193ca5cde96fb1476d6b901b67606",
                "patch": "@@ -673,12 +673,6 @@ public void append(HRegionInfo regionInfo, HLogKey logKey, WALEdit logEdit,\n \n     // sync txn to file system\n     this.sync(isMetaRegion);\n-\n-    if (this.writer.getLength() > this.logrollsize) {\n-      if (listener != null) {\n-        listener.logRollRequested();\n-      }\n-    }\n   }\n \n   /**\n@@ -729,9 +723,6 @@ public void append(HRegionInfo info, byte [] tableName, WALEdit edits,\n     }\n     // sync txn to file system\n     this.sync(info.isMetaRegion());\n-    if (this.writer.getLength() > this.logrollsize) {\n-        requestLogRoll();\n-    }\n   }\n \n   /**\n@@ -840,6 +831,7 @@ public void hflush() throws IOException {\n       if (this.closed) {\n         return;\n       }\n+      boolean logRollRequested = false;\n       if (this.forceSync ||\n           this.unflushedEntries.get() >= this.flushlogentries) {\n         try {\n@@ -849,12 +841,17 @@ public void hflush() throws IOException {\n           syncOps++;\n           this.forceSync = false;\n           this.unflushedEntries.set(0);\n+          // TODO: HBASE-2401\n         } catch (IOException e) {\n           LOG.fatal(\"Could not append. Requesting close of hlog\", e);\n           requestLogRoll();\n           throw e;\n         }\n       }\n+\n+      if (!logRollRequested && (this.writer.getLength() > this.logrollsize)) {\n+        requestLogRoll();\n+      }\n     }\n   }\n ",
                "deletions": 9
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2026  NPE in StoreScanner on compaction\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@894219 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/f4406121af4ff600d2ff581e5dbe46ec34b06cca",
        "parent": "https://github.com/apache/hbase/commit/69a6ef5a9a8c09d8cf918751473240c5d77cfaae",
        "bug_id": "hbase_233",
        "file": [
            {
                "sha": "36591df27136788929bf6dbe97dad86e5176b692",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/f4406121af4ff600d2ff581e5dbe46ec34b06cca/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/f4406121af4ff600d2ff581e5dbe46ec34b06cca/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=f4406121af4ff600d2ff581e5dbe46ec34b06cca",
                "patch": "@@ -139,6 +139,7 @@ Release 0.21.0 - Unreleased\n                its regions around\n    HBASE-2065  Cannot disable a table if any of its region is opening \n                at the same time\n+   HBASE-2026  NPE in StoreScanner on compaction\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "d30432516294e98680fa5d4737e546cf3c0aa834",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/f4406121af4ff600d2ff581e5dbe46ec34b06cca/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/f4406121af4ff600d2ff581e5dbe46ec34b06cca/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=f4406121af4ff600d2ff581e5dbe46ec34b06cca",
                "patch": "@@ -255,6 +255,7 @@ public synchronized void updateReaders() throws IOException {\n \n     // Reset the state of the Query Matcher and set to top row\n     matcher.reset();\n-    matcher.setRow(heap.peek().getRow());\n+    KeyValue kv = heap.peek();\n+    matcher.setRow((kv == null ? topKey : kv).getRow());\n   }\n }\n\\ No newline at end of file",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2022  NPE in housekeeping kills RS\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@888193 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/28b6d90a93b4949a054a3b103fb7b34b027f0175",
        "parent": "https://github.com/apache/hbase/commit/5c6931f26315a3be8579a0483fcd8f4050de954a",
        "bug_id": "hbase_234",
        "file": [
            {
                "sha": "ca7a7fc66f270b84137b1a133eb55c571243b65f",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/28b6d90a93b4949a054a3b103fb7b34b027f0175/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/28b6d90a93b4949a054a3b103fb7b34b027f0175/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=28b6d90a93b4949a054a3b103fb7b34b027f0175",
                "patch": "@@ -121,6 +121,7 @@ Release 0.21.0 - Unreleased\n    HBASE-2018  Updates to .META. blocked under high MemStore load\n    HBASE-1994  Master will lose hlog entries while splitting if region has\n                empty oldlogfile.log (Lars George via Stack)\n+   HBASE-2022  NPE in housekeeping kills RS\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "26fc18818395c8928c2d2599bf3d7485e39b8b59",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/28b6d90a93b4949a054a3b103fb7b34b027f0175/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/28b6d90a93b4949a054a3b103fb7b34b027f0175/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=28b6d90a93b4949a054a3b103fb7b34b027f0175",
                "patch": "@@ -1122,9 +1122,13 @@ private void housekeeping() {\n     if (this.toDo.isEmpty()) {\n       return;\n     }\n-    // This iterator is 'safe'.  We are guaranteed a view on state of the\n-    // queue at time iterator was taken out.  Apparently goes from oldest.\n+    // This iterator isn't safe if elements are gone and HRS.Worker could\n+    // remove them (it already checks for null there). Goes from oldest.\n     for (ToDoEntry e: this.toDo) {\n+      if(e == null) {\n+        LOG.warn(\"toDo gave a null entry during iteration\");\n+        break;\n+      }\n       HMsg msg = e.msg;\n       if (msg != null) {\n         if (msg.isType(HMsg.Type.MSG_REGION_OPEN)) {",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1865 0.20.0 TableInputFormatBase NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@818651 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8525e1ee23a269ba64821d26060a3ef724e307eb",
        "parent": "https://github.com/apache/hbase/commit/dadeb0bdde6461f9fc061c1a316d85045038eca8",
        "bug_id": "hbase_235",
        "file": [
            {
                "sha": "3df60cc6deb9fe206c30fa04129779713aca22ea",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/8525e1ee23a269ba64821d26060a3ef724e307eb/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/8525e1ee23a269ba64821d26060a3ef724e307eb/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8525e1ee23a269ba64821d26060a3ef724e307eb",
                "patch": "@@ -44,6 +44,7 @@ Release 0.21.0 - Unreleased\n                split (Cosmin Lehane via Stack)\n    HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n    HBASE-1859  Misc shell fixes patch (Kyle Oba via Stack)\n+   HBASE-1865  0.20.0 TableInputFormatBase NPE\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "660b27171d7f900bd3fafe31fce4300b5c4d3c4b",
                "filename": "src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/8525e1ee23a269ba64821d26060a3ef724e307eb/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/8525e1ee23a269ba64821d26060a3ef724e307eb/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=8525e1ee23a269ba64821d26060a3ef724e307eb",
                "patch": "@@ -269,13 +269,13 @@ public float getProgress() {\n    */\n   @Override\n   public List<InputSplit> getSplits(JobContext context) throws IOException {\n+    if (table == null) {\n+      throw new IOException(\"No table was provided.\");\n+    }\n     byte [][] startKeys = table.getStartKeys();\n     if (startKeys == null || startKeys.length == 0) {\n       throw new IOException(\"Expecting at least one region.\");\n     }\n-    if (table == null) {\n-      throw new IOException(\"No table was provided.\");\n-    }\n     int realNumSplits = startKeys.length;\n     InputSplit[] splits = new InputSplit[realNumSplits];\n     int middle = startKeys.length / realNumSplits;",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1809 NPE thrown in BoundedRangeFileInputStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@817910 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/873bb2b7c35dd041a116f8b15c42c62558ce82b0",
        "parent": "https://github.com/apache/hbase/commit/23e790f50b43d59c27d9c75147ddb2f712df5703",
        "bug_id": "hbase_236",
        "file": [
            {
                "sha": "d582a2aa0043735687096614587f9ce05208fbee",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/873bb2b7c35dd041a116f8b15c42c62558ce82b0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/873bb2b7c35dd041a116f8b15c42c62558ce82b0/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=873bb2b7c35dd041a116f8b15c42c62558ce82b0",
                "patch": "@@ -42,6 +42,7 @@ Release 0.21.0 - Unreleased\n                (Lars George via Stack)\n    HBASE-1857  WrongRegionException when setting region online after .META.\n                split (Cosmin Lehane via Stack)\n+   HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "c1f4c64032d821567adff57d0c0624c30eb5b305",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "blob_url": "https://github.com/apache/hbase/blob/873bb2b7c35dd041a116f8b15c42c62558ce82b0/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "raw_url": "https://github.com/apache/hbase/raw/873bb2b7c35dd041a116f8b15c42c62558ce82b0/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "status": "modified",
                "changes": 50,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=873bb2b7c35dd041a116f8b15c42c62558ce82b0",
                "patch": "@@ -1406,34 +1406,38 @@ public void get(Get get, NavigableSet<byte[]> columns, List<KeyValue> result)\n     // Column matching and version enforcement\n     QueryMatcher matcher = new QueryMatcher(get, this.family.getName(), columns,\n       this.ttl, keyComparator, versionsToReturn(get.getMaxVersions()));\n+    this.lock.readLock().lock();\n+    try {\n+      // Read from memstore\n+      if(this.memstore.get(matcher, result)) {\n+        // Received early-out from memstore\n+        return;\n+      }\n     \n-    // Read from memstore\n-    if(this.memstore.get(matcher, result)) {\n-      // Received early-out from memstore\n-      return;\n-    }\n-    \n-    // Check if we even have storefiles\n-    if (this.storefiles.isEmpty()) {\n-      return;\n-    }\n+      // Check if we even have storefiles\n+      if (this.storefiles.isEmpty()) {\n+        return;\n+      }\n     \n-    // Get storefiles for this store\n-    List<HFileScanner> storefileScanners = new ArrayList<HFileScanner>();\n-    for (StoreFile sf : this.storefiles.descendingMap().values()) {\n-      HFile.Reader r = sf.getReader();\n-      if (r == null) {\n-        LOG.warn(\"StoreFile \" + sf + \" has a null Reader\");\n-        continue;\n+      // Get storefiles for this store\n+      List<HFileScanner> storefileScanners = new ArrayList<HFileScanner>();\n+      for (StoreFile sf : this.storefiles.descendingMap().values()) {\n+        HFile.Reader r = sf.getReader();\n+        if (r == null) {\n+          LOG.warn(\"StoreFile \" + sf + \" has a null Reader\");\n+          continue;\n+        }\n+        storefileScanners.add(r.getScanner());\n       }\n-      storefileScanners.add(r.getScanner());\n-    }\n     \n-    // StoreFileGetScan will handle reading this store's storefiles\n-    StoreFileGetScan scanner = new StoreFileGetScan(storefileScanners, matcher);\n+      // StoreFileGetScan will handle reading this store's storefiles\n+      StoreFileGetScan scanner = new StoreFileGetScan(storefileScanners, matcher);\n     \n-    // Run a GET scan and put results into the specified list \n-    scanner.get(result);\n+      // Run a GET scan and put results into the specified list \n+      scanner.get(result);\n+    } finally {\n+      this.lock.readLock().unlock();\n+    }\n   }\n \n   /**",
                "deletions": 23
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1809 NPE thrown in BoundedRangeFileInputStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@810301 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/231e0c0043ddbe5297cb8f0716e6de5a7381892d",
        "parent": "https://github.com/apache/hbase/commit/dea3c1207cd0b60ddb3bbda3ae120d5efba97121",
        "bug_id": "hbase_237",
        "file": [
            {
                "sha": "b057b966b1a05ead1000a665159ecb2c3c7d3625",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/231e0c0043ddbe5297cb8f0716e6de5a7381892d/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/231e0c0043ddbe5297cb8f0716e6de5a7381892d/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=231e0c0043ddbe5297cb8f0716e6de5a7381892d",
                "patch": "@@ -362,6 +362,7 @@ Release 0.20.0 - Unreleased\n                hudson too\n    HBASE-1780  HTable.flushCommits clears write buffer in finally clause\n    HBASE-1784  Missing rows after medium intensity insert\n+   HBASE-1809  NPE thrown in BoundedRangeFileInputStream\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "f67fea35dc4ad04a89a1386536972f4d3303426b",
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/231e0c0043ddbe5297cb8f0716e6de5a7381892d/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/231e0c0043ddbe5297cb8f0716e6de5a7381892d/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "status": "modified",
                "changes": 32,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java?ref=231e0c0043ddbe5297cb8f0716e6de5a7381892d",
                "patch": "@@ -967,18 +967,26 @@ ByteBuffer readBlock(int block, boolean cacheBlock) throws IOException {\n     private ByteBuffer decompress(final long offset, final int compressedSize,\n       final int decompressedSize) \n     throws IOException {\n-      Decompressor decompressor = this.compressAlgo.getDecompressor();\n-      // My guess is that the bounded range fis is needed to stop the \n-      // decompressor reading into next block -- IIRC, it just grabs a\n-      // bunch of data w/o regard to whether decompressor is coming to end of a\n-      // decompression.\n-      InputStream is = this.compressAlgo.createDecompressionStream(\n-        new BoundedRangeFileInputStream(this.istream, offset, compressedSize),\n-        decompressor, 0);\n-      ByteBuffer buf = ByteBuffer.allocate(decompressedSize);\n-      IOUtils.readFully(is, buf.array(), 0, buf.capacity());\n-      is.close();\n-      this.compressAlgo.returnDecompressor(decompressor);\n+      \n+      Decompressor decompressor = null;\n+      \n+      try {\n+        decompressor = this.compressAlgo.getDecompressor();\n+        // My guess is that the bounded range fis is needed to stop the \n+        // decompressor reading into next block -- IIRC, it just grabs a\n+        // bunch of data w/o regard to whether decompressor is coming to end of a\n+        // decompression.\n+        InputStream is = this.compressAlgo.createDecompressionStream(\n+          new BoundedRangeFileInputStream(this.istream, offset, compressedSize),\n+          decompressor, 0);\n+        ByteBuffer buf = ByteBuffer.allocate(decompressedSize);\n+        IOUtils.readFully(is, buf.array(), 0, buf.capacity());\n+        is.close();        \n+      } finally {\n+        if (null != decompressor) {\n+          this.compressAlgo.returnDecompressor(decompressor);          \n+        }\n+      }\n       return buf;\n     }\n ",
                "deletions": 12
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1693 NPE close_region .META. in shell\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@797851 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b",
        "parent": "https://github.com/apache/hbase/commit/247f508b8ec4e25e5ad56525e3a4bedf17ba7938",
        "bug_id": "hbase_238",
        "file": [
            {
                "sha": "dc782399c101acc7abdbe41c29fb489e508f0fec",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=64e7770cb7c2a35cbc91161bbf3551c1d5190d0b",
                "patch": "@@ -284,6 +284,7 @@ Release 0.20.0 - Unreleased\n                defined types (Clint Morgan via Stack and Jon Gray)\n    HBASE-1607  transactions / indexing fixes: trx deletes not handeled, index\n                scan can't specify stopRow (Clint Morgan via Stack)\n+   HBASE-1693  NPE close_region \".META.\" in shell\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "cfa79f4e8b69583313a45768792f1fef7d233721",
                "filename": "bin/HBase.rb",
                "blob_url": "https://github.com/apache/hbase/blob/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/bin/HBase.rb",
                "raw_url": "https://github.com/apache/hbase/raw/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/bin/HBase.rb",
                "status": "modified",
                "changes": 15,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/bin/HBase.rb?ref=64e7770cb7c2a35cbc91161bbf3551c1d5190d0b",
                "patch": "@@ -13,6 +13,8 @@\n \n import org.apache.hadoop.hbase.client.HBaseAdmin\n import org.apache.hadoop.hbase.client.HTable\n+import org.apache.hadoop.hbase.client.Get\n+import org.apache.hadoop.hbase.client.Put\n import org.apache.hadoop.hbase.client.Delete\n import org.apache.hadoop.hbase.HConstants\n import org.apache.hadoop.hbase.io.BatchUpdate\n@@ -144,13 +146,16 @@ def online(regionName, onOrOff)\n       now = Time.now \n       meta = HTable.new(HConstants::META_TABLE_NAME)\n       bytes = Bytes.toBytes(regionName)\n-      hriBytes = meta.get(bytes, HConstants::COL_REGIONINFO).getValue()\n+      g = Get.new(bytes)\n+      g.addColumn(HConstants::CATALOG_FAMILY,\n+        HConstants::REGIONINFO_QUALIFIER)\n+      hriBytes = meta.get(g).value()\n       hri = Writables.getWritable(hriBytes, HRegionInfo.new());\n       hri.setOffline(onOrOff)\n-      p hri\n-      bu = BatchUpdate.new(bytes)\n-      bu.put(HConstants::COL_REGIONINFO, Writables.getBytes(hri))\n-      meta.commit(bu);\n+      put = Put.new(bytes)\n+      put.add(HConstants::CATALOG_FAMILY,\n+        HConstants::REGIONINFO_QUALIFIER, Writables.getBytes(hri))\n+      meta.put(put);\n       @formatter.header()\n       @formatter.footer(now)\n     end",
                "deletions": 5
            },
            {
                "sha": "0e040475da8d8896ae2663daba8d7d7b8f849753",
                "filename": "src/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/src/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/src/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 2,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/HMaster.java?ref=64e7770cb7c2a35cbc91161bbf3551c1d5190d0b",
                "patch": "@@ -950,10 +950,8 @@ protected Result getFromMETA(final byte [] row, final byte [] family)\n   throws IOException {\n     MetaRegion meta = this.regionManager.getMetaRegionForRow(row);\n     HRegionInterface srvr = getMETAServer(meta);\n-\n     Get get = new Get(row);\n     get.addFamily(family);\n-    \n     return srvr.get(meta.getRegionName(), get);\n   }\n   ",
                "deletions": 2
            },
            {
                "sha": "b9c9b58db6e2aa26ecf127b6fdfd79104b925816",
                "filename": "src/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/src/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/64e7770cb7c2a35cbc91161bbf3551c1d5190d0b/src/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "status": "modified",
                "changes": 10,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/RegionManager.java?ref=64e7770cb7c2a35cbc91161bbf3551c1d5190d0b",
                "patch": "@@ -82,6 +82,8 @@\n     new ConcurrentSkipListMap<byte [], MetaRegion>(Bytes.BYTES_COMPARATOR);\n \n   private static final byte[] OVERLOADED = Bytes.toBytes(\"Overloaded\");\n+  \n+  private static final byte [] META_REGION_PREFIX = Bytes.toBytes(\".META.,\");\n \n   /**\n    * Map of region name to RegionState for regions that are in transition such as\n@@ -692,14 +694,20 @@ public MetaRegion getFirstMetaRegionForRegion(HRegionInfo newRegion) {\n   /**\n    * Get metaregion that would host passed in row.\n    * @param row Row need to know all the meta regions for\n-   * @return set of MetaRegion objects that contain the table\n+   * @return MetaRegion for passed row.\n    * @throws NotAllMetaRegionsOnlineException\n    */\n   public MetaRegion getMetaRegionForRow(final byte [] row)\n   throws NotAllMetaRegionsOnlineException {\n     if (!areAllMetaRegionsOnline()) {\n       throw new NotAllMetaRegionsOnlineException();\n     }\n+    // Row might be in -ROOT- table.  If so, return -ROOT- region.\n+    int prefixlen = META_REGION_PREFIX.length;\n+    if (row.length > prefixlen &&\n+     Bytes.compareTo(META_REGION_PREFIX, 0, prefixlen, row, 0, prefixlen) == 0) {\n+    \treturn new MetaRegion(this.master.getRootRegionLocation(), HRegionInfo.ROOT_REGIONINFO);\n+    }\n     return this.onlineMetaRegions.floorEntry(row).getValue();\n   }\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1523 - NPE in BaseScanner\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@784670 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/32a380b12adafc7656c002a9f11181d5861c60c8",
        "parent": "https://github.com/apache/hbase/commit/3f02e3be7aa75f9f63e439b3625e005fa2fb9658",
        "bug_id": "hbase_239",
        "file": [
            {
                "sha": "a3ced48478b7028a69df6063e11511b6ac9fcfc4",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/32a380b12adafc7656c002a9f11181d5861c60c8/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/32a380b12adafc7656c002a9f11181d5861c60c8/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=32a380b12adafc7656c002a9f11181d5861c60c8",
                "patch": "@@ -177,6 +177,7 @@ Release 0.20.0 - Unreleased\n                earlied-out of previous row (Jon Gray)\n    HBASE-1520  StoreFileScanner catches and ignore IOExceptions from HFile\n    HBASE-1522  We delete splits before their time occasionally\n+   HBASE-1523  NPE in BaseScanner\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "28e15891ab6a81b6329d77542e1bbc266f3a3fa6",
                "filename": "src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/32a380b12adafc7656c002a9f11181d5861c60c8/src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/32a380b12adafc7656c002a9f11181d5861c60c8/src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/BaseScanner.java?ref=32a380b12adafc7656c002a9f11181d5861c60c8",
                "patch": "@@ -298,7 +298,7 @@ private boolean hasReferences(final byte [] metaRegionName,\n   throws IOException {\n     boolean result = false;\n     HRegionInfo split =\n-      Writables.getHRegionInfo(rowContent.getValue(splitFamily, splitQualifier));\n+      Writables.getHRegionInfoOrNull(rowContent.getValue(splitFamily, splitQualifier));\n     if (split == null) {\n       return result;\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1441 NPE in ProcessRegionStatusChange#getMetaRegion\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@776535 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a16ee0714c9282b64ade6b39931e4dfbc5e27bc0",
        "parent": "https://github.com/apache/hbase/commit/47a59d15aabd7c70ecbeef15ed952459fe16f851",
        "bug_id": "hbase_240",
        "file": [
            {
                "sha": "03e9a6c313c54ebd3ed3b67f6ffc24413cf2bc9e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/a16ee0714c9282b64ade6b39931e4dfbc5e27bc0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/a16ee0714c9282b64ade6b39931e4dfbc5e27bc0/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=a16ee0714c9282b64ade6b39931e4dfbc5e27bc0",
                "patch": "@@ -143,6 +143,7 @@ Release 0.20.0 - Unreleased\n    HBASE-1438  HBASE-1421 broke the build (#602 up on hudson)\n    HBASE-1440  master won't go down because joined on a rootscanner that is\n                waiting for ever\n+   HBASE-1441  NPE in ProcessRegionStatusChange#getMetaRegion\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "f409b36a4ca7bf2fe36a456ddabb9c1edf400d93",
                "filename": "src/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java",
                "blob_url": "https://github.com/apache/hbase/blob/a16ee0714c9282b64ade6b39931e4dfbc5e27bc0/src/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java",
                "raw_url": "https://github.com/apache/hbase/raw/a16ee0714c9282b64ade6b39931e4dfbc5e27bc0/src/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/ProcessRegionStatusChange.java?ref=a16ee0714c9282b64ade6b39931e4dfbc5e27bc0",
                "patch": "@@ -71,8 +71,10 @@ protected MetaRegion getMetaRegion() {\n     } else {\n       this.metaRegion =\n         master.regionManager.getFirstMetaRegionForRegion(regionInfo);\n-      this.metaRegionName = this.metaRegion.getRegionName();\n+      if (this.metaRegion != null) {\n+        this.metaRegionName = this.metaRegion.getRegionName();\n+      }\n     }\n     return this.metaRegion;\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1386 NPE in housekeeping -- part 2\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@776067 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/16c92b7ce8d90fac02ab4e8b71fd7065b65860f7",
        "parent": "https://github.com/apache/hbase/commit/c40632b210e05f4ccd95d409975c0ced670a8895",
        "bug_id": "hbase_241",
        "file": [
            {
                "sha": "c3027e081c50977c95a6fe0e32578fe3ba357d6c",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/16c92b7ce8d90fac02ab4e8b71fd7065b65860f7/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/16c92b7ce8d90fac02ab4e8b71fd7065b65860f7/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=16c92b7ce8d90fac02ab4e8b71fd7065b65860f7",
                "patch": "@@ -1388,11 +1388,10 @@ void reportSplit(HRegionInfo oldRegion, HRegionInfo newRegionA,\n    * Data structure to hold a HMsg and retries count.\n    */\n   private static final class ToDoEntry {\n-    protected volatile int tries;\n+    protected final AtomicInteger tries = new AtomicInteger(0);\n     protected final HMsg msg;\n \n     ToDoEntry(final HMsg msg) {\n-      this.tries = 0;\n       this.msg = msg;\n     }\n   }\n@@ -1487,9 +1486,9 @@ public void run() {\n             if (ex instanceof IOException) {\n               ex = RemoteExceptionHandler.checkIOException((IOException) ex);\n             }\n-            if(e != null && e.tries < numRetries) {\n+            if(e != null && e.tries.get() < numRetries) {\n               LOG.warn(ex);\n-              e.tries++;\n+              e.tries.incrementAndGet();\n               try {\n                 toDo.put(e);\n               } catch (InterruptedException ie) {",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1386 NPE in housekeeping\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@772703 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/bc6ad67d673dfdebd216b021193f736dcf5a76f8",
        "parent": "https://github.com/apache/hbase/commit/6940db02c9d586b982a574aaf4f1744ccbb06366",
        "bug_id": "hbase_242",
        "file": [
            {
                "sha": "254a4e31494dd2d61654a1cd3ac4027fa60d3418",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/bc6ad67d673dfdebd216b021193f736dcf5a76f8/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/bc6ad67d673dfdebd216b021193f736dcf5a76f8/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=bc6ad67d673dfdebd216b021193f736dcf5a76f8",
                "patch": "@@ -115,6 +115,7 @@ Release 0.20.0 - Unreleased\n    HBASE-1377  RS address is null in master web UI\n    HBASE-1344  WARN IllegalStateException: Cannot set a region as open if it has\n                not been pending\n+   HBASE-1386  NPE in housekeeping\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "9f6b1db91e1bb56016022f438d1f36311c383cf1",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/bc6ad67d673dfdebd216b021193f736dcf5a76f8/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/bc6ad67d673dfdebd216b021193f736dcf5a76f8/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 16,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=bc6ad67d673dfdebd216b021193f736dcf5a76f8",
                "patch": "@@ -1124,6 +1124,7 @@ private boolean isHealthy() {\n     }\n     return true;\n   }\n+\n   /*\n    * Run some housekeeping tasks before we go into 'hibernation' sleeping at\n    * the end of the main HRegionServer run loop.\n@@ -1132,12 +1133,16 @@ private void housekeeping() {\n     // If the todo list has > 0 messages, iterate looking for open region\n     // messages. Send the master a message that we're working on its\n     // processing so it doesn't assign the region elsewhere.\n-    if (this.toDo.size() <= 0) {\n+    if (this.toDo.isEmpty()) {\n       return;\n     }\n     // This iterator is 'safe'.  We are guaranteed a view on state of the\n     // queue at time iterator was taken out.  Apparently goes from oldest.\n     for (ToDoEntry e: this.toDo) {\n+      HMsg msg = e.msg;\n+      if (msg == null) {\n+        LOG.warn(\"Message is empty: \" + e);\n+      }\n       if (e.msg.isType(HMsg.Type.MSG_REGION_OPEN)) {\n         addProcessingMessage(e.msg.getRegionInfo());\n       }\n@@ -1299,15 +1304,16 @@ void reportSplit(HRegionInfo oldRegion, HRegionInfo newRegionA,\n   /*\n    * Data structure to hold a HMsg and retries count.\n    */\n-  private static class ToDoEntry {\n-    protected int tries;\n+  private static final class ToDoEntry {\n+    protected volatile int tries;\n     protected final HMsg msg;\n-    ToDoEntry(HMsg msg) {\n+\n+    ToDoEntry(final HMsg msg) {\n       this.tries = 0;\n       this.msg = msg;\n     }\n   }\n-  \n+\n   final BlockingQueue<ToDoEntry> toDo = new LinkedBlockingQueue<ToDoEntry>();\n   private Worker worker;\n   private Thread workerThread;",
                "deletions": 5
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1374 NPE out of ZooKeeperWrapper.loadZooKeeperConfig\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@771996 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
        "parent": "https://github.com/apache/hbase/commit/1a2a1764adad96d79b678f047c5960b8b72f8810",
        "bug_id": "hbase_243",
        "file": [
            {
                "sha": "7be5e34ccd72f18b79c5b63074ed051843fba487",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
                "patch": "@@ -109,6 +109,7 @@ Release 0.20.0 - Unreleased\n    HBASE-1368  HBASE-1279 broke the build\n    HBASE-1264  Wrong return values of comparators for ColumnValueFilter\n                (Thomas Schneider via Andrew Purtell)\n+   HBASE-1374  NPE out of ZooKeeperWrapper.loadZooKeeperConfig\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "9d17b36f0b15e5bdb5a8ae49d115c6a14188a62c",
                "filename": "src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "blob_url": "https://github.com/apache/hbase/blob/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "raw_url": "https://github.com/apache/hbase/raw/92e5efdcb6746c5a06d67efd5e5eaccc2efc242e/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java?ref=92e5efdcb6746c5a06d67efd5e5eaccc2efc242e",
                "patch": "@@ -76,6 +76,9 @@ public static void main(String[] args) {\n   public static Properties parseZooKeeperConfig() throws IOException {\n     ClassLoader cl = HQuorumPeer.class.getClassLoader();\n     InputStream inputStream = cl.getResourceAsStream(ZOOKEEPER_CONFIG_NAME);\n+    if (inputStream == null) {\n+      throw new IOException(ZOOKEEPER_CONFIG_NAME + \" not found\");\n+    }\n     return parseConfig(inputStream);\n   }\n ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1114 Weird NPEs compacting\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@733213 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fc2157ae44ed05b36127c5284f171e286ffcc4dd",
        "parent": "https://github.com/apache/hbase/commit/7acb2ad4402259c1b3467153c620c53edd520e22",
        "bug_id": "hbase_244",
        "file": [
            {
                "sha": "fc95740985fea33d83a84ed8717b24631ba1e3e2",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "patch": "@@ -137,6 +137,7 @@ Release 0.19.0 - Unreleased\n                IllegalStateException: Cannot set a region to be closed it it was\n                not already marked as closing, Does not recover if HRS carrying \n                -ROOT- goes down\n+   HBASE-1114  Weird NPEs compacting\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "a3fc106846b04e500dbece1c45bfc0fbc0ef0ade",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "patch": "@@ -696,8 +696,9 @@ private IOException convertThrowableToIOE(final Throwable t,\n   private boolean checkOOME(final Throwable e) {\n     boolean stop = false;\n     if (e instanceof OutOfMemoryError ||\n-        (e.getCause()!= null && e.getCause() instanceof OutOfMemoryError) ||\n-        e.getMessage().contains(\"java.lang.OutOfMemoryError\")) {\n+      (e.getCause() != null && e.getCause() instanceof OutOfMemoryError) ||\n+      (e.getMessage() != null &&\n+        e.getMessage().contains(\"java.lang.OutOfMemoryError\"))) {\n       LOG.fatal(\"OutOfMemoryError, aborting.\", e);\n       abort();\n       stop = true;",
                "deletions": 2
            },
            {
                "sha": "b56df68a801367ef0e9390d3a0cd3dbb6a80b9c1",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "patch": "@@ -866,8 +866,10 @@ StoreSize compact(final boolean majorCompaction) throws IOException {\n           return null;\n         }\n         int len = 0;\n-        for (FileStatus fstatus:fs.listStatus(path)) {\n-          len += fstatus.getLen();\n+        // listStatus can come back null.\n+        FileStatus [] fss = this.fs.listStatus(path);\n+        for (int ii = 0; fss != null && i < fss.length; ii++) {\n+          len += fss[ii].getLen();\n         }\n         fileSizes[i] = len;\n         totalSize += len;",
                "deletions": 2
            },
            {
                "sha": "2c7766d2c19e29ef17860fad6886f058f939cba8",
                "filename": "src/webapps/master/WEB-INF/web.xml",
                "blob_url": "https://github.com/apache/hbase/blob/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/webapps/master/WEB-INF/web.xml",
                "raw_url": "https://github.com/apache/hbase/raw/fc2157ae44ed05b36127c5284f171e286ffcc4dd/src/webapps/master/WEB-INF/web.xml",
                "status": "modified",
                "changes": 16,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/webapps/master/WEB-INF/web.xml?ref=fc2157ae44ed05b36127c5284f171e286ffcc4dd",
                "patch": "@@ -15,13 +15,13 @@ Automatically created by Tomcat JspC.\n     </servlet>\n \n     <servlet>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n-        <servlet-class>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-class>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n+        <servlet-class>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-class>\n     </servlet>\n \n     <servlet>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n-        <servlet-class>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-class>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n+        <servlet-class>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-class>\n     </servlet>\n \n     <servlet-mapping>\n@@ -30,13 +30,13 @@ Automatically created by Tomcat JspC.\n     </servlet-mapping>\n \n     <servlet-mapping>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n-        <url-pattern>/table.jsp</url-pattern>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n+        <url-pattern>/regionhistorian.jsp</url-pattern>\n     </servlet-mapping>\n \n     <servlet-mapping>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n-        <url-pattern>/regionhistorian.jsp</url-pattern>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n+        <url-pattern>/table.jsp</url-pattern>\n     </servlet-mapping>\n \n </web-app>",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1101 NPE in HConnectionManager.processBatchOfRows\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@731817 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2",
        "parent": "https://github.com/apache/hbase/commit/5f8b9e3317dc0fca5e534b882ebebfc25d7a69f6",
        "bug_id": "hbase_245",
        "file": [
            {
                "sha": "3a574f0c5576c64f0ce3c44517cdce990682b7ce",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2",
                "patch": "@@ -130,6 +130,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1107  NPE in HStoreScanner.updateReaders\n    HBASE-1083  Will keep scheduling major compactions if last time one ran, we\n                didn't.\n+   HBASE-1101  NPE in HConnectionManager$TableServers.processBatchOfRows\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "6c684bb9f7c4ab59266c584e918690d92416b963",
                "filename": "src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 54,
                "additions": 44,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2",
                "patch": "@@ -872,32 +872,65 @@ private HRegionLocation locateRootRegion()\n       }\n       return null;    \n     }\n-    \n+\n+    private HRegionLocation\n+    getRegionLocationForRowWithRetries(byte[] tableName, byte[] rowKey)\n+        throws IOException {\n+      getMaster();\n+      List<Throwable> exceptions = new ArrayList<Throwable>();\n+      HRegionLocation location = null;\n+      int tries = 0;\n+      boolean reload = false;\n+      while (tries < numRetries) {\n+        try {\n+          location = getRegionLocation(tableName, rowKey, reload);\n+        } catch (Throwable t) {\n+          exceptions.add(t);\n+        }\n+        if (location != null) {\n+          break;\n+        }\n+        reload = true;\n+        tries++;\n+        try {\n+          Thread.sleep(getPauseTime(tries));\n+        } catch (InterruptedException e) {\n+          // continue\n+        }\n+      }\n+      if (location == null) {\n+        throw new RetriesExhaustedException(\"Some server\",\n+          HConstants.EMPTY_BYTE_ARRAY, rowKey, tries, exceptions);\n+      }\n+      return location;\n+    }\n+\n     public void processBatchOfRows(ArrayList<BatchUpdate> list, byte[] tableName)\n         throws IOException {\n-      // See HBASE-748 for pseudo code of this method\n       if (list.isEmpty()) {\n         return;\n       }\n       boolean retryOnlyOne = false;\n+      int tries = 0;\n       Collections.sort(list);\n       List<BatchUpdate> tempUpdates = new ArrayList<BatchUpdate>();\n-      byte [] currentRegion = getRegionLocation(tableName, list.get(0).getRow(),\n-        false).getRegionInfo().getRegionName();\n+      HRegionLocation location =\n+        getRegionLocationForRowWithRetries(tableName, list.get(0).getRow());\n+      byte [] currentRegion = location.getRegionInfo().getRegionName();\n       byte [] region = currentRegion;\n       boolean isLastRow = false;\n-      int tries = 0;\n       for (int i = 0; i < list.size() && tries < numRetries; i++) {\n         BatchUpdate batchUpdate = list.get(i);\n         tempUpdates.add(batchUpdate);\n         isLastRow = (i + 1) == list.size();\n         if (!isLastRow) {\n-          region = getRegionLocation(tableName, list.get(i + 1).getRow(), false)\n-              .getRegionInfo().getRegionName();\n+          location = getRegionLocationForRowWithRetries(tableName,\n+            list.get(i+1).getRow());\n+          region = location.getRegionInfo().getRegionName();\n         }\n         if (!Bytes.equals(currentRegion, region) || isLastRow || retryOnlyOne) {\n           final BatchUpdate[] updates = tempUpdates.toArray(new BatchUpdate[0]);\n-          int index = getRegionServerForWithoutRetries(new ServerCallable<Integer>(\n+          int index = getRegionServerWithRetries(new ServerCallable<Integer>(\n               this, tableName, batchUpdate.getRow()) {\n             public Integer call() throws IOException {\n               int i = server.batchUpdates(location.getRegionInfo()\n@@ -926,8 +959,9 @@ public Integer call() throws IOException {\n             }\n             i = i - updates.length + index;\n             retryOnlyOne = true;\n-            region = getRegionLocation(tableName, list.get(i + 1).getRow(),\n-                true).getRegionInfo().getRegionName();\n+            location = getRegionLocationForRowWithRetries(tableName, \n+              list.get(i + 1).getRow());\n+            region = location.getRegionInfo().getRegionName();\n           }\n           else {\n             retryOnlyOne = false;",
                "deletions": 10
            },
            {
                "sha": "6f1d7998038f7be9be353952ce7e29336bdc86e8",
                "filename": "src/webapps/master/WEB-INF/web.xml",
                "blob_url": "https://github.com/apache/hbase/blob/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2/src/webapps/master/WEB-INF/web.xml",
                "raw_url": "https://github.com/apache/hbase/raw/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2/src/webapps/master/WEB-INF/web.xml",
                "status": "modified",
                "changes": 16,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/webapps/master/WEB-INF/web.xml?ref=700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2",
                "patch": "@@ -15,13 +15,13 @@ Automatically created by Tomcat JspC.\n     </servlet>\n \n     <servlet>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n-        <servlet-class>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-class>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n+        <servlet-class>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-class>\n     </servlet>\n \n     <servlet>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n-        <servlet-class>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-class>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n+        <servlet-class>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-class>\n     </servlet>\n \n     <servlet-mapping>\n@@ -30,13 +30,13 @@ Automatically created by Tomcat JspC.\n     </servlet-mapping>\n \n     <servlet-mapping>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n-        <url-pattern>/regionhistorian.jsp</url-pattern>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n+        <url-pattern>/table.jsp</url-pattern>\n     </servlet-mapping>\n \n     <servlet-mapping>\n-        <servlet-name>org.apache.hadoop.hbase.generated.master.table_jsp</servlet-name>\n-        <url-pattern>/table.jsp</url-pattern>\n+        <servlet-name>org.apache.hadoop.hbase.generated.master.regionhistorian_jsp</servlet-name>\n+        <url-pattern>/regionhistorian.jsp</url-pattern>\n     </servlet-mapping>\n \n </web-app>",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1107 NPE in HStoreScanner.updateReaders\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@731730 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/18f3f2269bbb4d9a43015e569bca43690ae5a7d9",
        "parent": "https://github.com/apache/hbase/commit/fe345b23bcc0553d8c192fa858ccaad832209016",
        "bug_id": "hbase_246",
        "file": [
            {
                "sha": "bb3d8b109a406ae6dfbc6cd70025d653da693c10",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/18f3f2269bbb4d9a43015e569bca43690ae5a7d9/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/18f3f2269bbb4d9a43015e569bca43690ae5a7d9/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=18f3f2269bbb4d9a43015e569bca43690ae5a7d9",
                "patch": "@@ -127,6 +127,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1093  NPE in HStore#compact\n    HBASE-1097  SequenceFile.Reader keeps around buffer whose size is that of\n                largest item read -> results in lots of dead heap\n+   HBASE-1107  NPE in HStoreScanner.updateReaders\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "33f5a3b9a8ca1e0fb7c685ebe6af6cd80b2942d9",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/18f3f2269bbb4d9a43015e569bca43690ae5a7d9/src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/18f3f2269bbb4d9a43015e569bca43690ae5a7d9/src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java",
                "status": "modified",
                "changes": 8,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HStoreScanner.java?ref=18f3f2269bbb4d9a43015e569bca43690ae5a7d9",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.TreeMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n import org.apache.commons.logging.Log;\n@@ -59,6 +60,9 @@\n   \n   // Used around transition from no storefile to the first.\n   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n+\n+  // Used to indicate that the scanner has closed (see HBASE-1107)\n+  private final AtomicBoolean closing = new AtomicBoolean(false);\n   \n   /** Create an Scanner with a handle on the memcache and HStore files. */\n   @SuppressWarnings(\"unchecked\")\n@@ -294,6 +298,7 @@ void closeScanner(int i) {\n   }\n \n   public void close() {\n+    this.closing.set(true);\n     this.store.deleteChangedReaderObserver(this);\n     doClose();\n   }\n@@ -309,6 +314,9 @@ private void doClose() {\n   // Implementation of ChangedReadersObserver\n   \n   public void updateReaders() throws IOException {\n+    if (this.closing.get()) {\n+      return;\n+    }\n     this.lock.writeLock().lock();\n     try {\n       MapFile.Reader [] readers = this.store.getReaders();",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1093 NPE in HStore#compact\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@730068 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
        "parent": "https://github.com/apache/hbase/commit/9f9198e21eee3bbf73c609cfd2fb9d8978d46bc3",
        "bug_id": "hbase_247",
        "file": [
            {
                "sha": "44c33ae9a93dc28e5824bb4ea1a1ec2464519470",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
                "patch": "@@ -124,6 +124,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1100  HBASE-1062 broke TestForceSplit\n    HBASE-1191  shell tools -> close_region does not work for regions that did\n                not deploy properly on startup\n+   HBASE-1093  NPE in HStore#compact\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "c6ad3b76e9e70a981c8190083c277301f4c1122c",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/5c0d3a5ec892b27197f098a97dc2177cb0d8135e/src/java/org/apache/hadoop/hbase/regionserver/HStore.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HStore.java?ref=5c0d3a5ec892b27197f098a97dc2177cb0d8135e",
                "patch": "@@ -866,6 +866,10 @@ StoreSize compact(final boolean majorCompaction) throws IOException {\n       for (int i = 0; i < countOfFiles; i++) {\n         HStoreFile file = filesToCompact.get(i);\n         Path path = file.getMapFilePath();\n+        if (path == null) {\n+          LOG.warn(\"Path is null for \" + file);\n+          return null;\n+        }\n         int len = 0;\n         for (FileStatus fstatus:fs.listStatus(path)) {\n           len += fstatus.getLen();",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1079 Dumb NPE in ServerCallable hides the RetriesExhausted exception\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@728688 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32",
        "parent": "https://github.com/apache/hbase/commit/80eb7de81d1af33b2bf3072952f89458e5db0cb1",
        "bug_id": "hbase_248",
        "file": [
            {
                "sha": "e264ecff87fec8dedf4174efdfc8f2047fd63475",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32",
                "patch": "@@ -112,6 +112,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1067  TestRegionRebalancing broken by running of hdfs shutdown thread\n    HBASE-1070  Up default index interval in TRUNK and branch\n    HBASE-1045  Hangup by regionserver causes write to fail\n+   HBASE-1079  Dumb NPE in ServerCallable hides the RetriesExhausted exception\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "0172ff716d6613c03de9ffc465c2597a7313e39d",
                "filename": "src/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "blob_url": "https://github.com/apache/hbase/blob/7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32/src/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "raw_url": "https://github.com/apache/hbase/raw/7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32/src/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/ServerCallable.java?ref=7e439ed42f73b7f4ca3d9bf5865b88ad8667fb32",
                "patch": "@@ -68,6 +68,9 @@ public String getServerName() {\n   \n   /** @return the region name */\n   public byte[] getRegionName() {\n+    if (location == null) {\n+      return null;\n+    }\n     return location.getRegionInfo().getRegionName();\n   }\n   ",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1054 Index NPE on scanning\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@725324 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
        "parent": "https://github.com/apache/hbase/commit/db520ca53cdef9d7f3e2b6232484b137e1844ce9",
        "bug_id": "hbase_249",
        "file": [
            {
                "sha": "3a453ff5b388cabf22df1b733e9af1eb72893639",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "patch": "@@ -103,6 +103,7 @@ Release 0.19.0 - Unreleased\n                and no stop flag given.\n    HBASE-900   Regionserver memory leak causing OOME during relatively\n                modest bulk importing; part 1\n+   HBASE-1054  Index NPE on scanning (Clint Morgan via Andrew Purtell)\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "79e0db58699c1c516fa4a5177103205477c4da57",
                "filename": "src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTableDescriptor.java?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "patch": "@@ -188,6 +188,7 @@ public HTableDescriptor(final HTableDescriptor desc) {\n         desc.values.entrySet()) {\n       this.values.put(e.getKey(), e.getValue());\n     }\n+    this.indexes.putAll(desc.indexes);\n   }\n \n   /*\n@@ -494,6 +495,11 @@ public String toString() {\n     s.append(FAMILIES);\n     s.append(\" => \");\n     s.append(families.values());\n+\n+    s.append(\", \");\n+    s.append(\"INDEXES\");\n+    s.append(\" => \");\n+    s.append(indexes.values());\n     s.append('}');\n     return s.toString();\n   }",
                "deletions": 0
            },
            {
                "sha": "0e469d7c1f1d52b9758b700c85a137d9dcd0125c",
                "filename": "src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "status": "modified",
                "changes": 10,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "patch": "@@ -175,4 +175,14 @@ public void write(DataOutput out) throws IOException {\n         .writeObject(out, keyGenerator, IndexKeyGenerator.class, conf);\n   }\n \n+  /** {@inheritDoc} */\n+  @Override\n+  public String toString() {\n+    StringBuilder sb = new StringBuilder();\n+    sb.append(\"ID => \");\n+    sb.append(indexId);\n+    return sb.toString();\n+  }\n+  \n+  \n }",
                "deletions": 0
            },
            {
                "sha": "d384c4aef0da7287dd28511361150208c8e8e47e",
                "filename": "src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java?ref=7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8",
                "patch": "@@ -164,7 +164,10 @@ public RowResult next() throws IOException {\n         if (columns != null && columns.length > 0) {\n           LOG.debug(\"Going to base table for remaining columns\");\n           RowResult baseResult = IndexedTable.this.getRow(baseRow, columns);\n-          colValues.putAll(baseResult);\n+          \n+          if (baseResult != null) {\n+            colValues.putAll(baseResult);\n+          }\n         }\n         for (Entry<byte[], Cell> entry : row.entrySet()) {\n           byte[] col = entry.getKey();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1041 migration throwing NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@722609 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/ab7a3d33f2577858c4d993f7e4539bf2c4488de0",
        "parent": "https://github.com/apache/hbase/commit/9f5dd5eca9fd210f1c9c9d7aed1deb30ce21e094",
        "bug_id": "hbase_250",
        "file": [
            {
                "sha": "98193117e1f8c7d9cdd42772e86a34c7f2f26901",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/ab7a3d33f2577858c4d993f7e4539bf2c4488de0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/ab7a3d33f2577858c4d993f7e4539bf2c4488de0/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=ab7a3d33f2577858c4d993f7e4539bf2c4488de0",
                "patch": "@@ -87,6 +87,7 @@ Release 0.19.0 - Unreleased\n    HBASE-1023  Check global flusher\n    HBASE-1036  HBASE-1028 broke Thrift\n    HBASE-1037  Some test cases failing on Windows/Cygwin but not UNIX/Linux\n+   HBASE-1041  Migration throwing NPE\n       \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "5ca64bc1cb4d6ecb8c2dd5ef78b71e9e22ca60a0",
                "filename": "src/java/org/apache/hadoop/hbase/util/Migrate.java",
                "blob_url": "https://github.com/apache/hbase/blob/ab7a3d33f2577858c4d993f7e4539bf2c4488de0/src/java/org/apache/hadoop/hbase/util/Migrate.java",
                "raw_url": "https://github.com/apache/hbase/raw/ab7a3d33f2577858c4d993f7e4539bf2c4488de0/src/java/org/apache/hadoop/hbase/util/Migrate.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Migrate.java?ref=ab7a3d33f2577858c4d993f7e4539bf2c4488de0",
                "patch": "@@ -260,6 +260,10 @@ private boolean updateVersions(final HRegionInfo hri) {\n     boolean result = false;\n     HColumnDescriptor hcd =\n       hri.getTableDesc().getFamily(HConstants.COLUMN_FAMILY_HISTORIAN);\n+    if (hcd == null) {\n+      LOG.info(\"No region historian family in: \" + hri.getRegionNameAsString());\n+      return result;\n+    }\n     // Set historian records so they timeout after a week.\n     if (hcd.getTimeToLive() == HConstants.FOREVER) {\n       hcd.setTimeToLive(HConstants.WEEK_IN_SECONDS);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-939 NPE in HStoreKey\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@706348 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fd5543190c43feb8ede2056338ccf4622e94c99a",
        "parent": "https://github.com/apache/hbase/commit/5e3815484cde31fd975423b6b1d423198dadf90d",
        "bug_id": "hbase_251",
        "file": [
            {
                "sha": "4e027bcade37594216abf2eaab48c55c18a163a1",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/fd5543190c43feb8ede2056338ccf4622e94c99a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/fd5543190c43feb8ede2056338ccf4622e94c99a/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fd5543190c43feb8ede2056338ccf4622e94c99a",
                "patch": "@@ -35,6 +35,7 @@ Release 0.19.0 - Unreleased\n                they are using HTable\n                With J-D's one line patch, test cases now appear to work and\n                PerformanceEvaluation works as before.\n+   HBASE-939   NPE in HStoreKey\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "91dfde7a4fa8d23bc4932879564e8a5a24cd41a0",
                "filename": "src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "blob_url": "https://github.com/apache/hbase/blob/fd5543190c43feb8ede2056338ccf4622e94c99a/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "raw_url": "https://github.com/apache/hbase/raw/fd5543190c43feb8ede2056338ccf4622e94c99a/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreKey.java?ref=fd5543190c43feb8ede2056338ccf4622e94c99a",
                "patch": "@@ -351,6 +351,11 @@ public int compareTo(Object o) {\n   \n   static int compareTo(final HRegionInfo hri, final HStoreKey left,\n       final HStoreKey right) {\n+    // We can be passed null\n+    if (left == null && right == null) return 0;\n+    if (left == null) return -1;\n+    if (right == null) return 1;\n+    \n     int result = compareTwoRowKeys(hri, left.getRow(), right.getRow());\n     if (result != 0) {\n       return result;",
                "deletions": 0
            },
            {
                "sha": "b33317624f35fba48c78d33dab80c7e8e34b46b8",
                "filename": "src/test/org/apache/hadoop/hbase/TestCompare.java",
                "blob_url": "https://github.com/apache/hbase/blob/fd5543190c43feb8ede2056338ccf4622e94c99a/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "raw_url": "https://github.com/apache/hbase/raw/fd5543190c43feb8ede2056338ccf4622e94c99a/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestCompare.java?ref=fd5543190c43feb8ede2056338ccf4622e94c99a",
                "patch": "@@ -51,6 +51,11 @@ public void testHStoreKey() {\n     nocolumn = new HStoreKey(a, HConstants.LATEST_TIMESTAMP);\n     withcolumn = new HStoreKey(a, a, timestamp);\n     assertTrue(nocolumn.compareTo(withcolumn) < 0);\n+    // Test null keys.\n+    HStoreKey normal = new HStoreKey(\"a\", \"b\");\n+    assertTrue(normal.compareTo(null) > 0);\n+    assertTrue(HStoreKey.compareTo(null, null, null) == 0);\n+    assertTrue(HStoreKey.compareTo(null, null, normal) < 0);\n   }\n   \n   /**",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-926 NPE throwing RetriesExhaustedException\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@704642 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fb1659cf07434d4b179c7df7621fcf454f83ab2a",
        "parent": "https://github.com/apache/hbase/commit/767f6057ffff460082478258e8259776a13b2a10",
        "bug_id": "hbase_252",
        "file": [
            {
                "sha": "a44ccab955533eefc6604d3eae2738ecae1c217b",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/fb1659cf07434d4b179c7df7621fcf454f83ab2a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/fb1659cf07434d4b179c7df7621fcf454f83ab2a/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fb1659cf07434d4b179c7df7621fcf454f83ab2a",
                "patch": "@@ -24,6 +24,7 @@ Release 0.19.0 - Unreleased\n    HBASE-921   region close and open processed out of order; makes for \n                disagreement between master and regionserver on region state\n    HBASE-925   HRS NPE on way out if no master to connect to\n+   HBASE-928   NPE throwing RetriesExhaustedException\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "ab5fe6b4cb482a8432d4dafb5e0e8881de437920",
                "filename": "src/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "blob_url": "https://github.com/apache/hbase/blob/fb1659cf07434d4b179c7df7621fcf454f83ab2a/src/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "raw_url": "https://github.com/apache/hbase/raw/fb1659cf07434d4b179c7df7621fcf454f83ab2a/src/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "status": "modified",
                "changes": 3,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/ServerCallable.java?ref=fb1659cf07434d4b179c7df7621fcf454f83ab2a",
                "patch": "@@ -68,9 +68,6 @@ public String getServerName() {\n   \n   /** @return the region name */\n   public byte[] getRegionName() {\n-    if (location == null) {\n-      return null;\n-    }\n     return location.getRegionInfo().getRegionName();\n   }\n   ",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-750 NPE caused by StoreFileScanner.updateReaders\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@679162 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/47904a89056bdb5c08e732f2de2a33780a4f6192",
        "parent": "https://github.com/apache/hbase/commit/16de09b3ad949b2f6b2b5e2a51fd2bbf3e4aca4c",
        "bug_id": "hbase_253",
        "file": [
            {
                "sha": "4c141e81e3d2cd8d7d9b420ea8c96ac3cda08639",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/47904a89056bdb5c08e732f2de2a33780a4f6192/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/47904a89056bdb5c08e732f2de2a33780a4f6192/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=47904a89056bdb5c08e732f2de2a33780a4f6192",
                "patch": "@@ -211,6 +211,7 @@ Release 0.2.0\n                (Andrew Purtell via Stack)\r\n    HBASE-764   The name of column request has padding zero using REST interface\r\n                (Sishen Freecity via Stack)\r\n+   HBASE-750   NPE caused by StoreFileScanner.updateReaders\r\n    \r\n   IMPROVEMENTS\r\n    HBASE-559   MR example job to count table rows\r",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-750 NPE caused by StoreFileScanner.updateReaders\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@678898 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/80843cd04dcf2bc2a7950945195289293df939f3",
        "parent": "https://github.com/apache/hbase/commit/6591077efca867ff62deb34b6ea303804a07ac2d",
        "bug_id": "hbase_254",
        "file": [
            {
                "sha": "dce845f4989d357f060f71aede707ec1734d6440",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/80843cd04dcf2bc2a7950945195289293df939f3/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/80843cd04dcf2bc2a7950945195289293df939f3/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java?ref=80843cd04dcf2bc2a7950945195289293df939f3",
                "patch": "@@ -377,7 +377,8 @@ public void updateReaders() throws IOException {\n       ViableRow viableRow = getNextViableRow();\n       openReaders(viableRow.getRow());\n       LOG.debug(\"Replaced Scanner Readers at row \" +\n-        Bytes.toString(viableRow.getRow()));\n+        (viableRow == null || viableRow.getRow() == null? \"null\":\n+          Bytes.toString(viableRow.getRow())));\n     } finally {\n       this.lock.writeLock().unlock();\n     }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "NPE getting scanner\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@647953 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/278bd7e772665af9cbd9368fdbc6222f3c8317e7",
        "parent": "https://github.com/apache/hbase/commit/fd202765c9b5af7bc49fb5e71ff2a4e682ab2d00",
        "bug_id": "hbase_255",
        "file": [
            {
                "sha": "396a0e0e692e7772115c5b0d7ec46c226db0e73d",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/278bd7e772665af9cbd9368fdbc6222f3c8317e7/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/278bd7e772665af9cbd9368fdbc6222f3c8317e7/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=278bd7e772665af9cbd9368fdbc6222f3c8317e7",
                "patch": "@@ -1,6 +1,7 @@\n Hbase Change Log\r\n   INCOMPATIBLE CHANGES\r\n    HBASE-521   Improve client scanner interface\r\n+   HBASE-577   NPE getting scanner\r\n   \r\n   BUG FIXES\r\n    HBASE-550   EOF trying to read reconstruction log stops region deployment\r",
                "deletions": 0
            },
            {
                "sha": "0b7dabdf04bcfce94b2fd30372d542f113f31bf3",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/278bd7e772665af9cbd9368fdbc6222f3c8317e7/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/278bd7e772665af9cbd9368fdbc6222f3c8317e7/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 13,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=278bd7e772665af9cbd9368fdbc6222f3c8317e7",
                "patch": "@@ -1114,6 +1114,19 @@ public long openScanner(Text regionName, Text[] cols, Text firstRow,\n     final long timestamp, final RowFilterInterface filter)\n   throws IOException {\n     checkOpen();\n+    NullPointerException npe = null;\n+    if (regionName == null) {\n+      npe = new NullPointerException(\"regionName is null\");\n+    } else if (cols == null) {\n+      npe = new NullPointerException(\"columns to scan is null\");\n+    } else if (firstRow == null) {\n+      npe = new NullPointerException(\"firstRow for scanner is null\");\n+    }\n+    if (npe != null) {\n+      IOException io = new IOException(\"Invalid arguments to openScanner\");\n+      io.initCause(npe);\n+      throw io;\n+    }\n     requestCount.incrementAndGet();\n     try {\n       HRegion r = getRegion(regionName);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-2693 NPE in getClosestRowBefore\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/trunk/src/contrib/hbase@617724 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/b7d932f4dc1d1cee73142376baaee212df9d6ae0",
        "parent": "https://github.com/apache/hbase/commit/3e351091b6695c6344ad382ee28db8af0627e2e5",
        "bug_id": "hbase_256",
        "file": [
            {
                "sha": "1e60afff71fd9da9c9ff74959202eecdcd388ab1",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/b7d932f4dc1d1cee73142376baaee212df9d6ae0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/b7d932f4dc1d1cee73142376baaee212df9d6ae0/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=b7d932f4dc1d1cee73142376baaee212df9d6ae0",
                "patch": "@@ -15,6 +15,7 @@ Trunk (unreleased changes)\n   BUG FIXES\r\n    HADOOP-2731 Under load, regions become extremely large and eventually cause\r\n                region servers to become unresponsive\r\n+   HADOOP-2693 NPE in getClosestRowBefore (Bryan Duxbury & Stack)\r\n \r\n   IMPROVEMENTS\r\n    HADOOP-2555 Refactor the HTable#get and HTable#getRow methods to avoid\r",
                "deletions": 0
            },
            {
                "sha": "ab5f7f1fe7f29d44504fe9d7e5b3e653f3cb66ab",
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/b7d932f4dc1d1cee73142376baaee212df9d6ae0/src/java/org/apache/hadoop/hbase/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/b7d932f4dc1d1cee73142376baaee212df9d6ae0/src/java/org/apache/hadoop/hbase/HStore.java",
                "status": "modified",
                "changes": 20,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=b7d932f4dc1d1cee73142376baaee212df9d6ae0",
                "patch": "@@ -1830,20 +1830,13 @@ public Text getRowKeyAtOrBefore(final Text row, final long timestamp)\n       for(int i = maparray.length - 1; i >= 0; i--) {\n         Text row_from_mapfile = \n           rowAtOrBeforeFromMapFile(maparray[i], row, timestamp);\n-\n-        // for when we have MapFile.Reader#getClosest before functionality\n-/*        Text row_from_mapfile = null;\n-        WritableComparable value = null; \n-        \n-        HStoreKey hskResult = \n-          (HStoreKey)maparray[i].getClosest(rowKey, value, true);\n-        \n-        if (hskResult != null) {\n-          row_from_mapfile = hskResult.getRow();\n-        }*/\n-                \n-/*        LOG.debug(\"Best from this mapfile was \" + row_from_mapfile);*/\n         \n+        // if the result from the mapfile is null, then we know that\n+        // the mapfile was empty and can move on to the next one.\n+        if (row_from_mapfile == null) {\n+          continue;\n+        }\n+      \n         // short circuit on an exact match\n         if (row.equals(row_from_mapfile)) {\n           return row;\n@@ -1855,7 +1848,6 @@ public Text getRowKeyAtOrBefore(final Text row, final long timestamp)\n         }\n       }\n       \n-/*      LOG.debug(\"Went searching for \" + row + \", found \" + bestSoFar);*/\n       return bestSoFar;\n     } finally {\n       this.lock.readLock().unlock();",
                "deletions": 14
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1797 Fix NPEs in MetaScanner constructor\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@571333 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/aba565a22873439f7ee8f6d53091b3ce709fc96c",
        "parent": "https://github.com/apache/hbase/commit/12a62a6333dbe317af987a303e4e3c9405608ee3",
        "bug_id": "hbase_257",
        "file": [
            {
                "sha": "163c015136543d62b576f34adbde4ad701262830",
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=aba565a22873439f7ee8f6d53091b3ce709fc96c",
                "patch": "@@ -58,9 +58,7 @@\n   /**\n    * Constructor\n    */\n-  public HMemcache() {\n-    super();\n-  }\n+  public HMemcache() {}\n \n   /** represents the state of the memcache at a specified point in time */\n   static class Snapshot {\n@@ -320,7 +318,7 @@ HInternalScannerInterface getScanner(long timestamp,\n         // Generate list of iterators\n         HStoreKey firstKey = new HStoreKey(firstRow);\n         for(int i = 0; i < backingMaps.length; i++) {\n-          keyIterators[i] = (firstRow.getLength() != 0)?\n+          keyIterators[i] = (/*firstRow != null &&*/ firstRow.getLength() != 0)?\n             backingMaps[i].tailMap(firstKey).keySet().iterator():\n             backingMaps[i].keySet().iterator();\n           while(getNext(i)) {",
                "deletions": 4
            },
            {
                "sha": "f7e4b55bc4cf3ae43ec07a5f904d8de64c0574be",
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/aba565a22873439f7ee8f6d53091b3ce709fc96c/src/java/org/apache/hadoop/hbase/HRegion.java",
                "status": "modified",
                "changes": 30,
                "additions": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=aba565a22873439f7ee8f6d53091b3ce709fc96c",
                "patch": "@@ -208,6 +208,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n \n   final int memcacheFlushSize;\n   final int blockingMemcacheSize;\n+  protected final long threadWakeFrequency;\n   private final HLocking lock = new HLocking();\n   private long desiredMaxFileSize;\n   private final long maxSequenceId;\n@@ -244,6 +245,7 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     this.conf = conf;\n     this.regionInfo = regionInfo;\n     this.memcache = new HMemcache();\n+    this.threadWakeFrequency = conf.getLong(THREAD_WAKE_FREQUENCY, 10 * 1000);\n \n     // Declare the regionName.  This is a unique string for the region, used to \n     // build a unique filename.\n@@ -1055,24 +1057,28 @@ public long startUpdate(Text row) throws IOException {\n    * the notify.\n    */\n   private synchronized void checkResources() {\n-    if (checkCommitsSinceFlush()) {\n-      return;\n-    }\n+    boolean blocked = false;\n     \n-    LOG.warn(\"Blocking updates for '\" + Thread.currentThread().getName() +\n-      \"': Memcache size \" +\n-      StringUtils.humanReadableInt(this.memcache.getSize()) +\n-      \" is >= than blocking \" +\n-      StringUtils.humanReadableInt(this.blockingMemcacheSize) + \" size\");\n     while (!checkCommitsSinceFlush()) {\n+      if (!blocked) {\n+        LOG.info(\"Blocking updates for '\" + Thread.currentThread().getName() +\n+            \"': Memcache size \" +\n+            StringUtils.humanReadableInt(this.memcache.getSize()) +\n+            \" is >= than blocking \" +\n+            StringUtils.humanReadableInt(this.blockingMemcacheSize) + \" size\");\n+      }\n+\n+      blocked = true;\n       try {\n-        wait();\n+        wait(threadWakeFrequency);\n       } catch (InterruptedException e) {\n         // continue;\n       }\n     }\n-    LOG.warn(\"Unblocking updates for '\" + Thread.currentThread().getName() +\n-      \"'\");\n+    if (blocked) {\n+      LOG.info(\"Unblocking updates for '\" + Thread.currentThread().getName() +\n+          \"'\");\n+    }\n   }\n   \n   /*\n@@ -1635,4 +1641,4 @@ static boolean deleteRegion(FileSystem fs, Path baseDirectory,\n   public static Path getRegionDir(final Path dir, final Text regionName) {\n     return new Path(dir, new Path(HREGIONDIR_PREFIX + regionName));\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 12
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1797 Fix NPEs in MetaScanner constructor\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@570583 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/660bce1d278f3eb471b75bb85864efdc5e53f354",
        "parent": "https://github.com/apache/hbase/commit/f56ee6b375ec904beb37d1a3c303d8d6c9bd71b3",
        "bug_id": "hbase_258",
        "file": [
            {
                "sha": "f2dbccd9485f0f43d335a4473b1768ed225a4ed7",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/660bce1d278f3eb471b75bb85864efdc5e53f354/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/660bce1d278f3eb471b75bb85864efdc5e53f354/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=660bce1d278f3eb471b75bb85864efdc5e53f354",
                "patch": "@@ -21,6 +21,7 @@ Trunk (unreleased changes)\n     HADOOP-1776 Fix for sporadic compaction failures closing and moving\n     compaction result\n     HADOOP-1780 Regions are still being doubly assigned\n+    HADOOP-1797 Fix NPEs in MetaScanner constructor\n \n   IMPROVEMENTS\n     HADOOP-1737 Make HColumnDescriptor data publically members settable",
                "deletions": 0
            },
            {
                "sha": "af825fce4ed3f434fc7e8e2026256fc71f6dabbd",
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/660bce1d278f3eb471b75bb85864efdc5e53f354/src/java/org/apache/hadoop/hbase/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/660bce1d278f3eb471b75bb85864efdc5e53f354/src/java/org/apache/hadoop/hbase/HMaster.java",
                "status": "modified",
                "changes": 96,
                "additions": 63,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=660bce1d278f3eb471b75bb85864efdc5e53f354",
                "patch": "@@ -189,16 +189,16 @@ protected void scanRegion(final MetaRegion region) throws IOException {\n       HRegionInterface regionServer = null;\n       long scannerId = -1L;\n       LOG.info(Thread.currentThread().getName() + \" scanning meta region \" +\n-          region.regionName + \" on \" + region.server.toString());\n+          region.getRegionName() + \" on \" + region.getServer().toString());\n \n       // Array to hold list of split parents found.  Scan adds to list.  After\n       // scan we go check if parents can be removed.\n       Map<HRegionInfo, SortedMap<Text, byte[]>> splitParents =\n         new HashMap<HRegionInfo, SortedMap<Text, byte[]>>();\n       try {\n-        regionServer = connection.getHRegionConnection(region.server);\n+        regionServer = connection.getHRegionConnection(region.getServer());\n         scannerId =\n-          regionServer.openScanner(region.regionName, COLUMN_FAMILY_ARRAY,\n+          regionServer.openScanner(region.getRegionName(), COLUMN_FAMILY_ARRAY,\n               EMPTY_START_ROW, System.currentTimeMillis(), null);\n \n         int numberOfRegionsFound = 0;\n@@ -256,7 +256,8 @@ protected void scanRegion(final MetaRegion region) throws IOException {\n           }\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n-            e = RemoteExceptionHandler.decodeRemoteException((RemoteException) e);\n+            e = RemoteExceptionHandler.decodeRemoteException(\n+                (RemoteException) e);\n           }\n           LOG.error(\"Closing scanner\", e);\n         }\n@@ -268,11 +269,11 @@ protected void scanRegion(final MetaRegion region) throws IOException {\n         for (Map.Entry<HRegionInfo, SortedMap<Text, byte[]>> e:\n             splitParents.entrySet()) {\n           HRegionInfo hri = e.getKey();\n-          cleanupSplits(region.regionName, regionServer, hri, e.getValue());\n+          cleanupSplits(region.getRegionName(), regionServer, hri, e.getValue());\n         }\n       }\n       LOG.info(Thread.currentThread().getName() + \" scan of meta region \" +\n-          region.regionName + \" complete\");\n+          region.getRegionName() + \" complete\");\n     }\n \n     /*\n@@ -542,14 +543,40 @@ protected void maintenanceScan() {\n \n   @SuppressWarnings(\"unchecked\")\n   static class MetaRegion implements Comparable {\n-    HServerAddress server;\n-    Text regionName;\n-    Text startKey;\n+    private HServerAddress server;\n+    private Text regionName;\n+    private Text startKey;\n \n     MetaRegion(HServerAddress server, Text regionName, Text startKey) {\n+      if (server == null) {\n+        throw new IllegalArgumentException(\"server cannot be null\");\n+      }\n       this.server = server;\n-      this.regionName = regionName;\n-      this.startKey = startKey;\n+      \n+      if (regionName == null) {\n+        throw new IllegalArgumentException(\"regionName cannot be null\");\n+      }\n+      this.regionName = new Text(regionName);\n+      \n+      this.startKey = new Text();\n+      if (startKey != null) {\n+        this.startKey.set(startKey);\n+      }\n+    }\n+\n+    /** @return the regionName */\n+    public Text getRegionName() {\n+      return regionName;\n+    }\n+\n+    /** @return the server */\n+    public HServerAddress getServer() {\n+      return server;\n+    }\n+\n+    /** @return the startKey */\n+    public Text getStartKey() {\n+      return startKey;\n     }\n \n     /** {@inheritDoc} */\n@@ -572,9 +599,9 @@ public int hashCode() {\n     public int compareTo(Object o) {\n       MetaRegion other = (MetaRegion)o;\n \n-      int result = this.regionName.compareTo(other.regionName);\n+      int result = this.regionName.compareTo(other.getRegionName());\n       if(result == 0) {\n-        result = this.startKey.compareTo(other.startKey);\n+        result = this.startKey.compareTo(other.getStartKey());\n       }\n       return result;\n     }\n@@ -625,7 +652,7 @@ private void scanOneMetaRegion(MetaRegion region) {\n           // Don't interrupt us while we're working\n           synchronized (metaScannerLock) {\n             scanRegion(region);\n-            onlineMetaRegions.put(region.startKey, region);\n+            onlineMetaRegions.put(region.getStartKey(), region);\n           }\n           break;\n         } catch (IOException e) {\n@@ -1970,19 +1997,21 @@ boolean process() throws IOException {\n             long scannerId = -1L;\n \n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"process server shutdown scanning \" + r.regionName +\n-                  \" on \" + r.server + \" \" + Thread.currentThread().getName());\n+              LOG.debug(\"process server shutdown scanning \" +\n+                  r.getRegionName() + \" on \" + r.getServer() + \" \" +\n+                  Thread.currentThread().getName());\n             }\n-            server = connection.getHRegionConnection(r.server);\n+            server = connection.getHRegionConnection(r.getServer());\n \n-            scannerId = server.openScanner(r.regionName, COLUMN_FAMILY_ARRAY,\n-                EMPTY_START_ROW, System.currentTimeMillis(), null);\n+            scannerId =\n+              server.openScanner(r.getRegionName(), COLUMN_FAMILY_ARRAY,\n+                  EMPTY_START_ROW, System.currentTimeMillis(), null);\n             \n-            scanMetaRegion(server, scannerId, r.regionName);\n+            scanMetaRegion(server, scannerId, r.getRegionName());\n             \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"process server shutdown finished scanning \" +\n-                  r.regionName + \" on \" + r.server + \" \" +\n+                  r.getRegionName() + \" on \" + r.getServer() + \" \" +\n                   Thread.currentThread().getName());\n             }\n           }\n@@ -2086,8 +2115,8 @@ boolean process() throws IOException {\n             r = onlineMetaRegions.get(onlineMetaRegions.headMap(\n                 regionInfo.getRegionName()).lastKey());\n           }\n-          metaRegionName = r.regionName;\n-          server = connection.getHRegionConnection(r.server);\n+          metaRegionName = r.getRegionName();\n+          server = connection.getHRegionConnection(r.getServer());\n         }\n \n         try {\n@@ -2227,8 +2256,8 @@ boolean process() throws IOException {\n             r = onlineMetaRegions.get(onlineMetaRegions.headMap(\n                 region.getRegionName()).lastKey());\n           }\n-          metaRegionName = r.regionName;\n-          server = connection.getHRegionConnection(r.server);\n+          metaRegionName = r.getRegionName();\n+          server = connection.getHRegionConnection(r.getServer());\n         }\n         LOG.info(\"updating row \" + region.getRegionName() + \" in table \" +\n           metaRegionName + \" with startcode \" +\n@@ -2365,8 +2394,8 @@ private void createTable(final HRegionInfo newRegion) throws IOException {\n             onlineMetaRegions.get(onlineMetaRegions.headMap(\n                 newRegion.getTableDesc().getName()).lastKey()));\n           \n-      Text metaRegionName = m.regionName;\n-      HRegionInterface server = connection.getHRegionConnection(m.server);\n+      Text metaRegionName = m.getRegionName();\n+      HRegionInterface server = connection.getHRegionConnection(m.getServer());\n       long scannerid = server.openScanner(metaRegionName, COL_REGIONINFO_ARRAY,\n           tableName, System.currentTimeMillis(), null);\n       try {\n@@ -2504,13 +2533,14 @@ void process() throws IOException {\n \n               // Get a connection to a meta server\n \n-              HRegionInterface server = connection.getHRegionConnection(m.server);\n+              HRegionInterface server =\n+                connection.getHRegionConnection(m.getServer());\n \n               // Open a scanner on the meta region\n \n               long scannerId =\n-                server.openScanner(m.regionName, COLUMN_FAMILY_ARRAY, tableName,\n-                    System.currentTimeMillis(), null);\n+                server.openScanner(m.getRegionName(), COLUMN_FAMILY_ARRAY,\n+                    tableName, System.currentTimeMillis(), null);\n \n               try {\n                 while (true) {\n@@ -2694,7 +2724,7 @@ protected void postProcessMeta(MetaRegion m, HRegionInterface server)\n \n         for (int tries = 0; tries < numRetries; tries++) {\n           try {\n-            server.batchUpdate(m.regionName, System.currentTimeMillis(), b);\n+            server.batchUpdate(m.getRegionName(), System.currentTimeMillis(), b);\n             \n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"updated columns in row: \" + i.regionName);\n@@ -2890,7 +2920,7 @@ protected void postProcessMeta(MetaRegion m, HRegionInterface server)\n \n       for (HRegionInfo i: unservedRegions) {\n         i.tableDesc.families().remove(columnName);\n-        updateRegionInfo(server, m.regionName, i);\n+        updateRegionInfo(server, m.getRegionName(), i);\n \n         // Delete the directories used by the column\n \n@@ -2939,7 +2969,7 @@ protected void postProcessMeta(MetaRegion m, HRegionInterface server)\n         // and create it.\n \n         i.tableDesc.addFamily(newColumn);\n-        updateRegionInfo(server, m.regionName, i);\n+        updateRegionInfo(server, m.getRegionName(), i);\n       }\n     }\n   }",
                "deletions": 33
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1560 NPE in MiniHBaseCluster on Windows\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@553080 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/9eb369c26699fc265c2a0e780318513bd01b85a5",
        "parent": "https://github.com/apache/hbase/commit/655728f3bf87f21104a6f02f4564901d79cf6bf2",
        "bug_id": "hbase_259",
        "file": [
            {
                "sha": "b1fc395ae1593ac704bdd754a011ef47ad806542",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "patch": "@@ -45,3 +45,4 @@ Trunk (unreleased changes)\n  26. HADOOP-1543 [hbase] Add HClient.tableExists\n  27. HADOOP-1519 [hbase] map/reduce interface for HBase\n  28. HADOOP-1523 Hung region server waiting on write locks \n+ 29. HADOOP-1560 NPE in MiniHBaseCluster on Windows",
                "deletions": 0
            },
            {
                "sha": "8664c53314018736217cede68ac2b7dee71a9c89",
                "filename": "src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "patch": "@@ -32,7 +32,7 @@\n   protected HTableDescriptor desc;\n   protected ImmutableBytesWritable value;\n \n-  protected MiniDFSCluster dfsCluster;\n+  protected MiniDFSCluster dfsCluster = null;\n   protected FileSystem fs;\n   protected Path dir;\n \n@@ -104,6 +104,9 @@ public void setUp() throws Exception {\n       \n     } catch(Throwable t) {\n       t.printStackTrace();\n+      if(dfsCluster != null) {\n+        dfsCluster.shutdown();\n+      }\n       fail();\n     }\n   }",
                "deletions": 1
            },
            {
                "sha": "183db5dd0f00945c5e79a49f8eea1177346f2961",
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "status": "modified",
                "changes": 24,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "patch": "@@ -37,8 +37,8 @@\n   private MiniDFSCluster cluster;\n   private FileSystem fs;\n   private Path parentdir;\n-  private HMaster master;\n-  private Thread masterThread;\n+  private HMaster master = null;\n+  private Thread masterThread = null;\n   List<HRegionServer> regionServers;\n   List<Thread> regionThreads;\n   private boolean deleteOnExit = true;\n@@ -83,6 +83,8 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n \n     this.conf = conf;\n     this.cluster = dfsCluster;\n+    this.regionServers = new ArrayList<HRegionServer>(nRegionNodes);\n+    this.regionThreads = new ArrayList<Thread>(nRegionNodes);\n     init(nRegionNodes);\n   }\n \n@@ -102,6 +104,8 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n   throws IOException {\n     this.conf = conf;\n     this.deleteOnExit = deleteOnExit;\n+    this.regionServers = new ArrayList<HRegionServer>(nRegionNodes);\n+    this.regionThreads = new ArrayList<Thread>(nRegionNodes);\n \n     if (miniHdfsFilesystem) {\n       try {\n@@ -167,8 +171,6 @@ public MiniDFSCluster getDFSCluster() {\n \n   private void startRegionServers(final int nRegionNodes)\n   throws IOException {\n-    this.regionServers = new ArrayList<HRegionServer>(nRegionNodes);\n-    this.regionThreads = new ArrayList<Thread>(nRegionNodes);    \n     for(int i = 0; i < nRegionNodes; i++) {\n       startRegionServer();\n     }\n@@ -239,7 +241,9 @@ public void shutdown() {\n     for(HRegionServer hsr: this.regionServers) {\n       hsr.stop();\n     }\n-    master.shutdown();\n+    if(master != null) {\n+      master.shutdown();\n+    }\n     for(Thread t: this.regionThreads) {\n       if (t.isAlive()) {\n         try {\n@@ -249,11 +253,13 @@ public void shutdown() {\n         }\n       }\n     }\n-    try {\n-      masterThread.join();\n+    if (masterThread != null) {\n+      try {\n+        masterThread.join();\n \n-    } catch(InterruptedException e) {\n-      // continue\n+      } catch(InterruptedException e) {\n+        // continue\n+      }\n     }\n     LOG.info(\"HBase Cluster shutdown complete\");\n ",
                "deletions": 9
            },
            {
                "sha": "f370bb7fbd2a3deb59997f8cbc39a5facaa9e9bb",
                "filename": "src/test/org/apache/hadoop/hbase/TestHRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestHRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestHRegion.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHRegion.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "patch": "@@ -61,6 +61,9 @@ public void testHRegion() {\n       cleanup();\n       \n     } catch(Exception e) {\n+      if(cluster != null) {\n+        cluster.shutdown();\n+      }\n       e.printStackTrace();\n       fail();\n     }\n@@ -798,6 +801,7 @@ private void cleanup() {\n     // Shut down the mini cluster\n \n     cluster.shutdown();\n+    cluster = null;\n \n     // Delete all the DFS files\n ",
                "deletions": 0
            },
            {
                "sha": "48adb45db23f6493c145bbf14266e58138d7901e",
                "filename": "src/test/org/apache/hadoop/hbase/TestScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "status": "modified",
                "changes": 3,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestScanner.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "patch": "@@ -260,9 +260,6 @@ public void testScanner() throws IOException {\n       throw e;\n       \n     } finally {\n-      if(fs != null) {\n-        fs.close();\n-      }\n       if(cluster != null) {\n         cluster.shutdown();\n       }",
                "deletions": 3
            },
            {
                "sha": "f6b51b54316e7c19c35247e6e86ba98443d70000",
                "filename": "src/test/org/apache/hadoop/hbase/TestTableMapReduce.java",
                "blob_url": "https://github.com/apache/hbase/blob/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestTableMapReduce.java",
                "raw_url": "https://github.com/apache/hbase/raw/9eb369c26699fc265c2a0e780318513bd01b85a5/src/test/org/apache/hadoop/hbase/TestTableMapReduce.java",
                "status": "modified",
                "changes": 59,
                "additions": 33,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestTableMapReduce.java?ref=9eb369c26699fc265c2a0e780318513bd01b85a5",
                "patch": "@@ -75,39 +75,46 @@ public void setUp() throws Exception {\n     desc.addFamily(new HColumnDescriptor(OUTPUT_COLUMN));\n     \n     dfsCluster = new MiniDFSCluster(conf, 1, true, (String[])null);\n-    fs = dfsCluster.getFileSystem();\n-    dir = new Path(\"/hbase\");\n-    fs.mkdirs(dir);\n+    try {\n+      fs = dfsCluster.getFileSystem();\n+      dir = new Path(\"/hbase\");\n+      fs.mkdirs(dir);\n \n-    // create the root and meta regions and insert the data region into the meta\n+      // create the root and meta regions and insert the data region into the meta\n \n-    HRegion root = createNewHRegion(fs, dir, conf, HGlobals.rootTableDesc, 0L, null, null);\n-    HRegion meta = createNewHRegion(fs, dir, conf, HGlobals.metaTableDesc, 1L, null, null);\n-    HRegion.addRegionToMETA(root, meta);\n+      HRegion root = createNewHRegion(fs, dir, conf, HGlobals.rootTableDesc, 0L, null, null);\n+      HRegion meta = createNewHRegion(fs, dir, conf, HGlobals.metaTableDesc, 1L, null, null);\n+      HRegion.addRegionToMETA(root, meta);\n \n-    HRegion region = createNewHRegion(fs, dir, conf, desc, rand.nextLong(), null, null);\n-    HRegion.addRegionToMETA(meta, region);\n+      HRegion region = createNewHRegion(fs, dir, conf, desc, rand.nextLong(), null, null);\n+      HRegion.addRegionToMETA(meta, region);\n \n-    // insert some data into the test table\n+      // insert some data into the test table\n \n-    for(int i = 0; i < values.length; i++) {\n-      long lockid = region.startUpdate(new Text(\"row_\"\n-          + String.format(\"%1$05d\", i)));\n+      for(int i = 0; i < values.length; i++) {\n+        long lockid = region.startUpdate(new Text(\"row_\"\n+            + String.format(\"%1$05d\", i)));\n \n-      region.put(lockid, TEXT_INPUT_COLUMN, values[i]);\n-      region.commit(lockid);\n-    }\n+        region.put(lockid, TEXT_INPUT_COLUMN, values[i]);\n+        region.commit(lockid);\n+      }\n \n-    region.close();\n-    region.getLog().closeAndDelete();\n-    meta.close();\n-    meta.getLog().closeAndDelete();\n-    root.close();\n-    root.getLog().closeAndDelete();\n-  \n-    // Start up HBase cluster\n-    \n-    hCluster = new MiniHBaseCluster(conf, 1, dfsCluster);\n+      region.close();\n+      region.getLog().closeAndDelete();\n+      meta.close();\n+      meta.getLog().closeAndDelete();\n+      root.close();\n+      root.getLog().closeAndDelete();\n+\n+      // Start up HBase cluster\n+\n+      hCluster = new MiniHBaseCluster(conf, 1, dfsCluster);\n+      \n+    } catch (Exception e) {\n+      if (dfsCluster != null) {\n+        dfsCluster.shutdown();\n+      }\n+    }\n   }\n \n   @Override",
                "deletions": 26
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10957 HMaster can abort with NPE in #rebuildUserRegions (Nicolas Liochon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/branches/hbase-10070@1590184 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
        "parent": "https://github.com/apache/hbase/commit/48ffa4d5e615c78f6db8f6c2beddd93460887642",
        "bug_id": "hbase_260",
        "file": [
            {
                "sha": "9517113fcc6763c1952dc38326a3c003c1a81159",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java?ref=e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
                "patch": "@@ -718,7 +718,8 @@ private static long getSeqNumDuringOpen(final Result r, final int replicaId) {\n \n   /**\n    * Returns an HRegionLocationList extracted from the result.\n-   * @return an HRegionLocationList containing all locations for the region range\n+   * @return an HRegionLocationList containing all locations for the region range or null if\n+   *  we can't deserialize the result.\n    */\n   public static RegionLocations getRegionLocations(final Result r) {\n     if (r == null) return null;",
                "deletions": 1
            },
            {
                "sha": "f4075698228bf80af018ad7d4283e5a923e9c74f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 9,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=e86b13f75ad2f1e88dc5f225e1297c1fd3ef954a",
                "patch": "@@ -54,6 +54,7 @@\n import org.apache.hadoop.hbase.HRegionLocation;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.NotServingRegionException;\n+import org.apache.hadoop.hbase.RegionLocations;\n import org.apache.hadoop.hbase.RegionTransition;\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n@@ -2760,7 +2761,13 @@ boolean waitUntilNoRegionsInTransition(final long timeout)\n     Set<ServerName> offlineServers = new HashSet<ServerName>();\n     // Iterate regions in META\n     for (Result result : results) {\n-      HRegionLocation[] locations = MetaReader.getRegionLocations(result).getRegionLocations();\n+      if (result == null && LOG.isDebugEnabled()){\n+        LOG.debug(\"null result from meta - ignoring but this is strange.\");\n+        continue;\n+      }\n+      RegionLocations rl =  MetaReader.getRegionLocations(result);\n+      if (rl == null) continue;\n+      HRegionLocation[] locations = rl.getRegionLocations();\n       if (locations == null) continue;\n       for (HRegionLocation hrl : locations) {\n         HRegionInfo regionInfo = hrl.getRegionInfo();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10438 NPE from LRUDictionary when size reaches the max init value (Ramkrishna S. Vasudevan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1562578 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/475420205c05adf821896d6e4c436a428ee8981a",
        "parent": "https://github.com/apache/hbase/commit/5ecb63b539bc99e019a317e47c328d2a4d4d9b18",
        "bug_id": "hbase_261",
        "file": [
            {
                "sha": "513e2e22e9b5615bfe14bfe7d4861dae1b7e70b1",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "blob_url": "https://github.com/apache/hbase/blob/475420205c05adf821896d6e4c436a428ee8981a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "raw_url": "https://github.com/apache/hbase/raw/475420205c05adf821896d6e4c436a428ee8981a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java?ref=475420205c05adf821896d6e4c436a428ee8981a",
                "patch": "@@ -183,6 +183,9 @@ public int compareKey(KVComparator comparator, byte[] key, int offset, int lengt\n \n     @Override\n     public void setCurrentBuffer(ByteBuffer buffer) {\n+      if (this.tagCompressionContext != null) {\n+        this.tagCompressionContext.clear();\n+      }\n       currentBuffer = buffer;\n       decodeFirst();\n       previous.invalidate();",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Amend HBASE-10338 Fix NPE if the server is terminated before the RegionServerCoprocessorHost is initialized (Vandana Ayyalasomayajula)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1560494 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6cc0c86133666a249746a61cc6ce68d68a1e3fb7",
        "parent": "https://github.com/apache/hbase/commit/ca59922702991f9163a7bfe4674e0c5ce9a14f28",
        "bug_id": "hbase_262",
        "file": [
            {
                "sha": "b74358e54f4148f30456aa6553afbda8bab81db0",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/6cc0c86133666a249746a61cc6ce68d68a1e3fb7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/6cc0c86133666a249746a61cc6ce68d68a1e3fb7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=6cc0c86133666a249746a61cc6ce68d68a1e3fb7",
                "patch": "@@ -1734,7 +1734,9 @@ public CatalogTracker getCatalogTracker() {\n   @Override\n   public void stop(final String msg) {\n     try {\n-      this.rsHost.preStop(msg);\n+    \tif (this.rsHost != null) {\n+    \t\tthis.rsHost.preStop(msg);\n+    \t}\n       this.stopped = true;\n       LOG.info(\"STOPPED: \" + msg);\n       // Wakes run() if it is sleeping",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-10061 TableMapReduceUtil.findOrCreateJar calls updateMap(null, ) resulting in thrown NPE (Amit Sela)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1548747 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/b5e2145879254acb416e8414924ab9256d55bd31",
        "parent": "https://github.com/apache/hbase/commit/656b5ca9f73ee9cb806ec39fc32001fa9f32df3c",
        "bug_id": "hbase_263",
        "file": [
            {
                "sha": "cc66882c03aaedf21611960675482825e2c6e04d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/b5e2145879254acb416e8414924ab9256d55bd31/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/b5e2145879254acb416e8414924ab9256d55bd31/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java?ref=b5e2145879254acb416e8414924ab9256d55bd31",
                "patch": "@@ -762,7 +762,7 @@ private static Path findOrCreateJar(Class<?> my_class, FileSystem fs,\n     }\n \n     if (null == jar || jar.isEmpty()) {\n-      throw new IOException(\"Cannot locate resource for class \" + my_class.getName());\n+      return null;\n     }\n \n     LOG.debug(String.format(\"For class %s, using jar %s\", my_class.getName(), jar));\n@@ -776,6 +776,9 @@ private static Path findOrCreateJar(Class<?> my_class, FileSystem fs,\n    * @param packagedClasses map[class -> jar]\n    */\n   private static void updateMap(String jar, Map<String, String> packagedClasses) throws IOException {\n+    if (null == jar || jar.isEmpty()) {\n+      return;\n+    }\n     ZipFile zip = null;\n     try {\n       zip = new ZipFile(jar);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9709: LogReplay throws NPE when no KVs to be replayed in a WALEdit\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1529247 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/26ed93141d68963754eab7807cf77a234bdd76cb",
        "parent": "https://github.com/apache/hbase/commit/4bde0d30f659b37279a781f58b6e12481aaed723",
        "bug_id": "hbase_264",
        "file": [
            {
                "sha": "5ff65759a0380c6413ffdcf870e7eed953f64dda",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "blob_url": "https://github.com/apache/hbase/blob/26ed93141d68963754eab7807cf77a234bdd76cb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "raw_url": "https://github.com/apache/hbase/raw/26ed93141d68963754eab7807cf77a234bdd76cb/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java?ref=26ed93141d68963754eab7807cf77a234bdd76cb",
                "patch": "@@ -1478,7 +1478,7 @@ private void groupEditsByServer(List<Entry> entries) throws IOException {\n         }\n \n         // skip the edit\n-        if (needSkip) continue;\n+        if (loc == null || needSkip) continue;\n \n         if (!skippedKVs.isEmpty()) {\n           kvs.removeAll(skippedKVs);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9672 LoadTestTool NPE's when -num_tables is given, but -tn is not\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1527067 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a7bae17b16ef53796ac26702801a6cbf677b2c2a",
        "parent": "https://github.com/apache/hbase/commit/4ae82009f41be6499184c1395473c60853a09230",
        "bug_id": "hbase_265",
        "file": [
            {
                "sha": "37682cb0263213472d4d77cf03a10bf26b623d6c",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/a7bae17b16ef53796ac26702801a6cbf677b2c2a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/a7bae17b16ef53796ac26702801a6cbf677b2c2a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java",
                "status": "modified",
                "changes": 28,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java?ref=a7bae17b16ef53796ac26702801a6cbf677b2c2a",
                "patch": "@@ -98,7 +98,7 @@\n   public static final String OPT_ENCODE_IN_CACHE_ONLY_USAGE =\n       \"If this is specified, data blocks will only be encoded in block \" +\n       \"cache but not on disk\";\n-  \n+\n   public static final String OPT_INMEMORY = \"in_memory\";\n   public static final String OPT_USAGE_IN_MEMORY = \"Tries to keep the HFiles of the CF \" +\n   \t\t\"inmemory as far as possible.  Not guaranteed that reads are always served from inmemory\";\n@@ -107,7 +107,7 @@\n   \t\t\" only if the HFileV3 version is used\";\n \n   public static final String OPT_NUM_TAGS = \"num_tags\";\n-  public static final String OPT_USAGE_NUM_TAGS = \"Specifies the minimum and number of tags to be\" \n+  public static final String OPT_USAGE_NUM_TAGS = \"Specifies the minimum and number of tags to be\"\n       +      \" added per KV\";\n \n   protected static final String OPT_KEY_WINDOW = \"key_window\";\n@@ -163,7 +163,7 @@\n   private int keyWindow = MultiThreadedReader.DEFAULT_KEY_WINDOW;\n   private int maxReadErrors = MultiThreadedReader.DEFAULT_MAX_ERRORS;\n   private int verifyPercent;\n- \n+\n   private int numTables = 1;\n \n   // TODO: refactor LoadTestToolImpl somewhere to make the usage from tests less bad,\n@@ -261,7 +261,7 @@ protected void addOptions() {\n         DEFAULT_START_KEY + \".\");\n     addOptNoArg(OPT_SKIP_INIT, \"Skip the initialization; assume test table \"\n         + \"already exists\");\n-    \n+\n     addOptWithArg(NUM_TABLES,\n       \"A positive integer number. When a number n is speicfied, load test \"\n           + \"tool  will load n table parallely. -tn parameter value becomes \"\n@@ -367,7 +367,7 @@ protected void processOptions(CommandLine cmd) {\n       System.out.println(\"Percent of keys to verify: \" + verifyPercent);\n       System.out.println(\"Reader threads: \" + numReaderThreads);\n     }\n-    \n+\n     numTables = 1;\n     if(cmd.hasOption(NUM_TABLES)) {\n       numTables = parseInt(cmd.getOptionValue(NUM_TABLES), 1, Short.MAX_VALUE);\n@@ -390,7 +390,7 @@ private void parseColumnFamilyOptions(CommandLine cmd) {\n     String bloomStr = cmd.getOptionValue(OPT_BLOOM);\n     bloomType = bloomStr == null ? null :\n         BloomType.valueOf(bloomStr);\n-    \n+\n     inMemoryCF = cmd.hasOption(OPT_INMEMORY);\n     useTags = cmd.hasOption(OPT_USETAGS);\n     if (useTags) {\n@@ -405,7 +405,7 @@ private void parseColumnFamilyOptions(CommandLine cmd) {\n       System.out.println(\"Using tags, number of tags per KV: min=\" + minNumTags + \", max=\"\n           + maxNumTags);\n     }\n-    \n+\n   }\n \n   public void initTestTable() throws IOException {\n@@ -539,21 +539,22 @@ public static void main(String[] args) {\n   }\n \n   /**\n-   * When NUM_TABLES is specified, the function starts multiple worker threads \n-   * which individually start a LoadTestTool instance to load a table. Each \n+   * When NUM_TABLES is specified, the function starts multiple worker threads\n+   * which individually start a LoadTestTool instance to load a table. Each\n    * table name is in format <tn>_<index>. For example, \"-tn test -num_tables 2\"\n    * , table names will be \"test_1\", \"test_2\"\n-   * \n+   *\n    * @throws IOException\n    */\n-  private int parallelLoadTables() \n+  private int parallelLoadTables()\n       throws IOException {\n     // create new command args\n     String tableName = cmd.getOptionValue(OPT_TABLE_NAME, DEFAULT_TABLE_NAME);\n     String[] newArgs = null;\n     if (!cmd.hasOption(LoadTestTool.OPT_TABLE_NAME)) {\n       newArgs = new String[cmdLineArgs.length + 2];\n       newArgs[0] = \"-\" + LoadTestTool.OPT_TABLE_NAME;\n+      newArgs[1] = LoadTestTool.DEFAULT_TABLE_NAME;\n       for (int i = 0; i < cmdLineArgs.length; i++) {\n         newArgs[i + 2] = cmdLineArgs[i];\n       }\n@@ -567,7 +568,7 @@ private int parallelLoadTables()\n         tableNameValueIndex = j + 1;\n       } else if (newArgs[j].endsWith(NUM_TABLES)) {\n         // change NUM_TABLES to 1 so that each worker loads one table\n-        newArgs[j + 1] = \"1\"; \n+        newArgs[j + 1] = \"1\";\n       }\n     }\n \n@@ -594,7 +595,7 @@ private int parallelLoadTables()\n       }\n       checkForErrors();\n     }\n-    \n+\n     return EXIT_SUCCESS;\n   }\n \n@@ -627,6 +628,7 @@ private void checkForErrors() throws IOException {\n       workerArgs = args;\n     }\n \n+    @Override\n     public void run() {\n       try {\n         int ret = ToolRunner.run(HBaseConfiguration.create(), new LoadTestTool(), workerArgs);",
                "deletions": 13
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9649 HFilePrettyPrinter should not throw a NPE if FirstKey or LastKey is null (Jean-Marc Spaggiari)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1526113 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/f6d44db9c694281ab3df6854c83033cd7dc3d29c",
        "parent": "https://github.com/apache/hbase/commit/91508fd56478e73e125c2eb236338afd1c155f23",
        "bug_id": "hbase_266",
        "file": [
            {
                "sha": "06675b29d590c0106b0b9ad3fd47b63cae6984d0",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "blob_url": "https://github.com/apache/hbase/blob/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "raw_url": "https://github.com/apache/hbase/raw/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java",
                "status": "modified",
                "changes": 5,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java?ref=f6d44db9c694281ab3df6854c83033cd7dc3d29c",
                "patch": "@@ -1130,9 +1130,12 @@ public String toString() {\n \n   /**\n    * @param k Key portion of a KeyValue.\n-   * @return Key as a String.\n+   * @return Key as a String, empty string if k is null. \n    */\n   public static String keyToString(final byte [] k) {\n+    if (k == null) { \n+      return \"\";\n+    }\n     return keyToString(k, 0, k.length);\n   }\n ",
                "deletions": 1
            },
            {
                "sha": "a7484e2c7a774e6a0dab8631beece000f4c19716",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "blob_url": "https://github.com/apache/hbase/blob/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "raw_url": "https://github.com/apache/hbase/raw/f6d44db9c694281ab3df6854c83033cd7dc3d29c/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java?ref=f6d44db9c694281ab3df6854c83033cd7dc3d29c",
                "patch": "@@ -350,7 +350,11 @@ private void printMeta(HFile.Reader reader, Map<byte[], byte[]> fileInfo)\n       }\n     }\n \n-    System.out.println(\"Mid-key: \" + Bytes.toStringBinary(reader.midkey()));\n+    try {\n+      System.out.println(\"Mid-key: \" + Bytes.toStringBinary(reader.midkey()));\n+    } catch (Exception e) {\n+      System.out.println (\"Unable to retrieve the midkey\");\n+    }\n \n     // Printing general bloom information\n     DataInput bloomMeta = reader.getGeneralBloomFilterMetadata();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9541 icv fails w/ npe if client does not support cellblocks\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1523812 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
        "parent": "https://github.com/apache/hbase/commit/2ee04ae2cfbce4a36662176385c1464d531971b7",
        "bug_id": "hbase_267",
        "file": [
            {
                "sha": "da53bec7811d939d2d6d0cfedd2c16ea6a149028",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java",
                "status": "modified",
                "changes": 15,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HTable.java?ref=10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
                "patch": "@@ -1617,4 +1617,17 @@ public int getOperationTimeout() {\n     return operationTimeout;\n   }\n \n-}\n+  /**\n+   * Run basic test.\n+   * @param args Pass table name and row and will get the content.\n+   * @throws IOException\n+   */\n+  public static void main(String[] args) throws IOException {\n+    HTable t = new HTable(HBaseConfiguration.create(), args[0]);\n+    try {\n+      System.out.println(t.get(new Get(Bytes.toBytes(args[1]))));\n+    } finally {\n+      t.close();\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "deletions": 1
            },
            {
                "sha": "7728b7d211aed1ae10e70f0b118256c276056544",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java?ref=10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
                "patch": "@@ -150,9 +150,9 @@ public static Cell createCell(final byte[] row, final byte[] family, final byte[\n    * @param cellScannerables\n    * @return CellScanner interface over <code>cellIterables</code>\n    */\n-  public static CellScanner createCellScanner(final List<CellScannable> cellScannerables) {\n+  public static CellScanner createCellScanner(final List<? extends CellScannable> cellScannerables) {\n     return new CellScanner() {\n-      private final Iterator<CellScannable> iterator = cellScannerables.iterator();\n+      private final Iterator<? extends CellScannable> iterator = cellScannerables.iterator();\n       private CellScanner cellScanner = null;\n \n       @Override",
                "deletions": 2
            },
            {
                "sha": "56c844ccedcc005d89afdae7772298525bc437c9",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 52,
                "additions": 31,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=10eec81e09695e0ba6b8ca36b8a866d66d0d9dd7",
                "patch": "@@ -875,7 +875,7 @@ public void run() {\n       mxBean = null;\n     }\n     if (this.thriftServer != null) this.thriftServer.shutdown();\n-    this.leases.closeAfterLeasesExpire();\n+    if (this.leases != null) this.leases.closeAfterLeasesExpire();\n     this.rpcServer.stop();\n     if (this.splitLogWorker != null) {\n       splitLogWorker.stop();\n@@ -2785,7 +2785,8 @@ public GetResponse get(final RpcController controller,\n       if (existence != null) {\n         builder.setExists(existence.booleanValue());\n       } else if (r != null) {\n-        builder.setResult(ProtobufUtil.toResult(r));\n+        ClientProtos.Result pbr = ProtobufUtil.toResult(r);\n+        builder.setResult(pbr);\n       }\n       return builder.build();\n     } catch (IOException ie) {\n@@ -2810,8 +2811,7 @@ public MultiGetResponse multiGet(final RpcController controller, final MultiGetR\n       requestCount.add(request.getGetCount());\n       HRegion region = getRegion(request.getRegion());\n       MultiGetResponse.Builder builder = MultiGetResponse.newBuilder();\n-      for (ClientProtos.Get get: request.getGetList())\n-      {\n+      for (ClientProtos.Get get: request.getGetList()) {\n         Boolean existence = null;\n         Result r = null;\n         if (request.getClosestRowBefore()) {\n@@ -2947,23 +2947,36 @@ public MutateResponse mutate(final RpcController rpcc,\n           throw new DoNotRetryIOException(\n             \"Unsupported mutate type: \" + type.name());\n       }\n-      CellScannable cellsToReturn = null;\n-      if (processed != null) {\n-        builder.setProcessed(processed.booleanValue());\n-      } else if (r != null) {\n-        builder.setResult(ProtobufUtil.toResultNoData(r));\n-        cellsToReturn = r;\n-      }\n-      if (controller != null && cellsToReturn != null) {\n-        controller.setCellScanner(cellsToReturn.cellScanner());\n-      }\n+      if (processed != null) builder.setProcessed(processed.booleanValue());\n+      addResult(builder, r, controller);\n       return builder.build();\n     } catch (IOException ie) {\n       checkFileSystem();\n       throw new ServiceException(ie);\n     }\n   }\n \n+\n+  /**\n+   * @return True if current call supports cellblocks\n+   */\n+  private boolean isClientCellBlockSupport() {\n+    RpcCallContext context = RpcServer.getCurrentCall();\n+    return context != null && context.isClientCellBlockSupport();\n+  }\n+\n+  private void addResult(final MutateResponse.Builder builder,\n+      final Result result, final PayloadCarryingRpcController rpcc) {\n+    if (result == null) return;\n+    if (isClientCellBlockSupport()) {\n+      builder.setResult(ProtobufUtil.toResultNoData(result));\n+      rpcc.setCellScanner(result.cellScanner());\n+    } else {\n+      ClientProtos.Result pbr = ProtobufUtil.toResult(result);\n+      builder.setResult(pbr);\n+    }\n+  }\n+\n   //\n   // remote scanner interface\n   //\n@@ -3148,7 +3161,7 @@ public ScanResponse scan(final RpcController controller, final ScanRequest reque\n               moreResults = false;\n               results = null;\n             } else {\n-              formatResults(builder, results, controller);\n+              addResults(builder, results, controller);\n             }\n           } finally {\n             // We're done. On way out re-add the above removed lease.\n@@ -3196,18 +3209,15 @@ public ScanResponse scan(final RpcController controller, final ScanRequest reque\n     }\n   }\n \n-  private void formatResults(final ScanResponse.Builder builder, final List<Result> results,\n+  private void addResults(final ScanResponse.Builder builder, final List<Result> results,\n       final RpcController controller) {\n     if (results == null || results.isEmpty()) return;\n-    RpcCallContext context = RpcServer.getCurrentCall();\n-    if (context != null && context.isClientCellBlockSupport()) {\n-      List<CellScannable> cellScannables = new ArrayList<CellScannable>(results.size());\n+    if (isClientCellBlockSupport()) {\n       for (Result res : results) {\n-        cellScannables.add(res);\n         builder.addCellsPerResult(res.size());\n       }\n       ((PayloadCarryingRpcController)controller).\n-        setCellScanner(CellUtil.createCellScanner(cellScannables));\n+        setCellScanner(CellUtil.createCellScanner(results));\n     } else {\n       for (Result res: results) {\n         ClientProtos.Result pbr = ProtobufUtil.toResult(res);",
                "deletions": 21
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-9224 Print out name of the method we do not support rather than throw NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1514102 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/92190aa97364fc4c8d2da521df049e2b6954aced",
        "parent": "https://github.com/apache/hbase/commit/e926483258351fa7ba87931c039fec2c419979bd",
        "bug_id": "hbase_268",
        "file": [
            {
                "sha": "71ebda477d37a759bb278085557c88f3b489aa02",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/92190aa97364fc4c8d2da521df049e2b6954aced/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/92190aa97364fc4c8d2da521df049e2b6954aced/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java?ref=92190aa97364fc4c8d2da521df049e2b6954aced",
                "patch": "@@ -1650,6 +1650,7 @@ protected void processRequest(byte[] buf) throws IOException, InterruptedExcepti\n       try {\n         if (header.hasRequestParam() && header.getRequestParam()) {\n           md = this.service.getDescriptorForType().findMethodByName(header.getMethodName());\n+          if (md == null) throw new UnsupportedOperationException(header.getMethodName());\n           Builder builder = this.service.getRequestPrototype(md).newBuilderForType();\n           // To read the varint, I need an inputstream; might as well be a CIS.\n           cis = CodedInputStream.newInstance(buf, offset, buf.length);",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8975  NPE/HTTP 500 when opening the master's web UI too early\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1504955 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/a97739bd3fe6654c26057630ea5055e97a6eb487",
        "parent": "https://github.com/apache/hbase/commit/581c3f2634e82343ed0d27a4a0ce5d407f445567",
        "bug_id": "hbase_269",
        "file": [
            {
                "sha": "49ab9d6bd80d43653032b346ec2f383e4c5c115a",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/a97739bd3fe6654c26057630ea5055e97a6eb487/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/a97739bd3fe6654c26057630ea5055e97a6eb487/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java?ref=a97739bd3fe6654c26057630ea5055e97a6eb487",
                "patch": "@@ -63,7 +63,11 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n     List<ServerName> servers = null;\n     Set<ServerName> deadServers = null;\n     \n-    if(master.isActiveMaster()){\n+    if(master.isActiveMaster()) {\n+      if (master.getServerManager() == null) {\n+        response.sendError(503, \"Master not ready\");\n+        return;\n+      }\n       metaLocation = getMetaLocationOrNull(master);\n       //ServerName metaLocation = master.getCatalogTracker().getMetaLocation();\n       servers = master.getServerManager().getOnlineServersList();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8814 Possible NPE in split if a region has empty store files; ADDENDUM\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1499801 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/eed3a473435006c62f693cda66b1da3f49154963",
        "parent": "https://github.com/apache/hbase/commit/3631ba5022548377000cc2753ca29b984abc5caa",
        "bug_id": "hbase_270",
        "file": [
            {
                "sha": "b13324647af8863e1b40fbbf2c2ad7d2174bbe81",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/eed3a473435006c62f693cda66b1da3f49154963/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/eed3a473435006c62f693cda66b1da3f49154963/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "status": "modified",
                "changes": 10,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java?ref=eed3a473435006c62f693cda66b1da3f49154963",
                "patch": "@@ -442,7 +442,6 @@ public void testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles() throws Exc\n     HColumnDescriptor hcd = new HColumnDescriptor(\"col\");\n     htd.addFamily(hcd);\n     admin.createTable(htd);\n-    ZKAssign.blockUntilNoRIT(zkw);\n     HTable table = new HTable(conf, userTableName);\n     try {\n       for (int i = 0; i <= 5; i++) {\n@@ -678,12 +677,14 @@ public void testSplitBeforeSettingSplittingInZK() throws Exception,\n \n   @Test(timeout = 60000)\n   public void testTableExistsIfTheSpecifiedTableRegionIsSplitParent() throws Exception {\n+    ZooKeeperWatcher zkw = HBaseTestingUtility.getZooKeeperWatcher(TESTING_UTIL);\n     final byte[] tableName =\n         Bytes.toBytes(\"testTableExistsIfTheSpecifiedTableRegionIsSplitParent\");\n     // Create table then get the single region for our new table.\n     HTable t = createTableAndWait(tableName, Bytes.toBytes(\"cf\"));\n+    List<HRegion> regions = null;\n     try {\n-      List<HRegion> regions = cluster.getRegions(tableName);\n+      regions = cluster.getRegions(tableName);\n       int regionServerIndex = cluster.getServerWith(regions.get(0).getRegionName());\n       HRegionServer regionServer = cluster.getRegionServer(regionServerIndex);\n       insertData(tableName, admin, t);\n@@ -707,6 +708,11 @@ public void testTableExistsIfTheSpecifiedTableRegionIsSplitParent() throws Excep\n           Bytes.toString(tableName));\n       assertEquals(\"The specified table should present.\", true, tableExists);\n     } finally {\n+      if (regions != null) {\n+        String node = ZKAssign.getNodeName(zkw, regions.get(0).getRegionInfo()\n+            .getEncodedName());\n+        ZKUtil.deleteNodeFailSilent(zkw, node);\n+      }\n       admin.setBalancerRunning(true, false);\n       cluster.getMaster().setCatalogJanitorEnabled(true);\n       t.close();",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8814 Possible NPE in split if a region has empty store files\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1499213 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/e13718b872dbf7266c4fc4000855441f0397657b",
        "parent": "https://github.com/apache/hbase/commit/9ccf16db51a9ac0948459dbc8499ffea9b83632f",
        "bug_id": "hbase_271",
        "file": [
            {
                "sha": "f9902cc0e44929ec86e41e7aa428a1df6ac54b59",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "blob_url": "https://github.com/apache/hbase/blob/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "raw_url": "https://github.com/apache/hbase/raw/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java?ref=e13718b872dbf7266c4fc4000855441f0397657b",
                "patch": "@@ -527,6 +527,8 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName,\n       //check if larger than last key.\n       KeyValue splitKey = KeyValue.createFirstOnRow(splitRow);\n       byte[] lastKey = f.createReader().getLastKey();      \n+      // If lastKey is null means storefile is empty.\n+      if (lastKey == null) return null;\n       if (f.getReader().getComparator().compare(splitKey.getBuffer(), \n           splitKey.getKeyOffset(), splitKey.getKeyLength(), lastKey, 0, lastKey.length) > 0) {\n         return null;\n@@ -535,6 +537,8 @@ Path splitStoreFile(final HRegionInfo hri, final String familyName,\n       //check if smaller than first key\n       KeyValue splitKey = KeyValue.createLastOnRow(splitRow);\n       byte[] firstKey = f.createReader().getFirstKey();\n+      // If firstKey is null means storefile is empty.\n+      if (firstKey == null) return null;\n       if (f.getReader().getComparator().compare(splitKey.getBuffer(), \n           splitKey.getKeyOffset(), splitKey.getKeyLength(), firstKey, 0, firstKey.length) < 0) {\n         return null;",
                "deletions": 0
            },
            {
                "sha": "671d3a28a384aef8d22ce989898766369ff07970",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/e13718b872dbf7266c4fc4000855441f0397657b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java",
                "status": "modified",
                "changes": 69,
                "additions": 69,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java?ref=e13718b872dbf7266c4fc4000855441f0397657b",
                "patch": "@@ -40,8 +40,10 @@\n import org.apache.hadoop.hbase.Abortable;\n import org.apache.hadoop.hbase.HBaseIOException;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.LargeTests;\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.RegionTransition;\n@@ -52,6 +54,9 @@\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hbase.exceptions.MasterNotRunningException;\n import org.apache.hadoop.hbase.exceptions.UnknownRegionException;\n@@ -75,6 +80,7 @@\n import org.apache.zookeeper.data.Stat;\n import org.junit.After;\n import org.junit.AfterClass;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -427,6 +433,69 @@ public void run() {\n     }\n   }\n \n+  @Test(timeout = 180000)\n+  public void testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles() throws Exception {\n+    Configuration conf = TESTING_UTIL.getConfiguration();\n+    ZooKeeperWatcher zkw = HBaseTestingUtility.getZooKeeperWatcher(TESTING_UTIL);\n+    String userTableName = \"testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles\";\n+    HTableDescriptor htd = new HTableDescriptor(userTableName);\n+    HColumnDescriptor hcd = new HColumnDescriptor(\"col\");\n+    htd.addFamily(hcd);\n+    admin.createTable(htd);\n+    ZKAssign.blockUntilNoRIT(zkw);\n+    HTable table = new HTable(conf, userTableName);\n+    try {\n+      for (int i = 0; i <= 5; i++) {\n+        String row = \"row\" + i;\n+        Put p = new Put(row.getBytes());\n+        String val = \"Val\" + i;\n+        p.add(\"col\".getBytes(), \"ql\".getBytes(), val.getBytes());\n+        table.put(p);\n+        admin.flush(userTableName);\n+        Delete d = new Delete(row.getBytes());\n+        // Do a normal delete\n+        table.delete(d);\n+        admin.flush(userTableName);\n+      }\n+      admin.majorCompact(userTableName);\n+      List<HRegionInfo> regionsOfTable = TESTING_UTIL.getMiniHBaseCluster()\n+          .getMaster().getAssignmentManager().getRegionStates()\n+          .getRegionsOfTable(userTableName.getBytes());\n+      HRegionInfo hRegionInfo = regionsOfTable.get(0);\n+      Put p = new Put(\"row6\".getBytes());\n+      p.add(\"col\".getBytes(), \"ql\".getBytes(), \"val\".getBytes());\n+      table.put(p);\n+      p = new Put(\"row7\".getBytes());\n+      p.add(\"col\".getBytes(), \"ql\".getBytes(), \"val\".getBytes());\n+      table.put(p);\n+      p = new Put(\"row8\".getBytes());\n+      p.add(\"col\".getBytes(), \"ql\".getBytes(), \"val\".getBytes());\n+      table.put(p);\n+      admin.flush(userTableName);\n+      admin.split(hRegionInfo.getRegionName(), \"row7\".getBytes());\n+      regionsOfTable = TESTING_UTIL.getMiniHBaseCluster().getMaster()\n+          .getAssignmentManager().getRegionStates()\n+          .getRegionsOfTable(userTableName.getBytes());\n+\n+      while (regionsOfTable.size() != 2) {\n+        Thread.sleep(2000);\n+        regionsOfTable = TESTING_UTIL.getMiniHBaseCluster().getMaster()\n+            .getAssignmentManager().getRegionStates()\n+            .getRegionsOfTable(userTableName.getBytes());\n+      }\n+      Assert.assertEquals(2, regionsOfTable.size());\n+      Scan s = new Scan();\n+      ResultScanner scanner = table.getScanner(s);\n+      int mainTableCount = 0;\n+      for (Result rr = scanner.next(); rr != null; rr = scanner.next()) {\n+        mainTableCount++;\n+      }\n+      Assert.assertEquals(3, mainTableCount);\n+    } finally {\n+      table.close();\n+    }\n+  }\n+\n   /**\n    * Noop Abortable implementation used below in tests.\n    */",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8657 Miscellaneous log fixups for hbase-it; tidier logging, fix a few NPEs\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1487945 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/c920eb3c4c3fba8a52a6be8794e63d865a38196b",
        "parent": "https://github.com/apache/hbase/commit/957f580d8390ee77673280003bfd5a51b22504c6",
        "bug_id": "hbase_272",
        "file": [
            {
                "sha": "ebab79921ae1b73336b4d76ed5a0d6c0445fef5e",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -93,9 +93,8 @@ public ClientScanner(final Configuration conf, final Scan scan,\n     public ClientScanner(final Configuration conf, final Scan scan,\n       final byte[] tableName, HConnection connection) throws IOException {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Creating scanner over \"\n-            + Bytes.toString(tableName)\n-            + \" starting at key '\" + Bytes.toStringBinary(scan.getStartRow()) + \"'\");\n+        LOG.debug(\"Scan table=\" + Bytes.toString(tableName)\n+            + \", startRow=\" + Bytes.toStringBinary(scan.getStartRow()));\n       }\n       this.scan = scan;\n       this.tableName = tableName;\n@@ -192,7 +191,7 @@ private boolean nextScanner(final boolean done)\n             done) {\n           close();\n           if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Finished scanning region \" + this.currentRegion);\n+            LOG.debug(\"Finished region=\" + this.currentRegion);\n           }\n           return false;\n         }",
                "deletions": 4
            },
            {
                "sha": "35d44dac9af6c8938a49195894860451d5a16489",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 33,
                "additions": 19,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -1188,14 +1188,18 @@ private void cacheLocation(final byte [] tableName, final HRegionLocation source\n         }\n       }\n       if (isNewCacheEntry) {\n-        LOG.debug(\"Cached location for \" +\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Cached location for \" +\n             location.getRegionInfo().getRegionNameAsString() +\n             \" is \" + location.getHostnamePort());\n+        }\n       } else if (isStaleUpdate && !location.equals(oldLocation)) {\n-        LOG.debug(\"Ignoring stale location update for \"\n-          + location.getRegionInfo().getRegionNameAsString() + \": \"\n-          + location.getHostnamePort() + \" at \" + location.getSeqNum() + \"; local \"\n-          + oldLocation.getHostnamePort() + \" at \" + oldLocation.getSeqNum());\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Ignoring stale location update for \"\n+            + location.getRegionInfo().getRegionNameAsString() + \": \"\n+            + location.getHostnamePort() + \" at \" + location.getSeqNum() + \"; local \"\n+            + oldLocation.getHostnamePort() + \" at \" + oldLocation.getSeqNum());\n+        }\n       }\n     }\n \n@@ -1388,7 +1392,7 @@ Object makeStub() throws MasterNotRunningException {\n                 // tries at this point is 1 or more; decrement to start from 0.\n                 long pauseTime = ConnectionUtils.getPauseTime(pause, tries - 1);\n                 LOG.info(\"getMaster attempt \" + tries + \" of \" + numTries +\n-                    \" failed; retrying after sleep of \" +pauseTime + \", exception=\" +\n+                    \" failed; retrying after sleep of \" + pauseTime + \", exception=\" +\n                   exceptionCaught);\n \n                 try {\n@@ -2217,10 +2221,11 @@ private void submit(List<Action<R>> actionsList, final boolean isRetry) throws I\n           if (LOG.isTraceEnabled() && isRetry) {\n             StringBuilder sb = new StringBuilder();\n             for (Action<R> action : e.getValue().allActions()) {\n-              sb.append(Bytes.toStringBinary(action.getAction().getRow())).append(';');\n+              if (sb.length() > 0) sb.append(' ');\n+              sb.append(Bytes.toStringBinary(action.getAction().getRow()));\n             }\n-            LOG.trace(\"Will retry requests to [\" + e.getKey().getHostnamePort()\n-              + \"] after delay of [\" + backoffTime + \"] for rows [\" + sb.toString() + \"]\");\n+            LOG.trace(\"Attempt #\" + this.curNumRetries + \" against \" + e.getKey().getHostnamePort()\n+              + \" after=\" + backoffTime + \"ms, row(s)=\" + sb.toString());\n           }\n           Triple<MultiAction<R>, HRegionLocation, Future<MultiResponse>> p =\n             new Triple<MultiAction<R>, HRegionLocation, Future<MultiResponse>>(\n@@ -2280,11 +2285,10 @@ private void processBatchCallback() throws IOException, InterruptedException {\n         //  we had more than numRetries for any action\n         //  In this case, we will finish the current retries but we won't start new ones.\n         boolean lastRetry = false;\n-        // despite its name numRetries means number of tries. So if numRetries == 1 it means we\n-        //  won't retry. And we compare vs. 2 in case someone set it to zero.\n+        // If hci.numTries is 1 or 0, we do not retry.\n         boolean noRetry = (hci.numTries < 2);\n \n-        // Analyze and resubmit until all actions are done successfully or failed after numRetries\n+        // Analyze and resubmit until all actions are done successfully or failed after numTries\n         while (!this.inProgress.isEmpty()) {\n           // We need the original multi action to find out what actions to replay if\n           //  we have a 'total' failure of the Future<MultiResponse>\n@@ -2355,8 +2359,9 @@ private void processBatchCallback() throws IOException, InterruptedException {\n           // Retry all actions in toReplay then clear it.\n           if (!noRetry && !toReplay.isEmpty()) {\n             if (isTraceEnabled) {\n-              LOG.trace(\"Retrying due to errors\" + (lastRetry ? \" (one last time)\" : \"\")\n-                   + \": \" + retriedErrors.getDescriptionAndClear());\n+              LOG.trace(\"Retrying #\" + this.curNumRetries +\n+                (lastRetry ? \" (one last time)\": \"\") + \" because \" +\n+                retriedErrors.getDescriptionAndClear());\n             }\n             doRetry();\n             if (lastRetry) {",
                "deletions": 14
            },
            {
                "sha": "bdcf32638cb48eff090340718b4cf302b281d46a",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -156,8 +156,8 @@ public static void metaScan(Configuration configuration,\n       int rows = Math.min(rowLimit, configuration.getInt(HConstants.HBASE_META_SCANNER_CACHING,\n         HConstants.DEFAULT_HBASE_META_SCANNER_CACHING));\n       scan.setCaching(rows);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Scanning \" + Bytes.toString(metaTableName) + \" starting at row=\" +\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"Scanning \" + Bytes.toString(metaTableName) + \" starting at row=\" +\n           Bytes.toStringBinary(startRow) + \" for max=\" + rowUpperLimit + \" with caching=\" + rows);\n       }\n       // Run the scan",
                "deletions": 2
            },
            {
                "sha": "a1e22858d490f3e29d5f46d2616aeb2aaf2f67a0",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -126,13 +126,14 @@ public String getExhaustiveDescription() {\n       Throwable t = this.exceptions.get(i);\n       Row action = this.actions.get(i);\n       String server = this.hostnameAndPort.get(i);\n-      pw.append(\"Error\");\n+      pw.append(\"exception\");\n       if (this.exceptions.size() > 1) {\n         pw.append(\" #\" + i);\n       }\n-      pw.append(\" from [\" + server + \"] for [\"\n-        + ((action == null) ? \"unknown key\" : Bytes.toStringBinary(action.getRow())) + \"]\");\n+      pw.append(\" from \" + server + \" for \"\n+        + ((action == null) ? \"unknown key\" : Bytes.toStringBinary(action.getRow())));\n       if (t != null) {\n+        pw.println();\n         t.printStackTrace(pw);\n       }\n     }",
                "deletions": 3
            },
            {
                "sha": "d98e461d4d2c4aeeb689a59058124837aff68ab5",
                "filename": "hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -94,6 +94,10 @@ public ServerCallable(HConnection connection, byte [] tableName, byte [] row, in\n    */\n   public void prepare(final boolean reload) throws IOException {\n     this.location = connection.getRegionLocation(tableName, row, reload);\n+    if (this.location == null) {\n+      throw new IOException(\"Failed to find location, tableName=\" + tableName + \", row=\" +\n+        Bytes.toString(row) + \", reload=\" + reload);\n+    }\n     this.stub = connection.getClient(location.getServerName());\n   }\n \n@@ -169,7 +173,7 @@ public T withRetries()\n         prepare(tries != 0); // if called with false, check table status on ZK\n         return call();\n       } catch (Throwable t) {\n-        LOG.warn(\"Call exception, tries=\" + tries + \", numRetries=\" + numRetries + \": \" + t);\n+        LOG.warn(\"Call exception, tries=\" + tries + \", numRetries=\" + numRetries, t);\n \n         t = translateException(t);\n         // translateException throws an exception when we should not retry, i.e. when it's the",
                "deletions": 1
            },
            {
                "sha": "e18d9da64a149607eb10f053ada4e8414b3b4173",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -574,7 +574,7 @@\n   /**\n    * Default value of {@link #HBASE_CLIENT_RETRIES_NUMBER}.\n    */\n-  public static int DEFAULT_HBASE_CLIENT_RETRIES_NUMBER = 10;\n+  public static int DEFAULT_HBASE_CLIENT_RETRIES_NUMBER = 20;\n \n   /**\n    * Parameter name for client prefetch limit, used as the maximum number of regions",
                "deletions": 1
            },
            {
                "sha": "9d91a58a9e4003ae99638a3b62149ae0ca066a77",
                "filename": "hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestSlowDeterministic.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -31,13 +31,13 @@\n /**\n  * A system test which does large data ingestion and verify using {@link LoadTestTool}.\n  * It performs a set of actions deterministically using ChaosMonkey, then starts killing\n- * things randomly. You can configure how long should the load test run by using \n+ * things randomly. You can configure how long should the load test run by using\n  * \"hbase.IntegrationTestDataIngestSlowDeterministic.runtime\" configuration parameter.\n  */\n @Category(IntegrationTests.class)\n public class IntegrationTestDataIngestSlowDeterministic extends IngestIntegrationTestBase {\n   private static final int SERVER_COUNT = 4; // number of slaves for the smallest cluster\n-  private static final long DEFAULT_RUN_TIME = 30 * 60 * 1000;\n+  private static final long DEFAULT_RUN_TIME = 10 * 60 * 1000;\n   private static final long CHAOS_EVERY_MS = 150 * 1000; // Chaos every 2.5 minutes.\n \n   private ChaosMonkey monkey;",
                "deletions": 2
            },
            {
                "sha": "aa252ee0a621bf02846066e507807d1f4e5d0ae2",
                "filename": "hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-it/src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -22,13 +22,11 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n-import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n import java.util.Queue;\n import java.util.Random;\n-import java.util.Set;\n \n import org.apache.commons.cli.CommandLine;\n import org.apache.commons.logging.Log;\n@@ -38,8 +36,8 @@\n import org.apache.hadoop.hbase.HBaseCluster;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HRegionInfo;\n-import org.apache.hadoop.hbase.IntegrationTestingUtility;\n import org.apache.hadoop.hbase.IntegrationTestDataIngestWithChaosMonkey;\n+import org.apache.hadoop.hbase.IntegrationTestingUtility;\n import org.apache.hadoop.hbase.ServerLoad;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.Stoppable;\n@@ -49,7 +47,6 @@\n \n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n-import com.google.protobuf.ServiceException;\n \n /**\n  * A utility to injects faults in a running cluster.\n@@ -158,6 +155,7 @@ void init(ActionContext context) throws Exception {\n     /** Returns current region servers */\n     protected ServerName[] getCurrentServers() throws IOException {\n       Collection<ServerName> regionServers = cluster.getClusterStatus().getServers();\n+      if (regionServers == null || regionServers.size() <= 0) return new ServerName [] {};\n       return regionServers.toArray(new ServerName[regionServers.size()]);\n     }\n ",
                "deletions": 4
            },
            {
                "sha": "41ba1262641a61281c78267b9cd2c30f5617c64d",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java",
                "blob_url": "https://github.com/apache/hbase/blob/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java",
                "raw_url": "https://github.com/apache/hbase/raw/c920eb3c4c3fba8a52a6be8794e63d865a38196b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java?ref=c920eb3c4c3fba8a52a6be8794e63d865a38196b",
                "patch": "@@ -173,8 +173,8 @@ public void run() {\n   }\n \n   public void insert(HTable table, Put put, long keyBase) {\n+    long start = System.currentTimeMillis();\n     try {\n-      long start = System.currentTimeMillis();\n       table.put(put);\n       totalOpTimeMs.addAndGet(System.currentTimeMillis() - start);\n     } catch (IOException e) {\n@@ -190,8 +190,8 @@ public void insert(HTable table, Put put, long keyBase) {\n         pw.flush();\n         exceptionInfo = StringUtils.stringifyException(e);\n       }\n-      LOG.error(\"Failed to insert: \" + keyBase + \"; region information: \"\n-          + getRegionDebugInfoSafe(table, put.getRow()) + \"; errors: \"\n+      LOG.error(\"Failed to insert: \" + keyBase + \" after \" + (System.currentTimeMillis() - start) +\n+        \"ms; region information: \" + getRegionDebugInfoSafe(table, put.getRow()) + \"; errors: \"\n           + exceptionInfo);\n     }\n   }",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8096 [replication] NPE while replicating a log that is acquiring a new block from HDFS\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1467662 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
        "parent": "https://github.com/apache/hbase/commit/64863bb03e4ca4e747503209794cdc1c85c396c7",
        "bug_id": "hbase_273",
        "file": [
            {
                "sha": "fe6924c91e092e19f439abae654c6e783c82eb2f",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java",
                "status": "modified",
                "changes": 6,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java?ref=6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
                "patch": "@@ -68,7 +68,11 @@ public ReplicationHLogReaderManager(FileSystem fs, Configuration conf) {\n       this.reader = HLogFactory.createReader(this.fs, path, this.conf);\n       this.lastPath = path;\n     } else {\n-      this.reader.reset();\n+      try {\n+        this.reader.reset();\n+      } catch (NullPointerException npe) {\n+        throw new IOException(\"NPE resetting reader, likely HDFS-4380\", npe);\n+      }\n     }\n     return this.reader;\n   }",
                "deletions": 1
            },
            {
                "sha": "bde35466ac747db05ddc1997f13bec4b2bb9ec87",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "blob_url": "https://github.com/apache/hbase/blob/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "raw_url": "https://github.com/apache/hbase/raw/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
                "status": "modified",
                "changes": 11,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
                "patch": "@@ -622,9 +622,14 @@ protected boolean openReader(int sleepMultiplier) {\n     } catch (IOException ioe) {\n       LOG.warn(peerClusterZnode + \" Got: \", ioe);\n       this.reader = null;\n-      // TODO Need a better way to determinate if a file is really gone but\n-      // TODO without scanning all logs dir\n-      if (sleepMultiplier == this.maxRetriesMultiplier) {\n+      if (ioe.getCause() instanceof NullPointerException) {\n+        // Workaround for race condition in HDFS-4380\n+        // which throws a NPE if we open a file before any data node has the most recent block\n+        // Just sleep and retry. Will require re-reading compressed HLogs for compressionContext.\n+        LOG.warn(\"Got NPE opening reader, will retry.\");\n+      } else if (sleepMultiplier == this.maxRetriesMultiplier) {\n+        // TODO Need a better way to determine if a file is really gone but\n+        // TODO without scanning all logs dir\n         LOG.warn(\"Waited too long for this file, considering dumping\");\n         return !processEndOfFile();\n       }",
                "deletions": 3
            },
            {
                "sha": "b9219668db4df29c2bbc42996ef6573692e10552",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "blob_url": "https://github.com/apache/hbase/blob/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "raw_url": "https://github.com/apache/hbase/raw/6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java?ref=6c1e484d36ffdcb654b0cf3ef0a74924ad120cb4",
                "patch": "@@ -428,7 +428,7 @@ public void loadTesting() throws Exception {\n     Result[] res = scanner.next(NB_ROWS_IN_BIG_BATCH);\n     scanner.close();\n \n-    assertEquals(NB_ROWS_IN_BATCH *10, res.length);\n+    assertEquals(NB_ROWS_IN_BIG_BATCH, res.length);\n \n     scan = new Scan();\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-8230 Possible NPE on regionserver abort if replication service has not been started\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1464280 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/c4911d3230749947f107f2188b4da1615bb1fab1",
        "parent": "https://github.com/apache/hbase/commit/e7bebd470650488cafd94a37462e6c1fd7833edf",
        "bug_id": "hbase_274",
        "file": [
            {
                "sha": "71aa2c76ea8241a2dcc70c4610759916782be527",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "blob_url": "https://github.com/apache/hbase/blob/c4911d3230749947f107f2188b4da1615bb1fab1/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "raw_url": "https://github.com/apache/hbase/raw/c4911d3230749947f107f2188b4da1615bb1fab1/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=c4911d3230749947f107f2188b4da1615bb1fab1",
                "patch": "@@ -154,7 +154,9 @@ public void stopReplicationService() {\n   public void join() {\n     if (this.replication) {\n       this.replicationManager.join();\n-      this.replicationSink.stopReplicationSinkServices();\n+      if (this.replicationSink != null) {\n+        this.replicationSink.stopReplicationSinkServices();\n+      }\n     }\n   }\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7982 TestReplicationQueueFailover* runs for a minute, spews 3/4million lines complaining 'Filesystem closed', has an NPE, and still passes?\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1453712 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
        "parent": "https://github.com/apache/hbase/commit/a92c7f761247de70a5ac98c4026ab9fadbec479a",
        "bug_id": "hbase_275",
        "file": [
            {
                "sha": "4a8e5bf47c24f4e15591a8928f590be9046d33ad",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 22,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "patch": "@@ -1159,23 +1159,27 @@ private void waitOnAllRegionsToClose(final boolean abort) {\n   }\n \n   private void closeWAL(final boolean delete) {\n-    try {\n-      if (this.hlogForMeta != null) {\n-        //All hlogs (meta and non-meta) are in the same directory. Don't call \n-        //closeAndDelete here since that would delete all hlogs not just the \n-        //meta ones. We will just 'close' the hlog for meta here, and leave\n-        //the directory cleanup to the follow-on closeAndDelete call.\n+    if (this.hlogForMeta != null) {\n+      // All hlogs (meta and non-meta) are in the same directory. Don't call\n+      // closeAndDelete here since that would delete all hlogs not just the\n+      // meta ones. We will just 'close' the hlog for meta here, and leave\n+      // the directory cleanup to the follow-on closeAndDelete call.\n+      try {\n         this.hlogForMeta.close();\n+      } catch (Throwable e) {\n+        LOG.error(\"Metalog close and delete failed\", RemoteExceptionHandler.checkThrowable(e));\n       }\n-      if (this.hlog != null) {\n+    }\n+    if (this.hlog != null) {\n+      try {\n         if (delete) {\n           hlog.closeAndDelete();\n         } else {\n           hlog.close();\n         }\n+      } catch (Throwable e) {\n+        LOG.error(\"Close and delete failed\", RemoteExceptionHandler.checkThrowable(e));\n       }\n-    } catch (Throwable e) {\n-      LOG.error(\"Close and delete failed\", RemoteExceptionHandler.checkThrowable(e));\n     }\n   }\n ",
                "deletions": 9
            },
            {
                "sha": "5ec4a65e9a19fc2fde2e44640aa09ce2f521825d",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java",
                "status": "modified",
                "changes": 25,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "patch": "@@ -193,9 +193,9 @@ public HLogSplitter(Configuration conf, Path rootDir, Path srcDir,\n \n     status = TaskMonitor.get().createStatus(\n         \"Splitting logs in \" + srcDir);\n-    \n+\n     long startTime = EnvironmentEdgeManager.currentTimeMillis();\n-    \n+\n     status.setStatus(\"Determining files to split...\");\n     List<Path> splits = null;\n     if (!fs.exists(srcDir)) {\n@@ -219,7 +219,7 @@ public HLogSplitter(Configuration conf, Path rootDir, Path srcDir,\n     LOG.info(msg);\n     return splits;\n   }\n-  \n+\n   private void logAndReport(String msg) {\n     status.setStatus(msg);\n     LOG.info(msg);\n@@ -321,7 +321,7 @@ void setDistributedLogSplittingHelper(DistributedLogSplittingHelper helper) {\n         }\n       }\n       status.setStatus(\"Log splits complete. Checking for orphaned logs.\");\n-      \n+\n       if (fs.listStatus(srcDir).length > processedLogs.size()\n           + corruptedLogs.size()) {\n         throw new OrphanHLogAfterSplitException(\n@@ -511,7 +511,12 @@ public static void finishSplitLogFile(Path rootdir, Path oldLogDir,\n     List<Path> corruptedLogs = new ArrayList<Path>();\n     FileSystem fs;\n     fs = rootdir.getFileSystem(conf);\n-    Path logPath = new Path(logfile);\n+    Path logPath = null;\n+    if (FSUtils.isStartingWithPath(rootdir, logfile)) {\n+      logPath = new Path(logfile);\n+    } else {\n+      logPath = new Path(rootdir, logfile);\n+    }\n     if (ZKSplitLog.isCorrupted(rootdir, logPath.getName(), fs)) {\n       corruptedLogs.add(logPath);\n     } else {\n@@ -842,7 +847,7 @@ void appendEntry(Entry entry) throws InterruptedException, IOException {\n           buffer = new RegionEntryBuffer(key.getTablename(), key.getEncodedRegionName());\n           buffers.put(key.getEncodedRegionName(), buffer);\n         }\n-        incrHeap= buffer.appendEntry(entry);        \n+        incrHeap= buffer.appendEntry(entry);\n       }\n \n       // If we crossed the chunk threshold, wait for more space to be available\n@@ -1092,7 +1097,7 @@ private boolean reportProgressIfIsDistributedLogSplitting() {\n \n   /**\n    * A class used in distributed log splitting\n-   * \n+   *\n    */\n   class DistributedLogSplittingHelper {\n     // Report progress, only used in distributed log splitting\n@@ -1143,7 +1148,7 @@ private boolean reportProgress() {\n         new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR));\n \n     private boolean closeAndCleanCompleted = false;\n-    \n+\n     private boolean logWritersClosed  = false;\n \n     private final int numThreads;\n@@ -1171,7 +1176,7 @@ synchronized void startWriterThreads() {\n     }\n \n     /**\n-     * \n+     *\n      * @return null if failed to report progress\n      * @throws IOException\n      */\n@@ -1303,7 +1308,7 @@ public Void call() throws Exception {\n       }\n       return paths;\n     }\n-    \n+\n     private List<IOException> closeLogWriters(List<IOException> thrown)\n         throws IOException {\n       if (!logWritersClosed) {",
                "deletions": 10
            },
            {
                "sha": "680def3e48fa131dd8bf2a4c789f0d70c81e061e",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java",
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java",
                "status": "modified",
                "changes": 55,
                "additions": 55,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "patch": "@@ -86,6 +86,61 @@ protected FSUtils() {\n     super();\n   }\n \n+  /**\n+   * Compare of path component. Does not consider schema; i.e. if schemas different but <code>path\n+   * <code> starts with <code>rootPath<code>, then the function returns true\n+   * @param rootPath\n+   * @param path \n+   * @return True if <code>path</code> starts with <code>rootPath</code>\n+   */\n+  public static boolean isStartingWithPath(final Path rootPath, final String path) {\n+    String uriRootPath = rootPath.toUri().getPath();\n+    String tailUriPath = (new Path(path)).toUri().getPath();\n+    return tailUriPath.startsWith(uriRootPath);\n+  }\n+\n+  /**\n+   * Compare path component of the Path URI; e.g. if hdfs://a/b/c and /a/b/c, it will compare the\n+   * '/a/b/c' part. Does not consider schema; i.e. if schemas different but path or subpath matches,\n+   * the two will equate.\n+   * @param pathToSearch Path we will be trying to match.\n+   * @param pathTail\n+   * @return True if <code>pathTail</code> is tail on the path of <code>pathToSearch</code>\n+   */\n+  public static boolean isMatchingTail(final Path pathToSearch, String pathTail) {\n+    return isMatchingTail(pathToSearch, new Path(pathTail));\n+  }\n+\n+  /**\n+   * Compare path component of the Path URI; e.g. if hdfs://a/b/c and /a/b/c, it will compare the\n+   * '/a/b/c' part. If you passed in 'hdfs://a/b/c and b/c, it would return true.  Does not consider\n+   * schema; i.e. if schemas different but path or subpath matches, the two will equate.\n+   * @param pathToSearch Path we will be trying to match.\n+   * @param pathTail\n+   * @return True if <code>pathTail</code> is tail on the path of <code>pathToSearch</code>\n+   */\n+  public static boolean isMatchingTail(final Path pathToSearch, final Path pathTail) {\n+    if (pathToSearch.depth() != pathTail.depth()) return false;\n+    Path tailPath = pathTail;\n+    String tailName;\n+    Path toSearch = pathToSearch;\n+    String toSearchName;\n+    boolean result = false;\n+    do {\n+      tailName = tailPath.getName();\n+      if (tailName == null || tailName.length() <= 0) {\n+        result = true;\n+        break;\n+      }\n+      toSearchName = toSearch.getName();\n+      if (toSearchName == null || toSearchName.length() <= 0) break;\n+      // Move up a parent on each path for next go around.  Path doesn't let us go off the end.\n+      tailPath = tailPath.getParent();\n+      toSearch = toSearch.getParent();\n+    } while(tailName.equals(toSearchName));\n+    return result;\n+  }\n+\n   public static FSUtils getInstance(FileSystem fs, Configuration conf) {\n     String scheme = fs.getUri().getScheme();\n     if (scheme == null) {",
                "deletions": 0
            },
            {
                "sha": "e2d3c3e3da71b8d883cfc3e8f36bb247b052a890",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java",
                "blob_url": "https://github.com/apache/hbase/blob/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java",
                "raw_url": "https://github.com/apache/hbase/raw/21ea0b5ecf8bd410acba23d7158fa16f9a08d865/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java",
                "status": "modified",
                "changes": 29,
                "additions": 28,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java?ref=21ea0b5ecf8bd410acba23d7158fa16f9a08d865",
                "patch": "@@ -34,12 +34,12 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n-import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HDFSBlocksDistribution;\n import org.apache.hadoop.hbase.MediumTests;\n+import org.apache.hadoop.hbase.exceptions.DeserializationException;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n@@ -49,6 +49,33 @@\n  */\n @Category(MediumTests.class)\n public class TestFSUtils {\n+  /**\n+   * Test path compare and prefix checking.\n+   * @throws IOException \n+   */\n+  @Test\n+  public void testMatchingTail() throws IOException {\n+    HBaseTestingUtility htu = new HBaseTestingUtility();\n+    final FileSystem fs = htu.getTestFileSystem();\n+    Path rootdir = htu.getDataTestDir();\n+    assertTrue(rootdir.depth() > 1);\n+    Path partPath = new Path(\"a\", \"b\");\n+    Path fullPath = new Path(rootdir, partPath);\n+    Path fullyQualifiedPath = fs.makeQualified(fullPath);\n+    assertFalse(FSUtils.isMatchingTail(fullPath, partPath));\n+    assertFalse(FSUtils.isMatchingTail(fullPath, partPath.toString()));\n+    assertTrue(FSUtils.isStartingWithPath(rootdir, fullPath.toString()));\n+    assertTrue(FSUtils.isStartingWithPath(fullyQualifiedPath, fullPath.toString()));\n+    assertFalse(FSUtils.isStartingWithPath(rootdir, partPath.toString()));\n+    assertFalse(FSUtils.isMatchingTail(fullyQualifiedPath, partPath));\n+    assertTrue(FSUtils.isMatchingTail(fullyQualifiedPath, fullPath));\n+    assertTrue(FSUtils.isMatchingTail(fullyQualifiedPath, fullPath.toString()));\n+    assertTrue(FSUtils.isMatchingTail(fullyQualifiedPath, fs.makeQualified(fullPath)));\n+    assertTrue(FSUtils.isStartingWithPath(rootdir, fullyQualifiedPath.toString()));\n+    assertFalse(FSUtils.isMatchingTail(fullPath, new Path(\"x\")));\n+    assertFalse(FSUtils.isMatchingTail(new Path(\"x\"), fullPath));\n+  }\n+\n   @Test\n   public void testVersion() throws DeserializationException, IOException {\n     HBaseTestingUtility htu = new HBaseTestingUtility();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7528 Unhelpful NPE in hbck -repair when adopting orphans if no tableinfo is found (Sergey Shelukhin)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1431637 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/2eb9249ac233008ff4e280a1f5a20bbb43a97ab2",
        "parent": "https://github.com/apache/hbase/commit/f26e14a1b1e2ef51e8bca9132d98ccd29270ec19",
        "bug_id": "hbase_276",
        "file": [
            {
                "sha": "132f15da4504a34ec7b2faa121d9b9d4d9793d60",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "blob_url": "https://github.com/apache/hbase/blob/2eb9249ac233008ff4e280a1f5a20bbb43a97ab2/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "raw_url": "https://github.com/apache/hbase/raw/2eb9249ac233008ff4e280a1f5a20bbb43a97ab2/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java?ref=2eb9249ac233008ff4e280a1f5a20bbb43a97ab2",
                "patch": "@@ -482,7 +482,7 @@ private void adoptHdfsOrphan(HbckInfo hi) throws IOException {\n \n     String tableName = Bytes.toString(hi.getTableName());\n     TableInfo tableInfo = tablesInfo.get(tableName);\n-    Preconditions.checkNotNull(\"Table \" + tableName + \"' not present!\", tableInfo);\n+    Preconditions.checkNotNull(tableInfo, \"Table '\" + tableName + \"' not present!\");\n     HTableDescriptor template = tableInfo.getHTD();\n \n     // find min and max key values",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7450 orphan RPC connection in HBaseClient leaves \"null\" out member, causing NPE in HCM (Zavier Gao)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1429017 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/94bd157d214f05e820d2ea7521e685f4e00f24af",
        "parent": "https://github.com/apache/hbase/commit/e107a640877bf0bfdefc8efaa08b1a94bb670f01",
        "bug_id": "hbase_277",
        "file": [
            {
                "sha": "bbb91d3ef722d0cd0733bb223e38ca6e1f753fd9",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "status": "modified",
                "changes": 12,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=94bd157d214f05e820d2ea7521e685f4e00f24af",
                "patch": "@@ -847,11 +847,17 @@ public Boolean run() throws IOException {\n           start();\n           return;\n         }\n-      } catch (IOException e) {\n+      } catch (Throwable t) {\n         failedServers.addToFailedServers(remoteId.address);\n-        markClosed(e);\n+        IOException e = null;\n+        if (t instanceof IOException) {\n+          e = (IOException)t;\n+          markClosed(e);\n+        } else {\n+          e = new IOException(\"Coundn't set up IO Streams\", t);\n+          markClosed(e);\n+        }\n         close();\n-\n         throw e;\n       }\n     }",
                "deletions": 3
            },
            {
                "sha": "a1fa7b6a2e68a192d19370c9b96aaf9c0979590e",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java",
                "blob_url": "https://github.com/apache/hbase/blob/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java",
                "raw_url": "https://github.com/apache/hbase/raw/94bd157d214f05e820d2ea7521e685f4e00f24af/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java",
                "status": "added",
                "changes": 100,
                "additions": 100,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/ipc/TestIPC.java?ref=94bd157d214f05e820d2ea7521e685f4e00f24af",
                "patch": "@@ -0,0 +1,100 @@\n+/**\n+  *\n+  * Licensed to the Apache Software Foundation (ASF) under one\n+  * or more contributor license agreements.  See the NOTICE file\n+  * distributed with this work for additional information\n+  * regarding copyright ownership.  The ASF licenses this file\n+  * to you under the Apache License, Version 2.0 (the\n+  * \"License\"); you may not use this file except in compliance\n+  * with the License.  You may obtain a copy of the License at\n+  *\n+  *     http://www.apache.org/licenses/LICENSE-2.0\n+  *\n+  * Unless required by applicable law or agreed to in writing, software\n+  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  * See the License for the specific language governing permissions and\n+  * limitations under the License.\n+  */\n+package org.apache.hadoop.hbase.ipc;\n+\n+import java.io.IOException;\n+import java.net.Socket;\n+import java.net.InetSocketAddress;\n+import java.net.SocketTimeoutException;\n+import javax.net.SocketFactory;\n+import java.lang.reflect.Method;\n+import java.util.*;\n+\n+import static org.junit.Assert.*;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.mockito.Mockito.*;\n+import org.mockito.Mockito;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler;\n+import org.apache.hadoop.hbase.SmallTests;\n+\n+import org.apache.hadoop.hbase.protobuf.generated.RPCProtos.RpcRequestBody;\n+import com.google.protobuf.Message;\n+import org.apache.hadoop.hbase.security.User;\n+\n+import org.apache.commons.logging.*;\n+import org.apache.log4j.Logger;\n+\n+@Category(SmallTests.class)\n+public class TestIPC {\n+  public static final Log LOG = LogFactory.getLog(TestIPC.class);\n+  private static final Random RANDOM = new Random();\n+\n+  private static class TestRpcServer extends HBaseServer {\n+    TestRpcServer() throws IOException {\n+      super(\"0.0.0.0\", 0, 1, 1, HBaseConfiguration.create(), \"TestRpcServer\", 0);\n+    }\n+\n+    @Override\n+    public Message call(Class<? extends VersionedProtocol> protocol,\n+        RpcRequestBody rpcRequest, \n+        long receiveTime, \n+        MonitoredRPCHandler status) throws IOException {\n+      return rpcRequest;\n+    }\n+  }\n+\n+  @Test\n+  public void testRTEDuringConnectionSetup() throws Exception {\n+    Configuration conf = HBaseConfiguration.create();\n+    SocketFactory spyFactory = spy(NetUtils.getDefaultSocketFactory(conf));\n+    Mockito.doAnswer(new Answer<Socket>() {\n+      @Override\n+      public Socket answer(InvocationOnMock invocation) throws Throwable {\n+        Socket s = spy((Socket)invocation.callRealMethod());\n+        doThrow(new RuntimeException(\"Injected fault\")).when(s).setSoTimeout(anyInt());\n+        return s;\n+      }\n+    }).when(spyFactory).createSocket();\n+\n+    TestRpcServer rpcServer = new TestRpcServer();\n+    rpcServer.start();\n+\n+    HBaseClient client = new HBaseClient(\n+        conf,\n+        spyFactory);\n+    InetSocketAddress address = rpcServer.getListenerAddress();\n+\n+    try {\n+      client.call(RpcRequestBody.getDefaultInstance(), address, User.getCurrent(), 0);\n+      fail(\"Expected an exception to have been thrown!\");\n+    } catch (Exception e) {\n+      LOG.info(\"Caught expected exception: \" + e.toString());\n+      assertTrue(StringUtils.stringifyException(e).contains(\"Injected fault\"));\n+    }\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-7145 ReusableStreamGzipCodec NPE upon reset with IBM JDK (Renata and Ted Yu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1424090 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/c4957a21d26a889658ca5c9fd7d46ce77117a874",
        "parent": "https://github.com/apache/hbase/commit/593fe025b51c08422d2530369cdf16478dd1c977",
        "bug_id": "hbase_278",
        "file": [
            {
                "sha": "0ff8974b860e0b8fe4ddaab92b73d1f5eb47d77f",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java",
                "blob_url": "https://github.com/apache/hbase/blob/c4957a21d26a889658ca5c9fd7d46ce77117a874/hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java",
                "raw_url": "https://github.com/apache/hbase/raw/c4957a21d26a889658ca5c9fd7d46ce77117a874/hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java",
                "status": "modified",
                "changes": 58,
                "additions": 58,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/io/compress/ReusableStreamGzipCodec.java?ref=c4957a21d26a889658ca5c9fd7d46ce77117a874",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.hbase.util.JVM;\n import org.apache.hadoop.io.compress.CompressionOutputStream;\n import org.apache.hadoop.io.compress.CompressorStream;\n import org.apache.hadoop.io.compress.GzipCodec;\n@@ -77,6 +78,10 @@\n     }\n \n     private static class ResetableGZIPOutputStream extends GZIPOutputStream {\n+\n+      private static final int TRAILER_SIZE = 8;\n+      private static final boolean HAS_BROKEN_FINISH = JVM.isGZIPOutputStreamFinishBroken();\n+\n       public ResetableGZIPOutputStream(OutputStream out) throws IOException {\n         super(out);\n       }\n@@ -86,6 +91,59 @@ public void resetState() throws IOException {\n         crc.reset();\n         out.write(GZIP_HEADER);\n       }\n+\n+      /**\n+       * Override because certain implementation calls def.end() which\n+       * causes problem when resetting the stream for reuse.\n+       */\n+      @Override\n+      public void finish() throws IOException {\n+        if (HAS_BROKEN_FINISH) { \n+          if (!def.finished()) {\n+            def.finish();\n+            while (!def.finished()) {\n+              int i = def.deflate(this.buf, 0, this.buf.length);\n+              if ((def.finished()) && (i <= this.buf.length - TRAILER_SIZE)) {\n+                writeTrailer(this.buf, i);\n+                i += TRAILER_SIZE;\n+                out.write(this.buf, 0, i);\n+\n+                return;\n+              }\n+              if (i > 0) {\n+                out.write(this.buf, 0, i);\n+              }\n+            }\n+\n+            byte[] arrayOfByte = new byte[TRAILER_SIZE];\n+            writeTrailer(arrayOfByte, 0);\n+            out.write(arrayOfByte);\n+          }\n+        } else {\n+          super.finish();\n+        }\n+      }\n+\n+      /** re-implement because the relative method in jdk is invisible */\n+      private void writeTrailer(byte[] paramArrayOfByte, int paramInt)\n+          throws IOException {\n+        writeInt((int)this.crc.getValue(), paramArrayOfByte, paramInt);\n+        writeInt(this.def.getTotalIn(), paramArrayOfByte, paramInt + 4);\n+      }\n+\n+      /** re-implement because the relative method in jdk is invisible */\n+      private void writeInt(int paramInt1, byte[] paramArrayOfByte, int paramInt2)\n+          throws IOException {\n+        writeShort(paramInt1 & 0xFFFF, paramArrayOfByte, paramInt2);\n+        writeShort(paramInt1 >> 16 & 0xFFFF, paramArrayOfByte, paramInt2 + 2);\n+      }\n+\n+      /** re-implement because the relative method in jdk is invisible */\n+      private void writeShort(int paramInt1, byte[] paramArrayOfByte, int paramInt2)\n+          throws IOException {\n+        paramArrayOfByte[paramInt2] = (byte)(paramInt1 & 0xFF);\n+        paramArrayOfByte[(paramInt2 + 1)] = (byte)(paramInt1 >> 8 & 0xFF);\n+      }\n     }\n \n     public ReusableGzipOutputStream(OutputStream out) throws IOException {",
                "deletions": 0
            },
            {
                "sha": "7a9790da9a20f7fd159f79203608b19a769634a5",
                "filename": "hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java",
                "blob_url": "https://github.com/apache/hbase/blob/c4957a21d26a889658ca5c9fd7d46ce77117a874/hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java",
                "raw_url": "https://github.com/apache/hbase/raw/c4957a21d26a889658ca5c9fd7d46ce77117a874/hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java",
                "status": "modified",
                "changes": 15,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/main/java/org/apache/hadoop/hbase/util/JVM.java?ref=c4957a21d26a889658ca5c9fd7d46ce77117a874",
                "patch": "@@ -57,6 +57,7 @@\n     System.getProperty(\"os.name\").startsWith(\"Windows\");\n   private static final boolean linux =\n     System.getProperty(\"os.name\").startsWith(\"Linux\");\n+  private static final String JVMVersion = System.getProperty(\"java.version\");\n \n   /**\n    * Constructor. Get the running Operating System instance\n@@ -70,21 +71,29 @@ public JVM () {\n    * \n    * @return whether this is unix or not.\n    */\n-  public boolean isUnix() {\n+  public static boolean isUnix() {\n     if (windows) {\n       return false;\n     }\n     return (ibmvendor ? linux : true);\n   }\n+  \n+  /**\n+   * Check if the finish() method of GZIPOutputStream is broken\n+   * \n+   * @return whether GZIPOutputStream.finish() is broken.\n+   */\n+  public static boolean isGZIPOutputStreamFinishBroken() {\n+    return ibmvendor && JVMVersion.contains(\"1.6.0\");\n+  }\n \n   /**\n    * Load the implementation of UnixOperatingSystemMXBean for Oracle jvm\n    * and runs the desired method. \n    * @param mBeanMethodName : method to run from the interface UnixOperatingSystemMXBean\n    * @return the method result\n    */\n-  private Long runUnixMXBeanMethod (String mBeanMethodName) {\n-  \n+  private Long runUnixMXBeanMethod (String mBeanMethodName) {  \n     Object unixos;\n     Class<?> classRef;\n     Method mBeanMethod;",
                "deletions": 3
            },
            {
                "sha": "13cfc9bfbf093ba7bd46efed1b99f47a3c27ff4b",
                "filename": "hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java",
                "blob_url": "https://github.com/apache/hbase/blob/c4957a21d26a889658ca5c9fd7d46ce77117a874/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java",
                "raw_url": "https://github.com/apache/hbase/raw/c4957a21d26a889658ca5c9fd7d46ce77117a874/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java",
                "status": "modified",
                "changes": 12,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-common/src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitListener.java?ref=c4957a21d26a889658ca5c9fd7d46ce77117a874",
                "patch": "@@ -89,11 +89,9 @@ public int getMax() {\n   static class OpenFileDescriptorResourceAnalyzer extends ResourceChecker.ResourceAnalyzer {\n     @Override\n     public int getVal(Phase phase) {\n+      if (JVM.isUnix() == false) return 0;\n       JVM jvm = new JVM();\n-      if (jvm != null && jvm.isUnix() == true)\n-          return (int)jvm.getOpenFileDescriptorCount();\n-      else\n-           return 0;\n+      return (int)jvm.getOpenFileDescriptorCount();\n     }\n \n     @Override\n@@ -105,11 +103,9 @@ public int getMax() {\n   static class MaxFileDescriptorResourceAnalyzer extends ResourceChecker.ResourceAnalyzer {\n     @Override\n     public int getVal(Phase phase) {\n+      if (JVM.isUnix() == false) return 0;\n       JVM jvm = new JVM();\n-      if (jvm != null && jvm.isUnix() == true)\n-           return (int)jvm.getMaxFileDescriptorCount();\n-      else\n-           return 0;\n+      return (int)jvm.getMaxFileDescriptorCount();\n      } \n    }\n ",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6900 RegionScanner.reseek() creates NPE when a flush or compaction happens before the reseek.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1394377 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/30c7e958ee20256a3d3fb88606eec7302139680f",
        "parent": "https://github.com/apache/hbase/commit/6f1af4dba4b2bf46722cb1e21f98b06d0476bc0f",
        "bug_id": "hbase_279",
        "file": [
            {
                "sha": "b595c066bac18a539ef5579154934564ef1f34c4",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=30c7e958ee20256a3d3fb88606eec7302139680f",
                "patch": "@@ -561,8 +561,10 @@ private void resetScannerStack(KeyValue lastTopKey) throws IOException {\n \n   @Override\n   public synchronized boolean reseek(KeyValue kv) throws IOException {\n-    //Heap cannot be null, because this is only called from next() which\n-    //guarantees that heap will never be null before this call.\n+    //Heap will not be null, if this is called from next() which.\n+    //If called from RegionScanner.reseek(...) make sure the scanner\n+    //stack is reset if needed.\n+    checkReseek();\n     if (explicitColumnQuery && lazySeekEnabledGlobally) {\n       return heap.requestSeek(kv, true, useRowColBloom);\n     } else {",
                "deletions": 2
            },
            {
                "sha": "b908763b50ddca118fcb710f7c6b81ad8fe1f40b",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/30c7e958ee20256a3d3fb88606eec7302139680f/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java",
                "status": "modified",
                "changes": 35,
                "additions": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java?ref=30c7e958ee20256a3d3fb88606eec7302139680f",
                "patch": "@@ -93,6 +93,7 @@\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.util.PairOfSameType;\n import org.apache.hadoop.hbase.util.Threads;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.mockito.Mockito;\n@@ -205,6 +206,40 @@ public void testCompactionAffectedByScanners() throws Exception {\n     System.out.println(results);\n     assertEquals(0, results.size());\n   }\n+  \n+  @Test\n+  public void testToShowNPEOnRegionScannerReseek() throws Exception{\n+    String method = \"testToShowNPEOnRegionScannerReseek\";\n+    byte[] tableName = Bytes.toBytes(method);\n+    byte[] family = Bytes.toBytes(\"family\");\n+    Configuration conf = HBaseConfiguration.create();\n+    this.region = initHRegion(tableName, method, conf, family);\n+\n+    Put put = new Put(Bytes.toBytes(\"r1\"));\n+    put.add(family, Bytes.toBytes(\"q1\"), Bytes.toBytes(\"v1\"));\n+    region.put(put);\n+    put = new Put(Bytes.toBytes(\"r2\"));\n+    put.add(family, Bytes.toBytes(\"q1\"), Bytes.toBytes(\"v1\"));\n+    region.put(put);\n+    region.flushcache();\n+\n+\n+    Scan scan = new Scan();\n+    scan.setMaxVersions(3);\n+    // open the first scanner\n+    RegionScanner scanner1 = region.getScanner(scan);\n+\n+    System.out.println(\"Smallest read point:\" + region.getSmallestReadPoint());\n+    \n+    region.compactStores(true);\n+\n+    scanner1.reseek(Bytes.toBytes(\"r2\"));\n+    List<KeyValue> results = new ArrayList<KeyValue>();\n+    scanner1.next(results);\n+    KeyValue keyValue = results.get(0);\n+    Assert.assertTrue(Bytes.compareTo(keyValue.getRow(), Bytes.toBytes(\"r2\")) == 0);\n+    scanner1.close();\n+  }\n \n   public void testSkipRecoveredEditsReplay() throws Exception {\n     String method = \"testSkipRecoveredEditsReplay\";",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-6069 TableInputFormatBase#createRecordReader() doesn't initialize TableRecordReader which causes NPE (Jie Huang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1341922 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf",
        "parent": "https://github.com/apache/hbase/commit/6a5244912a7a7ef2e1d9f4d10beee0ecc75216b6",
        "bug_id": "hbase_280",
        "file": [
            {
                "sha": "6054b36ecf946010cc82c1c0923cf4f617e97db6",
                "filename": "src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/dfc63998d20a956bf7a44ed6bcd7c495801fc4bf/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java",
                "status": "modified",
                "changes": 7,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java?ref=dfc63998d20a956bf7a44ed6bcd7c495801fc4bf",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.hbase.mapreduce;\n \n import java.io.IOException;\n+import java.io.InterruptedIOException;\n import java.net.InetAddress;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -31,7 +32,6 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Result;\n@@ -128,6 +128,11 @@\n     sc.setStopRow(tSplit.getEndRow());\n     trr.setScan(sc);\n     trr.setHTable(table);\n+    try {\n+      trr.initialize(tSplit, context);\n+    } catch (InterruptedException e) {\n+      throw new InterruptedIOException(e.getMessage());\n+    }\n     return trr;\n   }\n ",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5928 Hbck shouldn't npe when there are no tables (Elliott Clark)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1333691 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/41da06b7a6395c58974be5e1d0d3835707f1c646",
        "parent": "https://github.com/apache/hbase/commit/d45f53a3079c3618ff81a88f4ab39da36b3ee8a2",
        "bug_id": "hbase_281",
        "file": [
            {
                "sha": "11d8bf912a2ba6575edecd05a77606b00e48121c",
                "filename": "src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/41da06b7a6395c58974be5e1d0d3835707f1c646/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/41da06b7a6395c58974be5e1d0d3835707f1c646/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=41da06b7a6395c58974be5e1d0d3835707f1c646",
                "patch": "@@ -2152,7 +2152,7 @@ protected void finalize() throws Throwable {\n \n     @Override\n     public HTableDescriptor[] getHTableDescriptors(List<String> tableNames) throws IOException {\n-      if (tableNames == null || tableNames.isEmpty()) return null;\n+      if (tableNames == null || tableNames.isEmpty()) return new HTableDescriptor[0];\n       MasterKeepAliveConnection master = getKeepAliveMaster();\n       try {\n         return master.getHTableDescriptors(tableNames);",
                "deletions": 1
            },
            {
                "sha": "7b4f4a259be8dd2d96e134129ad04044bb475392",
                "filename": "src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "blob_url": "https://github.com/apache/hbase/blob/41da06b7a6395c58974be5e1d0d3835707f1c646/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "raw_url": "https://github.com/apache/hbase/raw/41da06b7a6395c58974be5e1d0d3835707f1c646/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java?ref=41da06b7a6395c58974be5e1d0d3835707f1c646",
                "patch": "@@ -2095,7 +2095,7 @@ public void dumpSidelinedRegions(Map<Path, HbckInfo> regions) {\n   }\n \n    HTableDescriptor[] getHTableDescriptors(List<String> tableNames) {\n-    HTableDescriptor[] htd = null;\n+    HTableDescriptor[] htd = new HTableDescriptor[0];\n      try {\n        LOG.info(\"getHTableDescriptors == tableNames => \" + tableNames);\n        htd = new HBaseAdmin(conf).getTableDescriptors(tableNames);",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5733 AssignmentManager#processDeadServersAndRegionsInTransition can fail with NPE (Uma Maheswara Rao G)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1327364 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8d84537c1930d89d14b0772d7fcd91b4b07bfea6",
        "parent": "https://github.com/apache/hbase/commit/34d88b8e234a0748df4d89f84921cb8a555c8ac5",
        "bug_id": "hbase_282",
        "file": [
            {
                "sha": "6e214c8f0927f7c2380c57cab8d06c188f522141",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 7,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8d84537c1930d89d14b0772d7fcd91b4b07bfea6",
                "patch": "@@ -402,6 +402,13 @@ void processDeadServersAndRegionsInTransition(\n   throws KeeperException, IOException, InterruptedException {\n     List<String> nodes = ZKUtil.listChildrenAndWatchForNewChildren(watcher,\n       watcher.assignmentZNode);\n+    \n+    if (nodes == null) {\n+      String errorMessage = \"Failed to get the children from ZK\";\n+      master.abort(errorMessage, new IOException(errorMessage));\n+      return;\n+    }\n+    \n     // Run through all regions.  If they are not assigned and not in RIT, then\n     // its a clean cluster startup, else its a failover.\n     for (Map.Entry<HRegionInfo, ServerName> e: this.regions.entrySet()) {",
                "deletions": 0
            },
            {
                "sha": "14830ef9b076e411e1e9b252340e168b95d94eb5",
                "filename": "src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/8d84537c1930d89d14b0772d7fcd91b4b07bfea6/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java",
                "status": "modified",
                "changes": 49,
                "additions": 46,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java?ref=8d84537c1930d89d14b0772d7fcd91b4b07bfea6",
                "patch": "@@ -17,8 +17,10 @@\n  */\n package org.apache.hadoop.hbase.master;\n \n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotSame;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.IOException;\n import java.util.ArrayList;\n@@ -27,6 +29,7 @@\n import java.util.Map;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HBaseTestingUtility;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n@@ -39,10 +42,10 @@\n import org.apache.hadoop.hbase.client.HConnection;\n import org.apache.hadoop.hbase.client.HConnectionTestingUtility;\n import org.apache.hadoop.hbase.client.Result;\n-import org.apache.hadoop.hbase.executor.EventHandler.EventType;\n import org.apache.hadoop.hbase.executor.ExecutorService;\n-import org.apache.hadoop.hbase.executor.ExecutorService.ExecutorType;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n+import org.apache.hadoop.hbase.executor.EventHandler.EventType;\n+import org.apache.hadoop.hbase.executor.ExecutorService.ExecutorType;\n import org.apache.hadoop.hbase.master.handler.ServerShutdownHandler;\n import org.apache.hadoop.hbase.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.protobuf.ClientProtocol;\n@@ -54,10 +57,12 @@\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Pair;\n import org.apache.hadoop.hbase.util.Threads;\n+import org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper;\n import org.apache.hadoop.hbase.zookeeper.ZKAssign;\n import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.Watcher;\n import org.apache.zookeeper.KeeperException.NodeExistsException;\n import org.junit.After;\n import org.junit.AfterClass;\n@@ -477,6 +482,39 @@ public void testUnassignWithSplitAtSameTime() throws KeeperException, IOExceptio\n     }\n   }\n \n+  /**\n+   * Tests the processDeadServersAndRegionsInTransition should not fail with NPE\n+   * when it failed to get the children. Let's abort the system in this\n+   * situation\n+   * @throws ServiceException \n+   */\n+  @Test(timeout = 5000)\n+  public void testProcessDeadServersAndRegionsInTransitionShouldNotFailWithNPE()\n+      throws IOException, KeeperException, InterruptedException, ServiceException {\n+    final RecoverableZooKeeper recoverableZk = Mockito\n+        .mock(RecoverableZooKeeper.class);\n+    AssignmentManagerWithExtrasForTesting am = setUpMockedAssignmentManager(\n+        this.server, this.serverManager);\n+    Watcher zkw = new ZooKeeperWatcher(HBaseConfiguration.create(), \"unittest\",\n+        null) {\n+      public RecoverableZooKeeper getRecoverableZooKeeper() {\n+        return recoverableZk;\n+      }\n+    };\n+    ((ZooKeeperWatcher) zkw).registerListener(am);\n+    Mockito.doThrow(new InterruptedException()).when(recoverableZk)\n+        .getChildren(\"/hbase/unassigned\", zkw);\n+    am.setWatcher((ZooKeeperWatcher) zkw);\n+    try {\n+      am.processDeadServersAndRegionsInTransition();\n+      fail(\"Expected to abort\");\n+    } catch (NullPointerException e) {\n+      fail(\"Should not throw NPE\");\n+    } catch (RuntimeException e) {\n+      assertEquals(\"Aborted\", e.getLocalizedMessage());\n+    }\n+  }\n+  \n   /**\n    * Creates a new ephemeral node in the SPLITTING state for the specified region.\n    * Create it ephemeral in case regionserver dies mid-split.\n@@ -610,7 +648,12 @@ void processRegionsInTransition(final RegionTransitionData data,\n       while (this.gate.get()) Threads.sleep(1);\n       super.processRegionsInTransition(data, regionInfo, deadServers, expectedVersion);\n     }\n-\n+    \n+    /** reset the watcher */\n+    void setWatcher(ZooKeeperWatcher watcher) {\n+      this.watcher = watcher;\n+    }\n+    \n     /**\n      * @return ExecutorService used by this instance.\n      */",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5722 NPE in ZKUtil#getChildDataAndWatchForNewChildren when ZK not available or NW down.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1310104 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fa97600ea9142dde236376fd0c0dcea343d91c5f",
        "parent": "https://github.com/apache/hbase/commit/1140530cc228a6c9b13ba3b4d1653962935739dd",
        "bug_id": "hbase_283",
        "file": [
            {
                "sha": "ba064668e3941aaf7cff593b0249e8d855e01f50",
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java",
                "status": "modified",
                "changes": 21,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java?ref=fa97600ea9142dde236376fd0c0dcea343d91c5f",
                "patch": "@@ -26,6 +26,7 @@\n import java.net.InetSocketAddress;\n import java.net.Socket;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n \n@@ -35,17 +36,19 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hbase.*;\n+import org.apache.hadoop.hbase.EmptyWatcher;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.executor.RegionTransitionData;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Threads;\n import org.apache.zookeeper.AsyncCallback;\n import org.apache.zookeeper.CreateMode;\n import org.apache.zookeeper.KeeperException;\n-import org.apache.zookeeper.KeeperException.NoNodeException;\n import org.apache.zookeeper.Watcher;\n-import org.apache.zookeeper.ZooDefs.Ids;\n import org.apache.zookeeper.ZooKeeper;\n+import org.apache.zookeeper.KeeperException.NoNodeException;\n+import org.apache.zookeeper.ZooDefs.Ids;\n import org.apache.zookeeper.data.ACL;\n import org.apache.zookeeper.data.Stat;\n \n@@ -596,11 +599,13 @@ public static int getNumberOfChildren(ZooKeeperWatcher zkw, String znode)\n       ZooKeeperWatcher zkw, String baseNode) throws KeeperException {\n     List<String> nodes =\n       ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);\n-    List<NodeAndData> newNodes = new ArrayList<NodeAndData>();\n-    for (String node: nodes) {\n-      String nodePath = ZKUtil.joinZNode(baseNode, node);\n-      byte [] data = ZKUtil.getDataAndWatch(zkw, nodePath);\n-      newNodes.add(new NodeAndData(nodePath, data));\n+    List<NodeAndData> newNodes = Collections.emptyList();\n+    if (nodes != null) {\n+      for (String node : nodes) {\n+        String nodePath = ZKUtil.joinZNode(baseNode, node);\n+        byte[] data = ZKUtil.getDataAndWatch(zkw, nodePath);\n+        newNodes.add(new NodeAndData(nodePath, data));\n+      }\n     }\n     return newNodes;\n   }",
                "deletions": 8
            },
            {
                "sha": "d4f76e9605807d7d531fad4558e1954144558d2a",
                "filename": "src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa97600ea9142dde236376fd0c0dcea343d91c5f/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java",
                "status": "modified",
                "changes": 12,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java?ref=fa97600ea9142dde236376fd0c0dcea343d91c5f",
                "patch": "@@ -342,6 +342,18 @@ public void testCreateSilentIsReallySilent() throws InterruptedException,\n \n     ZKUtil.createAndFailSilent(zk2, aclZnode);\n  }\n+  \n+  @Test\n+  /**\n+   * Test should not fail with NPE when getChildDataAndWatchForNewChildren\n+   * invoked with wrongNode\n+   */\n+  public void testGetChildDataAndWatchForNewChildrenShouldNotThrowNPE()\n+      throws Exception {\n+    ZooKeeperWatcher zkw = new ZooKeeperWatcher(TEST_UTIL.getConfiguration(),\n+        \"testGetChildDataAndWatchForNewChildrenShouldNotThrowNPE\", null);\n+    ZKUtil.getChildDataAndWatchForNewChildren(zkw, \"/wrongNode\");\n+  }\n \n   @org.junit.Rule\n   public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5638 Readability improvements on HBASE-5633: NPE reading ZK config in HBase (Matteo Bertozzi)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1307085 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fbfe3f29e5911146361e04f4514689e05257a491",
        "parent": "https://github.com/apache/hbase/commit/26de676dea8b829c4fe746da4f06bc4a4cd6c3aa",
        "bug_id": "hbase_284",
        "file": [
            {
                "sha": "21ac4bad171c7444b41cf67dc89bcd2aa5c9db4b",
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "patch": "@@ -77,13 +77,13 @@\n   public static final String HBASE_MASTER_LOADBALANCER_CLASS = \"hbase.master.loadbalancer.class\";\n \n   /** Cluster is standalone or pseudo-distributed */\n-  public static final String CLUSTER_IS_LOCAL = \"false\";\n+  public static final boolean CLUSTER_IS_LOCAL = false;\n \n   /** Cluster is fully-distributed */\n-  public static final String CLUSTER_IS_DISTRIBUTED = \"true\";\n+  public static final boolean CLUSTER_IS_DISTRIBUTED = true;\n \n   /** Default value for cluster distributed mode */  \n-  public static final String DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n+  public static final boolean DEFAULT_CLUSTER_DISTRIBUTED = CLUSTER_IS_LOCAL;\n \n   /** default host address */\n   public static final String DEFAULT_HOST = \"0.0.0.0\";",
                "deletions": 3
            },
            {
                "sha": "9c9c7cc441573ef872caa1fe8a7921a67894b479",
                "filename": "src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "patch": "@@ -436,8 +436,8 @@ public void shutdown() {\n    * @return True if a 'local' address in hbase.master value.\n    */\n   public static boolean isLocal(final Configuration c) {\n-    final String mode = c.get(HConstants.CLUSTER_DISTRIBUTED);\n-    return mode == null || mode.equals(HConstants.CLUSTER_IS_LOCAL);\n+    boolean mode = c.getBoolean(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED);\n+    return(mode == HConstants.CLUSTER_IS_LOCAL);\n   }\n \n   /**",
                "deletions": 2
            },
            {
                "sha": "99c04bdc75734361a057ac2b4114491a1a2b26a6",
                "filename": "src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "blob_url": "https://github.com/apache/hbase/blob/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "raw_url": "https://github.com/apache/hbase/raw/fbfe3f29e5911146361e04f4514689e05257a491/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java?ref=fbfe3f29e5911146361e04f4514689e05257a491",
                "patch": "@@ -163,9 +163,8 @@ public static Properties parseZooCfg(Configuration conf,\n       }\n       // Special case for 'hbase.cluster.distributed' property being 'true'\n       if (key.startsWith(\"server.\")) {\n-        if (conf.get(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED).\n-              equals(HConstants.CLUSTER_IS_DISTRIBUTED)\n-            && value.startsWith(HConstants.LOCALHOST)) {\n+        boolean mode = conf.getBoolean(HConstants.CLUSTER_DISTRIBUTED, HConstants.DEFAULT_CLUSTER_DISTRIBUTED);\n+        if (mode == HConstants.CLUSTER_IS_DISTRIBUTED && value.startsWith(HConstants.LOCALHOST)) {\n           String msg = \"The server in zoo.cfg cannot be set to localhost \" +\n               \"in a fully-distributed setup because it won't be reachable. \" +\n               \"See \\\"Getting Started\\\" for more information.\";",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5097 RegionObserver implementation whose preScannerOpen and postScannerOpen Impl return null can stall the system initialization through NPE (Ram)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1307036 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/26de676dea8b829c4fe746da4f06bc4a4cd6c3aa",
        "parent": "https://github.com/apache/hbase/commit/2e959382ef72709fb14ec13d855ced9e4b075310",
        "bug_id": "hbase_285",
        "file": [
            {
                "sha": "9c215b456bb298405072986ff86002c14173bcae",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/26de676dea8b829c4fe746da4f06bc4a4cd6c3aa/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/26de676dea8b829c4fe746da4f06bc4a4cd6c3aa/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 9,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=26de676dea8b829c4fe746da4f06bc4a4cd6c3aa",
                "patch": "@@ -2326,7 +2326,14 @@ public long openScanner(byte[] regionName, Scan scan) throws IOException {\n         s = r.getScanner(scan);\n       }\n       if (r.getCoprocessorHost() != null) {\n-        s = r.getCoprocessorHost().postScannerOpen(scan, s);\n+        RegionScanner savedScanner = r.getCoprocessorHost().postScannerOpen(\n+            scan, s);\n+        if (savedScanner == null) {\n+          LOG.warn(\"PostScannerOpen impl returning null. \"\n+              + \"Check the RegionObserver implementation.\");\n+        } else {\n+          s = savedScanner;\n+        }\n       }\n       return addScanner(s);\n     } catch (Throwable t) {",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "   HBASE-5586  [replication] NPE in ReplicationSource when creating a stream\n               to an inexistent cluster\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1303945 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fa98df639ead80e1d374a134ceb289fa1189114e",
        "parent": "https://github.com/apache/hbase/commit/536ff218258177d923e8771b44fce2420dc4c9e5",
        "bug_id": "hbase_286",
        "file": [
            {
                "sha": "e1a7398119ee60f995e58f98d77a74afddafebff",
                "filename": "src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa98df639ead80e1d374a134ceb289fa1189114e/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa98df639ead80e1d374a134ceb289fa1189114e/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java?ref=fa98df639ead80e1d374a134ceb289fa1189114e",
                "patch": "@@ -218,11 +218,11 @@ private void connectExistingPeers() throws IOException, KeeperException {\n    */\n   public List<ServerName> getSlavesAddresses(String peerClusterId) {\n     if (this.peerClusters.size() == 0) {\n-      return new ArrayList<ServerName>(0);\n+      return Collections.emptyList();\n     }\n     ReplicationPeer peer = this.peerClusters.get(peerClusterId);\n     if (peer == null) {\n-      return new ArrayList<ServerName>(0);\n+      return Collections.emptyList();\n     }\n     \n     List<ServerName> addresses;\n@@ -281,7 +281,7 @@ private void connectExistingPeers() throws IOException, KeeperException {\n   throws KeeperException {\n     List<String> children = ZKUtil.listChildrenNoWatch(zkw, znode);\n     if(children == null) {\n-      return null;\n+      return Collections.emptyList();\n     }\n     List<ServerName> addresses = new ArrayList<ServerName>(children.size());\n     for (String child : children) {",
                "deletions": 3
            },
            {
                "sha": "553b5cd7d27ef90c08e90b503946e02b9ac70489",
                "filename": "src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java",
                "blob_url": "https://github.com/apache/hbase/blob/fa98df639ead80e1d374a134ceb289fa1189114e/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java",
                "raw_url": "https://github.com/apache/hbase/raw/fa98df639ead80e1d374a134ceb289fa1189114e/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java",
                "status": "added",
                "changes": 118,
                "additions": 118,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java?ref=fa98df639ead80e1d374a134ceb289fa1189114e",
                "patch": "@@ -0,0 +1,118 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.replication;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseTestingUtility;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.MediumTests;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.catalog.CatalogTracker;\n+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;\n+import org.apache.zookeeper.KeeperException;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+@Category(MediumTests.class)\n+public class TestReplicationZookeeper {\n+\n+  private static Configuration conf;\n+\n+  private static HBaseTestingUtility utility;\n+\n+  private static ZooKeeperWatcher zkw;\n+\n+  private static ReplicationZookeeper repZk;\n+\n+  private static String slaveClusterKey;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    utility = new HBaseTestingUtility();\n+    utility.startMiniZKCluster();\n+    conf = utility.getConfiguration();\n+    zkw = HBaseTestingUtility.getZooKeeperWatcher(utility);\n+    DummyServer server = new DummyServer();\n+    repZk = new ReplicationZookeeper(server, new AtomicBoolean());\n+    slaveClusterKey = conf.get(HConstants.ZOOKEEPER_QUORUM) + \":\" +\n+      conf.get(\"hbase.zookeeper.property.clientPort\") + \":/1\";\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    utility.shutdownMiniZKCluster();\n+  }\n+\n+  @Test\n+  public void testGetAddressesMissingSlave()\n+    throws IOException, KeeperException {\n+    repZk.addPeer(\"1\", slaveClusterKey);\n+    // HBASE-5586 used to get an NPE\n+    assertEquals(0, repZk.getSlavesAddresses(\"1\").size());\n+  }\n+\n+  static class DummyServer implements Server {\n+\n+    @Override\n+    public Configuration getConfiguration() {\n+      return conf;\n+    }\n+\n+    @Override\n+    public ZooKeeperWatcher getZooKeeper() {\n+      return zkw;\n+    }\n+\n+    @Override\n+    public CatalogTracker getCatalogTracker() {\n+      return null;\n+    }\n+\n+    @Override\n+    public ServerName getServerName() {\n+      return new ServerName(\"hostname.example.org\", 1234, -1L);\n+    }\n+\n+    @Override\n+    public void abort(String why, Throwable e) {\n+    }\n+\n+    @Override\n+    public boolean isAborted() {\n+      return false;\n+    }\n+\n+    @Override\n+    public void stop(String why) {\n+    }\n+\n+    @Override\n+    public boolean isStopped() {\n+      return false;\n+    }\n+  }\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5279 NPE in Master after upgrading to 0.92.0 -- REVERT OVERCOMMIT TO HREGION\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1245768 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/0caad4616e2c67cc21e50ec4d8efce89f3469006",
        "parent": "https://github.com/apache/hbase/commit/5994c5014382bb09d040c0f9e9e0aae13d025e8b",
        "bug_id": "hbase_287",
        "file": [
            {
                "sha": "54675ffb7e3fae75eeaf0b688accd91f577f79d7",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/0caad4616e2c67cc21e50ec4d8efce89f3469006/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/0caad4616e2c67cc21e50ec4d8efce89f3469006/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 3,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=0caad4616e2c67cc21e50ec4d8efce89f3469006",
                "patch": "@@ -4112,7 +4112,6 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n   private List<KeyValue> get(Get get, boolean withCoprocessor)\n   throws IOException {\n     long now = EnvironmentEdgeManager.currentTimeMillis();\n-    Scan scan = new Scan(get);\n \n     List<KeyValue> results = new ArrayList<KeyValue>();\n \n@@ -4123,8 +4122,6 @@ public Result get(final Get get, final Integer lockid) throws IOException {\n        }\n     }\n \n-    Scan scan = new Scan(get);\n-\n     RegionScanner scanner = null;\n     try {\n       scanner = getScanner(scan);",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-5029 TestDistributedLogSplitting fails on occasion; Added catch of NPE and reenabled ignored test\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1220991 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/1222383f1d89688f2eed7d48b02c7a05b59f4ca9",
        "parent": "https://github.com/apache/hbase/commit/2062ea1d004172684a625790c85a459e1ffe3559",
        "bug_id": "hbase_288",
        "file": [
            {
                "sha": "cbef70f159291edc1583aa85278f4879f162b698",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java",
                "blob_url": "https://github.com/apache/hbase/blob/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java",
                "raw_url": "https://github.com/apache/hbase/raw/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java",
                "status": "modified",
                "changes": 7,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java?ref=1222383f1d89688f2eed7d48b02c7a05b59f4ca9",
                "patch": "@@ -211,7 +211,12 @@ public void append(HLog.Entry entry) throws IOException {\n   @Override\n   public void close() throws IOException {\n     if (this.writer != null) {\n-      this.writer.close();\n+      try {\n+        this.writer.close();\n+      } catch (NullPointerException npe) {\n+        // Can get a NPE coming up from down in DFSClient$DFSOutputStream#close\n+        LOG.warn(npe);\n+      }\n       this.writer = null;\n     }\n   }",
                "deletions": 1
            },
            {
                "sha": "a348f0c9f4edc3dd63749e9a6e7c8cb911103a6f",
                "filename": "src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java",
                "blob_url": "https://github.com/apache/hbase/blob/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java",
                "raw_url": "https://github.com/apache/hbase/raw/1222383f1d89688f2eed7d48b02c7a05b59f4ca9/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java",
                "status": "modified",
                "changes": 1,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java?ref=1222383f1d89688f2eed7d48b02c7a05b59f4ca9",
                "patch": "@@ -254,7 +254,6 @@ public void testRecoveredEdits() throws Exception {\n    * detects that the region server has aborted.\n    * @throws Exception\n    */\n-  @Ignore\n   @Test (timeout=300000)\n   public void testWorkerAbort() throws Exception {\n     LOG.info(\"testWorkerAbort\");",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4753 org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD throws NPE on trunk\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1198581 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/02f6104dc289a3b9d691fdf5abcd4bf226600610",
        "parent": "https://github.com/apache/hbase/commit/7d8d42d01bdfca946425cabf84dedecbf0d8602b",
        "bug_id": "hbase_289",
        "file": [
            {
                "sha": "803c7eb6c04b8c94159126507eafb0816c205fa0",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/02f6104dc289a3b9d691fdf5abcd4bf226600610/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/02f6104dc289a3b9d691fdf5abcd4bf226600610/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=02f6104dc289a3b9d691fdf5abcd4bf226600610",
                "patch": "@@ -455,6 +455,8 @@ Release 0.92.0 - Unreleased\n    HBASE-4745  LRU statistics thread should be a daemon\n    HBASE-4749  TestMasterFailover#testMasterFailoverWithMockedRITOnDeadRS\n                occasionally fails\n+   HBASE-4753  org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD\n+               throws NPE on trunk (nkeywal)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "deletions": 0
            },
            {
                "sha": "b2783aedbb08c62dc8a5ab1d1d4ed8ff09a88754",
                "filename": "src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "blob_url": "https://github.com/apache/hbase/blob/02f6104dc289a3b9d691fdf5abcd4bf226600610/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "raw_url": "https://github.com/apache/hbase/raw/02f6104dc289a3b9d691fdf5abcd4bf226600610/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java?ref=02f6104dc289a3b9d691fdf5abcd4bf226600610",
                "patch": "@@ -430,7 +430,9 @@ public static void deleteTableDescriptorIfExists(String tableName,\n     FileSystem fs = FSUtils.getCurrentFileSystem(conf);\n     FileStatus status = getTableInfoPath(fs, FSUtils.getRootDir(conf), tableName);\n     // The below deleteDirectory works for either file or directory.\n-    if (fs.exists(status.getPath())) FSUtils.deleteDirectory(fs, status.getPath());\n+    if (status != null && fs.exists(status.getPath()))  {\n+      FSUtils.deleteDirectory(fs, status.getPath());\n+    }\n   }\n \n   /**",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4673  NPE in HFileReaderV2.close during major compaction when hfile.block.cache.size is set to 0\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1189481 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/9f2204fbe33fa351f75291a16a0ef8607cf9c36d",
        "parent": "https://github.com/apache/hbase/commit/055d8e3b8c2e25aff05371079774895b01900339",
        "bug_id": "hbase_290",
        "file": [
            {
                "sha": "b1a698b1b36fee59d44b7efc0d9e8a85447bb36a",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/9f2204fbe33fa351f75291a16a0ef8607cf9c36d/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/9f2204fbe33fa351f75291a16a0ef8607cf9c36d/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9f2204fbe33fa351f75291a16a0ef8607cf9c36d",
                "patch": "@@ -27,7 +27,9 @@ Release 0.93.0 - Unreleased\n \n   BUG FIXES\n    HBASE-4488  Store could miss rows during flush (Lars H via jgray)\n-\n+   HBASE-4673  NPE in HFileReaderV2.close during major compaction when\n+               hfile.block.cache.size is set to 0 (Lars H)\n+ \n   TESTS\n    HBASE-4534  A new unit test for lazy seek and StoreScanner in general\n                (mikhail via jgray)",
                "deletions": 1
            },
            {
                "sha": "3f94396927fac1846a4a04f22e36c12e6c086e61",
                "filename": "src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java",
                "blob_url": "https://github.com/apache/hbase/blob/9f2204fbe33fa351f75291a16a0ef8607cf9c36d/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java",
                "raw_url": "https://github.com/apache/hbase/raw/9f2204fbe33fa351f75291a16a0ef8607cf9c36d/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java?ref=9f2204fbe33fa351f75291a16a0ef8607cf9c36d",
                "patch": "@@ -323,7 +323,7 @@ public void close() throws IOException {\n   }\n \n   public void close(boolean evictOnClose) throws IOException {\n-    if (evictOnClose) {\n+    if (evictOnClose && cacheConf.isBlockCacheEnabled()) {\n       int numEvicted = cacheConf.getBlockCache().evictBlocksByPrefix(name\n           + HFile.CACHE_KEY_SEPARATOR);\n       LOG.debug(\"On close of file \" + name + \" evicted \" + numEvicted",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4494 AvroServer:: get fails with NPE on a non-existent row\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1179014 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/2b981abcbbd550f432fcf368e5a703cde8d614d6",
        "parent": "https://github.com/apache/hbase/commit/fa6fb569324a622fcd421e6624d6655576acfb40",
        "bug_id": "hbase_291",
        "file": [
            {
                "sha": "3e1d4d4a9a4e4a7547da9cb9aa8da1ba290d8749",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/2b981abcbbd550f432fcf368e5a703cde8d614d6/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/2b981abcbbd550f432fcf368e5a703cde8d614d6/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2b981abcbbd550f432fcf368e5a703cde8d614d6",
                "patch": "@@ -335,6 +335,8 @@ Release 0.92.0 - Unreleased\n    HBASE-4531  hbase-4454 failsafe broke mvn site; back it out or fix\n                (Akash Ashok)\n    HBASE-4334  HRegion.get never validates row (Lars Hofhansl)\n+   HBASE-4494  AvroServer:: get fails with NPE on a non-existent row\n+               (Kay Kay)\n \n   TESTS\n    HBASE-4450  test for number of blocks read: to serve as baseline for expected",
                "deletions": 0
            },
            {
                "sha": "abd2ae6ace7f2a789034acc6adaaebaef4fa5004",
                "filename": "src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/avro/AvroUtil.java?ref=2b981abcbbd550f432fcf368e5a703cde8d614d6",
                "patch": "@@ -302,7 +302,8 @@ static public Get agetToGet(AGet aget) throws IOException {\n   // TODO(hammer): Pick one: Timestamp or TimeStamp\n   static public AResult resultToAResult(Result result) {\n     AResult aresult = new AResult();\n-    aresult.row = ByteBuffer.wrap(result.getRow());\n+    byte[] row = result.getRow();\n+    aresult.row = ByteBuffer.wrap(row != null ? row : new byte[1]);\n     Schema s = Schema.createArray(AResultEntry.SCHEMA$);\n     GenericData.Array<AResultEntry> entries = null;\n     List<KeyValue> resultKeyValues = result.list();",
                "deletions": 1
            },
            {
                "sha": "1e4d31034b8daa2150c7ad6df400333092595809",
                "filename": "src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java",
                "blob_url": "https://github.com/apache/hbase/blob/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java",
                "raw_url": "https://github.com/apache/hbase/raw/2b981abcbbd550f432fcf368e5a703cde8d614d6/src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java",
                "status": "added",
                "changes": 40,
                "additions": 40,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/avro/TestAvroUtil.java?ref=2b981abcbbd550f432fcf368e5a703cde8d614d6",
                "patch": "@@ -0,0 +1,40 @@\n+/** \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.avro;\n+\n+\n+import org.apache.hadoop.hbase.avro.generated.AResult;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+public class TestAvroUtil {\n+\n+  \n+  @Test\n+  public void testGetEmpty() {\n+    Result result = Mockito.mock(Result.class);\n+    Mockito.when(result.getRow()).thenReturn(null);\n+    //Get on a row, that does not exist, returns a result, \n+    //whose row is null.\n+    AResult aresult = AvroUtil.resultToAResult(result);\n+    Assert.assertNotNull(aresult);\n+  }\n+\n+}",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-4473  NPE when executors are down but events are still coming in\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1178520 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/4ecbf3c8c08e4b78592e2b5826059d927924cfed",
        "parent": "https://github.com/apache/hbase/commit/cb845c04d8f8f0365b6fdff40cdbcb5f938c3202",
        "bug_id": "hbase_292",
        "file": [
            {
                "sha": "e393f4e5a4e1dc61348146a63a969d523e24baa5",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/4ecbf3c8c08e4b78592e2b5826059d927924cfed/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/4ecbf3c8c08e4b78592e2b5826059d927924cfed/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=4ecbf3c8c08e4b78592e2b5826059d927924cfed",
                "patch": "@@ -689,6 +689,7 @@ Release 0.90.5 - Unreleased\n    HBASE-4295  rowcounter does not return the correct number of rows in\n                certain circumstances (David Revell)\n    HBASE-4515  User.getCurrent() can fail to initialize the current user\n+   HBASE-4473  NPE when executors are down but events are still coming in\n \n   IMPROVEMENT\n    HBASE-4205  Enhance HTable javadoc (Eric Charles)",
                "deletions": 0
            },
            {
                "sha": "7fb4266ac6e1c0d143dee6a43078f95b09d98a25",
                "filename": "src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java",
                "blob_url": "https://github.com/apache/hbase/blob/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java",
                "raw_url": "https://github.com/apache/hbase/raw/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java",
                "status": "modified",
                "changes": 14,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java?ref=4ecbf3c8c08e4b78592e2b5826059d927924cfed",
                "patch": "@@ -213,9 +213,6 @@ Executor getExecutor(final ExecutorType type) {\n \n   Executor getExecutor(String name) {\n     Executor executor = this.executorMap.get(name);\n-    if (executor == null) {\n-      LOG.debug(\"Executor service [\" + name + \"] not found in \" + this.executorMap);\n-    }\n     return executor;\n   }\n \n@@ -231,7 +228,16 @@ public void startExecutorService(final ExecutorType type, final int maxThreads)\n   }\n \n   public void submit(final EventHandler eh) {\n-    getExecutor(getExecutorServiceType(eh.getEventType())).submit(eh);\n+    Executor executor = getExecutor(getExecutorServiceType(eh.getEventType()));\n+    if (executor == null) {\n+      // This happens only when events are submitted after shutdown() was\n+      // called, so dropping them should be \"ok\" since it means we're\n+      // shutting down.\n+      LOG.error(\"Cannot submit [\" + eh + \"] because the executor is missing.\" +\n+        \" Is this process shutting down?\");\n+    } else {\n+      executor.submit(eh);\n+    }\n   }\n \n   /**",
                "deletions": 4
            },
            {
                "sha": "cb10fff74f72e299dfdc19c8b4b841f6ef5a863d",
                "filename": "src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java",
                "blob_url": "https://github.com/apache/hbase/blob/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java",
                "raw_url": "https://github.com/apache/hbase/raw/4ecbf3c8c08e4b78592e2b5826059d927924cfed/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java",
                "status": "modified",
                "changes": 9,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java?ref=4ecbf3c8c08e4b78592e2b5826059d927924cfed",
                "patch": "@@ -124,6 +124,15 @@ public void testExecutorService() throws Exception {\n     // Make sure threads are still around even after their timetolive expires.\n     Thread.sleep(executor.keepAliveTimeInMillis * 2);\n     assertEquals(maxThreads, pool.getPoolSize());\n+\n+    executorService.shutdown();\n+\n+    assertEquals(0, executorService.getAllExecutorStatuses().size());\n+\n+    // Test that submit doesn't throw NPEs\n+    executorService.submit(\n+      new TestEventHandler(mockedServer, EventType.M_SERVER_SHUTDOWN,\n+            lock, counter));\n   }\n \n   private void checkStatusDump(ExecutorStatus status) throws IOException {",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "   HBASE-3874  ServerShutdownHandler fails on NPE if a plan has a random\n               region assignment\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1124477 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
        "parent": "https://github.com/apache/hbase/commit/cf8be18f159b21137885e8c579e415a0cc1e323e",
        "bug_id": "hbase_293",
        "file": [
            {
                "sha": "d48d7d7fdf89099029f2ef59d662d3ac2b07f779",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/CHANGES.txt",
                "status": "modified",
                "changes": 13,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
                "patch": "@@ -256,14 +256,15 @@ Release 0.91.0 - Unreleased\n Release 0.90.4 - Unreleased\n \n   BUG FIXES\n-   HBASE-3878 Hbase client throws NoSuchElementException (Ted Yu)\n-   HBASE-3878 Hbase client throws NoSuchElementException (Ted Yu)\n-   HBASE-3881 Add disable balancer in graceful_stop.sh script\n-   HBASE-3895 Fix order of parameters after HBASE-1511\n+   HBASE-3878  Hbase client throws NoSuchElementException (Ted Yu)\n+   HBASE-3881  Add disable balancer in graceful_stop.sh script\n+   HBASE-3895  Fix order of parameters after HBASE-1511\n+   HBASE-3874  ServerShutdownHandler fails on NPE if a plan has a random\n+               region assignment\n \n   IMPROVEMENT\n-   HBASE-3882 hbase-config.sh needs to be updated so it can auto-detects the\n-              sun jre provided by RHEL6 (Roman Shaposhnik)\n+   HBASE-3882  hbase-config.sh needs to be updated so it can auto-detects the\n+               sun jre provided by RHEL6 (Roman Shaposhnik)\n \n Release 0.90.3 - Unreleased\n ",
                "deletions": 6
            },
            {
                "sha": "7e473095b11a15233e608aa48d6c9c431ebd6aed",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=d64c76fe105c9fa20fed733f8ffbbefdf1e8b8a0",
                "patch": "@@ -1995,7 +1995,9 @@ protected void chore() {\n       for (Iterator <Map.Entry<String, RegionPlan>> i =\n           this.regionPlans.entrySet().iterator(); i.hasNext();) {\n         Map.Entry<String, RegionPlan> e = i.next();\n-        if (e.getValue().getDestination().equals(sn)) {\n+        ServerName otherSn = e.getValue().getDestination();\n+        // The name will be null if the region is planned for a random assign.\n+        if (otherSn != null && otherSn.equals(sn)) {\n           // Use iterator's remove else we'll get CME\n           i.remove();\n         }",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3838 RegionCoprocesorHost.preWALRestore throws npe in case there is no RegionObserver registered\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1098705 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
        "parent": "https://github.com/apache/hbase/commit/6c32f36250df2538598fcb7c8f99a615a8e4846d",
        "bug_id": "hbase_294",
        "file": [
            {
                "sha": "331392dc81ed4ddcb4156c11e3af4456162839ce",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
                "patch": "@@ -93,6 +93,8 @@ Release 0.91.0 - Unreleased\n                cluster and was returning master hostname for rs to use\n    HBASE-3829  TestMasterFailover failures in jenkins\n    HBASE-3843  splitLogWorker starts too early (Prakash Khemani)\n+   HBASE-3838  RegionCoprocesorHost.preWALRestore throws npe in case there is\n+               no RegionObserver registered (Himanshu Vashishtha)\n \n   IMPROVEMENTS\n    HBASE-3290  Max Compaction Size (Nicolas Spiegelberg via Stack)  ",
                "deletions": 0
            },
            {
                "sha": "ac76db421126e8a49d1aa476b368d9e4f5bcbe78",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "blob_url": "https://github.com/apache/hbase/blob/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "raw_url": "https://github.com/apache/hbase/raw/2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java",
                "status": "modified",
                "changes": 16,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java?ref=2a4f052bcc14163c7256cb6b3290fbc71cbe2c4a",
                "patch": "@@ -920,11 +920,12 @@ public boolean preWALRestore(HRegionInfo info, HLogKey logKey,\n         ctx = ObserverContext.createAndPrepare(env, ctx);\n         ((RegionObserver)env.getInstance()).preWALRestore(ctx, info, logKey,\n             logEdit);\n+        bypass |= ctx.shouldBypass();\n+        if (ctx.shouldComplete()) {\n+          break;\n+        }\n       }\n-      bypass |= ctx.shouldBypass();\n-      if (ctx.shouldComplete()) {\n-        break;\n-      }\n+     \n     }\n     return bypass;\n   }\n@@ -943,10 +944,11 @@ public void postWALRestore(HRegionInfo info, HLogKey logKey,\n         ctx = ObserverContext.createAndPrepare(env, ctx);\n         ((RegionObserver)env.getInstance()).postWALRestore(ctx, info,\n             logKey, logEdit);\n+        if (ctx.shouldComplete()) {\n+          break;\n+        }\n       }\n-      if (ctx.shouldComplete()) {\n-        break;\n-      }\n+      \n     }\n   }\n }",
                "deletions": 7
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3360  ReplicationLogCleaner is enabled by default in 0.90 -- causes NPE\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1049680 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/39330cff754d9b088523703409698ec2b180df79",
        "parent": "https://github.com/apache/hbase/commit/1842a1ee8573b356e4140bf7b47a7102461aacd1",
        "bug_id": "hbase_295",
        "file": [
            {
                "sha": "8407559c5deb9ec9fd5c22b3dbcde9a5818d0eea",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=39330cff754d9b088523703409698ec2b180df79",
                "patch": "@@ -782,6 +782,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3356  Add more checks in replication if RS is stopped\n    HBASE-3358  Recovered replication queue wait on themselves when terminating\n    HBASE-3359  LogRoller not added as a WAL listener when replication is enabled\n+   HBASE-3360  ReplicationLogCleaner is enabled by default in 0.90 -- causes NPE\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "a44a0b958df879d971799d1822b8de455d2e5167",
                "filename": "src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/HConstants.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/HConstants.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "patch": "@@ -361,6 +361,9 @@\n   /** HBCK special code name used as server name when manipulating ZK nodes */\n   public static final String HBCK_CODE_NAME = \"HBCKServerName\";\n \n+  public static final String HBASE_MASTER_LOGCLEANER_PLUGINS =\n+      \"hbase.master.logcleaner.plugins\";\n+\n   private HConstants() {\n     // Can't be instantiated with this ctor.\n   }",
                "deletions": 0
            },
            {
                "sha": "18f7787e1e199b4d2955bf6d0637b859e2371440",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "patch": "@@ -74,6 +74,7 @@\n import org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler;\n import org.apache.hadoop.hbase.master.metrics.MasterMetrics;\n import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.replication.regionserver.Replication;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.InfoServer;\n import org.apache.hadoop.hbase.util.Pair;\n@@ -203,6 +204,8 @@ public HMaster(final Configuration conf)\n     // set the thread name now we have an address\n     setName(MASTER + \"-\" + this.address);\n \n+    Replication.decorateMasterConfiguration(this.conf);\n+\n     this.rpcServer.startThreads();\n \n     // Hack! Maps DFSClient => Master for logs.  HDFS made this",
                "deletions": 0
            },
            {
                "sha": "f4252a62ab4bb9a8afeeaa2fb6f9803234f2d470",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "patch": "@@ -34,6 +34,8 @@\n import org.apache.hadoop.hbase.Stoppable;\n import org.apache.hadoop.hbase.regionserver.wal.HLog;\n \n+import static org.apache.hadoop.hbase.HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS;\n+\n /**\n  * This Chore, everytime it runs, will clear the wal logs in the old logs folder\n  * that are deletable for each log cleaner in the chain, in order to limit the\n@@ -79,7 +81,7 @@ public LogCleaner(final int p, final Stoppable s,\n    * ReplicationLogCleaner and SnapshotLogCleaner.\n    */\n   private void initLogCleanersChain() {\n-    String[] logCleaners = conf.getStrings(\"hbase.master.logcleaner.plugins\");\n+    String[] logCleaners = conf.getStrings(HBASE_MASTER_LOGCLEANER_PLUGINS);\n     if (logCleaners != null) {\n       for (String className : logCleaners) {\n         LogCleanerDelegate logCleaner = newLogCleaner(className, conf);",
                "deletions": 1
            },
            {
                "sha": "1a87947ba251d42fc0a8d0011dc1c413df0ba666",
                "filename": "src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
                "status": "modified",
                "changes": 26,
                "additions": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=39330cff754d9b088523703409698ec2b180df79",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.KeyValue;\n import org.apache.hadoop.hbase.Server;\n@@ -36,9 +35,14 @@\n import org.apache.hadoop.hbase.regionserver.wal.WALEdit;\n import org.apache.hadoop.hbase.regionserver.wal.WALObserver;\n import org.apache.hadoop.hbase.replication.ReplicationZookeeper;\n+import org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.zookeeper.KeeperException;\n \n+import static org.apache.hadoop.hbase.HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS;\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_ENABLE_KEY;\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL;\n+\n /**\n  * Gateway to Replication.  Used by {@link org.apache.hadoop.hbase.regionserver.HRegionServer}.\n  */\n@@ -82,7 +86,7 @@ public Replication(final Server server, final FileSystem fs,\n    * @return True if replication is enabled.\n    */\n   public static boolean isReplication(final Configuration c) {\n-    return c.getBoolean(HConstants.REPLICATION_ENABLE_KEY, false);\n+    return c.getBoolean(REPLICATION_ENABLE_KEY, false);\n   }\n \n   /**\n@@ -134,7 +138,7 @@ public void visitLogEntryBeforeWrite(HRegionInfo info, HLogKey logKey,\n     for (KeyValue kv : logEdit.getKeyValues()) {\n       family = kv.getFamily();\n       int scope = info.getTableDesc().getFamily(family).getScope();\n-      if (scope != HConstants.REPLICATION_SCOPE_LOCAL &&\n+      if (scope != REPLICATION_SCOPE_LOCAL &&\n           !scopes.containsKey(family)) {\n         scopes.put(family, scope);\n       }\n@@ -149,6 +153,22 @@ public void logRolled(Path p) {\n     getReplicationManager().logRolled(p);\n   }\n \n+  /**\n+   * This method modifies the master's configuration in order to inject\n+   * replication-related features\n+   * @param conf\n+   */\n+  public static void decorateMasterConfiguration(Configuration conf) {\n+    if (!isReplication(conf)) {\n+      return;\n+    }\n+    String plugins = conf.get(HBASE_MASTER_LOGCLEANER_PLUGINS);\n+    if (!plugins.contains(ReplicationLogCleaner.class.toString())) {\n+      conf.set(HBASE_MASTER_LOGCLEANER_PLUGINS,\n+          plugins + \",\" + ReplicationLogCleaner.class.getCanonicalName());\n+    }\n+  }\n+\n   @Override\n   public void logRollRequested() {\n     // Not interested",
                "deletions": 3
            },
            {
                "sha": "f1cc4ae0f8557c552a44dd246824e0efd72b5325",
                "filename": "src/main/resources/hbase-default.xml",
                "blob_url": "https://github.com/apache/hbase/blob/39330cff754d9b088523703409698ec2b180df79/src/main/resources/hbase-default.xml",
                "raw_url": "https://github.com/apache/hbase/raw/39330cff754d9b088523703409698ec2b180df79/src/main/resources/hbase-default.xml",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/resources/hbase-default.xml?ref=39330cff754d9b088523703409698ec2b180df79",
                "patch": "@@ -290,7 +290,7 @@\n   </property>\n   <property>\n     <name>hbase.master.logcleaner.plugins</name>\n-    <value>org.apache.hadoop.hbase.master.TimeToLiveLogCleaner,org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner</value>\n+    <value>org.apache.hadoop.hbase.master.TimeToLiveLogCleaner</value>\n     <description>A comma-separated list of LogCleanerDelegate invoked by\n     the LogsCleaner service. These WAL/HLog cleaners are called in order,\n     so put the HLog cleaner that prunes the most HLog files in front. To",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3283 NPE in AssignmentManager if processing shutdown of RS who doesn't have any regions assigned to it\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1040302 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8b693bf547bc055a83de6aa0f239f481961173a1",
        "parent": "https://github.com/apache/hbase/commit/2778fced7580043d58116f83badc8629b290505f",
        "bug_id": "hbase_296",
        "file": [
            {
                "sha": "eae17ae9baa7ad371f81f204b9d1b038eba6b9ae",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/8b693bf547bc055a83de6aa0f239f481961173a1/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/8b693bf547bc055a83de6aa0f239f481961173a1/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=8b693bf547bc055a83de6aa0f239f481961173a1",
                "patch": "@@ -9,6 +9,8 @@ Release 0.91.0 - Unreleased\n    HBASE-3280  YouAreDeadException being swallowed in HRS getMaster\n    HBASE-3282  Need to retain DeadServers to ensure we don't allow\n                previously expired RS instances to rejoin cluster\n+   HBASE-3283  NPE in AssignmentManager if processing shutdown of RS who\n+               doesn't have any regions assigned to it\n \n   IMPROVEMENTS\n    HBASE-2001  Coprocessors: Colocate user code with regions (Mingjie Lai via",
                "deletions": 0
            },
            {
                "sha": "6811cdc6438af677ce02985f27d830f49b032905",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/8b693bf547bc055a83de6aa0f239f481961173a1/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/8b693bf547bc055a83de6aa0f239f481961173a1/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 9,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=8b693bf547bc055a83de6aa0f239f481961173a1",
                "patch": "@@ -1597,16 +1597,21 @@ protected void chore() {\n     // Remove this server from map of servers to regions, and remove all regions\n     // of this server from online map of regions.\n     Set<HRegionInfo> deadRegions = null;\n+    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();\n     synchronized (this.regions) {\n-      deadRegions = new TreeSet<HRegionInfo>(this.servers.remove(hsi));\n+      List<HRegionInfo> assignedRegions = this.servers.remove(hsi);\n+      if (assignedRegions == null || assignedRegions.isEmpty()) {\n+        // No regions on this server, we are done, return empty list of RITs\n+        return rits;\n+      }\n+      deadRegions = new TreeSet<HRegionInfo>(assignedRegions);\n       for (HRegionInfo region : deadRegions) {\n         this.regions.remove(region);\n       }\n     }\n     // See if any of the regions that were online on this server were in RIT\n     // If they are, normal timeouts will deal with them appropriately so\n     // let's skip a manual re-assignment.\n-    List<HRegionInfo> rits = new ArrayList<HRegionInfo>();\n     synchronized (regionsInTransition) {\n       for (RegionState region : this.regionsInTransition.values()) {\n         if (deadRegions.remove(region.getRegion())) {",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3261  NPE out of HRS.run at startup when clock is out of sync\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1038756 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/0bbfd1a364632a314f78966de44114ccfb8d9065",
        "parent": "https://github.com/apache/hbase/commit/9b45d9996f8180fc105382de80ad9a08f6e5c1ec",
        "bug_id": "hbase_297",
        "file": [
            {
                "sha": "062b841bde46f96509fe6ff3612db22945edd095",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/0bbfd1a364632a314f78966de44114ccfb8d9065/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/0bbfd1a364632a314f78966de44114ccfb8d9065/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=0bbfd1a364632a314f78966de44114ccfb8d9065",
                "patch": "@@ -711,6 +711,7 @@ Release 0.90.0 - Unreleased\n    HBASE-3269  HBase table truncate semantics seems broken as \"disable\" table\n                is now async by default\n    HBASE-3275  [rest] No gzip/deflate content encoding support\n+   HBASE-3261  NPE out of HRS.run at startup when clock is out of sync\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "a4a5264423b5d1f26fd3e1d471f4f3a6cbe30828",
                "filename": "src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/0bbfd1a364632a314f78966de44114ccfb8d9065/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/0bbfd1a364632a314f78966de44114ccfb8d9065/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=0bbfd1a364632a314f78966de44114ccfb8d9065",
                "patch": "@@ -613,12 +613,12 @@ public void run() {\n \n     // Send interrupts to wake up threads if sleeping so they notice shutdown.\n     // TODO: Should we check they are alive? If OOME could have exited already\n-    cacheFlusher.interruptIfNecessary();\n-    compactSplitThread.interruptIfNecessary();\n-    hlogRoller.interruptIfNecessary();\n-    this.majorCompactionChecker.interrupt();\n+    if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();\n+    if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();\n+    if (this.hlogRoller != null) this.hlogRoller.interruptIfNecessary();\n+    if (this.majorCompactionChecker != null) this.majorCompactionChecker.interrupt();\n \n-    if (killed) {\n+    if (this.killed) {\n       // Just skip out w/o closing regions.\n     } else if (abortRequested) {\n       if (this.fsOk) {\n@@ -634,7 +634,7 @@ public void run() {\n     }\n     // Interrupt catalog tracker here in case any regions being opened out in\n     // handlers are stuck waiting on meta or root.\n-    this.catalogTracker.stop();\n+    if (this.catalogTracker != null) this.catalogTracker.stop();\n     waitOnAllRegionsToClose();\n \n     // Make sure the proxy is down.",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3213 If do abort of backup master will get NPE instead of graceful abort\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1033360 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/afeb06a5aae1f2cca7dbf6334236121eb40961b6",
        "parent": "https://github.com/apache/hbase/commit/9f9de3410b15cabb345371ef60a1b85221b0bec9",
        "bug_id": "hbase_298",
        "file": [
            {
                "sha": "a1fcb24bea005da8cc986420039eb8813d3de749",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/afeb06a5aae1f2cca7dbf6334236121eb40961b6/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/afeb06a5aae1f2cca7dbf6334236121eb40961b6/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=afeb06a5aae1f2cca7dbf6334236121eb40961b6",
                "patch": "@@ -669,6 +669,8 @@ Release 0.90.0 - Unreleased\n                place; i.e. that only one enable/disable runs at a time\n    HBASE-2898  MultiPut makes proper error handling impossible and leads to \n    \t       corrupted data\n+   HBASE-3213  If do abort of backup master will get NPE instead of graceful\n+               abort\n \n \n   IMPROVEMENTS",
                "deletions": 0
            },
            {
                "sha": "efcbc3ada685e5bde954d520404968bfe9314ee2",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/afeb06a5aae1f2cca7dbf6334236121eb40961b6/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/afeb06a5aae1f2cca7dbf6334236121eb40961b6/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=afeb06a5aae1f2cca7dbf6334236121eb40961b6",
                "patch": "@@ -279,7 +279,8 @@ public void run() {\n       stopChores();\n       // Wait for all the remaining region servers to report in IFF we were\n       // running a cluster shutdown AND we were NOT aborting.\n-      if (!this.abort && this.serverManager.isClusterShutdown()) {\n+      if (!this.abort && this.serverManager != null &&\n+          this.serverManager.isClusterShutdown()) {\n         this.serverManager.letRegionServersShutdown();\n       }\n       stopServiceThreads();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-3169  NPE when master joins running cluster if a RIT references a RS no longer present\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@1028872 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/022f70b7220fdd791fcb5eef23ac69852156201a",
        "parent": "https://github.com/apache/hbase/commit/73727e534a8590d233255db780b6eb8784ce8beb",
        "bug_id": "hbase_299",
        "file": [
            {
                "sha": "91446ab4b4ac2b53ee1292a817de93c02428bf64",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/022f70b7220fdd791fcb5eef23ac69852156201a/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/022f70b7220fdd791fcb5eef23ac69852156201a/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=022f70b7220fdd791fcb5eef23ac69852156201a",
                "patch": "@@ -1056,6 +1056,8 @@ Release 0.21.0 - Unreleased\n                (Kannan Muthukkaruppan via Stack)\n    HBASE-3102  Enhance HBase rMetrics for Long-running Stats\n                (Nicolas Spiegelberg via Stack)\n+   HBASE-3169  NPE when master joins running cluster if a RIT references\n+               a RS no longer present\n \n   NEW FEATURES\n    HBASE-1961  HBase EC2 scripts",
                "deletions": 0
            },
            {
                "sha": "78fc7c3f4061d6bda3e15be7ae7f317a8fae1785",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/022f70b7220fdd791fcb5eef23ac69852156201a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/022f70b7220fdd791fcb5eef23ac69852156201a/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java",
                "status": "modified",
                "changes": 13,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java?ref=022f70b7220fdd791fcb5eef23ac69852156201a",
                "patch": "@@ -292,8 +292,17 @@ void processRegionsInTransition(final RegionTransitionData data,\n         // Region is opened, insert into RIT and handle it\n         regionsInTransition.put(encodedRegionName, new RegionState(\n             regionInfo, RegionState.State.OPENING, data.getStamp()));\n-        new OpenedRegionHandler(master, this, data, regionInfo,\n-            serverManager.getServerInfo(data.getServerName())).process();\n+        HServerInfo hsi = serverManager.getServerInfo(data.getServerName());\n+        // hsi could be null if this server is no longer online.  If\n+        // that the case, just let this RIT timeout; it'll be assigned\n+        // to new server then.\n+        if (hsi == null) {\n+          LOG.warn(\"Region in transition \" + regionInfo.getEncodedName() +\n+            \" references a server no longer up \" + data.getServerName() +\n+            \"; letting RIT timeout so will be assigned elsewhere\");\n+          break;\n+        }\n+        new OpenedRegionHandler(master, this, data, regionInfo, hsi).process();\n         break;\n       }\n     }",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "Fix NPE in LoadBalancer that I added with my commit of HBASE-2979\n\ngit-svn-id: https://svn.apache.org/repos/asf/hbase/trunk@997510 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/eecc426f785abf9976ce070d62e603aa6af155bb",
        "parent": "https://github.com/apache/hbase/commit/fb42084ba91d904bda8c6f3618f9b9f2e7feffef",
        "bug_id": "hbase_300",
        "file": [
            {
                "sha": "5fbf0f7b4c3b84d6dea80b1eef56585329d7939f",
                "filename": "src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java",
                "blob_url": "https://github.com/apache/hbase/blob/eecc426f785abf9976ce070d62e603aa6af155bb/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java",
                "raw_url": "https://github.com/apache/hbase/raw/eecc426f785abf9976ce070d62e603aa6af155bb/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java?ref=eecc426f785abf9976ce070d62e603aa6af155bb",
                "patch": "@@ -591,7 +591,8 @@ public int compareTo(RegionPlan o) {\n     @Override\n     public String toString() {\n       return \"hri=\" + this.hri.getRegionNameAsString() + \", src=\" +\n-        this.source.getServerName() + \", dest=\" + this.dest.getServerName();\n+        (this.source == null? \"\": this.source.getServerName()) +\n+        \", dest=\" + this.dest.getServerName();\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-2443 IPC client can throw NPE if socket creation fails\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@936107 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/3cb1168601bee83e82a21de7c359d70d27c8c767",
        "parent": "https://github.com/apache/hbase/commit/2b2f1d1670040f2e0d27921a3f002099ff6ba447",
        "bug_id": "hbase_301",
        "file": [
            {
                "sha": "fa5c3828acc854ca8071a0a6356abe21368c0c35",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/3cb1168601bee83e82a21de7c359d70d27c8c767/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/3cb1168601bee83e82a21de7c359d70d27c8c767/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3cb1168601bee83e82a21de7c359d70d27c8c767",
                "patch": "@@ -279,6 +279,8 @@ Release 0.21.0 - Unreleased\n                is a prefix (Todd Lipcon via Stack)\n    HBASE-2463  Various Bytes.* functions silently ignore invalid arguments\n                (Benoit Sigoure via Stack)\n+   HBASE-2443  IPC client can throw NPE if socket creation fails\n+               (Todd Lipcon via Stack)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "359a5b1b87d942fd18915c72729d94702b058217",
                "filename": "core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "blob_url": "https://github.com/apache/hbase/blob/3cb1168601bee83e82a21de7c359d70d27c8c767/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "raw_url": "https://github.com/apache/hbase/raw/3cb1168601bee83e82a21de7c359d70d27c8c767/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java",
                "status": "modified",
                "changes": 10,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/core/src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java?ref=3cb1168601bee83e82a21de7c359d70d27c8c767",
                "patch": "@@ -351,10 +351,12 @@ protected synchronized void setupIOstreams() throws IOException {\n     private void handleConnectionFailure(\n         int curRetries, int maxRetries, IOException ioe) throws IOException {\n       // close the current connection\n-      try {\n-        socket.close();\n-      } catch (IOException e) {\n-        LOG.warn(\"Not able to close a socket\", e);\n+      if (socket != null) { // could be null if the socket creation failed\n+        try {\n+          socket.close();\n+        } catch (IOException e) {\n+          LOG.warn(\"Not able to close a socket\", e);\n+        }\n       }\n       // set socket to null so that the next call to setupIOstreams\n       // can start the process of connect all over again.",
                "deletions": 4
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1912  When adding a secondary index to an existing table, it will cause NPE during re-indexing\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@826468 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d4cf962c8a490ee200d774763ac3c8cc243179da",
        "parent": "https://github.com/apache/hbase/commit/9375e3c8b051a55d8d07d8be967d0cd48e277c13",
        "bug_id": "hbase_302",
        "file": [
            {
                "sha": "fc9238670e8b5a6a40e80f32601e3fe25a50197a",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/d4cf962c8a490ee200d774763ac3c8cc243179da/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/d4cf962c8a490ee200d774763ac3c8cc243179da/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d4cf962c8a490ee200d774763ac3c8cc243179da",
                "patch": "@@ -68,6 +68,9 @@ Release 0.21.0 - Unreleased\n    HBASE-1906  FilterList of prefix and columnvalue not working properly with\n                deletes and multiple values\n    HBASE-1896  WhileMatchFilter.reset should call encapsulated filter reset\n+   HBASE-1912  When adding a secondary index to an existing table, it will\n+               cause NPE during re-indexing (Mingjui Ray Liao via Andrew\n+               Purtell)\n \n   IMPROVEMENTS\n    HBASE-1760  Cleanup TODOs in HTable",
                "deletions": 0
            },
            {
                "sha": "b1c0a8738a7b52738076c18b667fc9a345041bd7",
                "filename": "src/contrib/transactional/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "blob_url": "https://github.com/apache/hbase/blob/d4cf962c8a490ee200d774763ac3c8cc243179da/src/contrib/transactional/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "raw_url": "https://github.com/apache/hbase/raw/d4cf962c8a490ee200d774763ac3c8cc243179da/src/contrib/transactional/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/contrib/transactional/src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java?ref=d4cf962c8a490ee200d774763ac3c8cc243179da",
                "patch": "@@ -78,9 +78,10 @@ public IndexSpecification(String indexId, byte[][] indexedColumns,\n       byte[][] additionalColumns, IndexKeyGenerator keyGenerator) {\n     this.indexId = indexId;\n     this.indexedColumns = indexedColumns;\n-    this.additionalColumns = additionalColumns;\n     this.keyGenerator = keyGenerator;\n     this.makeAllColumns();\n+    this.additionalColumns = (additionalColumns == null)? new byte[0][0] :\n+                                                          additionalColumns;\n   }\n \n   public IndexSpecification() {",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1534 Got ZooKeeper event, state: Disconnected on HRS and then NPE on reinit\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@799117 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/4267838a0cde02ecbafe323a889c857f37faccb6",
        "parent": "https://github.com/apache/hbase/commit/21d54c0c8c33a2b4af36f022c63a61ffc4432482",
        "bug_id": "hbase_303",
        "file": [
            {
                "sha": "d6f403e95990f7719af72fffdee22ef404e78c6d",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/4267838a0cde02ecbafe323a889c857f37faccb6/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/4267838a0cde02ecbafe323a889c857f37faccb6/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=4267838a0cde02ecbafe323a889c857f37faccb6",
                "patch": "@@ -304,6 +304,8 @@ Release 0.20.0 - Unreleased\n                data to be used\n    HBASE-1573  Holes in master state change; updated startcode and server\n                go into .META. but catalog scanner just got old values (redux)\n+   HBASE-1534  Got ZooKeeper event, state: Disconnected on HRS and then NPE\n+               on reinit\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "a0633a867157be912c5154f72fb04a86a36902de",
                "filename": "src/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/4267838a0cde02ecbafe323a889c857f37faccb6/src/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/4267838a0cde02ecbafe323a889c857f37faccb6/src/java/org/apache/hadoop/hbase/master/RegionManager.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/RegionManager.java?ref=4267838a0cde02ecbafe323a889c857f37faccb6",
                "patch": "@@ -240,10 +240,9 @@ private void assignRegionsToMultipleServers(final HServerLoad thisServersLoad,\n \n     int nRegionsToAssign = regionsToAssign.size();\n     int nregions = regionsPerServer(nRegionsToAssign, thisServersLoad);\n-    LOG.debug(\"multi assing for \" + info + \": nregions to assign: \"\n-        + nRegionsToAssign\n-        +\" and nregions: \" + nregions\n-        + \" metaAssign: \" + isMetaAssign);\n+    LOG.debug(\"Assigning for \" + info + \": total nregions to assign=\" +\n+      nRegionsToAssign + \", nregions to reach balance=\" + nregions +\n+      \", isMetaAssign=\" + isMetaAssign);\n     nRegionsToAssign -= nregions;\n     if (nRegionsToAssign > 0 || isMetaAssign) {\n       // We still have more regions to assign. See how many we can assign",
                "deletions": 4
            },
            {
                "sha": "ff3a5f768a5ce8ceffe88990e403c4f6928ed190",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/4267838a0cde02ecbafe323a889c857f37faccb6/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/4267838a0cde02ecbafe323a889c857f37faccb6/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 12,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=4267838a0cde02ecbafe323a889c857f37faccb6",
                "patch": "@@ -417,7 +417,17 @@ public void run() {\n     regionServerThread = Thread.currentThread();\n     boolean quiesceRequested = false;\n     try {\n-      init(reportForDuty());\n+      MapWritable w = null;\n+      while (!stopRequested.get()) {\n+        w = reportForDuty();\n+        if (w != null) {\n+          init(w);\n+          break;\n+        }\n+        sleeper.sleep();\n+        LOG.warn(\"No response from master on reportForDuty. Sleeping and \" +\n+          \"then trying again.\");\n+      }\n       long lastMsg = 0;\n       // Now ask master what it wants us to do and tell it what we have done\n       for (int tries = 0; !stopRequested.get() && isHealthy();) {",
                "deletions": 1
            },
            {
                "sha": "b4a2d36c92fd8fb8d5214a1a28d9d4e7feab09aa",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "blob_url": "https://github.com/apache/hbase/blob/4267838a0cde02ecbafe323a889c857f37faccb6/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "raw_url": "https://github.com/apache/hbase/raw/4267838a0cde02ecbafe323a889c857f37faccb6/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=4267838a0cde02ecbafe323a889c857f37faccb6",
                "patch": "@@ -295,6 +295,7 @@ private void doReconstructionLog(final Path reconstructionLog,\n     // TODO: This could grow large and blow heap out.  Need to get it into\n     // general memory usage accounting.\n     long maxSeqIdInLog = -1;\n+    long firstSeqIdInLog = -1;\n     // TODO: Move this memstoring over into MemStore.\n     KeyValueSkipListSet reconstructedCache =\n       new KeyValueSkipListSet(this.comparator);\n@@ -309,6 +310,9 @@ private void doReconstructionLog(final Path reconstructionLog,\n       int reportInterval =\n         this.conf.getInt(\"hbase.hstore.report.interval.edits\", 2000);\n       while (logReader.next(key, val)) {\n+        if (firstSeqIdInLog == -1) {\n+          firstSeqIdInLog = key.getLogSeqNum();\n+        }\n         maxSeqIdInLog = Math.max(maxSeqIdInLog, key.getLogSeqNum());\n         if (key.getLogSeqNum() <= maxSeqID) {\n           skippedEdits++;\n@@ -335,7 +339,9 @@ private void doReconstructionLog(final Path reconstructionLog,\n       }\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Applied \" + editsCount + \", skipped \" + skippedEdits +\n-          \" because sequence id <= \" + maxSeqID);\n+          \"; store maxSeqID=\" + maxSeqID +\n+          \", firstSeqIdInLog=\" + firstSeqIdInLog +\n+          \", maxSeqIdInLog=\" + maxSeqIdInLog);\n       }\n     } finally {\n       logReader.close();",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1568 Client doesnt consult old row filter interface in filterSaysStop() - could result in NPE or excessive scanning\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@787742 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/fe66233cfa04efe581b023fe11e98cba33f7c77c",
        "parent": "https://github.com/apache/hbase/commit/fdf64a39888de253227c0d38b62dcf5786b97716",
        "bug_id": "hbase_304",
        "file": [
            {
                "sha": "deb2f4b76e9b6b87012eea67adaab7591e87e2c7",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/fe66233cfa04efe581b023fe11e98cba33f7c77c/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/fe66233cfa04efe581b023fe11e98cba33f7c77c/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=fe66233cfa04efe581b023fe11e98cba33f7c77c",
                "patch": "@@ -216,6 +216,8 @@ Release 0.20.0 - Unreleased\n                that into 'now'\n    HBASE-1508  Shell \"close_region\" reveals a Master<>HRS problem, regions are\n                not reassigned\n+   HBASE-1568  Client doesnt consult old row filter interface in\n+               filterSaysStop() - could result in NPE or excessive scanning\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "01f77219fa25936cd44c1478e73070258a47c142",
                "filename": "src/java/org/apache/hadoop/hbase/client/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/fe66233cfa04efe581b023fe11e98cba33f7c77c/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/fe66233cfa04efe581b023fe11e98cba33f7c77c/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "status": "modified",
                "changes": 13,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HTable.java?ref=fe66233cfa04efe581b023fe11e98cba33f7c77c",
                "patch": "@@ -1866,9 +1866,16 @@ private boolean filterSaysStop(final byte [] endKey) {\n       if(!scan.hasFilter()) {\n         return false;\n       }\n-      // Let the filter see current row.\n-      scan.getFilter().filterRowKey(endKey, 0, endKey.length);\n-      return scan.getFilter().filterAllRemaining();\n+      if (scan.getFilter() != null) {\n+        // Let the filter see current row.\n+        scan.getFilter().filterRowKey(endKey, 0, endKey.length);\n+        return scan.getFilter().filterAllRemaining();\n+      }\n+      if (scan.getOldFilter() != null) {\n+        scan.getOldFilter().filterRowKey(endKey, 0, endKey.length);\n+        return scan.getOldFilter().filterAllRemaining();\n+      }\n+      return false; //unlikely.\n     }\n \n     public Result next() throws IOException {",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1534 Got ZooKeeper event, state: Disconnected on HRS and then NPE on reinit\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@786676 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/22e101074b78df529d554ed93e81b925693a7a70",
        "parent": "https://github.com/apache/hbase/commit/4719eb4c3a919403648b5aa8cfcb47668ed8c227",
        "bug_id": "hbase_305",
        "file": [
            {
                "sha": "4399e2f98d816159ef439d7c53447ea5928ace0b",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/22e101074b78df529d554ed93e81b925693a7a70/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/22e101074b78df529d554ed93e81b925693a7a70/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=22e101074b78df529d554ed93e81b925693a7a70",
                "patch": "@@ -204,6 +204,8 @@ Release 0.20.0 - Unreleased\n                and merge tool\n    HBASE-1531  Change new Get to use new filter API\n    HBASE-1549  in zookeeper.sh, use localhost instead of 127.0.0.1\n+   HBASE-1534  Got ZooKeeper event, state: Disconnected on HRS and then NPE on\n+               reinit\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "b547d73b274e590d55ed796b7440126625d6b123",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/22e101074b78df529d554ed93e81b925693a7a70/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/22e101074b78df529d554ed93e81b925693a7a70/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=22e101074b78df529d554ed93e81b925693a7a70",
                "patch": "@@ -1311,9 +1311,11 @@ private boolean getMaster() {\n    * Run initialization using parameters passed us by the master.\n    */\n   private MapWritable reportForDuty() {\n-    if (!getMaster()) {\n-      return null;\n+    while (!getMaster()) {\n+      sleeper.sleep();\n+      LOG.warn(\"Unable to get master for initialization\");\n     }\n+\n     MapWritable result = null;\n     long lastMsg = 0;\n     while(!stopRequested.get()) {",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": " HBASE-1431 NPE in HTable.checkAndSave when row doesn't exist\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@775739 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
        "parent": "https://github.com/apache/hbase/commit/755ea927b4c381d01025e9b66f05d780451b0f23",
        "bug_id": "hbase_306",
        "file": [
            {
                "sha": "bd4122c7ee7d44cb32f28a2db351f6a3c1ed1e2f",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
                "patch": "@@ -131,6 +131,8 @@ Release 0.20.0 - Unreleased\n    HBASE-1323  hbase-1234 broke TestThriftServer; fix and reenable\n    HBASE-1425  ColumnValueFilter and WhileMatchFilter fixes on trunk\n                (Clint Morgan via Stack)\n+   HBASE-1431  NPE in HTable.checkAndSave when row doesn't exist (Guilherme\n+               Mauro Germoglio Barbosa via Andrew Purtell)\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "8b2342cdb324872190ef643f85ef4e9c219d13db",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 18,
                "additions": 11,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
                "patch": "@@ -1396,13 +1396,17 @@ public boolean checkAndSave(BatchUpdate b,\n         Map<byte[],Cell> actualValues = getFull(row, keySet,\n           HConstants.LATEST_TIMESTAMP, 1,lid);\n         for (byte[] key : keySet) {\n-          // If test fails exit\n-          if(!Bytes.equals(actualValues.get(key).getValue(),\n-              expectedValues.get(key))) {\n-            success = false;\n-            break;\n-          }\n-        }\n+\t  // If test fails exit\n+\t  Cell cell = actualValues.get(key);\n+\t  byte[] actualValue = new byte[] {};\n+\t  if (cell != null) \n+\t    actualValue = cell.getValue();\n+\t  if(!Bytes.equals(actualValue,\n+\t\t\t   expectedValues.get(key))) {\n+\t    success = false;\n+\t    break;\n+\t  }\n+\t}\n         if (success) {\n           long commitTime = (b.getTimestamp() == LATEST_TIMESTAMP)?\n             now: b.getTimestamp();",
                "deletions": 7
            },
            {
                "sha": "fbc9c1d8c475cf145eb65ae731d28cda8d864bf2",
                "filename": "src/test/org/apache/hadoop/hbase/client/TestHTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/test/org/apache/hadoop/hbase/client/TestHTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767/src/test/org/apache/hadoop/hbase/client/TestHTable.java",
                "status": "modified",
                "changes": 83,
                "additions": 80,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/client/TestHTable.java?ref=6eb8f1e58faa8c6963c3d5ab98c447cdaf9da767",
                "patch": "@@ -235,12 +235,24 @@ public void testCheckAndSave() throws IOException {\n     BatchUpdate batchUpdate = new BatchUpdate(row);\n     BatchUpdate batchUpdate2 = new BatchUpdate(row);\n     BatchUpdate batchUpdate3 = new BatchUpdate(row);\n+\n+    // this row doesn't exist when checkAndSave is invoked\n+    byte [] row1 = Bytes.toBytes(\"row1\");\n+    BatchUpdate batchUpdate4 = new BatchUpdate(row1);\n     \n+    // to be used for a checkAndSave for expected empty columns\n+    BatchUpdate batchUpdate5 = new BatchUpdate(row);\n+\n     HbaseMapWritable<byte[],byte[]> expectedValues =\n       new HbaseMapWritable<byte[],byte[]>();\n     HbaseMapWritable<byte[],byte[]> badExpectedValues =\n       new HbaseMapWritable<byte[],byte[]>();\n-    \n+    HbaseMapWritable<byte[],byte[]> expectedNoValues =\n+      new HbaseMapWritable<byte[],byte[]>();\n+    // the columns used here must not be updated on batchupate\n+    HbaseMapWritable<byte[],byte[]> expectedNoValues1 =\n+      new HbaseMapWritable<byte[],byte[]>();\n+\n     for(int i = 0; i < 5; i++) {\n       // This batchupdate is our initial batch update,\n       // As such we also set our expected values to the same values\n@@ -250,14 +262,27 @@ public void testCheckAndSave() throws IOException {\n       \n       badExpectedValues.put(Bytes.toBytes(COLUMN_FAMILY_STR+i),\n         Bytes.toBytes(500));\n-      \n+\n+      expectedNoValues.put(Bytes.toBytes(COLUMN_FAMILY_STR+i), new byte[] {});\n+      // the columns used here must not be updated on batchupate\n+      expectedNoValues1.put(Bytes.toBytes(COLUMN_FAMILY_STR+i+\",\"+i), new byte[] {});\n+\n+\n       // This is our second batchupdate that we will use to update the initial\n       // batchupdate\n       batchUpdate2.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+1));\n       \n       // This final batch update is to check that our expected values (which\n       // are now wrong)\n       batchUpdate3.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+2));\n+\n+      // Batch update that will not happen because it is to happen with some \n+      // expected values, but the row doesn't exist\n+      batchUpdate4.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i));\n+\n+      // Batch update will happen: the row exists, but the expected columns don't,\n+      // just as the condition\n+      batchUpdate5.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+3));\n     }\n     \n     // Initialize rows\n@@ -279,6 +304,58 @@ public void testCheckAndSave() throws IOException {\n     \n     // make sure that the old expected values fail\n     assertFalse(table.checkAndSave(batchUpdate3, expectedValues,null));\n+\n+    // row doesn't exist, so doesn't matter the expected \n+    // values (unless they are empty) \n+    assertFalse(table.checkAndSave(batchUpdate4, badExpectedValues, null));\n+\n+    assertTrue(table.checkAndSave(batchUpdate4, expectedNoValues, null));\n+    // make sure check and save saves the data when expected values were empty and the row\n+    // didn't exist\n+    r = table.getRow(row1);\n+    columns = batchUpdate4.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate4.get(columns[i])));\n+    }  \n+\n+    // since the row isn't empty anymore, those expected (empty) values \n+    // are not valid anymore, so check and save method doesn't save. \n+    assertFalse(table.checkAndSave(batchUpdate4, expectedNoValues, null));\n+    \n+    // the row exists, but the columns don't. since the expected values are \n+    // for columns without value, checkAndSave must be successful. \n+    assertTrue(table.checkAndSave(batchUpdate5, expectedNoValues1, null));\n+    // make sure checkAndSave saved values for batchUpdate5.\n+    r = table.getRow(row);\n+    columns = batchUpdate5.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate5.get(columns[i])));\n+    }  \n+\n+    // since the condition wasn't changed, the following checkAndSave \n+    // must also be successful.\n+    assertTrue(table.checkAndSave(batchUpdate, expectedNoValues1, null));\n+    // make sure checkAndSave saved values for batchUpdate1\n+    r = table.getRow(row);\n+    columns = batchUpdate.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate.get(columns[i])));\n+    }\n+\n+    // one failing condition must make the following checkAndSave fail\n+    // the failing condition is a column to be empty, however, it has a value.\n+    HbaseMapWritable<byte[],byte[]> expectedValues1 =\n+      new HbaseMapWritable<byte[],byte[]>();\n+    expectedValues1.put(Bytes.toBytes(COLUMN_FAMILY_STR+0), new byte[] {});\n+    expectedValues1.put(Bytes.toBytes(COLUMN_FAMILY_STR+\"EMPTY+ROW\"), new byte[] {});\n+    assertFalse(table.checkAndSave(batchUpdate5, expectedValues1, null));\n+\n+    // assure the values on the row remain the same\n+    r = table.getRow(row);\n+    columns = batchUpdate.getColumns();\n+    for(int i = 0; i < columns.length;i++) {\n+      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate.get(columns[i])));\n+    }    \n   }\n \n   /**\n@@ -373,5 +450,5 @@ public void testTableNotFoundExceptionWithATable() {\n      fail(\"Should have thrown a TableNotFoundException instead of a \" +\n        e.getClass());\n    }\n-  }\n+   }\n }",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1391 NPE in TableInputFormatBase.restart if zoo.cfg is wrong or missing on tasktrackers\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@773825 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/62b33ba91052ee138c8110f2ebb858accd8da4ac",
        "parent": "https://github.com/apache/hbase/commit/58e206e9c80955372853227225d736ea1e992394",
        "bug_id": "hbase_307",
        "file": [
            {
                "sha": "e8e1fac4b9b20c9926d6a972c546e15c03620ba3",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "patch": "@@ -123,6 +123,8 @@ Release 0.20.0 - Unreleased\n                Purtell)\n    HBASE-1311  ZooKeeperWrapper: Failed to set watcher on ZNode /hbase/master\n                (Nitay Joffe via Stack)\n+   HBASE-1391  NPE in TableInputFormatBase$TableRecordReader.restart if zoo.cfg\n+               is wrong or missing on task trackers\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "38d56b8596d64d2b6a3e2432464610cb2f1114b7",
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "status": "modified",
                "changes": 9,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.mapred.FileInputFormat;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.JobConfigurable;\n+import org.apache.hadoop.util.StringUtils;\n \n /**\n  * Convert HBase tabular data into a format that is consumable by Map/Reduce.\n@@ -58,7 +59,7 @@ public void configure(JobConf job) {\n     try {\n       setHTable(new HTable(new HBaseConfiguration(job), tableNames[0].getName()));\n     } catch (Exception e) {\n-      LOG.error(e);\n+      LOG.error(StringUtils.stringifyException(e));\n     }\n   }\n \n@@ -69,6 +70,12 @@ public void validateInput(JobConf job) throws IOException {\n       throw new IOException(\"expecting one table name\");\n     }\n \n+    // connected to table?\n+    if (getHTable() == null) {\n+      throw new IOException(\"could not connect to table '\" +\n+        tableNames[0].getName() + \"'\");\n+    }\n+\n     // expecting at least one column\n     String colArg = job.get(COLUMN_LIST);\n     if (colArg == null || colArg.length() == 0) {",
                "deletions": 1
            },
            {
                "sha": "ba82b7a9f9bedf25a24d15e2e4a100ec0780311f",
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java",
                "status": "modified",
                "changes": 7,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "patch": "@@ -304,6 +304,13 @@ protected void setInputColumns(byte [][] inputColumns) {\n     this.inputColumns = inputColumns;\n   }\n \n+  /**\n+   * Allows subclasses to get the {@link HTable}.\n+   */\n+  protected HTable getHTable() {\n+    return this.table;\n+  }\n+\n   /**\n    * Allows subclasses to set the {@link HTable}.\n    *",
                "deletions": 0
            },
            {
                "sha": "a8e1f5e314119f1662e0ff5ded6514a33b664ab9",
                "filename": "src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java",
                "blob_url": "https://github.com/apache/hbase/blob/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java",
                "raw_url": "https://github.com/apache/hbase/raw/62b33ba91052ee138c8110f2ebb858accd8da4ac/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java",
                "status": "modified",
                "changes": 15,
                "additions": 15,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java?ref=62b33ba91052ee138c8110f2ebb858accd8da4ac",
                "patch": "@@ -20,6 +20,8 @@\n package org.apache.hadoop.hbase.zookeeper;\n \n import java.io.IOException;\n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Properties;\n@@ -32,6 +34,7 @@\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.HServerInfo;\n import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.zookeeper.CreateMode;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.Watcher;\n@@ -158,6 +161,7 @@ private static void loadZooKeeperConfig() {\n \n     // The clientPort option may come after the server.X hosts, so we need to\n     // grab everything and then create the final host:port comma separated list.\n+    boolean anyValid = false;\n     for (Entry<Object,Object> property : properties.entrySet()) {\n       String key = property.getKey().toString().trim();\n       String value = property.getValue().toString().trim();\n@@ -167,9 +171,20 @@ private static void loadZooKeeperConfig() {\n       else if (key.startsWith(\"server.\")) {\n         String host = value.substring(0, value.indexOf(':'));\n         servers.add(host);\n+        try {\n+          InetAddress.getByName(host);\n+          anyValid = true;\n+        } catch (UnknownHostException e) {\n+          LOG.warn(StringUtils.stringifyException(e));\n+        }\n       }\n     }\n \n+    if (!anyValid) {\n+      LOG.error(\"no valid quorum servers found in \" + ZOOKEEPER_CONFIG_NAME);\n+      return;\n+    }\n+\n     if (clientPort == null) {\n       LOG.error(\"no clientPort found in \" + ZOOKEEPER_CONFIG_NAME);\n       return;",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1101 NPE in HConnectionManager.processBatchOfRows; restore original (and correct) reload behavior\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@731904 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/8e047c5801fd44a2aaebcd7968e248955c4a20d4",
        "parent": "https://github.com/apache/hbase/commit/700b2a782d4d88bfa0f5c0ad78a8feba2c485cb2",
        "bug_id": "hbase_308",
        "file": [
            {
                "sha": "3caf4dc8d02b3f7cdb8680491601626451dca0f1",
                "filename": "src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/8e047c5801fd44a2aaebcd7968e248955c4a20d4/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/8e047c5801fd44a2aaebcd7968e248955c4a20d4/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=8e047c5801fd44a2aaebcd7968e248955c4a20d4",
                "patch": "@@ -874,13 +874,12 @@ private HRegionLocation locateRootRegion()\n     }\n \n     private HRegionLocation\n-    getRegionLocationForRowWithRetries(byte[] tableName, byte[] rowKey)\n-        throws IOException {\n+    getRegionLocationForRowWithRetries(byte[] tableName, byte[] rowKey, \n+        boolean reload) throws IOException {\n       getMaster();\n       List<Throwable> exceptions = new ArrayList<Throwable>();\n       HRegionLocation location = null;\n       int tries = 0;\n-      boolean reload = false;\n       while (tries < numRetries) {\n         try {\n           location = getRegionLocation(tableName, rowKey, reload);\n@@ -915,7 +914,8 @@ public void processBatchOfRows(ArrayList<BatchUpdate> list, byte[] tableName)\n       Collections.sort(list);\n       List<BatchUpdate> tempUpdates = new ArrayList<BatchUpdate>();\n       HRegionLocation location =\n-        getRegionLocationForRowWithRetries(tableName, list.get(0).getRow());\n+        getRegionLocationForRowWithRetries(tableName, list.get(0).getRow(),\n+            false);\n       byte [] currentRegion = location.getRegionInfo().getRegionName();\n       byte [] region = currentRegion;\n       boolean isLastRow = false;\n@@ -925,7 +925,7 @@ public void processBatchOfRows(ArrayList<BatchUpdate> list, byte[] tableName)\n         isLastRow = (i + 1) == list.size();\n         if (!isLastRow) {\n           location = getRegionLocationForRowWithRetries(tableName,\n-            list.get(i+1).getRow());\n+            list.get(i+1).getRow(), false);\n           region = location.getRegionInfo().getRegionName();\n         }\n         if (!Bytes.equals(currentRegion, region) || isLastRow || retryOnlyOne) {\n@@ -960,7 +960,7 @@ public Integer call() throws IOException {\n             i = i - updates.length + index;\n             retryOnlyOne = true;\n             location = getRegionLocationForRowWithRetries(tableName, \n-              list.get(i + 1).getRow());\n+              list.get(i + 1).getRow(), true);\n             region = location.getRegionInfo().getRegionName();\n           }\n           else {",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-925 HRS NPE on way out if no master to connect to\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@704597 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/767f6057ffff460082478258e8259776a13b2a10",
        "parent": "https://github.com/apache/hbase/commit/031c18faff7beeda40cfd2f780f16ad33c8843af",
        "bug_id": "hbase_309",
        "file": [
            {
                "sha": "ea436f6e24707ca58b368f6108ab1a2fdff43ec6",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/767f6057ffff460082478258e8259776a13b2a10/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/767f6057ffff460082478258e8259776a13b2a10/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=767f6057ffff460082478258e8259776a13b2a10",
                "patch": "@@ -23,6 +23,7 @@ Release 0.19.0 - Unreleased\n    HBASE-918   Region balancing during startup makes cluster unstable\n    HBASE-921   region close and open processed out of order; makes for \n                disagreement between master and regionserver on region state\n+   HBASE-925   HRS NPE on way out if no master to connect to\n \n   IMPROVEMENTS\n    HBASE-901   Add a limit to key length, check key and value length on client side",
                "deletions": 0
            },
            {
                "sha": "05efc1ed6f13172c24da6d8c60e46df4b69592f5",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/767f6057ffff460082478258e8259776a13b2a10/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/767f6057ffff460082478258e8259776a13b2a10/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 6,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=767f6057ffff460082478258e8259776a13b2a10",
                "patch": "@@ -446,8 +446,10 @@ public void run() {\n       if (this.fsOk) {\n         // Only try to clean up if the file system is available\n         try {\n-          this.log.close();\n-          LOG.info(\"On abort, closed hlog\");\n+          if (this.log != null) {\n+            this.log.close();\n+            LOG.info(\"On abort, closed hlog\");\n+          }\n         } catch (IOException e) {\n           LOG.error(\"Unable to close log in abort\",\n               RemoteExceptionHandler.checkIOException(e));",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-833 Doing an insert with an unknown family throws a NPE in HRS\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@686322 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/9460067fd85568f9eff7c57bb93aacfa01c9d6d0",
        "parent": "https://github.com/apache/hbase/commit/1fb69ec11a9d1dbc72a70d0d6a078bb4d609b80e",
        "bug_id": "hbase_310",
        "file": [
            {
                "sha": "46268e9f69712f4da0cb2c4aa5ed8ce1720dd098",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/9460067fd85568f9eff7c57bb93aacfa01c9d6d0/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/9460067fd85568f9eff7c57bb93aacfa01c9d6d0/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9460067fd85568f9eff7c57bb93aacfa01c9d6d0",
                "patch": "@@ -25,6 +25,7 @@ Release 0.3.0 - Unreleased\n                (Jean-Daniel Cryans via Stack)\n    HBASE-831   committing BatchUpdate with no row should complain\n                (Andrew Purtell via Jim Kellerman)\n+   HBASE-833   Doing an insert with an unknown family throws a NPE in HRS\n \n   IMPROVEMENTS\n    HBASE-801  When a table haven't disable, shell could response in a \"user",
                "deletions": 0
            },
            {
                "sha": "9a6b29c8b26a4e8b91055977d00bc054c0636d0b",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/9460067fd85568f9eff7c57bb93aacfa01c9d6d0/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/9460067fd85568f9eff7c57bb93aacfa01c9d6d0/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 20,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=9460067fd85568f9eff7c57bb93aacfa01c9d6d0",
                "patch": "@@ -53,6 +53,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HMsg;\n import org.apache.hadoop.hbase.HRegionInfo;\n@@ -1159,15 +1160,18 @@ private void validateValuesLength(BatchUpdate batchUpdate,\n       batchUpdate.iterator(); iter.hasNext();) {\n       \n       BatchOperation operation = iter.next();\n-      int maxLength = \n-        desc.getFamily(HStoreKey.getFamily(operation.getColumn())).\n-          getMaxValueLength();\n-      if(operation.getValue() != null)\n-        if(operation.getValue().length > maxLength) {\n-          throw new IOException(\"Value in column \" + \n-              Bytes.toString(operation.getColumn()) + \" is too long. \" + \n-              operation.getValue().length + \" instead of \" + maxLength);\n+      if (operation.getValue() != null) {\n+        HColumnDescriptor fam = \n+          desc.getFamily(HStoreKey.getFamily(operation.getColumn()));\n+        if (fam != null) {\n+          int maxLength = fam.getMaxValueLength();\n+          if (operation.getValue().length > maxLength) {\n+            throw new IOException(\"Value in column \"\n+                + Bytes.toString(operation.getColumn()) + \" is too long. \"\n+                + operation.getValue().length + \" instead of \" + maxLength);\n+          }\n         }\n+      }\n     }\n   }\n   ",
                "deletions": 8
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-2558 fixes for build up on hudson\nPart 5.  Fix NPE in make multiregion.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@611005 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/d004f6e545fc0d1bf575435623dfae6952ac54a8",
        "parent": "https://github.com/apache/hbase/commit/e3031f035f121f90591032606f46b16cb158a9f4",
        "bug_id": "hbase_311",
        "file": [
            {
                "sha": "4859919091432f080b9fea36eb92fd79058c6c52",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/d004f6e545fc0d1bf575435623dfae6952ac54a8/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/d004f6e545fc0d1bf575435623dfae6952ac54a8/CHANGES.txt",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=d004f6e545fc0d1bf575435623dfae6952ac54a8",
                "patch": "@@ -109,7 +109,7 @@ Trunk (unreleased changes)\n                (Bryan Duxbury via Stack)\n    HADOOP-2530 Missing type in new hbase custom RPC serializer\n    HADOOP-2490 Failure in nightly #346 (Added debugging of hudson failures).\n-   HADOOP-2558 fixes for build up on hudson (part 1, part 2, part 3)\n+   HADOOP-2558 fixes for build up on hudson (part 1, part 2, part 3, part 4)\n    \n   IMPROVEMENTS\n    HADOOP-2401 Add convenience put method that takes writable",
                "deletions": 1
            },
            {
                "sha": "7ec2c0a6339ca5a5bbad520d7cc6e9cea4a73995",
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/d004f6e545fc0d1bf575435623dfae6952ac54a8/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/d004f6e545fc0d1bf575435623dfae6952ac54a8/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=d004f6e545fc0d1bf575435623dfae6952ac54a8",
                "patch": "@@ -306,8 +306,12 @@ private static void recalibrate(final HTable t, final Text column,\n    * @throws IOException\n    */\n   private static void compact(final MiniHBaseCluster cluster,\n-      final HRegionInfo r) throws IOException {\n-    \n+      final HRegionInfo r)\n+  throws IOException {\n+    if (r == null) {\n+      LOG.debug(\"Passed region is null\");\n+      return;\n+    }\n     LOG.info(\"Starting compaction\");\n     for (LocalHBaseCluster.RegionServerThread thread:\n         cluster.getRegionThreads()) {",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-2380 REST servlet throws NPE when any value node has an empty string\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@602267 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/02022f99315a204b7ebfac3ccece5d913f373518",
        "parent": "https://github.com/apache/hbase/commit/612f446dbdd151a02700839705c5ebfd1fc33e2a",
        "bug_id": "hbase_312",
        "file": [
            {
                "sha": "8b0ca17887f558dc6cfa88b0139581f4da40e08e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/02022f99315a204b7ebfac3ccece5d913f373518/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/02022f99315a204b7ebfac3ccece5d913f373518/CHANGES.txt",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=02022f99315a204b7ebfac3ccece5d913f373518",
                "patch": "@@ -11,7 +11,7 @@ Trunk (unreleased changes)\n     HADOOP-2084 Add a LocalHBaseCluster\n     HADOOP-2068 RESTful interface (Bryan Duxbury via Stack)\n     HADOOP-2316 Run REST servlet outside of master\n-                (Bryan Duxbury via Stack)\n+                (Bryan Duxbury & Stack)\n \n   OPTIMIZATIONS\n \n@@ -61,6 +61,8 @@ Trunk (unreleased changes)\n    HADOOP-2365 Result of HashFunction.hash() contains all identical values\n    HADOOP-2362 Leaking hdfs file handle on region split\n    HADOOP-2338 Fix NullPointerException in master server.\n+   HADOOP-2380 REST servlet throws NPE when any value node has an empty string\n+               (Bryan Duxbury via Stack)\n \n   IMPROVEMENTS\n    HADOOP-2401 Add convenience put method that takes writable",
                "deletions": 1
            },
            {
                "sha": "acc3ce8eb4668224989123553823f901c3e55c30",
                "filename": "src/java/org/apache/hadoop/hbase/rest/TableHandler.java",
                "blob_url": "https://github.com/apache/hbase/blob/02022f99315a204b7ebfac3ccece5d913f373518/src/java/org/apache/hadoop/hbase/rest/TableHandler.java",
                "raw_url": "https://github.com/apache/hbase/raw/02022f99315a204b7ebfac3ccece5d913f373518/src/java/org/apache/hadoop/hbase/rest/TableHandler.java",
                "status": "modified",
                "changes": 12,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/rest/TableHandler.java?ref=02022f99315a204b7ebfac3ccece5d913f373518",
                "patch": "@@ -308,8 +308,16 @@ private void putRowXml(HTable table, final HttpServletRequest request,\n \n         Node value_node = column.getElementsByTagName(\"value\").item(0);\n \n-        // decode the base64'd value\n-        byte[] value = org.apache.hadoop.hbase.util.Base64.decode(value_node.getFirstChild().getNodeValue());\n+        byte[] value = new byte[0];\n+        \n+        // for some reason there's no value here. probably indicates that\n+        // the consumer passed a null as the cell value.\n+        if(value_node.getFirstChild() != null && \n+          value_node.getFirstChild().getNodeValue() != null){\n+          // decode the base64'd value\n+          value = org.apache.hadoop.hbase.util.Base64.decode(\n+            value_node.getFirstChild().getNodeValue());\n+        }\n \n         // put the value\n         table.put(lock_id, name, value);",
                "deletions": 2
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-2155 Method expecting HBaseConfiguration throw NPE when given Configuration\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@592549 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/9be624fc933db5f47af82652fc0d8bf6a6db6398",
        "parent": "https://github.com/apache/hbase/commit/cda8c597fc7c51050a154d42b38d23dffdfb2e1e",
        "bug_id": "hbase_313",
        "file": [
            {
                "sha": "1a96b8a1690877eff1cf4e668b9164268f9b9721",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -26,6 +26,7 @@ Trunk (unreleased changes)\n                always kill the region server for the META region. This makes\n                the test more deterministic and getting META reassigned was\n                problematic.\n+   HADOOP-2155 Method expecting HBaseConfiguration throws NPE when given Configuration\n \n   IMPROVEMENTS\n     HADOOP-2401 Add convenience put method that takes writable",
                "deletions": 0
            },
            {
                "sha": "6496b7fb519e0bf3b856fb9e38332d375a63681d",
                "filename": "src/java/org/apache/hadoop/hbase/HBaseAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseAdmin.java",
                "status": "modified",
                "changes": 11,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HBaseAdmin.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -20,22 +20,19 @@\n package org.apache.hadoop.hbase;\n \n import java.io.IOException;\n-import java.util.NoSuchElementException;\n import java.util.Map;\n+import java.util.NoSuchElementException;\n import java.util.SortedMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-\n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.MapWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.Writable;\n import org.apache.hadoop.ipc.RemoteException;\n \n-import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n-import org.apache.hadoop.hbase.util.Writables;\n-\n /**\n  * Provides administrative functions for HBase\n  */\n@@ -53,7 +50,7 @@\n    * @param conf Configuration object\n    * @throws MasterNotRunningException\n    */\n-  public HBaseAdmin(Configuration conf) throws MasterNotRunningException {\n+  public HBaseAdmin(HBaseConfiguration conf) throws MasterNotRunningException {\n     this.connection = HConnectionManager.getConnection(conf);\n     this.pause = conf.getLong(\"hbase.client.pause\", 30 * 1000);\n     this.numRetries = conf.getInt(\"hbase.client.retries.number\", 5);",
                "deletions": 7
            },
            {
                "sha": "d3323db461ba1f82312641d67624839d0d7c426a",
                "filename": "src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java",
                "status": "modified",
                "changes": 11,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HBaseConfiguration.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -28,8 +28,7 @@\n   /** constructor */\n   public HBaseConfiguration() {\n     super();\n-    addResource(\"hbase-default.xml\");\n-    addResource(\"hbase-site.xml\");\n+    addHbaseResources();\n   }\n   \n   /**\n@@ -38,5 +37,13 @@ public HBaseConfiguration() {\n    */\n   public HBaseConfiguration(final Configuration c) {\n     super(c);\n+    if (!(c instanceof HBaseConfiguration)) {\n+      addHbaseResources();\n+    }\n+  }\n+  \n+  private void addHbaseResources() {\n+    addResource(\"hbase-default.xml\");\n+    addResource(\"hbase-site.xml\");\n   }\n }",
                "deletions": 2
            },
            {
                "sha": "0300ff7dba653c03c7005d03ee0c873405dcbb5b",
                "filename": "src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "status": "modified",
                "changes": 18,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnectionManager.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -30,15 +30,13 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ipc.RPC;\n-import org.apache.hadoop.ipc.RemoteException;\n+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.MapWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.Writable;\n-\n-import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n-import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.ipc.RPC;\n+import org.apache.hadoop.ipc.RemoteException;\n \n /**\n  * A non-instantiable class that manages connections to multiple tables in\n@@ -65,7 +63,7 @@ private HConnectionManager() {\n    * @param conf\n    * @return HConnection object for the instance specified by the configuration\n    */\n-  public static HConnection getConnection(Configuration conf) {\n+  public static HConnection getConnection(HBaseConfiguration conf) {\n     HConnection connection;\n     synchronized (HBASE_INSTANCES) {\n       String instanceName = conf.get(HBASE_DIR, DEFAULT_HBASE_DIR);\n@@ -84,7 +82,7 @@ public static HConnection getConnection(Configuration conf) {\n    * Delete connection information for the instance specified by the configuration\n    * @param conf\n    */\n-  public static void deleteConnection(Configuration conf) {\n+  public static void deleteConnection(HBaseConfiguration conf) {\n     synchronized (HBASE_INSTANCES) {\n       HBASE_INSTANCES.remove(conf.get(HBASE_DIR, DEFAULT_HBASE_DIR));\n     }    \n@@ -106,7 +104,7 @@ public static void deleteConnection(Configuration conf) {\n     private final Integer rootRegionLock = new Integer(0);\n     private final Integer metaRegionLock = new Integer(0);\n     \n-    private volatile Configuration conf;\n+    private volatile HBaseConfiguration conf;\n \n     // Map tableName -> (Map startRow -> (HRegionInfo, HServerAddress)\n     private Map<Text, SortedMap<Text, HRegionLocation>> tablesToServers;\n@@ -125,7 +123,7 @@ public static void deleteConnection(Configuration conf) {\n      * @param conf Configuration object\n      */\n     @SuppressWarnings(\"unchecked\")\n-    public TableServers(Configuration conf) {\n+    public TableServers(HBaseConfiguration conf) {\n       this.conf = LocalHBaseCluster.doLocal(new HBaseConfiguration(conf));\n       \n       String serverClassName =",
                "deletions": 10
            },
            {
                "sha": "6f45ce9ee9f92f114dc78299b550b245809dc09b",
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMaster.java",
                "status": "modified",
                "changes": 8,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -94,7 +94,7 @@ public long getProtocolVersion(String protocol,\n   volatile AtomicBoolean closed = new AtomicBoolean(true);\n   volatile boolean fsOk;\n   Path dir;\n-  Configuration conf;\n+  HBaseConfiguration conf;\n   FileSystem fs;\n   Random rand;\n   int threadWakeFrequency; \n@@ -868,7 +868,7 @@ synchronized boolean waitForMetaRegionsOrClose() {\n    * @param conf - Configuration object\n    * @throws IOException\n    */\n-  public HMaster(Configuration conf) throws IOException {\n+  public HMaster(HBaseConfiguration conf) throws IOException {\n     this(new Path(conf.get(HBASE_DIR, DEFAULT_HBASE_DIR)),\n         new HServerAddress(conf.get(MASTER_ADDRESS, DEFAULT_MASTER_ADDRESS)),\n         conf);\n@@ -882,7 +882,7 @@ public HMaster(Configuration conf) throws IOException {\n    * \n    * @throws IOException\n    */\n-  public HMaster(Path dir, HServerAddress address, Configuration conf)\n+  public HMaster(Path dir, HServerAddress address, HBaseConfiguration conf)\n   throws IOException {\n     this.fsOk = true;\n     this.dir = dir;\n@@ -3044,7 +3044,7 @@ protected static void doMain(String [] args,\n       printUsageAndExit();\n     }\n \n-    Configuration conf = new HBaseConfiguration();\n+    HBaseConfiguration conf = new HBaseConfiguration();\n \n     // Process command-line args. TODO: Better cmd-line processing\n     // (but hopefully something not as painful as cli options).",
                "deletions": 4
            },
            {
                "sha": "ab83fbb9285249eda92683fc87f4ce0ffd9643bf",
                "filename": "src/java/org/apache/hadoop/hbase/HMerge.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMerge.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HMerge.java",
                "status": "modified",
                "changes": 14,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMerge.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -28,14 +28,12 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n \n-import org.apache.hadoop.hbase.util.Writables;\n-\n /** \n  * A non-instantiable class that has a static method capable of compacting\n  * a table by merging adjacent regions that have grown too small.\n@@ -61,7 +59,7 @@ private HMerge() {\n    * @param tableName   - Table to be compacted\n    * @throws IOException\n    */\n-  public static void merge(Configuration conf, FileSystem fs, Text tableName)\n+  public static void merge(HBaseConfiguration conf, FileSystem fs, Text tableName)\n       throws IOException {\n     HConnection connection = HConnectionManager.getConnection(conf);\n     boolean masterIsRunning = connection.isMasterRunning();\n@@ -82,7 +80,7 @@ public static void merge(Configuration conf, FileSystem fs, Text tableName)\n   }\n \n   private static abstract class Merger {\n-    protected Configuration conf;\n+    protected HBaseConfiguration conf;\n     protected FileSystem fs;\n     protected Text tableName;\n     protected Path dir;\n@@ -93,7 +91,7 @@ public static void merge(Configuration conf, FileSystem fs, Text tableName)\n     protected HStoreKey key;\n     protected HRegionInfo info;\n     \n-    protected Merger(Configuration conf, FileSystem fs, Text tableName)\n+    protected Merger(HBaseConfiguration conf, FileSystem fs, Text tableName)\n         throws IOException {\n       \n       this.conf = conf;\n@@ -200,7 +198,7 @@ protected abstract void updateMeta(Text oldRegion1, Text oldRegion2,\n     private HScannerInterface metaScanner;\n     private HRegionInfo latestRegion;\n     \n-    OnlineMerger(Configuration conf, FileSystem fs, Text tableName)\n+    OnlineMerger(HBaseConfiguration conf, FileSystem fs, Text tableName)\n     throws IOException {\n       \n       super(conf, fs, tableName);\n@@ -315,7 +313,7 @@ protected void updateMeta(Text oldRegion1, Text oldRegion2,\n     private TreeSet<HRegionInfo> metaRegions;\n     private TreeMap<Text, byte []> results;\n     \n-    OfflineMerger(Configuration conf, FileSystem fs, Text tableName)\n+    OfflineMerger(HBaseConfiguration conf, FileSystem fs, Text tableName)\n         throws IOException {\n       \n       super(conf, fs, tableName);",
                "deletions": 8
            },
            {
                "sha": "fb9355298578af70881de68e9f95aa0e7350d928",
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegion.java",
                "status": "modified",
                "changes": 11,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -111,7 +111,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n       throw new IOException(\"Cannot merge non-adjacent regions\");\n     }\n \n-    Configuration conf = a.getConf();\n+    HBaseConfiguration conf = a.getConf();\n     HTableDescriptor tabledesc = a.getTableDesc();\n     HLog log = a.getLog();\n     Path rootDir = a.getRootDir();\n@@ -194,7 +194,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n   Path rootDir;\n   HLog log;\n   FileSystem fs;\n-  Configuration conf;\n+  HBaseConfiguration conf;\n   HRegionInfo regionInfo;\n   Path regiondir;\n \n@@ -242,7 +242,7 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n    * \n    * @throws IOException\n    */\n-  public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf, \n+  public HRegion(Path rootDir, HLog log, FileSystem fs, HBaseConfiguration conf, \n       HRegionInfo regionInfo, Path initialFiles)\n   throws IOException {\n     this.rootDir = rootDir;\n@@ -559,7 +559,7 @@ public HLog getLog() {\n   }\n \n   /** @return Configuration object */\n-  public Configuration getConf() {\n+  public HBaseConfiguration getConf() {\n     return this.conf;\n   }\n \n@@ -1834,7 +1834,8 @@ public void close() {\n    * @throws IOException\n    */\n   static HRegion createHRegion(final HRegionInfo info, final Path rootDir,\n-      final Configuration conf, final Path initialFiles) throws IOException {\n+      final HBaseConfiguration conf, final Path initialFiles)\n+  throws IOException {\n     Path regionDir = HRegion.getRegionDir(rootDir,\n         HRegionInfo.encodeRegionName(info.getRegionName()));\n     FileSystem fs = FileSystem.get(conf);",
                "deletions": 5
            },
            {
                "sha": "a1825902f7e2775d38b3e128545f4bcc997dc6ef",
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -85,7 +85,7 @@\n   protected volatile boolean fsOk;\n   \n   protected final HServerInfo serverInfo;\n-  protected final Configuration conf;\n+  protected final HBaseConfiguration conf;\n   private final Random rand = new Random();\n   \n   // region name -> HRegion\n@@ -373,7 +373,7 @@ private void checkForLogRoll() {\n    * @param conf\n    * @throws IOException\n    */\n-  public HRegionServer(Configuration conf) throws IOException {\n+  public HRegionServer(HBaseConfiguration conf) throws IOException {\n     this(new HServerAddress(conf.get(REGIONSERVER_ADDRESS,\n         DEFAULT_REGIONSERVER_ADDRESS)), conf);\n   }\n@@ -384,7 +384,7 @@ public HRegionServer(Configuration conf) throws IOException {\n    * @param conf\n    * @throws IOException\n    */\n-  public HRegionServer(HServerAddress address, Configuration conf)\n+  public HRegionServer(HServerAddress address, HBaseConfiguration conf)\n   throws IOException {  \n     this.abortRequested = false;\n     this.fsOk = true;",
                "deletions": 3
            },
            {
                "sha": "c152b7b84201d57c52f25a8165130dc263404d39",
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStore.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -37,7 +37,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n@@ -77,7 +76,7 @@\n   Text familyName;\n   SequenceFile.CompressionType compression;\n   FileSystem fs;\n-  Configuration conf;\n+  HBaseConfiguration conf;\n   Path mapdir;\n   Path loginfodir;\n   Path filterDir;\n@@ -141,7 +140,7 @@\n    */\n   HStore(Path dir, Text regionName, String encodedName,\n       HColumnDescriptor family, FileSystem fs, Path reconstructionLog,\n-      Configuration conf)\n+      HBaseConfiguration conf)\n   throws IOException {  \n     this.dir = dir;\n     this.compactionDir = new Path(dir, \"compaction.dir\");",
                "deletions": 3
            },
            {
                "sha": "175312be228205af7ea37bedaa4d11dbbf70c2c5",
                "filename": "src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "status": "modified",
                "changes": 12,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreFile.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -122,11 +122,11 @@\n   private String encodedRegionName;\n   private Text colFamily;\n   private long fileId;\n-  private final Configuration conf;\n+  private final HBaseConfiguration conf;\n   private Reference reference;\n \n   /** Shutdown constructor used by Writable */\n-  HStoreFile(Configuration conf) {\n+  HStoreFile(HBaseConfiguration conf) {\n     this(conf, new Path(Path.CUR_DIR), \"\", new Text(), 0);\n   }\n   \n@@ -138,7 +138,7 @@\n    * @param colFamily name of the column family\n    * @param fileId file identifier\n    */\n-  HStoreFile(final Configuration conf, final Path dir, \n+  HStoreFile(final HBaseConfiguration conf, final Path dir, \n       final String encodedRegionName, final Text colFamily, final long fileId) {\n     this(conf, dir, encodedRegionName, colFamily, fileId, null);\n   }\n@@ -152,7 +152,7 @@\n    * @param fileId file identifier\n    * @param ref Reference to another HStoreFile.\n    */\n-  HStoreFile(Configuration conf, Path dir, String encodedRegionName, \n+  HStoreFile(HBaseConfiguration conf, Path dir, String encodedRegionName, \n       Text colFamily, long fileId, final Reference ref) {\n     this.conf = conf;\n     this.dir = dir;\n@@ -348,7 +348,7 @@ static Path getHStoreDir(Path dir, String encodedRegionName, Text colFamily) {\n    * Checks the filesystem to determine if the file already exists. If so, it\n    * will keep generating names until it generates a name that does not exist.\n    */\n-  static HStoreFile obtainNewHStoreFile(Configuration conf, Path dir, \n+  static HStoreFile obtainNewHStoreFile(HBaseConfiguration conf, Path dir, \n       String encodedRegionName, Text colFamily, FileSystem fs)\n       throws IOException {\n     \n@@ -378,7 +378,7 @@ static HStoreFile obtainNewHStoreFile(Configuration conf, Path dir,\n    * @return List of store file instances loaded from passed dir.\n    * @throws IOException\n    */\n-  static List<HStoreFile> loadHStoreFiles(Configuration conf, Path dir, \n+  static List<HStoreFile> loadHStoreFiles(HBaseConfiguration conf, Path dir, \n       String encodedRegionName, Text colFamily, FileSystem fs)\n   throws IOException {\n     // Look first at info files.  If a reference, these contain info we need",
                "deletions": 6
            },
            {
                "sha": "697b70e33d255b46f452a842e1b18dabd1469b53",
                "filename": "src/java/org/apache/hadoop/hbase/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/HTable.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTable.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -33,7 +33,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.filter.RowFilterInterface;\n import org.apache.hadoop.hbase.filter.StopRowFilter;\n import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;\n@@ -76,7 +75,7 @@ protected void checkClosed() {\n    * @param tableName name of the table\n    * @throws IOException\n    */\n-  public HTable(Configuration conf, Text tableName) throws IOException {\n+  public HTable(HBaseConfiguration conf, Text tableName) throws IOException {\n     closed = true;\n     this.connection = HConnectionManager.getConnection(conf);\n     this.tableName = tableName;",
                "deletions": 2
            },
            {
                "sha": "8a2c50f68c546b7aa810058b066ec72182f82b0a",
                "filename": "src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java",
                "status": "modified",
                "changes": 11,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -55,14 +55,14 @@\n   private final static int DEFAULT_NO = 1;\n   public static final String LOCAL = \"local\";\n   public static final String LOCAL_COLON = LOCAL + \":\";\n-  private final Configuration conf;\n+  private final HBaseConfiguration conf;\n \n   /**\n    * Constructor.\n    * @param conf\n    * @throws IOException\n    */\n-  public LocalHBaseCluster(final Configuration conf)\n+  public LocalHBaseCluster(final HBaseConfiguration conf)\n   throws IOException {\n     this(conf, DEFAULT_NO);\n   }\n@@ -74,7 +74,8 @@ public LocalHBaseCluster(final Configuration conf)\n    * @param noRegionServers Count of regionservers to start.\n    * @throws IOException\n    */\n-  public LocalHBaseCluster(final Configuration conf, final int noRegionServers)\n+  public LocalHBaseCluster(final HBaseConfiguration conf,\n+    final int noRegionServers)\n   throws IOException {\n     super();\n     this.conf = conf;\n@@ -234,7 +235,7 @@ public void shutdown() {\n    * @return The passed <code>c</code> configuration modified if hbase.master\n    * value was 'local' otherwise, unaltered.\n    */\n-  static Configuration doLocal(final Configuration c) {\n+  static HBaseConfiguration doLocal(final HBaseConfiguration c) {\n     if (!isLocal(c)) {\n       return c;\n     }\n@@ -263,7 +264,7 @@ public static boolean isLocal(final Configuration c) {\n    * @throws IOException\n    */\n   public static void main(String[] args) throws IOException {\n-    Configuration conf = new HBaseConfiguration();\n+    HBaseConfiguration conf = new HBaseConfiguration();\n     LocalHBaseCluster cluster = new LocalHBaseCluster(conf);\n     cluster.startup();\n     HBaseAdmin admin = new HBaseAdmin(conf);",
                "deletions": 5
            },
            {
                "sha": "ed47f9543f18d8e0415333ef8f140e523af96fca",
                "filename": "src/java/org/apache/hadoop/hbase/Shell.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/Shell.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/Shell.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/Shell.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -74,7 +74,7 @@ public static String executeTime(boolean watch, long start, long end) {\n    */\n   public static void main(@SuppressWarnings(\"unused\") String args[])\n   throws IOException {\n-    Configuration conf = new HBaseConfiguration();\n+    HBaseConfiguration conf = new HBaseConfiguration();\n     ConsoleReader reader = new ConsoleReader();\n     reader.setBellEnabled(conf.getBoolean(\"hbaseshell.jline.bell.enabled\",\n       DEFAULT_BELL_ENABLED));",
                "deletions": 1
            },
            {
                "sha": "24d016b9a8f42dd25673f4a200567b75077c26c8",
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.hadoop.mapred.RecordReader;\n import org.apache.hadoop.mapred.Reporter;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.hbase.HScannerInterface;\n import org.apache.hadoop.hbase.HStoreKey;\n@@ -195,7 +196,7 @@ public void configure(JobConf job) {\n       m_cols[i] = new Text(colNames[i]);\n     }\n     try {\n-      m_table = new HTable(job, m_tableName);\n+      m_table = new HTable(new HBaseConfiguration(job), m_tableName);\n     } catch (Exception e) {\n       LOG.error(e);\n     }",
                "deletions": 1
            },
            {
                "sha": "a53f24238816e429b8dffaac1218bd3b986fdeb6",
                "filename": "src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java",
                "status": "modified",
                "changes": 11,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.util.Progressable;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n \n@@ -51,7 +52,9 @@\n   static final Logger LOG = Logger.getLogger(TableOutputFormat.class.getName());\n \n   /** constructor */\n-  public TableOutputFormat() {}\n+  public TableOutputFormat() {\n+    super();\n+  }\n \n   /**\n    * Convert Reduce output (key, value) to (HStoreKey, KeyedDataArrayWritable) \n@@ -71,7 +74,9 @@ public TableRecordWriter(HTable table) {\n     }\n \n     /** {@inheritDoc} */\n-    public void close(@SuppressWarnings(\"unused\") Reporter reporter) {}\n+    public void close(@SuppressWarnings(\"unused\") Reporter reporter) {\n+      // Nothing to do.\n+    }\n \n     /** {@inheritDoc} */\n     public void write(Text key, MapWritable value) throws IOException {\n@@ -99,7 +104,7 @@ public RecordWriter getRecordWriter(\n     Text tableName = new Text(job.get(OUTPUT_TABLE));\n     HTable table = null;\n     try {\n-      table = new HTable(job, tableName);\n+      table = new HTable(new HBaseConfiguration(job), tableName);\n     } catch(IOException e) {\n       LOG.error(e);\n       throw e;",
                "deletions": 3
            },
            {
                "sha": "dcfd4385c8c3162caf8c5484eab92d5c68818cdc",
                "filename": "src/java/org/apache/hadoop/hbase/shell/AlterCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/AlterCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/AlterCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/AlterCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -25,8 +25,8 @@\n import java.util.Map;\n import java.util.Set;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.io.Text;\n \n@@ -45,7 +45,7 @@ public AlterCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     try {\n       HBaseAdmin admin = new HBaseAdmin(conf);\n       Set<String> columns = null;",
                "deletions": 2
            },
            {
                "sha": "0d580fe055a6e8b06047983b32ab09bfbe27e67c",
                "filename": "src/java/org/apache/hadoop/hbase/shell/ClearCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ClearCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ClearCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ClearCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -22,7 +22,7 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n \n /**\n  * Clears the console screen. \n@@ -32,7 +32,7 @@ public ClearCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     clear();\n     return null;\n   }",
                "deletions": 2
            },
            {
                "sha": "e59631c97ac3f2ca59fed3bf6c0e403e688245a8",
                "filename": "src/java/org/apache/hadoop/hbase/shell/Command.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/Command.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/Command.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/Command.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -19,7 +19,7 @@\n  */\n package org.apache.hadoop.hbase.shell;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n \n public interface Command {\n   /** family indicator */\n@@ -31,7 +31,7 @@\n    * @param conf Configuration\n    * @return Result of command execution\n    */\n-  public ReturnMsg execute(final Configuration conf);\n+  public ReturnMsg execute(final HBaseConfiguration conf);\n \n   /**\n    * @return Type of this command whether DDL, SELECT, INSERT, UPDATE, DELETE,",
                "deletions": 2
            },
            {
                "sha": "ef6927323721ee90a3321bf4aef4cce838332948",
                "filename": "src/java/org/apache/hadoop/hbase/shell/CreateCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/CreateCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/CreateCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/CreateCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -24,8 +24,8 @@\n import java.util.Map;\n import java.util.Set;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HTableDescriptor;\n \n@@ -41,7 +41,7 @@ public CreateCommand(Writer o) {\n     super(o);\n   }\n   \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     try {\n       HBaseAdmin admin = new HBaseAdmin(conf);\n       HTableDescriptor tableDesc = new HTableDescriptor(tableName);",
                "deletions": 2
            },
            {
                "sha": "bcde0a85c7fb5ef0c6ce5ad15b1b546a0c34b45d",
                "filename": "src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DeleteCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -24,8 +24,8 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.io.Text;\n \n@@ -41,7 +41,7 @@ public DeleteCommand(Writer o) {\n   private String rowKey;\n   private List<String> columnList;\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     if (columnList == null) {\n       throw new IllegalArgumentException(\"Column list is null\");\n     }",
                "deletions": 2
            },
            {
                "sha": "7b03a9266300c66a5a7314a3e28656d7ab36735b",
                "filename": "src/java/org/apache/hadoop/hbase/shell/DescCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DescCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DescCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DescCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -22,7 +22,7 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConnection;\n import org.apache.hadoop.hbase.HConnectionManager;\n@@ -49,7 +49,7 @@ public DescCommand(final Writer o, final TableFormatter f) {\n     this.formatter = f;\n   }\n   \n-  public ReturnMsg execute(final Configuration conf) {\n+  public ReturnMsg execute(final HBaseConfiguration conf) {\n     if (this.tableName == null) \n       return new ReturnMsg(0, \"Syntax error : Please check 'Describe' syntax\");\n     try {",
                "deletions": 2
            },
            {
                "sha": "dec1ff9bb3f4e43e1bae2344753118b31be507df",
                "filename": "src/java/org/apache/hadoop/hbase/shell/DisableCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DisableCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DisableCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DisableCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -36,7 +36,7 @@ public DisableCommand(Writer o) {\n     super(o);\n   }\n  \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     assert tableName != null;\n     \n     try {",
                "deletions": 2
            },
            {
                "sha": "87266f0f2b1ec88f46ba057a95fcbf890237ec46",
                "filename": "src/java/org/apache/hadoop/hbase/shell/DropCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DropCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/DropCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/DropCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -23,8 +23,8 @@\n import java.io.Writer;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -37,7 +37,7 @@ public DropCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     if (tableList == null) {\n       throw new IllegalArgumentException(\"List of tables is null\");\n     }",
                "deletions": 2
            },
            {
                "sha": "b4c5356f5ff221ae9edc0d73fdf3951b10ba0599",
                "filename": "src/java/org/apache/hadoop/hbase/shell/EnableCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/EnableCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/EnableCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/EnableCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -36,7 +36,7 @@ public EnableCommand(Writer o) {\n     super(o);\n   }\n  \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     assert tableName != null;\n     try {\n       HBaseAdmin admin = new HBaseAdmin(conf);",
                "deletions": 2
            },
            {
                "sha": "abd7d7966e67be29906b81b46d70e361035c39f7",
                "filename": "src/java/org/apache/hadoop/hbase/shell/ExitCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ExitCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ExitCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ExitCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -21,14 +21,14 @@\n \n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n \n public class ExitCommand extends BasicCommand {\n   public ExitCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     // TOD: Is this the best way to exit?  Would be a problem if shell is run\n     // inside another program -- St.Ack 09/11/2007\n     System.exit(1);",
                "deletions": 2
            },
            {
                "sha": "a9fe413dcd64475c416afe556f7fc61006de4f71",
                "filename": "src/java/org/apache/hadoop/hbase/shell/FsCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/FsCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/FsCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/FsCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.Writer;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FsShell;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.util.ToolRunner;\n \n /**\n@@ -36,7 +36,7 @@ public FsCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     // This commmand will write the \n     FsShell shell = new FsShell();\n     try {",
                "deletions": 2
            },
            {
                "sha": "4ee582f5d456b30808b6282d60e7cc0dd88ba773",
                "filename": "src/java/org/apache/hadoop/hbase/shell/HelpCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/HelpCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/HelpCommand.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/HelpCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -52,7 +52,7 @@ public HelpCommand(final Writer o, final TableFormatter f) {\n     this.formatter = f;\n   }\n \n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     try {\n       printHelp(this.argument);\n     } catch (IOException e) {",
                "deletions": 1
            },
            {
                "sha": "e352f27ad8671543b67daf443f299ed571c01f66",
                "filename": "src/java/org/apache/hadoop/hbase/shell/InsertCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/InsertCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/InsertCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/InsertCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -23,7 +23,7 @@\n import java.io.Writer;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTable;\n import org.apache.hadoop.io.Text;\n \n@@ -40,7 +40,7 @@ public InsertCommand(Writer o) {\n     super(o);\n   }\n \n-  public ReturnMsg execute(Configuration conf) {\n+  public ReturnMsg execute(HBaseConfiguration conf) {\n     if (this.tableName == null || this.values == null || this.rowKey == null)\n       return new ReturnMsg(0, \"Syntax error : Please check 'Insert' syntax.\");\n ",
                "deletions": 2
            },
            {
                "sha": "fa621871d3efff0b7efbe58b430530c22894bddb",
                "filename": "src/java/org/apache/hadoop/hbase/shell/JarCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/JarCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/JarCommand.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/JarCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -35,6 +35,7 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.util.RunJar;\n \n /**\n@@ -48,7 +49,7 @@ public JarCommand(Writer o) {\n   }\n \n   @SuppressWarnings(\"deprecation\")\n-  public ReturnMsg execute(@SuppressWarnings(\"unused\") Configuration conf) {\n+  public ReturnMsg execute(@SuppressWarnings(\"unused\") HBaseConfiguration conf) {\n     \n     try {\n       String[] args = getQuery();",
                "deletions": 1
            },
            {
                "sha": "9465749f67a10e2f39ae28b9aafd986563427761",
                "filename": "src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ReturnMsg.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -19,9 +19,11 @@\n  */\n package org.apache.hadoop.hbase.shell;\n \n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+\n /**\n  * Message returned when a {@link Command} is\n- * {@link Command#execute(org.apache.hadoop.conf.Configuration)}'ed.\n+ * {@link Command#execute(HBaseConfiguration)}'ed.\n  */\n public class ReturnMsg {\n   private final String msg;",
                "deletions": 1
            },
            {
                "sha": "37558b4234f3156e9761b68edbde606a56272a02",
                "filename": "src/java/org/apache/hadoop/hbase/shell/SelectCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/SelectCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/SelectCommand.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/SelectCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -28,7 +28,6 @@\n import java.util.Map;\n import java.util.TreeMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HConstants;\n@@ -75,7 +74,7 @@ public SelectCommand(final Writer o, final TableFormatter f) {\n     this.formatter = f;\n   }\n \n-  public ReturnMsg execute(final Configuration conf) {\n+  public ReturnMsg execute(final HBaseConfiguration conf) {\n     if (this.tableName.equals(\"\") || this.rowKey == null ||\n         this.columns.size() == 0) {\n       return new ReturnMsg(0, \"Syntax error : Please check 'Select' syntax.\");",
                "deletions": 2
            },
            {
                "sha": "b4da0b85549ccba1706a653e2a834abb4d0b5011",
                "filename": "src/java/org/apache/hadoop/hbase/shell/ShowCommand.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ShowCommand.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/java/org/apache/hadoop/hbase/shell/ShowCommand.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/shell/ShowCommand.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -22,8 +22,8 @@\n import java.io.IOException;\n import java.io.Writer;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HTableDescriptor;\n \n /**\n@@ -51,7 +51,7 @@ public ShowCommand(final Writer o, final TableFormatter f,\n     this.command = argument;\n   }\n \n-  public ReturnMsg execute(final Configuration conf) {\n+  public ReturnMsg execute(final HBaseConfiguration conf) {\n     if (this.command == null) {\n       return new ReturnMsg(0, \"Syntax error : Please check 'Show' syntax\");\n     }",
                "deletions": 2
            },
            {
                "sha": "9a263b9ca03c46daace6d7c89f1d16faab053aa9",
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -51,7 +51,7 @@\n     StaticTestEnvironment.initialize();\n   }\n   \n-  protected volatile Configuration conf;\n+  protected volatile HBaseConfiguration conf;\n \n   /**\n    * constructor",
                "deletions": 1
            },
            {
                "sha": "272f12685e2cc00814b5d41a6151ec89cf395ab4",
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "status": "modified",
                "changes": 11,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -23,7 +23,6 @@\n import java.io.IOException;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -37,7 +36,7 @@\n   static final Logger LOG =\n     Logger.getLogger(MiniHBaseCluster.class.getName());\n   \n-  private Configuration conf;\n+  private HBaseConfiguration conf;\n   private MiniDFSCluster cluster;\n   private FileSystem fs;\n   private boolean shutdownDFS;\n@@ -52,7 +51,7 @@\n    * @param nRegionNodes\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes)\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes)\n   throws IOException {\n     this(conf, nRegionNodes, true, true, true);\n   }\n@@ -66,7 +65,7 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes)\n    * @param miniHdfsFilesystem\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n       final boolean miniHdfsFilesystem) throws IOException {\n     this(conf, nRegionNodes, miniHdfsFilesystem, true, true);\n   }\n@@ -89,7 +88,7 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n    * @param dfsCluster\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n       MiniDFSCluster dfsCluster) throws IOException {\n \n     this.conf = conf;\n@@ -110,7 +109,7 @@ public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n    * @param deleteOnExit clean up mini hdfs files\n    * @throws IOException\n    */\n-  public MiniHBaseCluster(Configuration conf, int nRegionNodes,\n+  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n       final boolean miniHdfsFilesystem, boolean format, boolean deleteOnExit)\n     throws IOException {\n ",
                "deletions": 6
            },
            {
                "sha": "6e6f21421743c18c6f4ed01659cdae60b405b980",
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -27,7 +27,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.util.Writables;\n@@ -52,7 +51,7 @@\n    * @throws IOException\n    */\n   @SuppressWarnings(\"null\")\n-  public static void makeMultiRegionTable(Configuration conf,\n+  public static void makeMultiRegionTable(HBaseConfiguration conf,\n       MiniHBaseCluster cluster, FileSystem localFs, String tableName,\n       String columnName)\n   throws IOException {  ",
                "deletions": 2
            },
            {
                "sha": "67bc573ecc45dd2092238d6b6bd1608a79bf868a",
                "filename": "src/test/org/apache/hadoop/hbase/OOMEHMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMEHMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMEHMaster.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/OOMEHMaster.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -23,7 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n \n /**\n@@ -35,11 +34,11 @@\n public class OOMEHMaster extends HMaster {\n   private List<byte []> retainer = new ArrayList<byte[]>();\n   \n-  public OOMEHMaster(Configuration conf) throws IOException {\n+  public OOMEHMaster(HBaseConfiguration conf) throws IOException {\n     super(conf);\n   }\n \n-  public OOMEHMaster(Path dir, HServerAddress address, Configuration conf)\n+  public OOMEHMaster(Path dir, HServerAddress address, HBaseConfiguration conf)\n       throws IOException {\n     super(dir, address, conf);\n   }",
                "deletions": 3
            },
            {
                "sha": "638a3beb522bec8a91d2c7b8f86608391dd2db40",
                "filename": "src/test/org/apache/hadoop/hbase/OOMERegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMERegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/OOMERegionServer.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/OOMERegionServer.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -23,7 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.io.Text;\n \n@@ -37,11 +36,11 @@\n public class OOMERegionServer extends HRegionServer {\n   private List<BatchUpdate> retainer = new ArrayList<BatchUpdate>();\n \n-  public OOMERegionServer(Configuration conf) throws IOException {\n+  public OOMERegionServer(HBaseConfiguration conf) throws IOException {\n     super(conf);\n   }\n \n-  public OOMERegionServer(HServerAddress address, Configuration conf)\n+  public OOMERegionServer(HServerAddress address, HBaseConfiguration conf)\n   throws IOException {\n     super(address, conf);\n   }",
                "deletions": 3
            },
            {
                "sha": "4abc871aed1095e3b1231ea2715efd9cd6a93407",
                "filename": "src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 20,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -103,7 +103,7 @@\n       SEQUENTIAL_WRITE,\n       SCAN});\n   \n-  volatile Configuration conf;\n+  volatile HBaseConfiguration conf;\n   private boolean miniCluster = false;\n   private int N = 1;\n   private int R = ROWS_PER_GB;\n@@ -131,7 +131,7 @@\n    * Constructor\n    * @param c Configuration object\n    */\n-  public PerformanceEvaluation(final Configuration c) {\n+  public PerformanceEvaluation(final HBaseConfiguration c) {\n     this.conf = c;\n   }\n   \n@@ -163,7 +163,7 @@ public PerformanceEvaluation(final Configuration c) {\n     public void configure(JobConf j) {\n       this.cmd = j.get(CMD_KEY);\n \n-      this.pe = new PerformanceEvaluation(j);\n+      this.pe = new PerformanceEvaluation(new HBaseConfiguration(j));\n     }\n     \n     /** {@inheritDoc} */\n@@ -292,9 +292,9 @@ private Path writeInputFile(final Configuration c) throws IOException {\n     private final Status status;\n     protected HBaseAdmin admin;\n     protected HTable table;\n-    protected volatile Configuration conf;\n+    protected volatile HBaseConfiguration conf;\n     \n-    Test(final Configuration conf, final int startRow,\n+    Test(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super();\n       this.startRow = startRow;\n@@ -383,7 +383,7 @@ Text getRandomRow() {\n   }\n   \n   class RandomReadTest extends Test {\n-    RandomReadTest(final Configuration conf, final int startRow,\n+    RandomReadTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -406,7 +406,7 @@ String getTestName() {\n   }\n   \n   class RandomWriteTest extends Test {\n-    RandomWriteTest(final Configuration conf, final int startRow,\n+    RandomWriteTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -430,7 +430,7 @@ String getTestName() {\n     private HStoreKey key = new HStoreKey();\n     private TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n     \n-    ScanTest(final Configuration conf, final int startRow,\n+    ScanTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -464,7 +464,7 @@ String getTestName() {\n   }\n   \n   class SequentialReadTest extends Test {\n-    SequentialReadTest(final Configuration conf, final int startRow,\n+    SequentialReadTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }\n@@ -481,7 +481,7 @@ String getTestName() {\n   }\n   \n   class SequentialWriteTest extends Test {\n-    SequentialWriteTest(final Configuration conf, final int startRow,\n+    SequentialWriteTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n       super(conf, startRow, perClientRunRows, totalRows, status);\n     }",
                "deletions": 10
            },
            {
                "sha": "0fce6de5bcf761fe3c9dbda076fdfbe8fcf0024f",
                "filename": "src/test/org/apache/hadoop/hbase/TestScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/TestScanner.java",
                "status": "modified",
                "changes": 6,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestScanner.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -24,13 +24,11 @@\n import java.io.IOException;\n import java.util.TreeMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.io.Text;\n-\n import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.io.Text;\n \n /**\n  * Test of a long-lived scanner validating as we go.\n@@ -135,7 +133,7 @@ public void testScanner() throws IOException {\n       \n       // Initialization\n       \n-      Configuration conf = new HBaseConfiguration();\n+      HBaseConfiguration conf = new HBaseConfiguration();\n       cluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n       fs = cluster.getFileSystem();\n       Path dir = new Path(\"/hbase\");",
                "deletions": 4
            },
            {
                "sha": "b3e8a8ea3c7b13229f19440c70d3825d88fc707d",
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -30,11 +30,11 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HBaseTestCase;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n@@ -211,7 +211,8 @@ private String createIndexConfContent() {\n     return c.toString();\n   }\n \n-  private void scanTable(Configuration c, long firstK) throws IOException {\n+  private void scanTable(HBaseConfiguration c, long firstK)\n+  throws IOException {\n     HTable table = new HTable(c, new Text(TABLE_NAME));\n     Text[] columns = { TEXT_INPUT_COLUMN, TEXT_OUTPUT_COLUMN };\n     HScannerInterface scanner = table.obtainScanner(columns,\n@@ -235,7 +236,7 @@ private void scanTable(Configuration c, long firstK) throws IOException {\n     }\n   }\n \n-  private void verify(Configuration c) throws IOException {\n+  private void verify(HBaseConfiguration c) throws IOException {\n     Path localDir = new Path(this.testDir, \"index_\" +\n       Integer.toString(new Random().nextInt()));\n     this.fs.copyToLocalFile(new Path(INDEX_DIR), localDir);",
                "deletions": 3
            },
            {
                "sha": "448d4262796312a4ca8996d1f02085ce833ccce5",
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "blob_url": "https://github.com/apache/hbase/blob/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "raw_url": "https://github.com/apache/hbase/raw/9be624fc933db5f47af82652fc0d8bf6a6db6398/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "status": "modified",
                "changes": 23,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java?ref=9be624fc933db5f47af82652fc0d8bf6a6db6398",
                "patch": "@@ -26,17 +26,11 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.io.MapWritable;\n-import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.mapred.JobClient;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.MiniMRCluster;\n-import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.hbase.HBaseAdmin;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HScannerInterface;\n@@ -46,10 +40,12 @@\n import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.MultiRegionTable;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n-import org.apache.hadoop.hbase.mapred.TableMap;\n-import org.apache.hadoop.hbase.mapred.TableOutputCollector;\n-import org.apache.hadoop.hbase.mapred.TableReduce;\n-import org.apache.hadoop.hbase.mapred.IdentityTableReduce;\n+import org.apache.hadoop.io.MapWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.MiniMRCluster;\n+import org.apache.hadoop.mapred.Reporter;\n \n /**\n  * Test Map/Reduce job over HBase tables\n@@ -314,7 +310,7 @@ private void localTestMultiRegionTable() throws IOException {\n     verify(conf, MULTI_REGION_TABLE_NAME);\n   }\n \n-  private void scanTable(Configuration conf, String tableName)\n+  private void scanTable(HBaseConfiguration conf, String tableName)\n   throws IOException {\n     HTable table = new HTable(conf, new Text(tableName));\n     \n@@ -344,7 +340,8 @@ private void scanTable(Configuration conf, String tableName)\n   }\n \n   @SuppressWarnings(\"null\")\n-  private void verify(Configuration conf, String tableName) throws IOException {\n+  private void verify(HBaseConfiguration conf, String tableName)\n+  throws IOException {\n     HTable table = new HTable(conf, new Text(tableName));\n     \n     Text[] columns = {",
                "deletions": 13
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-2064 TestSplit assertion and NPE failures (Patch build #952 and #953)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@585293 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/0b92e8826afb00476e98837f35ba083fa164cc69",
        "parent": "https://github.com/apache/hbase/commit/04cac52745075093f9b5052e46950f76c8aeafdf",
        "bug_id": "hbase_314",
        "file": [
            {
                "sha": "3baafcb06e35cc7de0fe4f63ad0d9caad892d652",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/0b92e8826afb00476e98837f35ba083fa164cc69/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/0b92e8826afb00476e98837f35ba083fa164cc69/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=0b92e8826afb00476e98837f35ba083fa164cc69",
                "patch": "@@ -12,6 +12,7 @@ Trunk (unreleased changes)\n   BUG FIXES\n    HADOOP-2059 In tests, exceptions in min dfs shutdown should not fail test\n                (e.g. nightly #272)\n+   HADOOP-2064 TestSplit assertion and NPE failures (Patch build #952 and #953)\n \n   IMPROVEMENTS\n     HADOOP-2401 Add convenience put method that takes writable",
                "deletions": 0
            },
            {
                "sha": "117c4848c3e42f289c45e23f8b0332fd07dc6841",
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/0b92e8826afb00476e98837f35ba083fa164cc69/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/0b92e8826afb00476e98837f35ba083fa164cc69/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=0b92e8826afb00476e98837f35ba083fa164cc69",
                "patch": "@@ -51,8 +51,8 @@\n   private boolean shutdownDFS;\n   private Path parentdir;\n   private MasterThread masterThread = null;\n-  ArrayList<RegionServerThread> regionThreads =\n-    new ArrayList<RegionServerThread>();\n+  List<RegionServerThread> regionThreads =\n+    java.util.Collections.synchronizedList(new ArrayList<RegionServerThread>());\n   private boolean deleteOnExit = true;\n \n   /**\n@@ -460,7 +460,7 @@ void flushcache() throws IOException {\n     }\n   }\n \n-  public ArrayList<RegionServerThread> getRegionThreads() {\n+  public List<RegionServerThread> getRegionThreads() {\n     return this.regionThreads;\n   }\n }",
                "deletions": 3
            },
            {
                "sha": "2871dd9e1d3f30b29fa1b9fc356419460ad2424f",
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/0b92e8826afb00476e98837f35ba083fa164cc69/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/0b92e8826afb00476e98837f35ba083fa164cc69/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "status": "modified",
                "changes": 36,
                "additions": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=0b92e8826afb00476e98837f35ba083fa164cc69",
                "patch": "@@ -40,10 +40,10 @@\n   static final Log LOG = LogFactory.getLog(MultiRegionTable.class.getName());\n \n   /**\n-   * Make a multi-region table.  Presumption is that table already exists.\n-   * Makes it multi-region by filling with data and provoking splits.\n-   * Asserts parent region is cleaned up after its daughter splits release all\n-   * references.\n+   * Make a multi-region table.  Presumption is that table already exists and\n+   * that there is only one regionserver. Makes it multi-region by filling with\n+   * data and provoking splits. Asserts parent region is cleaned up after its\n+   * daughter splits release all references.\n    * @param conf\n    * @param cluster\n    * @param localFs\n@@ -75,14 +75,28 @@ public static void makeMultiRegionTable(Configuration conf,\n     int count = count(meta, tableName);\n     HTable t = new HTable(conf, new Text(tableName));\n     addContent(new HTableIncommon(t), columnName);\n+    LOG.info(\"Finished content loading\");\n     \n     // All is running in the one JVM so I should be able to get the single\n     // region instance and bring on a split.\n-    HRegionInfo hri =\n-      t.getRegionLocation(HConstants.EMPTY_START_ROW).getRegionInfo();\n-    HRegion r = cluster.regionThreads.get(0).getRegionServer().\n-      onlineRegions.get(hri.getRegionName());\n-    \n+    // Presumption is that there is only one regionserver.\n+    HRegionInfo hri = null;\n+    HRegion r = null;\n+    for (int i = 0; i < 30; i++) {\n+      hri = t.getRegionLocation(HConstants.EMPTY_START_ROW).getRegionInfo();\n+      LOG.info(\"Region location: \" + hri);\n+      r = cluster.getRegionThreads().get(0).getRegionServer().\n+          onlineRegions.get(hri.getRegionName());\n+      if (r != null) {\n+        break;\n+      }\n+      try {\n+        Thread.sleep(1000);\n+      } catch (InterruptedException e) {\n+        LOG.warn(\"Waiting on region to come online\", e);\n+      }\n+    }\n+\n     // Flush will provoke a split next time the split-checker thread runs.\n     r.flushcache(false);\n     \n@@ -109,6 +123,7 @@ public static void makeMultiRegionTable(Configuration conf,\n     Map<Text, byte []> data = getSplitParentInfo(meta, hri);\n     HRegionInfo parent =\n       Writables.getHRegionInfoOrNull(data.get(HConstants.COL_REGIONINFO));\n+    LOG.info(\"Found parent region: \" + parent);\n     assertTrue(parent.isOffline());\n     assertTrue(parent.isSplit());\n     HRegionInfo splitA =\n@@ -227,7 +242,8 @@ private static int count(final HTable t, final String tableName)\n         }\n         // Make sure I get the parent.\n         if (hri.getRegionName().toString().\n-            equals(parent.getRegionName().toString())) {\n+            equals(parent.getRegionName().toString()) &&\n+              hri.getRegionId() == parent.getRegionId()) {\n           return curVals;\n         }\n       }",
                "deletions": 10
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1941 StopRowFilter throws NPE when passed null row\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@579410 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
        "parent": "https://github.com/apache/hbase/commit/87f305b608f7449ba2b622d764dbb80ab870602b",
        "bug_id": "hbase_315",
        "file": [
            {
                "sha": "291671e0d8fe0b29a0d240fabbb0de7fd6bc6c69",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
                "patch": "@@ -61,6 +61,7 @@ Trunk (unreleased changes)\n     HADOOP-1923, HADOOP-1924 a) tests fail sporadically because set up and tear\n                  down is inconsistent b) TestDFSAbort failed in nightly #242\n     HADOOP-1929 Add hbase-default.xml to hbase jar\n+    HADOOP-1941 StopRowFilter throws NPE when passed null row\n \n   IMPROVEMENTS\n     HADOOP-1737 Make HColumnDescriptor data publically members settable",
                "deletions": 0
            },
            {
                "sha": "be638140097e79fb09007b6e1d63e99747234140",
                "filename": "src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java",
                "blob_url": "https://github.com/apache/hbase/blob/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java",
                "raw_url": "https://github.com/apache/hbase/raw/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java",
                "status": "modified",
                "changes": 6,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/filter/StopRowFilter.java?ref=ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
                "patch": "@@ -98,6 +98,12 @@ public boolean filterAllRemaining() {\n \n   /** {@inheritDoc} */\n   public boolean filter(final Text rowKey) {\n+    if (rowKey == null) {\n+      if (this.stopRowKey == null) {\n+        return true;\n+      }\n+      return false;\n+    }\n     boolean result = this.stopRowKey.compareTo(rowKey) <= 0;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Filter result for rowKey: \" + rowKey + \".  Result: \" + ",
                "deletions": 0
            },
            {
                "sha": "dc5b867741349928ae5ed1cd8f5d35116e2c8b62",
                "filename": "src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "blob_url": "https://github.com/apache/hbase/blob/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "raw_url": "https://github.com/apache/hbase/raw/ccf42acf70609e8086c3f46b5c18ca8f6c13117d/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "status": "modified",
                "changes": 2,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java?ref=ccf42acf70609e8086c3f46b5c18ca8f6c13117d",
                "patch": "@@ -87,5 +87,7 @@ private void stopRowTests(RowFilterInterface filter) throws Exception {\n \n     assertFalse(\"FilterAllRemaining\", filter.filterAllRemaining());\n     assertFalse(\"FilterNotNull\", filter.filterNotNull(null));\n+    \n+    assertFalse(\"Filter a null\", filter.filter(null));\n   }\n }",
                "deletions": 0
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1479 Fix NPE in HStore#get if store file only has keys < passed key.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@546275 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
        "parent": "https://github.com/apache/hbase/commit/09cf0a100f19517653806511b6b64e02fe5d31f8",
        "bug_id": "hbase_316",
        "file": [
            {
                "sha": "2c20370f465fbbfe06d2c4b7bfa76d29ea995c2e",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
                "patch": "@@ -28,3 +28,4 @@ Trunk (unreleased changes)\n  15. HADOOP-1421 Failover detection, split log files.\n      For the files modified, also clean up javadoc, class, field and method \n      visibility (HADOOP-1466)\n+ 16. HADOOP-1479 Fix NPE in HStore#get if store file only has keys < passed key.",
                "deletions": 0
            },
            {
                "sha": "a627bede8d36a81ec8ef42d941fe41b294a7e783",
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/src/java/org/apache/hadoop/hbase/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/3f5229c66f93e6e36c36cd6793a0daefcadb55ca/src/java/org/apache/hadoop/hbase/HStore.java",
                "status": "modified",
                "changes": 16,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=3f5229c66f93e6e36c36cd6793a0daefcadb55ca",
                "patch": "@@ -835,7 +835,7 @@ void getFull(HStoreKey key, TreeMap<Text, BytesWritable> results) throws IOExcep\n    * If 'numVersions' is negative, the method returns all available versions.\n    */\n   BytesWritable[] get(HStoreKey key, int numVersions) throws IOException {\n-    if(numVersions <= 0) {\n+    if (numVersions <= 0) {\n       throw new IllegalArgumentException(\"Number of versions must be > 0\");\n     }\n     \n@@ -852,15 +852,19 @@ void getFull(HStoreKey key, TreeMap<Text, BytesWritable> results) throws IOExcep\n           BytesWritable readval = new BytesWritable();\n           map.reset();\n           HStoreKey readkey = (HStoreKey)map.getClosest(key, readval);\n-          \n-          if(readkey.matchesRowCol(key)) {\n+          if (readkey == null) {\n+            // map.getClosest returns null if the passed key is > than the\n+            // last key in the map file.  getClosest is a bit of a misnomer\n+            // since it returns exact match or the next closest key AFTER not\n+            // BEFORE.\n+            continue;\n+          }\n+          if (readkey.matchesRowCol(key)) {\n             results.add(readval);\n             readval = new BytesWritable();\n-\n             while(map.next(readkey, readval) && readkey.matchesRowCol(key)) {\n-              if(numVersions > 0 && (results.size() >= numVersions)) {\n+              if (numVersions > 0 && (results.size() >= numVersions)) {\n                 break;\n-                \n               }\n               results.add(readval);\n               readval = new BytesWritable();",
                "deletions": 6
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-16317 revert all ESAPI changes\n\nRevert \"HBASE-15270 Use appropriate encoding for \"filter\" field in TaskMonitorTmpl.jamon.\"\n\nThis reverts commit bba4f107c19b92eb51c7772eddb408397bea3002.\n\nRevert \"HBASE-15122 Servlets generate XSS_REQUEST_PARAMETER_TO_SERVLET_WRITER findbugs warnings (Samir Ahmic)\"\n\nThis reverts commit 68b300173f82b2b3ae06da1ec303a8f6f072c414.\n\n Conflicts:\n\thbase-server/pom.xml\n\nRevert \"HBASE-15329 Cross-Site Scripting: Reflected in table.jsp (Samir Ahmic)\"\n\nThis reverts commit 4b3e38705cb24aee82615b1b9af47ed549ea1358.\n\n Conflicts:\n\thbase-server/src/main/resources/hbase-webapps/master/table.jsp\n\nRevert \"HBASE-15369 Handle NPE in region.jsp (Samir Ahmic)\"\n\nThis reverts commit 3826894f890a850270053a25b53f07a007555711.",
        "commit": "https://github.com/apache/hbase/commit/209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
        "parent": "https://github.com/apache/hbase/commit/81b06b3bdd56c1c434406fe3e3aea7e47948f958",
        "bug_id": "hbase_317",
        "file": [
            {
                "sha": "2f94226400e8f9223ee7ceb9902b6d4e95cae66e",
                "filename": "hbase-resource-bundle/src/main/resources/supplemental-models.xml",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-resource-bundle/src/main/resources/supplemental-models.xml",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-resource-bundle/src/main/resources/supplemental-models.xml",
                "status": "modified",
                "changes": 36,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-resource-bundle/src/main/resources/supplemental-models.xml?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -61,24 +61,6 @@ under the License.\n       </licenses>\n     </project>\n   </supplement>\n-  <supplement>\n-    <project>\n-      <groupId>commons-beanutils</groupId>\n-      <artifactId>commons-beanutils-core</artifactId>\n-\n-      <organization>\n-        <name>The Apache Software Foundation</name>\n-        <url>http://www.apache.org/</url>\n-      </organization>\n-      <licenses>\n-        <license>\n-          <name>Apache Software License, Version 2.0</name>\n-          <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>\n-          <distribution>repo</distribution>\n-        </license>\n-      </licenses>\n-    </project>\n-  </supplement>\n <!-- Artifacts with ambiguously named licenses in POM -->\n   <supplement>\n     <project>\n@@ -1213,22 +1195,4 @@ Copyright (c) 2007-2011 The JRuby project\n       </licenses>\n     </project>\n   </supplement>\n-  <supplement>\n-    <project>\n-      <groupId>xalan</groupId>\n-      <artifactId>xalan</artifactId>\n-\n-      <organization>\n-        <name>The Apache Software Foundation</name>\n-        <url>http://www.apache.org/</url>\n-      </organization>\n-      <licenses>\n-        <license>\n-          <name>The Apache Software License, Version 2.0</name>\n-          <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>\n-          <distribution>repo</distribution>\n-        </license>\n-      </licenses>\n-    </project>\n-  </supplement>\n </supplementalDataModels>",
                "deletions": 36
            },
            {
                "sha": "ff001b75fe248b6252d106b9c7a88fc1581f2b91",
                "filename": "hbase-server/pom.xml",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/pom.xml",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/pom.xml",
                "status": "modified",
                "changes": 11,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/pom.xml?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -561,17 +561,6 @@\n       <artifactId>bcprov-jdk16</artifactId>\n       <scope>test</scope>\n     </dependency>\n-    <dependency>\n-      <groupId>org.owasp.esapi</groupId>\n-      <artifactId>esapi</artifactId>\n-      <version>2.1.0.1</version>\n-      <exclusions>\n-        <exclusion>\n-          <artifactId>xercesImpl</artifactId>\n-          <groupId>xerces</groupId>\n-        </exclusion>\n-      </exclusions>\n-    </dependency>\n     <dependency>\n       <groupId>org.apache.kerby</groupId>\n       <artifactId>kerb-client</artifactId>",
                "deletions": 11
            },
            {
                "sha": "b4a5feae4567d740c99a5d2f0e27a832f2d35860",
                "filename": "hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon",
                "status": "modified",
                "changes": 12,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -20,22 +20,12 @@ limitations under the License.\n java.util.*;\n org.apache.hadoop.hbase.monitoring.*;\n org.apache.hadoop.util.StringUtils;\n-org.owasp.esapi.ESAPI;\n-org.owasp.esapi.errors.EncodingException;\n </%import>\n <%args>\n TaskMonitor taskMonitor = TaskMonitor.get();\n String filter = \"general\";\n String format = \"html\";\n </%args>\n-<%class>\n-    public String encodeFilter() {\n-    try {\n-    return ESAPI.encoder().encodeForURL(filter);\n-    }catch(EncodingException e) {}\n-    return ESAPI.encoder().encodeForHTML(filter);\n-    }\n-</%class>\n <%java>\n List<? extends MonitoredTask> tasks = taskMonitor.getTasks();\n Iterator<? extends MonitoredTask> iter = tasks.iterator();\n@@ -72,7 +62,7 @@ boolean first = true;\n     <li <%if filter.equals(\"handler\")%>class=\"active\"</%if>><a href=\"?filter=handler\">Show All RPC Handler Tasks</a></li>\n     <li <%if filter.equals(\"rpc\")%>class=\"active\"</%if>><a href=\"?filter=rpc\">Show Active RPC Calls</a></li>\n     <li <%if filter.equals(\"operation\")%>class=\"active\"</%if>><a href=\"?filter=operation\">Show Client Operations</a></li>\n-    <li><a href=\"?format=json&filter=<% encodeFilter() %>\">View as JSON</a></li>\n+    <li><a href=\"?format=json&filter=<% filter %>\">View as JSON</a></li>\n   </ul>\n   <%if tasks.isEmpty()%>\n     <p>No tasks currently running on this node.</p>",
                "deletions": 11
            },
            {
                "sha": "45c2c1515610348f814dfd7c4a97d28b7504a02c",
                "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java",
                "status": "modified",
                "changes": 8,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/http/jmx/JMXJsonServlet.java?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.http.HttpServer;\n import org.apache.hadoop.hbase.util.JSONBean;\n-import org.owasp.esapi.ESAPI;\n \n /*\n  * This servlet is based off of the JMXProxyServlet from Tomcat 7.0.14. It has\n@@ -168,7 +167,7 @@ public void doGet(HttpServletRequest request, HttpServletResponse response) {\n         jsonpcb = request.getParameter(CALLBACK_PARAM);\n         if (jsonpcb != null) {\n           response.setContentType(\"application/javascript; charset=utf8\");\n-          writer.write(encodeJS(jsonpcb) + \"(\");\n+          writer.write(jsonpcb + \"(\");\n         } else {\n           response.setContentType(\"application/json; charset=utf8\");\n         }\n@@ -221,9 +220,4 @@ public void doGet(HttpServletRequest request, HttpServletResponse response) {\n       response.setStatus(HttpServletResponse.SC_BAD_REQUEST);\n     }\n   }\n-\n-  private String encodeJS(String inputStr) {\n-    return ESAPI.encoder().encodeForJavaScript(inputStr);\n-  }\n-\n }",
                "deletions": 7
            },
            {
                "sha": "907400189d8dd33f34b03469af8fc1f250844cf8",
                "filename": "hbase-server/src/main/resources/ESAPI.properties",
                "blob_url": "https://github.com/apache/hbase/blob/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/main/resources/ESAPI.properties",
                "raw_url": "https://github.com/apache/hbase/raw/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/main/resources/ESAPI.properties",
                "status": "removed",
                "changes": 431,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/ESAPI.properties?ref=81b06b3bdd56c1c434406fe3e3aea7e47948f958",
                "patch": "@@ -1,431 +0,0 @@\n-#\n-# OWASP Enterprise Security API (ESAPI) Properties file -- PRODUCTION Version\n-#\n-# This file is part of the Open Web Application Security Project (OWASP)\n-# Enterprise Security API (ESAPI) project. For details, please see\n-# http://www.owasp.org/index.php/ESAPI.\n-#\n-# Copyright (c) 2008,2009 - The OWASP Foundation\n-#\n-# DISCUSS: This may cause a major backwards compatibility issue, etc. but\n-#           from a name space perspective, we probably should have prefaced\n-#           all the property names with ESAPI or at least OWASP. Otherwise\n-#           there could be problems is someone loads this properties file into\n-#           the System properties.  We could also put this file into the\n-#           esapi.jar file (perhaps as a ResourceBundle) and then allow an external\n-#           ESAPI properties be defined that would overwrite these defaults.\n-#           That keeps the application's properties relatively simple as usually\n-#           they will only want to override a few properties. If looks like we\n-#           already support multiple override levels of this in the\n-#           DefaultSecurityConfiguration class, but I'm suggesting placing the\n-#           defaults in the esapi.jar itself. That way, if the jar is signed,\n-#           we could detect if those properties had been tampered with. (The\n-#           code to check the jar signatures is pretty simple... maybe 70-90 LOC,\n-#           but off course there is an execution penalty (similar to the way\n-#           that the separate sunjce.jar used to be when a class from it was\n-#           first loaded). Thoughts?\n-###############################################################################\n-#\n-# WARNING: Operating system protection should be used to lock down the .esapi\n-# resources directory and all the files inside and all the directories all the\n-# way up to the root directory of the file system.  Note that if you are using\n-# file-based implementations, that some files may need to be read-write as they\n-# get updated dynamically.\n-#\n-# Before using, be sure to update the MasterKey and MasterSalt as described below.\n-# N.B.: If you had stored data that you have previously encrypted with ESAPI 1.4,\n-#        you *must* FIRST decrypt it using ESAPI 1.4 and then (if so desired)\n-#        re-encrypt it with ESAPI 2.0. If you fail to do this, you will NOT be\n-#        able to decrypt your data with ESAPI 2.0.\n-#\n-#        YOU HAVE BEEN WARNED!!! More details are in the ESAPI 2.0 Release Notes.\n-#\n-#===========================================================================\n-# ESAPI Configuration\n-#\n-# If true, then print all the ESAPI properties set here when they are loaded.\n-# If false, they are not printed. Useful to reduce output when running JUnit tests.\n-# If you need to troubleshoot a properties related problem, turning this on may help.\n-# This is 'false' in the src/test/resources/.esapi version. It is 'true' by\n-# default for reasons of backward compatibility with earlier ESAPI versions.\n-ESAPI.printProperties=true\n-\n-# ESAPI is designed to be easily extensible. You can use the reference implementation\n-# or implement your own providers to take advantage of your enterprise's security\n-# infrastructure. The functions in ESAPI are referenced using the ESAPI locator, like:\n-#\n-#    String ciphertext =\n-#        ESAPI.encryptor().encrypt(\"Secret message\");   // Deprecated in 2.0\n-#    CipherText cipherText =\n-#        ESAPI.encryptor().encrypt(new PlainText(\"Secret message\")); // Preferred\n-#\n-# Below you can specify the classname for the provider that you wish to use in your\n-# application. The only requirement is that it implement the appropriate ESAPI interface.\n-# This allows you to switch security implementations in the future without rewriting the\n-# entire application.\n-#\n-# ExperimentalAccessController requires ESAPI-AccessControlPolicy.xml in .esapi directory\n-ESAPI.AccessControl=org.owasp.esapi.reference.DefaultAccessController\n-# FileBasedAuthenticator requires users.txt file in .esapi directory\n-ESAPI.Authenticator=org.owasp.esapi.reference.FileBasedAuthenticator\n-ESAPI.Encoder=org.owasp.esapi.reference.DefaultEncoder\n-ESAPI.Encryptor=org.owasp.esapi.reference.crypto.JavaEncryptor\n-\n-ESAPI.Executor=org.owasp.esapi.reference.DefaultExecutor\n-ESAPI.HTTPUtilities=org.owasp.esapi.reference.DefaultHTTPUtilities\n-ESAPI.IntrusionDetector=org.owasp.esapi.reference.DefaultIntrusionDetector\n-# Log4JFactory Requires log4j.xml or log4j.properties in classpath - http://www.laliluna.de/log4j-tutorial.html\n-ESAPI.Logger=org.owasp.esapi.reference.Log4JLogFactory\n-#ESAPI.Logger=org.owasp.esapi.reference.JavaLogFactory\n-ESAPI.Randomizer=org.owasp.esapi.reference.DefaultRandomizer\n-ESAPI.Validator=org.owasp.esapi.reference.DefaultValidator\n-\n-#===========================================================================\n-# ESAPI Authenticator\n-#\n-Authenticator.AllowedLoginAttempts=3\n-Authenticator.MaxOldPasswordHashes=13\n-Authenticator.UsernameParameterName=username\n-Authenticator.PasswordParameterName=password\n-# RememberTokenDuration (in days)\n-Authenticator.RememberTokenDuration=14\n-# Session Timeouts (in minutes)\n-Authenticator.IdleTimeoutDuration=20\n-Authenticator.AbsoluteTimeoutDuration=120\n-\n-#===========================================================================\n-# ESAPI Encoder\n-#\n-# ESAPI canonicalizes input before validation to prevent bypassing filters with encoded attacks.\n-# Failure to canonicalize input is a very common mistake when implementing validation schemes.\n-# Canonicalization is automatic when using the ESAPI Validator, but you can also use the\n-# following code to canonicalize data.\n-#\n-#      ESAPI.Encoder().canonicalize( \"%22hello world&#x22;\" );\n-#\n-# Multiple encoding is when a single encoding format is applied multiple times, multiple\n-# different encoding formats are applied, or when multiple formats are nested. Allowing\n-# multiple encoding is strongly discouraged.\n-Encoder.AllowMultipleEncoding=false\n-#\n-# The default list of codecs to apply when canonicalizing untrusted data. The list should include the codecs\n-# for all downstream interpreters or decoders. For example, if the data is likely to end up in a URL, HTML, or\n-# inside JavaScript, then the list of codecs below is appropriate. The order of the list is not terribly important.\n-Encoder.DefaultCodecList=HTMLEntityCodec,PercentCodec,JavaScriptCodec\n-\n-\n-#===========================================================================\n-# ESAPI Encryption\n-#\n-# The ESAPI Encryptor provides basic cryptographic functions with a simplified API.\n-# To get started, generate a new key using java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-# There is not currently any support for key rotation, so be careful when changing your key and salt as it\n-# will invalidate all signed, encrypted, and hashed data.\n-#\n-# WARNING: Not all combinations of algorithms and key lengths are supported.\n-# If you choose to use a key length greater than 128, you MUST download the\n-# unlimited strength policy files and install in the lib directory of your JRE/JDK.\n-# See http://java.sun.com/javase/downloads/index.jsp for more information.\n-#\n-# Backward compatibility with ESAPI Java 1.4 is supported by the two deprecated API\n-# methods, Encryptor.encrypt(String) and Encryptor.decrypt(String). However, whenever\n-# possible, these methods should be avoided as they use ECB cipher mode, which in almost\n-# all circumstances a poor choice because of it's weakness. CBC cipher mode is the default\n-# for the new Encryptor encrypt / decrypt methods for ESAPI Java 2.0.  In general, you\n-# should only use this compatibility setting if you have persistent data encrypted with\n-# version 1.4 and even then, you should ONLY set this compatibility mode UNTIL\n-# you have decrypted all of your old encrypted data and then re-encrypted it with\n-# ESAPI 2.0 using CBC mode. If you have some reason to mix the deprecated 1.4 mode\n-# with the new 2.0 methods, make sure that you use the same cipher algorithm for both\n-# (256-bit AES was the default for 1.4; 128-bit is the default for 2.0; see below for\n-# more details.) Otherwise, you will have to use the new 2.0 encrypt / decrypt methods\n-# where you can specify a SecretKey. (Note that if you are using the 256-bit AES,\n-# that requires downloading the special jurisdiction policy files mentioned above.)\n-#\n-#        ***** IMPORTANT: Do NOT forget to replace these with your own values! *****\n-# To calculate these values, you can run:\n-#        java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-#\n-Encryptor.MasterKey=\n-Encryptor.MasterSalt=\n-\n-# Provides the default JCE provider that ESAPI will \"prefer\" for its symmetric\n-# encryption and hashing. (That is it will look to this provider first, but it\n-# will defer to other providers if the requested algorithm is not implemented\n-# by this provider.) If left unset, ESAPI will just use your Java VM's current\n-# preferred JCE provider, which is generally set in the file\n-# \"$JAVA_HOME/jre/lib/security/java.security\".\n-#\n-# The main intent of this is to allow ESAPI symmetric encryption to be\n-# used with a FIPS 140-2 compliant crypto-module. For details, see the section\n-# \"Using ESAPI Symmetric Encryption with FIPS 140-2 Cryptographic Modules\" in\n-# the ESAPI 2.0 Symmetric Encryption User Guide, at:\n-# http://owasp-esapi-java.googlecode.com/svn/trunk/documentation/esapi4java-core-2.0-symmetric-crypto-user-guide.html\n-# However, this property also allows you to easily use an alternate JCE provider\n-# such as \"Bouncy Castle\" without having to make changes to \"java.security\".\n-# See Javadoc for SecurityProviderLoader for further details. If you wish to use\n-# a provider that is not known to SecurityProviderLoader, you may specify the\n-# fully-qualified class name of the JCE provider class that implements\n-# java.security.Provider. If the name contains a '.', this is interpreted as\n-# a fully-qualified class name that implements java.security.Provider.\n-#\n-# NOTE: Setting this property has the side-effect of changing it in your application\n-#       as well, so if you are using JCE in your application directly rather than\n-#       through ESAPI (you wouldn't do that, would you? ;-), it will change the\n-#       preferred JCE provider there as well.\n-#\n-# Default: Keeps the JCE provider set to whatever JVM sets it to.\n-Encryptor.PreferredJCEProvider=\n-\n-# AES is the most widely used and strongest encryption algorithm. This\n-# should agree with your Encryptor.CipherTransformation property.\n-# By default, ESAPI Java 1.4 uses \"PBEWithMD5AndDES\" and which is\n-# very weak. It is essentially a password-based encryption key, hashed\n-# with MD5 around 1K times and then encrypted with the weak DES algorithm\n-# (56-bits) using ECB mode and an unspecified padding (it is\n-# JCE provider specific, but most likely \"NoPadding\"). However, 2.0 uses\n-# \"AES/CBC/PKCSPadding\". If you want to change these, change them here.\n-# Warning: This property does not control the default reference implementation for\n-#           ESAPI 2.0 using JavaEncryptor. Also, this property will be dropped\n-#           in the future.\n-# @deprecated\n-Encryptor.EncryptionAlgorithm=AES\n-#        For ESAPI Java 2.0 - New encrypt / decrypt methods use this.\n-Encryptor.CipherTransformation=AES/CBC/PKCS5Padding\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Comma-separated list of cipher modes that provide *BOTH*\n-# confidentiality *AND* message authenticity. (NIST refers to such cipher\n-# modes as \"combined modes\" so that's what we shall call them.) If any of these\n-# cipher modes are used then no MAC is calculated and stored\n-# in the CipherText upon encryption. Likewise, if one of these\n-# cipher modes is used with decryption, no attempt will be made\n-# to validate the MAC contained in the CipherText object regardless\n-# of whether it contains one or not. Since the expectation is that\n-# these cipher modes support support message authenticity already,\n-# injecting a MAC in the CipherText object would be at best redundant.\n-#\n-# Note that as of JDK 1.5, the SunJCE provider does not support *any*\n-# of these cipher modes. Of these listed, only GCM and CCM are currently\n-# NIST approved. YMMV for other JCE providers. E.g., Bouncy Castle supports\n-# GCM and CCM with \"NoPadding\" mode, but not with \"PKCS5Padding\" or other\n-# padding modes.\n-Encryptor.cipher_modes.combined_modes=GCM,CCM,IAPM,EAX,OCB,CWC\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Additional cipher modes allowed for ESAPI 2.0 encryption. These\n-# cipher modes are in _addition_ to those specified by the property\n-# 'Encryptor.cipher_modes.combined_modes'.\n-# Note: We will add support for streaming modes like CFB & OFB once\n-# we add support for 'specified' to the property 'Encryptor.ChooseIVMethod'\n-# (probably in ESAPI 2.1).\n-# DISCUSS: Better name?\n-Encryptor.cipher_modes.additional_allowed=CBC\n-\n-# 128-bit is almost always sufficient and appears to be more resistant to\n-# related key attacks than is 256-bit AES. Use '_' to use default key size\n-# for cipher algorithms (where it makes sense because the algorithm supports\n-# a variable key size). Key length must agree to what's provided as the\n-# cipher transformation, otherwise this will be ignored after logging a\n-# warning.\n-#\n-# NOTE: This is what applies BOTH ESAPI 1.4 and 2.0. See warning above about mixing!\n-Encryptor.EncryptionKeyLength=128\n-\n-# Because 2.0 uses CBC mode by default, it requires an initialization vector (IV).\n-# (All cipher modes except ECB require an IV.) There are two choices: we can either\n-# use a fixed IV known to both parties or allow ESAPI to choose a random IV. While\n-# the IV does not need to be hidden from adversaries, it is important that the\n-# adversary not be allowed to choose it. Also, random IVs are generally much more\n-# secure than fixed IVs. (In fact, it is essential that feed-back cipher modes\n-# such as CFB and OFB use a different IV for each encryption with a given key so\n-# in such cases, random IVs are much preferred. By default, ESAPI 2.0 uses random\n-# IVs. If you wish to use 'fixed' IVs, set 'Encryptor.ChooseIVMethod=fixed' and\n-# uncomment the Encryptor.fixedIV.\n-#\n-# Valid values:        random|fixed|specified        'specified' not yet implemented; planned for 2.1\n-Encryptor.ChooseIVMethod=random\n-# If you choose to use a fixed IV, then you must place a fixed IV here that\n-# is known to all others who are sharing your secret key. The format should\n-# be a hex string that is the same length as the cipher block size for the\n-# cipher algorithm that you are using. The following is an example for AES\n-# from an AES test vector for AES-128/CBC as described in:\n-# NIST Special Publication 800-38A (2001 Edition)\n-# \"Recommendation for Block Cipher Modes of Operation\".\n-# (Note that the block size for AES is 16 bytes == 128 bits.)\n-#\n-Encryptor.fixedIV=0x000102030405060708090a0b0c0d0e0f\n-\n-# Whether or not CipherText should use a message authentication code (MAC) with it.\n-# This prevents an adversary from altering the IV as well as allowing a more\n-# fool-proof way of determining the decryption failed because of an incorrect\n-# key being supplied. This refers to the \"separate\" MAC calculated and stored\n-# in CipherText, not part of any MAC that is calculated as a result of a\n-# \"combined mode\" cipher mode.\n-#\n-# If you are using ESAPI with a FIPS 140-2 cryptographic module, you *must* also\n-# set this property to false.\n-Encryptor.CipherText.useMAC=true\n-\n-# Whether or not the PlainText object may be overwritten and then marked\n-# eligible for garbage collection. If not set, this is still treated as 'true'.\n-Encryptor.PlainText.overwrite=true\n-\n-# Do not use DES except in a legacy situations. 56-bit is way too small key size.\n-#Encryptor.EncryptionKeyLength=56\n-#Encryptor.EncryptionAlgorithm=DES\n-\n-# TripleDES is considered strong enough for most purposes.\n-#    Note:    There is also a 112-bit version of DESede. Using the 168-bit version\n-#            requires downloading the special jurisdiction policy from Sun.\n-#Encryptor.EncryptionKeyLength=168\n-#Encryptor.EncryptionAlgorithm=DESede\n-\n-Encryptor.HashAlgorithm=SHA-512\n-Encryptor.HashIterations=1024\n-Encryptor.DigitalSignatureAlgorithm=SHA1withDSA\n-Encryptor.DigitalSignatureKeyLength=1024\n-Encryptor.RandomAlgorithm=SHA1PRNG\n-Encryptor.CharacterEncoding=UTF-8\n-\n-\n-#===========================================================================\n-# ESAPI HttpUtilties\n-#\n-# The HttpUtilities provide basic protections to HTTP requests and responses. Primarily these methods\n-# protect against malicious data from attackers, such as unprintable characters, escaped characters,\n-# and other simple attacks. The HttpUtilities also provides utility methods for dealing with cookies,\n-# headers, and CSRF tokens.\n-#\n-# Default file upload location (remember to escape backslashes with \\\\)\n-HttpUtilities.UploadDir=C:\\\\ESAPI\\\\testUpload\n-HttpUtilities.UploadTempDir=C:\\\\temp\n-# Force flags on cookies, if you use HttpUtilities to set cookies\n-HttpUtilities.ForceHttpOnlySession=false\n-HttpUtilities.ForceSecureSession=false\n-HttpUtilities.ForceHttpOnlyCookies=true\n-HttpUtilities.ForceSecureCookies=true\n-# Maximum size of HTTP headers\n-HttpUtilities.MaxHeaderSize=4096\n-# File upload configuration\n-HttpUtilities.ApprovedUploadExtensions=.zip,.pdf,.doc,.docx,.ppt,.pptx,.tar,.gz,.tgz,.rar,.war,.jar,.ear,.xls,.rtf,.properties,.java,.class,.txt,.xml,.jsp,.jsf,.exe,.dll\n-HttpUtilities.MaxUploadFileBytes=500000000\n-# Using UTF-8 throughout your stack is highly recommended. That includes your database driver,\n-# container, and any other technologies you may be using. Failure to do this may expose you\n-# to Unicode transcoding injection attacks. Use of UTF-8 does not hinder internationalization.\n-HttpUtilities.ResponseContentType=text/html; charset=UTF-8\n-\n-\n-\n-#===========================================================================\n-# ESAPI Executor\n-# CHECKME - Not sure what this is used for, but surely it should be made OS independent.\n-Executor.WorkingDirectory=C:\\\\Windows\\\\Temp\n-Executor.ApprovedExecutables=C:\\\\Windows\\\\System32\\\\cmd.exe,C:\\\\Windows\\\\System32\\\\runas.exe\n-\n-\n-#===========================================================================\n-# ESAPI Logging\n-# Set the application name if these logs are combined with other applications\n-Logger.ApplicationName=ExampleApplication\n-# If you use an HTML log viewer that does not properly HTML escape log data, you can set LogEncodingRequired to true\n-Logger.LogEncodingRequired=false\n-# Determines whether ESAPI should log the application name. This might be clutter in some single-server/single-app environments.\n-Logger.LogApplicationName=true\n-# Determines whether ESAPI should log the server IP and port. This might be clutter in some single-server environments.\n-Logger.LogServerIP=true\n-# LogFileName, the name of the logging file. Provide a full directory path (e.g., C:\\\\ESAPI\\\\ESAPI_logging_file) if you\n-# want to place it in a specific directory.\n-Logger.LogFileName=ESAPI_logging_file\n-# MaxLogFileSize, the max size (in bytes) of a single log file before it cuts over to a new one (default is 10,000,000)\n-Logger.MaxLogFileSize=10000000\n-\n-\n-#===========================================================================\n-# ESAPI Intrusion Detection\n-#\n-# Each event has a base to which .count, .interval, and .action are added\n-# The IntrusionException will fire if we receive \"count\" events within \"interval\" seconds\n-# The IntrusionDetector is configurable to take the following actions: log, logout, and disable\n-#  (multiple actions separated by commas are allowed e.g. event.test.actions=log,disable\n-#\n-# Custom Events\n-# Names must start with \"event.\" as the base\n-# Use IntrusionDetector.addEvent( \"test\" ) in your code to trigger \"event.test\" here\n-# You can also disable intrusion detection completely by changing\n-# the following parameter to true\n-#\n-IntrusionDetector.Disable=false\n-#\n-IntrusionDetector.event.test.count=2\n-IntrusionDetector.event.test.interval=10\n-IntrusionDetector.event.test.actions=disable,log\n-\n-# Exception Events\n-# All EnterpriseSecurityExceptions are registered automatically\n-# Call IntrusionDetector.getInstance().addException(e) for Exceptions that do not extend EnterpriseSecurityException\n-# Use the fully qualified classname of the exception as the base\n-\n-# any intrusion is an attack\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.count=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.interval=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.actions=log,disable,logout\n-\n-# for test purposes\n-# CHECKME: Shouldn't there be something in the property name itself that designates\n-#           that these are for testing???\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.count=10\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.interval=5\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.actions=log,disable,logout\n-\n-# rapid validation errors indicate scans or attacks in progress\n-# org.owasp.esapi.errors.ValidationException.count=10\n-# org.owasp.esapi.errors.ValidationException.interval=10\n-# org.owasp.esapi.errors.ValidationException.actions=log,logout\n-\n-# sessions jumping between hosts indicates session hijacking\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.count=2\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.interval=10\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.actions=log,logout\n-\n-\n-#===========================================================================\n-# ESAPI Validation\n-#\n-# The ESAPI Validator works on regular expressions with defined names. You can define names\n-# either here, or you may define application specific patterns in a separate file defined below.\n-# This allows enterprises to specify both organizational standards as well as application specific\n-# validation rules.\n-#\n-Validator.ConfigurationFile=validation.properties\n-\n-# Validators used by ESAPI\n-Validator.AccountName=^[a-zA-Z0-9]{3,20}$\n-Validator.SystemCommand=^[a-zA-Z\\\\-\\\\/]{1,64}$\n-Validator.RoleName=^[a-z]{1,20}$\n-\n-#the word TEST below should be changed to your application\n-#name - only relative URL's are supported\n-Validator.Redirect=^\\\\/test.*$\n-\n-# Global HTTP Validation Rules\n-# Values with Base64 encoded data (e.g. encrypted state) will need at least [a-zA-Z0-9\\/+=]\n-Validator.HTTPScheme=^(http|https)$\n-Validator.HTTPServerName=^[a-zA-Z0-9_.\\\\-]*$\n-Validator.HTTPParameterName=^[a-zA-Z0-9_]{1,32}$\n-Validator.HTTPParameterValue=^[a-zA-Z0-9.\\\\-\\\\/+=_ ]*$\n-Validator.HTTPCookieName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPCookieValue=^[a-zA-Z0-9\\\\-\\\\/+=_ ]*$\n-Validator.HTTPHeaderName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPHeaderValue=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPContextPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPServletPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPPath=^[a-zA-Z0-9.\\\\-_]*$\n-Validator.HTTPQueryString=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ %]*$\n-Validator.HTTPURI=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPURL=^.*$\n-Validator.HTTPJSESSIONID=^[A-Z0-9]{10,30}$\n-\n-# Validation of file related input\n-Validator.FileName=^[a-zA-Z0-9!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$\n-Validator.DirectoryName=^[a-zA-Z0-9:/\\\\\\\\!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$",
                "deletions": 431
            },
            {
                "sha": "27388e78210544df7c47f0bbf0e6b595120d95bf",
                "filename": "hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/master/table.jsp",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/master/table.jsp?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -29,7 +29,6 @@\n   import=\"java.util.Collection\"\n   import=\"java.util.Collections\"\n   import=\"java.util.Comparator\"\n-  import=\"org.owasp.esapi.ESAPI\"\n   import=\"org.apache.hadoop.conf.Configuration\"\n   import=\"org.apache.hadoop.util.StringUtils\"\n   import=\"org.apache.hadoop.hbase.HRegionInfo\"\n@@ -85,7 +84,7 @@\n     <% if ( !readOnly && action != null ) { %>\n         <title>HBase Master: <%= master.getServerName() %></title>\n     <% } else { %>\n-        <title>Table: <%= ESAPI.encoder().encodeForHTML(fqtn) %></title>\n+        <title>Table: <%= fqtn %></title>\n     <% } %>\n     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n     <meta name=\"description\" content=\"\">\n@@ -181,7 +180,7 @@ if ( fqtn != null ) {\n <div class=\"container-fluid content\">\n     <div class=\"row inner_header\">\n         <div class=\"page-header\">\n-            <h1>Table <small><%= ESAPI.encoder().encodeForHTML(fqtn) %></small></h1>\n+            <h1>Table <small><%= fqtn %></small></h1>\n         </div>\n     </div>\n     <div class=\"row\">",
                "deletions": 3
            },
            {
                "sha": "874ac4392440667aa6d23a8e314d7ca7c87ce0ee",
                "filename": "hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp",
                "status": "modified",
                "changes": 18,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/resources/hbase-webapps/regionserver/region.jsp?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -21,7 +21,6 @@\n   import=\"java.util.Collection\"\n   import=\"java.util.Date\"\n   import=\"java.util.List\"\n-  import=\"org.owasp.esapi.ESAPI\"\n   import=\"static org.apache.commons.lang.StringEscapeUtils.escapeXml\"\n   import=\"org.apache.hadoop.conf.Configuration\"\n   import=\"org.apache.hadoop.hbase.HTableDescriptor\"\n@@ -36,14 +35,10 @@\n   String regionName = request.getParameter(\"name\");\n   HRegionServer rs = (HRegionServer) getServletContext().getAttribute(HRegionServer.REGIONSERVER);\n   Configuration conf = rs.getConfiguration();\n-  String displayName = null;\n+\n   Region region = rs.getFromOnlineRegions(regionName);\n-  if(region == null) {\n-    displayName= ESAPI.encoder().encodeForHTML(regionName) + \" does not exist\";\n-  } else {\n-    displayName = HRegionInfo.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n+  String displayName = HRegionInfo.getRegionNameAsStringForDisplay(region.getRegionInfo(),\n     rs.getConfiguration());\n-  }\n %>\n <!--[if IE]>\n <!DOCTYPE html>\n@@ -126,14 +121,7 @@\n          <p> <%= storeFiles.size() %> StoreFile(s) in set.</p>\n          </table>\n    <%  }\n-   } else { %>\n-   <div class=\"container-fluid content\">\n-   <div class=\"row inner_header\">\n-   </div>\n-   <p><hr><p>\n-   <p>Go <a href=\"javascript:history.back()\">Back</a>\n-   </div>\n-  <% } %>\n+   }%>\n </div>\n <script src=\"/static/js/jquery.min.js\" type=\"text/javascript\"></script>\n <script src=\"/static/js/bootstrap.min.js\" type=\"text/javascript\"></script>",
                "deletions": 15
            },
            {
                "sha": "031ddce8bf06bc89f9fb7818845f0473ae594e3c",
                "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java",
                "status": "modified",
                "changes": 6,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/http/jmx/TestJMXJsonServlet.java?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -105,11 +105,5 @@ public static void assertReFind(String re, String value) {\n     assertReFind(\"\\\"committed\\\"\\\\s*:\", result);\n     assertReFind(\"\\\\}\\\\);$\", result);\n \n-    // test to get XSS JSONP result\n-    result = readOutput(new URL(baseUrl, \"/jmx?qry=java.lang:type=Memory&callback=<script>alert('hello')</script>\"));\n-    LOG.info(\"/jmx?qry=java.lang:type=Memory&callback=<script>alert('hello')</script> RESULT: \"+result);\n-    assertTrue(!result.contains(\"<script>\"));\n-\n-\n   }\n }",
                "deletions": 6
            },
            {
                "sha": "907400189d8dd33f34b03469af8fc1f250844cf8",
                "filename": "hbase-server/src/test/resources/ESAPI.properties",
                "blob_url": "https://github.com/apache/hbase/blob/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/test/resources/ESAPI.properties",
                "raw_url": "https://github.com/apache/hbase/raw/81b06b3bdd56c1c434406fe3e3aea7e47948f958/hbase-server/src/test/resources/ESAPI.properties",
                "status": "removed",
                "changes": 431,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/resources/ESAPI.properties?ref=81b06b3bdd56c1c434406fe3e3aea7e47948f958",
                "patch": "@@ -1,431 +0,0 @@\n-#\n-# OWASP Enterprise Security API (ESAPI) Properties file -- PRODUCTION Version\n-#\n-# This file is part of the Open Web Application Security Project (OWASP)\n-# Enterprise Security API (ESAPI) project. For details, please see\n-# http://www.owasp.org/index.php/ESAPI.\n-#\n-# Copyright (c) 2008,2009 - The OWASP Foundation\n-#\n-# DISCUSS: This may cause a major backwards compatibility issue, etc. but\n-#           from a name space perspective, we probably should have prefaced\n-#           all the property names with ESAPI or at least OWASP. Otherwise\n-#           there could be problems is someone loads this properties file into\n-#           the System properties.  We could also put this file into the\n-#           esapi.jar file (perhaps as a ResourceBundle) and then allow an external\n-#           ESAPI properties be defined that would overwrite these defaults.\n-#           That keeps the application's properties relatively simple as usually\n-#           they will only want to override a few properties. If looks like we\n-#           already support multiple override levels of this in the\n-#           DefaultSecurityConfiguration class, but I'm suggesting placing the\n-#           defaults in the esapi.jar itself. That way, if the jar is signed,\n-#           we could detect if those properties had been tampered with. (The\n-#           code to check the jar signatures is pretty simple... maybe 70-90 LOC,\n-#           but off course there is an execution penalty (similar to the way\n-#           that the separate sunjce.jar used to be when a class from it was\n-#           first loaded). Thoughts?\n-###############################################################################\n-#\n-# WARNING: Operating system protection should be used to lock down the .esapi\n-# resources directory and all the files inside and all the directories all the\n-# way up to the root directory of the file system.  Note that if you are using\n-# file-based implementations, that some files may need to be read-write as they\n-# get updated dynamically.\n-#\n-# Before using, be sure to update the MasterKey and MasterSalt as described below.\n-# N.B.: If you had stored data that you have previously encrypted with ESAPI 1.4,\n-#        you *must* FIRST decrypt it using ESAPI 1.4 and then (if so desired)\n-#        re-encrypt it with ESAPI 2.0. If you fail to do this, you will NOT be\n-#        able to decrypt your data with ESAPI 2.0.\n-#\n-#        YOU HAVE BEEN WARNED!!! More details are in the ESAPI 2.0 Release Notes.\n-#\n-#===========================================================================\n-# ESAPI Configuration\n-#\n-# If true, then print all the ESAPI properties set here when they are loaded.\n-# If false, they are not printed. Useful to reduce output when running JUnit tests.\n-# If you need to troubleshoot a properties related problem, turning this on may help.\n-# This is 'false' in the src/test/resources/.esapi version. It is 'true' by\n-# default for reasons of backward compatibility with earlier ESAPI versions.\n-ESAPI.printProperties=true\n-\n-# ESAPI is designed to be easily extensible. You can use the reference implementation\n-# or implement your own providers to take advantage of your enterprise's security\n-# infrastructure. The functions in ESAPI are referenced using the ESAPI locator, like:\n-#\n-#    String ciphertext =\n-#        ESAPI.encryptor().encrypt(\"Secret message\");   // Deprecated in 2.0\n-#    CipherText cipherText =\n-#        ESAPI.encryptor().encrypt(new PlainText(\"Secret message\")); // Preferred\n-#\n-# Below you can specify the classname for the provider that you wish to use in your\n-# application. The only requirement is that it implement the appropriate ESAPI interface.\n-# This allows you to switch security implementations in the future without rewriting the\n-# entire application.\n-#\n-# ExperimentalAccessController requires ESAPI-AccessControlPolicy.xml in .esapi directory\n-ESAPI.AccessControl=org.owasp.esapi.reference.DefaultAccessController\n-# FileBasedAuthenticator requires users.txt file in .esapi directory\n-ESAPI.Authenticator=org.owasp.esapi.reference.FileBasedAuthenticator\n-ESAPI.Encoder=org.owasp.esapi.reference.DefaultEncoder\n-ESAPI.Encryptor=org.owasp.esapi.reference.crypto.JavaEncryptor\n-\n-ESAPI.Executor=org.owasp.esapi.reference.DefaultExecutor\n-ESAPI.HTTPUtilities=org.owasp.esapi.reference.DefaultHTTPUtilities\n-ESAPI.IntrusionDetector=org.owasp.esapi.reference.DefaultIntrusionDetector\n-# Log4JFactory Requires log4j.xml or log4j.properties in classpath - http://www.laliluna.de/log4j-tutorial.html\n-ESAPI.Logger=org.owasp.esapi.reference.Log4JLogFactory\n-#ESAPI.Logger=org.owasp.esapi.reference.JavaLogFactory\n-ESAPI.Randomizer=org.owasp.esapi.reference.DefaultRandomizer\n-ESAPI.Validator=org.owasp.esapi.reference.DefaultValidator\n-\n-#===========================================================================\n-# ESAPI Authenticator\n-#\n-Authenticator.AllowedLoginAttempts=3\n-Authenticator.MaxOldPasswordHashes=13\n-Authenticator.UsernameParameterName=username\n-Authenticator.PasswordParameterName=password\n-# RememberTokenDuration (in days)\n-Authenticator.RememberTokenDuration=14\n-# Session Timeouts (in minutes)\n-Authenticator.IdleTimeoutDuration=20\n-Authenticator.AbsoluteTimeoutDuration=120\n-\n-#===========================================================================\n-# ESAPI Encoder\n-#\n-# ESAPI canonicalizes input before validation to prevent bypassing filters with encoded attacks.\n-# Failure to canonicalize input is a very common mistake when implementing validation schemes.\n-# Canonicalization is automatic when using the ESAPI Validator, but you can also use the\n-# following code to canonicalize data.\n-#\n-#      ESAPI.Encoder().canonicalize( \"%22hello world&#x22;\" );\n-#\n-# Multiple encoding is when a single encoding format is applied multiple times, multiple\n-# different encoding formats are applied, or when multiple formats are nested. Allowing\n-# multiple encoding is strongly discouraged.\n-Encoder.AllowMultipleEncoding=false\n-#\n-# The default list of codecs to apply when canonicalizing untrusted data. The list should include the codecs\n-# for all downstream interpreters or decoders. For example, if the data is likely to end up in a URL, HTML, or\n-# inside JavaScript, then the list of codecs below is appropriate. The order of the list is not terribly important.\n-Encoder.DefaultCodecList=HTMLEntityCodec,PercentCodec,JavaScriptCodec\n-\n-\n-#===========================================================================\n-# ESAPI Encryption\n-#\n-# The ESAPI Encryptor provides basic cryptographic functions with a simplified API.\n-# To get started, generate a new key using java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-# There is not currently any support for key rotation, so be careful when changing your key and salt as it\n-# will invalidate all signed, encrypted, and hashed data.\n-#\n-# WARNING: Not all combinations of algorithms and key lengths are supported.\n-# If you choose to use a key length greater than 128, you MUST download the\n-# unlimited strength policy files and install in the lib directory of your JRE/JDK.\n-# See http://java.sun.com/javase/downloads/index.jsp for more information.\n-#\n-# Backward compatibility with ESAPI Java 1.4 is supported by the two deprecated API\n-# methods, Encryptor.encrypt(String) and Encryptor.decrypt(String). However, whenever\n-# possible, these methods should be avoided as they use ECB cipher mode, which in almost\n-# all circumstances a poor choice because of it's weakness. CBC cipher mode is the default\n-# for the new Encryptor encrypt / decrypt methods for ESAPI Java 2.0.  In general, you\n-# should only use this compatibility setting if you have persistent data encrypted with\n-# version 1.4 and even then, you should ONLY set this compatibility mode UNTIL\n-# you have decrypted all of your old encrypted data and then re-encrypted it with\n-# ESAPI 2.0 using CBC mode. If you have some reason to mix the deprecated 1.4 mode\n-# with the new 2.0 methods, make sure that you use the same cipher algorithm for both\n-# (256-bit AES was the default for 1.4; 128-bit is the default for 2.0; see below for\n-# more details.) Otherwise, you will have to use the new 2.0 encrypt / decrypt methods\n-# where you can specify a SecretKey. (Note that if you are using the 256-bit AES,\n-# that requires downloading the special jurisdiction policy files mentioned above.)\n-#\n-#        ***** IMPORTANT: Do NOT forget to replace these with your own values! *****\n-# To calculate these values, you can run:\n-#        java -classpath esapi.jar org.owasp.esapi.reference.crypto.JavaEncryptor\n-#\n-Encryptor.MasterKey=\n-Encryptor.MasterSalt=\n-\n-# Provides the default JCE provider that ESAPI will \"prefer\" for its symmetric\n-# encryption and hashing. (That is it will look to this provider first, but it\n-# will defer to other providers if the requested algorithm is not implemented\n-# by this provider.) If left unset, ESAPI will just use your Java VM's current\n-# preferred JCE provider, which is generally set in the file\n-# \"$JAVA_HOME/jre/lib/security/java.security\".\n-#\n-# The main intent of this is to allow ESAPI symmetric encryption to be\n-# used with a FIPS 140-2 compliant crypto-module. For details, see the section\n-# \"Using ESAPI Symmetric Encryption with FIPS 140-2 Cryptographic Modules\" in\n-# the ESAPI 2.0 Symmetric Encryption User Guide, at:\n-# http://owasp-esapi-java.googlecode.com/svn/trunk/documentation/esapi4java-core-2.0-symmetric-crypto-user-guide.html\n-# However, this property also allows you to easily use an alternate JCE provider\n-# such as \"Bouncy Castle\" without having to make changes to \"java.security\".\n-# See Javadoc for SecurityProviderLoader for further details. If you wish to use\n-# a provider that is not known to SecurityProviderLoader, you may specify the\n-# fully-qualified class name of the JCE provider class that implements\n-# java.security.Provider. If the name contains a '.', this is interpreted as\n-# a fully-qualified class name that implements java.security.Provider.\n-#\n-# NOTE: Setting this property has the side-effect of changing it in your application\n-#       as well, so if you are using JCE in your application directly rather than\n-#       through ESAPI (you wouldn't do that, would you? ;-), it will change the\n-#       preferred JCE provider there as well.\n-#\n-# Default: Keeps the JCE provider set to whatever JVM sets it to.\n-Encryptor.PreferredJCEProvider=\n-\n-# AES is the most widely used and strongest encryption algorithm. This\n-# should agree with your Encryptor.CipherTransformation property.\n-# By default, ESAPI Java 1.4 uses \"PBEWithMD5AndDES\" and which is\n-# very weak. It is essentially a password-based encryption key, hashed\n-# with MD5 around 1K times and then encrypted with the weak DES algorithm\n-# (56-bits) using ECB mode and an unspecified padding (it is\n-# JCE provider specific, but most likely \"NoPadding\"). However, 2.0 uses\n-# \"AES/CBC/PKCSPadding\". If you want to change these, change them here.\n-# Warning: This property does not control the default reference implementation for\n-#           ESAPI 2.0 using JavaEncryptor. Also, this property will be dropped\n-#           in the future.\n-# @deprecated\n-Encryptor.EncryptionAlgorithm=AES\n-#        For ESAPI Java 2.0 - New encrypt / decrypt methods use this.\n-Encryptor.CipherTransformation=AES/CBC/PKCS5Padding\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Comma-separated list of cipher modes that provide *BOTH*\n-# confidentiality *AND* message authenticity. (NIST refers to such cipher\n-# modes as \"combined modes\" so that's what we shall call them.) If any of these\n-# cipher modes are used then no MAC is calculated and stored\n-# in the CipherText upon encryption. Likewise, if one of these\n-# cipher modes is used with decryption, no attempt will be made\n-# to validate the MAC contained in the CipherText object regardless\n-# of whether it contains one or not. Since the expectation is that\n-# these cipher modes support support message authenticity already,\n-# injecting a MAC in the CipherText object would be at best redundant.\n-#\n-# Note that as of JDK 1.5, the SunJCE provider does not support *any*\n-# of these cipher modes. Of these listed, only GCM and CCM are currently\n-# NIST approved. YMMV for other JCE providers. E.g., Bouncy Castle supports\n-# GCM and CCM with \"NoPadding\" mode, but not with \"PKCS5Padding\" or other\n-# padding modes.\n-Encryptor.cipher_modes.combined_modes=GCM,CCM,IAPM,EAX,OCB,CWC\n-\n-# Applies to ESAPI 2.0 and later only!\n-# Additional cipher modes allowed for ESAPI 2.0 encryption. These\n-# cipher modes are in _addition_ to those specified by the property\n-# 'Encryptor.cipher_modes.combined_modes'.\n-# Note: We will add support for streaming modes like CFB & OFB once\n-# we add support for 'specified' to the property 'Encryptor.ChooseIVMethod'\n-# (probably in ESAPI 2.1).\n-# DISCUSS: Better name?\n-Encryptor.cipher_modes.additional_allowed=CBC\n-\n-# 128-bit is almost always sufficient and appears to be more resistant to\n-# related key attacks than is 256-bit AES. Use '_' to use default key size\n-# for cipher algorithms (where it makes sense because the algorithm supports\n-# a variable key size). Key length must agree to what's provided as the\n-# cipher transformation, otherwise this will be ignored after logging a\n-# warning.\n-#\n-# NOTE: This is what applies BOTH ESAPI 1.4 and 2.0. See warning above about mixing!\n-Encryptor.EncryptionKeyLength=128\n-\n-# Because 2.0 uses CBC mode by default, it requires an initialization vector (IV).\n-# (All cipher modes except ECB require an IV.) There are two choices: we can either\n-# use a fixed IV known to both parties or allow ESAPI to choose a random IV. While\n-# the IV does not need to be hidden from adversaries, it is important that the\n-# adversary not be allowed to choose it. Also, random IVs are generally much more\n-# secure than fixed IVs. (In fact, it is essential that feed-back cipher modes\n-# such as CFB and OFB use a different IV for each encryption with a given key so\n-# in such cases, random IVs are much preferred. By default, ESAPI 2.0 uses random\n-# IVs. If you wish to use 'fixed' IVs, set 'Encryptor.ChooseIVMethod=fixed' and\n-# uncomment the Encryptor.fixedIV.\n-#\n-# Valid values:        random|fixed|specified        'specified' not yet implemented; planned for 2.1\n-Encryptor.ChooseIVMethod=random\n-# If you choose to use a fixed IV, then you must place a fixed IV here that\n-# is known to all others who are sharing your secret key. The format should\n-# be a hex string that is the same length as the cipher block size for the\n-# cipher algorithm that you are using. The following is an example for AES\n-# from an AES test vector for AES-128/CBC as described in:\n-# NIST Special Publication 800-38A (2001 Edition)\n-# \"Recommendation for Block Cipher Modes of Operation\".\n-# (Note that the block size for AES is 16 bytes == 128 bits.)\n-#\n-Encryptor.fixedIV=0x000102030405060708090a0b0c0d0e0f\n-\n-# Whether or not CipherText should use a message authentication code (MAC) with it.\n-# This prevents an adversary from altering the IV as well as allowing a more\n-# fool-proof way of determining the decryption failed because of an incorrect\n-# key being supplied. This refers to the \"separate\" MAC calculated and stored\n-# in CipherText, not part of any MAC that is calculated as a result of a\n-# \"combined mode\" cipher mode.\n-#\n-# If you are using ESAPI with a FIPS 140-2 cryptographic module, you *must* also\n-# set this property to false.\n-Encryptor.CipherText.useMAC=true\n-\n-# Whether or not the PlainText object may be overwritten and then marked\n-# eligible for garbage collection. If not set, this is still treated as 'true'.\n-Encryptor.PlainText.overwrite=true\n-\n-# Do not use DES except in a legacy situations. 56-bit is way too small key size.\n-#Encryptor.EncryptionKeyLength=56\n-#Encryptor.EncryptionAlgorithm=DES\n-\n-# TripleDES is considered strong enough for most purposes.\n-#    Note:    There is also a 112-bit version of DESede. Using the 168-bit version\n-#            requires downloading the special jurisdiction policy from Sun.\n-#Encryptor.EncryptionKeyLength=168\n-#Encryptor.EncryptionAlgorithm=DESede\n-\n-Encryptor.HashAlgorithm=SHA-512\n-Encryptor.HashIterations=1024\n-Encryptor.DigitalSignatureAlgorithm=SHA1withDSA\n-Encryptor.DigitalSignatureKeyLength=1024\n-Encryptor.RandomAlgorithm=SHA1PRNG\n-Encryptor.CharacterEncoding=UTF-8\n-\n-\n-#===========================================================================\n-# ESAPI HttpUtilties\n-#\n-# The HttpUtilities provide basic protections to HTTP requests and responses. Primarily these methods\n-# protect against malicious data from attackers, such as unprintable characters, escaped characters,\n-# and other simple attacks. The HttpUtilities also provides utility methods for dealing with cookies,\n-# headers, and CSRF tokens.\n-#\n-# Default file upload location (remember to escape backslashes with \\\\)\n-HttpUtilities.UploadDir=C:\\\\ESAPI\\\\testUpload\n-HttpUtilities.UploadTempDir=C:\\\\temp\n-# Force flags on cookies, if you use HttpUtilities to set cookies\n-HttpUtilities.ForceHttpOnlySession=false\n-HttpUtilities.ForceSecureSession=false\n-HttpUtilities.ForceHttpOnlyCookies=true\n-HttpUtilities.ForceSecureCookies=true\n-# Maximum size of HTTP headers\n-HttpUtilities.MaxHeaderSize=4096\n-# File upload configuration\n-HttpUtilities.ApprovedUploadExtensions=.zip,.pdf,.doc,.docx,.ppt,.pptx,.tar,.gz,.tgz,.rar,.war,.jar,.ear,.xls,.rtf,.properties,.java,.class,.txt,.xml,.jsp,.jsf,.exe,.dll\n-HttpUtilities.MaxUploadFileBytes=500000000\n-# Using UTF-8 throughout your stack is highly recommended. That includes your database driver,\n-# container, and any other technologies you may be using. Failure to do this may expose you\n-# to Unicode transcoding injection attacks. Use of UTF-8 does not hinder internationalization.\n-HttpUtilities.ResponseContentType=text/html; charset=UTF-8\n-\n-\n-\n-#===========================================================================\n-# ESAPI Executor\n-# CHECKME - Not sure what this is used for, but surely it should be made OS independent.\n-Executor.WorkingDirectory=C:\\\\Windows\\\\Temp\n-Executor.ApprovedExecutables=C:\\\\Windows\\\\System32\\\\cmd.exe,C:\\\\Windows\\\\System32\\\\runas.exe\n-\n-\n-#===========================================================================\n-# ESAPI Logging\n-# Set the application name if these logs are combined with other applications\n-Logger.ApplicationName=ExampleApplication\n-# If you use an HTML log viewer that does not properly HTML escape log data, you can set LogEncodingRequired to true\n-Logger.LogEncodingRequired=false\n-# Determines whether ESAPI should log the application name. This might be clutter in some single-server/single-app environments.\n-Logger.LogApplicationName=true\n-# Determines whether ESAPI should log the server IP and port. This might be clutter in some single-server environments.\n-Logger.LogServerIP=true\n-# LogFileName, the name of the logging file. Provide a full directory path (e.g., C:\\\\ESAPI\\\\ESAPI_logging_file) if you\n-# want to place it in a specific directory.\n-Logger.LogFileName=ESAPI_logging_file\n-# MaxLogFileSize, the max size (in bytes) of a single log file before it cuts over to a new one (default is 10,000,000)\n-Logger.MaxLogFileSize=10000000\n-\n-\n-#===========================================================================\n-# ESAPI Intrusion Detection\n-#\n-# Each event has a base to which .count, .interval, and .action are added\n-# The IntrusionException will fire if we receive \"count\" events within \"interval\" seconds\n-# The IntrusionDetector is configurable to take the following actions: log, logout, and disable\n-#  (multiple actions separated by commas are allowed e.g. event.test.actions=log,disable\n-#\n-# Custom Events\n-# Names must start with \"event.\" as the base\n-# Use IntrusionDetector.addEvent( \"test\" ) in your code to trigger \"event.test\" here\n-# You can also disable intrusion detection completely by changing\n-# the following parameter to true\n-#\n-IntrusionDetector.Disable=false\n-#\n-IntrusionDetector.event.test.count=2\n-IntrusionDetector.event.test.interval=10\n-IntrusionDetector.event.test.actions=disable,log\n-\n-# Exception Events\n-# All EnterpriseSecurityExceptions are registered automatically\n-# Call IntrusionDetector.getInstance().addException(e) for Exceptions that do not extend EnterpriseSecurityException\n-# Use the fully qualified classname of the exception as the base\n-\n-# any intrusion is an attack\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.count=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.interval=1\n-IntrusionDetector.org.owasp.esapi.errors.IntrusionException.actions=log,disable,logout\n-\n-# for test purposes\n-# CHECKME: Shouldn't there be something in the property name itself that designates\n-#           that these are for testing???\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.count=10\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.interval=5\n-IntrusionDetector.org.owasp.esapi.errors.IntegrityException.actions=log,disable,logout\n-\n-# rapid validation errors indicate scans or attacks in progress\n-# org.owasp.esapi.errors.ValidationException.count=10\n-# org.owasp.esapi.errors.ValidationException.interval=10\n-# org.owasp.esapi.errors.ValidationException.actions=log,logout\n-\n-# sessions jumping between hosts indicates session hijacking\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.count=2\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.interval=10\n-IntrusionDetector.org.owasp.esapi.errors.AuthenticationHostException.actions=log,logout\n-\n-\n-#===========================================================================\n-# ESAPI Validation\n-#\n-# The ESAPI Validator works on regular expressions with defined names. You can define names\n-# either here, or you may define application specific patterns in a separate file defined below.\n-# This allows enterprises to specify both organizational standards as well as application specific\n-# validation rules.\n-#\n-Validator.ConfigurationFile=validation.properties\n-\n-# Validators used by ESAPI\n-Validator.AccountName=^[a-zA-Z0-9]{3,20}$\n-Validator.SystemCommand=^[a-zA-Z\\\\-\\\\/]{1,64}$\n-Validator.RoleName=^[a-z]{1,20}$\n-\n-#the word TEST below should be changed to your application\n-#name - only relative URL's are supported\n-Validator.Redirect=^\\\\/test.*$\n-\n-# Global HTTP Validation Rules\n-# Values with Base64 encoded data (e.g. encrypted state) will need at least [a-zA-Z0-9\\/+=]\n-Validator.HTTPScheme=^(http|https)$\n-Validator.HTTPServerName=^[a-zA-Z0-9_.\\\\-]*$\n-Validator.HTTPParameterName=^[a-zA-Z0-9_]{1,32}$\n-Validator.HTTPParameterValue=^[a-zA-Z0-9.\\\\-\\\\/+=_ ]*$\n-Validator.HTTPCookieName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPCookieValue=^[a-zA-Z0-9\\\\-\\\\/+=_ ]*$\n-Validator.HTTPHeaderName=^[a-zA-Z0-9\\\\-_]{1,32}$\n-Validator.HTTPHeaderValue=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPContextPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPServletPath=^[a-zA-Z0-9.\\\\-\\\\/_]*$\n-Validator.HTTPPath=^[a-zA-Z0-9.\\\\-_]*$\n-Validator.HTTPQueryString=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ %]*$\n-Validator.HTTPURI=^[a-zA-Z0-9()\\\\-=\\\\*\\\\.\\\\?;,+\\\\/:&_ ]*$\n-Validator.HTTPURL=^.*$\n-Validator.HTTPJSESSIONID=^[A-Z0-9]{10,30}$\n-\n-# Validation of file related input\n-Validator.FileName=^[a-zA-Z0-9!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$\n-Validator.DirectoryName=^[a-zA-Z0-9:/\\\\\\\\!@#$%^&{}\\\\[\\\\]()_+\\\\-=,.~'` ]{1,255}$",
                "deletions": 431
            },
            {
                "sha": "b116ae49c986a533bb742511fbc8d9430d46bdbb",
                "filename": "pom.xml",
                "blob_url": "https://github.com/apache/hbase/blob/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/pom.xml",
                "raw_url": "https://github.com/apache/hbase/raw/209b6f74c7f203ff4b5de79420d7115d89b4d9ca/pom.xml",
                "status": "modified",
                "changes": 1,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/pom.xml?ref=209b6f74c7f203ff4b5de79420d7115d89b4d9ca",
                "patch": "@@ -874,7 +874,6 @@\n               <exclude>**/patchprocess/**</exclude>\n               <exclude>src/main/site/resources/repo/**</exclude>\n               <exclude>**/dependency-reduced-pom.xml</exclude>\n-              <exclude>**/ESAPI.properties</exclude>\n               <exclude>**/rat.txt</exclude>\n             </excludes>\n           </configuration>",
                "deletions": 1
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-613 Timestamp-anchored scanning fails to find all records\nHBASE-681 NPE in Memcache\n\nHAbstractScanner\n- remove HAbstactScanner.iterator() - iterator is not a method on InternalScanner\n\nHRegion\n- make getScanner more efficient by iterating only once to find the stores we need to scan\n- only pass columns relevant to a store to a HStoreScanner\n- remove HScanner.iterator() - iterator is not a method on InternalScanner\n\nMemcache, MemcacheScanner\n- Fix NPE in Memcache\n- never return HConstants.LATEST_TIMESTAMP as the timestamp value for a row. Instead use the largest timestamp from the cells being returned. This allows a scanner to determine a timestamp that can be used to fetch the same data again should new versions be inserted later.\n\nStoreFileScanner\n- getNextViableRow would find a row that matched the row key, but did not consider the requested timestamp. Now if the row it finds has a timestamp greater than the one desired it advances to determine if a row with a timestamp less than or equal to the requested one exists since timestamps are sorted descending.\n- removed an unnecessary else\n\ntestScanMultipleVersions\n- Test program that fails on current trunk but passes when this patch is applied.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@670124 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
        "parent": "https://github.com/apache/hbase/commit/d5b1dfe30c20f697105a89124042ba11ea7e3657",
        "bug_id": "hbase_318",
        "file": [
            {
                "sha": "9a04347c467c2b28609a9aee7ef7dae06bb277df",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -62,6 +62,9 @@ Hbase Change Log\n                (Rong-En Fan via Stack)\r\n    HBASE-699   Fix TestMigrate up on Hudson\r\n    HBASE-615   Region balancer oscillates during cluster startup\r\n+   HBASE-613   Timestamp-anchored scanning fails to find all records\r\n+   HBASE-681   NPE in Memcache\r\n+   \r\n    \r\n   IMPROVEMENTS\r\n    HBASE-559   MR example job to count table rows\r",
                "deletions": 0
            },
            {
                "sha": "213f969d8f8dc1c231b06da474bf66f71bd3e0a8",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "status": "modified",
                "changes": 9,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -21,11 +21,9 @@\n \n import java.io.IOException;\n import java.util.HashMap;\n-import java.util.Iterator;\n import java.util.Map;\n import java.util.SortedMap;\n import java.util.Vector;\n-import java.util.Map.Entry;\n import java.util.regex.Pattern;\n \n import org.apache.commons.logging.Log;\n@@ -183,12 +181,9 @@ public boolean isMultipleMatchScanner() {\n     return this.multipleMatchers;\n   }\n \n+  /** {@inheritDoc} */\n   public abstract boolean next(HStoreKey key,\n-    SortedMap<byte [], byte []> results)\n+      SortedMap<byte [], byte []> results)\n   throws IOException;\n   \n-  public Iterator<Entry<HStoreKey, SortedMap<byte [], byte[]>>> iterator() {\n-    throw new UnsupportedOperationException(\"Unimplemented serverside. \" +\n-      \"next(HStoreKey, StortedMap(...) is more efficient\");\n-  }\n }",
                "deletions": 7
            },
            {
                "sha": "b20efd3bfe1007ffc4ad1835c2799143b188fe84",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 56,
                "additions": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -23,15 +23,14 @@\n import java.io.UnsupportedEncodingException;\n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.Iterator;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n import java.util.Set;\n import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.TreeSet;\n-import java.util.Map.Entry;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n@@ -64,7 +63,6 @@\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.FSUtils;\n import org.apache.hadoop.hbase.util.Writables;\n-import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.WritableUtils;\n import org.apache.hadoop.util.Progressable;\n import org.apache.hadoop.util.StringUtils;\n@@ -1268,20 +1266,15 @@ public InternalScanner getScanner(byte[][] cols, byte [] firstRow,\n       if (this.closed.get()) {\n         throw new IOException(\"Region \" + this + \" closed\");\n       }\n-      TreeSet<byte []> families = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);\n+      HashSet<HStore> storeSet = new HashSet<HStore>();\n       for (int i = 0; i < cols.length; i++) {\n-        families.add(HStoreKey.getFamily(cols[i]));\n-      }\n-      List<HStore> storelist = new ArrayList<HStore>();\n-      for (byte [] family: families) {\n-        HStore s = stores.get(Bytes.mapKey(family));\n-        if (s == null) {\n-          continue;\n+        HStore s = stores.get(Bytes.mapKey(HStoreKey.getFamily(cols[i])));\n+        if (s != null) {\n+          storeSet.add(s);\n         }\n-        storelist.add(s);\n       }\n       return new HScanner(cols, firstRow, timestamp,\n-        storelist.toArray(new HStore [storelist.size()]), filter);\n+        storeSet.toArray(new HStore [storeSet.size()]), filter);\n     } finally {\n       splitsAndClosesLock.readLock().unlock();\n     }\n@@ -1750,15 +1743,26 @@ public Path getBaseDir() {\n       this.scanners = new InternalScanner[stores.length];\n       try {\n         for (int i = 0; i < stores.length; i++) {\n-          // TODO: The cols passed in here can include columns from other\n-          // stores; add filter so only pertinent columns are passed.\n-          //\n-          // Also, if more than one store involved, need to replicate filters.\n-          // At least WhileMatchRowFilter will mess up the scan if only\n-          // one shared across many rows. See HADOOP-2467.\n-          scanners[i] = stores[i].getScanner(timestamp, cols, firstRow,\n-            filter != null ?\n-              (RowFilterInterface)WritableUtils.clone(filter, conf) : filter);\n+          \n+          // Only pass relevant columns to each store\n+          \n+          List<byte[]> columns = new ArrayList<byte[]>();\n+          for (int j = 0; j < cols.length; j++) {\n+            if (Bytes.equals(HStoreKey.getFamily(cols[j]),\n+                stores[i].getFamily().getName())) {\n+              columns.add(cols[j]);\n+            }\n+          }\n+\n+          RowFilterInterface f = filter;\n+          if (f != null) {\n+            // Need to replicate filters.\n+            // At least WhileMatchRowFilter will mess up the scan if only\n+            // one shared across many rows. See HADOOP-2467.\n+            f = (RowFilterInterface)WritableUtils.clone(filter, conf);\n+          }\n+          scanners[i] = stores[i].getScanner(timestamp,\n+              columns.toArray(new byte[columns.size()][]), firstRow, f);\n         }\n       } catch (IOException e) {\n         for (int i = 0; i < this.scanners.length; i++) {\n@@ -1928,14 +1932,6 @@ public void close() {\n       }\n     }\n \n-    /**\n-     * @return an iterator for the scanner\n-     */\n-    public Iterator<Entry<HStoreKey, SortedMap<Text, byte[]>>> iterator() {\n-      throw new UnsupportedOperationException(\"Unimplemented serverside. \" +\n-        \"next(HStoreKey, StortedMap(...) is more efficient\");\n-    }\n-    \n     /** {@inheritDoc} */\n     public boolean isWildcardScanner() {\n       throw new UnsupportedOperationException(\"Unimplemented on HScanner\");",
                "deletions": 30
            },
            {
                "sha": "9caa00579adf4148065635229f632e7a805800d6",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "status": "modified",
                "changes": 15,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -584,7 +584,7 @@ static HStoreKey stripTimestamp(HStoreKey key) {\n       HStoreKey key = es.getKey();\n   \n       // if there's no column name, then compare rows and timestamps\n-      if (origin.getColumn().length == 0) {\n+      if (origin.getColumn() != null && origin.getColumn().length == 0) {\n         // if the current and origin row don't match, then we can jump\n         // out of the loop entirely.\n         if (!Bytes.equals(key.getRow(), origin.getRow())) {\n@@ -697,6 +697,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], byte []> results)\n       if (results.size() > 0) {\n         results.clear();\n       }\n+      long latestTimestamp = -1;\n       while (results.size() <= 0 && this.currentRow != null) {\n         if (deletes.size() > 0) {\n           deletes.clear();\n@@ -723,11 +724,23 @@ public boolean next(HStoreKey key, SortedMap<byte [], byte []> results)\n               continue;\n             }\n           }\n+          // We should never return HConstants.LATEST_TIMESTAMP as the time for\n+          // the row. As a compromise, we return the largest timestamp for the\n+          // entries that we find that match.\n+          if (c.getTimestamp() != HConstants.LATEST_TIMESTAMP &&\n+              c.getTimestamp() > latestTimestamp) {\n+            latestTimestamp = c.getTimestamp();\n+          }\n           results.put(column, c.getValue());\n         }\n         this.currentRow = getNextRow(this.currentRow);\n \n       }\n+      // Set the timestamp to the largest one for the row if we would otherwise\n+      // return HConstants.LATEST_TIMESTAMP\n+      if (key.getTimestamp() == HConstants.LATEST_TIMESTAMP) {\n+        key.setVersion(latestTimestamp);\n+      }\n       return results.size() > 0;\n     }\n ",
                "deletions": 1
            },
            {
                "sha": "24e9d5699de25c16f9922d118b276ce3bd9a4a07",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "status": "modified",
                "changes": 34,
                "additions": 27,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -49,6 +49,13 @@\n   // Used around replacement of Readers if they change while we're scanning.\n   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n   \n+  /**\n+   * @param store\n+   * @param timestamp\n+   * @param targetCols\n+   * @param firstRow\n+   * @throws IOException\n+   */\n   public StoreFileScanner(final HStore store, final long timestamp,\n     final byte [][] targetCols, final byte [] firstRow)\n   throws IOException {\n@@ -209,7 +216,7 @@ long getTimestamp() {\n       return this.ts;\n     }\n   }\n-  \n+\n   /*\n    * @return An instance of <code>ViableRow</code>\n    * @throws IOException\n@@ -221,9 +228,21 @@ private ViableRow getNextViableRow() throws IOException {\n     long now = System.currentTimeMillis();\n     long ttl = store.ttl;\n     for(int i = 0; i < keys.length; i++) {\n+      // The first key that we find that matches may have a timestamp greater\n+      // than the one we're looking for. We have to advance to see if there\n+      // is an older version present, since timestamps are sorted descending\n+      while (keys[i] != null &&\n+          keys[i].getTimestamp() > this.timestamp &&\n+          columnMatch(i) &&\n+          getNext(i)) {\n+        if (columnMatch(i)) {\n+          break;\n+        }\n+      }\n       if((keys[i] != null)\n-          && (columnMatch(i))\n-          && (keys[i].getTimestamp() <= this.timestamp)\n+          // If we get here and keys[i] is not null, we already know that the\n+          // column matches and the timestamp of the row is less than or equal\n+          // to this.timestamp, so we do not need to test that here\n           && ((viableRow == null)\n               || (Bytes.compareTo(keys[i].getRow(), viableRow) < 0)\n               || ((Bytes.compareTo(keys[i].getRow(), viableRow) == 0)\n@@ -293,10 +312,9 @@ private boolean getNext(int i) throws IOException {\n           vals[i] = ibw.get();\n           result = true;\n           break;\n-        } else {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"getNext: \" + keys[i] + \": expired, skipped\");\n-          }\n+        }\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"getNext: \" + keys[i] + \": expired, skipped\");\n         }\n       }\n     }\n@@ -343,6 +361,8 @@ public void close() {\n   }\n \n   // Implementation of ChangedReadersObserver\n+  \n+  /** {@inheritDoc} */\n   public void updateReaders() throws IOException {\n     this.lock.writeLock().lock();\n     try {",
                "deletions": 7
            },
            {
                "sha": "c703705a16f1b2506a2ef6a7f3de732945871094",
                "filename": "src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "status": "modified",
                "changes": 15,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -99,21 +99,14 @@ public void preHBaseClusterSetup() throws Exception {\n     \n     // Now create the root and meta regions and insert the data regions\n     // created above into the meta\n-    \n-    HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-      testDir, this.conf);\n-    HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-      testDir, this.conf);\n-    HRegion.addRegionToMETA(root, meta);\n+\n+    createRootAndMetaRegions();\n     \n     for(int i = 0; i < regions.length; i++) {\n       HRegion.addRegionToMETA(meta, regions[i]);\n     }\n-    \n-    root.close();\n-    root.getLog().closeAndDelete();\n-    meta.close();\n-    meta.getLog().closeAndDelete();\n+\n+    closeRootAndMeta();\n   }\n \n   private HRegion createAregion(byte [] startKey, byte [] endKey, int firstRow,",
                "deletions": 11
            },
            {
                "sha": "d4a38900cec855b1e5b178fc7ae05ef8d07686c8",
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "status": "modified",
                "changes": 20,
                "additions": 20,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -60,6 +60,8 @@\n   private boolean localfs = false;\n   protected Path testDir = null;\n   protected FileSystem fs = null;\n+  protected HRegion root = null;\n+  protected HRegion meta = null;\n   protected static final char FIRST_CHAR = 'a';\n   protected static final char LAST_CHAR = 'z';\n   protected static final String PUNCTUATION = \"~`@#$%^&*()-_+=:;',.<>/?[]{}|\";\n@@ -626,4 +628,22 @@ public static void shutdownDfs(MiniDFSCluster cluster) {\n       }\n     }\n   }\n+  \n+  protected void createRootAndMetaRegions() throws IOException {\n+    root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO, testDir, conf);\n+    meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO, testDir, \n+        conf);\n+    HRegion.addRegionToMETA(root, meta);\n+  }\n+  \n+  protected void closeRootAndMeta() throws IOException {\n+    if (meta != null) {\n+      meta.close();\n+      meta.getLog().closeAndDelete();\n+    }\n+    if (root != null) {\n+      root.close();\n+      root.getLog().closeAndDelete();\n+    }\n+  }\n }",
                "deletions": 0
            },
            {
                "sha": "17d48dca011a13abaee36cded4ffc20a5ce23145",
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "status": "modified",
                "changes": 12,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -85,16 +85,14 @@ protected void preHBaseClusterSetup() throws Exception {\n \n       // Now create the root and meta regions and insert the data regions\n       // created above into the meta\n-      HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-          testDir, this.conf);\n-      HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-          testDir, this.conf);\n-      HRegion.addRegionToMETA(root, meta);\n+\n+      createRootAndMetaRegions();\n+\n       for(int i = 0; i < regions.length; i++) {\n         HRegion.addRegionToMETA(meta, regions[i]);\n       }\n-      closeRegionAndDeleteLog(root);\n-      closeRegionAndDeleteLog(meta);\n+      \n+      closeRootAndMeta();\n     } catch (Exception e) {\n       shutdownDfs(dfsCluster);\n       throw e;",
                "deletions": 7
            },
            {
                "sha": "f3b59ad008ccf7a92e2c9bf2b2130cab617dc17d",
                "filename": "src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java",
                "status": "modified",
                "changes": 11,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -84,20 +84,13 @@ public void preHBaseClusterSetup() throws IOException {\n     // Now create the root and meta regions and insert the data regions\n     // created above into the meta\n     \n-    HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-      testDir, conf);\n-    HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-      testDir, conf);\n-    HRegion.addRegionToMETA(root, meta);\n+    createRootAndMetaRegions();\n     \n     for (HRegion region : regions) {\n       HRegion.addRegionToMETA(meta, region);\n     }\n     \n-    root.close();\n-    root.getLog().closeAndDelete();\n-    meta.close();\n-    meta.getLog().closeAndDelete();\n+    closeRootAndMeta();\n   }\n   \n   /**",
                "deletions": 9
            },
            {
                "sha": "0af704708df469e36c8fd271418c15ac7704d29e",
                "filename": "src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java",
                "status": "added",
                "changes": 167,
                "additions": 167,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -0,0 +1,167 @@\n+/**\n+ * Copyright 2008 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.io.BatchUpdate;\n+import org.apache.hadoop.hbase.regionserver.HRegion;\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * Regression test for HBASE-613\n+ */\n+public class TestScanMultipleVersions extends HBaseClusterTestCase {\n+  private final byte[] TABLE_NAME = Bytes.toBytes(\"TestScanMultipleVersions\");\n+  private final HRegionInfo[] INFOS = new HRegionInfo[2];\n+  private final HRegion[] REGIONS = new HRegion[2];\n+  private final byte[][] ROWS = new byte[][] {\n+      Bytes.toBytes(\"row_0200\"),\n+      Bytes.toBytes(\"row_0800\")\n+  };\n+  private final long[] TIMESTAMPS = new long[] {\n+      100L,\n+      1000L\n+  };\n+  private HTableDescriptor desc = null;\n+\n+  @Override\n+  protected void preHBaseClusterSetup() throws Exception {\n+    testDir = new Path(conf.get(HConstants.HBASE_DIR));\n+    \n+    // Create table description\n+    \n+    this.desc = new HTableDescriptor(TABLE_NAME);\n+    this.desc.addFamily(new HColumnDescriptor(HConstants.COLUMN_FAMILY));\n+\n+    // Region 0 will contain the key range [,row_0500)\n+    INFOS[0] = new HRegionInfo(this.desc, HConstants.EMPTY_START_ROW,\n+        Bytes.toBytes(\"row_0500\"));\n+    // Region 1 will contain the key range [row_0500,)\n+    INFOS[1] = new HRegionInfo(this.desc, Bytes.toBytes(\"row_0500\"),\n+        HConstants.EMPTY_END_ROW);\n+\n+    // Create root and meta regions\n+    createRootAndMetaRegions();\n+    // Create the regions\n+    for (int i = 0; i < REGIONS.length; i++) {\n+      REGIONS[i] =\n+        HRegion.createHRegion(this.INFOS[i], this.testDir, this.conf);\n+      // Insert data\n+      for (int j = 0; j < TIMESTAMPS.length; j++) {\n+        BatchUpdate b = new BatchUpdate(ROWS[i], TIMESTAMPS[j]);\n+        b.put(HConstants.COLUMN_FAMILY, Bytes.toBytes(TIMESTAMPS[j]));\n+        REGIONS[i].batchUpdate(b);\n+      }\n+      // Insert the region we created into the meta\n+      HRegion.addRegionToMETA(meta, REGIONS[i]);\n+      // Close region\n+      REGIONS[i].close();\n+      REGIONS[i].getLog().closeAndDelete();\n+    }\n+    // Close root and meta regions\n+    closeRootAndMeta();\n+  }\n+  \n+  /**\n+   * @throws Exception\n+   */\n+  public void testScanMultipleVersions() throws Exception {\n+    // At this point we have created multiple regions and both HDFS and HBase\n+    // are running. There are 5 cases we have to test. Each is described below.\n+\n+    HTable t = new HTable(conf, TABLE_NAME);\n+    \n+    // Case 1: scan with LATEST_TIMESTAMP. Should get two rows\n+    \n+    int count = 0;\n+    Scanner s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+\n+    // Case 2: Scan with a timestamp greater than most recent timestamp\n+    // (in this case > 1000 and < LATEST_TIMESTAMP. Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        10000L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+    \n+    // Case 3: scan with timestamp equal to most recent timestamp\n+    // (in this case == 1000. Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        1000L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+    \n+    // Case 4: scan with timestamp greater than first timestamp but less than\n+    // second timestamp (100 < timestamp < 1000). Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        500L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+    \n+    // Case 5: scan with timestamp equal to first timestamp (100)\n+    // Should get 2 rows.\n+    \n+    count = 0;\n+    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,\n+        100L);\n+    try {\n+      while (s.next() != null) {\n+        count += 1;\n+      }\n+      assertEquals(\"Number of rows should be 2\", 2, count);\n+    } finally {\n+      s.close();\n+    }\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "e9892d98a5a5dcbd35a3177576bbf5fdb26e8754",
                "filename": "src/test/org/apache/hadoop/hbase/util/TestMergeTool.java",
                "blob_url": "https://github.com/apache/hbase/blob/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java",
                "raw_url": "https://github.com/apache/hbase/raw/80fa33698fc4c6aed02c2fd33b150e1eb543cfe1/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java",
                "status": "modified",
                "changes": 23,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java?ref=80fa33698fc4c6aed02c2fd33b150e1eb543cfe1",
                "patch": "@@ -47,7 +47,6 @@\n   private final HRegion[] regions = new HRegion[5];\n   private HTableDescriptor desc;\n   private byte [][][] rows;\n-  private Path rootdir = null;\n   private MiniDFSCluster dfsCluster = null;\n   private FileSystem fs;\n   \n@@ -102,9 +101,6 @@ public void setUp() throws Exception {\n     // Start up dfs\n     this.dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n     this.fs = this.dfsCluster.getFileSystem();\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.rootdir = new Path(this.fs.getHomeDirectory(), \"hbase\");\n-    this.conf.set(HConstants.HBASE_DIR, this.rootdir.toString());\n     \n     // Note: we must call super.setUp after starting the mini cluster or\n     // we will end up with a local file system\n@@ -117,7 +113,7 @@ public void setUp() throws Exception {\n        */\n       for (int i = 0; i < sourceRegions.length; i++) {\n         regions[i] =\n-          HRegion.createHRegion(this.sourceRegions[i], this.rootdir, this.conf);\n+          HRegion.createHRegion(this.sourceRegions[i], this.testDir, this.conf);\n         /*\n          * Insert data\n          */\n@@ -128,23 +124,14 @@ public void setUp() throws Exception {\n           regions[i].batchUpdate(b);\n         }\n       }\n-      // Create root region\n-      HRegion root = HRegion.createHRegion(HRegionInfo.ROOT_REGIONINFO,\n-          this.rootdir, this.conf);\n-      // Create meta region\n-      HRegion meta = HRegion.createHRegion(HRegionInfo.FIRST_META_REGIONINFO,\n-          this.rootdir, this.conf);\n-      // Insert meta into root region\n-      HRegion.addRegionToMETA(root, meta);\n+      // Create root and meta regions\n+      createRootAndMetaRegions();\n       // Insert the regions we created into the meta\n       for(int i = 0; i < regions.length; i++) {\n         HRegion.addRegionToMETA(meta, regions[i]);\n       }\n       // Close root and meta regions\n-      root.close();\n-      root.getLog().closeAndDelete();\n-      meta.close();\n-      meta.getLog().closeAndDelete();\n+      closeRootAndMeta();\n       \n     } catch (Exception e) {\n       shutdownDfs(dfsCluster);\n@@ -182,7 +169,7 @@ private HRegion mergeAndVerify(final String msg, final String regionName1,\n     // Now verify that we can read all the rows from regions 0, 1\n     // in the new merged region.\n     HRegion merged =\n-      HRegion.openHRegion(mergedInfo, this.rootdir, log, this.conf);\n+      HRegion.openHRegion(mergedInfo, this.testDir, log, this.conf);\n     verifyMerge(merged, upperbound);\n     merged.close();\n     LOG.info(\"Verified \" + msg);",
                "deletions": 18
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-536 Remove MiniDFS startup from MiniHBaseCluster\n-Changed MiniHBaseCluster to not start up a MiniDFS\n-Changed HBaseClusterTestCase to do the work of starting up a MiniDFS.\n-Added pre and post setup method to HBaseClusterTestCase so you can control what happen before MiniHBaseCluster is booted up\n-Converted AbstractMergeTestCase to be a HBaseClusterTestCase\n-Converted any test that used a raw MIniDFS or MiniHBaseCluster to use HBaseClusterTestCase instead\n-Split TestTimestamp into two tests - one for clientside (now in o.a.h.h.client) and one for serverside (o.a.h.h.regionserver)\n-Merged in Stack's changes to make bin/hbase have hadoop jars first on the classpath\n-Updated PerformanceEvaluation (in --miniCluster mode) to start up a DFS first\n-Fixed a bug in BaseScanner that would have allowed NPEs to be generated\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@640526 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/65d961ae783475b04e2b25b13d38871c4f4d13b7",
        "parent": "https://github.com/apache/hbase/commit/aabe9f09a91b5612170975d7085c73636f51d2d1",
        "bug_id": "hbase_319",
        "file": [
            {
                "sha": "cfd14ca6256c2d3d4e650c8d0da46b542579bc5b",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -103,6 +103,7 @@ Hbase Change Log\n    HBASE-515   At least double default timeouts between regionserver and master\r\n    HBASE-529   RegionServer needs to recover if datanode goes down\r\n    HBASE-456   Clearly state which ports need to be opened in order to run HBase\r\n+   HBASE-536   Remove MiniDFS startup from MiniHBaseCluster\r\n    \r\n Branch 0.1\r\n \r",
                "deletions": 0
            },
            {
                "sha": "044c07560490bb2c63948e0e70b6b5d90866c5dd",
                "filename": "bin/hbase",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/bin/hbase",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/bin/hbase",
                "status": "modified",
                "changes": 20,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/bin/hbase?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -97,6 +97,18 @@ if [ \"$HBASE_HEAPSIZE\" != \"\" ]; then\n   #echo $JAVA_HEAP_MAX\n fi\n \n+# so that filenames w/ spaces are handled correctly in loops below\n+IFS=\n+\n+# Add libs to CLASSPATH\n+# Do this early so hadoop jar comes before hbase classes; otherwise\n+# complaint because way webapps are loaded, expectation is that\n+# loading class is from same jar as webapps themselves (only true\n+# if hadoop comes first).\n+for f in $HBASE_HOME/lib/*.jar; do\n+  CLASSPATH=${CLASSPATH}:$f;\n+done\n+\n # CLASSPATH initially contains $HBASE_CONF_DIR\n CLASSPATH=\"${CLASSPATH}:${HBASE_CONF_DIR}\"\n CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar\n@@ -112,9 +124,6 @@ if [ -d \"$HBASE_HOME/build/webapps\" ]; then\n   CLASSPATH=${CLASSPATH}:$HBASE_HOME/build\n fi\n \n-# so that filenames w/ spaces are handled correctly in loops below\n-IFS=\n-\n # for releases, add hbase, hadoop & webapps to CLASSPATH\n for f in $HBASE_HOME/hbase*.jar; do\n   if [ -f $f ]; then\n@@ -125,11 +134,6 @@ if [ -d \"$HBASE_HOME/webapps\" ]; then\n   CLASSPATH=${CLASSPATH}:$HBASE_HOME\n fi\n \n-# add libs to CLASSPATH\n-for f in $HBASE_HOME/lib/*.jar; do\n-  CLASSPATH=${CLASSPATH}:$f;\n-done\n-\n for f in $HBASE_HOME/lib/jetty-ext/*.jar; do\n   CLASSPATH=${CLASSPATH}:$f;\n done",
                "deletions": 8
            },
            {
                "sha": "dcdced9f2a3c475ac7bd4cf17236629a4f555afd",
                "filename": "src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/java/org/apache/hadoop/hbase/master/BaseScanner.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/BaseScanner.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -308,7 +308,7 @@ private boolean hasReferences(final Text metaRegionName,\n   throws IOException {\n     boolean result = false;\n     HRegionInfo split =\n-      Writables.getHRegionInfoOrNull(rowContent.get(splitColumn).getValue());\n+      Writables.getHRegionInfo(rowContent.get(splitColumn));\n     if (split == null) {\n       return result;\n     }",
                "deletions": 1
            },
            {
                "sha": "51564c78b5ab0865a307de3893921ddb3b879df1",
                "filename": "src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java",
                "status": "modified",
                "changes": 98,
                "additions": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -31,18 +31,25 @@\n import org.apache.hadoop.hbase.regionserver.HRegion;\n \n /** Abstract base class for merge tests */\n-public abstract class AbstractMergeTestBase extends HBaseTestCase {\n+public abstract class AbstractMergeTestBase extends HBaseClusterTestCase {\n   static final Logger LOG =\n     Logger.getLogger(AbstractMergeTestBase.class.getName());\n   protected static final Text COLUMN_NAME = new Text(\"contents:\");\n   protected final Random rand = new Random();\n   protected HTableDescriptor desc;\n   protected ImmutableBytesWritable value;\n-\n-  /** constructor */\n+  protected boolean startMiniHBase;\n+  \n   public AbstractMergeTestBase() {\n+    this(true);\n+  }\n+  \n+  /** constructor */\n+  public AbstractMergeTestBase(boolean startMiniHBase) {\n     super();\n     \n+    this.startMiniHBase = startMiniHBase;\n+    \n     // We will use the same value for the rows as that is not really important here\n     \n     String partialValue = String.valueOf(System.currentTimeMillis());\n@@ -61,72 +68,53 @@ public AbstractMergeTestBase() {\n     desc.addFamily(new HColumnDescriptor(COLUMN_NAME.toString()));\n   }\n \n-  protected MiniDFSCluster dfsCluster = null;\n+  @Override\n+  protected void HBaseClusterSetup() throws Exception {\n+    if (startMiniHBase) {\n+      super.HBaseClusterSetup();\n+    }\n+  }\n \n   /**\n    * {@inheritDoc}\n    */\n   @Override\n-  public void setUp() throws Exception {\n+  public void preHBaseClusterSetup() throws Exception {\n     conf.setLong(\"hbase.hregion.max.filesize\", 64L * 1024L * 1024L);\n-    dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.dfsCluster.getFileSystem().getHomeDirectory().toString());\n-    \n-    // Note: we must call super.setUp after starting the mini cluster or\n-    // we will end up with a local file system\n-    \n-    super.setUp();\n-      \n+\n     // We create three data regions: The first is too large to merge since it \n     // will be > 64 MB in size. The second two will be smaller and will be \n     // selected for merging.\n     \n     // To ensure that the first region is larger than 64MB we need to write at\n     // least 65536 rows. We will make certain by writing 70000\n \n-    try {\n-      Text row_70001 = new Text(\"row_70001\");\n-      Text row_80001 = new Text(\"row_80001\");\n-      \n-      HRegion[] regions = {\n-        createAregion(null, row_70001, 1, 70000),\n-        createAregion(row_70001, row_80001, 70001, 10000),\n-        createAregion(row_80001, null, 80001, 10000)\n-      };\n-      \n-      // Now create the root and meta regions and insert the data regions\n-      // created above into the meta\n-      \n-      HRegion root = HRegion.createHRegion(HRegionInfo.rootRegionInfo,\n-          testDir, this.conf);\n-      HRegion meta = HRegion.createHRegion(HRegionInfo.firstMetaRegionInfo,\n-        testDir, this.conf);\n-      HRegion.addRegionToMETA(root, meta);\n-      \n-      for(int i = 0; i < regions.length; i++) {\n-        HRegion.addRegionToMETA(meta, regions[i]);\n-      }\n-      \n-      root.close();\n-      root.getLog().closeAndDelete();\n-      meta.close();\n-      meta.getLog().closeAndDelete();\n-      \n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfsCluster);\n-      throw e;\n+    Text row_70001 = new Text(\"row_70001\");\n+    Text row_80001 = new Text(\"row_80001\");\n+    \n+    HRegion[] regions = {\n+      createAregion(null, row_70001, 1, 70000),\n+      createAregion(row_70001, row_80001, 70001, 10000),\n+      createAregion(row_80001, null, 80001, 10000)\n+    };\n+    \n+    // Now create the root and meta regions and insert the data regions\n+    // created above into the meta\n+    \n+    HRegion root = HRegion.createHRegion(HRegionInfo.rootRegionInfo,\n+      testDir, this.conf);\n+    HRegion meta = HRegion.createHRegion(HRegionInfo.firstMetaRegionInfo,\n+      testDir, this.conf);\n+    HRegion.addRegionToMETA(root, meta);\n+    \n+    for(int i = 0; i < regions.length; i++) {\n+      HRegion.addRegionToMETA(meta, regions[i]);\n     }\n-  }\n-\n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void tearDown() throws Exception {\n-    super.tearDown();\n-    StaticTestEnvironment.shutdownDfs(dfsCluster);\n+    \n+    root.close();\n+    root.getLog().closeAndDelete();\n+    meta.close();\n+    meta.getLog().closeAndDelete();\n   }\n \n   private HRegion createAregion(Text startKey, Text endKey, int firstRow,",
                "deletions": 55
            },
            {
                "sha": "cba88debb3cd72b6f2309266b8ccb83a7101e63c",
                "filename": "src/test/org/apache/hadoop/hbase/DFSAbort.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/DFSAbort.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/DFSAbort.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/DFSAbort.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -57,7 +57,7 @@ public void testDFSAbort() throws Exception {\n     try {\n       // By now the Mini DFS is running, Mini HBase is running and we have\n       // created a table. Now let's yank the rug out from HBase\n-      cluster.getDFSCluster().shutdown();\n+      dfsCluster.shutdown();\n       threadDumpingJoin();\n     } catch (Exception e) {\n       e.printStackTrace();",
                "deletions": 1
            },
            {
                "sha": "bb8cf975447bcae722d3130a4c1205db6c216b84",
                "filename": "src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java",
                "status": "modified",
                "changes": 125,
                "additions": 90,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseClusterTestCase.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -21,10 +21,16 @@\n \n import java.io.PrintWriter;\n \n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.util.ReflectionUtils;\n import org.apache.hadoop.hbase.client.HConnectionManager;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.io.Text;\n \n /**\n  * Abstract base class for HBase cluster junit tests.  Spins up an hbase\n@@ -35,67 +41,116 @@\n     LogFactory.getLog(HBaseClusterTestCase.class.getName());\n   \n   protected MiniHBaseCluster cluster;\n-  final boolean miniHdfs;\n-  int regionServers;\n+  protected MiniDFSCluster dfsCluster;\n+  protected int regionServers;\n+  protected boolean startDfs;\n   \n-  /**\n-   * constructor\n-   */\n   public HBaseClusterTestCase() {\n-    this(true);\n+    this(1);\n   }\n   \n   /**\n-   * @param regionServers\n-   */\n+   * Start a MiniHBaseCluster with regionServers region servers in-process to\n+   * start with. Also, start a MiniDfsCluster before starting the hbase cluster.\n+   * The configuration used will be edited so that this works correctly.\n+   */  \n   public HBaseClusterTestCase(int regionServers) {\n-    this(true);\n-    this.regionServers = regionServers;\n+    this(regionServers, true);\n   }\n-\n+  \n   /**\n-   * @param name\n+   * Start a MiniHBaseCluster with regionServers region servers in-process to\n+   * start with. Optionally, startDfs indicates if a MiniDFSCluster should be\n+   * started. If startDfs is false, the assumption is that an external DFS is\n+   * configured in hbase-site.xml and is already started, or you have started a\n+   * MiniDFSCluster on your own and edited the configuration in memory. (You \n+   * can modify the config used by overriding the preHBaseClusterSetup method.)\n    */\n-  public HBaseClusterTestCase(String name) {\n-    this(name, true);\n+  public HBaseClusterTestCase(int regionServers, boolean startDfs) {\n+    super();\n+    this.startDfs = startDfs;\n+    this.regionServers = regionServers;\n   }\n   \n   /**\n-   * @param miniHdfs\n+   * Run after dfs is ready but before hbase cluster is started up.\n    */\n-  public HBaseClusterTestCase(final boolean miniHdfs) {\n-    super();\n-    this.miniHdfs = miniHdfs;\n-    this.regionServers = 1;\n-  }\n+  protected void preHBaseClusterSetup() throws Exception {\n+  } \n \n   /**\n-   * @param name\n-   * @param miniHdfs\n+   * Actually start the MiniHBase instance.\n    */\n-  public HBaseClusterTestCase(String name, final boolean miniHdfs) {\n-    super(name);\n-    this.miniHdfs = miniHdfs;\n-    this.regionServers = 1;\n+  protected void HBaseClusterSetup() throws Exception {\n+    // start the mini cluster\n+    this.cluster = new MiniHBaseCluster(conf, regionServers);\n+    HTable meta = new HTable(conf, new Text(\".META.\"));\n   }\n+  \n+  /**\n+   * Run after hbase cluster is started up.\n+   */\n+  protected void postHBaseClusterSetup() throws Exception {\n+  } \n \n   @Override\n   protected void setUp() throws Exception {\n-    this.cluster =\n-      new MiniHBaseCluster(this.conf, this.regionServers, this.miniHdfs);\n-    super.setUp();\n+    try {\n+      if (startDfs) {\n+        // start up the dfs\n+        dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n+\n+        // mangle the conf so that the fs parameter points to the minidfs we\n+        // just started up\n+        FileSystem fs = dfsCluster.getFileSystem();\n+        conf.set(\"fs.default.name\", fs.getName());      \n+        Path parentdir = fs.getHomeDirectory();\n+        conf.set(HConstants.HBASE_DIR, parentdir.toString());\n+        fs.mkdirs(parentdir);\n+        FSUtils.setVersion(fs, parentdir);\n+      }\n+\n+      // do the super setup now. if we had done it first, then we would have\n+      // gotten our conf all mangled and a local fs started up.\n+      super.setUp();\n+    \n+      // run the pre-cluster setup\n+      preHBaseClusterSetup();    \n+    \n+      // start the instance\n+      HBaseClusterSetup();\n+      \n+      // run post-cluster setup\n+      postHBaseClusterSetup();\n+    } catch (Exception e) {\n+      LOG.error(\"Exception in setup!\", e);\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+      if (dfsCluster != null) {\n+        StaticTestEnvironment.shutdownDfs(dfsCluster);\n+      }\n+      throw e;\n+    }\n   }\n \n   @Override\n   protected void tearDown() throws Exception {\n     super.tearDown();\n-    HConnectionManager.deleteConnection(conf);\n-    if (this.cluster != null) {\n-      try {\n-        this.cluster.shutdown();\n-      } catch (Exception e) {\n-        LOG.warn(\"Closing mini dfs\", e);\n+    try {\n+      HConnectionManager.deleteConnection(conf);\n+      if (this.cluster != null) {\n+        try {\n+          this.cluster.shutdown();\n+        } catch (Exception e) {\n+          LOG.warn(\"Closing mini dfs\", e);\n+        }\n+      }\n+      if (startDfs) {\n+        StaticTestEnvironment.shutdownDfs(dfsCluster);\n       }\n+    } catch (Exception e) {\n+      LOG.error(e);\n     }\n     // ReflectionUtils.printThreadInfo(new PrintWriter(System.out),\n     //  \"Temporary end-of-test thread dump debugging HADOOP-2040: \" + getName());",
                "deletions": 35
            },
            {
                "sha": "4c169387fec623ebf48cf5df728e64cd3f274bf0",
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "status": "modified",
                "changes": 122,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -1,5 +1,5 @@\n /**\n- * Copyright 2007 The Apache Software Foundation\n+ * Copyright 2008 The Apache Software Foundation\n  *\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n@@ -23,7 +23,6 @@\n import java.io.IOException;\n import java.util.List;\n \n-import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.log4j.Logger;\n@@ -42,111 +41,23 @@\n     Logger.getLogger(MiniHBaseCluster.class.getName());\n   \n   private HBaseConfiguration conf;\n-  private MiniDFSCluster cluster;\n-  private FileSystem fs;\n-  private boolean shutdownDFS;\n-  private Path parentdir;\n   private LocalHBaseCluster hbaseCluster;\n-  private boolean deleteOnExit = true;\n \n   /**\n-   * Starts a MiniHBaseCluster on top of a new MiniDFSCluster\n-   *\n-   * @param conf\n-   * @param nRegionNodes\n-   * @throws IOException\n+   * Start a MiniHBaseCluster. conf is assumed to contain a valid fs name to \n+   * hook up to.\n    */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes)\n+  public MiniHBaseCluster(HBaseConfiguration conf, int numRegionServers) \n   throws IOException {\n-    this(conf, nRegionNodes, true, true, true);\n-  }\n-\n-  /**\n-   * Start a MiniHBaseCluster. Use the native file system unless\n-   * miniHdfsFilesystem is set to true.\n-   *\n-   * @param conf\n-   * @param nRegionNodes\n-   * @param miniHdfsFilesystem\n-   * @throws IOException\n-   */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n-      final boolean miniHdfsFilesystem) throws IOException {\n-    this(conf, nRegionNodes, miniHdfsFilesystem, true, true);\n-  }\n-\n-  /**\n-   * Starts a MiniHBaseCluster on top of an existing HDFSCluster\n-   *<pre>\n-   ****************************************************************************\n-   *            *  *  *  *  *  N O T E  *  *  *  *  *\n-   *\n-   * If you use this constructor, you should shut down the mini dfs cluster\n-   * in your test case.\n-   *\n-   *            *  *  *  *  *  N O T E  *  *  *  *  *\n-   ****************************************************************************\n-   *</pre>\n-   *\n-   * @param conf\n-   * @param nRegionNodes\n-   * @param dfsCluster\n-   * @param deleteOnExit\n-   * @throws IOException\n-   */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n-      MiniDFSCluster dfsCluster, boolean deleteOnExit) throws IOException {\n-\n     this.conf = conf;\n-    this.fs = dfsCluster.getFileSystem();\n-    this.cluster = dfsCluster;\n-    this.shutdownDFS = false;\n-    this.deleteOnExit = deleteOnExit;\n-    init(nRegionNodes);\n-  }\n-\n-  /**\n-   * Constructor.\n-   * @param conf\n-   * @param nRegionNodes\n-   * @param miniHdfsFilesystem If true, set the hbase mini\n-   * cluster atop a mini hdfs cluster.  Otherwise, use the\n-   * filesystem configured in <code>conf</code>.\n-   * @param format the mini hdfs cluster\n-   * @param deleteOnExit clean up mini hdfs files\n-   * @throws IOException\n-   */\n-  public MiniHBaseCluster(HBaseConfiguration conf, int nRegionNodes,\n-      final boolean miniHdfsFilesystem, boolean format, boolean deleteOnExit)\n-    throws IOException {\n-\n-    this.conf = conf;\n-    this.deleteOnExit = deleteOnExit;\n-    this.shutdownDFS = false;\n-    if (miniHdfsFilesystem) {\n-      try {\n-        this.cluster = new MiniDFSCluster(this.conf, 2, format, (String[])null);\n-        this.fs = cluster.getFileSystem();\n-        this.shutdownDFS = true;\n-      } catch (IOException e) {\n-        StaticTestEnvironment.shutdownDfs(cluster);\n-        throw e;\n-      }\n-    } else {\n-      this.cluster = null;\n-      this.fs = FileSystem.get(conf);\n-    }\n-    init(nRegionNodes);\n+    init(numRegionServers);\n   }\n \n   private void init(final int nRegionNodes) throws IOException {\n     try {\n-      this.parentdir = this.fs.getHomeDirectory();\n-      this.conf.set(HConstants.HBASE_DIR, this.parentdir.toString());\n-      this.fs.mkdirs(parentdir);\n-      FSUtils.setVersion(fs, parentdir);\n-      this.hbaseCluster = new LocalHBaseCluster(this.conf, nRegionNodes);\n-      this.hbaseCluster.startup();\n+      // start up a LocalHBaseCluster\n+      hbaseCluster = new LocalHBaseCluster(conf, nRegionNodes);\n+      hbaseCluster.startup();\n     } catch(IOException e) {\n       shutdown();\n       throw e;\n@@ -166,15 +77,6 @@ public String startRegionServer() throws IOException {\n     return t.getName();\n   }\n \n-  /**\n-   * Get the cluster on which this HBase cluster is running\n-   *\n-   * @return MiniDFSCluster\n-   */\n-  public MiniDFSCluster getDFSCluster() {\n-    return cluster;\n-  }\n-\n   /**\n    * @return Returns the rpc address actually used by the master server, because\n    * the supplied port is not necessarily the actual port used.\n@@ -240,14 +142,6 @@ public void shutdown() {\n     if (this.hbaseCluster != null) {\n       this.hbaseCluster.shutdown();\n     }\n-    if (shutdownDFS) {\n-      StaticTestEnvironment.shutdownDfs(cluster);\n-    }\n-    // Delete all DFS files\n-    if(deleteOnExit) {\n-      deleteFile(new File(System.getProperty(\n-          StaticTestEnvironment.TEST_DIRECTORY_KEY), \"dfs\"));\n-    }\n   }\n \n   private void deleteFile(File f) {",
                "deletions": 114
            },
            {
                "sha": "716e5c8dbb2d7fc8cd72f8743d90b2c808a2e85f",
                "filename": "src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/MultiRegionTable.java",
                "status": "modified",
                "changes": 8,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MultiRegionTable.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -39,16 +39,14 @@\n /**\n  * Utility class to build a table of multiple regions.\n  */\n-public class MultiRegionTable extends HBaseTestCase {\n+public class MultiRegionTable extends HBaseClusterTestCase {\n   static final Log LOG = LogFactory.getLog(MultiRegionTable.class.getName());\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void setUp() throws Exception {\n+  public MultiRegionTable() {\n+    super();\n     // These are needed for the new and improved Map/Reduce framework\n     System.setProperty(\"hadoop.log.dir\", conf.get(\"hadoop.log.dir\"));\n     conf.set(\"mapred.output.dir\", conf.get(\"hadoop.tmp.dir\"));\n-    super.setUp();\n   }\n \n   /**",
                "deletions": 5
            },
            {
                "sha": "bc321395a5c76200406cae3cd171a30c9d11f6ce",
                "filename": "src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 14,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -31,6 +31,8 @@\n import java.util.regex.Pattern;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.LongWritable;\n@@ -561,7 +563,18 @@ private void runTest(final String cmd) throws IOException {\n     }\n     \n     MiniHBaseCluster hbaseMiniCluster = null;\n+    MiniDFSCluster dfsCluster = null;\n     if (this.miniCluster) {\n+      dfsCluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n+      // mangle the conf so that the fs parameter points to the minidfs we\n+      // just started up\n+      FileSystem fs = dfsCluster.getFileSystem();\n+      conf.set(\"fs.default.name\", fs.getName());      \n+      Path parentdir = fs.getHomeDirectory();\n+      conf.set(HConstants.HBASE_DIR, parentdir.toString());\n+      fs.mkdirs(parentdir);\n+      FSUtils.setVersion(fs, parentdir);\n+      \n       hbaseMiniCluster = new MiniHBaseCluster(this.conf, N);\n     }\n     \n@@ -577,6 +590,7 @@ private void runTest(final String cmd) throws IOException {\n     } finally {\n       if(this.miniCluster && hbaseMiniCluster != null) {\n         hbaseMiniCluster.shutdown();\n+        StaticTestEnvironment.shutdownDfs(dfsCluster);\n       }\n     }\n   }",
                "deletions": 0
            },
            {
                "sha": "f057aa4a78a199ea39927ff2b386f86b4b9b7e48",
                "filename": "src/test/org/apache/hadoop/hbase/TestHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestHBaseCluster.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHBaseCluster.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -40,7 +40,7 @@\n \n   /** constructor */\n   public TestHBaseCluster() {\n-    super(true);\n+    super();\n     this.desc = null;\n     this.admin = null;\n     this.table = null;",
                "deletions": 1
            },
            {
                "sha": "97c3e327a9cddea0b1994d479cb61716dde2ecd5",
                "filename": "src/test/org/apache/hadoop/hbase/TestInfoServers.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestInfoServers.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestInfoServers.java",
                "status": "modified",
                "changes": 48,
                "additions": 18,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestInfoServers.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -26,49 +26,37 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.client.HTable;\n \n /**\n  * Testing, info servers are disabled.  This test enables then and checks that\n  * they serve pages.\n  */\n-public class TestInfoServers extends HBaseTestCase {\n+public class TestInfoServers extends HBaseClusterTestCase {\n   static final Log LOG = LogFactory.getLog(TestInfoServers.class);\n-\n-  @Override  \n-  protected void setUp() throws Exception {\n-    super.setUp();\n-  }\n-\n+  \n   @Override\n-  protected void tearDown() throws Exception {\n-    super.tearDown();\n+  protected void preHBaseClusterSetup() {\n+    // Bring up info servers on 'odd' port numbers in case the test is not\n+    // sourcing the src/test/hbase-default.xml.\n+    conf.setInt(\"hbase.master.info.port\", 60011);\n+    conf.setInt(\"hbase.regionserver.info.port\", 60031);\n   }\n   \n   /**\n    * @throws Exception\n    */\n   public void testInfoServersAreUp() throws Exception {\n-    // Bring up info servers on 'odd' port numbers in case the test is not\n-    // sourcing the src/test/hbase-default.xml.\n-    this.conf.setInt(\"hbase.master.info.port\", 60011);\n-    this.conf.setInt(\"hbase.regionserver.info.port\", 60031);\n-    MiniHBaseCluster miniHbase = new MiniHBaseCluster(this.conf, 1);\n-    // Create table so info servers are given time to spin up.\n-    HBaseAdmin a = new HBaseAdmin(conf);\n-    a.createTable(new HTableDescriptor(getName()));\n-    assertTrue(a.tableExists(new Text(getName())));\n-    try {\n-      int port = miniHbase.getMaster().getInfoServer().getPort();\n-      assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n-        \"/index.html\"), \"Master\");\n-      port = miniHbase.getRegionThreads().get(0).getRegionServer().\n-        getInfoServer().getPort();\n-      assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n-        \"/index.html\"), \"Region Server\");\n-    } finally {\n-      miniHbase.shutdown();\n-    }\n+    // give the cluster time to start up\n+    HTable table = new HTable(conf, new Text(\".META.\"));\n+    \n+    int port = cluster.getMaster().getInfoServer().getPort();\n+    assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n+      \"/index.html\"), \"Master\");\n+    port = cluster.getRegionThreads().get(0).getRegionServer().\n+      getInfoServer().getPort();\n+    assertHasExpectedContent(new URL(\"http://localhost:\" + port +\n+      \"/index.html\"), \"Region Server\");\n   }\n   \n   private void assertHasExpectedContent(final URL u, final String expected)",
                "deletions": 30
            },
            {
                "sha": "57af6176243b4babc1f9ea1fa2c02351eab78eab",
                "filename": "src/test/org/apache/hadoop/hbase/TestMasterAdmin.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMasterAdmin.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMasterAdmin.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestMasterAdmin.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -40,7 +40,7 @@\n \n   /** constructor */\n   public TestMasterAdmin() {\n-    super(true);\n+    super();\n     admin = null;\n \n     // Make the thread wake frequency a little slower so other threads",
                "deletions": 1
            },
            {
                "sha": "69418abfcfce61dcc72ccb3d68c6911195819847",
                "filename": "src/test/org/apache/hadoop/hbase/TestMergeMeta.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeMeta.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeMeta.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestMergeMeta.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -25,8 +25,8 @@\n public class TestMergeMeta extends AbstractMergeTestBase {\n \n   /** constructor */\n-  public TestMergeMeta() {\n-    super();\n+  public TestMergeMeta() throws Exception {\n+    super(false);\n     conf.setLong(\"hbase.client.pause\", 1 * 1000);\n     conf.setInt(\"hbase.client.retries.number\", 2);\n   }",
                "deletions": 2
            },
            {
                "sha": "ec37f9bd74eaf440ecb5d24ce2e6fbeb12e284e2",
                "filename": "src/test/org/apache/hadoop/hbase/TestMergeTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TestMergeTable.java",
                "status": "modified",
                "changes": 7,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestMergeTable.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -32,11 +32,6 @@\n    */\n   public void testMergeTable() throws IOException {\n     assertNotNull(dfsCluster);\n-    MiniHBaseCluster hCluster = new MiniHBaseCluster(conf, 1, dfsCluster, true);\n-    try {\n-      HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n-    } finally {\n-      hCluster.shutdown();\n-    }\n+    HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n   }\n }\n\\ No newline at end of file",
                "deletions": 6
            },
            {
                "sha": "73ab1006bdd43040f370ce6a0d2a627a56d5608b",
                "filename": "src/test/org/apache/hadoop/hbase/TimestampTestBase.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TimestampTestBase.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/TimestampTestBase.java",
                "status": "added",
                "changes": 253,
                "additions": 253,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TimestampTestBase.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -0,0 +1,253 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+import java.util.TreeMap;\n+\n+import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n+import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HStoreKey;\n+import org.apache.hadoop.hbase.HScannerInterface;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.HBaseTestCase;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Tests user specifiable time stamps putting, getting and scanning.  Also\n+ * tests same in presence of deletes.  Test cores are written so can be\n+ * run against an HRegion and against an HTable: i.e. both local and remote.\n+ */\n+public class TimestampTestBase extends HBaseTestCase {\n+  private static final Log LOG =\n+    LogFactory.getLog(TimestampTestBase.class.getName());\n+\n+  private static final long T0 = 10L;\n+  private static final long T1 = 100L;\n+  private static final long T2 = 200L;\n+  \n+  private static final String COLUMN_NAME = \"contents:\";\n+  \n+  private static final Text COLUMN = new Text(COLUMN_NAME);\n+  private static final Text ROW = new Text(\"row\");\n+  \n+  // When creating column descriptor, how many versions of a cell to allow.\n+  private static final int VERSIONS = 3;\n+  \n+    /*\n+   * Run test that delete works according to description in <a\n+   * href=\"https://issues.apache.org/jira/browse/HADOOP-1784\">hadoop-1784</a>.\n+   * @param incommon\n+   * @param flusher\n+   * @throws IOException\n+   */\n+  public static void doTestDelete(final Incommon incommon, FlushCache flusher)\n+  throws IOException {\n+    // Add values at various timestamps (Values are timestampes as bytes).\n+    put(incommon, T0);\n+    put(incommon, T1);\n+    put(incommon, T2);\n+    put(incommon);\n+    // Verify that returned versions match passed timestamps.\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n+    // If I delete w/o specifying a timestamp, this means I'm deleting the\n+    // latest.\n+    delete(incommon);\n+    // Verify that I get back T2 through T1 -- that the latest version has\n+    // been deleted.\n+    assertVersions(incommon, new long [] {T2, T1, T0});\n+    \n+    // Flush everything out to disk and then retry\n+    flusher.flushcache();\n+    assertVersions(incommon, new long [] {T2, T1, T0});\n+    \n+    // Now add, back a latest so I can test remove other than the latest.\n+    put(incommon);\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n+    delete(incommon, T2);\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n+    // Flush everything out to disk and then retry\n+    flusher.flushcache();\n+    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n+    \n+    // Now try deleting all from T2 back inclusive (We first need to add T2\n+    // back into the mix and to make things a little interesting, delete and\n+    // then readd T1.\n+    put(incommon, T2);\n+    delete(incommon, T1);\n+    put(incommon, T1);\n+    incommon.deleteAll(ROW, COLUMN, T2);\n+    // Should only be current value in set.  Assert this is so\n+    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n+    \n+    // Flush everything out to disk and then redo above tests\n+    flusher.flushcache();\n+    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n+  }\n+  \n+  private static void assertOnlyLatest(final Incommon incommon,\n+    final long currentTime)\n+  throws IOException {\n+    Cell[] cellValues = incommon.get(ROW, COLUMN, 3/*Ask for too much*/);\n+    assertEquals(1, cellValues.length);\n+    long time = Writables.bytesToLong(cellValues[0].getValue());\n+    assertEquals(time, currentTime);\n+    assertNull(incommon.get(ROW, COLUMN, T1, 3 /*Too many*/));\n+    assertTrue(assertScanContentTimestamp(incommon, T1) == 0);\n+  }\n+  \n+  /*\n+   * Assert that returned versions match passed in timestamps and that results\n+   * are returned in the right order.  Assert that values when converted to\n+   * longs match the corresponding passed timestamp.\n+   * @param r\n+   * @param tss\n+   * @throws IOException\n+   */\n+  public static void assertVersions(final Incommon incommon, final long [] tss)\n+  throws IOException {\n+    // Assert that 'latest' is what we expect.\n+    byte [] bytes = incommon.get(ROW, COLUMN).getValue();\n+    assertEquals(Writables.bytesToLong(bytes), tss[0]);\n+    // Now assert that if we ask for multiple versions, that they come out in\n+    // order.\n+    Cell[] cellValues = incommon.get(ROW, COLUMN, tss.length);\n+    assertEquals(tss.length, cellValues.length);\n+    for (int i = 0; i < cellValues.length; i++) {\n+      long ts = Writables.bytesToLong(cellValues[i].getValue());\n+      assertEquals(ts, tss[i]);\n+    }\n+    // Specify a timestamp get multiple versions.\n+    cellValues = incommon.get(ROW, COLUMN, tss[0], cellValues.length - 1);\n+    for (int i = 1; i < cellValues.length; i++) {\n+      long ts = Writables.bytesToLong(cellValues[i].getValue());\n+      assertEquals(ts, tss[i]);\n+    }\n+    // Test scanner returns expected version\n+    assertScanContentTimestamp(incommon, tss[0]);\n+  }\n+  \n+  /*\n+   * Run test scanning different timestamps.\n+   * @param incommon\n+   * @param flusher\n+   * @throws IOException\n+   */\n+  public static void doTestTimestampScanning(final Incommon incommon,\n+    final FlushCache flusher)\n+  throws IOException {\n+    // Add a couple of values for three different timestamps.\n+    put(incommon, T0);\n+    put(incommon, T1);\n+    put(incommon, HConstants.LATEST_TIMESTAMP);\n+    // Get count of latest items.\n+    int count = assertScanContentTimestamp(incommon,\n+      HConstants.LATEST_TIMESTAMP);\n+    // Assert I get same count when I scan at each timestamp.\n+    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n+    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n+    // Flush everything out to disk and then retry\n+    flusher.flushcache();\n+    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n+    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n+  }\n+  \n+  /*\n+   * Assert that the scan returns only values < timestamp. \n+   * @param r\n+   * @param ts\n+   * @return Count of items scanned.\n+   * @throws IOException\n+   */\n+  public static int assertScanContentTimestamp(final Incommon in, final long ts)\n+  throws IOException {\n+    HScannerInterface scanner =\n+      in.getScanner(COLUMNS, HConstants.EMPTY_START_ROW, ts);\n+    int count = 0;\n+    try {\n+      HStoreKey key = new HStoreKey();\n+      TreeMap<Text, byte []>value = new TreeMap<Text, byte[]>();\n+      while (scanner.next(key, value)) {\n+        assertTrue(key.getTimestamp() <= ts);\n+        // Content matches the key or HConstants.LATEST_TIMESTAMP.\n+        // (Key does not match content if we 'put' with LATEST_TIMESTAMP).\n+        long l = Writables.bytesToLong(value.get(COLUMN));\n+        assertTrue(key.getTimestamp() == l ||\n+          HConstants.LATEST_TIMESTAMP == l);\n+        count++;\n+        value.clear();\n+      }\n+    } finally {\n+      scanner.close(); \n+    }\n+    return count;\n+  }\n+  \n+  public static void put(final Incommon loader, final long ts)\n+  throws IOException {\n+    put(loader, Writables.longToBytes(ts), ts);\n+  }\n+  \n+  public static void put(final Incommon loader)\n+  throws IOException {\n+    long ts = HConstants.LATEST_TIMESTAMP;\n+    put(loader, Writables.longToBytes(ts), ts);\n+  }\n+  \n+  /*\n+   * Put values.\n+   * @param loader\n+   * @param bytes\n+   * @param ts\n+   * @throws IOException\n+   */\n+  public static void put(final Incommon loader, final byte [] bytes,\n+    final long ts)\n+  throws IOException {\n+    long lockid = loader.startUpdate(ROW);\n+    loader.put(lockid, COLUMN, bytes);\n+    if (ts == HConstants.LATEST_TIMESTAMP) {\n+      loader.commit(lockid);\n+    } else {\n+      loader.commit(lockid, ts);\n+    }\n+  }\n+  \n+  public static void delete(final Incommon loader) throws IOException {\n+    delete(loader, HConstants.LATEST_TIMESTAMP);\n+  }\n+\n+  public static void delete(final Incommon loader, final long ts) throws IOException {\n+    long lockid = loader.startUpdate(ROW);\n+    loader.delete(lockid, COLUMN);\n+    if (ts == HConstants.LATEST_TIMESTAMP) {\n+      loader.commit(lockid);\n+    } else {\n+      loader.commit(lockid, ts);\n+    }\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "3e597c521f8b47785d993c84e44b1347932cc612",
                "filename": "src/test/org/apache/hadoop/hbase/client/TestTimestamp.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java",
                "status": "added",
                "changes": 95,
                "additions": 95,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -0,0 +1,95 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hbase.client;\n+\n+import java.io.IOException;\n+import java.util.TreeMap;\n+\n+import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n+import org.apache.hadoop.hbase.util.Writables;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.HBaseAdmin;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HStoreKey;\n+import org.apache.hadoop.hbase.HScannerInterface;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.HBaseClusterTestCase;\n+import org.apache.hadoop.hbase.TimestampTestBase;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Tests user specifiable time stamps putting, getting and scanning.  Also\n+ * tests same in presence of deletes.  Test cores are written so can be\n+ * run against an HRegion and against an HTable: i.e. both local and remote.\n+ */\n+public class TestTimestamp extends HBaseClusterTestCase {\n+  private static final Log LOG =\n+    LogFactory.getLog(TestTimestamp.class.getName());\n+  \n+  private static final String COLUMN_NAME = \"contents:\";\n+  private static final Text COLUMN = new Text(COLUMN_NAME);\n+  // When creating column descriptor, how many versions of a cell to allow.\n+  private static final int VERSIONS = 3;\n+  \n+  /** constructor */\n+  public TestTimestamp() {\n+    super();\n+  }\n+\n+  /**\n+   * Basic test of timestamps.\n+   * Do the above tests from client side.\n+   * @throws IOException\n+   */\n+  public void testTimestamps() throws IOException {\n+    HTable t = createTable();\n+    Incommon incommon = new HTableIncommon(t);\n+    TimestampTestBase.doTestDelete(incommon, new FlushCache() {\n+      public void flushcache() throws IOException {\n+        cluster.flushcache();\n+      }\n+     });\n+    \n+    // Perhaps drop and readd the table between tests so the former does\n+    // not pollute this latter?  Or put into separate tests.\n+    TimestampTestBase.doTestTimestampScanning(incommon, new FlushCache() {\n+      public void flushcache() throws IOException {\n+        cluster.flushcache();\n+      }\n+    });\n+  }\n+  \n+  /* \n+   * Create a table named TABLE_NAME.\n+   * @return An instance of an HTable connected to the created table.\n+   * @throws IOException\n+   */\n+  private HTable createTable() throws IOException {\n+    HTableDescriptor desc = new HTableDescriptor(getName());\n+    desc.addFamily(new HColumnDescriptor(COLUMN_NAME));\n+    HBaseAdmin admin = new HBaseAdmin(conf);\n+    admin.createTable(desc);\n+    return new HTable(conf, new Text(getName()));\n+  }\n+}",
                "deletions": 0
            },
            {
                "sha": "c2559f9932e54c9fdceb92fb900023c992665a70",
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java",
                "status": "modified",
                "changes": 61,
                "additions": 13,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -30,7 +30,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -42,7 +41,6 @@\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.HTableDescriptor;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.MultiRegionTable;\n import org.apache.hadoop.hbase.StaticTestEnvironment;\n import org.apache.hadoop.io.Text;\n@@ -79,13 +77,9 @@\n \n   private HTableDescriptor desc;\n \n-  private MiniDFSCluster dfsCluster = null;\n   private Path dir;\n-  private MiniHBaseCluster hCluster = null;\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void setUp() throws Exception {\n+  public TestTableIndex() {\n     // Enable DEBUG-level MR logging.\n     Logger.getLogger(\"org.apache.hadoop.mapred\").setLevel(Level.DEBUG);\n     \n@@ -103,52 +97,23 @@ public void setUp() throws Exception {\n     desc = new HTableDescriptor(TABLE_NAME);\n     desc.addFamily(new HColumnDescriptor(INPUT_COLUMN));\n     desc.addFamily(new HColumnDescriptor(OUTPUT_COLUMN));\n-\n-    dfsCluster = new MiniDFSCluster(conf, 1, true, (String[]) null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.dfsCluster.getFileSystem().getHomeDirectory().toString());\n-\n-    // Must call super.setUp after mini dfs cluster is started or else\n-    // filesystem ends up being local\n-    \n-    super.setUp();\n-\n-    try {\n-      dir = new Path(\"/hbase\");\n-      fs.mkdirs(dir);\n-\n-      // Start up HBase cluster\n-      hCluster = new MiniHBaseCluster(conf, 1, dfsCluster, true);\n-\n-      // Create a table.\n-      HBaseAdmin admin = new HBaseAdmin(conf);\n-      admin.createTable(desc);\n-\n-      // Populate a table into multiple regions\n-      makeMultiRegionTable(conf, hCluster, this.fs, TABLE_NAME, INPUT_COLUMN);\n-\n-      // Verify table indeed has multiple regions\n-      HTable table = new HTable(conf, new Text(TABLE_NAME));\n-      Text[] startKeys = table.getStartKeys();\n-      assertTrue(startKeys.length > 1);\n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfsCluster);\n-      throw e;\n-    }\n-    LOG.debug(\"\\n\\n\\n\\n\\t\\t\\tSetup Complete\\n\\n\\n\\n\");\n   }\n \n   /** {@inheritDoc} */\n   @Override\n-  public void tearDown() throws Exception {\n-    super.tearDown();\n+  protected void postHBaseClusterSetup() throws Exception {\n+    // Create a table.\n+    HBaseAdmin admin = new HBaseAdmin(conf);\n+    admin.createTable(desc);\n \n-    if (hCluster != null) {\n-      hCluster.shutdown();\n-    }\n+    // Populate a table into multiple regions\n+    makeMultiRegionTable(conf, cluster, dfsCluster.getFileSystem(), TABLE_NAME,\n+      INPUT_COLUMN);\n \n-    StaticTestEnvironment.shutdownDfs(dfsCluster);\n+    // Verify table indeed has multiple regions\n+    HTable table = new HTable(conf, new Text(TABLE_NAME));\n+    Text[] startKeys = table.getStartKeys();\n+    assertTrue(startKeys.length > 1);\n   }\n \n   /**\n@@ -260,7 +225,7 @@ private void scanTable(boolean printResults)\n   private void verify() throws IOException {\n     // Force a cache flush for every online region to ensure that when the\n     // scanner takes its snapshot, all the updates have made it into the cache.\n-    for (HRegion r : hCluster.getRegionThreads().get(0).getRegionServer().\n+    for (HRegion r : cluster.getRegionThreads().get(0).getRegionServer().\n         getOnlineRegions().values()) {\n       HRegionIncommon region = new HRegionIncommon(r);\n       region.flushcache();",
                "deletions": 48
            },
            {
                "sha": "cbee64752eeee9b8878c8b5ccf84dcc838ffc58b",
                "filename": "src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java",
                "status": "modified",
                "changes": 48,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -26,7 +26,6 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.dfs.MiniDFSCluster;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n@@ -35,7 +34,6 @@\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.HTableDescriptor;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.MultiRegionTable;\n import org.apache.hadoop.hbase.StaticTestEnvironment;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n@@ -68,9 +66,7 @@\n     TEXT_OUTPUT_COLUMN\n   };\n \n-  private MiniDFSCluster dfsCluster = null;\n   private Path dir;\n-  private MiniHBaseCluster hCluster = null;\n   \n   private static byte[][] values = null;\n   \n@@ -110,46 +106,6 @@ public TestTableMapReduce() {\n     conf.setInt(\"hbase.client.pause\", 10 * 1000);\n   }\n \n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void setUp() throws Exception {\n-    dfsCluster = new MiniDFSCluster(conf, 1, true, (String[])null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.dfsCluster.getFileSystem().getHomeDirectory().toString());\n-\n-    // Must call super.setup() after starting mini dfs cluster. Otherwise\n-    // we get a local file system instead of hdfs\n-    \n-    super.setUp();\n-    try {\n-      dir = new Path(\"/hbase\");\n-      fs.mkdirs(dir);\n-      // Start up HBase cluster\n-      // Only one region server.  MultiRegionServer manufacturing code below\n-      // depends on there being one region server only.\n-      hCluster = new MiniHBaseCluster(conf, 1, dfsCluster, true);\n-      LOG.info(\"Master is at \" + this.conf.get(HConstants.MASTER_ADDRESS));\n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfsCluster);\n-      throw e;\n-    }\n-  }\n-\n-  /**\n-   * {@inheritDoc}\n-   */\n-  @Override\n-  public void tearDown() throws Exception {\n-    super.tearDown();\n-    if(hCluster != null) {\n-      hCluster.shutdown();\n-    }\n-    StaticTestEnvironment.shutdownDfs(dfsCluster);\n-  }\n-\n   /**\n    * Pass the given key and processed record reduce\n    */\n@@ -276,8 +232,8 @@ private void localTestMultiRegionTable() throws IOException {\n     admin.createTable(desc);\n \n     // Populate a table into multiple regions\n-    makeMultiRegionTable(conf, hCluster, fs, MULTI_REGION_TABLE_NAME,\n-        INPUT_COLUMN);\n+    makeMultiRegionTable(conf, cluster, dfsCluster.getFileSystem(), \n+      MULTI_REGION_TABLE_NAME, INPUT_COLUMN);\n     \n     // Verify table indeed has multiple regions\n     HTable table = new HTable(conf, new Text(MULTI_REGION_TABLE_NAME));",
                "deletions": 46
            },
            {
                "sha": "984a1983c86347da0220dcb7451a1749a19446b8",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java",
                "status": "modified",
                "changes": 95,
                "additions": 29,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -28,8 +28,7 @@\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n-import org.apache.hadoop.hbase.HBaseTestCase;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n+import org.apache.hadoop.hbase.HBaseClusterTestCase;\n \n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.StaticTestEnvironment;\n@@ -40,10 +39,8 @@\n /**\n  * Test log deletion as logs are rolled.\n  */\n-public class TestLogRolling extends HBaseTestCase {\n+public class TestLogRolling extends HBaseClusterTestCase {\n   private static final Log LOG = LogFactory.getLog(TestLogRolling.class);\n-  private MiniDFSCluster dfs;\n-  private MiniHBaseCluster cluster;\n   private HRegionServer server;\n   private HLog log;\n   private String tableName;\n@@ -54,37 +51,13 @@\n    * @throws Exception\n    */\n   public TestLogRolling() throws Exception {\n+    // start one regionserver and a minidfs.\n     super();\n     try {\n-      this.dfs = null;\n-      this.cluster = null;\n       this.server = null;\n       this.log = null;\n       this.tableName = null;\n       this.value = null;\n-\n-      // Force a region split after every 768KB\n-      conf.setLong(\"hbase.hregion.max.filesize\", 768L * 1024L);\n-\n-      // We roll the log after every 32 writes\n-      conf.setInt(\"hbase.regionserver.maxlogentries\", 32);\n-\n-      // For less frequently updated regions flush after every 2 flushes\n-      conf.setInt(\"hbase.hregion.memcache.optionalflushcount\", 2);\n-\n-      // We flush the cache after every 8192 bytes\n-      conf.setInt(\"hbase.hregion.memcache.flush.size\", 8192);\n-\n-      // Make lease timeout longer, lease checks less frequent\n-      conf.setInt(\"hbase.master.lease.period\", 10 * 1000);\n-      conf.setInt(\"hbase.master.lease.thread.wakefrequency\", 5 * 1000);\n-\n-      // Increase the amount of time between client retries\n-      conf.setLong(\"hbase.client.pause\", 15 * 1000);\n-\n-      // Reduce thread wake frequency so that other threads can get\n-      // a chance to run.\n-      conf.setInt(HConstants.THREAD_WAKE_FREQUENCY, 2 * 1000);\n       \n       String className = this.getClass().getName();\n       StringBuilder v = new StringBuilder(className);\n@@ -99,50 +72,40 @@ public TestLogRolling() throws Exception {\n     }\n   }\n \n-  /** {@inheritDoc} */\n+  // Need to override this setup so we can edit the config before it gets sent\n+  // to the cluster startup.\n   @Override\n-  public void setUp() throws Exception {\n-    try {\n-      dfs = new MiniDFSCluster(conf, 2, true, (String[]) null);\n-      // Set the hbase.rootdir to be the home directory in mini dfs.\n-      this.conf.set(HConstants.HBASE_DIR,\n-        this.dfs.getFileSystem().getHomeDirectory().toString());\n-      super.setUp();\n-    } catch (Exception e) {\n-      StaticTestEnvironment.shutdownDfs(dfs);\n-      LOG.fatal(\"error during setUp: \", e);\n-      throw e;\n-    }\n-  }\n+  protected void preHBaseClusterSetup() {\n+    // Force a region split after every 768KB\n+    conf.setLong(\"hbase.hregion.max.filesize\", 768L * 1024L);\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void tearDown() throws Exception {\n-    try {\n-      super.tearDown();\n-      if (cluster != null) {                      // shutdown mini HBase cluster\n-        cluster.shutdown();\n-      }\n-      StaticTestEnvironment.shutdownDfs(dfs);\n-    } catch (Exception e) {\n-      LOG.fatal(\"error in tearDown\", e);\n-      throw e;\n-    }\n+    // We roll the log after every 32 writes\n+    conf.setInt(\"hbase.regionserver.maxlogentries\", 32);\n+\n+    // For less frequently updated regions flush after every 2 flushes\n+    conf.setInt(\"hbase.hregion.memcache.optionalflushcount\", 2);\n+\n+    // We flush the cache after every 8192 bytes\n+    conf.setInt(\"hbase.hregion.memcache.flush.size\", 8192);\n+\n+    // Make lease timeout longer, lease checks less frequent\n+    conf.setInt(\"hbase.master.lease.period\", 10 * 1000);\n+    conf.setInt(\"hbase.master.lease.thread.wakefrequency\", 5 * 1000);\n+\n+    // Increase the amount of time between client retries\n+    conf.setLong(\"hbase.client.pause\", 15 * 1000);\n+\n+    // Reduce thread wake frequency so that other threads can get\n+    // a chance to run.\n+    conf.setInt(HConstants.THREAD_WAKE_FREQUENCY, 2 * 1000);\n   }\n   \n   private void startAndWriteData() throws Exception {\n-    cluster = new MiniHBaseCluster(conf, 1, dfs, true);\n-    try {\n-      Thread.sleep(10 * 1000);                  // Wait for region server to start\n-    } catch (InterruptedException e) {\n-      // continue\n-    }\n+    // When the META table can be opened, the region servers are running\n+    new HTable(conf, HConstants.META_TABLE_NAME);\n \n     this.server = cluster.getRegionThreads().get(0).getRegionServer();\n     this.log = server.getLog();\n-\n-    // When the META table can be opened, the region servers are running\n-    new HTable(conf, HConstants.META_TABLE_NAME);\n     \n     // Create the test table and open it\n     HTableDescriptor desc = new HTableDescriptor(tableName);",
                "deletions": 66
            },
            {
                "sha": "4dd408013791bb96eb2093dc70fc75ba33ad8132",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java",
                "status": "modified",
                "changes": 8,
                "additions": 0,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -72,13 +72,8 @@ public TestSplit() {\n    * @throws Exception\n    */\n   public void testBasicSplit() throws Exception {\n-    MiniDFSCluster cluster = null;\n     HRegion region = null;\n     try {\n-      cluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n-      // Set the hbase.rootdir to be the home directory in mini dfs.\n-      this.conf.set(HConstants.HBASE_DIR,\n-        cluster.getFileSystem().getHomeDirectory().toString());\n       HTableDescriptor htd = createTableDescriptor(getName());\n       region = createNewHRegion(htd, null, null);\n       basicSplit(region);\n@@ -87,9 +82,6 @@ public void testBasicSplit() throws Exception {\n         region.close();\n         region.getLog().closeAndDelete();\n       }\n-      if (cluster != null) {\n-        StaticTestEnvironment.shutdownDfs(cluster);\n-      }\n     }\n   }\n   ",
                "deletions": 8
            },
            {
                "sha": "54b7fbba9cbe848b5183d9a13ce7e7ffc39fd20b",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "blob_url": "https://github.com/apache/hbase/blob/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "raw_url": "https://github.com/apache/hbase/raw/65d961ae783475b04e2b25b13d38871c4f4d13b7/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "status": "modified",
                "changes": 283,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java?ref=65d961ae783475b04e2b25b13d38871c4f4d13b7",
                "patch": "@@ -21,67 +21,41 @@\n import java.io.IOException;\n import java.util.TreeMap;\n \n-import org.apache.hadoop.dfs.MiniDFSCluster;\n-\n import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n-import org.apache.hadoop.hbase.HBaseTestCase;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HStoreKey;\n-import org.apache.hadoop.hbase.StaticTestEnvironment;\n-import org.apache.hadoop.hbase.MiniHBaseCluster;\n import org.apache.hadoop.hbase.HScannerInterface;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.HBaseTestCase;\n+import org.apache.hadoop.hbase.TimestampTestBase;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n /**\n  * Tests user specifiable time stamps putting, getting and scanning.  Also\n  * tests same in presence of deletes.  Test cores are written so can be\n  * run against an HRegion and against an HTable: i.e. both local and remote.\n  */\n public class TestTimestamp extends HBaseTestCase {\n-  private static final long T0 = 10L;\n-  private static final long T1 = 100L;\n-  private static final long T2 = 200L;\n-  \n+  private static final Log LOG =\n+    LogFactory.getLog(TestTimestamp.class.getName());\n+\n   private static final String COLUMN_NAME = \"contents:\";\n-  \n   private static final Text COLUMN = new Text(COLUMN_NAME);\n-  private static final Text ROW = new Text(\"row\");\n-  \n-  // When creating column descriptor, how many versions of a cell to allow.\n   private static final int VERSIONS = 3;\n   \n-  private MiniDFSCluster cluster;\n-\n   /** constructor */\n   public TestTimestamp() {\n     super();\n-    this.cluster = null;\n-  }\n-\n-  /** {@inheritDoc} */\n-  @Override\n-  public void setUp() throws Exception {\n-    this.cluster = new MiniDFSCluster(conf, 2, true, (String[])null);\n-    // Set the hbase.rootdir to be the home directory in mini dfs.\n-    this.conf.set(HConstants.HBASE_DIR,\n-      this.cluster.getFileSystem().getHomeDirectory().toString());\n-    super.setUp();\n   }\n \n-  /** {@inheritDoc} */\n-  @Override\n-  public void tearDown() throws Exception {\n-    if (this.cluster != null) {\n-      StaticTestEnvironment.shutdownDfs(cluster);\n-    }\n-  }\n-  \n   /**\n    * Test that delete works according to description in <a\n    * href=\"https://issues.apache.org/jira/browse/HADOOP-1784\">hadoop-1784</a>.\n@@ -91,11 +65,12 @@ public void testDelete() throws IOException {\n     final HRegion r = createRegion();\n     try {\n       final HRegionIncommon region = new HRegionIncommon(r);\n-      doTestDelete(region, region);\n+      TimestampTestBase.doTestDelete(region, region);\n     } finally {\n       r.close();\n       r.getLog().closeAndDelete();\n     }\n+    LOG.info(\"testDelete() finished\");    \n   }\n \n   /**\n@@ -106,246 +81,12 @@ public void testTimestampScanning() throws IOException {\n     final HRegion r = createRegion();\n     try {\n       final HRegionIncommon region = new HRegionIncommon(r);\n-      doTestTimestampScanning(region, region);\n+      TimestampTestBase.doTestTimestampScanning(region, region);\n     } finally {\n       r.close();\n       r.getLog().closeAndDelete();\n     }\n-  }\n-\n-  /**\n-   * Basic test of timestamps.\n-   * Do the above tests from client side.\n-   * @throws IOException\n-   */\n-  public void testTimestamps() throws IOException {\n-    final MiniHBaseCluster cluster =\n-      new MiniHBaseCluster(this.conf, 1, this.cluster, true);\n-    try {\n-      HTable t = createTable();\n-      Incommon incommon = new HTableIncommon(t);\n-      doTestDelete(incommon, new FlushCache() {\n-        public void flushcache() throws IOException {\n-          cluster.flushcache();\n-        }\n-       });\n-      \n-      // Perhaps drop and readd the table between tests so the former does\n-      // not pollute this latter?  Or put into separate tests.\n-      doTestTimestampScanning(incommon, new FlushCache() {\n-        public void flushcache() throws IOException {\n-          cluster.flushcache();\n-        }\n-       });\n-    } catch (Exception e) {\n-      cluster.shutdown();\n-    }\n-  }\n-  \n-  /*\n-   * Run test that delete works according to description in <a\n-   * href=\"https://issues.apache.org/jira/browse/HADOOP-1784\">hadoop-1784</a>.\n-   * @param incommon\n-   * @param flusher\n-   * @throws IOException\n-   */\n-  private void doTestDelete(final Incommon incommon, FlushCache flusher)\n-  throws IOException {\n-    // Add values at various timestamps (Values are timestampes as bytes).\n-    put(incommon, T0);\n-    put(incommon, T1);\n-    put(incommon, T2);\n-    put(incommon);\n-    // Verify that returned versions match passed timestamps.\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n-    // If I delete w/o specifying a timestamp, this means I'm deleting the\n-    // latest.\n-    delete(incommon);\n-    // Verify that I get back T2 through T1 -- that the latest version has\n-    // been deleted.\n-    assertVersions(incommon, new long [] {T2, T1, T0});\n-    \n-    // Flush everything out to disk and then retry\n-    flusher.flushcache();\n-    assertVersions(incommon, new long [] {T2, T1, T0});\n-    \n-    // Now add, back a latest so I can test remove other than the latest.\n-    put(incommon);\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T2, T1});\n-    delete(incommon, T2);\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n-    // Flush everything out to disk and then retry\n-    flusher.flushcache();\n-    assertVersions(incommon, new long [] {HConstants.LATEST_TIMESTAMP, T1, T0});\n-    \n-    // Now try deleting all from T2 back inclusive (We first need to add T2\n-    // back into the mix and to make things a little interesting, delete and\n-    // then readd T1.\n-    put(incommon, T2);\n-    delete(incommon, T1);\n-    put(incommon, T1);\n-    incommon.deleteAll(ROW, COLUMN, T2);\n-    // Should only be current value in set.  Assert this is so\n-    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n-    \n-    // Flush everything out to disk and then redo above tests\n-    flusher.flushcache();\n-    assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);\n-  }\n-  \n-  private void assertOnlyLatest(final Incommon incommon,\n-      final long currentTime)\n-  throws IOException {\n-    Cell[] cellValues = incommon.get(ROW, COLUMN, 3/*Ask for too much*/);\n-    assertEquals(1, cellValues.length);\n-    long time = Writables.bytesToLong(cellValues[0].getValue());\n-    assertEquals(time, currentTime);\n-    assertNull(incommon.get(ROW, COLUMN, T1, 3 /*Too many*/));\n-    assertTrue(assertScanContentTimestamp(incommon, T1) == 0);\n-  }\n-  \n-  /*\n-   * Assert that returned versions match passed in timestamps and that results\n-   * are returned in the right order.  Assert that values when converted to\n-   * longs match the corresponding passed timestamp.\n-   * @param r\n-   * @param tss\n-   * @throws IOException\n-   */\n-  private void assertVersions(final Incommon incommon, final long [] tss)\n-  throws IOException {\n-    // Assert that 'latest' is what we expect.\n-    byte [] bytes = incommon.get(ROW, COLUMN).getValue();\n-    assertEquals(Writables.bytesToLong(bytes), tss[0]);\n-    // Now assert that if we ask for multiple versions, that they come out in\n-    // order.\n-    Cell[] cellValues = incommon.get(ROW, COLUMN, tss.length);\n-    assertEquals(tss.length, cellValues.length);\n-    for (int i = 0; i < cellValues.length; i++) {\n-      long ts = Writables.bytesToLong(cellValues[i].getValue());\n-      assertEquals(ts, tss[i]);\n-    }\n-    // Specify a timestamp get multiple versions.\n-    cellValues = incommon.get(ROW, COLUMN, tss[0], cellValues.length - 1);\n-    for (int i = 1; i < cellValues.length; i++) {\n-      long ts = Writables.bytesToLong(cellValues[i].getValue());\n-      assertEquals(ts, tss[i]);\n-    }\n-    // Test scanner returns expected version\n-    assertScanContentTimestamp(incommon, tss[0]);\n-  }\n-  \n-  /*\n-   * Run test scanning different timestamps.\n-   * @param incommon\n-   * @param flusher\n-   * @throws IOException\n-   */\n-  private void doTestTimestampScanning(final Incommon incommon,\n-      final FlushCache flusher)\n-  throws IOException {\n-    // Add a couple of values for three different timestamps.\n-    put(incommon, T0);\n-    put(incommon, T1);\n-    put(incommon, HConstants.LATEST_TIMESTAMP);\n-    // Get count of latest items.\n-    int count = assertScanContentTimestamp(incommon,\n-      HConstants.LATEST_TIMESTAMP);\n-    // Assert I get same count when I scan at each timestamp.\n-    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n-    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n-    // Flush everything out to disk and then retry\n-    flusher.flushcache();\n-    assertEquals(count, assertScanContentTimestamp(incommon, T0));\n-    assertEquals(count, assertScanContentTimestamp(incommon, T1));\n-  }\n-  \n-  /*\n-   * Assert that the scan returns only values < timestamp. \n-   * @param r\n-   * @param ts\n-   * @return Count of items scanned.\n-   * @throws IOException\n-   */\n-  private int assertScanContentTimestamp(final Incommon in, final long ts)\n-  throws IOException {\n-    HScannerInterface scanner =\n-      in.getScanner(COLUMNS, HConstants.EMPTY_START_ROW, ts);\n-    int count = 0;\n-    try {\n-      HStoreKey key = new HStoreKey();\n-      TreeMap<Text, byte []>value = new TreeMap<Text, byte[]>();\n-      while (scanner.next(key, value)) {\n-        assertTrue(key.getTimestamp() <= ts);\n-        // Content matches the key or HConstants.LATEST_TIMESTAMP.\n-        // (Key does not match content if we 'put' with LATEST_TIMESTAMP).\n-        long l = Writables.bytesToLong(value.get(COLUMN));\n-        assertTrue(key.getTimestamp() == l ||\n-          HConstants.LATEST_TIMESTAMP == l);\n-        count++;\n-        value.clear();\n-      }\n-    } finally {\n-      scanner.close(); \n-    }\n-    return count;\n-  }\n-  \n-  private void put(final Incommon loader, final long ts)\n-  throws IOException {\n-    put(loader, Writables.longToBytes(ts), ts);\n-  }\n-  \n-  private void put(final Incommon loader)\n-  throws IOException {\n-    long ts = HConstants.LATEST_TIMESTAMP;\n-    put(loader, Writables.longToBytes(ts), ts);\n-  }\n-  \n-  /*\n-   * Put values.\n-   * @param loader\n-   * @param bytes\n-   * @param ts\n-   * @throws IOException\n-   */\n-  private void put(final Incommon loader, final byte [] bytes,\n-    final long ts)\n-  throws IOException {\n-    long lockid = loader.startUpdate(ROW);\n-    loader.put(lockid, COLUMN, bytes);\n-    if (ts == HConstants.LATEST_TIMESTAMP) {\n-      loader.commit(lockid);\n-    } else {\n-      loader.commit(lockid, ts);\n-    }\n-  }\n-  \n-  private void delete(final Incommon loader) throws IOException {\n-    delete(loader, HConstants.LATEST_TIMESTAMP);\n-  }\n-\n-  private void delete(final Incommon loader, final long ts) throws IOException {\n-    long lockid = loader.startUpdate(ROW);\n-    loader.delete(lockid, COLUMN);\n-    if (ts == HConstants.LATEST_TIMESTAMP) {\n-      loader.commit(lockid);\n-    } else {\n-      loader.commit(lockid, ts);\n-    }\n-  }\n-  \n-  /* \n-   * Create a table named TABLE_NAME.\n-   * @return An instance of an HTable connected to the created table.\n-   * @throws IOException\n-   */\n-  private HTable createTable() throws IOException {\n-    HTableDescriptor desc = new HTableDescriptor(getName());\n-    desc.addFamily(new HColumnDescriptor(COLUMN_NAME));\n-    HBaseAdmin admin = new HBaseAdmin(conf);\n-    admin.createTable(desc);\n-    return new HTable(conf, new Text(getName()));\n+    LOG.info(\"testTimestampScanning() finished\");\n   }\n   \n   private HRegion createRegion() throws IOException {",
                "deletions": 271
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1730 unexpected null value causes META scanner to exit (silently)\n\nAdded handling for legal null value scanning META table and added\nlogging of unexpected exceptions that arise scanning.\n\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestSplit.java\n    Refactored to do a staged removal of daughter references.\n    (compact, recalibrate): Added.\n    (getSplitParent): Refactored as getSplitParentInfo.\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HConnectionManager.java\n    Added formatting of the find table result string so shorter\n    (when 30-odd regions fills page with its output).\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HTable.java\n    Formatting to clean eclipse warnings.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HMaster.java\n    The split column in a parent meta table entry can be null (Happens\n    if a daughter split no longer has references -- it removes its\n    entry from parent).  Add handling and clean up around split\n    management code.  Added logging of unexpected exceptions\n    scanning a region.\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HRegion.java\n    Added fix for NPE when client asks for scanner but passes\n    non-existent columns.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/util/Writables.java\n    (getHRegionInfo, getHRegionInfoOrNull): Added.:\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@567308 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/00c1b877e8bb104516139dbad4d70e27c67c688b",
        "parent": "https://github.com/apache/hbase/commit/17cc1759fceb3ee260e18c5e11ce7598cb8c31aa",
        "bug_id": "hbase_320",
        "file": [
            {
                "sha": "5dcf4dea33cd7e7038a525923cfbc37daada696a",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -11,6 +11,7 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n     HADOOP-1729 Recent renaming or META tables breaks hbase shell\n+    HADOOP-1730 unexpected null value causes META scanner to exit (silently)\n \n   IMPROVEMENTS\n ",
                "deletions": 0
            },
            {
                "sha": "089f5395c57662562960d296be0ce7e487c7e29b",
                "filename": "src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "status": "modified",
                "changes": 56,
                "additions": 31,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnectionManager.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -46,7 +46,12 @@\n  * multiple HBase instances\n  */\n public class HConnectionManager implements HConstants {\n-  private HConnectionManager() {}                        // Not instantiable\n+  /*\n+   * Private. Not instantiable.\n+   */\n+  private HConnectionManager() {\n+    super();\n+  }\n   \n   // A Map of master HServerAddress -> connection information for that instance\n   // Note that although the Map is synchronized, the objects it contains\n@@ -298,28 +303,37 @@ public boolean tableExists(final Text tableName) {\n       }\n       SortedMap<Text, HRegionLocation> servers =\n         new TreeMap<Text, HRegionLocation>();\n-\n       servers.putAll(tableServers);\n       return servers;\n     }\n \n     /** {@inheritDoc} */\n     public SortedMap<Text, HRegionLocation>\n     reloadTableServers(final Text tableName) throws IOException {\n-      \n       closedTables.remove(tableName);\n-\n-      SortedMap<Text, HRegionLocation> servers =\n+      SortedMap<Text, HRegionLocation> tableServers =\n         new TreeMap<Text, HRegionLocation>();\n-      \n       // Reload information for the whole table\n-\n-      servers.putAll(findServersForTable(tableName));\n+      tableServers.putAll(findServersForTable(tableName));\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Result of findTable: \" + servers.toString());\n+        StringBuilder sb = new StringBuilder();\n+        int count = 0;\n+        for (HRegionLocation location: tableServers.values()) {\n+          if (sb.length() > 0) {\n+            sb.append(\" \");\n+          }\n+          sb.append(count++);\n+          sb.append(\". \");\n+          sb.append(\"address=\");\n+          sb.append(location.getServerAddress());\n+          sb.append(\", \");\n+          sb.append(location.getRegionInfo().getRegionName());\n+        }\n+        LOG.debug(\"Result of findTable on \" + tableName.toString() +\n+          \": \" + sb.toString());\n       }\n       \n-      return servers;\n+      return tableServers;\n     }\n \n     /** {@inheritDoc} */\n@@ -413,7 +427,7 @@ public void close(Text tableName) {\n         }\n       }\n       \n-      SortedMap<Text, HRegionLocation> servers =\n+      SortedMap<Text, HRegionLocation> srvrs =\n         new TreeMap<Text, HRegionLocation>();\n       \n       if (tableName.equals(ROOT_TABLE_NAME)) {\n@@ -428,7 +442,7 @@ public void close(Text tableName) {\n           if (tableServers == null) {\n             tableServers = locateRootRegion();\n           }\n-          servers.putAll(tableServers);\n+          srvrs.putAll(tableServers);\n         }\n         \n       } else if (tableName.equals(META_TABLE_NAME)) {\n@@ -459,7 +473,7 @@ public void close(Text tableName) {\n               }\n             }\n           }\n-          servers.putAll(tableServers);\n+          srvrs.putAll(tableServers);\n         }\n       } else {\n         boolean waited = false;\n@@ -486,7 +500,7 @@ public void close(Text tableName) {\n             if (tableServers == null) {\n               throw new TableNotFoundException(\"table not found: \" + tableName);\n             }\n-            servers.putAll(tableServers);\n+            srvrs.putAll(tableServers);\n           }\n         }\n         if (!waited) {\n@@ -504,7 +518,7 @@ public void close(Text tableName) {\n \n               for (HRegionLocation t: metaServers.values()) {\n                 try {\n-                  servers.putAll(scanOneMetaRegion(t, tableName));\n+                  srvrs.putAll(scanOneMetaRegion(t, tableName));\n \n                 } catch (IOException e) {\n                   if (tries < numRetries - 1) {\n@@ -528,15 +542,8 @@ public void close(Text tableName) {\n           }\n         }\n       }\n-      this.tablesToServers.put(tableName, servers);\n-      if (LOG.isDebugEnabled()) {\n-        int count = 0;\n-        for (Map.Entry<Text, HRegionLocation> e: servers.entrySet()) {\n-          LOG.debug(\"Region \" + (1 + count++) + \" of \" + servers.size() +\n-            \": \" + e.getValue());\n-        }\n-      }\n-      return servers;\n+      this.tablesToServers.put(tableName, srvrs);\n+      return srvrs;\n     }\n \n     /*\n@@ -598,7 +605,6 @@ public void close(Text tableName) {\n         try {\n           rootRegion.getRegionInfo(HGlobals.rootRegionInfo.regionName);\n           break;\n-          \n         } catch (IOException e) {\n           if (tries == numRetries - 1) {\n             // Don't bother sleeping. We've run out of retries.",
                "deletions": 25
            },
            {
                "sha": "deac39ed8b8b8c09080ec516d3c7c7c68a558d11",
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HMaster.java",
                "status": "modified",
                "changes": 141,
                "additions": 75,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -262,109 +262,114 @@ protected void scanRegion(final MetaRegion region) throws IOException {\n         }\n       }\n \n-      // Scan is finished.  Take a look at split parents to see if any we can clean up.\n-\n+      // Scan is finished.  Take a look at split parents to see if any we can\n+      // clean up.\n       if (splitParents.size() > 0) {\n         for (Map.Entry<HRegionInfo, SortedMap<Text, byte[]>> e:\n-          splitParents.entrySet()) {\n-          \n-          SortedMap<Text, byte[]> results = e.getValue();\n-          cleanupSplits(region.regionName, regionServer, e.getKey(), \n-              (HRegionInfo) Writables.getWritable(results.get(COL_SPLITA),\n-                  new HRegionInfo()),\n-              (HRegionInfo) Writables.getWritable(results.get(COL_SPLITB),\n-                  new HRegionInfo()));\n+            splitParents.entrySet()) {\n+          HRegionInfo hri = e.getKey();\n+          cleanupSplits(region.regionName, regionServer, hri, e.getValue());\n         }\n       }\n       LOG.info(Thread.currentThread().getName() + \" scan of meta region \" +\n           region.regionName + \" complete\");\n     }\n \n+    /*\n+     * @param info Region to check.\n+     * @return True if this is a split parent.\n+     */\n     private boolean isSplitParent(final HRegionInfo info) {\n-      boolean result = false;\n-\n-      // Skip if not a split region.\n-      \n       if (!info.isSplit()) {\n-        return result;\n+        return false;\n       }\n       if (!info.isOffline()) {\n         LOG.warn(\"Region is split but not offline: \" + info.regionName);\n       }\n       return true;\n     }\n \n-    /**\n-     * @param metaRegionName\n+    /*\n+     * If daughters no longer hold reference to the parents, delete the parent.\n+     * @param metaRegionName Meta region name.\n      * @param server HRegionInterface of meta server to talk to \n-     * @param info HRegionInfo of split parent\n-     * @param splitA low key range child region \n-     * @param splitB upper key range child region\n-     * @return True if we removed <code>info</code> and this region has\n-     * been cleaned up.\n+     * @param parent HRegionInfo of split parent\n+     * @param rowContent Content of <code>parent</code> row in\n+     * <code>metaRegionName</code>\n+     * @return True if we removed <code>parent</code> from meta table and from\n+     * the filesystem.\n      * @throws IOException\n      */\n     private boolean cleanupSplits(final Text metaRegionName, \n-        final HRegionInterface server, final HRegionInfo info,\n-        final HRegionInfo splitA, final HRegionInfo splitB) throws IOException {\n-    \n+        final HRegionInterface srvr, final HRegionInfo parent,\n+        SortedMap<Text, byte[]> rowContent)\n+    throws IOException {\n       boolean result = false;\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Checking \" + info.getRegionName() + \" to see if daughter \" +\n-        \"splits still hold references\");\n+        LOG.debug(\"Checking \" + parent.getRegionName() +\n+          \" to see if daughter splits still hold references\");\n       }\n-      boolean noReferencesA = splitA == null;\n-      boolean noReferencesB = splitB == null;\n+\n+      boolean hasReferencesA = hasReferences(metaRegionName, srvr,\n+          parent.getRegionName(), rowContent, COL_SPLITA);\n+      boolean hasReferencesB = hasReferences(metaRegionName, srvr,\n+          parent.getRegionName(), rowContent, COL_SPLITB);\n       \n-      if (!noReferencesA) {\n-        noReferencesA = hasReferences(metaRegionName, server,\n-          info.getRegionName(), splitA, COL_SPLITA);\n-      }\n-      if (!noReferencesB) {\n-        noReferencesB = hasReferences(metaRegionName, server,\n-          info.getRegionName(), splitB, COL_SPLITB);\n-      }\n-      if (!noReferencesA && !noReferencesB) {\n-        // No references.  Remove this item from table and deleted region on\n-        // disk.\n-        LOG.info(\"Deleting region \" + info.getRegionName() +\n+      if (!hasReferencesA && !hasReferencesB) {\n+        LOG.info(\"Deleting region \" + parent.getRegionName() +\n         \" because daughter splits no longer hold references\");\n-        \n-        if (!HRegion.deleteRegion(fs, dir, info.getRegionName())) {\n-          LOG.warn(\"Deletion of \" + info.getRegionName() + \" failed\");\n+\n+        if (!HRegion.deleteRegion(fs, dir, parent.getRegionName())) {\n+          LOG.warn(\"Deletion of \" + parent.getRegionName() + \" failed\");\n         }\n         \n         BatchUpdate b = new BatchUpdate();\n-        long lockid = b.startUpdate(info.getRegionName());\n+        long lockid = b.startUpdate(parent.getRegionName());\n         b.delete(lockid, COL_REGIONINFO);\n         b.delete(lockid, COL_SERVER);\n         b.delete(lockid, COL_STARTCODE);\n-        server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n+        srvr.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n         result = true;\n       }\n       \n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Done checking \" + info.getRegionName() + \": splitA: \" +\n-            noReferencesA + \", splitB: \"+ noReferencesB);\n+        LOG.debug(\"Done checking \" + parent.getRegionName() + \": splitA: \" +\n+            hasReferencesA + \", splitB: \"+ hasReferencesB);\n       }\n       return result;\n     }\n-\n+    \n+    /* \n+     * Checks if a daughter region -- either splitA or splitB -- still holds\n+     * references to parent.  If not, removes reference to the split from\n+     * the parent meta region row.\n+     * @param metaRegionName Name of meta region to look in.\n+     * @param srvr Where region resides.\n+     * @param parent Parent region name. \n+     * @param rowContent Keyed content of the parent row in meta region.\n+     * @param splitColumn Column name of daughter split to examine\n+     * @return True if still has references to parent.\n+     * @throws IOException\n+     */\n     protected boolean hasReferences(final Text metaRegionName, \n-        final HRegionInterface server, final Text regionName,\n-        final HRegionInfo split, final Text column) throws IOException {\n-      \n+      final HRegionInterface srvr, final Text parent,\n+      SortedMap<Text, byte[]> rowContent, final Text splitColumn)\n+    throws IOException {\n       boolean result = false;\n+      HRegionInfo split =\n+        Writables.getHRegionInfoOrNull(rowContent.get(splitColumn));\n+      if (split == null) {\n+        return result;\n+      }\n       for (Text family: split.getTableDesc().families().keySet()) {\n         Path p = HStoreFile.getMapDir(fs.makeQualified(dir),\n             split.getRegionName(), HStoreKey.extractFamily(family));\n-        \n-        // Look for reference files.\n-        \n+        // Look for reference files.  Call listPaths with an anonymous\n+        // instance of PathFilter.\n         Path [] ps = fs.listPaths(p,\n             new PathFilter () {\n-              public boolean accept(Path p) {\n-                return HStoreFile.isReference(p);\n+              public boolean accept(Path path) {\n+                return HStoreFile.isReference(path);\n               }\n             }\n         );\n@@ -381,13 +386,13 @@ public boolean accept(Path p) {\n       \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(split.getRegionName().toString()\n-            +\" no longer has references to \" + regionName.toString());\n+            +\" no longer has references to \" + parent.toString());\n       }\n       \n       BatchUpdate b = new BatchUpdate();\n-      long lockid = b.startUpdate(regionName);\n-      b.delete(lockid, column);\n-      server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n+      long lockid = b.startUpdate(parent);\n+      b.delete(lockid, splitColumn);\n+      srvr.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n         \n       return result;\n     }\n@@ -468,7 +473,6 @@ private void scanRoot() {\n                 HGlobals.rootRegionInfo.regionName, null));\n           }\n           break;\n-\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n             try {\n@@ -485,6 +489,10 @@ private void scanRoot() {\n           } else {\n             LOG.error(\"Scan ROOT region\", e);\n           }\n+        } catch (Exception e) {\n+          // If for some reason we get some other kind of exception, \n+          // at least log it rather than go out silently.\n+          LOG.error(\"Unexpected exception\", e);\n         }\n         if (!closed) {\n           // sleep before retry\n@@ -597,19 +605,16 @@ private void scanOneMetaRegion(MetaRegion region) {\n \n         try {\n           // Don't interrupt us while we're working\n-\n           synchronized (metaScannerLock) {\n             scanRegion(region);\n             onlineMetaRegions.put(region.startKey, region);\n           }\n           break;\n-\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n             try {\n               e = RemoteExceptionHandler.decodeRemoteException(\n                   (RemoteException) e);\n-\n             } catch (IOException ex) {\n               e = ex;\n             }\n@@ -620,10 +625,14 @@ private void scanOneMetaRegion(MetaRegion region) {\n           } else {\n             LOG.error(\"Scan one META region\", e);\n           }\n+        } catch (Exception e) {\n+          // If for some reason we get some other kind of exception, \n+          // at least log it rather than go out silently.\n+          LOG.error(\"Unexpected exception\", e);\n         }\n+        \n         if (!closed) {\n           // sleep before retry\n-\n           try {\n             Thread.sleep(threadWakeFrequency);                  \n           } catch (InterruptedException e) {",
                "deletions": 66
            },
            {
                "sha": "fc08e62d42befadb6b42b21c29e05eb8872d534d",
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HRegion.java",
                "status": "modified",
                "changes": 17,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -20,7 +20,9 @@\n package org.apache.hadoop.hbase;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.Random;\n import java.util.TreeMap;\n@@ -971,20 +973,25 @@ void internalFlushcache() throws IOException {\n    * @throws IOException\n    */\n   public HInternalScannerInterface getScanner(Text[] cols, Text firstRow,\n-      long timestamp, RowFilterInterface filter) throws IOException {\n+      long timestamp, RowFilterInterface filter)\n+  throws IOException {\n     lock.obtainReadLock();\n     try {\n       TreeSet<Text> families = new TreeSet<Text>();\n       for(int i = 0; i < cols.length; i++) {\n         families.add(HStoreKey.extractFamily(cols[i]));\n       }\n \n-      HStore[] storelist = new HStore[families.size()];\n-      int i = 0;\n+      List<HStore> storelist = new ArrayList<HStore>();\n       for (Text family: families) {\n-        storelist[i++] = stores.get(family);\n+        HStore s = stores.get(family);\n+        if (s == null) {\n+          continue;\n+        }\n+        storelist.add(stores.get(family));\n       }\n-      return new HScanner(cols, firstRow, timestamp, memcache, storelist, filter);\n+      return new HScanner(cols, firstRow, timestamp, memcache,\n+        storelist.toArray(new HStore [] {}), filter);\n     } finally {\n       lock.releaseReadLock();\n     }",
                "deletions": 5
            },
            {
                "sha": "e95ecf1d3d73420666208775b673efe3c7666d7a",
                "filename": "src/java/org/apache/hadoop/hbase/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/HTable.java",
                "status": "modified",
                "changes": 7,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTable.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -586,11 +586,10 @@ public synchronized void commit(long lockid, long timestamp)\n         HRegionLocation r = getRegionLocation(batch.getRow());\n         HRegionInterface server =\n           connection.getHRegionConnection(r.getServerAddress());\n-\n         try {\n-          server.batchUpdate(r.getRegionInfo().getRegionName(), timestamp, batch);\n+          server.batchUpdate(r.getRegionInfo().getRegionName(), timestamp,\n+            batch);\n           break;\n-\n         } catch (IOException e) {\n           if (e instanceof RemoteException) {\n             e = RemoteExceptionHandler.decodeRemoteException(\n@@ -601,7 +600,6 @@ public synchronized void commit(long lockid, long timestamp)\n               LOG.debug(\"reloading table servers because: \" + e.getMessage());\n             }\n             tableServers = connection.reloadTableServers(tableName);\n-\n           } else {\n             throw e;\n           }\n@@ -628,6 +626,7 @@ public synchronized void commit(long lockid, long timestamp)\n    */\n   @Deprecated\n   public synchronized void renewLease(@SuppressWarnings(\"unused\") long lockid) {\n+    // noop\n   }\n \n   /**",
                "deletions": 4
            },
            {
                "sha": "90d3148e2d8394208f1f4586814d8d7c91013aa4",
                "filename": "src/java/org/apache/hadoop/hbase/util/Writables.java",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "status": "modified",
                "changes": 23,
                "additions": 23,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Writables.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.io.Writable;\n \n import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HRegionInfo;\n \n /**\n  * Utility class with methods for manipulating Writable objects\n@@ -89,6 +90,28 @@ public static Writable getWritable(final byte [] bytes, final Writable w)\n       in.close();\n     }\n   }\n+  \n+  /**\n+   * @param bytes\n+   * @return A HRegionInfo instance built out of passed <code>bytes</code>.\n+   * @throws IOException\n+   */\n+  public static HRegionInfo getHRegionInfo(final byte [] bytes)\n+  throws IOException {\n+    return (HRegionInfo)getWritable(bytes, new HRegionInfo());\n+  }\n+ \n+  /**\n+   * @param bytes\n+   * @return A HRegionInfo instance built out of passed <code>bytes</code>\n+   * or <code>null</code> if passed bytes are null or an empty array.\n+   * @throws IOException\n+   */\n+  public static HRegionInfo getHRegionInfoOrNull(final byte [] bytes)\n+  throws IOException {\n+    return (bytes == null || bytes.length <= 0)?\n+      (HRegionInfo)null: getHRegionInfo(bytes);\n+  }\n \n   /**\n    * Copy one Writable to another.  Copies bytes using data streams.",
                "deletions": 0
            },
            {
                "sha": "4344d9c6105a1a344a398e1fa9e3bc1b2cf9d7b9",
                "filename": "src/test/org/apache/hadoop/hbase/TestSplit.java",
                "blob_url": "https://github.com/apache/hbase/blob/00c1b877e8bb104516139dbad4d70e27c67c688b/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "raw_url": "https://github.com/apache/hbase/raw/00c1b877e8bb104516139dbad4d70e27c67c688b/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "status": "modified",
                "changes": 183,
                "additions": 112,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestSplit.java?ref=00c1b877e8bb104516139dbad4d70e27c67c688b",
                "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.util.ConcurrentModificationException;\n+import java.util.Map;\n import java.util.SortedMap;\n import java.util.TreeMap;\n \n@@ -168,86 +169,63 @@ public void testSplitRegionIsDeleted() throws Exception {\n       int count = count(meta, HConstants.COLUMN_FAMILY_STR);\n       t = new HTable(this.conf, new Text(getName()));\n       addContent(new HTableLoader(t), COLFAMILY_NAME3);\n-      // All is running in the one JVM so I should be able to get the\n+      // All is running in the one JVM so I should be able to get the single\n       // region instance and bring on a split.\n       HRegionInfo hri =\n         t.getRegionLocation(HConstants.EMPTY_START_ROW).getRegionInfo();\n-      HRegion r =\n-        cluster.regionThreads.get(0).getRegionServer().onlineRegions.get(\n-            hri.getRegionName());\n+      HRegion r = cluster.regionThreads.get(0).getRegionServer().\n+        onlineRegions.get(hri.getRegionName());\n       // Flush will provoke a split next time the split-checker thread runs.\n       r.flushcache(false);\n       // Now, wait until split makes it into the meta table.\n       for (int i = 0; i < retries &&\n-      (count(meta, HConstants.COLUMN_FAMILY_STR) <= count); i++) {\n+          (count(meta, HConstants.COLUMN_FAMILY_STR) <= count); i++) {\n         Thread.sleep(5000);\n       }\n       int oldCount = count;\n       count = count(meta, HConstants.COLUMN_FAMILY_STR);\n       if (count <= oldCount) {\n         throw new IOException(\"Failed waiting on splits to show up\");\n       }\n-      HRegionInfo parent = getSplitParent(meta);\n+      // Get info on the parent from the meta table.  Pass in 'hri'. Its the\n+      // region we have been dealing with up to this. Its the parent of the\n+      // region split.\n+      Map<Text, byte []> data = getSplitParentInfo(meta, hri);\n+      HRegionInfo parent =\n+        Writables.getHRegionInfoOrNull(data.get(HConstants.COL_REGIONINFO));\n       assertTrue(parent.isOffline());\n+      assertTrue(parent.isSplit());\n+      HRegionInfo splitA =\n+        Writables.getHRegionInfoOrNull(data.get(HConstants.COL_SPLITA));\n+      HRegionInfo splitB =\n+        Writables.getHRegionInfoOrNull(data.get(HConstants.COL_SPLITB));\n       Path parentDir = HRegion.getRegionDir(d, parent.getRegionName());\n       assertTrue(fs.exists(parentDir));\n-      LOG.info(\"Split happened and parent \" + parent.getRegionName() + \" is \" +\n-      \"offline\");\n-      for (int i = 0; i < retries; i++) {\n-        // Now open a scanner on the table. This will force HTable to recalibrate\n-        // and in doing so, will force us to wait until the new child regions\n-        // come on-line (since they are no longer automatically served by the \n-        // HRegionServer that was serving the parent. In this test they will\n-        // end up on the same server (since there is only one), but we have to\n-        // wait until the master assigns them.\n-        try {\n-          HScannerInterface s =\n-            t.obtainScanner(new Text[] {new Text(COLFAMILY_NAME3)},\n-                HConstants.EMPTY_START_ROW);\n-          try {\n-            HStoreKey key = new HStoreKey();\n-            TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n-            s.next(key, results);\n-            break;\n-\n-          } finally {\n-            s.close();\n-          }\n-        } catch (NotServingRegionException x) {\n-          Thread.sleep(5000);\n-        }\n-      }\n-      // Now, force a compaction.  This will rewrite references and make it\n-      // so the parent region becomes deletable.\n-      LOG.info(\"Starting compaction\");\n-      for (MiniHBaseCluster.RegionServerThread thread: cluster.regionThreads) {\n-        SortedMap<Text, HRegion> regions =\n-          thread.getRegionServer().onlineRegions;\n-        // Retry if ConcurrentModification... alternative of sync'ing is not\n-        // worth it for sake of unit test.\n-        for (int i = 0; i < 10; i++) {\n-          try {\n-            for (HRegion online: regions.values()) {\n-              if (online.getTableDesc().getName().toString().equals(getName())) {\n-                online.compactStores();\n-              }\n-            }\n-            break;\n-          } catch (ConcurrentModificationException e) {\n-            LOG.warn(\"Retrying because ...\" + e.toString() + \" -- one or \" +\n-            \"two should be fine\");\n-            continue;\n-          }\n-        }\n+      LOG.info(\"Split happened. Parent is \" + parent.getRegionName() +\n+        \" and daughters are \" + splitA.getRegionName() + \", \" +\n+        splitB.getRegionName());\n+      // Recalibrate will cause us to wait on new regions' deployment\n+      recalibrate(t, new Text(COLFAMILY_NAME3), retries);\n+      // Compact a region at a time so we can test case where one region has\n+      // no references but the other still has some\n+      compact(cluster, splitA);\n+      // Wait till the parent only has reference to remaining split, one that\n+      // still has references.\n+      while (getSplitParentInfo(meta, parent).size() == 3) {\n+        Thread.sleep(5000);\n       }\n-\n+      LOG.info(\"Parent split returned \" +\n+          getSplitParentInfo(meta, parent).keySet().toString());\n+      // Call second split.\n+      compact(cluster, splitB);\n       // Now wait until parent disappears.\n       LOG.info(\"Waiting on parent \" + parent.getRegionName() +\n       \" to disappear\");\n-      for (int i = 0; i < retries && getSplitParent(meta) != null; i++) {\n+      for (int i = 0; i < retries &&\n+          getSplitParentInfo(meta, parent) != null; i++) {\n         Thread.sleep(5000);\n       }\n-      assertTrue(getSplitParent(meta) == null);\n+      assertTrue(getSplitParentInfo(meta, parent) == null);\n       // Assert cleaned up.\n       for (int i = 0; i < retries && fs.exists(parentDir); i++) {\n         Thread.sleep(5000);\n@@ -258,6 +236,70 @@ public void testSplitRegionIsDeleted() throws Exception {\n     }\n   }\n   \n+  /*\n+   * Compact the passed in region <code>r</code>. \n+   * @param cluster\n+   * @param r\n+   * @throws IOException\n+   */\n+  private void compact(final MiniHBaseCluster cluster, final HRegionInfo r)\n+  throws IOException {\n+    LOG.info(\"Starting compaction\");\n+    for (MiniHBaseCluster.RegionServerThread thread: cluster.regionThreads) {\n+      SortedMap<Text, HRegion> regions =\n+        thread.getRegionServer().onlineRegions;\n+      // Retry if ConcurrentModification... alternative of sync'ing is not\n+      // worth it for sake of unit test.\n+      for (int i = 0; i < 10; i++) {\n+        try {\n+          for (HRegion online: regions.values()) {\n+            if (online.getRegionName().toString().\n+                equals(r.getRegionName().toString())) {\n+              online.compactStores();\n+            }\n+          }\n+          break;\n+        } catch (ConcurrentModificationException e) {\n+          LOG.warn(\"Retrying because ...\" + e.toString() + \" -- one or \" +\n+          \"two should be fine\");\n+          continue;\n+        }\n+      }\n+    }\n+  }\n+  \n+  /*\n+   * Recalibrate passed in HTable.  Run after change in region geography.\n+   * Open a scanner on the table. This will force HTable to recalibrate\n+   * and in doing so, will force us to wait until the new child regions\n+   * come on-line (since they are no longer automatically served by the \n+   * HRegionServer that was serving the parent. In this test they will\n+   * end up on the same server (since there is only one), but we have to\n+   * wait until the master assigns them. \n+   * @param t\n+   * @param retries\n+   */\n+  private void recalibrate(final HTable t, final Text column,\n+      final int retries)\n+  throws IOException, InterruptedException {\n+    for (int i = 0; i < retries; i++) {\n+      try {\n+        HScannerInterface s =\n+          t.obtainScanner(new Text[] {column}, HConstants.EMPTY_START_ROW);\n+        try {\n+          HStoreKey key = new HStoreKey();\n+          TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n+          s.next(key, results);\n+          break;\n+        } finally {\n+          s.close();\n+        }\n+      } catch (NotServingRegionException x) {\n+        Thread.sleep(5000);\n+      }\n+    }\n+  }\n+  \n   private void assertGet(final HRegion r, final String family, final Text k)\n   throws IOException {\n     // Now I have k, get values out and assert they are as expected.\n@@ -270,30 +312,29 @@ private void assertGet(final HRegion r, final String family, final Text k)\n     }\n   }\n   \n-  private HRegionInfo getSplitParent(final HTable t)\n+  /*\n+   * @return Return row info for passed in region or null if not found in scan.\n+   */\n+  private Map<Text, byte []> getSplitParentInfo(final HTable t,\n+    final HRegionInfo parent)\n   throws IOException {\n-    HRegionInfo result = null;\n-    HScannerInterface s = t.obtainScanner(HConstants.COL_REGIONINFO_ARRAY,\n+    HScannerInterface s = t.obtainScanner(HConstants.COLUMN_FAMILY_ARRAY,\n       HConstants.EMPTY_START_ROW, System.currentTimeMillis(), null);\n     try {\n       HStoreKey curKey = new HStoreKey();\n       TreeMap<Text, byte []> curVals = new TreeMap<Text, byte []>();\n       while(s.next(curKey, curVals)) {\n-        byte[] bytes = curVals.get(HConstants.COL_REGIONINFO);\n-        if (bytes == null || bytes.length == 0) {\n+        HRegionInfo hri = Writables.\n+          getHRegionInfoOrNull(curVals.get(HConstants.COL_REGIONINFO));\n+        if (hri == null) {\n           continue;\n         }\n-        HRegionInfo hri =\n-          (HRegionInfo) Writables.getWritable(bytes, new HRegionInfo());\n-        \n-        // Assert that if region is a split region, that it is also offline.\n-        // Otherwise, if not a split region, assert that it is online.\n-        if (hri.isSplit() && hri.isOffline()) {\n-          result = hri;\n-          break;\n+        if (hri.getRegionName().toString().\n+            equals(parent.getRegionName().toString())) {\n+          return curVals;\n         }\n       }\n-      return result;\n+      return null;\n     } finally {\n       s.close();\n     }   ",
                "deletions": 71
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HBASE-1217 Changed HColumnDescriptor so it now has blocksize, and compression\nis not lzo, gz or none (rather than block, key or none).  Made changes to\nshell so you can do this configuration therein.  Bloomfilter still not\nhooked up.\n\nHBASE-859 HStoreKey needs a reworking... binary keys work again.\n\nHBASE-1211 NPE in retries exhausted exception\n\n\nM src/test/org/apache/hadoop/hbase/TestHStoreKey.java\n    Added tests comparing binary keys, plain, meta and root keys.\n    Enabled the commented-out testHStoreKeyBorderCases.\nM  src/test/org/apache/hadoop/hbase/HBaseTestCase.java\n    Removed PUNCTUATION pollution of keys.\nM src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java\n    Use different compression type.\nM src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java\n    Use new HCD constructor.\nM src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java\n    Was using ROOT table but a plain hstorekey comparator; that won't\n    work (have to be careful about which comparator is used on which\n    table from here on out).\nM src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java\n    getWriter now takes whether or not bloom filter, etc.\nM  src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java\n    Removed PUNCTUATION pollution.\nM src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java\nM src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java\n    Whitespace.\nM src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java\n    Pass compressor and whether bloomfilter.\nM src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java\n    Added new seek+scan test.\nM src/java/org/apache/hadoop/hbase/HColumnDescriptor.java\n    Added in BLOCKSIZE, changed COMPRESSION arguments.\n    Removed DEFAULT_MAPFILE_INDEX_INTERVAL.\n    Upped the version from 6 to 7.\nM src/java/org/apache/hadoop/hbase/HStoreKey.java\n    New object and raw byte comparators, one for each context of\n    plain, meta, and root.  Use same code.  Other utility such\n    as static creates that take a ByteBuffer or a byte array of\n    a serialized HStoreKey.  Old compareTo methods are deprecated\n    since they can return wrong answer if binary keys.\n    New getBytes without creating Streams.\n    Removed the old BeforeThisStoreKey trick.\n    Custom vint math methods that work with ByteBuffers and byte []\n    instead of streams.\nM src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java\n    Fixup for new compression.\nM src/java/org/apache/hadoop/hbase/regionserver/Memcache.java\n    Pass in comparator to use.\nM src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java\nM src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java\n    Use right comparator comparing rows\nM src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java\nM src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n    Formatting.\n    Might be issues here with key compares but no store in the\n    context... leave it for now.\nM src/java/org/apache/hadoop/hbase/regionserver/Store.java\n    Use new comparators and hcd changes.\nM src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java\n    Added override of HFile.Reader so could format the toString\n    (so HFile doesn't have to know about HStoreKeys though thats\n    what we put in there always -- let it just be about\n    byte []).\nM src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java\nM src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java\nM src/java/org/apache/hadoop/hbase/HTableDescriptor.java\n    Fix up for new HCD.\nM src/java/org/apache/hadoop/hbase/HRegionInfo.java\nM src/java/org/apache/hadoop/hbase/master/MetaRegion.java\n    Use Bytes.compareTo.\nM src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java\n    (isTop): Added.\nM src/java/org/apache/hadoop/hbase/io/RowResult.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/io/hfile/Compression.java\n    Make getCompressionAlgorithmByName public.\nM src/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n    Add in writer constructor that takes compression and bloom filter.\n    Fix some of the comparator use.\nM src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java\n    Remove commented out code\nM src/java/org/apache/hadoop/hbase/io/Cell.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java\n    Use new comparator names.\nA src/java/org/apache/hadoop/hbase/util/TestBytes.java\n    Small bytes test.\nM src/java/org/apache/hadoop/hbase/util/Bytes.java\n    Some overrides to help when source is ByteBuffer.\n    Added in some vint math utility.\nM src/java/org/apache/hadoop/hbase/client/HTable.java\n    Formatting.\nM src/java/org/apache/hadoop/hbase/client/MetaScanner.java\n    Use Bytes.compareTO.\nM src/java/org/apache/hadoop/hbase/client/HConnectionManager.java\n    Use right comparator.\nM src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java\n    Make it match HCD.\nM src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java\n    Fix NPE\nM src/java/org/apache/hadoop/hbase/client/ScannerCallable.java\n    Formatting.\nM bin/HBase.rb\n    Support for new HCD.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hbase/trunk@749546 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/08693f66a877ef782ac706e8425aba6fcb72d267",
        "parent": "https://github.com/apache/hbase/commit/8206da62c0b4f0f87c97ec21572e8253664b90d0",
        "bug_id": "hbase_321",
        "file": [
            {
                "sha": "607fc9fa34f54287d5e2a46c3e01921426ae50e5",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/CHANGES.txt",
                "status": "modified",
                "changes": 3,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -26,6 +26,9 @@ Release 0.20.0 - Unreleased\n    HBASE-1198  OOME in IPC server does not trigger abort behavior\n    HBASE-1209  Make port displayed the same as is used in URL for RegionServer\n                table in UI (Lars George via Stack)\n+   HBASE-1217  add new compression and hfile blocksize to HColumnDescriptor\n+   HBASE-859   HStoreKey needs a reworking\n+   HBASE-1211  NPE in retries exhausted exception\n \n   IMPROVEMENTS\n    HBASE-1089  Add count of regions on filesystem to master UI; add percentage",
                "deletions": 0
            },
            {
                "sha": "ef7909e4eb2b4720d6fc0985cac19a1f6e83197b",
                "filename": "src/java/org/apache/hadoop/hbase/HColumnDescriptor.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HColumnDescriptor.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HColumnDescriptor.java",
                "status": "modified",
                "changes": 142,
                "additions": 94,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HColumnDescriptor.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -27,6 +27,8 @@\n import java.util.Map;\n \n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n import org.apache.hadoop.hbase.rest.exception.HBaseRestException;\n import org.apache.hadoop.hbase.rest.serializer.IRestSerializer;\n import org.apache.hadoop.hbase.rest.serializer.ISerializable;\n@@ -52,11 +54,12 @@\n   // Time-to-live feature.  Version 4 was when we moved to byte arrays, HBASE-82.\n   // Version 5 was when bloom filter descriptors were removed.\n   // Version 6 adds metadata as a map where keys and values are byte[].\n-  private static final byte COLUMN_DESCRIPTOR_VERSION = (byte)6;\n+  private static final byte COLUMN_DESCRIPTOR_VERSION = (byte)7;\n \n   /** \n    * The type of compression.\n    * @see org.apache.hadoop.io.SequenceFile.Writer\n+   * @deprecated Replaced by {@link Compression.Algorithm}.\n    */\n   public static enum CompressionType {\n     /** Do not compress records. */\n@@ -67,20 +70,21 @@\n     BLOCK\n   }\n \n-  public static final String COMPRESSION = \"COMPRESSION\";       //TODO: change to protected\n-  public static final String BLOCKCACHE = \"BLOCKCACHE\";         //TODO: change to protected\n-  public static final String LENGTH = \"LENGTH\";                 //TODO: change to protected\n-  public static final String TTL = \"TTL\";                       //TODO: change to protected\n-  public static final String BLOOMFILTER = \"BLOOMFILTER\";       //TODO: change to protected\n-  public static final String FOREVER = \"FOREVER\";               //TODO: change to protected\n-  public static final String MAPFILE_INDEX_INTERVAL =           //TODO: change to protected\n+  public static final String COMPRESSION = \"COMPRESSION\";\n+  public static final String BLOCKCACHE = \"BLOCKCACHE\";\n+  public static final String BLOCKSIZE = \"BLOCKSIZE\";\n+  public static final String LENGTH = \"LENGTH\";\n+  public static final String TTL = \"TTL\";\n+  public static final String BLOOMFILTER = \"BLOOMFILTER\";\n+  public static final String FOREVER = \"FOREVER\";\n+  public static final String MAPFILE_INDEX_INTERVAL =\n       \"MAPFILE_INDEX_INTERVAL\";\n \n   /**\n    * Default compression type.\n    */\n-  public static final CompressionType DEFAULT_COMPRESSION =\n-    CompressionType.NONE;\n+  public static final String DEFAULT_COMPRESSION =\n+    Compression.Algorithm.NONE.getName();\n \n   /**\n    * Default number of versions of a record to keep.\n@@ -100,6 +104,12 @@\n    */\n   private volatile Integer maxValueLength = null;\n \n+  /*\n+   * Cache here the HCD value.\n+   * Question: its OK to cache since when we're reenable, we create a new HCD?\n+   */\n+  private volatile Integer blocksize = null;\n+\n   /**\n    * Default setting for whether to serve from memory or not.\n    */\n@@ -110,6 +120,12 @@\n    */\n   public static final boolean DEFAULT_BLOCKCACHE = false;\n \n+  /**\n+   * Default size of blocks in files store to the filesytem.  Use smaller for\n+   * faster random-access at expense of larger indices (more memory consumption).\n+   */\n+  public static final int DEFAULT_BLOCKSIZE = HFile.DEFAULT_BLOCKSIZE;\n+\n   /**\n    * Default setting for whether or not to use bloomfilters.\n    */\n@@ -123,16 +139,10 @@\n   // Column family name\n   private byte [] name;\n \n-  /**\n-   * Default mapfile index interval.\n-   */\n-  public static final int DEFAULT_MAPFILE_INDEX_INTERVAL = 128;\n-\n   // Column metadata\n   protected Map<ImmutableBytesWritable,ImmutableBytesWritable> values =\n     new HashMap<ImmutableBytesWritable,ImmutableBytesWritable>();\n \n-\n   /**\n    * Default constructor. Must be present for Writable.\n    */\n@@ -200,9 +210,37 @@ public HColumnDescriptor(HColumnDescriptor desc) {\n    * @throws IllegalArgumentException if the number of versions is &lt;= 0\n    */\n   public HColumnDescriptor(final byte [] familyName, final int maxVersions,\n-      final CompressionType compression, final boolean inMemory,\n+      final String compression, final boolean inMemory,\n       final boolean blockCacheEnabled, final int maxValueLength,\n       final int timeToLive, final boolean bloomFilter) {\n+    this(familyName, maxVersions, compression, inMemory, blockCacheEnabled,\n+      DEFAULT_BLOCKSIZE, maxValueLength, timeToLive, bloomFilter);\n+  }\n+\n+  /**\n+   * Constructor\n+   * @param familyName Column family name. Must be 'printable' -- digit or\n+   * letter -- and end in a <code>:<code>\n+   * @param maxVersions Maximum number of versions to keep\n+   * @param compression Compression type\n+   * @param inMemory If true, column data should be kept in an HRegionServer's\n+   * cache\n+   * @param blockCacheEnabled If true, MapFile blocks should be cached\n+   * @param maxValueLength Restrict values to &lt;= this value\n+   * @param timeToLive Time-to-live of cell contents, in seconds\n+   * (use HConstants.FOREVER for unlimited TTL)\n+   * @param bloomFilter Enable the specified bloom filter for this column\n+   * \n+   * @throws IllegalArgumentException if passed a family name that is made of \n+   * other than 'word' characters: i.e. <code>[a-zA-Z_0-9]</code> and does not\n+   * end in a <code>:</code>\n+   * @throws IllegalArgumentException if the number of versions is &lt;= 0\n+   */\n+  public HColumnDescriptor(final byte [] familyName, final int maxVersions,\n+      final String compression, final boolean inMemory,\n+      final boolean blockCacheEnabled, final int blocksize,\n+      final int maxValueLength,\n+      final int timeToLive, final boolean bloomFilter) {\n     isLegalFamilyName(familyName);\n     this.name = stripColon(familyName);\n     if (maxVersions <= 0) {\n@@ -215,10 +253,12 @@ public HColumnDescriptor(final byte [] familyName, final int maxVersions,\n     setBlockCacheEnabled(blockCacheEnabled);\n     setMaxValueLength(maxValueLength);\n     setTimeToLive(timeToLive);\n-    setCompressionType(compression);\n+    setCompressionType(Compression.Algorithm.\n+      valueOf(compression.toUpperCase()));\n     setBloomfilter(bloomFilter);\n+    setBlocksize(blocksize);\n   }\n-  \n+\n   private static byte [] stripColon(final byte [] n) {\n     byte [] result = new byte [n.length - 1];\n     // Have the stored family name be absent the colon delimiter\n@@ -321,15 +361,8 @@ public void setValue(String key, String value) {\n \n   /** @return compression type being used for the column family */\n   @TOJSON\n-  public CompressionType getCompression() {\n-    String value = getValue(COMPRESSION);\n-    if (value != null) {\n-      if (value.equalsIgnoreCase(\"BLOCK\"))\n-        return CompressionType.BLOCK;\n-      else if (value.equalsIgnoreCase(\"RECORD\"))\n-        return CompressionType.RECORD;\n-    }\n-    return CompressionType.NONE;\n+  public Compression.Algorithm getCompression() {\n+    return Compression.Algorithm.valueOf(getValue(COMPRESSION));\n   }\n   \n   /** @return maximum number of versions */\n@@ -347,24 +380,44 @@ public int getMaxVersions() {\n   public void setMaxVersions(int maxVersions) {\n     setValue(HConstants.VERSIONS, Integer.toString(maxVersions));\n   }\n-  \n+\n+  /**\n+   * @return Blocksize.\n+   */\n+  @TOJSON\n+  public synchronized int getBlocksize() {\n+    if (this.blocksize == null) {\n+      String value = getValue(BLOCKSIZE);\n+      this.blocksize = (value != null)?\n+        Integer.decode(value): Integer.valueOf(DEFAULT_BLOCKSIZE);\n+    }\n+    return this.blocksize.intValue();\n+  }\n+\n+  /**\n+   * @param s\n+   */\n+  public void setBlocksize(int s) {\n+    setValue(BLOCKSIZE, Integer.toString(s));\n+    this.blocksize = null;\n+  }\n+\n   /**\n    * @return Compression type setting.\n    */\n   @TOJSON\n-  public CompressionType getCompressionType() {\n+  public Compression.Algorithm getCompressionType() {\n     return getCompression();\n   }\n \n   /**\n    * @param type Compression type setting.\n    */\n-  public void setCompressionType(CompressionType type) {\n+  public void setCompressionType(Compression.Algorithm type) {\n     String compressionType;\n     switch (type) {\n-      case BLOCK:  compressionType = \"BLOCK\";   break;\n-      case RECORD: compressionType = \"RECORD\";  break;\n-      default:     compressionType = \"NONE\";    break;\n+      case GZ: compressionType = \"GZ\"; break;\n+      default: compressionType = \"NONE\"; break;\n     }\n     setValue(COMPRESSION, compressionType);\n   }\n@@ -461,17 +514,6 @@ public void setBloomfilter(final boolean onOff) {\n     setValue(BLOOMFILTER, Boolean.toString(onOff));\n   }\n \n-  /**\n-   * @return The number of entries that are added to the store MapFile before\n-   * an index entry is added.\n-   */\n-  public int getMapFileIndexInterval() {\n-    String value = getValue(MAPFILE_INDEX_INTERVAL);\n-    if (value != null)\n-      return Integer.valueOf(value).intValue();\n-    return DEFAULT_MAPFILE_INDEX_INTERVAL;\n-  }\n-\n   /**\n    * @param interval The number of entries that are added to the store MapFile before\n    * an index entry is added.\n@@ -531,7 +573,7 @@ public void readFields(DataInput in) throws IOException {\n       this.values.clear();\n       setMaxVersions(in.readInt());\n       int ordinal = in.readInt();\n-      setCompressionType(CompressionType.values()[ordinal]);\n+      setCompressionType(Compression.Algorithm.values()[ordinal]);\n       setInMemory(in.readBoolean());\n       setMaxValueLength(in.readInt());\n       setBloomfilter(in.readBoolean());\n@@ -551,7 +593,7 @@ public void readFields(DataInput in) throws IOException {\n        setTimeToLive(in.readInt());\n       }\n     } else {\n-      // version 6+\n+      // version 7+\n       this.name = Bytes.readByteArray(in);\n       this.values.clear();\n       int numValues = in.readInt();\n@@ -562,6 +604,10 @@ public void readFields(DataInput in) throws IOException {\n         value.readFields(in);\n         values.put(key, value);\n       }\n+      if (version == 6) {\n+        // Convert old values.\n+        setValue(COMPRESSION, Compression.Algorithm.NONE.getName());\n+      }\n     }\n   }\n \n@@ -597,4 +643,4 @@ else if (result > 0)\n   public void restSerialize(IRestSerializer serializer) throws HBaseRestException {\n     serializer.serializeColumnDescriptor(this);    \n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 48
            },
            {
                "sha": "be0fc3386511dabf3fec3ca33941d48ddc9368de",
                "filename": "src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HRegionInfo.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionInfo.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -427,13 +427,13 @@ public int compareTo(HRegionInfo o) {\n     }\n \n     // Compare start keys.\n-    result = HStoreKey.compareTwoRowKeys(this.startKey, o.startKey);\n+    result = Bytes.compareTo(this.startKey, o.startKey);\n     if (result != 0) {\n       return result;\n     }\n     \n     // Compare end keys.\n-    return HStoreKey.compareTwoRowKeys(this.endKey, o.endKey);\n+    return Bytes.compareTo(this.endKey, o.endKey);\n   }\n \n   /**",
                "deletions": 2
            },
            {
                "sha": "c2f204ed04b8599a38d2e3259e99c6e242a1f137",
                "filename": "src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "status": "modified",
                "changes": 710,
                "additions": 442,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreKey.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -20,10 +20,8 @@\n package org.apache.hadoop.hbase;\n \n \n-import java.io.ByteArrayOutputStream;\n import java.io.DataInput;\n import java.io.DataOutput;\n-import java.io.DataOutputStream;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n \n@@ -44,10 +42,6 @@\n    * Colon character in UTF-8\n    */\n   public static final char COLUMN_FAMILY_DELIMITER = ':';\n-  \n-  private byte [] row = HConstants.EMPTY_BYTE_ARRAY;\n-  private byte [] column = HConstants.EMPTY_BYTE_ARRAY;\n-  private long timestamp = Long.MAX_VALUE;\n \n   /**\n    * Estimated size tax paid for each instance of HSK.  Estimate based on\n@@ -56,9 +50,17 @@\n   // In jprofiler, says shallow size is 48 bytes.  Add to it cost of two\n   // byte arrays and then something for the HRI hosting.\n   public static final int ESTIMATED_HEAP_TAX = 48;\n-  \n-  public static final StoreKeyByteComparator BYTECOMPARATOR =\n-    new StoreKeyByteComparator();\n+\n+  private byte [] row = HConstants.EMPTY_BYTE_ARRAY;\n+  private byte [] column = HConstants.EMPTY_BYTE_ARRAY;\n+  private long timestamp = Long.MAX_VALUE;\n+\n+  private static final HStoreKey.StoreKeyComparator PLAIN_COMPARATOR =\n+    new HStoreKey.StoreKeyComparator();\n+  private static final HStoreKey.StoreKeyComparator META_COMPARATOR =\n+    new HStoreKey.MetaStoreKeyComparator();\n+  private static final HStoreKey.StoreKeyComparator ROOT_COMPARATOR =\n+    new HStoreKey.RootStoreKeyComparator();\n \n   /** Default constructor used in conjunction with Writable interface */\n   public HStoreKey() {\n@@ -93,7 +95,6 @@ public HStoreKey(final String row) {\n    * \n    * @param row row key\n    * @param timestamp timestamp value\n-   * @param hri HRegionInfo\n    */\n   public HStoreKey(final byte [] row, final long timestamp) {\n     this(row, HConstants.EMPTY_BYTE_ARRAY, timestamp);\n@@ -130,7 +131,6 @@ public HStoreKey(final byte [] row, final byte [] column) {\n    * @param row row key\n    * @param column column key\n    * @param timestamp timestamp value\n-   * @param regionInfo region info\n    */\n   public HStoreKey(final String row, final String column, final long timestamp) {\n     this (Bytes.toBytes(row), Bytes.toBytes(column), timestamp);\n@@ -143,7 +143,6 @@ public HStoreKey(final String row, final String column, final long timestamp) {\n    * @param row row key\n    * @param column column key\n    * @param timestamp timestamp value\n-   * @param regionInfo region info\n    */\n   public HStoreKey(final byte [] row, final byte [] column, final long timestamp) {\n     // Make copies\n@@ -256,7 +255,7 @@ public boolean matchesWithoutColumn(final HStoreKey other) {\n   public boolean matchesRowFamily(final HStoreKey that) {\n     final int delimiterIndex = getFamilyDelimiterIndex(getColumn());\n     return equalsTwoRowKeys(getRow(), that.getRow()) &&\n-      Bytes.compareTo(getColumn(), 0, delimiterIndex, that.getColumn(), 0,\n+    Bytes.compareTo(getColumn(), 0, delimiterIndex, that.getColumn(), 0,\n         delimiterIndex) == 0;\n   }\n   \n@@ -277,34 +276,44 @@ public boolean equals(final Object obj) {\n     }\n     return compareTo(other) == 0;\n   }\n-  \n-  @Override\n+\n   public int hashCode() {\n-    int result = Bytes.hashCode(getRow());\n-    result ^= Bytes.hashCode(getColumn());\n-    result ^= getTimestamp();\n-    return result;\n+    int c = Bytes.hashCode(getRow());\n+    c ^= Bytes.hashCode(getColumn());\n+    c ^= getTimestamp();\n+    return c;\n   }\n \n   // Comparable\n \n+  /**\n+   * @deprecated Use Comparators instead.  This can give wrong results.\n+   */\n   public int compareTo(final HStoreKey o) {\n     return compareTo(this, o);\n   }\n+\n+  /**\n+   * @param left\n+   * @param right\n+   * @return\n+   * @deprecated Use Comparators instead.  This can give wrong results because\n+   * does not take into account special handling needed for meta and root rows.\n+   */\n   static int compareTo(final HStoreKey left, final HStoreKey right) {\n     // We can be passed null\n     if (left == null && right == null) return 0;\n     if (left == null) return -1;\n     if (right == null) return 1;\n     \n-    int result = compareTwoRowKeys(left.getRow(), right.getRow());\n+    int result = Bytes.compareTo(left.getRow(), right.getRow());\n     if (result != 0) {\n       return result;\n     }\n     result = left.getColumn() == null && right.getColumn() == null? 0:\n       left.getColumn() == null && right.getColumn() != null? -1:\n         left.getColumn() != null && right.getColumn() == null? 1:\n-      Bytes.compareTo(left.getColumn(), right.getColumn());\n+          Bytes.compareTo(left.getColumn(), right.getColumn());\n     if (result != 0) {\n       return result;\n     }\n@@ -365,7 +374,7 @@ public static boolean matchingFamily(final byte [] family,\n     }\n     return Bytes.compareTo(family, 0, index, column, 0, index) == 0;\n   }\n-  \n+\n   /**\n    * @param family\n    * @return Return <code>family</code> plus the family delimiter.\n@@ -413,39 +422,66 @@ public static boolean matchingFamily(final byte [] family,\n       len);\n     return result;\n   }\n-  \n+\n   /**\n    * @param b\n    * @return Index of the family-qualifier colon delimiter character in passed\n    * buffer.\n    */\n   public static int getFamilyDelimiterIndex(final byte [] b) {\n+    return getDelimiter(b, 0, b.length, (int)COLUMN_FAMILY_DELIMITER);\n+  }\n+\n+  private static int getRequiredDelimiterInReverse(final byte [] b,\n+      final int offset, final int length, final int delimiter) {\n+    int index = getDelimiterInReverse(b, offset, length, delimiter);\n+    if (index < 0) {\n+      throw new IllegalArgumentException(\"No \" + delimiter + \" in <\" +\n+        Bytes.toString(b) + \">\" + \", length=\" + length + \", offset=\" + offset);\n+    }\n+    return index;\n+  }\n+  /*\n+   * @param b\n+   * @param delimiter\n+   * @return Index of delimiter having started from end of <code>b</code> moving\n+   * leftward.\n+   */\n+  private static int getDelimiter(final byte [] b, int offset, final int length,\n+      final int delimiter) {\n     if (b == null) {\n       throw new NullPointerException();\n     }\n     int result = -1;\n-    for (int i = 0; i < b.length; i++) {\n-      if (b[i] == COLUMN_FAMILY_DELIMITER) {\n+    for (int i = offset; i < length + offset; i++) {\n+      if (b[i] == delimiter) {\n         result = i;\n         break;\n       }\n     }\n     return result;\n   }\n \n-  /**\n-   * Utility method to compare two row keys.\n-   * This is required because of the meta delimiters.\n-   * This is a hack.\n-   * @param regionInfo\n-   * @param rowA\n-   * @param rowB\n-   * @return value of the comparison\n+  /*\n+   * @param b\n+   * @param delimiter\n+   * @return Index of delimiter\n    */\n-  public static int compareTwoRowKeys(final byte[] rowA, final byte[] rowB) {\n-    return Bytes.compareTo(rowA, rowB);\n+  private static int getDelimiterInReverse(final byte [] b, final int offset,\n+      final int length, final int delimiter) {\n+    if (b == null) {\n+      throw new NullPointerException();\n+    }\n+    int result = -1;\n+    for (int i = (offset + length) - 1; i >= offset; i--) {\n+      if (b[i] == delimiter) {\n+        result = i;\n+        break;\n+      }\n+    }\n+    return result;\n   }\n-  \n+\n   /**\n    * Utility method to check if two row keys are equal.\n    * This is required because of the meta delimiters\n@@ -457,7 +493,7 @@ public static int compareTwoRowKeys(final byte[] rowA, final byte[] rowB) {\n   public static boolean equalsTwoRowKeys(final byte[] rowA, final byte[] rowB) {\n     return ((rowA == null) && (rowB == null)) ? true:\n       (rowA == null) || (rowB == null) || (rowA.length != rowB.length) ? false:\n-        compareTwoRowKeys(rowA,rowB) == 0;\n+        Bytes.compareTo(rowA, rowB) == 0;\n   }\n \n   // Writable\n@@ -518,153 +554,38 @@ public long heapSize() {\n    * @throws IOException\n    */\n   public static byte [] getBytes(final HStoreKey hsk) throws IOException {\n-    // TODO: Redo with system.arraycopy instead of DOS.\n-    if (hsk == null) {\n-      throw new IllegalArgumentException(\"Writable cannot be null\");\n-    }\n-    final int serializedSize = getSerializedSize(hsk);\n-    final ByteArrayOutputStream byteStream = new ByteArrayOutputStream(serializedSize);\n-    DataOutputStream out = new DataOutputStream(byteStream);\n-    try {\n-      hsk.write(out);\n-      out.close();\n-      out = null;\n-      final byte [] serializedKey = byteStream.toByteArray();\n-      if (serializedKey.length != serializedSize) {\n-        // REMOVE THIS AFTER CONFIDENCE THAT OUR SIZING IS BEING DONE PROPERLY\n-        throw new AssertionError(\"Sizes do not agree \" + serializedKey.length +\n-          \", \" + serializedSize);\n-      }\n-      return serializedKey;\n-    } finally {\n-      if (out != null) {\n-        out.close();\n-      }\n-    }\n+    return getBytes(hsk.getRow(), hsk.getColumn(), hsk.getTimestamp());\n   }\n-  \n+\n   /**\n-   * Pass this class into {@link org.apache.hadoop.io.MapFile}.getClosest when\n-   * searching for the key that comes BEFORE this one but NOT this one.  This\n-   * class will return > 0 when asked to compare against itself rather than 0.\n-   * This is a hack for case where getClosest returns a deleted key and we want\n-   * to get the previous.  Can't unless use use this class; it'll just keep\n-   * returning us the deleted key (getClosest gets exact or nearest before when\n-   * you pass true argument).  TODO: Throw this class away when MapFile has\n-   * a real 'previous' method.  See HBASE-751.\n-   * @deprecated\n+   * @param row Can't be null\n+   * @return Passed arguments as a serialized HSK.\n+   * @throws IOException\n    */\n-  public static class BeforeThisStoreKey extends HStoreKey {\n-    private final HStoreKey beforeThisKey;\n-\n-    /**\n-     * @param beforeThisKey \n-     */\n-    public BeforeThisStoreKey(final HStoreKey beforeThisKey) {\n-      super();\n-      this.beforeThisKey = beforeThisKey;\n-    }\n-    \n-    @Override\n-    public int compareTo(final HStoreKey o) {\n-      final int result = this.beforeThisKey.compareTo(o);\n-      return result == 0? -1: result;\n-    }\n-    \n-    @Override\n-    public boolean equals(final Object obj) {\n-      return false;\n-    }\n-\n-    @Override\n-    public byte[] getColumn() {\n-      return this.beforeThisKey.getColumn();\n-    }\n-\n-    @Override\n-    public byte[] getRow() {\n-      return this.beforeThisKey.getRow();\n-    }\n-\n-    @Override\n-    public long heapSize() {\n-      return this.beforeThisKey.heapSize();\n-    }\n-\n-    @Override\n-    public long getTimestamp() {\n-      return this.beforeThisKey.getTimestamp();\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-      return this.beforeThisKey.hashCode();\n-    }\n-\n-    @Override\n-    public boolean matchesRowCol(final HStoreKey other) {\n-      return this.beforeThisKey.matchesRowCol(other);\n-    }\n-\n-    @Override\n-    public boolean matchesRowFamily(final HStoreKey that) {\n-      return this.beforeThisKey.matchesRowFamily(that);\n-    }\n-\n-    @Override\n-    public boolean matchesWithoutColumn(final HStoreKey other) {\n-      return this.beforeThisKey.matchesWithoutColumn(other);\n-    }\n-\n-    @Override\n-    public void readFields(final DataInput in) throws IOException {\n-      this.beforeThisKey.readFields(in);\n-    }\n-\n-    @Override\n-    public void set(final HStoreKey k) {\n-      this.beforeThisKey.set(k);\n-    }\n-\n-    @Override\n-    public void setColumn(final byte[] c) {\n-      this.beforeThisKey.setColumn(c);\n-    }\n-\n-    @Override\n-    public void setRow(final byte[] newrow) {\n-      this.beforeThisKey.setRow(newrow);\n-    }\n-\n-    @Override\n-    public void setVersion(final long timestamp) {\n-      this.beforeThisKey.setVersion(timestamp);\n-    }\n-\n-    @Override\n-    public String toString() {\n-      return this.beforeThisKey.toString();\n-    }\n-\n-    @Override\n-    public void write(final DataOutput out) throws IOException {\n-      this.beforeThisKey.write(out);\n-    }\n+  public static byte [] getBytes(final byte [] row)\n+  throws IOException {\n+    return getBytes(row, null, HConstants.LATEST_TIMESTAMP);\n   }\n \n   /**\n-   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   * @param row Can't be null\n+   * @param column Can be null\n+   * @param ts\n+   * @return Passed arguments as a serialized HSK.\n+   * @throws IOException\n    */\n-  public static class HStoreKeyWritableComparator extends WritableComparator {\n-    public HStoreKeyWritableComparator() {\n-      super(HStoreKey.class);\n-    }\n-    \n-    @SuppressWarnings(\"unchecked\")\n-    public int compare(final WritableComparable left,\n-        final WritableComparable right) {\n-      return compareTo((HStoreKey)left, (HStoreKey)right);\n-    }\n+  public static byte [] getBytes(final byte [] row, final byte [] column,\n+    final long ts)\n+  throws IOException {\n+    // TODO: Get vint sizes as I calculate serialized size of hsk.\n+    byte [] b = new byte [getSerializedSize(row) +\n+      getSerializedSize(column) + Bytes.SIZEOF_LONG];\n+    int offset = Bytes.writeByteArray(b, 0, row, 0, row.length);\n+    byte [] c = column == null? HConstants.EMPTY_BYTE_ARRAY: column;\n+    offset = Bytes.writeByteArray(b, offset, c, 0, c.length);\n+    byte [] timestamp = Bytes.toBytes(ts);\n+    System.arraycopy(timestamp, 0, b, offset, timestamp.length);\n+    return b;\n   }\n \n   /**\n@@ -688,17 +609,11 @@ public int compare(final WritableComparable left,\n    * @return Column\n    */\n   public static byte [] getColumn(final ByteBuffer bb) {\n-    byte firstByte = bb.get(0);\n+    // Skip over row.\n+    int offset = skipVintdByteArray(bb, 0);\n+    byte firstByte = bb.get(offset);\n     int vint = firstByte;\n     int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n-    if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(), bb.arrayOffset());\n-    }\n-    // Skip over row.\n-    int offset = vint + vintWidth;\n-    firstByte = bb.get(offset);\n-    vint = firstByte;\n-    vintWidth = WritableUtils.decodeVIntSize(firstByte);\n     if (vintWidth != 1) {\n       vint = getBigVint(vintWidth, firstByte, bb.array(),\n         bb.arrayOffset() + offset);\n@@ -714,32 +629,318 @@ public int compare(final WritableComparable left,\n    * @return Timestamp\n    */\n   public static long getTimestamp(final ByteBuffer bb) {\n-    byte firstByte = bb.get(0);\n+    return bb.getLong(bb.limit() - Bytes.SIZEOF_LONG);\n+  }\n+\n+  /*\n+   * @param bb\n+   * @param offset\n+   * @return Amount to skip to get paste a byte array that is preceded by a\n+   * vint of how long it is.\n+   */\n+  private static int skipVintdByteArray(final ByteBuffer bb, final int offset) {\n+    byte firstByte = bb.get(offset);\n     int vint = firstByte;\n     int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n     if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(), bb.arrayOffset());\n+      vint = getBigVint(vintWidth, firstByte, bb.array(),\n+        bb.arrayOffset() + offset);\n     }\n+    return vint + vintWidth + offset;\n+  }\n+\n+  /*\n+   * Vint is wider than one byte.  Find out how much bigger it is.\n+   * @param vintWidth\n+   * @param firstByte\n+   * @param buffer\n+   * @param offset\n+   * @return\n+   */\n+  static int getBigVint(final int vintWidth, final byte firstByte,\n+      final byte [] buffer, final int offset) {\n+    long i = 0;\n+    for (int idx = 0; idx < vintWidth - 1; idx++) {\n+      final byte b = buffer[offset + 1 + idx];\n+      i = i << 8;\n+      i = i | (b & 0xFF);\n+    }\n+    i = (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n+    if (i > Integer.MAX_VALUE) {\n+      throw new IllegalArgumentException(\"Calculated vint too large\");\n+    }\n+    return (int)i;\n+  }\n+\n+  /**\n+   * Create a store key.\n+   * @param bb\n+   * @return HStoreKey instance made of the passed <code>b</code>.\n+   * @throws IOException\n+   */\n+  public static HStoreKey create(final ByteBuffer bb)\n+  throws IOException {\n+    return HStoreKey.create(bb.array(), bb.arrayOffset(), bb.limit());\n+  }\n+\n+  /**\n+   * Create a store key.\n+   * @param b Serialized HStoreKey; a byte array with a row only in it won't do.\n+   * It must have all the vints denoting r/c/ts lengths.\n+   * @return HStoreKey instance made of the passed <code>b</code>.\n+   * @throws IOException\n+   */\n+  public static HStoreKey create(final byte [] b) throws IOException {\n+    return create(b, 0, b.length);\n+  }\n+\n+  /**\n+   * Create a store key.\n+   * @param b Serialized HStoreKey\n+   * @param offset\n+   * @param length\n+   * @return HStoreKey instance made of the passed <code>b</code>.\n+   * @throws IOException\n+   */\n+  public static HStoreKey create(final byte [] b, final int offset,\n+    final int length)\n+  throws IOException {\n+    byte firstByte = b[offset];\n+    int vint = firstByte;\n+    int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n+    if (vintWidth != 1) {\n+      vint = getBigVint(vintWidth, firstByte, b, offset);\n+    }\n+    byte [] row = new byte [vint];\n+    System.arraycopy(b, offset + vintWidth,\n+      row, 0, row.length);\n     // Skip over row.\n-    int offset = vint + vintWidth;\n-    firstByte = bb.get(offset);\n+    int extraOffset = vint + vintWidth;\n+    firstByte = b[offset + extraOffset];\n     vint = firstByte;\n     vintWidth = WritableUtils.decodeVIntSize(firstByte);\n     if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(),\n-        bb.arrayOffset() + offset);\n+      vint = getBigVint(vintWidth, firstByte, b, offset + extraOffset);\n     }\n+    byte [] column = new byte [vint];\n+    System.arraycopy(b, offset + extraOffset + vintWidth,\n+      column, 0, column.length);\n     // Skip over column\n-    offset += (vint + vintWidth);\n-    return bb.getLong(offset);\n+    extraOffset += (vint + vintWidth);\n+    return new HStoreKey(row, column, Bytes.toLong(b, offset + extraOffset));\n+  }\n+\n+  /**\n+   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   * Use this comparing keys in the -ROOT_ table.\n+   */\n+  public static class HStoreKeyRootComparator extends HStoreKeyMetaComparator {\n+    protected int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareRootRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   * Use this comprator for keys in the .META. table.\n+   */\n+  public static class HStoreKeyMetaComparator extends HStoreKeyComparator {\n+    protected int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareMetaRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * Passed as comparator for memcache and for store files.  See HBASE-868.\n+   */\n+  public static class HStoreKeyComparator extends WritableComparator {\n+    public HStoreKeyComparator() {\n+      super(HStoreKey.class);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public int compare(final WritableComparable l,\n+        final WritableComparable r) {\n+      HStoreKey left = (HStoreKey)l;\n+      HStoreKey right = (HStoreKey)r;\n+      // We can be passed null\n+      if (left == null && right == null) return 0;\n+      if (left == null) return -1;\n+      if (right == null) return 1;\n+      \n+      byte [] lrow = left.getRow();\n+      byte [] rrow = right.getRow();\n+      int result = compareRows(lrow, 0, lrow.length, rrow, 0, rrow.length);\n+      if (result != 0) {\n+        return result;\n+      }\n+      result = left.getColumn() == null && right.getColumn() == null? 0:\n+        left.getColumn() == null && right.getColumn() != null? -1:\n+          left.getColumn() != null && right.getColumn() == null? 1:\n+            Bytes.compareTo(left.getColumn(), right.getColumn());\n+      if (result != 0) {\n+        return result;\n+      }\n+      // The below older timestamps sorting ahead of newer timestamps looks\n+      // wrong but it is intentional. This way, newer timestamps are first\n+      // found when we iterate over a memcache and newer versions are the\n+      // first we trip over when reading from a store file.\n+      if (left.getTimestamp() < right.getTimestamp()) {\n+        result = 1;\n+      } else if (left.getTimestamp() > right.getTimestamp()) {\n+        result = -1;\n+      }\n+      return result;\n+    }\n+\n+    protected int compareRows(final byte [] left, final int loffset,\n+        final int llength, final byte [] right, final int roffset,\n+        final int rlength) {\n+      return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * StoreKeyComparator for the -ROOT- table.\n+   */\n+  public static class RootStoreKeyComparator\n+  extends MetaStoreKeyComparator {\n+    public int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareRootRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /**\n+   * StoreKeyComparator for the .META. table.\n+   */\n+  public static class MetaStoreKeyComparator extends StoreKeyComparator {\n+    public int compareRows(byte [] left, int loffset, int llength,\n+        byte [] right, int roffset, int rlength) {\n+      return compareMetaRows(left, loffset, llength, right, roffset, rlength);\n+    }\n+  }\n+\n+  /*\n+   * @param left\n+   * @param loffset\n+   * @param llength\n+   * @param right\n+   * @param roffset\n+   * @param rlength\n+   * @return Result of comparing two rows from the -ROOT- table both of which\n+   * are of the form .META.,(TABLE,REGIONNAME,REGIONID),REGIONID.\n+   */\n+  protected static int compareRootRows(byte [] left, int loffset, int llength,\n+      byte [] right, int roffset, int rlength) {\n+    // Rows look like this: .META.,ROW_FROM_META,RID\n+    // System.out.println(\"ROOT \" + Bytes.toString(left, loffset, llength) +\n+    //  \"---\" + Bytes.toString(right, roffset, rlength));\n+    int lmetaOffsetPlusDelimiter = loffset + 7; // '.META.,'\n+    int leftFarDelimiter = getDelimiterInReverse(left, lmetaOffsetPlusDelimiter,\n+      llength - lmetaOffsetPlusDelimiter, HRegionInfo.DELIMITER);\n+    int rmetaOffsetPlusDelimiter = roffset + 7; // '.META.,'\n+    int rightFarDelimiter = getDelimiterInReverse(right,\n+      rmetaOffsetPlusDelimiter, rlength - rmetaOffsetPlusDelimiter,\n+      HRegionInfo.DELIMITER);\n+    if (leftFarDelimiter < 0 && rightFarDelimiter >= 0) {\n+      // Nothing between .META. and regionid.  Its first key.\n+      return -1;\n+    } else if (rightFarDelimiter < 0 && leftFarDelimiter >= 0) {\n+       return 1;\n+    } else if (leftFarDelimiter < 0 && rightFarDelimiter < 0) {\n+      return 0;\n+    }\n+    int result = compareMetaRows(left, lmetaOffsetPlusDelimiter,\n+      leftFarDelimiter - lmetaOffsetPlusDelimiter,\n+      right, rmetaOffsetPlusDelimiter,\n+      rightFarDelimiter - rmetaOffsetPlusDelimiter);\n+    if (result != 0) {\n+      return result;\n+    }\n+    // Compare last part of row, the rowid.\n+    leftFarDelimiter++;\n+    rightFarDelimiter++;\n+    result = compareRowid(left, leftFarDelimiter, llength - leftFarDelimiter,\n+      right, rightFarDelimiter, rlength - rightFarDelimiter);\n+    return result;\n+  }\n+\n+  /*\n+   * @param left\n+   * @param loffset\n+   * @param llength\n+   * @param right\n+   * @param roffset\n+   * @param rlength\n+   * @return Result of comparing two rows from the .META. table both of which\n+   * are of the form TABLE,REGIONNAME,REGIONID.\n+   */\n+  protected static int compareMetaRows(final byte[] left, final int loffset,\n+      final int llength, final byte[] right, final int roffset,\n+      final int rlength) {\n+//    System.out.println(\"META \" + Bytes.toString(left, loffset, llength) +\n+//      \"---\" + Bytes.toString(right, roffset, rlength));\n+    int leftDelimiter = getDelimiter(left, loffset, llength,\n+      HRegionInfo.DELIMITER);\n+    int rightDelimiter = getDelimiter(right, roffset, rlength,\n+      HRegionInfo.DELIMITER);\n+    if (leftDelimiter < 0 && rightDelimiter >= 0) {\n+      // Nothing between .META. and regionid.  Its first key.\n+      return -1;\n+    } else if (rightDelimiter < 0 && leftDelimiter >= 0) {\n+      return 1;\n+    } else if (leftDelimiter < 0 && rightDelimiter < 0) {\n+      return 0;\n+    }\n+    // Compare up to the delimiter\n+    int result = Bytes.compareTo(left, loffset, leftDelimiter - loffset,\n+      right, roffset, rightDelimiter - roffset);\n+    if (result != 0) {\n+      return result;\n+    }\n+    // Compare middle bit of the row.\n+    // Move past delimiter\n+    leftDelimiter++;\n+    rightDelimiter++;\n+    int leftFarDelimiter = getRequiredDelimiterInReverse(left, leftDelimiter,\n+        llength - (leftDelimiter - loffset), HRegionInfo.DELIMITER);\n+    int rightFarDelimiter = getRequiredDelimiterInReverse(right,\n+        rightDelimiter, rlength - (rightDelimiter - roffset),\n+        HRegionInfo.DELIMITER);\n+    // Now compare middlesection of row.\n+    result = Bytes.compareTo(left, leftDelimiter,\n+      leftFarDelimiter - leftDelimiter, right, rightDelimiter,\n+      rightFarDelimiter - rightDelimiter);\n+    if (result != 0) {\n+      return result;\n+    }\n+    // Compare last part of row, the rowid.\n+    leftFarDelimiter++;\n+    rightFarDelimiter++;\n+    result = compareRowid(left, leftFarDelimiter,\n+      llength - (leftFarDelimiter - loffset),\n+      right, rightFarDelimiter, rlength - (rightFarDelimiter - roffset));\n+    return result;\n+  }\n+\n+  private static int compareRowid(byte[] left, int loffset, int llength,\n+      byte[] right, int roffset, int rlength) {\n+    return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);\n   }\n \n   /**\n    * RawComparator for plain -- i.e. non-catalog table keys such as \n-   * -ROOT- and .META. -- HStoreKeys.  Compares at byte level.\n+   * -ROOT- and .META. -- HStoreKeys.  Compares at byte level.  Knows how to\n+   * handle the vints that introduce row and columns in the HSK byte array\n+   * representation. Adds\n+   * {@link #compareRows(byte[], int, int, byte[], int, int)} to\n+   * {@link RawComparator}\n    */\n-  public static class StoreKeyByteComparator implements RawComparator<byte []> {\n-    public StoreKeyByteComparator() {\n+  public static class StoreKeyComparator implements RawComparator<byte []> {\n+    public StoreKeyComparator() {\n       super();\n     }\n \n@@ -770,7 +971,7 @@ public int compare(final byte [] b1, int o1, int l1,\n         vint2 = getBigVint(vintWidth2, firstByte2, b2, o2);\n       }\n       // Compare the rows.\n-      int result = WritableComparator.compareBytes(b1, o1 + vintWidth1, vint1,\n+      int result = compareRows(b1, o1 + vintWidth1, vint1,\n           b2, o2 + vintWidth2, vint2);\n       if (result != 0) {\n         return result;\n@@ -797,8 +998,10 @@ public int compare(final byte [] b1, int o1, int l1,\n         vint2 = getBigVint(vintWidth2, firstByte2, b2, o2);\n       }\n       // Compare columns.\n-      result = WritableComparator.compareBytes(b1, o1 + vintWidth1, vint1,\n-          b2, o2 + vintWidth2, vint2);\n+      // System.out.println(\"COL <\" + Bytes.toString(b1, o1 + vintWidth1, vint1) +\n+      //  \"> <\" + Bytes.toString(b2, o2 + vintWidth2, vint2) + \">\");\n+      result = Bytes.compareTo(b1, o1 + vintWidth1, vint1,\n+        b2, o2 + vintWidth2, vint2);\n       if (result != 0) {\n         return result;\n       }\n@@ -825,88 +1028,59 @@ public int compare(final byte [] b1, int o1, int l1,\n       }\n       return 0;\n     }\n-  }\n \n-  /*\n-   * Vint is wider than one byte.  Find out how much bigger it is.\n-   * @param vintWidth\n-   * @param firstByte\n-   * @param buffer\n-   * @param offset\n-   * @return\n-   */\n-  static int getBigVint(final int vintWidth, final byte firstByte,\n-      final byte [] buffer, final int offset) {\n-    long i = 0;\n-    for (int idx = 0; idx < vintWidth - 1; idx++) {\n-      final byte b = buffer[offset + 1 + idx];\n-      i = i << 8;\n-      i = i | (b & 0xFF);\n+    /**\n+     * @param left\n+     * @param right\n+     * @return Result comparing rows.\n+     */\n+    public int compareRows(final byte [] left, final byte [] right) {\n+      return compareRows(left, 0, left.length, right, 0, right.length);\n     }\n-    i = (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n-    if (i > Integer.MAX_VALUE) {\n-      throw new IllegalArgumentException(\"Calculated vint too large\");\n+\n+    /**\n+     * @param left\n+     * @param loffset\n+     * @param llength\n+     * @param right\n+     * @param roffset\n+     * @param rlength\n+     * @return Result comparing rows.\n+     */\n+    public int compareRows(final byte [] left, final int loffset,\n+        final int llength, final byte [] right, final int roffset, final int rlength) {\n+      return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);\n     }\n-    return (int)i;\n   }\n \n   /**\n-   * Create a store key.\n-   * @param bb\n-   * @return HStoreKey instance made of the passed <code>b</code>.\n-   * @throws IOException\n+   * @param hri\n+   * @return Compatible comparator\n    */\n-  public static HStoreKey create(final ByteBuffer bb)\n-  throws IOException {\n-    byte firstByte = bb.get(0);\n-    int vint = firstByte;\n-    int vintWidth = WritableUtils.decodeVIntSize(firstByte);\n-    if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(), bb.arrayOffset());\n-    }\n-    byte [] row = new byte [vint];\n-    System.arraycopy(bb.array(), bb.arrayOffset() + vintWidth,\n-      row, 0, row.length);\n-    // Skip over row.\n-    int offset = vint + vintWidth;\n-    firstByte = bb.get(offset);\n-    vint = firstByte;\n-    vintWidth = WritableUtils.decodeVIntSize(firstByte);\n-    if (vintWidth != 1) {\n-      vint = getBigVint(vintWidth, firstByte, bb.array(),\n-        bb.arrayOffset() + offset);\n-    }\n-    byte [] column = new byte [vint];\n-    System.arraycopy(bb.array(), bb.arrayOffset() + offset + vintWidth,\n-      column, 0, column.length);\n-    // Skip over column\n-    offset += (vint + vintWidth);\n-    long ts = bb.getLong(offset);\n-    return new HStoreKey(row, column, ts);\n+  public static WritableComparator getWritableComparator(final HRegionInfo hri) {\n+    return hri.isRootRegion()?\n+        new HStoreKey.HStoreKeyRootComparator(): hri.isMetaRegion()?\n+          new HStoreKey.HStoreKeyMetaComparator():\n+            new HStoreKey.HStoreKeyComparator();\n   }\n \n   /**\n-   * Create a store key.\n-   * @param b Serialized HStoreKey; a byte array with a row only in it won't do.\n-   * It must have all the vints denoting r/c/ts lengths.\n-   * @return HStoreKey instance made of the passed <code>b</code>.\n-   * @throws IOException\n+   * @param hri\n+   * @return Compatible raw comparator\n    */\n-  public static HStoreKey create(final byte [] b) throws IOException {\n-    return create(b, 0, b.length);\n+  public static StoreKeyComparator getRawComparator(final HRegionInfo hri) {\n+    return hri.isRootRegion()? ROOT_COMPARATOR:\n+      hri.isMetaRegion()? META_COMPARATOR: META_COMPARATOR;\n   }\n \n   /**\n-   * Create a store key.\n-   * @param b Serialized HStoreKey\n-   * @param offset\n-   * @param length\n-   * @return HStoreKey instance made of the passed <code>b</code>.\n-   * @throws IOException\n+   * @param tablename\n+   * @return Compatible raw comparator\n    */\n-  public static HStoreKey create(final byte [] b, final int offset,\n-    final int length)\n-  throws IOException {\n-    return (HStoreKey)Writables.getWritable(b, offset, length, new HStoreKey());\n+  public static HStoreKey.StoreKeyComparator getComparator(final byte [] tablename) {\n+    return Bytes.equals(HTableDescriptor.ROOT_TABLEDESC.getName(), tablename)?\n+      ROOT_COMPARATOR:\n+      (Bytes.equals(HTableDescriptor.META_TABLEDESC.getName(),tablename))?\n+      META_COMPARATOR: PLAIN_COMPARATOR;\n   }\n }\n\\ No newline at end of file",
                "deletions": 268
            },
            {
                "sha": "a2e49d3c9dd564b6fb0d22a15162b7158a2abd1b",
                "filename": "src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/HTableDescriptor.java",
                "status": "modified",
                "changes": 10,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HTableDescriptor.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.rest.exception.HBaseRestException;\n import org.apache.hadoop.hbase.rest.serializer.IRestSerializer;\n import org.apache.hadoop.hbase.rest.serializer.ISerializable;\n@@ -667,19 +668,20 @@ public static Path getTableDir(Path rootdir, final byte [] tableName) {\n       HConstants.ROOT_TABLE_NAME,\n       new HColumnDescriptor[] { new HColumnDescriptor(HConstants.COLUMN_FAMILY,\n           10,  // Ten is arbitrary number.  Keep versions to help debuggging.\n-          HColumnDescriptor.CompressionType.NONE, false, true,\n+          Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,\n           Integer.MAX_VALUE, HConstants.FOREVER, false) });\n   \n   /** Table descriptor for <code>.META.</code> catalog table */\n   public static final HTableDescriptor META_TABLEDESC = new HTableDescriptor(\n       HConstants.META_TABLE_NAME, new HColumnDescriptor[] {\n           new HColumnDescriptor(HConstants.COLUMN_FAMILY,\n             10, // Ten is arbitrary number.  Keep versions to help debuggging.\n-            HColumnDescriptor.CompressionType.NONE, false, true,\n+            Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,\n             Integer.MAX_VALUE, HConstants.FOREVER, false),\n           new HColumnDescriptor(HConstants.COLUMN_FAMILY_HISTORIAN,\n-            HConstants.ALL_VERSIONS, HColumnDescriptor.CompressionType.NONE,\n-            false, false, Integer.MAX_VALUE, HConstants.WEEK_IN_SECONDS, false)});\n+            HConstants.ALL_VERSIONS, Compression.Algorithm.NONE.getName(),\n+            false, false,  8 * 1024,\n+            Integer.MAX_VALUE, HConstants.WEEK_IN_SECONDS, false)});\n \n   /* (non-Javadoc)\n    * @see org.apache.hadoop.hbase.rest.xml.IOutputXML#toXML()",
                "deletions": 4
            },
            {
                "sha": "710f7f3b9ea3a76c35def692b39ea0e481aa752c",
                "filename": "src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -65,6 +65,7 @@\n  * Used by {@link HTable} and {@link HBaseAdmin}\n  */\n public class HConnectionManager implements HConstants {\n+\n   /*\n    * Not instantiable.\n    */\n@@ -646,7 +647,7 @@ private HRegionLocation getCachedLocation(final byte [] tableName,\n           // signifying that the region we're checking is actually the last\n           // region in the table.\n           if (HStoreKey.equalsTwoRowKeys(endKey, HConstants.EMPTY_END_ROW) ||\n-              HStoreKey.compareTwoRowKeys(endKey, row) > 0) {\n+              HStoreKey.getComparator(tableName).compareRows(endKey, row) > 0) {\n             return possibleRegion;\n           }\n         }\n@@ -683,7 +684,7 @@ private void deleteCachedLocation(final byte [] tableName,\n \n           // by nature of the map, we know that the start key has to be < \n           // otherwise it wouldn't be in the headMap. \n-          if (HStoreKey.compareTwoRowKeys(endKey, row) <= 0) {\n+          if (HStoreKey.getComparator(tableName).compareRows(endKey, row) <= 0) {\n             // delete any matching entry\n             HRegionLocation rl =\n               tableLocations.remove(matchingRegions.lastKey());\n@@ -1023,4 +1024,4 @@ void close(boolean stopProxy) {\n       }\n     }\n   } \n-}\n+}\n\\ No newline at end of file",
                "deletions": 3
            },
            {
                "sha": "d0001454ebb62aa194b419723a3d412aeb48e570",
                "filename": "src/java/org/apache/hadoop/hbase/client/HTable.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/HTable.java",
                "status": "modified",
                "changes": 11,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/HTable.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -303,7 +303,7 @@ public Cell get(final String row, final String column)\n    * @return value for specified row/column\n    * @throws IOException\n    */\n-  public Cell[] get(final String row, final String column, int numVersions)\n+  public Cell [] get(final String row, final String column, int numVersions)\n   throws IOException {\n     return get(Bytes.toBytes(row), Bytes.toBytes(column), numVersions);\n   }\n@@ -337,7 +337,7 @@ public Cell call() throws IOException {\n    * @return Array of Cells.\n    * @throws IOException\n    */\n-  public Cell[] get(final byte [] row, final byte [] column,\n+  public Cell [] get(final byte [] row, final byte [] column,\n     final int numVersions) \n   throws IOException {\n     return connection.getRegionServerWithRetries(\n@@ -1276,11 +1276,12 @@ public Boolean call() throws IOException {\n           if (rl != null) {\n             lockId = rl.getLockId();\n           }\n-          return server.exists(location.getRegionInfo().getRegionName(), row,\n-            column, timestamp, lockId);\n+          return Boolean.valueOf(server.\n+            exists(location.getRegionInfo().getRegionName(), row,\n+            column, timestamp, lockId));\n         }\n       }\n-    );\n+    ).booleanValue();\n   }\n \n   /**",
                "deletions": 5
            },
            {
                "sha": "540aff1f1f8047d1e0de9d62b34479114018e7ff",
                "filename": "src/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/MetaScanner.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/MetaScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -7,6 +7,7 @@\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.io.RowResult;\n+import org.apache.hadoop.hbase.util.Bytes;\n \n /**\n  * Scanner class that contains the <code>.META.</code> table scanning logic \n@@ -69,7 +70,7 @@ public static void metaScan(HBaseConfiguration configuration,\n         callable.setClose();\n         connection.getRegionServerWithRetries(callable);\n       }\n-    } while (HStoreKey.compareTwoRowKeys(startRow, LAST_ROW) != 0);\n+    } while (Bytes.compareTo(startRow, LAST_ROW) != 0);\n   }\n \n   /**",
                "deletions": 1
            },
            {
                "sha": "bdc768cde14894a065546a34e57558113335a0d8",
                "filename": "src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -47,9 +47,9 @@ private static String getMessage(String serverName, final byte [] regionName,\n     StringBuilder buffer = new StringBuilder(\"Trying to contact region server \");\n     buffer.append(serverName);\n     buffer.append(\" for region \");\n-    buffer.append(Bytes.toString(regionName));\n+    buffer.append(regionName == null? \"\": Bytes.toString(regionName));\n     buffer.append(\", row '\");\n-    buffer.append(Bytes.toString(row));\n+    buffer.append(row == null? \"\": Bytes.toString(row));\n     buffer.append(\"', but failed after \");\n     buffer.append(numTries + 1);\n     buffer.append(\" attempts.\\nExceptions:\\n\");",
                "deletions": 2
            },
            {
                "sha": "ad48401ab7b4124cfd2f1741830b41a697ce8d32",
                "filename": "src/java/org/apache/hadoop/hbase/client/ScannerCallable.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/ScannerCallable.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/ScannerCallable.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/ScannerCallable.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -76,7 +76,7 @@ public void instantiateServer(boolean reload) throws IOException {\n       // open the scanner\n       scannerId = openScanner();\n     } else {\n-      RowResult[] rrs = server.next(scannerId, caching);\n+      RowResult [] rrs = server.next(scannerId, caching);\n       return rrs.length == 0 ? null : rrs;\n     }\n     return null;",
                "deletions": 1
            },
            {
                "sha": "ac4bfa3e7509acfc6bc9eec414c68565930dc09c",
                "filename": "src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -1,6 +1,7 @@\n package org.apache.hadoop.hbase.client;\n \n import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n \n /**\n  * Immutable HColumnDescriptor\n@@ -50,12 +51,12 @@ public void setTimeToLive(int timeToLive) {\n   }\n \n   @Override\n-  public void setCompressionType(CompressionType type) {\n+  public void setCompressionType(Compression.Algorithm type) {\n     throw new UnsupportedOperationException(\"HColumnDescriptor is read-only\");\n   }\n \n   @Override\n   public void setMapFileIndexInterval(int interval) {\n     throw new UnsupportedOperationException(\"HTableDescriptor is read-only\");\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 2
            },
            {
                "sha": "dd0fa8578b16621255c87cb15ebfde35250890b7",
                "filename": "src/java/org/apache/hadoop/hbase/io/Cell.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/Cell.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/Cell.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/Cell.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -55,6 +55,7 @@ public int compare(Long l1, Long l2) {\n \n   /** For Writable compatibility */\n   public Cell() {\n+    super();\n   }\n \n   /**\n@@ -93,7 +94,7 @@ public Cell(final ByteBuffer bb, long timestamp) {\n    * @param ts\n    *          array of timestamps\n    */\n-  public Cell(String[] vals, long[] ts) {\n+  public Cell(String [] vals, long[] ts) {\n     this(Bytes.toByteArrays(vals), ts);\n   }\n \n@@ -235,4 +236,4 @@ public void restSerialize(IRestSerializer serializer)\n       throws HBaseRestException {\n     serializer.serializeCell(this);\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 2
            },
            {
                "sha": "73ad5359b651b211bf15698cb463e5d3c7dcd6d3",
                "filename": "src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java",
                "status": "modified",
                "changes": 6,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/HBaseMapFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -72,10 +72,10 @@ public HBaseReader(FileSystem fs, String dirName, Configuration conf,\n     public HBaseReader(FileSystem fs, String dirName, Configuration conf,\n         boolean blockCacheEnabled, HRegionInfo hri)\n     throws IOException {\n-      super(fs, dirName, new HStoreKey.HStoreKeyWritableComparator(), \n+      super(fs, dirName, new HStoreKey.HStoreKeyComparator(), \n           conf, false); // defer opening streams\n       this.blockCacheEnabled = blockCacheEnabled;\n-      open(fs, dirName, new HStoreKey.HStoreKeyWritableComparator(), conf);\n+      open(fs, dirName, new HStoreKey.HStoreKeyComparator(), conf);\n       \n       // Force reading of the mapfile index by calling midKey. Reading the\n       // index will bring the index into memory over here on the client and\n@@ -121,7 +121,7 @@ protected FSDataInputStream openFile(FileSystem fs, Path file,\n     public HBaseWriter(Configuration conf, FileSystem fs, String dirName,\n         SequenceFile.CompressionType compression, final HRegionInfo hri)\n     throws IOException {\n-      super(conf, fs, dirName, new HStoreKey.HStoreKeyWritableComparator(),\n+      super(conf, fs, dirName, new HStoreKey.HStoreKeyComparator(),\n          VALUE_CLASS, compression);\n       // Default for mapfiles is 128.  Makes random reads faster if we\n       // have more keys indexed and we're not 'next'-ing around in the",
                "deletions": 3
            },
            {
                "sha": "0b29f245560b7ebd84b88dd0f87fcc41a51f4710",
                "filename": "src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/HalfHFileReader.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -73,6 +73,10 @@ public HalfHFileReader(final FileSystem fs, final Path p, final BlockCache c,\n     this.top = Reference.isTopFileRegion(r.getFileRegion());\n   }\n \n+  protected boolean isTop() {\n+    return this.top;\n+  }\n+\n   public HFileScanner getScanner() {\n     final HFileScanner s = super.getScanner();\n     return new HFileScanner() {",
                "deletions": 0
            },
            {
                "sha": "60ba2d4a8eaa1666221243ecf99d006778af7e62",
                "filename": "src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java",
                "status": "modified",
                "changes": 30,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -46,25 +46,9 @@\n  * @param <V> value Expects a Writable or byte [].\n  */\n public class HbaseMapWritable <K,V>\n-implements SortedMap<byte[],V>, Configurable, Writable,\n-  CodeToClassAndBack{\n-  \n+implements SortedMap<byte[],V>, Configurable, Writable, CodeToClassAndBack{\n   private AtomicReference<Configuration> conf = null;\n   protected SortedMap<byte [], V> instance = null;\n-  \n-  // Static maps of code to class and vice versa.  Includes types used in hbase\n-  // only. These maps are now initialized in a static loader interface instead\n-  // of in a static contructor for this class, this is done so that it is\n-  // possible to have a regular contructor here, so that different params can\n-  // be used.\n-  \n-  // Removed the old types like Text from the maps, if needed to add more types\n-  // this can be done in the StaticHBaseMapWritableLoader interface. Only\n-  // byte[] and Cell are supported now.\n-  //   static final Map<Byte, Class<?>> CODE_TO_CLASS =\n-  //     new HashMap<Byte, Class<?>>();\n-  //   static final Map<Class<?>, Byte> CLASS_TO_CODE =\n-  //     new HashMap<Class<?>, Byte>();\n \n   /**\n    * The default contructor where a TreeMap is used\n@@ -73,11 +57,11 @@ public HbaseMapWritable(){\n      this (new TreeMap<byte [], V>(Bytes.BYTES_COMPARATOR));\n    }\n \n-   /**\n-  * Contructor where another SortedMap can be used\n-  * \n-  * @param map the SortedMap to be used \n-  **/\n+  /**\n+   * Contructor where another SortedMap can be used\n+   * \n+   * @param map the SortedMap to be used \n+   */\n   public HbaseMapWritable(SortedMap<byte[], V> map){\n     conf = new AtomicReference<Configuration>();\n     instance = map;\n@@ -233,4 +217,4 @@ public void readFields(DataInput in) throws IOException {\n       this.instance.put(key, value);\n     }\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 23
            },
            {
                "sha": "13e84028640f47b40c3723052ebdf6fe208b109f",
                "filename": "src/java/org/apache/hadoop/hbase/io/RowResult.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/RowResult.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/RowResult.java",
                "status": "modified",
                "changes": 14,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/RowResult.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -78,21 +78,21 @@ public RowResult (final byte [] row,\n   // Map interface\n   // \n   \n-  public Cell put(byte [] key,\n-    Cell value) {\n+  public Cell put(@SuppressWarnings(\"unused\") byte [] key,\n+    @SuppressWarnings(\"unused\") Cell value) {\n     throw new UnsupportedOperationException(\"RowResult is read-only!\");\n   }\n \n   @SuppressWarnings(\"unchecked\")\n-  public void putAll(Map map) {\n+  public void putAll(@SuppressWarnings(\"unused\") Map map) {\n     throw new UnsupportedOperationException(\"RowResult is read-only!\");\n   }\n \n   public Cell get(Object key) {\n     return this.cells.get(key);\n   }\n \n-  public Cell remove(Object key) {\n+  public Cell remove(@SuppressWarnings(\"unused\") Object key) {\n     throw new UnsupportedOperationException(\"RowResult is read-only!\");\n   }\n \n@@ -104,7 +104,7 @@ public boolean containsKey(String key) {\n     return cells.containsKey(Bytes.toBytes(key));\n   }\n \n-  public boolean containsValue(Object value) {\n+  public boolean containsValue(@SuppressWarnings(\"unused\") Object value) {\n     throw new UnsupportedOperationException(\"Don't support containsValue!\");\n   }\n \n@@ -135,7 +135,7 @@ public void clear() {\n   /**\n    * This method used solely for the REST serialization\n    * \n-   * @return\n+   * @return Cells\n    */\n   @TOJSON\n   public RestCell[] getCells() {\n@@ -211,7 +211,7 @@ public Cell get(String key) {\n       this.cell = cell;\n     }\n     \n-    public Cell setValue(Cell c) {\n+    public Cell setValue(@SuppressWarnings(\"unused\") Cell c) {\n       throw new UnsupportedOperationException(\"RowResult is read-only!\");\n     }\n     ",
                "deletions": 7
            },
            {
                "sha": "9f9efb0dc5206f6b3400651390430f2b78ec494d",
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/Compression.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/Compression.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/Compression.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/Compression.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -297,7 +297,7 @@ public String getName() {\n     }\n   }\n \n-  static Algorithm getCompressionAlgorithmByName(String compressName) {\n+  public static Algorithm getCompressionAlgorithmByName(String compressName) {\n     Algorithm[] algos = Algorithm.class.getEnumConstants();\n \n     for (Algorithm a : algos) {",
                "deletions": 1
            },
            {
                "sha": "609f8cccaf75166d2605f7b0416d4b7dce0b78e5",
                "filename": "src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java",
                "status": "modified",
                "changes": 104,
                "additions": 79,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/hfile/HFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -141,8 +141,10 @@\n   /**\n    * Default compression: none.\n    */\n+  public final static Compression.Algorithm DEFAULT_COMPRESSION_ALGORITHM =\n+    Compression.Algorithm.NONE;\n   public final static String DEFAULT_COMPRESSION =\n-    Compression.Algorithm.NONE.getName();\n+    DEFAULT_COMPRESSION_ALGORITHM.getName();\n \n   /**\n    * HFile Writer.\n@@ -216,7 +218,7 @@\n      */\n     public Writer(FileSystem fs, Path path)\n     throws IOException {\n-      this(fs, path, DEFAULT_BLOCKSIZE, null, null);\n+      this(fs, path, DEFAULT_BLOCKSIZE, null, null, false);\n     }\n \n     /**\n@@ -226,12 +228,35 @@ public Writer(FileSystem fs, Path path)\n      * @param blocksize\n      * @param compress\n      * @param comparator\n+     * @param bloomfilter\n+     * @throws IOException \n+     * @throws IOException\n+     */\n+    public Writer(FileSystem fs, Path path, int blocksize,\n+      String compress, final RawComparator<byte []> comparator)\n+    throws IOException {\n+      this(fs, path, blocksize,\n+        compress == null? DEFAULT_COMPRESSION_ALGORITHM:\n+          Compression.getCompressionAlgorithmByName(compress),\n+        comparator, false);\n+    }\n+\n+    /**\n+     * Constructor that takes a Path.\n+     * @param fs\n+     * @param path\n+     * @param blocksize\n+     * @param compress\n+     * @param comparator\n+     * @param bloomfilter\n      * @throws IOException\n      */\n-    public Writer(FileSystem fs, Path path, int blocksize, String compress,\n-      final RawComparator<byte []> comparator)\n+    public Writer(FileSystem fs, Path path, int blocksize,\n+      Compression.Algorithm compress,\n+      final RawComparator<byte []> comparator,\n+      final boolean bloomfilter)\n     throws IOException {\n-      this(fs.create(path), blocksize, compress, comparator);\n+      this(fs.create(path), blocksize, compress, comparator, bloomfilter);\n       this.closeOutputStream = true;\n       this.name = path.toString();\n       this.path = path;\n@@ -243,19 +268,38 @@ public Writer(FileSystem fs, Path path, int blocksize, String compress,\n      * @param blocksize\n      * @param compress\n      * @param c\n+     * @param bloomfilter\n      * @throws IOException\n      */\n     public Writer(final FSDataOutputStream ostream, final int blocksize,\n-        final String compress, final RawComparator<byte []> c)\n+        final String  compress, final RawComparator<byte []> c)\n+    throws IOException {\n+      this(ostream, blocksize,\n+        compress == null? DEFAULT_COMPRESSION_ALGORITHM:\n+          Compression.getCompressionAlgorithmByName(compress), c, false);\n+    }\n+\n+    /**\n+     * Constructor that takes a stream.\n+     * @param ostream Stream to use.\n+     * @param blocksize\n+     * @param compress\n+     * @param c\n+     * @param bloomfilter\n+     * @throws IOException\n+     */\n+    public Writer(final FSDataOutputStream ostream, final int blocksize,\n+        final Compression.Algorithm  compress,\n+        final RawComparator<byte []> c,\n+        final boolean bloomfilter)\n     throws IOException {\n       this.outputStream = ostream;\n       this.closeOutputStream = false;\n       this.blocksize = blocksize;\n       this.comparator = c == null? Bytes.BYTES_RAWCOMPARATOR: c;\n       this.name = this.outputStream.toString();\n-      this.compressAlgo =\n-        Compression.getCompressionAlgorithmByName(compress == null?\n-          Compression.Algorithm.NONE.getName(): compress);\n+      this.compressAlgo = compress == null?\n+        DEFAULT_COMPRESSION_ALGORITHM: compress;\n     }\n \n     /*\n@@ -391,7 +435,7 @@ public String toString() {\n \n     /**\n      * Add key/value to file.\n-     * Keys must be added in an order that agrees with the RawComparator passed\n+     * Keys must be added in an order that agrees with the Comparator passed\n      * on construction.\n      * @param key Key to add.  Cannot be empty nor null.\n      * @param value Value to add.  Cannot be empty nor null.\n@@ -430,7 +474,7 @@ private void checkKey(final byte [] key) throws IOException {\n       if (this.lastKey != null) {\n         if (this.comparator.compare(this.lastKey, key) > 0) {\n           throw new IOException(\"Added a key not lexically larger than\" +\n-            \" previous: key=\" + Bytes.toString(key) + \", lastkey=\" +\n+            \" previous key=\" + Bytes.toString(key) + \", lastkey=\" +\n             Bytes.toString(lastKey));\n         }\n       }\n@@ -622,14 +666,22 @@ public Reader(final FSDataInputStream fsdis, final long size,\n \n     public String toString() {\n       return \"reader=\" + this.name +\n-        (!isFileInfoLoaded()? \"\":\n-          \", compression=\" + this.compressAlgo.getName() +\n-          \", firstKey=\" + Bytes.toString(getFirstKey()) +\n-          \", lastKey=\" + Bytes.toString(getLastKey()) +\n-          \", avgKeyLen=\" + this.avgKeyLen +\n-          \", avgValueLen=\" + this.avgValueLen +\n-          \", entries=\" + this.trailer.entryCount +\n-          \", length=\" + this.fileSize);\n+          (!isFileInfoLoaded()? \"\":\n+            \", compression=\" + this.compressAlgo.getName() +\n+            \", firstKey=\" + toStringFirstKey() +\n+            \", lastKey=\" + toStringLastKey()) +\n+            \", avgKeyLen=\" + this.avgKeyLen +\n+            \", avgValueLen=\" + this.avgValueLen +\n+            \", entries=\" + this.trailer.entryCount +\n+            \", length=\" + this.fileSize;\n+    }\n+\n+    protected String toStringFirstKey() {\n+      return Bytes.toString(getFirstKey());\n+    }\n+\n+    protected String toStringLastKey() {\n+      return Bytes.toString(getFirstKey());\n     }\n \n     public long length() {\n@@ -661,7 +713,7 @@ public long length() {\n \n       // Read in the metadata index.\n       if (trailer.metaIndexCount > 0) {\n-        this.metaIndex = BlockIndex.readIndex(Bytes.BYTES_RAWCOMPARATOR,\n+        this.metaIndex = BlockIndex.readIndex(this.comparator,\n           this.istream, this.trailer.metaIndexOffset, trailer.metaIndexCount);\n       }\n       this.fileInfoLoaded = true;\n@@ -679,7 +731,7 @@ boolean isFileInfoLoaded() {\n         return null;\n       }\n       try {\n-        return (RawComparator)Class.forName(clazzName).newInstance();\n+        return (RawComparator<byte[]>) Class.forName(clazzName).newInstance();\n       } catch (InstantiationException e) {\n         throw new IOException(e);\n       } catch (IllegalAccessException e) {\n@@ -1014,7 +1066,7 @@ private int blockSeek(byte[] key, boolean seekBefore) {\n           klen = block.getInt();\n           vlen = block.getInt();\n           int comp = this.reader.comparator.compare(key, 0, key.length,\n-              block.array(), block.arrayOffset() + block.position(), klen);\n+            block.array(), block.arrayOffset() + block.position(), klen);\n           if (comp == 0) {\n             if (seekBefore) {\n               block.position(block.position() - lastLen - 16);\n@@ -1035,8 +1087,10 @@ private int blockSeek(byte[] key, boolean seekBefore) {\n           }\n           block.position(block.position() + klen + vlen);\n           lastLen = klen + vlen ;\n-        } while( block.remaining() > 0 );\n+        } while(block.remaining() > 0);\n         // ok we are at the end, so go back a littleeeeee....\n+        // The 8 in the below is intentionally different to the 16s in the above\n+        // Do the math you you'll figure it.\n         block.position(block.position() - lastLen - 8);\n         currKeyLen = block.getInt();\n         currValueLen = block.getInt();\n@@ -1213,7 +1267,7 @@ public String toString() {\n   \n     /* Needed doing lookup on blocks.\n      */\n-    RawComparator<byte []> comparator;\n+    final RawComparator<byte []> comparator;\n   \n     /*\n      * Shutdown default constructor\n@@ -1227,7 +1281,7 @@ private BlockIndex() {\n      * Constructor\n      * @param trailer File tail structure with index stats.\n      */\n-    BlockIndex(final RawComparator<byte []> c) {\n+    BlockIndex(final RawComparator<byte []>c) {\n       this.comparator = c;\n       // Guess that cost of three arrays + this object is 4 * 8 bytes.\n       this.size += (4 * 8);",
                "deletions": 25
            },
            {
                "sha": "3f1b0bbcad790863bfa4d295dd466619a52276b3",
                "filename": "src/java/org/apache/hadoop/hbase/master/MetaRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/master/MetaRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/master/MetaRegion.java",
                "status": "modified",
                "changes": 3,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/master/MetaRegion.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -21,7 +21,6 @@\n \n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HServerAddress;\n-import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.util.Bytes;\n \n \n@@ -87,7 +86,7 @@ public int hashCode() {\n   public int compareTo(MetaRegion other) {\n     int result = Bytes.compareTo(this.regionName, other.getRegionName());\n     if(result == 0) {\n-      result = HStoreKey.compareTwoRowKeys(this.startKey, other.getStartKey());\n+      result = Bytes.compareTo(this.startKey, other.getStartKey());\n       if (result == 0) {\n         // Might be on different host?\n         result = this.server.compareTo(other.server);",
                "deletions": 2
            },
            {
                "sha": "6e593915ee7d668218b12b7322ec356dac53ae49",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -125,7 +125,8 @@ boolean isWildCardMatch() {\n   private boolean multipleMatchers;\n \n   /** Constructor for abstract base class */\n-  protected HAbstractScanner(long timestamp, byte [][] targetCols) throws IOException {\n+  protected HAbstractScanner(long timestamp, byte [][] targetCols)\n+  throws IOException {\n     this.timestamp = timestamp;\n     this.wildcardMatch = false;\n     this.multipleMatchers = false;",
                "deletions": 1
            },
            {
                "sha": "dcb73a720d8d4a5ae705b9712df01d636a0dafd0",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java",
                "status": "modified",
                "changes": 61,
                "additions": 31,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -1440,11 +1440,10 @@ public void deleteAll(final byte [] row, final long ts, final Integer lockid)\n     long now = System.currentTimeMillis();\n     try {\n       for (Store store : stores.values()) {\n-        List<HStoreKey> keys =\n-          store.getKeys(new HStoreKey(row, ts),\n-            ALL_VERSIONS, now, null);\n+        List<HStoreKey> keys = store.getKeys(new HStoreKey(row, ts),\n+          ALL_VERSIONS, now, null);\n         TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-          new HStoreKey.HStoreKeyWritableComparator());\n+          HStoreKey.getWritableComparator(store.getHRegionInfo()));\n         for (HStoreKey key: keys) {\n           edits.put(key, HLogEdit.DELETED_BYTES);\n         }\n@@ -1474,15 +1473,14 @@ public void deleteAllByRegex(final byte [] row, final String columnRegex,\n     long now = System.currentTimeMillis();\n     try {\n       for (Store store : stores.values()) {\n-        List<HStoreKey> keys =\n-          store.getKeys(new HStoreKey(row, timestamp),\n+        List<HStoreKey> keys = store.getKeys(new HStoreKey(row, timestamp),\n             ALL_VERSIONS, now, columnPattern);\n-          TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-            new HStoreKey.HStoreKeyWritableComparator());\n-          for (HStoreKey key: keys) {\n-            edits.put(key, HLogEdit.DELETED_BYTES);\n-          }\n-          update(edits);\n+        TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n+          HStoreKey.getWritableComparator(store.getHRegionInfo()));\n+        for (HStoreKey key: keys) {\n+          edits.put(key, HLogEdit.DELETED_BYTES);\n+        }\n+        update(edits);\n       }\n     } finally {\n       if(lockid == null) releaseRowLock(lid);\n@@ -1514,7 +1512,7 @@ public void deleteFamily(byte [] row, byte [] family, long timestamp,\n         ALL_VERSIONS, now, null);\n       // delete all the cells\n       TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+        HStoreKey.getWritableComparator(store.getHRegionInfo()));\n       for (HStoreKey key: keys) {\n         edits.put(key, HLogEdit.DELETED_BYTES);\n       }\n@@ -1553,7 +1551,7 @@ public void deleteFamilyByRegex(byte [] row, String familyRegex, long timestamp,\n         List<HStoreKey> keys = store.getKeys(new HStoreKey(row, timestamp),\n           ALL_VERSIONS, now, null);\n         TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-          new HStoreKey.HStoreKeyWritableComparator());\n+          HStoreKey.getWritableComparator(store.getHRegionInfo()));\n         for (HStoreKey key: keys) {\n           edits.put(key, HLogEdit.DELETED_BYTES);\n         }\n@@ -1583,8 +1581,11 @@ private void deleteMultiple(final byte [] row, final byte [] column,\n     HStoreKey origin = new HStoreKey(row, column, ts);\n     Set<HStoreKey> keys = getKeys(origin, versions);\n     if (keys.size() > 0) {\n+      // I think the below map doesn't have to be exactly storetd.  Its deletes\n+      // they don't have to go in in exact sorted order (we don't have to worry\n+      // about the meta or root sort comparator here).\n       TreeMap<HStoreKey, byte []> edits = new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+        new HStoreKey.HStoreKeyComparator());\n       for (HStoreKey key: keys) {\n         edits.put(key, HLogEdit.DELETED_BYTES);\n       }\n@@ -1650,8 +1651,10 @@ private void localput(final Integer lockid, final HStoreKey key,\n     checkReadOnly();\n     TreeMap<HStoreKey, byte []> targets = this.targetColumns.get(lockid);\n     if (targets == null) {\n-      targets = new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+      // I think the below map doesn't have to be exactly storetd.  Its deletes\n+      // they don't have to go in in exact sorted order (we don't have to worry\n+      // about the meta or root sort comparator here).\n+      targets = new TreeMap<HStoreKey, byte []>(new HStoreKey.HStoreKeyComparator());\n       this.targetColumns.put(lockid, targets);\n     }\n     targets.put(key, val);\n@@ -1948,9 +1951,7 @@ public Path getBaseDir() {\n       this.scanners = new InternalScanner[stores.length];\n       try {\n         for (int i = 0; i < stores.length; i++) {\n-          \n           // Only pass relevant columns to each store\n-          \n           List<byte[]> columns = new ArrayList<byte[]>();\n           for (int j = 0; j < cols.length; j++) {\n             if (Bytes.equals(HStoreKey.getFamily(cols[j]),\n@@ -2007,8 +2008,8 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n         for (int i = 0; i < this.keys.length; i++) {\n           if (scanners[i] != null &&\n              (chosenRow == null ||\n-               (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) < 0) ||\n-               ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0) &&\n+               (Bytes.compareTo(this.keys[i].getRow(), chosenRow) < 0) ||\n+               ((Bytes.compareTo(this.keys[i].getRow(), chosenRow) == 0) &&\n                       (keys[i].getTimestamp() > chosenTimestamp)))) {\n             chosenRow = keys[i].getRow();\n             chosenTimestamp = keys[i].getTimestamp();\n@@ -2025,7 +2026,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n \n           for (int i = 0; i < scanners.length; i++) {\n             if (scanners[i] != null &&\n-              HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0) {\n+              Bytes.compareTo(this.keys[i].getRow(), chosenRow) == 0) {\n               // NOTE: We used to do results.putAll(resultSets[i]);\n               // but this had the effect of overwriting newer\n               // values with older ones. So now we only insert\n@@ -2047,7 +2048,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n           // If the current scanner is non-null AND has a lower-or-equal\n           // row label, then its timestamp is bad. We need to advance it.\n           while ((scanners[i] != null) &&\n-              (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) <= 0)) {\n+              (Bytes.compareTo(this.keys[i].getRow(), chosenRow) <= 0)) {\n             resultSets[i].clear();\n             if (!scanners[i].next(keys[i], resultSets[i])) {\n               closeScanner(i);\n@@ -2228,7 +2229,7 @@ public static void addRegionToMETA(HRegion meta, HRegion r)\n       HStoreKey key = new HStoreKey(row, COL_REGIONINFO,\n         System.currentTimeMillis());\n       TreeMap<HStoreKey, byte[]> edits = new TreeMap<HStoreKey, byte[]>(\n-        new HStoreKey.HStoreKeyWritableComparator());\n+        HStoreKey.getWritableComparator(r.getRegionInfo()));\n       edits.put(key, Writables.getBytes(r.getRegionInfo()));\n       meta.update(edits);\n     } finally {\n@@ -2351,9 +2352,9 @@ public static Path getRegionDir(final Path rootdir, final HRegionInfo info) {\n    */\n   public static boolean rowIsInRange(HRegionInfo info, final byte [] row) {\n     return ((info.getStartKey().length == 0) ||\n-        (HStoreKey.compareTwoRowKeys(info.getStartKey(), row) <= 0)) &&\n+        (Bytes.compareTo(info.getStartKey(), row) <= 0)) &&\n         ((info.getEndKey().length == 0) ||\n-            (HStoreKey.compareTwoRowKeys(info.getEndKey(), row) > 0));\n+            (Bytes.compareTo(info.getEndKey(), row) > 0));\n   }\n \n   /**\n@@ -2396,7 +2397,7 @@ public static HRegion mergeAdjacent(final HRegion srcA, final HRegion srcB)\n       }\n       // A's start key is null but B's isn't. Assume A comes before B\n     } else if ((srcB.getStartKey() == null) ||\n-      (HStoreKey.compareTwoRowKeys(srcA.getStartKey(), srcB.getStartKey()) > 0)) {\n+      (Bytes.compareTo(srcA.getStartKey(), srcB.getStartKey()) > 0)) {\n       a = srcB;\n       b = srcA;\n     }\n@@ -2448,14 +2449,14 @@ public static HRegion merge(HRegion a, HRegion b) throws IOException {\n     final byte [] startKey = HStoreKey.equalsTwoRowKeys(a.getStartKey(),\n         EMPTY_BYTE_ARRAY) ||\n       HStoreKey.equalsTwoRowKeys(b.getStartKey(), EMPTY_BYTE_ARRAY)?\n-        EMPTY_BYTE_ARRAY: HStoreKey.compareTwoRowKeys(a.getStartKey(), \n+        EMPTY_BYTE_ARRAY: Bytes.compareTo(a.getStartKey(), \n           b.getStartKey()) <= 0?\n         a.getStartKey(): b.getStartKey();\n     final byte [] endKey = HStoreKey.equalsTwoRowKeys(a.getEndKey(),\n         EMPTY_BYTE_ARRAY) ||\n       HStoreKey.equalsTwoRowKeys(b.getEndKey(), EMPTY_BYTE_ARRAY)?\n         EMPTY_BYTE_ARRAY:\n-        HStoreKey.compareTwoRowKeys(a.getEndKey(), b.getEndKey()) <= 0? b.getEndKey(): a.getEndKey();\n+        Bytes.compareTo(a.getEndKey(), b.getEndKey()) <= 0? b.getEndKey(): a.getEndKey();\n \n     HRegionInfo newRegionInfo = new HRegionInfo(tabledesc, startKey, endKey);\n     LOG.info(\"Creating new region \" + newRegionInfo.toString());\n@@ -2584,4 +2585,4 @@ private static void listPaths(FileSystem fs, Path dir) throws IOException {\n       }\n     }\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 30
            },
            {
                "sha": "7bee630a0d0296419257ffd87414a20f43be877e",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java",
                "status": "modified",
                "changes": 4,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -1608,8 +1608,8 @@ public RowResult next(final long scannerId) throws IOException {\n     }\n   }\n \n-  public void batchUpdate(final byte [] regionName, BatchUpdate b,\n-      long lockId) throws IOException {\n+  public void batchUpdate(final byte [] regionName, BatchUpdate b, long lockId)\n+  throws IOException {\n     if (b.getRow() == null)\n       throw new IllegalArgumentException(\"update has null row\");\n     ",
                "deletions": 2
            },
            {
                "sha": "d33fb99ff1d915462e6994dafdbd0932545558e8",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java",
                "status": "modified",
                "changes": 47,
                "additions": 26,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Memcache.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -26,6 +26,7 @@\n import java.rmi.UnexpectedException;\n import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n@@ -68,35 +69,38 @@\n \n   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n \n+  private final Comparator<HStoreKey> comparator;\n+  private final HStoreKey.StoreKeyComparator rawcomparator;\n+\n   /**\n    * Default constructor. Used for tests.\n    */\n+  @SuppressWarnings(\"unchecked\")\n   public Memcache() {\n-    this.ttl = HConstants.FOREVER;\n-    this.memcache = createSynchronizedSortedMap();\n-    this.snapshot = createSynchronizedSortedMap();\n+    this(HConstants.FOREVER, new HStoreKey.HStoreKeyComparator(),\n+      new HStoreKey.StoreKeyComparator());\n   }\n \n   /**\n    * Constructor.\n    * @param ttl The TTL for cache entries, in milliseconds.\n    * @param regionInfo The HRI for this cache \n    */\n-  public Memcache(final long ttl) {\n+  public Memcache(final long ttl, final Comparator<HStoreKey> c,\n+      final HStoreKey.StoreKeyComparator rc) {\n     this.ttl = ttl;\n-    this.memcache = createSynchronizedSortedMap();\n-    this.snapshot = createSynchronizedSortedMap();\n+    this.comparator = c;\n+    this.rawcomparator = rc;\n+    this.memcache = createSynchronizedSortedMap(c);\n+    this.snapshot = createSynchronizedSortedMap(c);\n   }\n \n   /*\n    * Utility method using HSKWritableComparator\n    * @return synchronized sorted map of HStoreKey to byte arrays.\n    */\n-  @SuppressWarnings(\"unchecked\")\n-  private SortedMap<HStoreKey, byte[]> createSynchronizedSortedMap() {\n-    return Collections.synchronizedSortedMap(\n-      new TreeMap<HStoreKey, byte []>(\n-        new HStoreKey.HStoreKeyWritableComparator()));\n+  private SortedMap<HStoreKey, byte[]> createSynchronizedSortedMap(final Comparator<HStoreKey> c) {\n+    return Collections.synchronizedSortedMap(new TreeMap<HStoreKey, byte []>(c));\n   }\n \n   /**\n@@ -119,7 +123,7 @@ void snapshot() {\n         // mistake. St.Ack\n         if (this.memcache.size() != 0) {\n           this.snapshot = this.memcache;\n-          this.memcache = createSynchronizedSortedMap();\n+          this.memcache = createSynchronizedSortedMap(this.comparator);\n         }\n       }\n     } finally {\n@@ -156,7 +160,7 @@ void clearSnapshot(final SortedMap<HStoreKey, byte []> ss)\n       // OK. Passed in snapshot is same as current snapshot.  If not-empty,\n       // create a new snapshot and let the old one go.\n       if (ss.size() != 0) {\n-        this.snapshot = createSynchronizedSortedMap();\n+        this.snapshot = createSynchronizedSortedMap(this.comparator);\n       }\n     } finally {\n       this.lock.writeLock().unlock();\n@@ -261,7 +265,7 @@ long heapSize(final HStoreKey key, final byte [] value,\n     if (b == null) {\n       return a;\n     }\n-    return HStoreKey.compareTwoRowKeys(a, b) <= 0? a: b;\n+    return Bytes.compareTo(a, b) <= 0? a: b;\n   }\n \n   /**\n@@ -296,7 +300,7 @@ long heapSize(final HStoreKey key, final byte [] value,\n       // Iterate until we fall into the next row; i.e. move off current row\n       for (Map.Entry<HStoreKey, byte []> es: tailMap.entrySet()) {\n         HStoreKey itKey = es.getKey();\n-        if (HStoreKey.compareTwoRowKeys(itKey.getRow(), row) <= 0)\n+        if (Bytes.compareTo(itKey.getRow(), row) <= 0)\n           continue;\n         // Note: Not suppressing deletes or expired cells.\n         result = itKey.getRow();\n@@ -344,7 +348,8 @@ private void internalGetFull(SortedMap<HStoreKey, byte[]> map, HStoreKey key,\n       HStoreKey itKey = es.getKey();\n       byte [] itCol = itKey.getColumn();\n       Cell cell = results.get(itCol);\n-      if ((cell == null || cell.getNumValues() < numVersions) && key.matchesWithoutColumn(itKey)) {\n+      if ((cell == null || cell.getNumValues() < numVersions) &&\n+          key.matchesWithoutColumn(itKey)) {\n         if (columns == null || columns.contains(itKey.getColumn())) {\n           byte [] val = tailMap.get(itKey);\n           if (HLogEdit.isDeleted(val)) {\n@@ -367,7 +372,7 @@ private void internalGetFull(SortedMap<HStoreKey, byte[]> map, HStoreKey key,\n             }\n           }\n         }\n-      } else if (HStoreKey.compareTwoRowKeys(key.getRow(), itKey.getRow()) < 0) {\n+      } else if (this.rawcomparator.compareRows(key.getRow(), itKey.getRow()) < 0) {\n         break;\n       }\n     }\n@@ -428,7 +433,7 @@ private void getRowKeyAtOrBefore(final SortedMap<HStoreKey, byte []> map,\n     // the search key, or a range of values between the first candidate key\n     // and the ultimate search key (or the end of the cache)\n     if (!tailMap.isEmpty() &&\n-        HStoreKey.compareTwoRowKeys(tailMap.firstKey().getRow(),\n+        Bytes.compareTo(tailMap.firstKey().getRow(),\n           search_key.getRow()) <= 0) {\n       Iterator<HStoreKey> key_iterator = tailMap.keySet().iterator();\n \n@@ -437,9 +442,9 @@ private void getRowKeyAtOrBefore(final SortedMap<HStoreKey, byte []> map,\n       HStoreKey deletedOrExpiredRow = null;\n       for (HStoreKey found_key = null; key_iterator.hasNext() &&\n           (found_key == null ||\n-            HStoreKey.compareTwoRowKeys(found_key.getRow(), row) <= 0);) {\n+            Bytes.compareTo(found_key.getRow(), row) <= 0);) {\n         found_key = key_iterator.next();\n-        if (HStoreKey.compareTwoRowKeys(found_key.getRow(), row) <= 0) {\n+        if (Bytes.compareTo(found_key.getRow(), row) <= 0) {\n           if (HLogEdit.isDeleted(tailMap.get(found_key))) {\n             Store.handleDeleted(found_key, candidateKeys, deletes);\n             if (deletedOrExpiredRow == null) {\n@@ -900,4 +905,4 @@ public static void main(String [] args) throws InterruptedException {\n     }\n     LOG.info(\"Exiting.\");\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 21
            },
            {
                "sha": "c1c1b0abeef3314f1b52fc3a164917b806eeef33",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/Store.java",
                "status": "modified",
                "changes": 80,
                "additions": 59,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/Store.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n@@ -54,6 +55,7 @@\n import org.apache.hadoop.hbase.filter.RowFilterInterface;\n import org.apache.hadoop.hbase.io.Cell;\n import org.apache.hadoop.hbase.io.SequenceFile;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.io.hfile.HFile;\n import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n import org.apache.hadoop.hbase.util.Bytes;\n@@ -126,6 +128,14 @@\n   private final Path compactionDir;\n   private final Integer compactLock = new Integer(0);\n   private final int compactionThreshold;\n+  private final int blocksize;\n+  private final boolean bloomfilter;\n+  private final Compression.Algorithm compression;\n+  \n+  // Comparing HStoreKeys in byte arrays.\n+  final HStoreKey.StoreKeyComparator rawcomparator;\n+  // Comparing HStoreKey objects.\n+  final Comparator<HStoreKey> comparator;\n \n   /**\n    * Constructor\n@@ -141,6 +151,7 @@\n    * failed.  Can be null.\n    * @throws IOException\n    */\n+  @SuppressWarnings(\"unchecked\")\n   protected Store(Path basedir, HRegionInfo info, HColumnDescriptor family,\n     FileSystem fs, Path reconstructionLog, HBaseConfiguration conf,\n     final Progressable reporter)\n@@ -151,12 +162,23 @@ protected Store(Path basedir, HRegionInfo info, HColumnDescriptor family,\n     this.family = family;\n     this.fs = fs;\n     this.conf = conf;\n+    this.bloomfilter = family.isBloomfilter();\n+    this.blocksize = family.getBlocksize();\n+    this.compression = family.getCompression();\n+    this.rawcomparator = info.isRootRegion()?\n+      new HStoreKey.RootStoreKeyComparator(): info.isMetaRegion()?\n+        new HStoreKey.MetaStoreKeyComparator():\n+          new HStoreKey.StoreKeyComparator();\n+    this.comparator = info.isRootRegion()?\n+      new HStoreKey.HStoreKeyRootComparator(): info.isMetaRegion()?\n+        new HStoreKey.HStoreKeyMetaComparator():\n+          new HStoreKey.HStoreKeyComparator();\n     // getTimeToLive returns ttl in seconds.  Convert to milliseconds.\n     this.ttl = family.getTimeToLive();\n     if (ttl != HConstants.FOREVER) {\n       this.ttl *= 1000;\n     }\n-    this.memcache = new Memcache(this.ttl);\n+    this.memcache = new Memcache(this.ttl, this.comparator, this.rawcomparator);\n     this.compactionDir = HRegion.getCompactionDir(basedir);\n     this.storeName = Bytes.toBytes(this.regioninfo.getEncodedName() + \"/\" +\n       Bytes.toString(this.family.getName()));\n@@ -254,7 +276,6 @@ private void runReconstructionLog(final Path reconstructionLog,\n    * lower than maxSeqID.  (Because we know such log messages are already \n    * reflected in the MapFiles.)\n    */\n-  @SuppressWarnings(\"unchecked\")\n   private void doReconstructionLog(final Path reconstructionLog,\n     final long maxSeqID, final Progressable reporter)\n   throws UnsupportedEncodingException, IOException {\n@@ -273,7 +294,7 @@ private void doReconstructionLog(final Path reconstructionLog,\n     // general memory usage accounting.\n     long maxSeqIdInLog = -1;\n     NavigableMap<HStoreKey, byte []> reconstructedCache =\n-      new TreeMap<HStoreKey, byte []>(new HStoreKey.HStoreKeyWritableComparator());\n+      new TreeMap<HStoreKey, byte []>(this.comparator);\n     SequenceFile.Reader logReader = new SequenceFile.Reader(this.fs,\n       reconstructionLog, this.conf);\n     try {\n@@ -463,7 +484,7 @@ private StoreFile internalFlushCache(final SortedMap<HStoreKey, byte []> cache,\n     // if we fail.\n     synchronized (flushLock) {\n       // A. Write the map out to the disk\n-      writer = StoreFile.getWriter(this.fs, this.homedir);\n+      writer = getWriter();\n       int entries = 0;\n       try {\n         for (Map.Entry<HStoreKey, byte []> es: cache.entrySet()) {\n@@ -493,7 +514,16 @@ private StoreFile internalFlushCache(final SortedMap<HStoreKey, byte []> cache,\n     }\n     return sf;\n   }\n-  \n+\n+  /**\n+   * @return Writer for this store.\n+   * @throws IOException\n+   */\n+  HFile.Writer getWriter() throws IOException {\n+    return StoreFile.getWriter(this.fs, this.homedir, this.blocksize,\n+        this.compression, this.rawcomparator, this.bloomfilter);\n+  }\n+\n   /*\n    * Change storefiles adding into place the Reader produced by this new flush.\n    * @param logCacheFlushId\n@@ -650,7 +680,7 @@ StoreSize compact(final boolean mc) throws IOException {\n       }\n  \n       // Step through them, writing to the brand-new file\n-      HFile.Writer writer = StoreFile.getWriter(this.fs, this.homedir);\n+      HFile.Writer writer = getWriter();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Started compaction of \" + filesToCompact.size() + \" file(s)\" +\n           (references? \", hasReferences=true,\": \" \") + \" into \" +\n@@ -992,6 +1022,7 @@ void getFull(HStoreKey key, final Set<byte []> columns,\n       final int numVersions, Map<byte [], Cell> results)\n   throws IOException {\n     int versions = versionsToReturn(numVersions);\n+    // This is map of columns to timestamp\n     Map<byte [], Long> deletes =\n       new TreeMap<byte [], Long>(Bytes.BYTES_COMPARATOR);\n     // if the key is null, we're not even looking for anything. return.\n@@ -1061,7 +1092,9 @@ private void getFullFromStoreFile(StoreFile f, HStoreKey key,\n             }\n           }\n         }\n-      } else if (HStoreKey.compareTwoRowKeys(key.getRow(), readkey.getRow()) < 0) {\n+      } else if (this.rawcomparator.compareRows(key.getRow(), 0,\n+            key.getRow().length,\n+          readkey.getRow(), 0, readkey.getRow().length) < 0) {\n         // if we've crossed into the next row, then we can just stop\n         // iterating\n         break;\n@@ -1280,15 +1313,14 @@ private boolean hasEnoughVersions(final int versions, final List<Cell> c) {\n    * @return Found row\n    * @throws IOException\n    */\n-  @SuppressWarnings(\"unchecked\")\n   byte [] getRowKeyAtOrBefore(final byte [] row)\n   throws IOException{\n     // Map of HStoreKeys that are candidates for holding the row key that\n     // most closely matches what we're looking for. We'll have to update it as\n     // deletes are found all over the place as we go along before finally\n     // reading the best key out of it at the end.\n-    NavigableMap<HStoreKey, Long> candidateKeys = new TreeMap<HStoreKey, Long>(\n-      new HStoreKey.HStoreKeyWritableComparator());\n+    NavigableMap<HStoreKey, Long> candidateKeys =\n+      new TreeMap<HStoreKey, Long>(this.comparator);\n     \n     // Keep a list of deleted cell keys.  We need this because as we go through\n     // the store files, the cell with the delete marker may be in one file and\n@@ -1365,11 +1397,13 @@ private void rowAtOrBeforeCandidate(final HStoreKey startKey,\n     // up to the row before and return that.\n     HStoreKey finalKey = HStoreKey.create(f.getReader().getLastKey());\n     HStoreKey searchKey = null;\n-    if (HStoreKey.compareTwoRowKeys(finalKey.getRow(), row) < 0) {\n+    if (this.rawcomparator.compareRows(finalKey.getRow(), 0,\n+          finalKey.getRow().length,\n+        row, 0, row.length) < 0) {\n       searchKey = finalKey;\n     } else {\n       searchKey = new HStoreKey(row);\n-      if (searchKey.compareTo(startKey) < 0) {\n+      if (this.comparator.compare(searchKey, startKey) < 0) {\n         searchKey = startKey;\n       }\n     }\n@@ -1435,8 +1469,9 @@ private void rowAtOrBeforeCandidate(final StoreFile f,\n           if (deletedOrExpiredRow == null) {\n             deletedOrExpiredRow = copy;\n           }\n-        } else if (HStoreKey.compareTwoRowKeys(readkey.getRow(),\n-            searchKey.getRow()) > 0) {\n+        } else if (this.rawcomparator.compareRows(readkey.getRow(), 0,\n+              readkey.getRow().length,\n+            searchKey.getRow(), 0, searchKey.getRow().length) > 0) {\n           // if the row key we just read is beyond the key we're searching for,\n           // then we're done.\n           break;\n@@ -1457,7 +1492,7 @@ private void rowAtOrBeforeCandidate(final StoreFile f,\n           }\n         }\n       } while(scanner.next() && (knownNoGoodKey == null ||\n-          readkey.compareTo(knownNoGoodKey) < 0));\n+          this.comparator.compare(readkey, knownNoGoodKey) < 0));\n \n       // If we get here and have no candidates but we did find a deleted or\n       // expired candidate, we need to look at the key before that\n@@ -1502,11 +1537,14 @@ private void rowAtOrBeforeWithCandidates(final HStoreKey startKey,\n     // of the row in case there are deletes for this candidate in this mapfile\n     // BUT do not backup before the first key in the store file.\n     // TODO: FIX THIS PROFLIGATE OBJECT MAKING!!!\n-    byte [] searchKey =\n-      new HStoreKey(candidateKeys.firstKey().getRow()).getBytes();\n-    if (f.getReader().getComparator().compare(searchKey, 0, searchKey.length,\n-        startKey.getRow(), 0, startKey.getRow().length) < 0) {\n+    HStoreKey firstCandidateKey = candidateKeys.firstKey();\n+    byte [] searchKey = null;\n+    HStoreKey.StoreKeyComparator c =\n+      (HStoreKey.StoreKeyComparator)f.getReader().getComparator();\n+    if (c.compareRows(firstCandidateKey.getRow(), startKey.getRow()) < 0) {\n       searchKey = startKey.getBytes();\n+    } else {\n+      searchKey = new HStoreKey(firstCandidateKey.getRow()).getBytes();\n     }\n \n     // Seek to the exact row, or the one that would be immediately before it\n@@ -1523,7 +1561,7 @@ private void rowAtOrBeforeWithCandidates(final HStoreKey startKey,\n       // as a candidate key\n       if (HStoreKey.equalsTwoRowKeys(k.getRow(), row)) {\n         handleKey(k, v, now, deletes, candidateKeys);\n-      } else if (HStoreKey.compareTwoRowKeys(k.getRow(), row) > 0 ) {\n+      } else if (this.rawcomparator.compareRows(k.getRow(), row) > 0 ) {\n         // if the row key we just read is beyond the key we're searching for,\n         // then we're done.\n         break;\n@@ -1804,4 +1842,4 @@ static boolean getClosest(final HFileScanner s, final byte [] b)\n     }\n     return true;\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 21
            },
            {
                "sha": "39b763b70a7dc2629b11d82262ab86d25b8631b0",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java",
                "status": "modified",
                "changes": 79,
                "additions": 73,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -35,9 +35,10 @@\n import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.io.HalfHFileReader;\n import org.apache.hadoop.hbase.io.Reference;\n+import org.apache.hadoop.hbase.io.hfile.BlockCache;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.io.hfile.HFile;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.apache.hadoop.io.RawComparator;\n \n /**\n  * A Store data file.  Stores usually have one or more of these files.  They\n@@ -210,7 +211,7 @@ public long getMaxSequenceId() {\n       this.reader = new HalfHFileReader(this.fs, this.referencePath, null,\n         this.reference);\n     } else {\n-      this.reader = new HFile.Reader(this.fs, this.path, null);\n+      this.reader = new StoreFileReader(this.fs, this.path, null);\n     }\n     // Load up indices and fileinfo.\n     Map<byte [], byte []> map = this.reader.loadFileInfo();\n@@ -241,6 +242,71 @@ public long getMaxSequenceId() {\n     }\n     return this.reader;\n   }\n+  \n+  /**\n+   * Override to add some customization on HFile.Reader\n+   */\n+  static class StoreFileReader extends HFile.Reader {\n+    public StoreFileReader(FileSystem fs, Path path, BlockCache cache)\n+        throws IOException {\n+      super(fs, path, cache);\n+    }\n+\n+    protected String toStringFirstKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getFirstKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString first key\", e);\n+      }\n+      return result;\n+    }\n+\n+    protected String toStringLastKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getLastKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString last key\", e);\n+      }\n+      return result;\n+    }\n+  }\n+\n+  /**\n+   * Override to add some customization on HalfHFileReader\n+   */\n+  static class HalfStoreFileReader extends HalfHFileReader {\n+    public HalfStoreFileReader(FileSystem fs, Path p, BlockCache c, Reference r)\n+        throws IOException {\n+      super(fs, p, c, r);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return super.toString() + (isTop()? \", half=top\": \", half=bottom\");\n+    }\n+\n+    protected String toStringFirstKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getFirstKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString first key\", e);\n+      }\n+      return result;\n+    }\n+\n+    protected String toStringLastKey() {\n+      String result = \"\";\n+      try {\n+        result = HStoreKey.create(getLastKey()).toString();\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed toString last key\", e);\n+      }\n+      return result;\n+    }\n+  }\n \n   /**\n    * @return Current reader.  Must call open first.\n@@ -309,7 +375,7 @@ public static Path rename(final FileSystem fs, final Path src,\n    */\n   public static HFile.Writer getWriter(final FileSystem fs, final Path dir)\n   throws IOException {\n-    return getWriter(fs, dir, DEFAULT_BLOCKSIZE_SMALL, null, null);\n+    return getWriter(fs, dir, DEFAULT_BLOCKSIZE_SMALL, null, null, false);\n   }\n \n   /**\n@@ -326,15 +392,16 @@ public static Path rename(final FileSystem fs, final Path src,\n    * @throws IOException\n    */\n   public static HFile.Writer getWriter(final FileSystem fs, final Path dir,\n-    final int blocksize, final String algorithm, final RawComparator<byte []> c)\n+    final int blocksize, final Compression.Algorithm algorithm,\n+    final HStoreKey.StoreKeyComparator c, final boolean bloomfilter)\n   throws IOException {\n     if (!fs.exists(dir)) {\n       fs.mkdirs(dir);\n     }\n     Path path = getUniqueFile(fs, dir);\n     return new HFile.Writer(fs, path, blocksize,\n-      algorithm == null? HFile.DEFAULT_COMPRESSION: algorithm,\n-      c == null? HStoreKey.BYTECOMPARATOR: c);\n+      algorithm == null? HFile.DEFAULT_COMPRESSION_ALGORITHM: algorithm,\n+      c == null? new HStoreKey.StoreKeyComparator(): c, bloomfilter);\n   }\n \n   /**",
                "deletions": 6
            },
            {
                "sha": "7bbbf048941c69570f2432911a63f01882338f15",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java",
                "status": "modified",
                "changes": 30,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -141,19 +141,17 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n     try {\n       // Find the next viable row label (and timestamp).\n       ViableRow viableRow = getNextViableRow();\n-      \n+\n       // Grab all the values that match this row/timestamp\n       boolean insertedItem = false;\n       if (viableRow.getRow() != null) {\n         key.setRow(viableRow.getRow());\n         key.setVersion(viableRow.getTimestamp());\n-\n         for (int i = 0; i < keys.length; i++) {\n           // Fetch the data\n           while ((keys[i] != null) &&\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(),\n-              viableRow.getRow()) == 0)) {\n-\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+                viableRow.getRow()) == 0)) {\n             // If we are doing a wild card match or there are multiple matchers\n             // per column, we need to scan all the older versions of this row\n             // to pick up the rest of the family members\n@@ -162,8 +160,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n                 && (keys[i].getTimestamp() != viableRow.getTimestamp())) {\n               break;\n             }\n-\n-            if(columnMatch(i)) {              \n+            if(columnMatch(i)) {\n               // We only want the first result for any specific family member\n               if(!results.containsKey(keys[i].getColumn())) {\n                 results.put(keys[i].getColumn(), \n@@ -179,10 +176,10 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n           // Advance the current scanner beyond the chosen row, to\n           // a valid timestamp, so we're ready next time.\n           while ((keys[i] != null) &&\n-              ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(),\n-                viableRow.getRow()) <= 0)\n-                  || (keys[i].getTimestamp() > this.timestamp)\n-                  || (! columnMatch(i)))) {\n+            ((this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+                viableRow.getRow()) <= 0) ||\n+              (keys[i].getTimestamp() > this.timestamp) ||\n+              (! columnMatch(i)))) {\n             getNext(i);\n           }\n         }\n@@ -192,7 +189,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n       this.lock.readLock().unlock();\n     }\n   }\n-  \n+\n   // Data stucture to hold next, viable row (and timestamp).\n   class ViableRow {\n     private final byte [] row;\n@@ -239,8 +236,10 @@ private ViableRow getNextViableRow() throws IOException {\n           // column matches and the timestamp of the row is less than or equal\n           // to this.timestamp, so we do not need to test that here\n           && ((viableRow == null) ||\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), viableRow) < 0) ||\n-            ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), viableRow) == 0) &&\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              viableRow) < 0) ||\n+            ((this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+                viableRow) == 0) &&\n               (keys[i].getTimestamp() > viableTimestamp)))) {\n         if (ttl == HConstants.FOREVER || now < keys[i].getTimestamp() + ttl) {\n           viableRow = keys[i].getRow();\n@@ -270,8 +269,7 @@ private boolean findFirstRow(int i, final byte [] firstRow) throws IOException {\n         return true;\n       }\n     } else {\n-      if (!Store.getClosest(this.scanners[i],\n-          new HStoreKey(firstRow).getBytes())) {\n+      if (!Store.getClosest(this.scanners[i], HStoreKey.getBytes(firstRow))) {\n         closeSubScanner(i);\n         return true;\n       }",
                "deletions": 16
            },
            {
                "sha": "084d0c5d0caf326c7973c865fec2a0662690f09b",
                "filename": "src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java",
                "status": "modified",
                "changes": 18,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -153,8 +153,10 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n       for (int i = 0; i < this.keys.length; i++) {\n         if (scanners[i] != null &&\n             (chosenRow == null ||\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) < 0) ||\n-            ((HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0) &&\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) < 0) ||\n+            ((this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) == 0) &&\n             (keys[i].getTimestamp() > chosenTimestamp)))) {\n           chosenRow = keys[i].getRow();\n           chosenTimestamp = keys[i].getTimestamp();\n@@ -181,10 +183,9 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n         // problem, could redo as bloom filter.\n         Set<HStoreKey> deletes = new HashSet<HStoreKey>();\n         for (int i = 0; i < scanners.length && !filtered; i++) {\n-          while ((scanners[i] != null\n-              && !filtered\n-              && moreToFollow)\n-              && (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) == 0)) {\n+          while ((scanners[i] != null && !filtered && moreToFollow) &&\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) == 0)) {\n             // If we are doing a wild card match or there are multiple\n             // matchers per column, we need to scan all the older versions of \n             // this row to pick up the rest of the family members\n@@ -206,7 +207,7 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n               if (HLogEdit.isDeleted(e.getValue().getValue())) {\n                 // Only first key encountered is added; deletes is a Set.\n                 deletes.add(new HStoreKey(hsk));\n-              } else if (!deletes.contains(hsk) &&\n+              } else if ((deletes.size() == 0 || !deletes.contains(hsk)) &&\n                   !filtered &&\n                   moreToFollow &&\n                   !results.containsKey(e.getKey())) {\n@@ -233,7 +234,8 @@ public boolean next(HStoreKey key, SortedMap<byte [], Cell> results)\n         // If the current scanner is non-null AND has a lower-or-equal\n         // row label, then its timestamp is bad. We need to advance it.\n         while ((scanners[i] != null) &&\n-            (HStoreKey.compareTwoRowKeys(this.keys[i].getRow(), chosenRow) <= 0)) {\n+            (this.store.rawcomparator.compareRows(this.keys[i].getRow(),\n+              chosenRow) <= 0)) {\n           resultSets[i].clear();\n           if (!scanners[i].next(keys[i], resultSets[i])) {\n             closeScanner(i);",
                "deletions": 8
            },
            {
                "sha": "caa6e75dc71b51cbe8e6edfaa1c2a1844cc31f37",
                "filename": "src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java",
                "status": "modified",
                "changes": 8,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/rest/parser/JsonRestParser.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -23,6 +23,7 @@\n \n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.rest.RESTConstants;\n import org.apache.hadoop.hbase.rest.descriptors.RowUpdateDescriptor;\n import org.apache.hadoop.hbase.rest.descriptors.ScannerDescriptor;\n@@ -76,7 +77,7 @@ private HColumnDescriptor getColumnDescriptor(JSONObject jsonObject)\n     byte[] name = Bytes.toBytes(strTemp);\n \n     int maxVersions;\n-    HColumnDescriptor.CompressionType cType;\n+    String cType;\n     boolean inMemory;\n     boolean blockCacheEnabled;\n     int maxValueLength;\n@@ -96,10 +97,9 @@ private HColumnDescriptor getColumnDescriptor(JSONObject jsonObject)\n     }\n \n     try {\n-      cType = HColumnDescriptor.CompressionType.valueOf(jsonObject\n-          .getString(\"compression_type\"));\n+      cType = jsonObject.getString(\"compression_type\").toUpperCase();\n     } catch (JSONException e) {\n-      cType = HColumnDescriptor.CompressionType.NONE;\n+      cType = HColumnDescriptor.DEFAULT_COMPRESSION;\n     }\n \n     try {",
                "deletions": 4
            },
            {
                "sha": "c9416045080d8db4943d7b4c0293b9bd5a6a0eae",
                "filename": "src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java",
                "status": "modified",
                "changes": 11,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/rest/parser/XMLRestParser.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.rest.RESTConstants;\n import org.apache.hadoop.hbase.rest.descriptors.RowUpdateDescriptor;\n import org.apache.hadoop.hbase.rest.descriptors.ScannerDescriptor;\n@@ -95,7 +96,7 @@ private HColumnDescriptor getColumnDescriptor(Element columnfamily,\n     String colname = makeColumnName(name_node.getFirstChild().getNodeValue());\n \n     int max_versions = HColumnDescriptor.DEFAULT_VERSIONS;\n-    CompressionType compression = HColumnDescriptor.DEFAULT_COMPRESSION;\n+    String compression = HColumnDescriptor.DEFAULT_COMPRESSION;\n     boolean in_memory = HColumnDescriptor.DEFAULT_IN_MEMORY;\n     boolean block_cache = HColumnDescriptor.DEFAULT_BLOCKCACHE;\n     int max_cell_size = HColumnDescriptor.DEFAULT_LENGTH;\n@@ -126,8 +127,8 @@ private HColumnDescriptor getColumnDescriptor(Element columnfamily,\n     NodeList compression_list = columnfamily\n         .getElementsByTagName(\"compression\");\n     if (compression_list.getLength() > 0) {\n-      compression = CompressionType.valueOf(compression_list.item(0)\n-          .getFirstChild().getNodeValue());\n+      compression = compression_list.item(0)\n+          .getFirstChild().getNodeValue().toUpperCase();\n     }\n \n     NodeList in_memory_list = columnfamily.getElementsByTagName(\"in-memory\");\n@@ -163,8 +164,8 @@ private HColumnDescriptor getColumnDescriptor(Element columnfamily,\n     }\n \n     HColumnDescriptor hcd = new HColumnDescriptor(Bytes.toBytes(colname),\n-        max_versions, compression, in_memory, block_cache, max_cell_size, ttl,\n-        bloomfilter);\n+        max_versions, compression, in_memory, block_cache,\n+        max_cell_size, ttl, bloomfilter);\n \n     NodeList metadataList = columnfamily.getElementsByTagName(\"metadata\");\n     for (int i = 0; i < metadataList.getLength(); i++) {",
                "deletions": 5
            },
            {
                "sha": "80ce5da3889c2fb327fcd99fb3a2e6337bc3fde8",
                "filename": "src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -22,9 +22,9 @@\n import java.util.TreeMap;\n \n import org.apache.hadoop.hbase.HColumnDescriptor;\n-import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n import org.apache.hadoop.hbase.io.Cell;\n import org.apache.hadoop.hbase.io.RowResult;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor;\n import org.apache.hadoop.hbase.thrift.generated.IllegalArgument;\n import org.apache.hadoop.hbase.thrift.generated.TCell;\n@@ -44,7 +44,8 @@\n    */\n   static public HColumnDescriptor colDescFromThrift(ColumnDescriptor in)\n       throws IllegalArgument {\n-    CompressionType comp = CompressionType.valueOf(in.compression);\n+    Compression.Algorithm comp =\n+      Compression.getCompressionAlgorithmByName(in.compression.toLowerCase());\n     boolean bloom = false;\n     if (in.bloomFilterType.compareTo(\"NONE\") != 0) {\n       bloom = true;\n@@ -54,7 +55,7 @@ static public HColumnDescriptor colDescFromThrift(ColumnDescriptor in)\n       throw new IllegalArgument(\"column name is empty\");\n     }\n     HColumnDescriptor col = new HColumnDescriptor(in.name,\n-        in.maxVersions, comp, in.inMemory, in.blockCacheEnabled,\n+        in.maxVersions, comp.getName(), in.inMemory, in.blockCacheEnabled,\n         in.maxValueLength, in.timeToLive, bloom);\n     return col;\n   }",
                "deletions": 3
            },
            {
                "sha": "d2d53b3a1358cfc3f76a6d987d8ec0563a153494",
                "filename": "src/java/org/apache/hadoop/hbase/util/Bytes.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/Bytes.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/Bytes.java",
                "status": "modified",
                "changes": 128,
                "additions": 114,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Bytes.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -116,6 +116,15 @@ public static void writeByteArray(final DataOutput out, final byte [] b)\n     out.write(b, 0, b.length);\n   }\n \n+  public static int writeByteArray(final byte [] tgt, final int tgtOffset,\n+      final byte [] src, final int srcOffset, final int srcLength) {\n+    byte [] vint = vintToBytes(srcLength);\n+    System.arraycopy(vint, 0, tgt, tgtOffset, vint.length);\n+    int offset = tgtOffset + vint.length;\n+    System.arraycopy(src, srcOffset, tgt, offset, srcLength);\n+    return offset + srcLength;\n+  }\n+\n   /**\n    * Reads a zero-compressed encoded long from input stream and returns it.\n    * @param buffer Binary array\n@@ -218,16 +227,99 @@ public static boolean toBoolean(final byte [] b) {\n     return bb.array();\n   }\n \n+  /**\n+   * @param vint Integer to make a vint of.\n+   * @return Vint as bytes array.\n+   */\n+  public static byte [] vintToBytes(final long vint) {\n+    long i = vint;\n+    int size = WritableUtils.getVIntSize(i);\n+    byte [] result = new byte[size];\n+    int offset = 0;\n+    if (i >= -112 && i <= 127) {\n+      result[offset] = ((byte)i);\n+      return result;\n+    }\n+    offset++;\n+\n+    int len = -112;\n+    if (i < 0) {\n+      i ^= -1L; // take one's complement'\n+      len = -120;\n+    }\n+\n+    long tmp = i;\n+    while (tmp != 0) {\n+      tmp = tmp >> 8;\n+    len--;\n+    }\n+\n+    result[offset++] = (byte)len;\n+\n+    len = (len < -120) ? -(len + 120) : -(len + 112);\n+\n+    for (int idx = len; idx != 0; idx--) {\n+      int shiftbits = (idx - 1) * 8;\n+      long mask = 0xFFL << shiftbits;\n+      result[offset++] = (byte)((i & mask) >> shiftbits);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * @param buffer\n+   * @return vint bytes as an integer.\n+   */\n+  public static long bytesToVint(final byte [] buffer) {\n+    int offset = 0;\n+    byte firstByte = buffer[offset++];\n+    int len = WritableUtils.decodeVIntSize(firstByte);\n+    if (len == 1) {\n+      return firstByte;\n+    }\n+    long i = 0;\n+    for (int idx = 0; idx < len-1; idx++) {\n+      byte b = buffer[offset++];\n+      i = i << 8;\n+      i = i | (b & 0xFF);\n+    }\n+    return (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);\n+  }\n+\n   /**\n    * Converts a byte array to a long value\n    * @param bytes\n    * @return the long value\n    */\n   public static long toLong(byte[] bytes) {\n-    if (bytes == null || bytes.length == 0) {\n+    return toLong(bytes, 0);\n+  }\n+\n+  /**\n+   * Converts a byte array to a long value\n+   * @param bytes\n+   * @return the long value\n+   */\n+  public static long toLong(byte[] bytes, int offset) {\n+    return toLong(bytes, offset, SIZEOF_LONG);\n+  }\n+\n+  /**\n+   * Converts a byte array to a long value\n+   * @param bytes\n+   * @return the long value\n+   */\n+  public static long toLong(byte[] bytes, int offset,final int length) {\n+    if (bytes == null || bytes.length == 0 ||\n+        (offset + length > bytes.length)) {\n       return -1L;\n     }\n-    return ByteBuffer.wrap(bytes).getLong();\n+    long l = 0;\n+    for(int i = offset; i < (offset + length); i++) {\n+      l <<= 8;\n+      l ^= (long)bytes[i] & 0xFF;\n+    }\n+    return l;\n   }\n   \n   /**\n@@ -309,21 +401,29 @@ public static int compareTo(final byte [] left, final byte [] right) {\n   }\n \n   /**\n-   * @param left\n-   * @param right\n-   * @param leftOffset Where to start comparing in the left buffer\n-   * @param rightOffset Where to start comparing in the right buffer\n-   * @param leftLength How much to compare from the left buffer\n-   * @param rightLength How much to compare from the right buffer\n+   * @param b1\n+   * @param b2\n+   * @param s1 Where to start comparing in the left buffer\n+   * @param s2 Where to start comparing in the right buffer\n+   * @param l1 How much to compare from the left buffer\n+   * @param l2 How much to compare from the right buffer\n    * @return 0 if equal, < 0 if left is less than right, etc.\n    */\n-  public static int compareTo(final byte [] left, final int leftOffset,\n-      final int leftLength, final byte [] right, final int rightOffset,\n-      final int rightLength) {\n-    return WritableComparator.compareBytes(left,leftOffset, leftLength,\n-        right, rightOffset, rightLength);\n+  public static int compareTo(byte[] b1, int s1, int l1,\n+      byte[] b2, int s2, int l2) {\n+    // Bring WritableComparator code local\n+    int end1 = s1 + l1;\n+    int end2 = s2 + l2;\n+    for (int i = s1, j = s2; i < end1 && j < end2; i++, j++) {\n+      int a = (b1[i] & 0xff);\n+      int b = (b2[j] & 0xff);\n+      if (a != b) {\n+        return a - b;\n+      }\n+    }\n+    return l1 - l2;\n   }\n-  \n+\n   /**\n    * @param left\n    * @param right",
                "deletions": 14
            },
            {
                "sha": "4c0cd9999547fcd414facc5cca493c1942a38402",
                "filename": "src/java/org/apache/hadoop/hbase/util/TestBytes.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/TestBytes.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/java/org/apache/hadoop/hbase/util/TestBytes.java",
                "status": "added",
                "changes": 32,
                "additions": 32,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/TestBytes.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -0,0 +1,32 @@\n+/**\n+ * Copyright 2009 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.util;\n+\n+import junit.framework.TestCase;\n+\n+public class TestBytes extends TestCase {\n+  public void testToLong() throws Exception {\n+    long [] longs = {-1l, 123l, 122232323232l};\n+    for (int i = 0; i < longs.length; i++) {\n+      byte [] b = Bytes.toBytes(longs[i]);\n+      assertEquals(longs[i], Bytes.toLong(b));\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "deletions": 0
            },
            {
                "sha": "a58142e14db4c41ed546dff86d3956e60ab0b1fb",
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "status": "modified",
                "changes": 27,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -22,25 +22,25 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n+import java.util.Iterator;\n import java.util.Map;\n import java.util.SortedMap;\n-import java.util.Iterator;\n+\n import junit.framework.TestCase;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n-import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.hbase.io.Cell;\n import org.apache.hadoop.hbase.io.RowResult;\n import org.apache.hadoop.hbase.regionserver.HRegion;\n import org.apache.hadoop.hbase.regionserver.InternalScanner;\n import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n \n /**\n  * Abstract base class for test cases. Performs all static initialization\n@@ -93,8 +93,7 @@ public HBaseTestCase(String name) {\n   private void init() {\n     conf = new HBaseConfiguration();\n     try {\n-      START_KEY =\n-        new String(START_KEY_BYTES, HConstants.UTF8_ENCODING) + PUNCTUATION;\n+      START_KEY = new String(START_KEY_BYTES, HConstants.UTF8_ENCODING);\n     } catch (UnsupportedEncodingException e) {\n       LOG.fatal(\"error during initialization\", e);\n       fail();\n@@ -191,14 +190,14 @@ protected HTableDescriptor createTableDescriptor(final String name,\n       final int versions) {\n     HTableDescriptor htd = new HTableDescriptor(name);\n     htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME1, versions,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE,\n-      HConstants.FOREVER, false));\n+      HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+      Integer.MAX_VALUE, HConstants.FOREVER, false));\n     htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME2, versions,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE,\n-      HConstants.FOREVER, false));\n+        HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+        Integer.MAX_VALUE, HConstants.FOREVER, false));\n     htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME3, versions,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE, \n-      HConstants.FOREVER, false));\n+        HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+        Integer.MAX_VALUE,  HConstants.FOREVER, false));\n     return htd;\n   }\n   \n@@ -279,9 +278,7 @@ protected static long addContent(final Incommon updater, final String column,\n     EXIT: for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n       for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n         for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n-          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n-          String s = Bytes.toString(bytes) + PUNCTUATION;\n-          byte [] t = Bytes.toBytes(s);\n+          byte [] t = new byte [] {(byte)c, (byte)d, (byte)e};\n           if (endKey != null && endKey.length > 0\n               && Bytes.compareTo(endKey, t) <= 0) {\n             break EXIT;",
                "deletions": 15
            },
            {
                "sha": "91001e6b64037d1e19a84ae6f4b819c663248f37",
                "filename": "src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java",
                "status": "modified",
                "changes": 47,
                "additions": 43,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -30,16 +30,21 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.client.HBaseAdmin;\n import org.apache.hadoop.hbase.client.HTable;\n import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.filter.PageRowFilter;\n+import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n+import org.apache.hadoop.hbase.io.RowResult;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.FSUtils;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.FileInputFormat;\n@@ -52,8 +57,6 @@\n import org.apache.hadoop.mapred.Reporter;\n import org.apache.hadoop.mapred.TextInputFormat;\n import org.apache.hadoop.mapred.TextOutputFormat;\n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n \n \n /**\n@@ -88,6 +91,7 @@\n   }\n   \n   private static final String RANDOM_READ = \"randomRead\";\n+  private static final String RANDOM_SEEK_SCAN = \"randomSeekScan\";\n   private static final String RANDOM_READ_MEM = \"randomReadMem\";\n   private static final String RANDOM_WRITE = \"randomWrite\";\n   private static final String SEQUENTIAL_READ = \"sequentialRead\";\n@@ -96,6 +100,7 @@\n   \n   private static final List<String> COMMANDS =\n     Arrays.asList(new String [] {RANDOM_READ,\n+      RANDOM_SEEK_SCAN,\n       RANDOM_READ_MEM,\n       RANDOM_WRITE,\n       SEQUENTIAL_READ,\n@@ -406,7 +411,36 @@ long test() throws IOException {\n      */\n     abstract String getTestName();\n   }\n-  \n+\n+  class RandomSeekScanTest extends Test {\n+    RandomSeekScanTest(final HBaseConfiguration conf, final int startRow,\n+        final int perClientRunRows, final int totalRows, final Status status) {\n+      super(conf, startRow, perClientRunRows, totalRows, status);\n+    }\n+    \n+    void testRow(@SuppressWarnings(\"unused\") final int i) throws IOException {\n+      Scanner s = this.table.getScanner(new byte [][] {COLUMN_NAME},\n+        getRandomRow(this.rand, this.totalRows),\n+        new WhileMatchRowFilter(new PageRowFilter(120)));\n+      int count = 0;\n+      for (RowResult rr = null; (rr = s.next()) != null;) {\n+        // LOG.info(\"\" + count++ + \" \" + rr.toString());\n+      }\n+      s.close();\n+    }\n+ \n+    @Override\n+    protected int getReportingPeriod() {\n+      // \n+      return this.perClientRunRows / 100;\n+    }\n+\n+    @Override\n+    String getTestName() {\n+      return \"randomSeekScanTest\";\n+    }\n+  }\n+\n   class RandomReadTest extends Test {\n     RandomReadTest(final HBaseConfiguration conf, final int startRow,\n         final int perClientRunRows, final int totalRows, final Status status) {\n@@ -581,6 +615,10 @@ long runOneClient(final String cmd, final int startRow,\n       Test t = new SequentialWriteTest(this.conf, startRow, perClientRunRows,\n         totalRows, status);\n       totalElapsedTime = t.test();\n+    } else if (cmd.equals(RANDOM_SEEK_SCAN)) {\n+      Test t = new RandomSeekScanTest(this.conf, startRow, perClientRunRows,\n+          totalRows, status);\n+        totalElapsedTime = t.test();\n     } else {\n       new IllegalArgumentException(\"Invalid command value: \" + cmd);\n     }\n@@ -671,6 +709,7 @@ private void printUsage(final String message) {\n     System.err.println(\" randomRead      Run random read test\");\n     System.err.println(\" randomReadMem   Run random read test where table \" +\n       \"is in memory\");\n+    System.err.println(\" randomSeekScan  Run random seek and scan 100 test\");\n     System.err.println(\" randomWrite     Run random write test\");\n     System.err.println(\" sequentialRead  Run sequential read test\");\n     System.err.println(\" sequentialWrite Run sequential write test\");",
                "deletions": 4
            },
            {
                "sha": "1f1278283b207965cc83e733ecd752d98c2914b2",
                "filename": "src/test/org/apache/hadoop/hbase/TestHStoreKey.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/TestHStoreKey.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/TestHStoreKey.java",
                "status": "modified",
                "changes": 225,
                "additions": 182,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestHStoreKey.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -19,10 +19,11 @@\n \n import java.io.IOException;\n import java.nio.ByteBuffer;\n+import java.util.Set;\n+import java.util.TreeSet;\n \n import junit.framework.TestCase;\n \n-import org.apache.hadoop.hbase.HStoreKey.StoreKeyByteComparator;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Writables;\n \n@@ -38,6 +39,184 @@ protected void tearDown() throws Exception {\n     super.tearDown();\n   }\n \n+  public void testMoreComparisons() throws Exception {\n+    // Root compares\n+    HStoreKey a = new HStoreKey(\".META.,,99999999999999\");\n+    HStoreKey b = new HStoreKey(\".META.,,1\");\n+    HStoreKey.StoreKeyComparator c = new HStoreKey.RootStoreKeyComparator();\n+    assertTrue(c.compare(b.getBytes(), a.getBytes()) < 0);\n+    HStoreKey aa = new HStoreKey(\".META.,,1\");\n+    HStoreKey bb = new HStoreKey(\".META.,,1\", \"info:regioninfo\", 1235943454602L);\n+    assertTrue(c.compare(aa.getBytes(), bb.getBytes()) < 0);\n+    \n+    // Meta compares\n+    HStoreKey aaa = new HStoreKey(\"TestScanMultipleVersions,row_0500,1236020145502\");\n+    HStoreKey bbb = new HStoreKey(\"TestScanMultipleVersions,,99999999999999\");\n+    c = new HStoreKey.MetaStoreKeyComparator();\n+    assertTrue(c.compare(bbb.getBytes(), aaa.getBytes()) < 0);\n+    \n+    HStoreKey aaaa = new HStoreKey(\"TestScanMultipleVersions,,1236023996656\",\n+      \"info:regioninfo\", 1236024396271L);\n+    assertTrue(c.compare(aaaa.getBytes(), bbb.getBytes()) < 0);\n+    \n+    HStoreKey x = new HStoreKey(\"TestScanMultipleVersions,row_0500,1236034574162\",\n+      \"\", 9223372036854775807L);\n+    HStoreKey y = new HStoreKey(\"TestScanMultipleVersions,row_0500,1236034574162\",\n+      \"info:regioninfo\", 1236034574912L);\n+    assertTrue(c.compare(x.getBytes(), y.getBytes()) < 0);\n+    \n+    comparisons(new HStoreKey.HStoreKeyRootComparator());\n+    comparisons(new HStoreKey.HStoreKeyMetaComparator());\n+    comparisons(new HStoreKey.HStoreKeyComparator());\n+    metacomparisons(new HStoreKey.HStoreKeyRootComparator());\n+    metacomparisons(new HStoreKey.HStoreKeyMetaComparator());\n+  }\n+\n+  /**\n+   * Tests cases where rows keys have characters below the ','.\n+   * See HBASE-832\n+   * @throws IOException \n+   */\n+  public void testHStoreKeyBorderCases() throws IOException {\n+    HStoreKey rowA = new HStoreKey(\"testtable,www.hbase.org/,1234\",\n+      \"\", Long.MAX_VALUE);\n+    byte [] rowABytes = Writables.getBytes(rowA);\n+    HStoreKey rowB = new HStoreKey(\"testtable,www.hbase.org/%20,99999\",\n+      \"\", Long.MAX_VALUE);\n+    byte [] rowBBytes = Writables.getBytes(rowB);\n+    // This is a plain compare on the row. It gives wrong answer for meta table\n+    // row entry.\n+    assertTrue(rowA.compareTo(rowB) > 0);\n+    HStoreKey.MetaStoreKeyComparator c =\n+      new HStoreKey.MetaStoreKeyComparator();\n+    assertTrue(c.compare(rowABytes, rowBBytes) < 0);\n+\n+    rowA = new HStoreKey(\"testtable,,1234\", \"\", Long.MAX_VALUE);\n+    rowB = new HStoreKey(\"testtable,$www.hbase.org/,99999\", \"\", Long.MAX_VALUE);\n+    assertTrue(rowA.compareTo(rowB) > 0);\n+    assertTrue(c.compare( Writables.getBytes(rowA),  Writables.getBytes(rowB)) < 0);\n+\n+    rowA = new HStoreKey(\".META.,testtable,www.hbase.org/,1234,4321\", \"\",\n+      Long.MAX_VALUE);\n+    rowB = new HStoreKey(\".META.,testtable,www.hbase.org/%20,99999,99999\", \"\",\n+      Long.MAX_VALUE);\n+    assertTrue(rowA.compareTo(rowB) > 0);\n+    HStoreKey.RootStoreKeyComparator rootComparator =\n+      new HStoreKey.RootStoreKeyComparator();\n+    assertTrue(rootComparator.compare( Writables.getBytes(rowA),\n+      Writables.getBytes(rowB)) < 0);\n+  }\n+\n+  private void metacomparisons(final HStoreKey.HStoreKeyComparator c) {\n+    assertTrue(c.compare(new HStoreKey(\".META.,a,,0,1\"),\n+      new HStoreKey(\".META.,a,,0,1\")) == 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,a,,0,1\"),\n+      new HStoreKey(\".META.,a,,0,2\")) < 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,a,,0,2\"),\n+      new HStoreKey(\".META.,a,,0,1\")) > 0);\n+  }\n+\n+  private void comparisons(final HStoreKey.HStoreKeyComparator c) {\n+    assertTrue(c.compare(new HStoreKey(\".META.,,1\"),\n+      new HStoreKey(\".META.,,1\")) == 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,,1\"),\n+      new HStoreKey(\".META.,,2\")) < 0);\n+    assertTrue(c.compare(new HStoreKey(\".META.,,2\"),\n+      new HStoreKey(\".META.,,1\")) > 0);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public void testBinaryKeys() throws Exception {\n+\tSet<HStoreKey> set = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyComparator());\n+\tHStoreKey [] keys = {new HStoreKey(\"aaaaa,\\u0000\\u0000,2\", getName(), 2),\n+\t  new HStoreKey(\"aaaaa,\\u0001,3\", getName(), 3),\n+\t  new HStoreKey(\"aaaaa,,1\", getName(), 1),\n+\t  new HStoreKey(\"aaaaa,\\u1000,5\", getName(), 5),\n+\t  new HStoreKey(\"aaaaa,a,4\", getName(), 4),\n+    new HStoreKey(\"a,a,0\", getName(), 0),\n+\t};\n+\t// Add to set with bad comparator\n+\tfor (int i = 0; i < keys.length; i++) {\n+\t  set.add(keys[i]);\n+\t}\n+\t// This will output the keys incorrectly.\n+\tboolean assertion = false;\n+\tint count = 0;\n+\ttry {\n+      for (HStoreKey k: set) {\n+        assertTrue(count++ == k.getTimestamp());\n+      }\n+\t} catch (junit.framework.AssertionFailedError e) {\n+\t  // Expected\n+\t  assertion = true;\n+\t}\n+\tassertTrue(assertion);\n+\t// Make set with good comparator\n+\tset = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyMetaComparator());\n+\tfor (int i = 0; i < keys.length; i++) {\n+      set.add(keys[i]);\n+    }\n+    count = 0;\n+    for (HStoreKey k: set) {\n+      assertTrue(count++ == k.getTimestamp());\n+    }\n+    // Make up -ROOT- table keys.\n+    HStoreKey [] rootKeys = {\n+        new HStoreKey(\".META.,aaaaa,\\u0000\\u0000,0,2\", getName(), 2),\n+        new HStoreKey(\".META.,aaaaa,\\u0001,0,3\", getName(), 3),\n+        new HStoreKey(\".META.,aaaaa,,0,1\", getName(), 1),\n+        new HStoreKey(\".META.,aaaaa,\\u1000,0,5\", getName(), 5),\n+        new HStoreKey(\".META.,aaaaa,a,0,4\", getName(), 4),\n+        new HStoreKey(\".META.,,0\", getName(), 0),\n+      };\n+    // This will output the keys incorrectly.\n+    set = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyMetaComparator());\n+    // Add to set with bad comparator\n+    for (int i = 0; i < keys.length; i++) {\n+      set.add(rootKeys[i]);\n+    }\n+    assertion = false;\n+    count = 0;\n+    try {\n+      for (HStoreKey k: set) {\n+        assertTrue(count++ == k.getTimestamp());\n+      }\n+    } catch (junit.framework.AssertionFailedError e) {\n+      // Expected\n+      assertion = true;\n+    }\n+    // Now with right comparator\n+    set = new TreeSet<HStoreKey>(new HStoreKey.HStoreKeyRootComparator());\n+    // Add to set with bad comparator\n+    for (int i = 0; i < keys.length; i++) {\n+      set.add(rootKeys[i]);\n+    }\n+    count = 0;\n+    for (HStoreKey k: set) {\n+      assertTrue(count++ == k.getTimestamp());\n+    }\n+  }\n+\n+  public void testSerialization() throws IOException {\n+    HStoreKey hsk = new HStoreKey(getName(), getName(), 123);\n+    byte [] b = hsk.getBytes();\n+    HStoreKey hsk2 = HStoreKey.create(b);\n+    assertTrue(hsk.equals(hsk2));\n+    // Test getBytes with empty column\n+    hsk = new HStoreKey(getName());\n+    assertTrue(Bytes.equals(hsk.getBytes(),\n+      HStoreKey.getBytes(Bytes.toBytes(getName()), null,\n+      HConstants.LATEST_TIMESTAMP)));\n+  }\n+\n+  public void testGetBytes() throws IOException {\n+    long now = System.currentTimeMillis();\n+    HStoreKey hsk = new HStoreKey(\"one\", \"two\", now);\n+    byte [] writablesBytes = Writables.getBytes(hsk);\n+    byte [] selfSerializationBytes = hsk.getBytes();\n+    Bytes.equals(writablesBytes, selfSerializationBytes);\n+  }\n+\n   public void testByteBuffer() throws Exception {\n     final long ts = 123;\n     final byte [] row = Bytes.toBytes(\"row\");\n@@ -61,7 +240,8 @@ public void testRawComparator() throws IOException {\n     byte [] nowBytes = Writables.getBytes(now);\n     HStoreKey future = new HStoreKey(a, a, timestamp + 10);\n     byte [] futureBytes = Writables.getBytes(future);\n-    StoreKeyByteComparator comparator = new HStoreKey.StoreKeyByteComparator();\n+    HStoreKey.StoreKeyComparator comparator =\n+      new HStoreKey.StoreKeyComparator();\n     assertTrue(past.compareTo(now) > 0);\n     assertTrue(comparator.compare(pastBytes, nowBytes) > 0);\n     assertTrue(now.compareTo(now) == 0);\n@@ -84,45 +264,4 @@ public void testRawComparator() throws IOException {\n     assertTrue(nocolumn.compareTo(withcolumn) < 0);\n     assertTrue(comparator.compare(nocolumnBytes, withcolumnBytes) < 0);\n   }\n-  \n-//  /**\n-//   * Tests cases where rows keys have characters below the ','.\n-//   * See HBASE-832\n-//   * @throws IOException \n-//   */\n-//  public void testHStoreKeyBorderCases() throws IOException {\n-//    HRegionInfo info = new HRegionInfo(new HTableDescriptor(\"testtable\"),\n-//        HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY);\n-//\n-//    HStoreKey rowA = new HStoreKey(\"testtable,www.hbase.org/,1234\",\n-//      \"\", Long.MAX_VALUE, info);\n-//    byte [] rowABytes = Writables.getBytes(rowA);\n-//    HStoreKey rowB = new HStoreKey(\"testtable,www.hbase.org/%20,99999\",\n-//      \"\", Long.MAX_VALUE, info);\n-//    byte [] rowBBytes = Writables.getBytes(rowB);\n-//    assertTrue(rowA.compareTo(rowB) > 0);\n-//    HStoreKey.Comparator comparator = new HStoreKey.PlainStoreKeyComparator();\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) > 0);\n-//\n-//    rowA = new HStoreKey(\"testtable,www.hbase.org/,1234\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    rowB = new HStoreKey(\"testtable,www.hbase.org/%20,99999\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    assertTrue(rowA.compareTo(rowB) < 0);\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) < 0);\n-//\n-//    rowA = new HStoreKey(\"testtable,,1234\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    rowB = new HStoreKey(\"testtable,$www.hbase.org/,99999\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.FIRST_META_REGIONINFO);\n-//    assertTrue(rowA.compareTo(rowB) < 0);\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) < 0);\n-//\n-//    rowA = new HStoreKey(\".META.,testtable,www.hbase.org/,1234,4321\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.ROOT_REGIONINFO);\n-//    rowB = new HStoreKey(\".META.,testtable,www.hbase.org/%20,99999,99999\",\n-//        \"\", Long.MAX_VALUE, HRegionInfo.ROOT_REGIONINFO);\n-//    assertTrue(rowA.compareTo(rowB) > 0);\n-//    assertTrue(comparator.compare(rowABytes, rowBBytes) > 0);\n-//  }\n }\n\\ No newline at end of file",
                "deletions": 43
            },
            {
                "sha": "f3559e732c891ad742c15bacb708d0e2d80ec581",
                "filename": "src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -66,7 +66,7 @@ public void setUp() throws Exception {\n     desc.addFamily(new HColumnDescriptor(CONTENTS_STR));\n     desc.addFamily(new HColumnDescriptor(SMALLFAM, \n         HColumnDescriptor.DEFAULT_VERSIONS, \n-        HColumnDescriptor.DEFAULT_COMPRESSION,  \n+        HColumnDescriptor.DEFAULT_COMPRESSION,\n         HColumnDescriptor.DEFAULT_IN_MEMORY, \n         HColumnDescriptor.DEFAULT_BLOCKCACHE, SMALL_LENGTH, \n         HColumnDescriptor.DEFAULT_TTL, HColumnDescriptor.DEFAULT_BLOOMFILTER));",
                "deletions": 1
            },
            {
                "sha": "a28e374b5445c1e9d73d09ca21f0d5cf4afa084b",
                "filename": "src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/filter/TestStopRowFilter.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -89,4 +89,4 @@ private void stopRowTests(RowFilterInterface filter) throws Exception {\n     \n     assertFalse(\"Filter a null\", filter.filterRowKey(null));\n   }\n-}\n+}\n\\ No newline at end of file",
                "deletions": 1
            },
            {
                "sha": "181511534736843062054e35b5f5c8d35a441a6f",
                "filename": "src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java",
                "status": "modified",
                "changes": 14,
                "additions": 8,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/io/hfile/TestHFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -35,10 +35,10 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.RawLocalFileSystem;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HStoreKey;\n import org.apache.hadoop.hbase.io.hfile.HFile.Reader;\n import org.apache.hadoop.hbase.io.hfile.HFile.Writer;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.apache.hadoop.io.RawComparator;\n \n /**\n  * test hfile features.\n@@ -129,7 +129,8 @@ private FSDataOutputStream createFSOutput(Path name) throws IOException {\n   void basicWithSomeCodec(String codec) throws IOException {\n     Path ncTFile = new Path(ROOT_DIR, \"basic.hfile\");\n     FSDataOutputStream fout = createFSOutput(ncTFile);\n-    Writer writer = new Writer(fout, minBlockSize, codec, null);\n+    Writer writer = new Writer(fout, minBlockSize,\n+      Compression.getCompressionAlgorithmByName(codec), null, false);\n     LOG.info(writer);\n     writeRecords(writer);\n     fout.close();\n@@ -192,7 +193,8 @@ private void someReadingWithMetaBlock(Reader reader) throws IOException {\n   private void metablocks(final String compress) throws Exception {\n     Path mFile = new Path(ROOT_DIR, \"meta.tfile\");\n     FSDataOutputStream fout = createFSOutput(mFile);\n-    Writer writer = new Writer(fout, minBlockSize, compress, null);\n+    Writer writer = new Writer(fout, minBlockSize,\n+      Compression.getCompressionAlgorithmByName(compress), null, false);\n     someTestingWithMetaBlock(writer);\n     writer.close();\n     fout.close();\n@@ -227,8 +229,8 @@ public void testCompressionOrdinance() {\n   public void testComparator() throws IOException {\n     Path mFile = new Path(ROOT_DIR, \"meta.tfile\");\n     FSDataOutputStream fout = createFSOutput(mFile);\n-    Writer writer = new Writer(fout, minBlockSize, \"none\",\n-      new RawComparator<byte []>() {\n+    Writer writer = new Writer(fout, minBlockSize, null,\n+      new HStoreKey.StoreKeyComparator() {\n         @Override\n         public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,\n             int l2) {\n@@ -239,7 +241,7 @@ public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,\n         public int compare(byte[] o1, byte[] o2) {\n           return compare(o1, 0, o1.length, o2, 0, o2.length);\n         }\n-      });\n+      }, false);\n     writer.append(\"3\".getBytes(), \"0\".getBytes());\n     writer.append(\"2\".getBytes(), \"0\".getBytes());\n     writer.append(\"1\".getBytes(), \"0\".getBytes());",
                "deletions": 6
            },
            {
                "sha": "359e3973399d232af5f60c3c4e0b78815b36f331",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -168,7 +168,7 @@ public void testComputedParameters() throws IOException {\n     desc.addFamily(\n         new HColumnDescriptor(CONTENTS,               // Column name\n             1,                                        // Max versions\n-            HColumnDescriptor.CompressionType.NONE,   // no compression\n+            HColumnDescriptor.DEFAULT_COMPRESSION,   // no compression\n             HColumnDescriptor.DEFAULT_IN_MEMORY,      // not in memory\n             HColumnDescriptor.DEFAULT_BLOCKCACHE,\n             HColumnDescriptor.DEFAULT_LENGTH,",
                "deletions": 1
            },
            {
                "sha": "5806d03d70ee7f18dcb96e04ac898a9ce228de7d",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -186,7 +186,7 @@ private void createStoreFile(final HRegion region) throws IOException {\n   private void createSmallerStoreFile(final HRegion region) throws IOException {\n     HRegionIncommon loader = new HRegionIncommon(region); \n     addContent(loader, Bytes.toString(COLUMN_FAMILY),\n-        (\"bbb\" + PUNCTUATION).getBytes(), null);\n+        (\"bbb\").getBytes(), null);\n     loader.flushcache();\n   }\n }",
                "deletions": 1
            },
            {
                "sha": "71ad82b9153be985c59f604048dd9b7d0beef4ed",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java",
                "status": "modified",
                "changes": 300,
                "additions": 175,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -28,12 +28,17 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hbase.HBaseTestCase;\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HRegionInfo;\n import org.apache.hadoop.hbase.HServerAddress;\n import org.apache.hadoop.hbase.HStoreKey;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.filter.StopRowFilter;\n+import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;\n import org.apache.hadoop.hbase.io.BatchUpdate;\n import org.apache.hadoop.hbase.io.Cell;\n+import org.apache.hadoop.hbase.io.hfile.Compression;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.hbase.util.Writables;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n@@ -55,10 +60,20 @@\n     HConstants.COL_STARTCODE\n   };\n   \n-  private static final byte [] ROW_KEY =\n-    HRegionInfo.ROOT_REGIONINFO.getRegionName();\n-  private static final HRegionInfo REGION_INFO =\n-    HRegionInfo.ROOT_REGIONINFO;\n+  static final HTableDescriptor TESTTABLEDESC =\n+    new HTableDescriptor(\"testscanner\");\n+  static {\n+    TESTTABLEDESC.addFamily(new HColumnDescriptor(HConstants.COLUMN_FAMILY,\n+      10,  // Ten is arbitrary number.  Keep versions to help debuggging.\n+      Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,\n+      Integer.MAX_VALUE, HConstants.FOREVER, false));\n+  }\n+  /** HRegionInfo for root region */\n+  public static final HRegionInfo REGION_INFO =\n+    new HRegionInfo(TESTTABLEDESC, HConstants.EMPTY_BYTE_ARRAY,\n+    HConstants.EMPTY_BYTE_ARRAY);\n+  \n+  private static final byte [] ROW_KEY = REGION_INFO.getRegionName();\n   \n   private static final long START_CODE = Long.MAX_VALUE;\n \n@@ -75,7 +90,127 @@ public void setUp() throws Exception {\n     super.setUp();\n     \n   }\n-  \n+\n+  /** The test!\n+   * @throws IOException\n+   */\n+  public void testScanner() throws IOException {\n+    try {\n+      r = createNewHRegion(TESTTABLEDESC, null, null);\n+      region = new HRegionIncommon(r);\n+      \n+      // Write information to the meta table\n+\n+      BatchUpdate batchUpdate =\n+        new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n+\n+      ByteArrayOutputStream byteStream = new ByteArrayOutputStream();\n+      DataOutputStream s = new DataOutputStream(byteStream);\n+      REGION_INFO.write(s);\n+      batchUpdate.put(HConstants.COL_REGIONINFO, byteStream.toByteArray());\n+      region.commit(batchUpdate);\n+\n+      // What we just committed is in the memcache. Verify that we can get\n+      // it back both with scanning and get\n+      \n+      scan(false, null);\n+      getRegionInfo();\n+      \n+      // Close and re-open\n+      \n+      r.close();\n+      r = openClosedRegion(r);\n+      region = new HRegionIncommon(r);\n+\n+      // Verify we can get the data back now that it is on disk.\n+      \n+      scan(false, null);\n+      getRegionInfo();\n+      \n+      // Store some new information\n+ \n+      HServerAddress address = new HServerAddress(\"foo.bar.com:1234\");\n+\n+      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n+\n+      batchUpdate.put(HConstants.COL_SERVER,  Bytes.toBytes(address.toString()));\n+\n+      batchUpdate.put(HConstants.COL_STARTCODE, Bytes.toBytes(START_CODE));\n+\n+      region.commit(batchUpdate);\n+      \n+      // Validate that we can still get the HRegionInfo, even though it is in\n+      // an older row on disk and there is a newer row in the memcache\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+      \n+      // flush cache\n+\n+      region.flushcache();\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // Close and reopen\n+      \n+      r.close();\n+      r = openClosedRegion(r);\n+      region = new HRegionIncommon(r);\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // Now update the information again\n+\n+      address = new HServerAddress(\"bar.foo.com:4321\");\n+      \n+      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n+\n+      batchUpdate.put(HConstants.COL_SERVER, \n+        Bytes.toBytes(address.toString()));\n+\n+      region.commit(batchUpdate);\n+      \n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // flush cache\n+\n+      region.flushcache();\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+\n+      // Close and reopen\n+      \n+      r.close();\n+      r = openClosedRegion(r);\n+      region = new HRegionIncommon(r);\n+\n+      // Validate again\n+      \n+      scan(true, address.toString());\n+      getRegionInfo();\n+      \n+      // clean up\n+      \n+      r.close();\n+      r.getLog().closeAndDelete();\n+      \n+    } finally {\n+      shutdownDfs(cluster);\n+    }\n+  }\n+\n   /** Compare the HRegionInfo we read from HBase to what we stored */\n   private void validateRegionInfo(byte [] regionBytes) throws IOException {\n     HRegionInfo info =\n@@ -145,7 +280,41 @@ private void getRegionInfo() throws IOException {\n     byte [] bytes = region.get(ROW_KEY, HConstants.COL_REGIONINFO).getValue();\n     validateRegionInfo(bytes);  \n   }\n-  \n+\n+  /**\n+   * Test basic stop row filter works.\n+   */\n+  public void testStopRow() throws Exception {\n+    byte [] startrow = Bytes.toBytes(\"bbb\");\n+    byte [] stoprow = Bytes.toBytes(\"ccc\");\n+    try {\n+      this.r = createNewHRegion(REGION_INFO.getTableDesc(), null, null);\n+      addContent(this.r, HConstants.COLUMN_FAMILY);\n+      InternalScanner s = r.getScanner(HConstants.COLUMN_FAMILY_ARRAY,\n+        startrow, HConstants.LATEST_TIMESTAMP,\n+        new WhileMatchRowFilter(new StopRowFilter(stoprow)));\n+      HStoreKey key = new HStoreKey();\n+      SortedMap<byte [], Cell> results =\n+        new TreeMap<byte [], Cell>(Bytes.BYTES_COMPARATOR);\n+      int count = 0;\n+      for (boolean first = true; s.next(key, results);) {\n+        if (first) {\n+          assertTrue(Bytes.BYTES_COMPARATOR.compare(startrow, key.getRow()) == 0);\n+          first = false;\n+        }\n+        count++;\n+      }\n+      assertTrue(Bytes.BYTES_COMPARATOR.compare(stoprow, key.getRow()) > 0);\n+      // We got something back.\n+      assertTrue(count > 10);\n+      s.close();\n+    } finally {\n+      this.r.close();\n+      this.r.getLog().closeAndDelete();\n+      shutdownDfs(this.cluster);\n+    }\n+  }\n+\n   /**\n    * HBase-910.\n    * @throws Exception\n@@ -194,123 +363,4 @@ private int count(final HRegionIncommon hri, final int flushIndex)\n     LOG.info(\"Found \" + count + \" items\");\n     return count;\n   }\n-\n-  /** The test!\n-   * @throws IOException\n-   */\n-  public void testScanner() throws IOException {\n-    try {\n-      r = createNewHRegion(REGION_INFO.getTableDesc(), null, null);\n-      region = new HRegionIncommon(r);\n-      \n-      // Write information to the meta table\n-\n-      BatchUpdate batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n-\n-      ByteArrayOutputStream byteStream = new ByteArrayOutputStream();\n-      DataOutputStream s = new DataOutputStream(byteStream);\n-      HRegionInfo.ROOT_REGIONINFO.write(s);\n-      batchUpdate.put(HConstants.COL_REGIONINFO, byteStream.toByteArray());\n-      region.commit(batchUpdate);\n-\n-      // What we just committed is in the memcache. Verify that we can get\n-      // it back both with scanning and get\n-      \n-      scan(false, null);\n-      getRegionInfo();\n-      \n-      // Close and re-open\n-      \n-      r.close();\n-      r = openClosedRegion(r);\n-      region = new HRegionIncommon(r);\n-\n-      // Verify we can get the data back now that it is on disk.\n-      \n-      scan(false, null);\n-      getRegionInfo();\n-      \n-      // Store some new information\n- \n-      HServerAddress address = new HServerAddress(\"foo.bar.com:1234\");\n-\n-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n-\n-      batchUpdate.put(HConstants.COL_SERVER,  Bytes.toBytes(address.toString()));\n-\n-      batchUpdate.put(HConstants.COL_STARTCODE, Bytes.toBytes(START_CODE));\n-\n-      region.commit(batchUpdate);\n-      \n-      // Validate that we can still get the HRegionInfo, even though it is in\n-      // an older row on disk and there is a newer row in the memcache\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-      \n-      // flush cache\n-\n-      region.flushcache();\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // Close and reopen\n-      \n-      r.close();\n-      r = openClosedRegion(r);\n-      region = new HRegionIncommon(r);\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // Now update the information again\n-\n-      address = new HServerAddress(\"bar.foo.com:4321\");\n-      \n-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());\n-\n-      batchUpdate.put(HConstants.COL_SERVER, \n-        Bytes.toBytes(address.toString()));\n-\n-      region.commit(batchUpdate);\n-      \n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // flush cache\n-\n-      region.flushcache();\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-\n-      // Close and reopen\n-      \n-      r.close();\n-      r = openClosedRegion(r);\n-      region = new HRegionIncommon(r);\n-\n-      // Validate again\n-      \n-      scan(true, address.toString());\n-      getRegionInfo();\n-      \n-      // clean up\n-      \n-      r.close();\n-      r.getLog().closeAndDelete();\n-      \n-    } finally {\n-      shutdownDfs(cluster);\n-    }\n-  }\n }",
                "deletions": 125
            },
            {
                "sha": "4b0d3c7d9335b6cb01678de66fdfc958fd4fd342",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java",
                "status": "modified",
                "changes": 5,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestStoreFile.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -70,7 +70,7 @@ public void testBasicHalfMapFile() throws Exception {\n     // Make up a directory hierarchy that has a regiondir and familyname.\n     HFile.Writer writer = StoreFile.getWriter(this.fs,\n       new Path(new Path(this.testDir, \"regionname\"), \"familyname\"),\n-      2 * 1024, null, null);\n+      2 * 1024, null, null, false);\n     writeStoreFile(writer);\n     checkHalfHFile(new StoreFile(this.fs, writer.getPath()));\n   }\n@@ -107,7 +107,8 @@ public void testReference()\n     Path storedir = new Path(new Path(this.testDir, \"regionname\"), \"familyname\");\n     Path dir = new Path(storedir, \"1234567890\");\n     // Make a store file and write data to it.\n-    HFile.Writer writer = StoreFile.getWriter(this.fs, dir, 8 * 1024, null, null);\n+    HFile.Writer writer = StoreFile.getWriter(this.fs, dir, 8 * 1024, null,\n+      null, false);\n     writeStoreFile(writer);\n     StoreFile hsf = new StoreFile(this.fs, writer.getPath());\n     HFile.Reader reader = hsf.getReader();",
                "deletions": 2
            },
            {
                "sha": "04da57c1adad37709e05eb959708328494c0bc8d",
                "filename": "src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "blob_url": "https://github.com/apache/hbase/blob/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "raw_url": "https://github.com/apache/hbase/raw/08693f66a877ef782ac706e8425aba6fcb72d267/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java",
                "status": "modified",
                "changes": 5,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/regionserver/TestTimestamp.java?ref=08693f66a877ef782ac706e8425aba6fcb72d267",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.HTableDescriptor;\n import org.apache.hadoop.hbase.TimestampTestBase;\n-import org.apache.hadoop.hbase.HColumnDescriptor.CompressionType;\n import org.apache.hadoop.hbase.util.Bytes;\n \n /**\n@@ -79,8 +78,8 @@ public void testTimestampScanning() throws IOException {\n   private HRegion createRegion() throws IOException {\n     HTableDescriptor htd = createTableDescriptor(getName());\n     htd.addFamily(new HColumnDescriptor(COLUMN, VERSIONS,\n-      CompressionType.NONE, false, false, Integer.MAX_VALUE,\n-      HConstants.FOREVER, false));\n+      HColumnDescriptor.DEFAULT_COMPRESSION, false, false,\n+      Integer.MAX_VALUE, HConstants.FOREVER, false));\n     return createNewHRegion(htd, null, null);\n   }\n }\n\\ No newline at end of file",
                "deletions": 3
            }
        ]
    },
    {
        "repo": "hbase",
        "message": "HADOOP-1644 [hbase] Compactions should not block updates\n\nDisentangles flushes and compactions; flushes can proceed while a\ncompaction is happening.  Also, don't compact unless we hit\ncompaction threshold: i.e. don't automatically compact on HRegion\nstartup so regions can come online the faster.\n\nM src/contrib/hbase/conf/hbase-default.xml\n    (hbase.hregion.compactionThreashold): Moved to be a hstore property\n    as part of encapsulating compaction decision inside hstore.\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/HBaseTestCase.java\n    Refactored.  Moved here generalized content loading code that can\n    be shared by tests.  Add to setup and teardown the setup and removal\n    of local test dir (if it exists).\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestCompare.java\n    Added test of HStoreKey compare (It works other than one would at\n    first expect).\nM src/contrib/hbase/src/test/org/apache/hadoop/hbase/TestSplit.java\n    Bulk of content loading code has been moved up into the parent class.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HConnectionManager.java\n    (tableExists): Restore to a check of if the asked-for table is in list of\n    tables.  As it was, a check for tableExists would just wait on all timeouts\n    and retries to expire and then report table does not exist..  Fixed up\n    debug message listing regions of a table.  Added protection against meta\n    table not having a COL_REGINFO (Seen in cluster testing -- probably a bug\n    in row removal).\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStoreFile.java\n    Loading store files, even if it was noticed that there was no corresponding\n    map file, was still counting file as valid.  Also fix merger -- was\n    constructing MapFile.Reader directly rather than asking HStoreFile for\n    the reader (HStoreFile knows how to do MapFile references)\n    (rename): Added check that move succeeded and logging.  In cluster-testing,\n    the hdfs move of compacted file into place has failed on occasion (Need\n    more info).\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStore.java\n    Encapsulate ruling on whether a compaction should take place inside HStore.\n    Added reading of the compactionThreshold her.  Compaction threshold is\n    currently just number of store files.  Later may include other factors such\n    as count of reference files.  Cleaned up debug messages around\n    reconstruction log.  Removed compaction if size > 1 from constructor.  Let\n    compaction happen after we've been deployed (Compactions that happen while\n    we are online can continue to take updates.  Compaction in the constructor\n    puts off our being able to take in updates).\n    (close): Changed so it now returns set of store files.  This used to be done\n    by calls to flush. Since flush and compaction have been disentangled, a\n    compaction can come in after flush and the list of files could be off.\n    Having it done by close, can be sure list of files is complete.\n    (flushCache): No longer returns set of store files.  Added 'merging compaction'\n    where we pick an arbitrary store file from disk and merge into it the content\n    of memcache (Needs work).\n    (getAllMapFiles): Renamed getAllStoreFiles.\n    (needsCompaction): Added.\n    (compactHelper): Added passing of maximum sequence number if already\n    calculated. If compacting one file only, we used skip without rewriting\n    the info file.  Fixed.\n    Refactored.  Moved guts to new  compact(outFile, listOfStores)  method.\n    (compact, CompactionReader): Added overrides and interface  to support\n    'merging compaction' that takes files and memcache.  In compaction,\n    if we failed the move of the compacted file, all data had already been\n    deleted.  Changing, so deletion happens after confirmed move of\n    compacted file.\n    (getFull): Fixed bug where NPE when read of maps came back null.\n    Revealed by our NOT compacting stores on startup.  Meant could be two\n    backing stores one of which had no data regards queried key.\n    (getNMaps): Renamed countOfStoreFiles.\n    (toString): Added.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HStoreKey.java\n    Added comment on 'odd'-looking comparison.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HRegionServer.java\n    Javadoc edit. \nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HLogEdit.java\n    Only return first 128 bytes of value when toStringing (On cluster,\n    was returning complete web pages in log).\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HMaster.java\n    Removed confusing debug message (made sense once -- but not now).\n    Test rootRegionLocation for null before using it (can be null).\nM  src/contrib/hbase/src/java/org/apache/hadoop/hbase/HMemcache.java\n    Added comment that delete behavior needs study.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/HRegion.java\n    Fixed merge so it doesn't do the incremental based off files\n    returned by flush.  Instead all is done in the one go after\n    region closes (using files returned by close).\n    Moved duplicated code to new filesByFamily method.\n    (WriteState): Removed writesOngoing in favor of compacting and\n    flushing flags.\n    (flushCache): No longer returns list of files.\nM src/contrib/hbase/src/java/org/apache/hadoop/hbase/util/Writables.java\n    Fix javadoc.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/hadoop/trunk/src/contrib/hbase@566459 13f79535-47bb-0310-9956-ffa450edef68",
        "commit": "https://github.com/apache/hbase/commit/be33a241ce1a2926fb72a961b8d02213057d14a8",
        "parent": "https://github.com/apache/hbase/commit/6c8e71367141628445b4cc843ad60afdfb5bb7c7",
        "bug_id": "hbase_322",
        "file": [
            {
                "sha": "97ec53b7fce109c5b065f4c454ee3050d0a1732f",
                "filename": "CHANGES.txt",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/CHANGES.txt",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/CHANGES.txt",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/CHANGES.txt?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -95,3 +95,4 @@ Trunk (unreleased changes)\n  58. HADOOP-1710 All updates should be batch updates\n  59. HADOOP-1711 HTable API should use interfaces instead of concrete classes as\n      method parameters and return values\n+ 60. HADOOP-1644 Compactions should not block updates",
                "deletions": 0
            },
            {
                "sha": "9a4316c22f6ef33640e948c765df8462d6e9ea26",
                "filename": "conf/hbase-default.xml",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/conf/hbase-default.xml",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/conf/hbase-default.xml",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/conf/hbase-default.xml?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -147,7 +147,7 @@\n     </description>\n   </property>\n   <property>\n-    <name>hbase.hregion.compactionThreshold</name>\n+    <name>hbase.hstore.compactionThreshold</name>\n     <value>3</value>\n     <description>\n     If more than this number of HStoreFiles in any one HStore",
                "deletions": 1
            },
            {
                "sha": "442b904f8c8ac4e3f1f5902748c115dab4e36597",
                "filename": "src/java/org/apache/hadoop/hbase/HConnection.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnection.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnection.java",
                "status": "modified",
                "changes": 1,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnection.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -38,6 +38,7 @@\n   public boolean isMasterRunning();\n   \n   /**\n+   * Checks if <code>tableName</code> exists.\n    * @param tableName Table to check.\n    * @return True if table exists already.\n    */",
                "deletions": 0
            },
            {
                "sha": "56215760df66eb0aee8fa3e002fbdee9808b44a4",
                "filename": "src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HConnectionManager.java",
                "status": "modified",
                "changes": 58,
                "additions": 35,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HConnectionManager.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -44,7 +44,7 @@\n  * multiple HBase instances\n  */\n public class HConnectionManager implements HConstants {\n-  private HConnectionManager(){}                        // Not instantiable\n+  private HConnectionManager() {}                        // Not instantiable\n   \n   // A Map of master HServerAddress -> connection information for that instance\n   // Note that although the Map is synchronized, the objects it contains\n@@ -209,15 +209,19 @@ public boolean isMasterRunning() {\n \n     /** {@inheritDoc} */\n     public boolean tableExists(final Text tableName) {\n-      boolean exists = true;\n+      if (tableName == null) {\n+        throw new IllegalArgumentException(\"Table name cannot be null\");\n+      }\n+      boolean exists = false;\n       try {\n-        SortedMap<Text, HRegionLocation> servers = getTableServers(tableName);\n-        if (servers == null || servers.size() == 0) {\n-          exists = false;\n+        HTableDescriptor[] tables = listTables();\n+        for (int i = 0; i < tables.length; i++) {\n+          if (tables[i].getName().equals(tableName)) {\n+            exists = true;\n+          }\n         }\n-\n       } catch (IOException e) {\n-        exists = false;\n+        LOG.warn(\"Testing for table existence threw exception\", e);\n       }\n       return exists;\n     }\n@@ -400,7 +404,6 @@ public void close(Text tableName) {\n     throws IOException {\n       \n       // Wipe out everything we know about this table\n-\n       if (this.tablesToServers.remove(tableName) != null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Wiping out all we know of \" + tableName);\n@@ -524,9 +527,10 @@ public void close(Text tableName) {\n       }\n       this.tablesToServers.put(tableName, servers);\n       if (LOG.isDebugEnabled()) {\n+        int count = 0;\n         for (Map.Entry<Text, HRegionLocation> e: servers.entrySet()) {\n-          LOG.debug(\"Server \" + e.getKey() + \" is serving: \" + e.getValue() +\n-              \" for table \" + tableName);\n+          LOG.debug(\"Region \" + (1 + count++) + \" of \" + servers.size() +\n+            \": \" + e.getValue());\n         }\n       }\n       return servers;\n@@ -650,40 +654,47 @@ public void close(Text tableName) {\n         new TreeMap<Text, HRegionLocation>();\n       \n       for (int tries = 0; servers.size() == 0 && tries < numRetries; tries++) {\n-\n         long scannerId = -1L;\n         try {\n-          scannerId =\n-            server.openScanner(t.getRegionInfo().getRegionName(),\n-                COLUMN_FAMILY_ARRAY, tableName, System.currentTimeMillis(), null);\n+          scannerId = server.openScanner(t.getRegionInfo().getRegionName(),\n+            COLUMN_FAMILY_ARRAY, tableName, System.currentTimeMillis(), null);\n \n           while (true) {\n-            HRegionInfo regionInfo = null;\n-            String serverAddress = null;\n             KeyedData[] values = server.next(scannerId);\n             if (values.length == 0) {\n               if (servers.size() == 0) {\n                 // If we didn't find any servers then the table does not exist\n                 throw new TableNotFoundException(\"table '\" + tableName +\n-                    \"' does not exist in \" + t);\n+                  \"' does not exist in \" + t);\n               }\n \n               // We found at least one server for the table and now we're done.\n               if (LOG.isDebugEnabled()) {\n                 LOG.debug(\"Found \" + servers.size() + \" server(s) for \" +\n-                    \"location: \" + t + \" for tablename \" + tableName);\n+                  tableName + \" at \" + t);\n               }\n               break;\n             }\n \n-            byte[] bytes = null;\n             TreeMap<Text, byte[]> results = new TreeMap<Text, byte[]>();\n             for (int i = 0; i < values.length; i++) {\n               results.put(values[i].getKey().getColumn(), values[i].getData());\n             }\n-            regionInfo = new HRegionInfo();\n-            regionInfo = (HRegionInfo) Writables.getWritable(\n-                results.get(COL_REGIONINFO), regionInfo);\n+            \n+            byte[] bytes = results.get(COL_REGIONINFO);\n+            if (bytes == null || bytes.length == 0) {\n+              // This can be null.  Looks like an info:splitA or info:splitB\n+              // is only item in the row.\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(COL_REGIONINFO.toString() + \" came back empty: \" +\n+                  results.toString());\n+              }\n+              servers.clear();\n+              break;\n+            }\n+            \n+            HRegionInfo regionInfo = (HRegionInfo) Writables.getWritable(\n+              results.get(COL_REGIONINFO), new HRegionInfo());\n \n             if (!regionInfo.tableDesc.getName().equals(tableName)) {\n               // We're done\n@@ -707,7 +718,8 @@ public void close(Text tableName) {\n               servers.clear();\n               break;\n             }\n-            serverAddress = Writables.bytesToString(bytes);\n+            \n+            String serverAddress = Writables.bytesToString(bytes);\n             servers.put(regionInfo.startKey, new HRegionLocation(\n                 regionInfo, new HServerAddress(serverAddress)));\n           }",
                "deletions": 23
            },
            {
                "sha": "6e5b6aa818bf79f38a000dd9ed30995611c1202e",
                "filename": "src/java/org/apache/hadoop/hbase/HLogEdit.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HLogEdit.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HLogEdit.java",
                "status": "modified",
                "changes": 17,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HLogEdit.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -34,6 +34,7 @@\n   private Text column = new Text();\n   private byte [] val;\n   private long timestamp;\n+  private final int MAX_VALUE_LEN = 128;\n \n   /**\n    * Default constructor used by Writable\n@@ -69,17 +70,23 @@ public long getTimestamp() {\n     return this.timestamp;\n   }\n \n-  /** {@inheritDoc} */\n+  /**\n+   * @return First column name, timestamp, and first 128 bytes of the value\n+   * bytes as a String.\n+   */\n   @Override\n   public String toString() {\n     String value = \"\";\n     try {\n-      value = new String(getVal(), HConstants.UTF8_ENCODING);\n-      \n+      value = (this.val.length > MAX_VALUE_LEN)?\n+        new String(this.val, 0, MAX_VALUE_LEN, HConstants.UTF8_ENCODING) +\n+          \"...\":\n+        new String(getVal(), HConstants.UTF8_ENCODING);\n     } catch (UnsupportedEncodingException e) {\n       throw new RuntimeException(\"UTF8 encoding not present?\", e);\n     }\n-    return \"(\" + getColumn().toString() + \"/\" + getTimestamp() + \"/\" + value + \")\";\n+    return \"(\" + getColumn().toString() + \"/\" + getTimestamp() + \"/\" +\n+      value + \")\";\n   }\n   \n   // Writable\n@@ -99,4 +106,4 @@ public void readFields(DataInput in) throws IOException {\n     in.readFully(this.val);\n     this.timestamp = in.readLong();\n   }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 5
            },
            {
                "sha": "aa008764b82a3dac20df3374d1a995da96ddb68c",
                "filename": "src/java/org/apache/hadoop/hbase/HMaster.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMaster.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMaster.java",
                "status": "modified",
                "changes": 43,
                "additions": 14,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMaster.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -312,18 +312,16 @@ private boolean cleanupSplits(final Text metaRegionName,\n       boolean noReferencesB = splitB == null;\n       \n       if (!noReferencesA) {\n-        noReferencesA =\n-          hasReferences(metaRegionName, server, info.getRegionName(), splitA, COL_SPLITA);\n+        noReferencesA = hasReferences(metaRegionName, server,\n+          info.getRegionName(), splitA, COL_SPLITA);\n       }\n       if (!noReferencesB) {\n-        noReferencesB =\n-          hasReferences(metaRegionName, server, info.getRegionName(), splitB, COL_SPLITB);\n+        noReferencesB = hasReferences(metaRegionName, server,\n+          info.getRegionName(), splitB, COL_SPLITB);\n       }\n-      if (!(noReferencesA && noReferencesB)) {\n-        \n+      if (!noReferencesA && !noReferencesB) {\n         // No references.  Remove this item from table and deleted region on\n         // disk.\n-        \n         LOG.info(\"Deleting region \" + info.getRegionName() +\n         \" because daughter splits no longer hold references\");\n         \n@@ -337,7 +335,6 @@ private boolean cleanupSplits(final Text metaRegionName,\n         b.delete(lockid, COL_SERVER);\n         b.delete(lockid, COL_STARTCODE);\n         server.batchUpdate(metaRegionName, System.currentTimeMillis(), b);\n-        \n         result = true;\n       }\n       \n@@ -361,8 +358,8 @@ protected boolean hasReferences(final Text metaRegionName,\n         \n         Path [] ps = fs.listPaths(p,\n             new PathFilter () {\n-              public boolean accept(Path path) {\n-                return HStoreFile.isReference(path);\n+              public boolean accept(Path p) {\n+                return HStoreFile.isReference(p);\n               }\n             }\n         );\n@@ -394,18 +391,11 @@ protected void checkAssigned(final HRegionInfo info,\n         final String serverName, final long startCode) {\n       \n       // Skip region - if ...\n-      \n-      if(info.offLine                                       // offline\n-          || killedRegions.contains(info.regionName)        // queued for offline\n-          || regionsToDelete.contains(info.regionName)) {   // queued for delete\n-\n+      if(info.offLine                                     // offline\n+          || killedRegions.contains(info.regionName)      // queued for offline\n+          || regionsToDelete.contains(info.regionName)) { // queued for delete\n         unassignedRegions.remove(info.regionName);\n         assignAttempts.remove(info.regionName);\n-\n-        if(LOG.isDebugEnabled()) {\n-          LOG.debug(\"not assigning region: \" + info.regionName + \" (offline: \" +\n-              info.isOffline() + \", split: \" + info.isSplit() + \")\");\n-        }\n         return;\n       }\n \n@@ -416,7 +406,6 @@ protected void checkAssigned(final HRegionInfo info,\n             regionsToKill.containsKey(info.regionName)) {\n           \n           // Skip if region is on kill list\n-\n           if(LOG.isDebugEnabled()) {\n             LOG.debug(\"not assigning region (on kill list): \" + info.regionName);\n           }\n@@ -431,14 +420,8 @@ protected void checkAssigned(final HRegionInfo info,\n           && (storedInfo == null || storedInfo.getStartCode() != startCode)) {\n         \n         // The current assignment is no good; load the region.\n-\n         unassignedRegions.put(info.regionName, info);\n         assignAttempts.put(info.regionName, Long.valueOf(0L));\n-      \n-      } else if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Finished if \" + info.getRegionName() + \" is assigned: \" +\n-            \"unassigned: \" + unassignedRegions.containsKey(info.regionName) +\n-            \", pending: \" + pendingRegions.contains(info.regionName));\n       }\n     }\n   }\n@@ -2155,8 +2138,10 @@ boolean process() throws IOException {\n           if (rootRegionLocation.get() == null || !rootScanned) {\n             // We can't proceed until the root region is online and has been scanned\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"root region=\" + rootRegionLocation.get().toString() +\n-                  \", rootScanned=\" + rootScanned);\n+              LOG.debug(\"root region: \" + \n+                ((rootRegionLocation != null)?\n+                  rootRegionLocation.toString(): \"null\") +\n+                \", rootScanned: \" + rootScanned);\n             }\n             return false;\n           }",
                "deletions": 29
            },
            {
                "sha": "16cb3968041aaa2fb0b1328f7cfb42c8da73cc81",
                "filename": "src/java/org/apache/hadoop/hbase/HMemcache.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HMemcache.java",
                "status": "modified",
                "changes": 11,
                "additions": 9,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HMemcache.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -243,6 +243,7 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n    *\n    * TODO - This is kinda slow.  We need a data structure that allows for \n    * proximity-searches, not just precise-matches.\n+   * \n    * @param map\n    * @param key\n    * @param numVersions\n@@ -251,13 +252,19 @@ void internalGetFull(TreeMap<HStoreKey, byte []> map, HStoreKey key,\n   ArrayList<byte []> get(final TreeMap<HStoreKey, byte []> map,\n       final HStoreKey key, final int numVersions) {\n     ArrayList<byte []> result = new ArrayList<byte []>();\n-    HStoreKey curKey =\n-      new HStoreKey(key.getRow(), key.getColumn(), key.getTimestamp());\n+    // TODO: If get is of a particular version -- numVersions == 1 -- we\n+    // should be able to avoid all of the tailmap creations and iterations\n+    // below.\n+    HStoreKey curKey = new HStoreKey(key);\n     SortedMap<HStoreKey, byte []> tailMap = map.tailMap(curKey);\n     for (Map.Entry<HStoreKey, byte []> es: tailMap.entrySet()) {\n       HStoreKey itKey = es.getKey();\n       if (itKey.matchesRowCol(curKey)) {\n         if(HConstants.DELETE_BYTES.compareTo(es.getValue()) == 0) {\n+          // TODO: Shouldn't this be a continue rather than a break?  Perhaps\n+          // the intent is that this DELETE_BYTES is meant to suppress older\n+          // info -- see 5.4 Compactions in BigTable -- but how does this jibe\n+          // with being able to remove one version only?\n           break;\n         }\n         result.add(tailMap.get(itKey));",
                "deletions": 2
            },
            {
                "sha": "0debc8323e320bcc1c5dfad44c853376a4daeaaf",
                "filename": "src/java/org/apache/hadoop/hbase/HRegion.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegion.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegion.java",
                "status": "modified",
                "changes": 232,
                "additions": 70,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegion.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -92,15 +92,13 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n     // Make sure that srcA comes first; important for key-ordering during\n     // write of the merged file.\n     FileSystem fs = srcA.getFilesystem();\n-    if(srcA.getStartKey() == null) {\n-      if(srcB.getStartKey() == null) {\n+    if (srcA.getStartKey() == null) {\n+      if (srcB.getStartKey() == null) {\n         throw new IOException(\"Cannot merge two regions with null start key\");\n       }\n       // A's start key is null but B's isn't. Assume A comes before B\n-      \n-    } else if((srcB.getStartKey() == null)         // A is not null but B is\n+    } else if ((srcB.getStartKey() == null)         // A is not null but B is\n         || (srcA.getStartKey().compareTo(srcB.getStartKey()) > 0)) { // A > B\n-      \n       a = srcB;\n       b = srcA;\n     }\n@@ -113,106 +111,29 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n     HTableDescriptor tabledesc = a.getTableDesc();\n     HLog log = a.getLog();\n     Path rootDir = a.getRootDir();\n-\n     Text startKey = a.getStartKey();\n     Text endKey = b.getEndKey();\n-\n     Path merges = new Path(a.getRegionDir(), MERGEDIR);\n     if(! fs.exists(merges)) {\n       fs.mkdirs(merges);\n     }\n     \n     HRegionInfo newRegionInfo\n       = new HRegionInfo(Math.abs(rand.nextLong()), tabledesc, startKey, endKey);\n-    \n     Path newRegionDir = HRegion.getRegionDir(merges, newRegionInfo.regionName);\n-\n     if(fs.exists(newRegionDir)) {\n-      throw new IOException(\"Cannot merge; target file collision at \" + newRegionDir);\n+      throw new IOException(\"Cannot merge; target file collision at \" +\n+        newRegionDir);\n     }\n \n     LOG.info(\"starting merge of regions: \" + a.getRegionName() + \" and \" +\n       b.getRegionName() + \" into new region \" + newRegionInfo.toString());\n-    \n-    // Flush each of the sources, and merge their files into a single \n-    // target for each column family.    \n-    TreeSet<HStoreFile> alreadyMerged = new TreeSet<HStoreFile>();\n-    TreeMap<Text, Vector<HStoreFile>> filesToMerge =\n-      new TreeMap<Text, Vector<HStoreFile>>();\n-    \n-    for(HStoreFile src: a.flushcache(true)) {\n-      Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-      if(v == null) {\n-        v = new Vector<HStoreFile>();\n-        filesToMerge.put(src.getColFamily(), v);\n-      }\n-      v.add(src);\n-    }\n-    \n-    for(HStoreFile src: b.flushcache(true)) {\n-      Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-      if(v == null) {\n-        v = new Vector<HStoreFile>();\n-        filesToMerge.put(src.getColFamily(), v);\n-      }\n-      v.add(src);\n-    }\n-    \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"merging stores\");\n-    }\n-    \n-    for (Map.Entry<Text, Vector<HStoreFile>> es: filesToMerge.entrySet()) {\n-      Text colFamily = es.getKey();\n-      Vector<HStoreFile> srcFiles = es.getValue();\n-      HStoreFile dst = new HStoreFile(conf, merges, newRegionInfo.regionName, \n-        colFamily, Math.abs(rand.nextLong()));\n-      dst.mergeStoreFiles(srcFiles, fs, conf);\n-      alreadyMerged.addAll(srcFiles);\n-    }\n \n-    // That should have taken care of the bulk of the data.\n-    // Now close the source HRegions for good, and repeat the above to take care\n-    // of any last-minute inserts\n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"flushing changes since start of merge for region \" \n-          + a.getRegionName());\n-    }\n-\n-    filesToMerge.clear();\n-    \n-    for(HStoreFile src: a.close()) {\n-      if(! alreadyMerged.contains(src)) {\n-        Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-        if(v == null) {\n-          v = new Vector<HStoreFile>();\n-          filesToMerge.put(src.getColFamily(), v);\n-        }\n-        v.add(src);\n-      }\n-    }\n-    \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"flushing changes since start of merge for region \" \n-          + b.getRegionName());\n-    }\n-    \n-    for(HStoreFile src: b.close()) {\n-      if(! alreadyMerged.contains(src)) {\n-        Vector<HStoreFile> v = filesToMerge.get(src.getColFamily());\n-        if(v == null) {\n-          v = new Vector<HStoreFile>();\n-          filesToMerge.put(src.getColFamily(), v);\n-        }\n-        v.add(src);\n-      }\n-    }\n-    \n-    if(LOG.isDebugEnabled()) {\n-      LOG.debug(\"merging changes since start of merge\");\n-    }\n-    \n-    for (Map.Entry<Text, Vector<HStoreFile>> es : filesToMerge.entrySet()) {\n+    Map<Text, Vector<HStoreFile>> byFamily =\n+      new TreeMap<Text, Vector<HStoreFile>>();\n+    byFamily = filesByFamily(byFamily, a.close());\n+    byFamily = filesByFamily(byFamily, b.close());\n+    for (Map.Entry<Text, Vector<HStoreFile>> es : byFamily.entrySet()) {\n       Text colFamily = es.getKey();\n       Vector<HStoreFile> srcFiles = es.getValue();\n       HStoreFile dst = new HStoreFile(conf, merges, newRegionInfo.regionName,\n@@ -233,6 +154,25 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n     \n     return dstRegion;\n   }\n+  \n+  /*\n+   * Fills a map with a vector of store files keyed by column family. \n+   * @param byFamily Map to fill.\n+   * @param storeFiles Store files to process.\n+   * @return Returns <code>byFamily</code>\n+   */\n+  private static Map<Text, Vector<HStoreFile>> filesByFamily(\n+      Map<Text, Vector<HStoreFile>> byFamily, Vector<HStoreFile> storeFiles) {\n+    for(HStoreFile src: storeFiles) {\n+      Vector<HStoreFile> v = byFamily.get(src.getColFamily());\n+      if(v == null) {\n+        v = new Vector<HStoreFile>();\n+        byFamily.put(src.getColFamily(), v);\n+      }\n+      v.add(src);\n+    }\n+    return byFamily;\n+  }\n \n   //////////////////////////////////////////////////////////////////////////////\n   // Members\n@@ -254,19 +194,19 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n   Path regiondir;\n \n   static class WriteState {\n-    volatile boolean writesOngoing;\n-    volatile boolean writesEnabled;\n-    WriteState() {\n-      this.writesOngoing = true;\n-      this.writesEnabled = true;\n-    }\n+    // Set while a memcache flush is happening.\n+    volatile boolean flushing = false;\n+    // Set while a compaction is running.\n+    volatile boolean compacting = false;\n+    // Gets set by last flush before close.  If set, cannot compact or flush\n+    // again.\n+    volatile boolean writesEnabled = true;\n   }\n   \n   volatile WriteState writestate = new WriteState();\n \n   final int memcacheFlushSize;\n   final int blockingMemcacheSize;\n-  int compactionThreshold = 0;\n   private final HLocking lock = new HLocking();\n   private long desiredMaxFileSize;\n   private final long maxSequenceId;\n@@ -297,15 +237,12 @@ static HRegion closeAndMerge(final HRegion srcA, final HRegion srcB)\n   public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf, \n       HRegionInfo regionInfo, Path initialFiles)\n   throws IOException {\n-    \n     this.rootDir = rootDir;\n     this.log = log;\n     this.fs = fs;\n     this.conf = conf;\n     this.regionInfo = regionInfo;\n     this.memcache = new HMemcache();\n-    this.writestate.writesOngoing = true;\n-    this.writestate.writesEnabled = true;\n \n     // Declare the regionName.  This is a unique string for the region, used to \n     // build a unique filename.\n@@ -319,7 +256,6 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     }\n \n     // Load in all the HStores.\n-\n     long maxSeqId = -1;\n     for(Map.Entry<Text, HColumnDescriptor> e :\n         this.regionInfo.tableDesc.families().entrySet()) {\n@@ -357,17 +293,12 @@ public HRegion(Path rootDir, HLog log, FileSystem fs, Configuration conf,\n     this.blockingMemcacheSize = this.memcacheFlushSize *\n       conf.getInt(\"hbase.hregion.memcache.block.multiplier\", 2);\n     \n-    // By default, we compact the region if an HStore has more than\n-    // MIN_COMMITS_FOR_COMPACTION map files\n-    this.compactionThreshold =\n-      conf.getInt(\"hbase.hregion.compactionThreshold\", 3);\n-    \n     // By default we split region if a file > DEFAULT_MAX_FILE_SIZE.\n     this.desiredMaxFileSize =\n       conf.getLong(\"hbase.hregion.max.filesize\", DEFAULT_MAX_FILE_SIZE);\n \n     // HRegion is ready to go!\n-    this.writestate.writesOngoing = false;\n+    this.writestate.compacting = false;\n     LOG.info(\"region \" + this.regionInfo.regionName + \" available\");\n   }\n   \n@@ -411,56 +342,48 @@ boolean isClosed() {\n    * \n    * @param abort true if server is aborting (only during testing)\n    * @return Vector of all the storage files that the HRegion's component \n-   * HStores make use of.  It's a list of HStoreFile objects.\n+   * HStores make use of.  It's a list of HStoreFile objects.  Can be null if\n+   * we are not to close at this time or we are already closed.\n    * \n    * @throws IOException\n    */\n   Vector<HStoreFile> close(boolean abort) throws IOException {\n     if (isClosed()) {\n       LOG.info(\"region \" + this.regionInfo.regionName + \" already closed\");\n-      return new Vector<HStoreFile>();\n+      return null;\n     }\n     lock.obtainWriteLock();\n     try {\n-      boolean shouldClose = false;\n       synchronized(writestate) {\n-        while(writestate.writesOngoing) {\n+        while(writestate.compacting || writestate.flushing) {\n           try {\n             writestate.wait();\n           } catch (InterruptedException iex) {\n             // continue\n           }\n         }\n-        writestate.writesOngoing = true;\n-        shouldClose = true;\n-      }\n-\n-      if(!shouldClose) {\n-        return null;\n+        // Disable compacting and flushing by background threads for this\n+        // region.\n+        writestate.writesEnabled = false;\n       }\n       \n       // Write lock means no more row locks can be given out.  Wait on\n       // outstanding row locks to come in before we close so we do not drop\n       // outstanding updates.\n       waitOnRowLocks();\n-\n-      Vector<HStoreFile> allHStoreFiles = null;\n+      \n       if (!abort) {\n         // Don't flush the cache if we are aborting during a test.\n-        allHStoreFiles = internalFlushcache();\n+        internalFlushcache();\n       }\n+      \n+      Vector<HStoreFile> result = new Vector<HStoreFile>();\n       for (HStore store: stores.values()) {\n-        store.close();\n-      }\n-      try {\n-        return allHStoreFiles;\n-      } finally {\n-        synchronized (writestate) {\n-          writestate.writesOngoing = false;\n-        }\n-        this.closed.set(true);\n-        LOG.info(\"closed \" + this.regionInfo.regionName);\n+        result.addAll(store.close());\n       }\n+      this.closed.set(true);\n+      LOG.info(\"closed \" + this.regionInfo.regionName);\n+      return result;\n     } finally {\n       lock.releaseWriteLock();\n     }\n@@ -527,6 +450,7 @@ boolean isClosed() {\n       HStoreFile a = new HStoreFile(this.conf, splits,\n         regionAInfo.regionName, h.getColFamily(), Math.abs(rand.nextLong()),\n         aReference);\n+      // Reference to top half of the hsf store file.\n       HStoreFile.Reference bReference = new HStoreFile.Reference(\n         getRegionName(), h.getFileId(), new HStoreKey(midKey),\n         HStoreFile.Range.top);\n@@ -721,12 +645,10 @@ boolean needsCompaction() {\n     boolean needsCompaction = false;\n     this.lock.obtainReadLock();\n     try {\n-      for(HStore store: stores.values()) {\n-        if(store.getNMaps() > this.compactionThreshold) {\n+      for (HStore store: stores.values()) {\n+        if (store.needsCompaction()) {\n           needsCompaction = true;\n-          LOG.info(getRegionName().toString() + \" needs compaction because \" +\n-            store.getNMaps() + \" store files present and threshold is \" +\n-            this.compactionThreshold);\n+          LOG.info(store.toString() + \" needs compaction\");\n           break;\n         }\n       }\n@@ -756,9 +678,9 @@ boolean compactStores() throws IOException {\n     lock.obtainReadLock();\n     try {\n       synchronized (writestate) {\n-        if ((!writestate.writesOngoing) &&\n+        if ((!writestate.compacting) &&\n             writestate.writesEnabled) {\n-          writestate.writesOngoing = true;\n+          writestate.compacting = true;\n           shouldCompact = true;\n         }\n       }\n@@ -783,7 +705,7 @@ boolean compactStores() throws IOException {\n     } finally {\n       lock.releaseReadLock();\n       synchronized (writestate) {\n-        writestate.writesOngoing = false;\n+        writestate.compacting = false;\n         writestate.notifyAll();\n       }\n     }\n@@ -825,23 +747,17 @@ void optionallyFlush() throws IOException {\n    * close() the HRegion shortly, so the HRegion should not take on any new and \n    * potentially long-lasting disk operations. This flush() should be the final\n    * pre-close() disk operation.\n-   * \n-   * @return List of store files including new flushes, if any.  If no flushes\n-   * because  memcache is null, returns all current store files.  Returns\n-   * null if no flush (Writes are going on elsewhere -- concurrently we are\n-   * compacting or splitting).\n    */\n-  Vector<HStoreFile> flushcache(boolean disableFutureWrites)\n+  void flushcache(boolean disableFutureWrites)\n   throws IOException {\n     if (this.closed.get()) {\n-      return null;\n+      return;\n     }\n     this.noFlushCount = 0;\n     boolean shouldFlush = false;\n     synchronized(writestate) {\n-      if((!writestate.writesOngoing) &&\n-          writestate.writesEnabled) {\n-        writestate.writesOngoing = true;\n+      if((!writestate.flushing) && writestate.writesEnabled) {\n+        writestate.flushing = true;\n         shouldFlush = true;\n         if(disableFutureWrites) {\n           writestate.writesEnabled = false;\n@@ -854,14 +770,14 @@ void optionallyFlush() throws IOException {\n         LOG.debug(\"NOT flushing memcache for region \" +\n           this.regionInfo.regionName);\n       }\n-      return null;  \n+      return;  \n     }\n     \n     try {\n-      return internalFlushcache();\n+      internalFlushcache();\n     } finally {\n       synchronized (writestate) {\n-        writestate.writesOngoing = false;\n+        writestate.flushing = false;\n         writestate.notifyAll();\n       }\n     }\n@@ -892,11 +808,8 @@ void optionallyFlush() throws IOException {\n    * routes.\n    * \n    * <p> This method may block for some time.\n-   * \n-   * @return List of store files including just-made new flushes per-store. If\n-   * not flush, returns list of all store files.\n    */\n-  Vector<HStoreFile> internalFlushcache() throws IOException {\n+  void internalFlushcache() throws IOException {\n     long startTime = -1;\n     if(LOG.isDebugEnabled()) {\n       startTime = System.currentTimeMillis();\n@@ -917,7 +830,7 @@ void optionallyFlush() throws IOException {\n     HMemcache.Snapshot retval = memcache.snapshotMemcacheForLog(log);\n     if(retval == null || retval.memcacheSnapshot == null) {\n       LOG.debug(\"Finished memcache flush; empty snapshot\");\n-      return getAllStoreFiles();\n+      return;\n     }\n     long logCacheFlushId = retval.sequenceId;\n     if(LOG.isDebugEnabled()) {\n@@ -929,11 +842,8 @@ void optionallyFlush() throws IOException {\n     // A.  Flush memcache to all the HStores.\n     // Keep running vector of all store files that includes both old and the\n     // just-made new flush store file.\n-    Vector<HStoreFile> allHStoreFiles = new Vector<HStoreFile>();\n     for(HStore hstore: stores.values()) {\n-      Vector<HStoreFile> hstoreFiles\n-        = hstore.flushCache(retval.memcacheSnapshot, retval.sequenceId);\n-      allHStoreFiles.addAll(0, hstoreFiles);\n+      hstore.flushCache(retval.memcacheSnapshot, retval.sequenceId);\n     }\n \n     // B.  Write a FLUSHCACHE-COMPLETE message to the log.\n@@ -958,13 +868,12 @@ void optionallyFlush() throws IOException {\n         this.regionInfo.regionName + \" in \" +\n           (System.currentTimeMillis() - startTime) + \"ms\");\n     }\n-    return allHStoreFiles;\n   }\n   \n   private Vector<HStoreFile> getAllStoreFiles() {\n     Vector<HStoreFile> allHStoreFiles = new Vector<HStoreFile>();\n     for(HStore hstore: stores.values()) {\n-      Vector<HStoreFile> hstoreFiles = hstore.getAllMapFiles();\n+      Vector<HStoreFile> hstoreFiles = hstore.getAllStoreFiles();\n       allHStoreFiles.addAll(0, hstoreFiles);\n     }\n     return allHStoreFiles;\n@@ -1020,7 +929,6 @@ void optionallyFlush() throws IOException {\n       }\n \n       // If unavailable in memcache, check the appropriate HStore\n-\n       Text colFamily = HStoreKey.extractFamily(key.getColumn());\n       HStore targetStore = stores.get(colFamily);\n       if(targetStore == null) {",
                "deletions": 162
            },
            {
                "sha": "ab64197d5042d0ea86389be87781d356c0bb4d60",
                "filename": "src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HRegionServer.java",
                "status": "modified",
                "changes": 13,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HRegionServer.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -158,7 +158,8 @@ public void run() {\n           try {\n             for(HRegion cur: regionsToCheck) {\n               if(cur.isClosed()) {\n-                continue;                               // Skip if closed\n+                // Skip if closed\n+                continue;\n               }\n               if (cur.needsCompaction()) {\n                 cur.compactStores();\n@@ -272,10 +273,6 @@ private void split(final HRegion region, final Text midKey)\n   protected final Integer cacheFlusherLock = new Integer(0);\n   \n   /* Runs periodically to flush memcache.\n-   * \n-   * Memcache flush is also called just before compaction and just before\n-   * split so memcache is best prepared for the the long trip across\n-   * compactions/splits during which it will not be able to flush to disk.\n    */\n   class Flusher implements Runnable {\n     /**\n@@ -286,9 +283,7 @@ public void run() {\n         long startTime = System.currentTimeMillis();\n \n         synchronized(cacheFlusherLock) {\n-\n           // Grab a list of items to flush\n-\n           Vector<HRegion> toFlush = new Vector<HRegion>();\n           lock.readLock().lock();\n           try {\n@@ -837,6 +832,7 @@ void reportSplit(HRegionInfo oldRegion, HRegionInfo newRegionA,\n   BlockingQueue<ToDoEntry> toDo;\n   private Worker worker;\n   private Thread workerThread;\n+  \n   /** Thread that performs long running requests from the master */\n   class Worker implements Runnable {\n     void stop() {\n@@ -910,7 +906,6 @@ void openRegion(HRegionInfo regionInfo) throws IOException {\n     HRegion region = onlineRegions.get(regionInfo.regionName);\n     if(region == null) {\n       region = new HRegion(rootDir, log, fs, conf, regionInfo, null);\n-\n       this.lock.writeLock().lock();\n       try {\n         this.log.setSequenceNumber(region.getMaxSequenceId());\n@@ -1193,7 +1188,7 @@ protected HRegion getRegion(final Text regionName)\n    * @return {@link HRegion} for <code>regionName</code>\n    * @throws NotServingRegionException\n    */\n-  protected HRegion getRegion(final Text regionName,\n+  protected HRegion getRegion(final Text regionName, \n       final boolean checkRetiringRegions)\n   throws NotServingRegionException {\n     HRegion region = null;",
                "deletions": 9
            },
            {
                "sha": "93ed53055824ddd67111446a0037d96e16c7171a",
                "filename": "src/java/org/apache/hadoop/hbase/HStore.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStore.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStore.java",
                "status": "modified",
                "changes": 509,
                "additions": 346,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStore.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -30,6 +30,7 @@\n import java.util.Random;\n import java.util.TreeMap;\n import java.util.Vector;\n+import java.util.Map.Entry;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -42,6 +43,7 @@\n import org.apache.hadoop.io.MapFile;\n import org.apache.hadoop.io.SequenceFile;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n import org.apache.hadoop.io.WritableComparable;\n import org.apache.hadoop.util.StringUtils;\n import org.onelab.filter.BloomFilter;\n@@ -92,6 +94,8 @@\n   Random rand = new Random();\n   \n   private long maxSeqId;\n+  \n+  private int compactionThreshold;\n \n   /**\n    * An HStore is a set of zero or more MapFiles, which stretch backwards over \n@@ -164,7 +168,7 @@\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"starting \" + this.storeName +\n-        ((reconstructionLog == null)?\n+        ((reconstructionLog == null || !fs.exists(reconstructionLog))?\n           \" (no reconstruction log)\": \" with reconstruction log: \" +\n           reconstructionLog.toString()));\n     }\n@@ -215,19 +219,19 @@\n     }\n     \n     doReconstructionLog(reconstructionLog, maxSeqId);\n-    this.maxSeqId += 1;\n \n-    // Compact all the MapFiles into a single file.  The resulting MapFile \n-    // should be \"timeless\"; that is, it should not have an associated seq-ID, \n-    // because all log messages have been reflected in the TreeMaps at this\n-    // point.  \n-    //\n-    // TODO: Only do the compaction if we are over a threshold, not\n-    // every time. Not necessary if only two or three store files.  Fix after\n-    // revamp of compaction.\n-    if(storefiles.size() > 1) {\n-      compactHelper(true);\n-    }\n+    // By default, we compact if an HStore has more than\n+    // MIN_COMMITS_FOR_COMPACTION map files\n+    this.compactionThreshold =\n+      conf.getInt(\"hbase.hstore.compactionThreshold\", 3);\n+    \n+    // We used to compact in here before bringing the store online.  Instead\n+    // get it online quick even if it needs compactions so we can start\n+    // taking updates as soon as possible (Once online, can take updates even\n+    // during a compaction).\n+\n+    // Move maxSeqId on by one. Why here?  And not in HRegion?\n+    this.maxSeqId += 1;\n     \n     // Finally, start up all the map readers! (There should be just one at this \n     // point, as we've compacted them all.)\n@@ -253,10 +257,6 @@ private void doReconstructionLog(final Path reconstructionLog,\n       final long maxSeqID)\n   throws UnsupportedEncodingException, IOException {\n     if (reconstructionLog == null || !fs.exists(reconstructionLog)) {\n-      if (reconstructionLog != null && !fs.exists(reconstructionLog)) {\n-        LOG.warn(\"Passed reconstruction log \" + reconstructionLog +\n-          \" does not exist\");\n-      }\n       // Nothing to do.\n       return;\n     }\n@@ -397,15 +397,18 @@ private void flushBloomFilter() throws IOException {\n    * Close all the MapFile readers\n    * @throws IOException\n    */\n-  void close() throws IOException {\n+  Vector<HStoreFile> close() throws IOException {\n+    Vector<HStoreFile> result = null;\n     this.lock.obtainWriteLock();\n     try {\n       for (MapFile.Reader reader: this.readers.values()) {\n         reader.close();\n       }\n       this.readers.clear();\n+      result = new Vector<HStoreFile>(storefiles.values());\n       this.storefiles.clear();\n       LOG.info(\"closed \" + this.storeName);\n+      return result;\n     } finally {\n       this.lock.releaseWriteLock();\n     }\n@@ -428,16 +431,15 @@ void close() throws IOException {\n    *\n    * @param inputCache memcache to flush\n    * @param logCacheFlushId flush sequence number\n-   * @return Vector of all the HStoreFiles in use\n    * @throws IOException\n    */\n-  Vector<HStoreFile> flushCache(final TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCache(final TreeMap<HStoreKey, byte []> inputCache,\n     final long logCacheFlushId)\n   throws IOException {\n-    return flushCacheHelper(inputCache, logCacheFlushId, true);\n+    flushCacheHelper(inputCache, logCacheFlushId, true);\n   }\n   \n-  Vector<HStoreFile> flushCacheHelper(TreeMap<HStoreKey, byte []> inputCache,\n+  void flushCacheHelper(TreeMap<HStoreKey, byte []> inputCache,\n       long logCacheFlushId, boolean addToAvailableMaps)\n   throws IOException {\n     synchronized(flushLock) {\n@@ -447,12 +449,31 @@ void close() throws IOException {\n       String name = flushedFile.toString();\n       MapFile.Writer out = flushedFile.getWriter(this.fs, this.compression,\n         this.bloomFilter);\n+      \n+      // hbase.hstore.compact.on.flush=true enables picking up an existing\n+      // HStoreFIle from disk interlacing the memcache flush compacting as we\n+      // go.  The notion is that interlacing would take as long as a pure\n+      // flush with the added benefit of having one less file in the store. \n+      // Experiments show that it takes two to three times the amount of time\n+      // flushing -- more column families makes it so the two timings come\n+      // closer together -- but it also complicates the flush. Disabled for\n+      // now.  Needs work picking which file to interlace (favor references\n+      // first, etc.)\n+      //\n+      // Related, looks like 'merging compactions' in BigTable paper interlaces\n+      // a memcache flush.  We don't.\n       try {\n-        for (Map.Entry<HStoreKey, byte []> es: inputCache.entrySet()) {\n-          HStoreKey curkey = es.getKey();\n-          if (this.familyName.\n-              equals(HStoreKey.extractFamily(curkey.getColumn()))) {\n-            out.append(curkey, new ImmutableBytesWritable(es.getValue()));\n+        if (this.conf.getBoolean(\"hbase.hstore.compact.on.flush\", false) &&\n+            this.storefiles.size() > 0) {\n+          compact(out, inputCache.entrySet().iterator(),\n+              this.readers.get(this.storefiles.firstKey()));\n+        } else {\n+          for (Map.Entry<HStoreKey, byte []> es: inputCache.entrySet()) {\n+            HStoreKey curkey = es.getKey();\n+            if (this.familyName.\n+                equals(HStoreKey.extractFamily(curkey.getColumn()))) {\n+              out.append(curkey, new ImmutableBytesWritable(es.getValue()));\n+            }\n           }\n         }\n       } finally {\n@@ -486,14 +507,14 @@ void close() throws IOException {\n           this.lock.releaseWriteLock();\n         }\n       }\n-      return getAllMapFiles();\n+      return;\n     }\n   }\n \n   /**\n    * @return - vector of all the HStore files in use\n    */\n-  Vector<HStoreFile> getAllMapFiles() {\n+  Vector<HStoreFile> getAllStoreFiles() {\n     this.lock.obtainReadLock();\n     try {\n       return new Vector<HStoreFile>(storefiles.values());\n@@ -505,6 +526,14 @@ void close() throws IOException {\n   //////////////////////////////////////////////////////////////////////////////\n   // Compaction\n   //////////////////////////////////////////////////////////////////////////////\n+  \n+  /**\n+   * @return True if this store needs compaction.\n+   */\n+  public boolean needsCompaction() {\n+    return this.storefiles != null &&\n+    this.storefiles.size() >= this.compactionThreshold;\n+  }\n \n   /**\n    * Compact the back-HStores.  This method may take some time, so the calling \n@@ -528,11 +557,24 @@ void compact() throws IOException {\n     compactHelper(false);\n   }\n   \n-  void compactHelper(boolean deleteSequenceInfo) throws IOException {\n+  void compactHelper(final boolean deleteSequenceInfo)\n+  throws IOException {\n+    compactHelper(deleteSequenceInfo, -1);\n+  }\n+  \n+  /* \n+   * @param deleteSequenceInfo True if we are to set the sequence number to -1\n+   * on compacted file.\n+   * @param maxSeenSeqID We may have already calculated the maxSeenSeqID.  If\n+   * so, pass it here.  Otherwise, pass -1 and it will be calculated inside in\n+   * this method.\n+   * @throws IOException\n+   */\n+  void compactHelper(final boolean deleteSequenceInfo, long maxSeenSeqID)\n+  throws IOException {\n     synchronized(compactLock) {\n       Path curCompactStore =\n         HStoreFile.getHStoreDir(compactdir, regionName, familyName);\n-      fs.mkdirs(curCompactStore);\n       if(LOG.isDebugEnabled()) {\n         LOG.debug(\"started compaction of \" + storefiles.size() + \" files in \" +\n           curCompactStore.toString());\n@@ -547,28 +589,32 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n           this.lock.releaseWriteLock();\n         }\n \n-        // Compute the max-sequenceID seen in any of the to-be-compacted\n-        // TreeMaps\n-        long maxSeenSeqID = -1;\n-        for (HStoreFile hsf: toCompactFiles) {\n-          long seqid = hsf.loadInfo(fs);\n-          if(seqid > 0) {\n-            if(seqid > maxSeenSeqID) {\n-              maxSeenSeqID = seqid;\n-            }\n-          }\n-        }\n-\n-        HStoreFile compactedOutputFile \n-          = new HStoreFile(conf, compactdir, regionName, familyName, -1);\n-        if(toCompactFiles.size() == 1) {\n-          // TODO: Only rewrite if NOT a HSF reference file.\n-          if(LOG.isDebugEnabled()) {\n+        HStoreFile compactedOutputFile =\n+          new HStoreFile(conf, compactdir, regionName, familyName, -1);\n+        if (toCompactFiles.size() < 1 ||\n+            (toCompactFiles.size() == 1 &&\n+              !toCompactFiles.get(0).isReference())) {\n+          if (LOG.isDebugEnabled()) {\n             LOG.debug(\"nothing to compact for \" + this.storeName);\n           }\n-          HStoreFile hsf = toCompactFiles.elementAt(0);\n-          if(hsf.loadInfo(fs) == -1) {\n-            return;\n+          if (deleteSequenceInfo && toCompactFiles.size() == 1) {\n+            toCompactFiles.get(0).writeInfo(fs, -1);\n+          }\n+          return;\n+        }\n+        \n+        fs.mkdirs(curCompactStore);\n+        \n+        // Compute the max-sequenceID seen in any of the to-be-compacted\n+        // TreeMaps if it hasn't been passed in to us.\n+        if (maxSeenSeqID == -1) {\n+          for (HStoreFile hsf: toCompactFiles) {\n+            long seqid = hsf.loadInfo(fs);\n+            if(seqid > 0) {\n+              if(seqid > maxSeenSeqID) {\n+                maxSeenSeqID = seqid;\n+              }\n+            }\n           }\n         }\n \n@@ -577,108 +623,11 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n           compactedOutputFile.getWriter(this.fs, this.compression,\n             this.bloomFilter);\n         try {\n-          // We create a new set of MapFile.Reader objects so we don't screw up \n-          // the caching associated with the currently-loaded ones.\n-          //\n-          // Our iteration-based access pattern is practically designed to ruin \n-          // the cache.\n-          //\n-          // We work by opening a single MapFile.Reader for each file, and \n-          // iterating through them in parallel.  We always increment the \n-          // lowest-ranked one.  Updates to a single row/column will appear \n-          // ranked by timestamp.  This allows us to throw out deleted values or\n-          // obsolete versions.\n-          MapFile.Reader[] rdrs = new MapFile.Reader[toCompactFiles.size()];\n-          HStoreKey[] keys = new HStoreKey[toCompactFiles.size()];\n-          ImmutableBytesWritable[] vals =\n-            new ImmutableBytesWritable[toCompactFiles.size()];\n-          boolean[] done = new boolean[toCompactFiles.size()];\n-          int pos = 0;\n-          for(HStoreFile hsf: toCompactFiles) {\n-            rdrs[pos] = hsf.getReader(this.fs, this.bloomFilter);\n-            keys[pos] = new HStoreKey();\n-            vals[pos] = new ImmutableBytesWritable();\n-            done[pos] = false;\n-            pos++;\n-          }\n-\n-          // Now, advance through the readers in order.  This will have the\n-          // effect of a run-time sort of the entire dataset.\n-          int numDone = 0;\n-          for(int i = 0; i < rdrs.length; i++) {\n-            rdrs[i].reset();\n-            done[i] = ! rdrs[i].next(keys[i], vals[i]);\n-            if(done[i]) {\n-              numDone++;\n-            }\n-          }\n-          \n-          int timesSeen = 0;\n-          Text lastRow = new Text();\n-          Text lastColumn = new Text();\n-          while(numDone < done.length) {\n-            // Find the reader with the smallest key\n-            int smallestKey = -1;\n-            for(int i = 0; i < rdrs.length; i++) {\n-              if(done[i]) {\n-                continue;\n-              }\n-              \n-              if(smallestKey < 0) {\n-                smallestKey = i;\n-              } else {\n-                if(keys[i].compareTo(keys[smallestKey]) < 0) {\n-                  smallestKey = i;\n-                }\n-              }\n-            }\n-\n-            // Reflect the current key/val in the output\n-            HStoreKey sk = keys[smallestKey];\n-            if(lastRow.equals(sk.getRow())\n-                && lastColumn.equals(sk.getColumn())) {\n-              timesSeen++;\n-            } else {\n-              timesSeen = 1;\n-            }\n-            \n-            if(timesSeen <= family.getMaxVersions()) {\n-              // Keep old versions until we have maxVersions worth.\n-              // Then just skip them.\n-              if(sk.getRow().getLength() != 0\n-                  && sk.getColumn().getLength() != 0) {\n-                // Only write out objects which have a non-zero length key and\n-                // value\n-                compactedOut.append(sk, vals[smallestKey]);\n-              }\n-            }\n-\n-            // TODO: I don't know what to do about deleted values.  I currently \n-            // include the fact that the item was deleted as a legitimate \n-            // \"version\" of the data.  Maybe it should just drop the deleted\n-            // val?\n-\n-            // Update last-seen items\n-            lastRow.set(sk.getRow());\n-            lastColumn.set(sk.getColumn());\n-\n-            // Advance the smallest key.  If that reader's all finished, then \n-            // mark it as done.\n-            if(! rdrs[smallestKey].next(keys[smallestKey],\n-                vals[smallestKey])) {\n-              done[smallestKey] = true;\n-              rdrs[smallestKey].close();\n-              numDone++;\n-            }\n-          }\n+          compact(compactedOut, toCompactFiles);\n         } finally {\n           compactedOut.close();\n         }\n \n-        if(LOG.isDebugEnabled()) {\n-          LOG.debug(\"writing new compacted HStore \" + compactedOutputFile);\n-        }\n-\n         // Now, write out an HSTORE_LOGINFOFILE for the brand-new TreeMap.\n         if((! deleteSequenceInfo) && maxSeenSeqID >= 0) {\n           compactedOutputFile.writeInfo(fs, maxSeenSeqID);\n@@ -691,8 +640,7 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n         DataOutputStream out = new DataOutputStream(fs.create(filesToReplace));\n         try {\n           out.writeInt(toCompactFiles.size());\n-          for(Iterator<HStoreFile> it = toCompactFiles.iterator(); it.hasNext(); ) {\n-            HStoreFile hsf = it.next();\n+          for(HStoreFile hsf: toCompactFiles) {\n             hsf.write(out);\n           }\n         } finally {\n@@ -706,7 +654,207 @@ void compactHelper(boolean deleteSequenceInfo) throws IOException {\n         // Move the compaction into place.\n         processReadyCompaction();\n       } finally {\n-        fs.delete(compactdir);\n+        if (fs.exists(compactdir)) {\n+          fs.delete(compactdir);\n+        }\n+      }\n+    }\n+  }\n+  \n+  /*\n+   * Compact passed <code>toCompactFiles</code> into <code>compactedOut</code>. \n+   * We create a new set of MapFile.Reader objects so we don't screw up \n+   * the caching associated with the currently-loaded ones. Our\n+   * iteration-based access pattern is practically designed to ruin \n+   * the cache.\n+   *\n+   * We work by opening a single MapFile.Reader for each file, and \n+   * iterating through them in parallel.  We always increment the \n+   * lowest-ranked one.  Updates to a single row/column will appear \n+   * ranked by timestamp.  This allows us to throw out deleted values or\n+   * obsolete versions.\n+   * @param compactedOut\n+   * @param toCompactFiles\n+   * @throws IOException\n+   */\n+  void compact(final MapFile.Writer compactedOut,\n+      final Vector<HStoreFile> toCompactFiles)\n+  throws IOException {\n+    int size = toCompactFiles.size();\n+    CompactionReader[] rdrs = new CompactionReader[size];\n+    int index = 0;\n+    for (HStoreFile hsf: toCompactFiles) {\n+      try {\n+        rdrs[index++] =\n+          new MapFileCompactionReader(hsf.getReader(fs, bloomFilter));\n+      } catch (IOException e) {\n+        // Add info about which file threw exception. It may not be in the\n+        // exception message so output a message here where we know the\n+        // culprit.\n+        LOG.warn(\"Failed with \" + e.toString() + \": \" + hsf.toString() +\n+          (hsf.isReference()? \" \" + hsf.getReference().toString(): \"\"));\n+        throw e;\n+      }\n+    }\n+    try {\n+      compact(compactedOut, rdrs);\n+    } finally {\n+      for (int i = 0; i < rdrs.length; i++) {\n+        if (rdrs[i] != null) {\n+          try {\n+            rdrs[i].close();\n+          } catch (IOException e) {\n+            LOG.warn(\"Exception closing reader\", e);\n+          }\n+        }\n+      }\n+    }\n+  }\n+  \n+  interface CompactionReader {\n+    public void close() throws IOException;\n+    public boolean next(WritableComparable key, Writable val)\n+      throws IOException;\n+    public void reset() throws IOException;\n+  }\n+  \n+  class MapFileCompactionReader implements CompactionReader {\n+    final MapFile.Reader reader;\n+    \n+    MapFileCompactionReader(final MapFile.Reader r) {\n+      this.reader = r;\n+    }\n+    \n+    public void close() throws IOException {\n+      this.reader.close();\n+    }\n+\n+    public boolean next(WritableComparable key, Writable val)\n+    throws IOException {\n+      return this.reader.next(key, val);\n+    }\n+\n+    public void reset() throws IOException {\n+      this.reader.reset();\n+    }\n+  }\n+  \n+  void compact(final MapFile.Writer compactedOut,\n+      final Iterator<Entry<HStoreKey, byte []>> iterator,\n+      final MapFile.Reader reader)\n+  throws IOException {\n+    // Make an instance of a CompactionReader that wraps the iterator.\n+    CompactionReader cr = new CompactionReader() {\n+      public boolean next(WritableComparable key, Writable val)\n+          throws IOException {\n+        boolean result = false;\n+        while (iterator.hasNext()) {\n+          Entry<HStoreKey, byte []> e = iterator.next();\n+          HStoreKey hsk = e.getKey();\n+          if (familyName.equals(HStoreKey.extractFamily(hsk.getColumn()))) {\n+            ((HStoreKey)key).set(hsk);\n+            ((ImmutableBytesWritable)val).set(e.getValue());\n+            result = true;\n+            break;\n+          }\n+        }\n+        return result;\n+      }\n+\n+      @SuppressWarnings(\"unused\")\n+      public void reset() throws IOException {\n+        // noop.\n+      }\n+      \n+      @SuppressWarnings(\"unused\")\n+      public void close() throws IOException {\n+        // noop.\n+      }\n+    };\n+    \n+    compact(compactedOut,\n+      new CompactionReader [] {cr, new MapFileCompactionReader(reader)});\n+  }\n+  \n+  void compact(final MapFile.Writer compactedOut,\n+      final CompactionReader [] rdrs)\n+  throws IOException {\n+    HStoreKey[] keys = new HStoreKey[rdrs.length];\n+    ImmutableBytesWritable[] vals = new ImmutableBytesWritable[rdrs.length];\n+    boolean[] done = new boolean[rdrs.length];\n+    for(int i = 0; i < rdrs.length; i++) {\n+      keys[i] = new HStoreKey();\n+      vals[i] = new ImmutableBytesWritable();\n+      done[i] = false;\n+    }\n+\n+    // Now, advance through the readers in order.  This will have the\n+    // effect of a run-time sort of the entire dataset.\n+    int numDone = 0;\n+    for(int i = 0; i < rdrs.length; i++) {\n+      rdrs[i].reset();\n+      done[i] = ! rdrs[i].next(keys[i], vals[i]);\n+      if(done[i]) {\n+        numDone++;\n+      }\n+    }\n+\n+    int timesSeen = 0;\n+    Text lastRow = new Text();\n+    Text lastColumn = new Text();\n+    while(numDone < done.length) {\n+      // Find the reader with the smallest key\n+      int smallestKey = -1;\n+      for(int i = 0; i < rdrs.length; i++) {\n+        if(done[i]) {\n+          continue;\n+        }\n+        if(smallestKey < 0) {\n+          smallestKey = i;\n+        } else {\n+          if(keys[i].compareTo(keys[smallestKey]) < 0) {\n+            smallestKey = i;\n+          }\n+        }\n+      }\n+\n+      // Reflect the current key/val in the output\n+      HStoreKey sk = keys[smallestKey];\n+      if(lastRow.equals(sk.getRow())\n+          && lastColumn.equals(sk.getColumn())) {\n+        timesSeen++;\n+      } else {\n+        timesSeen = 1;\n+      }\n+\n+      if(timesSeen <= family.getMaxVersions()) {\n+        // Keep old versions until we have maxVersions worth.\n+        // Then just skip them.\n+        if(sk.getRow().getLength() != 0\n+            && sk.getColumn().getLength() != 0) {\n+          // Only write out objects which have a non-zero length key and\n+          // value\n+          compactedOut.append(sk, vals[smallestKey]);\n+        }\n+      }\n+\n+      // TODO: I don't know what to do about deleted values.  I currently \n+      // include the fact that the item was deleted as a legitimate \n+      // \"version\" of the data.  Maybe it should just drop the deleted\n+      // val?\n+\n+      // Update last-seen items\n+      lastRow.set(sk.getRow());\n+      lastColumn.set(sk.getColumn());\n+\n+      // Advance the smallest key.  If that reader's all finished, then \n+      // mark it as done.\n+      if(!rdrs[smallestKey].next(keys[smallestKey],\n+          vals[smallestKey])) {\n+        done[smallestKey] = true;\n+        rdrs[smallestKey].close();\n+        rdrs[smallestKey] = null;\n+        numDone++;\n       }\n     }\n   }\n@@ -773,21 +921,19 @@ void processReadyCompaction() throws IOException {\n         }\n       }\n \n+      Vector<HStoreFile> toDelete = new Vector<HStoreFile>(keys.size());\n       for (Long key: keys) {\n         MapFile.Reader reader = this.readers.remove(key);\n         if (reader != null) {\n           reader.close();\n         }\n         HStoreFile hsf = this.storefiles.remove(key);\n-        // 4. Delete all old files, no longer needed\n-        hsf.delete();\n-      }\n-      if(LOG.isDebugEnabled()) {\n-        LOG.debug(\"deleted \" + toCompactFiles.size() + \" old file(s)\");\n+        // 4. Add to the toDelete files all old files, no longer needed\n+        toDelete.add(hsf);\n       }\n       \n-      // What if we fail now?  The above deletes will fail silently. We'd better\n-      // make sure not to write out any new files with the same names as \n+      // What if we fail now?  The above deletes will fail silently. We'd\n+      // better make sure not to write out any new files with the same names as \n       // something we delete, though.\n \n       // 5. Moving the new MapFile into place\n@@ -800,17 +946,30 @@ void processReadyCompaction() throws IOException {\n           compactdir.toString() +\n           \" to \" + finalCompactedFile.toString() + \" in \" + dir.toString());\n       }\n-      compactedFile.rename(this.fs, finalCompactedFile);\n+      if (!compactedFile.rename(this.fs, finalCompactedFile)) {\n+        LOG.error(\"Failed move of compacted file \" +\n+          finalCompactedFile.toString());\n+        return;\n+      }\n+      \n+      // Safe to delete now compaction has been moved into place.\n+      for (HStoreFile hsf: toDelete) {\n+        if (hsf.getFileId() == finalCompactedFile.getFileId()) {\n+          // Be careful we do not delte the just compacted file.\n+          LOG.warn(\"Weird. File to delete has same name as one we are \" +\n+            \"about to delete (skipping): \" + hsf.getFileId());\n+          continue;\n+        }\n+        hsf.delete();\n+      }\n \n-      // Fail here?  No worries.\n       Long orderVal = Long.valueOf(finalCompactedFile.loadInfo(fs));\n \n       // 6. Loading the new TreeMap.\n       this.readers.put(orderVal,\n         finalCompactedFile.getReader(this.fs, this.bloomFilter));\n       this.storefiles.put(orderVal, finalCompactedFile);\n     } finally {\n-      \n       // 7. Releasing the write-lock\n       this.lock.releaseWriteLock();\n     }\n@@ -838,6 +997,9 @@ void getFull(HStoreKey key, TreeMap<Text, byte []> results)\n           map.reset();\n           ImmutableBytesWritable readval = new ImmutableBytesWritable();\n           HStoreKey readkey = (HStoreKey)map.getClosest(key, readval);\n+          if (readkey == null) {\n+            continue;\n+          }\n           do {\n             Text readcol = readkey.getColumn();\n             if (results.get(readcol) == null\n@@ -1004,7 +1166,7 @@ HStoreSize size(Text midKey) {\n   /**\n    * @return    Returns the number of map files currently in use\n    */\n-  int getNMaps() {\n+  int countOfStoreFiles() {\n     this.lock.obtainReadLock();\n     try {\n       return storefiles.size();\n@@ -1014,6 +1176,22 @@ int getNMaps() {\n     }\n   }\n   \n+  boolean hasReferences() {\n+    boolean result = false;\n+    this.lock.obtainReadLock();\n+    try {\n+        for (HStoreFile hsf: this.storefiles.values()) {\n+          if (hsf.isReference()) {\n+            break;\n+          }\n+        }\n+      \n+    } finally {\n+      this.lock.releaseReadLock();\n+    }\n+    return result;\n+  }\n+  \n   //////////////////////////////////////////////////////////////////////////////\n   // File administration\n   //////////////////////////////////////////////////////////////////////////////\n@@ -1038,6 +1216,11 @@ HInternalScannerInterface getScanner(long timestamp, Text targetCols[],\n     \n     return new HStoreScanner(timestamp, targetCols, firstRow);\n   }\n+  \n+  @Override\n+  public String toString() {\n+    return this.storeName;\n+  }\n \n   //////////////////////////////////////////////////////////////////////////////\n   // This class implements the HScannerInterface.",
                "deletions": 163
            },
            {
                "sha": "a9e3c15c6f03180f6a4cd3ed956261f76cde39ba",
                "filename": "src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreFile.java",
                "status": "modified",
                "changes": 26,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreFile.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -399,7 +399,13 @@ static HStoreFile obtainNewHStoreFile(Configuration conf, Path dir,\n       Path mapfile = curfile.getMapFilePath();\n       if (!fs.exists(mapfile)) {\n         fs.delete(curfile.getInfoFilePath());\n+        LOG.warn(\"Mapfile \" + mapfile.toString() + \" does not exist. \" +\n+          \"Cleaned up info file.  Continuing...\");\n+        continue;\n       }\n+      \n+      // TODO: Confirm referent exists.\n+      \n       // Found map and sympathetic info file.  Add this hstorefile to result.\n       results.add(curfile);\n       // Keep list of sympathetic data mapfiles for cleaning info dir in next\n@@ -537,8 +543,7 @@ void mergeStoreFiles(Vector<HStoreFile> srcFiles, FileSystem fs,\n     \n     try {\n       for(HStoreFile src: srcFiles) {\n-        MapFile.Reader in =\n-          new MapFile.Reader(fs, src.getMapFilePath().toString(), conf);\n+        MapFile.Reader in = src.getReader(fs, null);\n         try {\n           HStoreKey readkey = new HStoreKey();\n           ImmutableBytesWritable readval = new ImmutableBytesWritable();\n@@ -627,12 +632,23 @@ private void delete(final Path p) throws IOException {\n    * <code>hsf</code> directory.\n    * @param fs\n    * @param hsf\n+   * @return True if succeeded.\n    * @throws IOException\n    */\n-  public void rename(final FileSystem fs, final HStoreFile hsf)\n+  public boolean rename(final FileSystem fs, final HStoreFile hsf)\n   throws IOException {\n-    fs.rename(getMapFilePath(), hsf.getMapFilePath());\n-    fs.rename(getInfoFilePath(), hsf.getInfoFilePath());\n+    boolean success = fs.rename(getMapFilePath(), hsf.getMapFilePath());\n+    if (!success) {\n+      LOG.warn(\"Failed rename of \" + getMapFilePath() + \" to \" +\n+        hsf.getMapFilePath());\n+      return success;\n+    }\n+    success = fs.rename(getInfoFilePath(), hsf.getInfoFilePath());\n+    if (!success) {\n+      LOG.warn(\"Failed rename of \" + getInfoFilePath() + \" to \" +\n+        hsf.getInfoFilePath());\n+    }\n+    return success;\n   }\n   \n   /**",
                "deletions": 5
            },
            {
                "sha": "452894ae1e8d6dd85a1d1614556d6b7260093da7",
                "filename": "src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/HStoreKey.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/HStoreKey.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -301,6 +301,10 @@ public int compareTo(Object o) {\n     if(result == 0) {\n       result = this.column.compareTo(other.column);\n       if(result == 0) {\n+        // The below older timestamps sorting ahead of newer timestamps looks\n+        // wrong but it is intentional.  This way, newer timestamps are first\n+        // found when we iterate over a memcache and newer versions are the\n+        // first we trip over when reading from a store file.\n         if(this.timestamp < other.timestamp) {\n           result = 1;\n         } else if(this.timestamp > other.timestamp) {",
                "deletions": 0
            },
            {
                "sha": "3f90032df5364799ed87ceb02d7048b33510458d",
                "filename": "src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java",
                "status": "modified",
                "changes": 7,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -87,6 +87,13 @@ public ImmutableBytesWritable(final byte[] newData, final int offset,\n     return this.bytes;\n   }\n   \n+  /**\n+   * @param b Use passed bytes as backing array for this instance.\n+   */\n+  public void set(final byte [] b) {\n+    this.bytes = b;\n+  }\n+  \n   /**\n    * @return the current size of the buffer.\n    */",
                "deletions": 0
            },
            {
                "sha": "0ad4598168e7b170c0c7b70f39f5cdbc07e61485",
                "filename": "src/java/org/apache/hadoop/hbase/util/Writables.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/java/org/apache/hadoop/hbase/util/Writables.java",
                "status": "modified",
                "changes": 9,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/java/org/apache/hadoop/hbase/util/Writables.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -66,15 +66,16 @@\n    * @param w An empty Writable (usually made by calling the null-arg\n    * constructor).\n    * @return The passed Writable after its readFields has been called fed\n-   * by the passed <code>bytes</code> array or null if passed null or\n-   * empty <code>bytes</code>.\n+   * by the passed <code>bytes</code> array or IllegalArgumentException\n+   * if passed null or an empty <code>bytes</code> array.\n    * @throws IOException\n+   * @throws IllegalArgumentException\n    */\n   public static Writable getWritable(final byte [] bytes, final Writable w)\n   throws IOException {\n     if (bytes == null || bytes.length == 0) {\n-      throw new IllegalArgumentException(\n-          \"Con't build a writable with empty bytes array\");\n+      throw new IllegalArgumentException(\"Can't build a writable with empty \" +\n+        \"bytes array\");\n     }\n     if (w == null) {\n       throw new IllegalArgumentException(\"Writable cannot be null\");",
                "deletions": 4
            },
            {
                "sha": "27218d361850436192ab6c2bc519600dacb1c830",
                "filename": "src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/HBaseTestCase.java",
                "status": "modified",
                "changes": 139,
                "additions": 139,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/HBaseTestCase.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -32,6 +32,14 @@\n  * Abstract base class for test cases. Performs all static initialization\n  */\n public abstract class HBaseTestCase extends TestCase {\n+  public final static String COLFAMILY_NAME1 = \"colfamily1:\";\n+  public final static String COLFAMILY_NAME2 = \"colfamily2:\";\n+  public final static String COLFAMILY_NAME3 = \"colfamily3:\";\n+  protected Path testDir = null;\n+  protected FileSystem localFs = null;\n+  public static final char FIRST_CHAR = 'a';\n+  public static final char LAST_CHAR = 'z';\n+  \n   static {\n     StaticTestEnvironment.initialize();\n   }\n@@ -47,6 +55,29 @@ protected HBaseTestCase(String name) {\n     super(name);\n     conf = new HBaseConfiguration();\n   }\n+  \n+  @Override\n+  protected void setUp() throws Exception {\n+    super.setUp();\n+    this.testDir = getUnitTestdir(getName());\n+    this.localFs = FileSystem.getLocal(this.conf);\n+    if (localFs.exists(testDir)) {\n+      localFs.delete(testDir);\n+    }\n+  }\n+  \n+  @Override\n+  protected void tearDown() throws Exception {\n+    try {\n+      if (this.localFs != null && this.testDir != null &&\n+          this.localFs.exists(testDir)) {\n+        this.localFs.delete(testDir);\n+      }\n+    } catch (Exception e) {\n+      e.printStackTrace();\n+    }\n+    super.tearDown();\n+  }\n \n   protected Path getUnitTestdir(String testName) {\n     return new Path(StaticTestEnvironment.TEST_DIRECTORY_KEY, testName);\n@@ -63,4 +94,112 @@ protected HRegion createNewHRegion(Path dir, Configuration c,\n       new HLog(fs, new Path(regionDir, HConstants.HREGION_LOGDIR_NAME), conf),\n       fs, conf, info, null);\n   }\n+  \n+  protected HTableDescriptor createTableDescriptor(final String name) {\n+    HTableDescriptor htd = new HTableDescriptor(name);\n+    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME1));\n+    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME2));\n+    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME3));\n+    return htd;\n+  }\n+  \n+  protected void addContent(final HRegion r, final String column)\n+  throws IOException {\n+    Text startKey = r.getRegionInfo().getStartKey();\n+    Text endKey = r.getRegionInfo().getEndKey();\n+    byte [] startKeyBytes = startKey.getBytes();\n+    if (startKeyBytes == null || startKeyBytes.length == 0) {\n+      startKeyBytes = new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR};\n+    }\n+    addContent(new HRegionLoader(r), column, startKeyBytes, endKey);\n+  }\n+  \n+  protected void addContent(final Loader updater, final String column)\n+  throws IOException {\n+    addContent(updater, column,\n+      new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR}, null);\n+  }\n+  \n+  protected void addContent(final Loader updater, final String column,\n+      final byte [] startKeyBytes, final Text endKey)\n+  throws IOException {\n+    // Add rows of three characters.  The first character starts with the\n+    // 'a' character and runs up to 'z'.  Per first character, we run the\n+    // second character over same range.  And same for the third so rows\n+    // (and values) look like this: 'aaa', 'aab', 'aac', etc.\n+    char secondCharStart = (char)startKeyBytes[1];\n+    char thirdCharStart = (char)startKeyBytes[2];\n+    EXIT: for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n+      for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n+        for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n+          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n+          Text t = new Text(new String(bytes));\n+          if (endKey != null && endKey.getLength() > 0\n+              && endKey.compareTo(t) <= 0) {\n+            break EXIT;\n+          }\n+          long lockid = updater.startBatchUpdate(t);\n+          try {\n+            updater.put(lockid, new Text(column), bytes);\n+            updater.commit(lockid);\n+            lockid = -1;\n+          } finally {\n+            if (lockid != -1) {\n+              updater.abort(lockid);\n+            }\n+          }\n+        }\n+        // Set start character back to FIRST_CHAR after we've done first loop.\n+        thirdCharStart = FIRST_CHAR;\n+      }\n+      secondCharStart = FIRST_CHAR;\n+    }\n+  }\n+  \n+  public interface Loader {\n+    public long startBatchUpdate(final Text row) throws IOException;\n+    public void put(long lockid, Text column, byte val[]) throws IOException;\n+    public void commit(long lockid) throws IOException;\n+    public void abort(long lockid) throws IOException;\n+  }\n+  \n+  public class HRegionLoader implements Loader {\n+    final HRegion region;\n+    public HRegionLoader(final HRegion HRegion) {\n+      super();\n+      this.region = HRegion;\n+    }\n+    public void abort(long lockid) throws IOException {\n+      this.region.abort(lockid);\n+    }\n+    public void commit(long lockid) throws IOException {\n+      this.region.commit(lockid, System.currentTimeMillis());\n+    }\n+    public void put(long lockid, Text column, byte[] val) throws IOException {\n+      this.region.put(lockid, column, val);\n+    }\n+    public long startBatchUpdate(Text row) throws IOException {\n+      return this.region.startUpdate(row);\n+    }\n+  }\n+  \n+  public class HTableLoader implements Loader {\n+    final HTable table;\n+    public HTableLoader(final HTable table) {\n+      super();\n+      this.table = table;\n+    }\n+    public void abort(long lockid) throws IOException {\n+      this.table.abort(lockid);\n+    }\n+    public void commit(long lockid) throws IOException {\n+      this.table.commit(lockid);\n+    }\n+    public void put(long lockid, Text column, byte[] val) throws IOException {\n+      this.table.put(lockid, column, val);\n+    }\n+    public long startBatchUpdate(Text row) {\n+      return this.table.startBatchUpdate(row);\n+    }\n+  }\n }\n\\ No newline at end of file",
                "deletions": 0
            },
            {
                "sha": "d1902f72ed96c7a6111830ef1de5e047deb5633e",
                "filename": "src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -365,7 +365,9 @@ void shutdown() {\n     shutdown(this.masterThread, this.regionThreads);\n     // Close the file system.  Will complain if files open so helps w/ leaks.\n     try {\n-      this.cluster.getFileSystem().close();\n+      if (this.cluster != null && this.cluster.getFileSystem() != null) {\n+        this.cluster.getFileSystem().close();\n+      }\n     } catch (IOException e) {\n       LOG.error(\"Closing down dfs\", e);\n     }",
                "deletions": 1
            },
            {
                "sha": "caa68ebbd9a46c4c169512e4e09df51aaec9b802",
                "filename": "src/test/org/apache/hadoop/hbase/TestCompaction.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompaction.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompaction.java",
                "status": "added",
                "changes": 101,
                "additions": 101,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestCompaction.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -0,0 +1,101 @@\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase;\n+\n+import java.io.IOException;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+\n+/**\n+ * Test compactions\n+ */\n+public class TestCompaction extends HBaseTestCase {\n+  static final Log LOG = LogFactory.getLog(TestCompaction.class.getName());\n+\n+  protected void setUp() throws Exception {\n+    super.setUp();\n+  }\n+  \n+  protected void tearDown() throws Exception {\n+    super.tearDown();\n+  }\n+  \n+  /**\n+   * Run compaction and flushing memcache\n+   * @throws Exception\n+   */\n+  public void testCompaction() throws Exception {\n+    HLog hlog = new HLog(this.localFs, this.testDir, this.conf);\n+    HTableDescriptor htd = createTableDescriptor(getName());\n+    HRegionInfo hri = new HRegionInfo(1, htd, null, null);\n+    final HRegion r =\n+      new HRegion(testDir, hlog, this.localFs, this.conf, hri, null);\n+    try {\n+      createStoreFile(r);\n+      assertFalse(r.needsCompaction());\n+      int compactionThreshold =\n+        this.conf.getInt(\"hbase.hstore.compactionThreshold\", 3);\n+      for (int i = 0; i < compactionThreshold; i++) {\n+        createStoreFile(r);\n+      }\n+      assertTrue(r.needsCompaction());\n+      // Try to run compaction concurrent with a thread flush.\n+      addContent(new HRegionLoader(r), COLFAMILY_NAME1);\n+      Thread t1 = new Thread() {\n+        @Override\n+        public void run() {\n+          try {\n+            r.flushcache(false);\n+          } catch (IOException e) {\n+            e.printStackTrace();\n+          }\n+        }\n+      };\n+      Thread t2 = new Thread() {\n+        @Override\n+        public void run() {\n+          try {\n+            assertTrue(r.compactStores());\n+          } catch (IOException e) {\n+            e.printStackTrace();\n+          }\n+        }\n+      };\n+      t1.setDaemon(true);\n+      t1.start();\n+      t2.setDaemon(true);\n+      t2.start();\n+      t1.join();\n+      t2.join();\n+    } finally {\n+      r.close();\n+      hlog.closeAndDelete();\n+    }\n+  }\n+  \n+  private void createStoreFile(final HRegion r) throws IOException {\n+    HRegionLoader loader = new HRegionLoader(r);\n+    for (int i = 0; i < 3; i++) {\n+      addContent(loader, COLFAMILY_NAME1);\n+    }\n+    r.flushcache(false);\n+  }\n+}\n\\ No newline at end of file",
                "deletions": 0
            },
            {
                "sha": "0bd3c1dfc2e0135b09680cc3872c4427e20325db",
                "filename": "src/test/org/apache/hadoop/hbase/TestCompare.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestCompare.java",
                "status": "modified",
                "changes": 22,
                "additions": 21,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestCompare.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -26,7 +26,27 @@\n  * Test comparing HBase objects.\n  */\n public class TestCompare extends TestCase {\n-  /** test case */\n+  \n+  /**\n+   * HStoreKey sorts as you would expect in the row and column portions but\n+   * for the timestamps, it sorts in reverse with the newest sorting before\n+   * the oldest (This is intentional so we trip over the latest first when\n+   * iterating or looking in store files).\n+   */\n+  public void testHStoreKey() {\n+    long timestamp = System.currentTimeMillis();\n+    Text a = new Text(\"a\");\n+    HStoreKey past = new HStoreKey(a, a, timestamp - 10);\n+    HStoreKey now = new HStoreKey(a, a, timestamp);\n+    HStoreKey future = new HStoreKey(a, a, timestamp + 10);\n+    assertTrue(past.compareTo(now) > 0);\n+    assertTrue(now.compareTo(now) == 0);\n+    assertTrue(future.compareTo(now) < 0);\n+  }\n+  \n+  /**\n+   * Sort of HRegionInfo.\n+   */\n   public void testHRegionInfo() {\n     HRegionInfo a = new HRegionInfo(1, new HTableDescriptor(\"a\"), null, null);\n     HRegionInfo b = new HRegionInfo(2, new HTableDescriptor(\"b\"), null, null);",
                "deletions": 1
            },
            {
                "sha": "7f5819e237c666fe4ceac057c700a26bba960f81",
                "filename": "src/test/org/apache/hadoop/hbase/TestSplit.java",
                "blob_url": "https://github.com/apache/hbase/blob/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "raw_url": "https://github.com/apache/hbase/raw/be33a241ce1a2926fb72a961b8d02213057d14a8/src/test/org/apache/hadoop/hbase/TestSplit.java",
                "status": "modified",
                "changes": 142,
                "additions": 16,
                "contents_url": "https://api.github.com/repos/apache/hbase/contents/src/test/org/apache/hadoop/hbase/TestSplit.java?ref=be33a241ce1a2926fb72a961b8d02213057d14a8",
                "patch": "@@ -38,61 +38,35 @@\n  * split and manufactures odd-ball split scenarios.\n  */\n public class TestSplit extends HBaseTestCase {\n-  static final Log LOG = LogFactory.getLog(TestSplit.class);\n-  private final static String COLFAMILY_NAME1 = \"colfamily1:\";\n-  private final static String COLFAMILY_NAME2 = \"colfamily2:\";\n-  private final static String COLFAMILY_NAME3 = \"colfamily3:\";\n-  private Path testDir = null;\n-  private FileSystem fs = null;\n-  private static final char FIRST_CHAR = 'a';\n-  private static final char LAST_CHAR = 'z';\n-\n+  static final Log LOG = LogFactory.getLog(TestSplit.class.getName());\n+  \n   /** constructor */\n   public TestSplit() {\n     Logger.getRootLogger().setLevel(Level.WARN);\n-    Logger.getLogger(this.getClass().getPackage().getName()).setLevel(Level.DEBUG);\n+    Logger.getLogger(this.getClass().getPackage().getName()).\n+      setLevel(Level.DEBUG);\n   }\n \n   /** {@inheritDoc} */\n   @Override\n   public void setUp() throws Exception {\n     super.setUp();\n-    this.testDir = getUnitTestdir(getName());\n-    this.fs = FileSystem.getLocal(this.conf);\n-    if (fs.exists(testDir)) {\n-      fs.delete(testDir);\n-    }\n     // This size should make it so we always split using the addContent\n     // below.  After adding all data, the first region is 1.3M\n     conf.setLong(\"hbase.hregion.max.filesize\", 1024 * 128);\n   }\n   \n-  /** {@inheritDoc} */\n-  @Override\n-  public void tearDown() throws Exception {\n-    if (fs != null) {\n-      try {\n-        if (this.fs.exists(testDir)) {\n-          this.fs.delete(testDir);\n-        }\n-      } catch (Exception e) {\n-        e.printStackTrace();\n-      }\n-    }\n-    super.tearDown();\n-  }\n-  \n   /**\n    * Splits twice and verifies getting from each of the split regions.\n    * @throws Exception\n    */\n   public void testBasicSplit() throws Exception {\n     HRegion region = null;\n-    HLog hlog = new HLog(this.fs, this.testDir, this.conf);\n+    HLog hlog = new HLog(this.localFs, this.testDir, this.conf);\n     try {\n       HTableDescriptor htd = createTableDescriptor(getName());\n       HRegionInfo hri = new HRegionInfo(1, htd, null, null);\n-      region = new HRegion(testDir, hlog, fs, this.conf, hri, null);\n+      region = new HRegion(testDir, hlog, this.localFs, this.conf, hri, null);\n       basicSplit(region);\n     } finally {\n       if (region != null) {\n@@ -102,14 +76,6 @@ public void testBasicSplit() throws Exception {\n     }\n   }\n   \n-  private HTableDescriptor createTableDescriptor(final String name) {\n-    HTableDescriptor htd = new HTableDescriptor(name);\n-    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME1));\n-    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME2));\n-    htd.addFamily(new HColumnDescriptor(COLFAMILY_NAME3));\n-    return htd;\n-  }\n-  \n   private void basicSplit(final HRegion region) throws Exception {\n     addContent(region, COLFAMILY_NAME3);\n     region.internalFlushcache();\n@@ -184,13 +150,13 @@ private void basicSplit(final HRegion region) throws Exception {\n    * @throws Exception\n    */\n   public void testSplitRegionIsDeleted() throws Exception {\n-    final int retries = 10;\n-    this.testDir = null;\n-    this.fs = null;\n+    final int retries = 10; \n     // Start up a hbase cluster\n-    MiniHBaseCluster cluster = new MiniHBaseCluster(conf, 1);\n-    Path testDir = cluster.regionThreads.get(0).getRegionServer().rootDir;\n-    FileSystem fs = cluster.getDFSCluster().getFileSystem();\n+    MiniHBaseCluster cluster = new MiniHBaseCluster(conf, 1, true);\n+    Path d = cluster.regionThreads.get(0).getRegionServer().rootDir;\n+    FileSystem fs = (cluster.getDFSCluster() == null)?\n+      this.localFs:\n+      cluster.getDFSCluster().getFileSystem();\n     HTable meta = null;\n     HTable t = null;\n     try {\n@@ -201,7 +167,7 @@ public void testSplitRegionIsDeleted() throws Exception {\n       meta = new HTable(this.conf, HConstants.META_TABLE_NAME);\n       int count = count(meta, HConstants.COLUMN_FAMILY_STR);\n       t = new HTable(this.conf, new Text(getName()));\n-      addContent(t, COLFAMILY_NAME3);\n+      addContent(new HTableLoader(t), COLFAMILY_NAME3);\n       // All is running in the one JVM so I should be able to get the\n       // region instance and bring on a split.\n       HRegionInfo hri =\n@@ -223,8 +189,7 @@ public void testSplitRegionIsDeleted() throws Exception {\n       }\n       HRegionInfo parent = getSplitParent(meta);\n       assertTrue(parent.isOffline());\n-      Path parentDir =\n-        HRegion.getRegionDir(testDir, parent.getRegionName());\n+      Path parentDir = HRegion.getRegionDir(d, parent.getRegionName());\n       assertTrue(fs.exists(parentDir));\n       LOG.info(\"Split happened and parent \" + parent.getRegionName() + \" is \" +\n       \"offline\");\n@@ -263,7 +228,7 @@ public void testSplitRegionIsDeleted() throws Exception {\n         for (int i = 0; i < 10; i++) {\n           try {\n             for (HRegion online: regions.values()) {\n-              if (online.getRegionName().toString().startsWith(getName())) {\n+              if (online.getTableDesc().getName().toString().equals(getName())) {\n                 online.compactStores();\n               }\n             }\n@@ -403,79 +368,4 @@ private void assertScan(final HRegion r, final String column,\n     assertEquals(regions.length, 2);\n     return regions;\n   }\n-  \n-  private void addContent(final HRegion r, final String column)\n-  throws IOException {\n-    Text startKey = r.getRegionInfo().getStartKey();\n-    Text endKey = r.getRegionInfo().getEndKey();\n-    byte [] startKeyBytes = startKey.getBytes();\n-    if (startKeyBytes == null || startKeyBytes.length == 0) {\n-      startKeyBytes = new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR};\n-    }\n-    // Add rows of three characters.  The first character starts with the\n-    // 'a' character and runs up to 'z'.  Per first character, we run the\n-    // second character over same range.  And same for the third so rows\n-    // (and values) look like this: 'aaa', 'aab', 'aac', etc.\n-    char secondCharStart = (char)startKeyBytes[1];\n-    char thirdCharStart = (char)startKeyBytes[2];\n-    EXIT_ALL_LOOPS: for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n-      for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n-        for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n-          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n-          Text t = new Text(new String(bytes));\n-          if (endKey != null && endKey.getLength() > 0\n-              && endKey.compareTo(t) <= 0) {\n-            break EXIT_ALL_LOOPS;\n-          }\n-          long lockid = r.startUpdate(t);\n-          try {\n-            r.put(lockid, new Text(column), bytes);\n-            r.commit(lockid, System.currentTimeMillis());\n-            lockid = -1;\n-          } finally {\n-            if (lockid != -1) {\n-              r.abort(lockid);\n-            }\n-          }\n-        }\n-        // Set start character back to FIRST_CHAR after we've done first loop.\n-        thirdCharStart = FIRST_CHAR;\n-      }\n-      secondCharStart = FIRST_CHAR;\n-    }\n-  }\n-  \n-  // TODO: Have HTable and HRegion implement interface that has in it\n-  // startUpdate, put, delete, commit, abort, etc.\n-  private void addContent(final HTable table, final String column)\n-  throws IOException {\n-    byte [] startKeyBytes = new byte [] {FIRST_CHAR, FIRST_CHAR, FIRST_CHAR};\n-    // Add rows of three characters.  The first character starts with the\n-    // 'a' character and runs up to 'z'.  Per first character, we run the\n-    // second character over same range.  And same for the third so rows\n-    // (and values) look like this: 'aaa', 'aab', 'aac', etc.\n-    char secondCharStart = (char)startKeyBytes[1];\n-    char thirdCharStart = (char)startKeyBytes[2];\n-    for (char c = (char)startKeyBytes[0]; c <= LAST_CHAR; c++) {\n-      for (char d = secondCharStart; d <= LAST_CHAR; d++) {\n-        for (char e = thirdCharStart; e <= LAST_CHAR; e++) {\n-          byte [] bytes = new byte [] {(byte)c, (byte)d, (byte)e};\n-          Text t = new Text(new String(bytes));\n-          long lockid = table.startUpdate(t);\n-          try {\n-            table.put(lockid, new Text(column), bytes);\n-            table.commit(lockid, System.currentTimeMillis());\n-            lockid = -1;\n-          } finally {\n-            if (lockid != -1) {\n-              table.abort(lockid);\n-            }\n-          }\n-        }\n-        // Set start character back to FIRST_CHAR after we've done first loop.\n-        thirdCharStart = FIRST_CHAR;\n-      }\n-      secondCharStart = FIRST_CHAR;\n-    }\n-  }\n-}\n\\ No newline at end of file\n+}",
                "deletions": 126
            }
        ]
    }
]