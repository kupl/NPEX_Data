{
    "asterixdb_02b35a1": {
        "bug_id": "asterixdb_02b35a1",
        "commit": "https://github.com/apache/asterixdb/commit/02b35a1bf98f4168dae91fa68c14d7e062bfd032",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/main/java/org/apache/asterix/file/DatasetOperations.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-app/src/main/java/org/apache/asterix/file/DatasetOperations.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 7,
                "filename": "asterix-app/src/main/java/org/apache/asterix/file/DatasetOperations.java",
                "patch": "@@ -69,7 +69,7 @@\n \n     public static JobSpecification createDropDatasetJobSpec(CompiledDatasetDropStatement datasetDropStmt,\n             AqlMetadataProvider metadataProvider)\n-                    throws AlgebricksException, HyracksDataException, RemoteException, ACIDException, AsterixException {\n+            throws AlgebricksException, HyracksDataException, RemoteException, ACIDException, AsterixException {\n \n         String dataverseName = null;\n         if (datasetDropStmt.getDataverseName() != null) {\n@@ -222,26 +222,22 @@ public static JobSpecification compactDatasetJobSpec(Dataverse dataverse, String\n             throw new AsterixException(\"Could not find dataset \" + datasetName + \" in dataverse \" + dataverseName);\n         }\n         boolean temp = dataset.getDatasetDetails().isTemp();\n-\n         ARecordType itemType = (ARecordType) metadata.findType(dataset.getItemTypeDataverseName(),\n                 dataset.getItemTypeName());\n+        ARecordType metaItemType = DatasetUtils.getMetaType(metadata, dataset);\n         JobSpecification spec = JobSpecificationUtils.createJobSpecification();\n         IBinaryComparatorFactory[] comparatorFactories = DatasetUtils.computeKeysBinaryComparatorFactories(dataset,\n                 itemType, format.getBinaryComparatorFactoryProvider());\n-        ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType);\n+        ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType, metaItemType);\n         int[] blooFilterKeyFields = DatasetUtils.createBloomFilterKeyFields(dataset);\n-\n         ITypeTraits[] filterTypeTraits = DatasetUtils.computeFilterTypeTraits(dataset, itemType);\n         IBinaryComparatorFactory[] filterCmpFactories = DatasetUtils.computeFilterBinaryComparatorFactories(dataset,\n                 itemType, format.getBinaryComparatorFactoryProvider());\n         int[] filterFields = DatasetUtils.createFilterFields(dataset);\n         int[] btreeFields = DatasetUtils.createBTreeFieldsWhenThereisAFilter(dataset);\n-\n         Pair<IFileSplitProvider, AlgebricksPartitionConstraint> splitsAndConstraint = metadata\n                 .splitProviderAndPartitionConstraintsForDataset(dataverseName, datasetName, datasetName, temp);\n-\n         AsterixStorageProperties storageProperties = AsterixAppContextInfo.getInstance().getStorageProperties();\n-\n         Pair<ILSMMergePolicyFactory, Map<String, String>> compactionInfo = DatasetUtils.getMergePolicyFactory(dataset,\n                 metadata.getMetadataTxnContext());\n         LSMTreeIndexCompactOperatorDescriptor compactOp = new LSMTreeIndexCompactOperatorDescriptor(spec,",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/main/java/org/apache/asterix/file/DatasetOperations.java",
                "sha": "905269600204bc754fa1a825f9530a833fc4cc20",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTest.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 4,
                "filename": "asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTest.java",
                "patch": "@@ -48,9 +48,10 @@\n             File.separator);\n \n     protected static final String TEST_CONFIG_FILE_NAME = \"asterix-build-configuration.xml\";\n-\n     protected static AsterixTransactionProperties txnProperties;\n-    private final static TestExecutor testExecutor = new TestExecutor();\n+    private static final TestExecutor testExecutor = new TestExecutor();\n+    private static final boolean cleanupOnStart = true;\n+    private static final boolean cleanupOnStop = true;\n \n     @BeforeClass\n     public static void setUp() throws Exception {\n@@ -60,7 +61,7 @@ public static void setUp() throws Exception {\n             // remove library directory\n             TestLibrarian.removeLibraryDir();\n             testExecutor.setLibrarian(new TestLibrarian());\n-            ExecutionTestUtil.setUp();\n+            ExecutionTestUtil.setUp(cleanupOnStart);\n         } catch (Throwable th) {\n             th.printStackTrace();\n             throw th;\n@@ -71,7 +72,7 @@ public static void setUp() throws Exception {\n     public static void tearDown() throws Exception {\n         // remove library directory\n         TestLibrarian.removeLibraryDir();\n-        ExecutionTestUtil.tearDown();\n+        ExecutionTestUtil.tearDown(cleanupOnStop);\n     }\n \n     @Parameters(name = \"ExecutionTest {index}: {0}\")",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTest.java",
                "sha": "e372d31a62ab8c4ae91c5cdcf7cd295edb24ca67",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTestUtil.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTestUtil.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 4,
                "filename": "asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTestUtil.java",
                "patch": "@@ -43,7 +43,7 @@\n \n     protected static TestGroup FailedGroup;\n \n-    public static void setUp() throws Exception {\n+    public static void setUp(boolean cleanup) throws Exception {\n         System.out.println(\"Starting setup\");\n         if (LOGGER.isLoggable(Level.INFO)) {\n             LOGGER.info(\"Starting setup\");\n@@ -53,7 +53,7 @@ public static void setUp() throws Exception {\n         if (LOGGER.isLoggable(Level.INFO)) {\n             LOGGER.info(\"initializing pseudo cluster\");\n         }\n-        AsterixHyracksIntegrationUtil.init(true);\n+        AsterixHyracksIntegrationUtil.init(cleanup);\n \n         if (LOGGER.isLoggable(Level.INFO)) {\n             LOGGER.info(\"initializing HDFS\");\n@@ -81,9 +81,9 @@ private static void validateBufferCacheState() {\n         }\n     }\n \n-    public static void tearDown() throws Exception {\n+    public static void tearDown(boolean cleanup) throws Exception {\n         // validateBufferCacheState(); <-- Commented out until bug is fixed -->\n-        AsterixHyracksIntegrationUtil.deinit(true);\n+        AsterixHyracksIntegrationUtil.deinit(cleanup);\n         File outdir = new File(PATH_ACTUAL);\n         File[] files = outdir.listFiles();\n         if (files == null || files.length == 0) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/test/java/org/apache/asterix/test/runtime/ExecutionTestUtil.java",
                "sha": "d919c92f60f4963e04b629091260e9b05ba768bb",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/test/java/org/apache/asterix/test/runtime/SqlppExecutionTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-app/src/test/java/org/apache/asterix/test/runtime/SqlppExecutionTest.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 3,
                "filename": "asterix-app/src/test/java/org/apache/asterix/test/runtime/SqlppExecutionTest.java",
                "patch": "@@ -51,20 +51,22 @@\n     protected static final String TEST_CONFIG_FILE_NAME = \"asterix-build-configuration.xml\";\n \n     protected static AsterixTransactionProperties txnProperties;\n-    private final static TestExecutor testExecutor = new TestExecutor();\n+    private static final TestExecutor testExecutor = new TestExecutor();\n+    private static final boolean cleanupOnStart = true;\n+    private static final boolean cleanupOnStop = true;\n \n     protected static TestGroup FailedGroup;\n \n     @BeforeClass\n     public static void setUp() throws Exception {\n         File outdir = new File(PATH_ACTUAL);\n         outdir.mkdirs();\n-        ExecutionTestUtil.setUp();\n+        ExecutionTestUtil.setUp(cleanupOnStart);\n     }\n \n     @AfterClass\n     public static void tearDown() throws Exception {\n-        ExecutionTestUtil.tearDown();\n+        ExecutionTestUtil.tearDown(cleanupOnStop);\n         AsterixHyracksIntegrationUtil.removeTestStorageFiles();\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-app/src/test/java/org/apache/asterix/test/runtime/SqlppExecutionTest.java",
                "sha": "b827a0da77ac999bf06293c38e4a5fff1de251e6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-external-data/src/main/java/org/apache/asterix/external/dataflow/FeedRecordDataFlowController.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-external-data/src/main/java/org/apache/asterix/external/dataflow/FeedRecordDataFlowController.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-external-data/src/main/java/org/apache/asterix/external/dataflow/FeedRecordDataFlowController.java",
                "patch": "@@ -85,6 +85,9 @@ public void start(IFrameWriter writer) throws HyracksDataException {\n                 }\n                 tupleForwarder.addTuple(tb);\n             }\n+        } catch (InterruptedException e) {\n+            //TODO: Find out what could cause an interrupted exception beside termination of a job/feed\n+            LOGGER.warn(\"Feed has been interrupted. Closing the feed\");\n         } catch (Exception e) {\n             failed = true;\n             tupleForwarder.flush();",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-external-data/src/main/java/org/apache/asterix/external/dataflow/FeedRecordDataFlowController.java",
                "sha": "387e2dc3e00e3cb0e319199e5c6ce253189f02fd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.1.script.aql",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.1.script.aql?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.1.script.aql",
                "patch": "@@ -0,0 +1 @@\n+create_and_start.sh",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.1.script.aql",
                "sha": "cc46136086bef2ff6f9d9dd534b382807376299a",
                "status": "added"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.2.ddl.aql",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.2.ddl.aql?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.2.ddl.aql",
                "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+/*\n+ * Description  : Create a change feed with meta-data and test ingestion of records\n+ * Expected Res : Success\n+ * Date         : 24th Feb 2016\n+ */\n+\n+drop dataverse KeyVerse if exists;\n+create dataverse KeyVerse;\n+use dataverse KeyVerse;\n+\n+create type DocumentType as open{\n+};\n+\n+create type KVMetaType as open{\n+\"key\":string,\n+bucket:string,\n+vbucket:int32,\n+seq:int64,\n+cas:int64,\n+creationTime:int64,\n+expiration:int32,\n+flags:int32,\n+revSeq:int64,\n+lockTime:int32\n+};\n+\n+create dataset KVStore(DocumentType) with meta(KVMetaType)primary key meta().\"key\";\n+\n+create feed KVChangeStream using adapter(\n+    (\"type-name\"=\"DocumentType\"),\n+    (\"meta-type-name\"=\"KVMetaType\"),\n+    (\"reader\"=\"kv_test\"),\n+    (\"parser\"=\"record-with-metadata\"),\n+    (\"format\"=\"dcp\"),\n+    (\"record-format\"=\"json\"),\n+    (\"change-feed\"=\"true\"),\n+    (\"key-indexes\"=\"0\"),\n+    (\"key-indicators\"=\"1\"),\n+    (\"num-of-records\"=\"1000\")\n+);\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.2.ddl.aql",
                "sha": "d3317e41bf2b606dcabdb5cd61482792e64b55c9",
                "status": "added"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.3.update.aql",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.3.update.aql?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.3.update.aql",
                "patch": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+/*\n+ * Description  : Create a change feed with meta-data and test ingestion of records\n+ * Expected Res : Success\n+ * Date         : 24th Feb 2016\n+ */\n+use dataverse KeyVerse;\n+\n+set wait-for-completion-feed \"true\";\n+connect feed KVChangeStream to dataset KVStore;\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.3.update.aql",
                "sha": "7faf01340863ead3e1ee0102a892b76a20ce8a0e",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.4.script.aql",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.4.script.aql?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.4.script.aql",
                "patch": "@@ -0,0 +1 @@\n+stop_and_start.sh",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.4.script.aql",
                "sha": "3ba1dc0e2f8a88caa6573165f74ca04503b7e4fc",
                "status": "added"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.query.aql",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.query.aql?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.query.aql",
                "patch": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+/*\n+ * Description  : Create a change feed and test ingestion of records\n+ * Expected Res : Success\n+ * Date         : 24th Feb 2016\n+ */\n+use dataverse KeyVerse;\n+\n+count(\n+    for $d in dataset KVStore\n+    return $d\n+);\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.query.aql",
                "sha": "9db20a9a4aa31fc349c0f5e13a13c63e9ae7862a",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.6.script.aql",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.6.script.aql?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.6.script.aql",
                "patch": "@@ -0,0 +1 @@\n+stop_and_delete.sh",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/queries/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.6.script.aql",
                "sha": "10e1a51c440715e1e9b5a320ce7e88ada8219817",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/results/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.adm",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/results/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.adm?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/results/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.adm",
                "patch": "@@ -0,0 +1 @@\n+804\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/results/query_after_restart/dataset-with-meta-record/dataset-with-meta-record.5.adm",
                "sha": "c31da8b3c449ed1fb8c6e37d77083f8a82d74e4a",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/create_and_start.sh",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/create_and_start.sh?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/create_and_start.sh",
                "patch": "@@ -0,0 +1 @@\n+$MANAGIX_HOME/bin/managix create -n nc1 -c $MANAGIX_HOME/clusters/local/local.xml;",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/create_and_start.sh",
                "sha": "945f01db9b3d8fd1eed3f07d906e1ac7fb194aa1",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_delete.sh",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_delete.sh?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_delete.sh",
                "patch": "@@ -0,0 +1,3 @@\n+$MANAGIX_HOME/bin/managix stop -n nc1;\n+$MANAGIX_HOME/bin/managix delete -n nc1;\n+",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_delete.sh",
                "sha": "d7deea3de811c5dacc09963570b93350635dc3f1",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_start.sh",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_start.sh?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 0,
                "filename": "asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_start.sh",
                "patch": "@@ -0,0 +1,2 @@\n+$MANAGIX_HOME/bin/managix stop -n nc1;\n+$MANAGIX_HOME/bin/managix start -n nc1;",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/scripts/query_after_restart/dataset-with-meta-record/stop_and_start.sh",
                "sha": "1271a2b2062896f7811e995c9f9bc6044deed4bc",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/testsuite.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-installer/src/test/resources/transactionts/testsuite.xml?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 1,
                "filename": "asterix-installer/src/test/resources/transactionts/testsuite.xml",
                "patch": "@@ -17,8 +17,12 @@\n  ! under the License.\n  !-->\n <test-suite xmlns=\"urn:xml.testframework.asterix.apache.org\" ResultOffsetPath=\"results\" QueryOffsetPath=\"queries\" QueryFileExtension=\".aql\">\n-\n   <test-group name=\"query_after_restart\">\n+      <test-case FilePath=\"query_after_restart\">\n+          <compilation-unit name=\"dataset-with-meta-record\">\n+              <output-dir compare=\"Text\">dataset-with-meta-record</output-dir>\n+          </compilation-unit>\n+      </test-case>\n       <test-case FilePath=\"query_after_restart\">\n           <compilation-unit name=\"external_index\">\n               <output-dir compare=\"Text\">external_index</output-dir>",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-installer/src/test/resources/transactionts/testsuite.xml",
                "sha": "0c12426a7e39cff4d0cc8f13aa696c7c7830054f",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/AqlMetadataProvider.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/AqlMetadataProvider.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 12,
                "filename": "asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/AqlMetadataProvider.java",
                "patch": "@@ -700,11 +700,11 @@ private IAdapterFactory getConfiguredAdapterFactory(Dataset dataset, String adap\n                 for (int i = 0; i < numPrimaryKeys; i++) {\n                     bloomFilterKeyFields[i] = i;\n                 }\n-\n-                typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType);\n+                // get meta item type\n+                ARecordType metaItemType = DatasetUtils.getMetaType(this, dataset);\n+                typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType, metaItemType);\n                 comparatorFactories = DatasetUtils.computeKeysBinaryComparatorFactories(dataset, itemType,\n                         context.getBinaryComparatorFactoryProvider());\n-\n                 filterFields = DatasetUtils.createFilterFields(dataset);\n                 btreeFields = DatasetUtils.createBTreeFieldsWhenThereisAFilter(dataset);\n             }\n@@ -1089,7 +1089,7 @@ public boolean scannerOperatorIsLeaf(IDataSource<AqlSourceId> dataSource) {\n             String itemTypeName = dataset.getItemTypeName();\n             ARecordType itemType = (ARecordType) MetadataManager.INSTANCE\n                     .getDatatype(mdTxnCtx, dataset.getItemTypeDataverseName(), itemTypeName).getDatatype();\n-            ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType);\n+            ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType, null);\n             IBinaryComparatorFactory[] comparatorFactories = DatasetUtils.computeKeysBinaryComparatorFactories(dataset,\n                     itemType, context.getBinaryComparatorFactoryProvider());\n \n@@ -1174,12 +1174,10 @@ public boolean scannerOperatorIsLeaf(IDataSource<AqlSourceId> dataSource) {\n             Index primaryIndex = MetadataManager.INSTANCE.getIndex(mdTxnCtx, dataset.getDataverseName(),\n                     dataset.getDatasetName(), dataset.getDatasetName());\n             String indexName = primaryIndex.getIndexName();\n-\n-            String itemTypeName = dataset.getItemTypeName();\n             ARecordType itemType = (ARecordType) MetadataManager.INSTANCE\n-                    .getDatatype(mdTxnCtx, dataset.getItemTypeDataverseName(), itemTypeName).getDatatype();\n-\n-            ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType);\n+                    .getDatatype(mdTxnCtx, dataset.getItemTypeDataverseName(), dataset.getItemTypeName()).getDatatype();\n+            ARecordType metaItemType = DatasetUtils.getMetaType(this, dataset);\n+            ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType, metaItemType);\n \n             IAsterixApplicationContextInfo appContext = (IAsterixApplicationContextInfo) context.getAppContext();\n             IBinaryComparatorFactory[] comparatorFactories = DatasetUtils.computeKeysBinaryComparatorFactories(dataset,\n@@ -2347,9 +2345,8 @@ public void setLocks(Map<String, Integer> locks) {\n             String itemTypeName = dataset.getItemTypeName();\n             ARecordType itemType = (ARecordType) MetadataManager.INSTANCE\n                     .getDatatype(mdTxnCtx, dataSource.getId().getDataverseName(), itemTypeName).getDatatype();\n-\n-            ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType);\n-\n+            ARecordType metaItemType = DatasetUtils.getMetaType(this, dataset);\n+            ITypeTraits[] typeTraits = DatasetUtils.computeTupleTypeTraits(dataset, itemType, metaItemType);\n             IAsterixApplicationContextInfo appContext = (IAsterixApplicationContextInfo) context.getAppContext();\n             IBinaryComparatorFactory[] comparatorFactories = DatasetUtils.computeKeysBinaryComparatorFactories(dataset,\n                     itemType, context.getBinaryComparatorFactoryProvider());",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/AqlMetadataProvider.java",
                "sha": "90ebfb7757db9c2f7aa72fb05f13e6a2359ccfc7",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/asterixdb/blob/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-metadata/src/main/java/org/apache/asterix/metadata/utils/DatasetUtils.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-metadata/src/main/java/org/apache/asterix/metadata/utils/DatasetUtils.java?ref=02b35a1bf98f4168dae91fa68c14d7e062bfd032",
                "deletions": 6,
                "filename": "asterix-metadata/src/main/java/org/apache/asterix/metadata/utils/DatasetUtils.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.asterix.metadata.MetadataException;\n import org.apache.asterix.metadata.MetadataManager;\n import org.apache.asterix.metadata.MetadataTransactionContext;\n+import org.apache.asterix.metadata.declared.AqlMetadataProvider;\n import org.apache.asterix.metadata.entities.CompactionPolicy;\n import org.apache.asterix.metadata.entities.Dataset;\n import org.apache.asterix.metadata.entities.ExternalDatasetDetails;\n@@ -105,11 +106,6 @@\n         return bhffs;\n     }\n \n-    public static ITypeTraits[] computeTupleTypeTraits(Dataset dataset, ARecordType itemType)\n-            throws AlgebricksException {\n-        return computeTupleTypeTraits(dataset, itemType, null);\n-    }\n-\n     public static ITypeTraits[] computeTupleTypeTraits(Dataset dataset, ARecordType itemType, ARecordType metaItemType)\n             throws AlgebricksException {\n         if (dataset.getDatasetType() == DatasetType.EXTERNAL) {\n@@ -156,7 +152,7 @@\n \n     public static IBinaryComparatorFactory[] computeFilterBinaryComparatorFactories(Dataset dataset,\n             ARecordType itemType, IBinaryComparatorFactoryProvider comparatorFactoryProvider)\n-                    throws AlgebricksException {\n+            throws AlgebricksException {\n         if (dataset.getDatasetType() == DatasetType.EXTERNAL) {\n             return null;\n         }\n@@ -274,4 +270,13 @@ public static void writePropertyTypeRecord(String name, String value, DataOutput\n \n         propertyRecordBuilder.write(out, true);\n     }\n+\n+    public static ARecordType getMetaType(AqlMetadataProvider metadataProvider, Dataset dataset)\n+            throws AlgebricksException {\n+        if (dataset.hasMetaPart()) {\n+            return (ARecordType) metadataProvider.findType(dataset.getMetaItemTypeDataverseName(),\n+                    dataset.getMetaItemTypeName());\n+        }\n+        return null;\n+    }\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/02b35a1bf98f4168dae91fa68c14d7e062bfd032/asterix-metadata/src/main/java/org/apache/asterix/metadata/utils/DatasetUtils.java",
                "sha": "0ac4f5600b154984b8e6d6959e9ca4dca5f91363",
                "status": "modified"
            }
        ],
        "message": "ASTERIXDB-1378 Fix NPE on Feed Connect After Restart\n\nThis issue was caused by the way type traits are computed for each\nindex. since we have duplicate code to create the index dataflow\nhelper, we had to fix the way type traits are created in all of\nthese different places. We propably need to do further refactoring\nto have common code for creating index dataflow helper instances.\n\nChange-Id: If8f8696d252868a8cce0afdbaeda0dd046f99186\nReviewed-on: https://asterix-gerrit.ics.uci.edu/766\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Yingyi Bu <buyingyi@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/442e49b9f174717035d0f4ebcc0810f5f3f3f29c",
        "patched_files": [
            "dataset-with-meta-record.1.script.aql",
            "dataset-with-meta-record.3.update.aql",
            "dataset-with-meta-record.5.adm",
            "create_and_start.sh",
            "AqlMetadataProvider.java",
            "dataset-with-meta-record.6.script.aql",
            "dataset-with-meta-record.5.query.aql",
            "dataset-with-meta-record.2.ddl.aql",
            "stop_and_start.sh",
            "testsuite.xml",
            "ExecutionTestUtil.java",
            "FeedRecordDataFlowController.java",
            "stop_and_delete.sh",
            "DatasetOperations.java",
            "dataset-with-meta-record.4.script.aql",
            "DatasetUtils.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "SqlppExecutionTest.java",
            "ExecutionTest.java"
        ]
    },
    "asterixdb_0902d2a": {
        "bug_id": "asterixdb_0902d2a",
        "commit": "https://github.com/apache/asterixdb/commit/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5",
        "file": [
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/asterixdb/blob/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5/asterix-common/src/test/java/edu/uci/ics/asterix/test/aql/TestsUtils.java",
                "changes": 102,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-common/src/test/java/edu/uci/ics/asterix/test/aql/TestsUtils.java?ref=0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5",
                "deletions": 57,
                "filename": "asterix-common/src/test/java/edu/uci/ics/asterix/test/aql/TestsUtils.java",
                "patch": "@@ -160,7 +160,7 @@ public static String aqlExtToResExt(String fname) {\n         return fname.substring(0, dot + 1) + EXTENSION_AQL_RESULT;\n     }\n \n-    public static void writeResultsToFile(File actualFile, InputStream resultStream) throws IOException, JSONException {\n+    public static void writeResultsToFile(File actualFile, InputStream resultStream) throws Exception {\n         BufferedWriter writer = new BufferedWriter(new FileWriter(actualFile));\n         try {\n             JsonFactory jsonFactory = new JsonFactory();\n@@ -176,10 +176,10 @@ public static void writeResultsToFile(File actualFile, InputStream resultStream)\n                             writer.write(record);\n                         }\n                     } else {\n-                        String summary = resultParser.getValueAsString();\n                         if (key.equals(\"summary\")) {\n+                            String summary = resultParser.nextTextValue();\n                             writer.write(summary);\n-                            throw new JsonMappingException(\"Could not find results key in the JSON Object\");\n+                            throw new Exception(\"Could not find results key in the JSON Object, result file is at \" + actualFile);\n                         }\n                     }\n                 }\n@@ -377,86 +377,75 @@ public static void executeTest(String actualPath, TestCaseContext testCaseCtx, P\n             for (TestFileContext ctx : testFileCtxs) {\n                 testFile = ctx.getFile();\n                 statement = TestsUtils.readTestFile(testFile);\n+                InputStream resultStream;\n                 try {\n                     switch (ctx.getType()) {\n                         case \"ddl\":\n                             TestsUtils.executeDDL(statement);\n                             break;\n                         case \"update\":\n-\n                             //isDmlRecoveryTest: set IP address\n                             if (isDmlRecoveryTest && statement.contains(\"nc1://\")) {\n                                 statement = statement\n                                         .replaceAll(\"nc1://\", \"127.0.0.1://../../../../../../asterix-app/\");\n-\n                             }\n \n                             TestsUtils.executeUpdate(statement);\n                             break;\n                         case \"query\":\n-                            try {\n-                                // isDmlRecoveryTest: insert Crash and Recovery\n-                                if (isDmlRecoveryTest) {\n-                                    executeScript(pb, pb.environment().get(\"SCRIPT_HOME\") + File.separator\n-                                            + \"dml_recovery\" + File.separator + \"kill_cc_and_nc.sh\");\n-                                    executeScript(pb, pb.environment().get(\"SCRIPT_HOME\") + File.separator\n-                                            + \"dml_recovery\" + File.separator + \"stop_and_start.sh\");\n-                                }\n+                            // isDmlRecoveryTest: insert Crash and Recovery\n+                            if (isDmlRecoveryTest) {\n+                                executeScript(pb, pb.environment().get(\"SCRIPT_HOME\") + File.separator + \"dml_recovery\"\n+                                        + File.separator + \"kill_cc_and_nc.sh\");\n+                                executeScript(pb, pb.environment().get(\"SCRIPT_HOME\") + File.separator + \"dml_recovery\"\n+                                        + File.separator + \"stop_and_start.sh\");\n+                            }\n \n-                                InputStream resultStream = executeQuery(statement);\n-                                expectedResultFile = expectedResultFileCtxs.get(queryCount).getFile();\n+                            resultStream = executeQuery(statement);\n+                            if (queryCount >= expectedResultFileCtxs.size()) {\n+                                throw new IllegalStateException(\"no result file for \" + testFile.toString());\n+                            }\n+                            expectedResultFile = expectedResultFileCtxs.get(queryCount).getFile();\n \n-                                File actualFile = new File(actualPath + File.separator\n-                                        + testCaseCtx.getTestCase().getFilePath().replace(File.separator, \"_\") + \"_\"\n-                                        + cUnit.getName() + \".adm\");\n-                                TestsUtils.writeResultsToFile(actualFile, resultStream);\n+                            File actualFile = new File(actualPath + File.separator\n+                                    + testCaseCtx.getTestCase().getFilePath().replace(File.separator, \"_\") + \"_\"\n+                                    + cUnit.getName() + \".adm\");\n+                            TestsUtils.writeResultsToFile(actualFile, resultStream);\n \n-                                File actualResultFile = testCaseCtx.getActualResultFile(cUnit, new File(actualPath));\n-                                actualResultFile.getParentFile().mkdirs();\n+                            File actualResultFile = testCaseCtx.getActualResultFile(cUnit, new File(actualPath));\n+                            actualResultFile.getParentFile().mkdirs();\n+\n+                            TestsUtils.runScriptAndCompareWithResult(testFile, new PrintWriter(System.err),\n+                                    expectedResultFile, actualFile);\n+                            LOGGER.info(\"[TEST]: \" + testCaseCtx.getTestCase().getFilePath() + \"/\" + cUnit.getName()\n+                                    + \" PASSED \");\n \n-                                TestsUtils.runScriptAndCompareWithResult(testFile, new PrintWriter(System.err),\n-                                        expectedResultFile, actualFile);\n-                                LOGGER.info(\"[TEST]: \" + testCaseCtx.getTestCase().getFilePath() + \"/\"\n-                                        + cUnit.getName() + \" PASSED \");\n-                            } catch (JsonMappingException e) {\n-                                throw new Exception(\"Test \\\"\" + testFile + \"\\\" FAILED!\\n\");\n-                            }\n                             queryCount++;\n                             break;\n                         case \"mgx\":\n                             executeManagixCommand(statement);\n                             break;\n                         case \"txnqbc\": //qbc represents query before crash\n-                            try {\n-                                InputStream resultStream = executeQuery(statement);\n-                                qbcFile = new File(actualPath + File.separator\n-                                        + testCaseCtx.getTestCase().getFilePath().replace(File.separator, \"_\") + \"_\"\n-                                        + cUnit.getName() + \"_qbc.adm\");\n-                                qbcFile.getParentFile().mkdirs();\n-                                TestsUtils.writeResultsToFile(qbcFile, resultStream);\n-                            } catch (JsonMappingException e) {\n-                                throw new Exception(\"Test \\\"\" + testFile + \"\\\" FAILED!\\n\");\n-                            }\n+                            resultStream = executeQuery(statement);\n+                            qbcFile = new File(actualPath + File.separator\n+                                    + testCaseCtx.getTestCase().getFilePath().replace(File.separator, \"_\") + \"_\"\n+                                    + cUnit.getName() + \"_qbc.adm\");\n+                            qbcFile.getParentFile().mkdirs();\n+                            TestsUtils.writeResultsToFile(qbcFile, resultStream);\n                             break;\n                         case \"txnqar\": //qar represents query after recovery\n-                            try {\n-\n-                                InputStream resultStream = executeQuery(statement);\n-\n-                                qarFile = new File(actualPath + File.separator\n-                                        + testCaseCtx.getTestCase().getFilePath().replace(File.separator, \"_\") + \"_\"\n-                                        + cUnit.getName() + \"_qar.adm\");\n-                                qarFile.getParentFile().mkdirs();\n-                                TestsUtils.writeResultsToFile(qarFile, resultStream);\n-\n-                                TestsUtils.runScriptAndCompareWithResult(testFile, new PrintWriter(System.err),\n-                                        qbcFile, qarFile);\n-\n-                                LOGGER.info(\"[TEST]: \" + testCaseCtx.getTestCase().getFilePath() + \"/\"\n-                                        + cUnit.getName() + \" PASSED \");\n-                            } catch (JsonMappingException e) {\n-                                throw new Exception(\"Test \\\"\" + testFile + \"\\\" FAILED!\\n\");\n-                            }\n+                            resultStream = executeQuery(statement);\n+                            qarFile = new File(actualPath + File.separator\n+                                    + testCaseCtx.getTestCase().getFilePath().replace(File.separator, \"_\") + \"_\"\n+                                    + cUnit.getName() + \"_qar.adm\");\n+                            qarFile.getParentFile().mkdirs();\n+                            TestsUtils.writeResultsToFile(qarFile, resultStream);\n+\n+                            TestsUtils.runScriptAndCompareWithResult(testFile, new PrintWriter(System.err),\n+                                    qbcFile, qarFile);\n+\n+                            LOGGER.info(\"[TEST]: \" + testCaseCtx.getTestCase().getFilePath() + \"/\"\n+                                    + cUnit.getName() + \" PASSED \");\n                             break;\n                         case \"txneu\": //eu represents erroneous update\n                             try {\n@@ -502,5 +491,4 @@ public static void executeTest(String actualPath, TestCaseContext testCaseCtx, P\n             }\n         }\n     }\n-\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5/asterix-common/src/test/java/edu/uci/ics/asterix/test/aql/TestsUtils.java",
                "sha": "b280ec975e0c541d9316c35d6cfa77564bbab10b",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/asterixdb/blob/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5/asterix-test-framework/src/main/java/edu/uci/ics/asterix/testframework/context/TestCaseContext.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterix-test-framework/src/main/java/edu/uci/ics/asterix/testframework/context/TestCaseContext.java?ref=0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5",
                "deletions": 44,
                "filename": "asterix-test-framework/src/main/java/edu/uci/ics/asterix/testframework/context/TestCaseContext.java",
                "patch": "@@ -60,66 +60,47 @@ public TestCase getTestCase() {\n         return testCase;\n     }\n \n-    public List<TestFileContext> getTestFiles(CompilationUnit cUnit) {\n+    public List<TestFileContext> getFilesInDir(String basePath, String dirName, boolean withType) {\n         List<TestFileContext> testFileCtxs = new ArrayList<TestFileContext>();\n \n         File path = tsRoot;\n-        path = new File(path, testSuite.getQueryOffsetPath());\n+        path = new File(path, basePath);\n         path = new File(path, testCase.getFilePath());\n-        path = new File(path, cUnit.getName());\n+        path = new File(path, dirName);\n \n-        String fileNames[] = path.list();\n-        for (String fName : fileNames) {\n-            if (fName.startsWith(\".\")) {\n-                continue;\n-            }\n-            \n-            File testFile = new File(path, fName);\n-            TestFileContext tfsc = new TestFileContext(testFile);\n-            String[] nameSplits = fName.split(\"\\\\.\");\n-            if (nameSplits.length < 3) {\n-                throw new IllegalArgumentException(\"Test file '\" + cUnit.getName() + File.separatorChar\n-                        + fName + \"' does not have the proper test file name format.\");\n-            }\n-            tfsc.setSeqNum(nameSplits[nameSplits.length - 3]);\n-            tfsc.setType(nameSplits[nameSplits.length - 2]);\n-            testFileCtxs.add(tfsc);\n-        }\n-        Collections.sort(testFileCtxs);\n-        return testFileCtxs;\n-    }\n-\n-    public List<TestFileContext> getExpectedResultFiles(CompilationUnit cUnit) {\n-        List<TestFileContext> resultFileCtxs = new ArrayList<TestFileContext>();\n-\n-        File path = tsRoot;\n-        path = new File(path, testSuite.getResultOffsetPath());\n-        path = new File(path, testCase.getFilePath());\n-        path = new File(path, cUnit.getOutputDir().getValue());\n-\n-        String fileNames[] = path.list();\n-\n-        if (fileNames != null) {\n+        if (path.isDirectory()) {\n+            String fileNames[] = path.list();\n             for (String fName : fileNames) {\n                 if (fName.startsWith(\".\")) {\n                     continue;\n                 }\n-                \n+\n                 File testFile = new File(path, fName);\n                 TestFileContext tfsc = new TestFileContext(testFile);\n                 String[] nameSplits = fName.split(\"\\\\.\");\n-                \n                 if (nameSplits.length < 3) {\n-                    throw new IllegalArgumentException(\"Test file '\" + cUnit.getName() + File.separatorChar\n-                            + fName + \"' does not have the proper test file name format.\");\n+                    throw new IllegalArgumentException(\"Test file '\" + dirName + File.separatorChar + fName\n+                            + \"' does not have the proper test file name format.\");\n                 }\n-                \n-                tfsc.setSeqNum(nameSplits[nameSplits.length - 2]);\n-                resultFileCtxs.add(tfsc);\n+                if (withType) {\n+                    tfsc.setSeqNum(nameSplits[nameSplits.length - 3]);\n+                    tfsc.setType(nameSplits[nameSplits.length - 2]);\n+                } else {\n+                    tfsc.setSeqNum(nameSplits[nameSplits.length - 2]);\n+                }\n+                testFileCtxs.add(tfsc);\n             }\n-            Collections.sort(resultFileCtxs);\n         }\n-        return resultFileCtxs;\n+        Collections.sort(testFileCtxs);\n+        return testFileCtxs;\n+    }\n+\n+    public List<TestFileContext> getTestFiles(CompilationUnit cUnit) {\n+        return getFilesInDir(testSuite.getQueryOffsetPath(), cUnit.getName(), true);\n+    }\n+\n+    public List<TestFileContext> getExpectedResultFiles(CompilationUnit cUnit) {\n+        return getFilesInDir(testSuite.getResultOffsetPath(), cUnit.getOutputDir().getValue(), false);\n     }\n \n     public File getActualResultFile(CompilationUnit cUnit, File actualResultsBase) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5/asterix-test-framework/src/main/java/edu/uci/ics/asterix/testframework/context/TestCaseContext.java",
                "sha": "04c10a506a3e5981db87a86912e374b9ee9367e9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/asterixdb/blob/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5/pom.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/pom.xml?ref=0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5",
                "deletions": 2,
                "filename": "pom.xml",
                "patch": "@@ -31,8 +31,8 @@\n     <metadata.tests>**/metadata/*Test.java</metadata.tests>\n     <execution.tests>**/ExecutionTest.java</execution.tests>\n     <invalid.tests>**/DmlTest.java</invalid.tests>\n-    <global.test.includes>**/*TestSuite.java,**/*Test.java</global.test.includes>\n-    <global.test.excludes>${optimizer.tests},${metadata.tests},${execution.tests},${invalid.tests}</global.test.excludes>\n+    <global.test.includes>**/*TestSuite.java,**/*Test.java,${execution.tests}</global.test.includes>\n+    <global.test.excludes>${optimizer.tests},${metadata.tests},${invalid.tests}</global.test.excludes>\n   </properties>\n \n \t<build>",
                "raw_url": "https://github.com/apache/asterixdb/raw/0902d2adbbb8e0abaf3d3230c4fc08e5d32346f5/pom.xml",
                "sha": "74592ed571ffbf0da492c7d365c50550292b25bf",
                "status": "modified"
            }
        ],
        "message": "some test cleanup\n- avoid NPE when reading the \"summary field\" of the result\n- avoid IndexOutOfBoundsException for missing result files\n- get rid of exception tunneling through JsonMappingException\n- refactor common code into TestCaseContext.getFilesInDir\nmove execution.tests to global.test.includes",
        "parent": "https://github.com/apache/asterixdb/commit/224f2ffe5b12d9ccd31c2d8b1e4fd8fb713cae0d",
        "patched_files": [
            "pom.xml"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "TestsUtils.java",
            "TestCaseContext.java"
        ]
    },
    "asterixdb_0d0a113": {
        "bug_id": "asterixdb_0d0a113",
        "commit": "https://github.com/apache/asterixdb/commit/0d0a113f40014384bcf54b68235581286c9b2c2b",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java",
                "patch": "@@ -64,6 +64,8 @@ private LoggerHolder() {\n     public NodeControllerService[] ncs = new NodeControllerService[0];\n     public IHyracksClientConnection hcc;\n \n+    private static final String DEFAULT_STORAGE_PATH = joinPath(\"target\", \"io\", \"dir\");\n+    private static String storagePath = DEFAULT_STORAGE_PATH;\n     private ConfigManager configManager;\n     private List<String> nodeNames;\n \n@@ -217,8 +219,16 @@ public void stopCC(boolean terminateNCService) throws Exception {\n         }\n     }\n \n+    public static void setStoragePath(String path) {\n+        storagePath = path;\n+    }\n+\n+    public static void restoreDefaultStoragePath() {\n+        storagePath = DEFAULT_STORAGE_PATH;\n+    }\n+\n     protected String getDefaultStoragePath() {\n-        return joinPath(\"target\", \"io\", \"dir\");\n+        return storagePath;\n     }\n \n     public void removeTestStorageFiles() {",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java",
                "sha": "279976548d3e77bd3cef1dd7c563f1398fd42443",
                "status": "modified"
            },
            {
                "additions": 173,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java",
                "changes": 173,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java",
                "patch": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.asterix.test.storage;\n+\n+import static org.apache.hyracks.util.StorageUtil.StorageUnit.MEGABYTE;\n+\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.asterix.api.common.AsterixHyracksIntegrationUtil;\n+import org.apache.asterix.app.bootstrap.TestNodeController;\n+import org.apache.asterix.app.data.gen.TupleGenerator;\n+import org.apache.asterix.app.data.gen.TupleGenerator.GenerationFunction;\n+import org.apache.asterix.common.config.DatasetConfig.DatasetType;\n+import org.apache.asterix.common.dataflow.LSMInsertDeleteOperatorNodePushable;\n+import org.apache.asterix.common.exceptions.ExceptionUtils;\n+import org.apache.asterix.common.transactions.ITransactionContext;\n+import org.apache.asterix.external.util.DataflowUtils;\n+import org.apache.asterix.file.StorageComponentProvider;\n+import org.apache.asterix.metadata.entities.Dataset;\n+import org.apache.asterix.metadata.entities.Index;\n+import org.apache.asterix.metadata.entities.InternalDatasetDetails;\n+import org.apache.asterix.metadata.entities.InternalDatasetDetails.PartitioningStrategy;\n+import org.apache.asterix.om.types.ARecordType;\n+import org.apache.asterix.om.types.BuiltinType;\n+import org.apache.asterix.om.types.IAType;\n+import org.apache.asterix.test.common.TestHelper;\n+import org.apache.commons.lang3.SystemUtils;\n+import org.apache.hyracks.api.comm.VSizeFrame;\n+import org.apache.hyracks.api.context.IHyracksTaskContext;\n+import org.apache.hyracks.api.exceptions.ErrorCode;\n+import org.apache.hyracks.api.exceptions.HyracksDataException;\n+import org.apache.hyracks.dataflow.common.comm.io.FrameTupleAppender;\n+import org.apache.hyracks.dataflow.common.data.accessors.ITupleReference;\n+import org.apache.hyracks.storage.am.lsm.common.impls.NoMergePolicyFactory;\n+import org.apache.hyracks.util.DiskUtil;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class DiskIsFullTest {\n+\n+    private static final IAType[] KEY_TYPES = { BuiltinType.AINT32 };\n+    private static final ARecordType RECORD_TYPE = new ARecordType(\"TestRecordType\", new String[] { \"key\", \"value\" },\n+            new IAType[] { BuiltinType.AINT32, BuiltinType.AINT64 }, false);\n+    private static final GenerationFunction[] RECORD_GEN_FUNCTION =\n+            { GenerationFunction.DETERMINISTIC, GenerationFunction.DETERMINISTIC };\n+    private static final boolean[] UNIQUE_RECORD_FIELDS = { true, false };\n+    private static final ARecordType META_TYPE = null;\n+    private static final GenerationFunction[] META_GEN_FUNCTION = null;\n+    private static final boolean[] UNIQUE_META_FIELDS = null;\n+    private static final int[] KEY_INDEXES = { 0 };\n+    private static final int[] KEY_INDICATOR = { Index.RECORD_INDICATOR };\n+    private static final List<Integer> KEY_INDICATOR_LIST = Arrays.asList(new Integer[] { Index.RECORD_INDICATOR });\n+    private static final int DATASET_ID = 101;\n+    private static final String DATAVERSE_NAME = \"TestDV\";\n+    private static final String DATASET_NAME = \"TestDS\";\n+    private static final String DATA_TYPE_NAME = \"DUMMY\";\n+    private static final String NODE_GROUP_NAME = \"DEFAULT\";\n+    private static final String TEST_DISK_NAME = \"asterixdb_ram_disk\";\n+    private boolean shouldRun = true;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        if (!SystemUtils.IS_OS_MAC) {\n+            System.out.println(\"Skipping test \" + DiskIsFullTest.class.getName() + \" due to unsupported OS\");\n+            shouldRun = false;\n+            return;\n+        }\n+        System.out.println(\"SetUp: \");\n+        TestHelper.deleteExistingInstanceFiles();\n+        // create RAM disk\n+        final Path ramDiskRoot = DiskUtil.mountRamDisk(TEST_DISK_NAME, 4, MEGABYTE);\n+        // Use RAM disk for storage\n+        AsterixHyracksIntegrationUtil.setStoragePath(ramDiskRoot.toAbsolutePath().toString());\n+    }\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (!shouldRun) {\n+            return;\n+        }\n+        System.out.println(\"TearDown\");\n+        TestHelper.deleteExistingInstanceFiles();\n+        DiskUtil.unmountRamDisk(TEST_DISK_NAME);\n+        AsterixHyracksIntegrationUtil.restoreDefaultStoragePath();\n+    }\n+\n+    @Test\n+    public void testDiskIsFull() {\n+        if (!shouldRun) {\n+            return;\n+        }\n+        HyracksDataException expectedException =\n+                HyracksDataException.create(ErrorCode.CANNOT_MODIFY_INDEX_DISK_IS_FULL);\n+        try {\n+            TestNodeController nc = new TestNodeController(null, false);\n+            nc.init();\n+            StorageComponentProvider storageManager = new StorageComponentProvider();\n+            List<List<String>> partitioningKeys = new ArrayList<>();\n+            partitioningKeys.add(Collections.singletonList(\"key\"));\n+            Dataset dataset =\n+                    new Dataset(DATAVERSE_NAME, DATASET_NAME, DATAVERSE_NAME, DATA_TYPE_NAME, NODE_GROUP_NAME, null,\n+                            null,\n+                            new InternalDatasetDetails(null, PartitioningStrategy.HASH, partitioningKeys, null, null,\n+                                    null, false, null, false), null, DatasetType.INTERNAL, DATASET_ID, 0);\n+            try {\n+                nc.createPrimaryIndex(dataset, KEY_TYPES, RECORD_TYPE, META_TYPE, new NoMergePolicyFactory(), null,\n+                        null, storageManager, KEY_INDEXES, KEY_INDICATOR_LIST);\n+                IHyracksTaskContext ctx = nc.createTestContext(false);\n+                nc.newJobId();\n+                ITransactionContext txnCtx = nc.getTransactionManager().getTransactionContext(nc.getTxnJobId(), true);\n+                // Prepare insert operation\n+                LSMInsertDeleteOperatorNodePushable insertOp =\n+                        nc.getInsertPipeline(ctx, dataset, KEY_TYPES, RECORD_TYPE, META_TYPE,\n+                                new NoMergePolicyFactory(), null, null, KEY_INDEXES, KEY_INDICATOR_LIST, storageManager)\n+                                .getLeft();\n+                insertOp.open();\n+                TupleGenerator tupleGenerator =\n+                        new TupleGenerator(RECORD_TYPE, META_TYPE, KEY_INDEXES, KEY_INDICATOR, RECORD_GEN_FUNCTION,\n+                                UNIQUE_RECORD_FIELDS, META_GEN_FUNCTION, UNIQUE_META_FIELDS);\n+                VSizeFrame frame = new VSizeFrame(ctx);\n+                FrameTupleAppender tupleAppender = new FrameTupleAppender(frame);\n+                // Insert records until disk becomes full\n+                int tupleCount = 100000;\n+                while (tupleCount > 0) {\n+                    ITupleReference tuple = tupleGenerator.next();\n+                    try {\n+                        DataflowUtils.addTupleToFrame(tupleAppender, tuple, insertOp);\n+                    } catch (Throwable t) {\n+                        final Throwable rootCause = ExceptionUtils.getRootCause(t);\n+                        rootCause.printStackTrace();\n+                        if (rootCause instanceof HyracksDataException) {\n+                            HyracksDataException cause = (HyracksDataException) rootCause;\n+                            Assert.assertEquals(cause.getErrorCode(), expectedException.getErrorCode());\n+                            Assert.assertEquals(cause.getMessage(), expectedException.getMessage());\n+                            return;\n+                        } else {\n+                            break;\n+                        }\n+                    }\n+                    tupleCount--;\n+                }\n+                Assert.fail(\"Expected exception (\" + expectedException + \") was not thrown\");\n+            } finally {\n+                nc.deInit();\n+            }\n+        } catch (Throwable e) {\n+            e.printStackTrace();\n+            Assert.fail(\"Expected exception (\" + expectedException + \") was not thrown\");\n+        }\n+    }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java",
                "sha": "58697a99bc9fa19b1d59e2aadad8931a7a619208",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 3,
                "filename": "asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java",
                "patch": "@@ -18,6 +18,8 @@\n  */\n package org.apache.asterix.transaction.management.resource;\n \n+import static org.apache.hyracks.api.exceptions.ErrorCode.CANNOT_CREATE_FILE;\n+\n import java.io.File;\n import java.io.FileInputStream;\n import java.io.FileOutputStream;\n@@ -190,10 +192,12 @@ public synchronized void insert(LocalResource resource) throws HyracksDataExcept\n         FileReference resourceFile = ioManager.resolve(relativePath);\n         if (resourceFile.getFile().exists()) {\n             throw new HyracksDataException(\"Duplicate resource: \" + resourceFile.getAbsolutePath());\n-        } else {\n-            resourceFile.getFile().getParentFile().mkdirs();\n         }\n-        resourceCache.put(resource.getPath(), resource);\n+\n+        final File parent = resourceFile.getFile().getParentFile();\n+        if (!parent.exists() && !parent.mkdirs()) {\n+            throw HyracksDataException.create(CANNOT_CREATE_FILE, parent.getAbsolutePath());\n+        }\n \n         try (FileOutputStream fos = new FileOutputStream(resourceFile.getFile());\n                 ObjectOutputStream oosToFos = new ObjectOutputStream(fos)) {\n@@ -203,6 +207,8 @@ public synchronized void insert(LocalResource resource) throws HyracksDataExcept\n             throw new HyracksDataException(e);\n         }\n \n+        resourceCache.put(resource.getPath(), resource);\n+\n         //if replication enabled, send resource metadata info to remote nodes\n         if (isReplicationEnabled) {\n             createReplicationJob(ReplicationOperation.REPLICATE, resourceFile);",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java",
                "sha": "b117cf1d6d1f40ae4558e53fad0489926833d778",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java",
                "patch": "@@ -121,6 +121,7 @@\n     public static final int FOUND_MULTIPLE_TRANSACTIONS = 85;\n     public static final int UNRECOGNIZED_INDEX_COMPONENT_FILE = 86;\n     public static final int UNEQUAL_NUM_FILTERS_TREES = 87;\n+    public static final int CANNOT_MODIFY_INDEX_DISK_IS_FULL = 88;\n \n     // Compilation error codes.\n     public static final int RULECOLLECTION_NOT_INSTANCE_OF_LIST = 10000;",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java",
                "sha": "e6fbc6f265cc4e0a0971d40b1db520e772bf3bc7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties",
                "patch": "@@ -104,5 +104,6 @@\n 85 = Found more than one transaction file in %1$s\n 86 = Found an unrecognized index file %1$s\n 87 = Unequal number of trees and filters found in %1$s\n+88 = Cannot modify index (Disk is full)\n \n 10000 = The given rule collection %1$s is not an instance of the List class.",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties",
                "sha": "d2e05e381c8bded26478c7abebfaffd4a3a2d1d1",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 10,
                "filename": "hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java",
                "patch": "@@ -66,28 +66,35 @@ public void open(IHyracksTaskContext ctx) throws HyracksDataException {\n     }\n \n     public void close() throws HyracksDataException {\n-        out.close();\n+        if (out != null) {\n+            out.close();\n+        }\n     }\n \n     public void appendFrame(ByteBuffer buffer) throws HyracksDataException {\n         out.nextFrame(buffer);\n     }\n \n     public void writeOut(IFrameWriter writer, IFrame frame, boolean failed) throws HyracksDataException {\n-        RunFileReader in = out.createReader();\n+        RunFileReader in = null;\n+        if (out != null) {\n+            in = out.createReader();\n+        }\n         writer.open();\n         try {\n             if (failed) {\n                 writer.fail();\n                 return;\n             }\n-            in.open();\n-            try {\n-                while (in.nextFrame(frame)) {\n-                    writer.nextFrame(frame.getBuffer());\n+            if (in != null) {\n+                in.open();\n+                try {\n+                    while (in.nextFrame(frame)) {\n+                        writer.nextFrame(frame.getBuffer());\n+                    }\n+                } finally {\n+                    in.close();\n                 }\n-            } finally {\n-                in.close();\n             }\n         } catch (Exception e) {\n             writer.fail();\n@@ -96,10 +103,10 @@ public void writeOut(IFrameWriter writer, IFrame frame, boolean failed) throws H\n             try {\n                 writer.close();\n             } finally {\n-                if (numConsumers.decrementAndGet() == 0) {\n+                if (numConsumers.decrementAndGet() == 0 && out != null) {\n                     out.getFileReference().delete();\n                 }\n             }\n         }\n     }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java",
                "sha": "31cbaad111562a950b3ec3a486552d0f24bcb6f1",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java",
                "patch": "@@ -144,6 +144,11 @@ public void threadExit(LSMOperationType opType, boolean failedOperation, boolean\n                     throw new IllegalStateException(\"Flush sees an illegal LSM memory compoenent state: \" + state);\n                 }\n                 readerCount--;\n+                if (failedOperation) {\n+                    // if flush failed, return the component state to READABLE_UNWRITABLE\n+                    state = ComponentState.READABLE_UNWRITABLE;\n+                    return;\n+                }\n                 if (readerCount == 0) {\n                     state = ComponentState.INACTIVE;\n                 } else {",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java",
                "sha": "1ee68d991223fdb082b677286173c437d1cd243f",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndex;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndexAccessor;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndexOperationContext;\n+import org.apache.hyracks.storage.am.lsm.common.api.ILSMMemoryComponent;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMMergePolicy;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMOperationTracker;\n import org.apache.hyracks.storage.am.lsm.common.api.LSMOperationType;\n@@ -131,6 +132,10 @@ protected boolean getAndEnterComponents(ILSMIndexOperationContext ctx, LSMOperat\n                     // Flush and merge operations should never reach this wait call, because they are always try operations.\n                     // If they fail to enter the components, then it means that there are an ongoing flush/merge operation on\n                     // the same components, so they should not proceed.\n+                    if (opType == LSMOperationType.MODIFICATION) {\n+                        // before waiting, make sure the index is in a modifiable state to avoid waiting forever.\n+                        ensureIndexModifiable();\n+                    }\n                     opTracker.wait();\n                 } catch (InterruptedException e) {\n                     throw new HyracksDataException(e);\n@@ -186,6 +191,7 @@ protected boolean enterComponents(ILSMIndexOperationContext ctx, LSMOperationTyp\n                 break;\n             case MERGE:\n                 lsmIndex.getIOOperationCallback().beforeOperation(LSMOperationType.MERGE);\n+                break;\n             default:\n                 break;\n         }\n@@ -498,15 +504,17 @@ public void flush(ILSMIndexOperationContext ctx, ILSMIOOperation operation) thro\n         }\n \n         ILSMDiskComponent newComponent = null;\n+        boolean failedOperation = false;\n         try {\n             newComponent = lsmIndex.flush(operation);\n             operation.getCallback().afterOperation(LSMOperationType.FLUSH, null, newComponent);\n             lsmIndex.markAsValid(newComponent);\n         } catch (Throwable e) {\n+            failedOperation = true;\n             e.printStackTrace();\n             throw e;\n         } finally {\n-            exitComponents(ctx, LSMOperationType.FLUSH, newComponent, false);\n+            exitComponents(ctx, LSMOperationType.FLUSH, newComponent, failedOperation);\n             operation.getCallback().afterFinalize(LSMOperationType.FLUSH, newComponent);\n         }\n         if (LOGGER.isLoggable(Level.INFO)) {\n@@ -545,15 +553,17 @@ public void merge(ILSMIndexOperationContext ctx, ILSMIOOperation operation) thro\n         }\n \n         ILSMDiskComponent newComponent = null;\n+        boolean failedOperation = false;\n         try {\n             newComponent = lsmIndex.merge(operation);\n             operation.getCallback().afterOperation(LSMOperationType.MERGE, ctx.getComponentHolder(), newComponent);\n             lsmIndex.markAsValid(newComponent);\n         } catch (Throwable e) {\n+            failedOperation = true;\n             e.printStackTrace();\n             throw e;\n         } finally {\n-            exitComponents(ctx, LSMOperationType.MERGE, newComponent, false);\n+            exitComponents(ctx, LSMOperationType.MERGE, newComponent, failedOperation);\n             operation.getCallback().afterFinalize(LSMOperationType.MERGE, newComponent);\n         }\n         if (LOGGER.isLoggable(Level.INFO)) {\n@@ -660,4 +670,23 @@ public void batchOperate(ILSMIndexOperationContext ctx, FrameTupleAccessor acces\n             exit(ctx);\n         }\n     }\n+\n+    /***\n+     * Ensures the index is in a modifiable state\n+     * @throws HyracksDataException if the index is not in a modifiable state\n+     */\n+    private void ensureIndexModifiable() throws HyracksDataException {\n+        // find if there is any memory component which is in a writable state or eventually will be in a writable state\n+        for (ILSMMemoryComponent memoryComponent : lsmIndex.getMemoryComponents()) {\n+            switch (memoryComponent.getState()) {\n+                case INACTIVE:\n+                case READABLE_WRITABLE:\n+                case READABLE_UNWRITABLE_FLUSHING:\n+                    return;\n+                default:\n+                    // continue to the next component\n+            }\n+        }\n+        throw HyracksDataException.create(ErrorCode.CANNOT_MODIFY_INDEX_DISK_IS_FULL);\n+    }\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java",
                "sha": "8ff907a30f74e38f4972f24ee85a9408570f034f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/pom.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-util/pom.xml?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-util/pom.xml",
                "patch": "@@ -64,6 +64,10 @@\n       <groupId>com.fasterxml.jackson.core</groupId>\n       <artifactId>jackson-core</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.commons</groupId>\n+      <artifactId>commons-lang3</artifactId>\n+    </dependency>\n   </dependencies>\n \n </project>",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/pom.xml",
                "sha": "3b03fce8a5dfdd305d7e8caafab865051c5d959c",
                "status": "modified"
            },
            {
                "additions": 126,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java",
                "changes": 126,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java",
                "patch": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hyracks.util;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import org.apache.commons.lang3.SystemUtils;\n+\n+public class DiskUtil {\n+\n+    private static final Logger LOGGER = Logger.getLogger(DiskUtil.class.getName());\n+\n+    private DiskUtil() {\n+        throw new AssertionError(\"Util class should not be initialized.\");\n+    }\n+\n+    /**\n+     * Mounts a RAM disk\n+     *\n+     * @param name\n+     * @param size\n+     * @param unit\n+     * @return The root of the mounted disk\n+     * @throws IOException\n+     * @throws InterruptedException\n+     */\n+    public static Path mountRamDisk(String name, int size, StorageUtil.StorageUnit unit)\n+            throws IOException, InterruptedException {\n+        if (SystemUtils.IS_OS_MAC) {\n+            return mountMacRamDisk(name, (StorageUtil.getIntSizeInBytes(size, unit) * 2) / StorageUtil.BASE);\n+        } else if (SystemUtils.IS_OS_LINUX) {\n+            return mountLinuxRamDisk(name, size + unit.getLinuxUnitTypeInLetter());\n+        }\n+        throw new UnsupportedOperationException(\"Unsupported OS: \" + System.getProperty(\"os.name\"));\n+    }\n+\n+    /**\n+     * Unmounts a disk\n+     *\n+     * @param name\n+     * @throws IOException\n+     * @throws InterruptedException\n+     */\n+    public static void unmountRamDisk(String name) throws IOException, InterruptedException {\n+        if (SystemUtils.IS_OS_MAC) {\n+            unmountMacRamDisk(name);\n+        } else if (SystemUtils.IS_OS_LINUX) {\n+            unmountLinuxRamDisk(name);\n+        }\n+    }\n+\n+    private static Path mountMacRamDisk(String name, long size) throws IOException, InterruptedException {\n+        final String cmd = \"diskutil erasevolume HFS+ '\" + name + \"' `hdiutil attach -nomount ram://\" + size + \"`\";\n+        final ProcessBuilder pb = new ProcessBuilder(\"/bin/sh\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+        return Paths.get(\"/Volumes\", name);\n+    }\n+\n+    private static void unmountMacRamDisk(String name) throws InterruptedException, IOException {\n+        final String cmd = \"diskutil unmount \" + name;\n+        final ProcessBuilder pb = new ProcessBuilder(\"/bin/sh\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+    }\n+\n+    private static Path mountLinuxRamDisk(String name, String size) throws IOException, InterruptedException {\n+        Path root = Paths.get(\"/tmp\", name);\n+        if (!Files.exists(root)) {\n+            Files.createFile(root);\n+        }\n+        final String cmd = \"mount -o size=\" + size + \" -t tmpfs none /tmp/\" + name;\n+        final ProcessBuilder pb = new ProcessBuilder(\"bash\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+        return root;\n+    }\n+\n+    private static void unmountLinuxRamDisk(String name) throws InterruptedException, IOException {\n+        final String cmd = \"umount /tmp/\" + name;\n+        final ProcessBuilder pb = new ProcessBuilder(\"bash\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+    }\n+\n+    private static void watchProcess(Process p) {\n+        new Thread(() -> {\n+            final BufferedReader input = new BufferedReader(new InputStreamReader(p.getInputStream()));\n+            String line;\n+            try {\n+                while ((line = input.readLine()) != null) {\n+                    LOGGER.info(line);\n+                }\n+            } catch (IOException e) {\n+                LOGGER.log(Level.WARNING, e.getMessage(), e);\n+            }\n+        }).start();\n+    }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java",
                "sha": "9a65d720a4e85b9470ce1ea8fc3dc1c44c90603b",
                "status": "added"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 8,
                "filename": "hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java",
                "patch": "@@ -23,17 +23,18 @@\n \n public class StorageUtil {\n \n-    private static final int BASE = 1024;\n+    public static final int BASE = 1024;\n \n     public enum StorageUnit {\n-        BYTE(\"B\", 1),\n-        KILOBYTE(\"KB\", BASE),\n-        MEGABYTE(\"MB\", KILOBYTE.multiplier * BASE),\n-        GIGABYTE(\"GB\", MEGABYTE.multiplier * BASE),\n-        TERABYTE(\"TB\", GIGABYTE.multiplier * BASE),\n-        PETABYTE(\"PB\", TERABYTE.multiplier * BASE);\n+        BYTE(\"B\", \"b\", 1),\n+        KILOBYTE(\"KB\", \"kb\", BASE),\n+        MEGABYTE(\"MB\", \"m\", KILOBYTE.multiplier * BASE),\n+        GIGABYTE(\"GB\", \"g\", MEGABYTE.multiplier * BASE),\n+        TERABYTE(\"TB\", \"t\", GIGABYTE.multiplier * BASE),\n+        PETABYTE(\"PB\", \"p\", TERABYTE.multiplier * BASE);\n \n         private final String unitTypeInLetter;\n+        private final String linuxUnitTypeInLetter;\n         private final long multiplier;\n         private static final Map<String, StorageUnit> SUFFIX_TO_UNIT_MAP = new HashMap<>();\n \n@@ -43,8 +44,9 @@\n             }\n         }\n \n-        StorageUnit(String unitTypeInLetter, long multiplier) {\n+        StorageUnit(String unitTypeInLetter, String linuxUnitTypeInLetter, long multiplier) {\n             this.unitTypeInLetter = unitTypeInLetter;\n+            this.linuxUnitTypeInLetter = linuxUnitTypeInLetter;\n             this.multiplier = multiplier;\n         }\n \n@@ -57,6 +59,10 @@ public double toBytes(double value) {\n             return value * multiplier;\n         }\n \n+        public String getLinuxUnitTypeInLetter() {\n+            return linuxUnitTypeInLetter;\n+        }\n+\n         public static StorageUnit lookupBySuffix(String name) {\n             return SUFFIX_TO_UNIT_MAP.get(name);\n         }",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java",
                "sha": "dbfe6f9503fb67090ec2fbbb5450bfb2ea2fe207",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-1995][STO] Abort write txn when index cannot be flushed\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Fix LSM memory component state transition on flush/merge failure\n- When index cannot be flushed, abort waiting threads\n- Prevent NPE in MateralizerTaskState when file creation fails\n- Check parent dirs creation for index metadata file\n\nChange-Id: I28592c30c788f4a6f44db8b47a84bc77f6b3f8f3\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1896\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: abdullah alamoudi <bamousaa@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/8b077a536aa7c1bb69f31067ace8f136ffdf5182",
        "patched_files": [
            "pom.xml",
            "ErrorCode.java",
            "StorageUtil.java",
            "LSMHarness.java",
            "DiskUtil.java",
            "AsterixHyracksIntegrationUtil.java",
            "PersistentLocalResourceRepository.java",
            "en.properties",
            "MaterializerTaskState.java",
            "AbstractLSMMemoryComponent.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "DiskIsFullTest.java"
        ]
    },
    "asterixdb_4af131a": {
        "bug_id": "asterixdb_4af131a",
        "commit": "https://github.com/apache/asterixdb/commit/4af131ada7f104cd8adddcabd8b5eb928f560f68",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/asterixdb/blob/4af131ada7f104cd8adddcabd8b5eb928f560f68/asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedAdapter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedAdapter.java?ref=4af131ada7f104cd8adddcabd8b5eb928f560f68",
                "deletions": 1,
                "filename": "asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedAdapter.java",
                "patch": "@@ -55,7 +55,7 @@\n \n     public TestTypedAdapter(ITupleParserFactory parserFactory, ARecordType sourceDatatype, IHyracksTaskContext ctx,\n             Map<String, String> configuration, int partition) throws IOException {\n-        super(null);\n+        super(new TestTypedFeedDataFlowController(ctx));\n         pos = new PipedOutputStream();\n         pis = new PipedInputStream(pos);\n         this.configuration = configuration;\n@@ -150,4 +150,5 @@ public boolean pause() {\n     public boolean resume() {\n         return false;\n     }\n+\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/4af131ada7f104cd8adddcabd8b5eb928f560f68/asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedAdapter.java",
                "sha": "effd59fcc087c3203ebf357718b04e09cb178792",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/asterixdb/blob/4af131ada7f104cd8adddcabd8b5eb928f560f68/asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedFeedDataFlowController.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedFeedDataFlowController.java?ref=4af131ada7f104cd8adddcabd8b5eb928f560f68",
                "deletions": 0,
                "filename": "asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedFeedDataFlowController.java",
                "patch": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.asterix.external.library.adapter;\n+\n+import org.apache.asterix.external.dataflow.AbstractFeedDataFlowController;\n+import org.apache.hyracks.api.comm.IFrameWriter;\n+import org.apache.hyracks.api.context.IHyracksTaskContext;\n+\n+class TestTypedFeedDataFlowController extends AbstractFeedDataFlowController {\n+    TestTypedFeedDataFlowController(IHyracksTaskContext ctx) {\n+        super(ctx, null, 0);\n+    }\n+\n+    @Override\n+    public String getStats() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void start(IFrameWriter writer) {\n+        throw new UnsupportedOperationException();\n+    }\n+}",
                "raw_url": "https://github.com/apache/asterixdb/raw/4af131ada7f104cd8adddcabd8b5eb928f560f68/asterixdb/asterix-external-data/src/test/java/org/apache/asterix/external/library/adapter/TestTypedFeedDataFlowController.java",
                "sha": "708cdd8488fa8e8071bbe506413831ffd72fa441",
                "status": "added"
            }
        ],
        "message": "[NO ISSUE][EXT] Prevent NPE in close() when using TestTypedAdapter\n\nWARN ...CleanupUtils - Failure closing a closeable resource\njava.lang.NullPointerException: null\n  at org.apache.asterix.external.dataset.adapter.FeedAdapter.close(FeedAdapter.java:63)\n\nChange-Id: If2d62ce00858ff9a9f8033bd21d5da5f1f207c56\nReviewed-on: https://asterix-gerrit.ics.uci.edu/2903\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: abdullah alamoudi <bamousaa@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/309c69c57a4fb79028eeee8efd9e179240299f3d",
        "patched_files": [],
        "repo": "asterixdb",
        "unit_tests": [
            "TestTypedAdapter.java",
            "TestTypedFeedDataFlowController.java"
        ]
    },
    "asterixdb_4eaaff5": {
        "bug_id": "asterixdb_4eaaff5",
        "commit": "https://github.com/apache/asterixdb/commit/4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/asterixdb/blob/4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4/hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java?ref=4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java",
                "patch": "@@ -333,8 +333,10 @@ protected void doStop() throws InterruptedException {\n         } catch (Exception e) {\n             LOGGER.log(Level.ERROR, \"Error while shutting down http server executor\", e);\n         }\n-        channel.close();\n-        channel.closeFuture().sync();\n+        if (channel != null) {\n+            channel.close();\n+            channel.closeFuture().sync();\n+        }\n     }\n \n     public IServlet getServlet(FullHttpRequest request) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4/hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java",
                "sha": "d9902da942921e10e7da92818644bcb17a50104a",
                "status": "modified"
            }
        ],
        "message": "[NO ISSUE][OTH] Ensure HttpServer Channel is Initialized\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Ensure the HttpServer channel is initialized when stopping\n  the server to avoid NPE.\n\nChange-Id: I5b7403e80f6118f99be46d166c6cfbee8d4305ac\nReviewed-on: https://asterix-gerrit.ics.uci.edu/3389\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Dmitry Lychagin <dmitry.lychagin@couchbase.com>",
        "parent": "https://github.com/apache/asterixdb/commit/a93a5c6fe4f83cef973f5caaa540723c96e29ba2",
        "patched_files": [
            "HttpServer.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "HttpServerTest.java"
        ]
    },
    "asterixdb_54a5070": {
        "bug_id": "asterixdb_54a5070",
        "commit": "https://github.com/apache/asterixdb/commit/54a507007e08cd84652774263bd7e1fe9ede8a0f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 1,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java",
                "patch": "@@ -521,7 +521,7 @@ private void startTasks(Map<String, List<TaskAttemptDescriptor>> taskAttemptMap)\n         }\n     }\n \n-    private void abortJob(List<Exception> exceptions) {\n+    public void abortJob(List<Exception> exceptions) {\n         Set<TaskCluster> inProgressTaskClustersCopy = new HashSet<>(inProgressTaskClusters);\n         for (TaskCluster tc : inProgressTaskClustersCopy) {\n             abortTaskCluster(findLastTaskClusterAttempt(tc), TaskClusterAttempt.TaskClusterStatus.ABORTED);",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java",
                "sha": "f18a9179b1c7b3c60804ee20858849059b89ccdf",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 3,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java",
                "patch": "@@ -45,7 +45,6 @@\n import org.apache.hyracks.control.cc.cluster.INodeManager;\n import org.apache.hyracks.control.cc.scheduler.FIFOJobQueue;\n import org.apache.hyracks.control.cc.scheduler.IJobQueue;\n-import org.apache.hyracks.control.cc.work.JobCleanupWork;\n import org.apache.hyracks.control.common.controllers.CCConfig;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -318,8 +317,12 @@ private void executeJobInternal(JobRun run) {\n         try {\n             run.getExecutor().startJob();\n         } catch (Exception e) {\n-            ccs.getWorkQueue().schedule(new JobCleanupWork(ccs.getJobManager(), run.getJobId(), JobStatus.FAILURE,\n-                    Collections.singletonList(e)));\n+            LOGGER.log(Level.SEVERE, \"Aborting \" + run.getJobId() + \" due to failure during job start\", e);\n+            final List<Exception> exceptions = Collections.singletonList(e);\n+            // fail the job then abort it\n+            run.setStatus(JobStatus.FAILURE, exceptions);\n+            // abort job will trigger JobCleanupWork\n+            run.getExecutor().abortJob(exceptions);\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java",
                "sha": "abf1d5793eb3fb1425184881b2690270f1cc3ecc",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java",
                "patch": "@@ -18,6 +18,7 @@\n  */\n package org.apache.hyracks.control.cc.work;\n \n+import java.util.ArrayList;\n import java.util.List;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n@@ -55,8 +56,12 @@ public void run() {\n         } catch (HyracksException e) {\n             // Fail the job with the caught exception during final completion.\n             JobRun run = jobManager.get(jobId);\n-            run.getExceptions().add(e);\n-            run.setStatus(JobStatus.FAILURE, run.getExceptions());\n+            List<Exception> completionException = new ArrayList<>();\n+            if (run.getExceptions() != null && !run.getExceptions().isEmpty()) {\n+                completionException.addAll(run.getExceptions());\n+            }\n+            completionException.add(0, e);\n+            run.setStatus(JobStatus.FAILURE, completionException);\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java",
                "sha": "502ac50e510e04c84611e73d7e60c56ad66c04d9",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java",
                "patch": "@@ -18,6 +18,8 @@\n  */\n package org.apache.hyracks.control.cc.work;\n \n+import java.util.ArrayList;\n+import java.util.List;\n import java.util.Set;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n@@ -67,8 +69,12 @@ public void runWork() {\n                 jobManager.finalComplete(run);\n             } catch (HyracksException e) {\n                 // Fail the job with the caught exception during final completion.\n-                run.getExceptions().add(e);\n-                run.setStatus(JobStatus.FAILURE, run.getExceptions());\n+                List<Exception> completionException = new ArrayList<>();\n+                if (run.getExceptions() != null && !run.getExceptions().isEmpty()) {\n+                    completionException.addAll(run.getExceptions());\n+                }\n+                completionException.add(0, e);\n+                run.setStatus(JobStatus.FAILURE, completionException);\n             }\n         }\n     }",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java",
                "sha": "5bf721b394925fa162d43fe65a4a713cf461657e",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-2003][FAIL] Abort jobs failing during job start\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Prevent NPE or unmodifiable list in JobCleanupWork and\n  JobletCleanupNotificationWork.\n- Abort job if a failure happens during job start\n\nChange-Id: If6fe4ed9084270f9f22ee4b4c71936d679c8b883\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1904\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Yingyi Bu <buyingyi@gmail.com>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>",
        "parent": "https://github.com/apache/asterixdb/commit/0d0a113f40014384bcf54b68235581286c9b2c2b",
        "patched_files": [
            "JobManager.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "JobManagerTest.java"
        ]
    },
    "asterixdb_5952778": {
        "bug_id": "asterixdb_5952778",
        "commit": "https://github.com/apache/asterixdb/commit/595277850582341bead228ce84d411b423a8be35",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/asterixdb/blob/595277850582341bead228ce84d411b423a8be35/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java?ref=595277850582341bead228ce84d411b423a8be35",
                "deletions": 2,
                "filename": "asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "patch": "@@ -1649,14 +1649,18 @@ public void cleanup(String testCase, List<String> badtestcases) throws Exception\n             InputStream resultStream = executeQueryService(\n                     \"select dv.DataverseName from Metadata.`Dataverse` as dv order by dv.DataverseName;\",\n                     getEndpoint(Servlets.QUERY_SERVICE), OutputFormat.CLEAN_JSON);\n-            String out = IOUtils.toString(resultStream);\n+            String out = IOUtils.toString(resultStream, StandardCharsets.UTF_8);\n             ObjectMapper om = new ObjectMapper();\n             om.setConfig(om.getDeserializationConfig().with(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT));\n             JsonNode result;\n             try {\n                 result = om.readValue(out, ObjectNode.class).get(\"results\");\n             } catch (JsonMappingException e) {\n-                result = om.createArrayNode();\n+                LOGGER.warn(\"error mapping response '{}' to json\", out, e);\n+                result = null;\n+            }\n+            if (result == null) {\n+                return;\n             }\n             for (int i = 0; i < result.size(); i++) {\n                 JsonNode json = result.get(i);",
                "raw_url": "https://github.com/apache/asterixdb/raw/595277850582341bead228ce84d411b423a8be35/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "sha": "b75df995e0aac89e861e4ad6ad5741f8479a313e",
                "status": "modified"
            }
        ],
        "message": "[NO ISSUE][TEST] Avoid NPE on empty result from cleanup query\n\nChange-Id: Ic645b0f2c5ff29a6178cfd784fcbffb331386e90\nReviewed-on: https://asterix-gerrit.ics.uci.edu/2406\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: abdullah alamoudi <bamousaa@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/b227a0d37893831f94a971cc116600860d8d23d7",
        "patched_files": [],
        "repo": "asterixdb",
        "unit_tests": [
            "TestExecutor.java"
        ]
    },
    "asterixdb_8026b2d": {
        "bug_id": "asterixdb_8026b2d",
        "commit": "https://github.com/apache/asterixdb/commit/8026b2dc95b0b48949b192409be97ec40ac3fa2d",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/asterixdb/blob/8026b2dc95b0b48949b192409be97ec40ac3fa2d/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java?ref=8026b2dc95b0b48949b192409be97ec40ac3fa2d",
                "deletions": 12,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java",
                "patch": "@@ -296,7 +296,7 @@ private synchronized void startRecoveryRedoPhase(Set<Integer> partitions, ILogRe\n                 ((INcApplicationContext) (serviceCtx.getApplicationContext())).getIndexCheckpointManagerProvider();\n \n         Map<Long, LocalResource> resourcesMap = localResourceRepository.loadAndGetAllResources();\n-        Map<Long, Long> resourceId2MaxLSNMap = new HashMap<>();\n+        final Map<Long, Long> resourceId2MaxLSNMap = new HashMap<>();\n         TxnEntityId tempKeyTxnEntityId = new TxnEntityId(-1, -1, -1, null, -1, false);\n \n         ILogRecord logRecord = null;\n@@ -399,19 +399,25 @@ private synchronized void startRecoveryRedoPhase(Set<Integer> partitions, ILogRe\n                             // we only need to flush open indexes here (opened by previous update records)\n                             // if an index has no ongoing updates, then it's memory component must be empty\n                             // and there is nothing to flush\n-                            for (IndexInfo iInfo : dsInfo.getIndexes().values()) {\n+                            for (final IndexInfo iInfo : dsInfo.getIndexes().values()) {\n                                 if (iInfo.isOpen() && iInfo.getPartition() == partition) {\n-                                    maxDiskLastLsn = resourceId2MaxLSNMap.get(iInfo.getResourceId());\n-                                    index = iInfo.getIndex();\n-                                    if (logRecord.getLSN() > maxDiskLastLsn\n-                                            && !index.isCurrentMutableComponentEmpty()) {\n-                                        // schedule flush\n-                                        redoFlush(index, logRecord);\n-                                        redoCount++;\n+                                    Long maxLsnBeforeFlush = resourceId2MaxLSNMap.get(iInfo.getResourceId());\n+                                    if (maxLsnBeforeFlush != null) {\n+                                        // If there was at least one update to the resource.\n+                                        // IMPORTANT: Don't remove the check above\n+                                        // This check is to support indexes without transaction logs\n+                                        maxDiskLastLsn = maxLsnBeforeFlush;\n+                                        index = iInfo.getIndex();\n+                                        if (logRecord.getLSN() > maxDiskLastLsn\n+                                                && !index.isCurrentMutableComponentEmpty()) {\n+                                            // schedule flush\n+                                            redoFlush(index, logRecord);\n+                                            redoCount++;\n+                                        } else {\n+                                            // TODO: update checkpoint file?\n+                                        }\n                                     } else {\n-                                        // otherwise, do nothing since this component had no records when flush was\n-                                        // scheduled.. TODO: update checkpoint file? and do the\n-                                        // lsn checks from the checkpoint file\n+                                        // TODO: update checkpoint file?\n                                     }\n                                 }\n                             }",
                "raw_url": "https://github.com/apache/asterixdb/raw/8026b2dc95b0b48949b192409be97ec40ac3fa2d/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/nc/RecoveryManager.java",
                "sha": "adf9960150c56b34814e4291f41dcfde8c30af70",
                "status": "modified"
            }
        ],
        "message": "[NO ISSUE][STO] Skip flush recovery of empty resources\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Before this change, recovery would throw a NullPointerException\n  on recovery of a flush operation on a component without\n  update logs.\n- Since this can happen, we simply check for the case and skip the\n  flush.\n\nChange-Id: Ib01d7513f43830109632760860d34ca3dcddeaee\nReviewed-on: https://asterix-gerrit.ics.uci.edu/2844\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: abdullah alamoudi <bamousaa@gmail.com>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Murtadha Hubail <mhubail@apache.org>",
        "parent": "https://github.com/apache/asterixdb/commit/6cd3127c4e492b73c29f81763fae70099f30e20e",
        "patched_files": [
            "RecoveryManager.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "RecoveryManagerTest.java"
        ]
    },
    "asterixdb_84ddcb9": {
        "bug_id": "asterixdb_84ddcb9",
        "commit": "https://github.com/apache/asterixdb/commit/84ddcb998e908cf4f8f27275aa6855063c14b3f0",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/asterixdb/blob/84ddcb998e908cf4f8f27275aa6855063c14b3f0/hyracks/hyracks-examples/hyracks-integration-tests/src/test/java/org/apache/hyracks/tests/integration/AbstractIntegrationTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks/hyracks-examples/hyracks-integration-tests/src/test/java/org/apache/hyracks/tests/integration/AbstractIntegrationTest.java?ref=84ddcb998e908cf4f8f27275aa6855063c14b3f0",
                "deletions": 3,
                "filename": "hyracks/hyracks-examples/hyracks-integration-tests/src/test/java/org/apache/hyracks/tests/integration/AbstractIntegrationTest.java",
                "patch": "@@ -145,7 +145,6 @@ protected void runTest(JobSpecification spec) throws Exception {\n         hcc.waitForCompletion(jobId);\n     }\n \n-\n     protected List<String> readResults(JobSpecification spec, JobId jobId, ResultSetId resultSetId) throws Exception {\n         int nReaders = 1;\n \n@@ -190,13 +189,18 @@ protected boolean runTestAndCompareResults(JobSpecification spec, String[] expec\n             results = readResults(spec, jobId, spec.getResultSetIds().get(i));\n             BufferedReader expectedFile = new BufferedReader(new FileReader(expectedFileNames[i]));\n \n+            //We're expecting some sort of result.\n+            Assert.assertTrue(results != null);\n+            Assert.assertTrue(results.size() > 0);\n+\n             String expectedLine, actualLine;\n             int j = 0;\n             while ((expectedLine = expectedFile.readLine()) != null) {\n                 actualLine = results.get(j).trim();\n                 Assert.assertEquals(expectedLine, actualLine);\n                 j++;\n             }\n+            //We also expect the same amount of results.\n             Assert.assertEquals(j, results.size());\n             expectedFile.close();\n         }\n@@ -212,7 +216,7 @@ protected void runTestAndStoreResult(JobSpecification spec, File file) throws Ex\n         List<String> results;\n         for (int i = 0; i < spec.getResultSetIds().size(); i++) {\n             results = readResults(spec, jobId, spec.getResultSetIds().get(i));\n-            for(String str : results) {\n+            for (String str : results) {\n                 output.write(str);\n             }\n         }\n@@ -229,4 +233,4 @@ protected File createTempFile() throws IOException {\n         outputFiles.add(tempFile);\n         return tempFile;\n     }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/asterixdb/raw/84ddcb998e908cf4f8f27275aa6855063c14b3f0/hyracks/hyracks-examples/hyracks-integration-tests/src/test/java/org/apache/hyracks/tests/integration/AbstractIntegrationTest.java",
                "sha": "7a339b774c393ab10d8df1f01b11c56961d45d13",
                "status": "modified"
            }
        ],
        "message": "Fix for ASTERIXDB-1200\n\nFixes an issue where in the Hyracks integration tests,\nif the result size is 0, a NPE is thrown instead of\nan assert passing or failing.\n\nChange-Id: Ib519882b9cbca941addcd66232c176a2eaeecc4b\nReviewed-on: https://asterix-gerrit.ics.uci.edu/524\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Till Westmann <tillw@apache.org>",
        "parent": "https://github.com/apache/asterixdb/commit/c10677f36fd1de1ce1a984419e29718a1cd82f55",
        "patched_files": [],
        "repo": "asterixdb",
        "unit_tests": [
            "AbstractIntegrationTest.java"
        ]
    },
    "asterixdb_98de3eb": {
        "bug_id": "asterixdb_98de3eb",
        "commit": "https://github.com/apache/asterixdb/commit/98de3eb14558ee64da8f2bab4a840dab119df401",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/asterixdb/blob/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/active/ActiveEntityEventsListener.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/active/ActiveEntityEventsListener.java?ref=98de3eb14558ee64da8f2bab4a840dab119df401",
                "deletions": 2,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/app/active/ActiveEntityEventsListener.java",
                "patch": "@@ -21,7 +21,6 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.EnumSet;\n-import java.util.HashMap;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.ExecutorService;\n@@ -112,7 +111,6 @@ public ActiveEntityEventsListener(IStatementExecutor statementExecutor, ICcAppli\n         this.appCtx = appCtx;\n         this.clusterStateManager = appCtx.getClusterStateManager();\n         this.metadataProvider = new MetadataProvider(appCtx, null);\n-        metadataProvider.setConfig(new HashMap<>());\n         this.hcc = hcc;\n         this.entityId = entityId;\n         this.datasets = datasets;",
                "raw_url": "https://github.com/apache/asterixdb/raw/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/active/ActiveEntityEventsListener.java",
                "sha": "e30272cbf519c3d4b2dffde50644aff2177146b5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/asterixdb/blob/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java?ref=98de3eb14558ee64da8f2bab4a840dab119df401",
                "deletions": 2,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java",
                "patch": "@@ -261,13 +261,13 @@ public void compileAndExecute(IHyracksClientConnection hcc, IHyracksDataset hdc,\n         FileSplit outputFile = null;\n         IAWriterFactory writerFactory = PrinterBasedWriterFactory.INSTANCE;\n         IResultSerializerFactoryProvider resultSerializerFactoryProvider = ResultSerializerFactoryProvider.INSTANCE;\n-        Map<String, String> config = new HashMap<>();\n         /* Since the system runs a large number of threads, when HTTP requests don't return, it becomes difficult to\n          * find the thread running the request to determine where it has stopped.\n          * Setting the thread name helps make that easier\n          */\n         String threadName = Thread.currentThread().getName();\n         Thread.currentThread().setName(QueryTranslator.class.getSimpleName());\n+        Map<String, String> config = new HashMap<>();\n         try {\n             for (Statement stmt : statements) {\n                 if (sessionConfig.is(SessionConfig.FORMAT_HTML)) {\n@@ -276,10 +276,10 @@ public void compileAndExecute(IHyracksClientConnection hcc, IHyracksDataset hdc,\n                 validateOperation(appCtx, activeDataverse, stmt);\n                 rewriteStatement(stmt); // Rewrite the statement's AST.\n                 MetadataProvider metadataProvider = new MetadataProvider(appCtx, activeDataverse);\n+                metadataProvider.getConfig().putAll(config);\n                 metadataProvider.setWriterFactory(writerFactory);\n                 metadataProvider.setResultSerializerFactoryProvider(resultSerializerFactoryProvider);\n                 metadataProvider.setOutputFile(outputFile);\n-                metadataProvider.setConfig(config);\n                 switch (stmt.getKind()) {\n                     case Statement.Kind.SET:\n                         handleSetStatement(stmt, config);",
                "raw_url": "https://github.com/apache/asterixdb/raw/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java",
                "sha": "b97c0146ee4dfc70b86f1a17cb5e801a74ed449d",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/asterixdb/blob/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/active/ActiveEventsListenerTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/active/ActiveEventsListenerTest.java?ref=98de3eb14558ee64da8f2bab4a840dab119df401",
                "deletions": 5,
                "filename": "asterixdb/asterix-app/src/test/java/org/apache/asterix/test/active/ActiveEventsListenerTest.java",
                "patch": "@@ -21,7 +21,6 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.EnumSet;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n@@ -117,7 +116,6 @@ public void setUp() throws Exception {\n         Mockito.when(ccService.getExecutor()).thenReturn(executor);\n         locations = new AlgebricksAbsolutePartitionConstraint(nodes);\n         metadataProvider = new MetadataProvider(appCtx, null);\n-        metadataProvider.setConfig(new HashMap<>());\n         clusterController = new TestClusterControllerActor(\"CC\", handler, allDatasets);\n         nodeControllers = new TestNodeControllerActor[2];\n         nodeControllers[0] = new TestNodeControllerActor(nodes[0], clusterController);\n@@ -133,7 +131,6 @@ public void setUp() throws Exception {\n \n     TestUserActor newUser(String name, CcApplicationContext appCtx) {\n         MetadataProvider actorMdProvider = new MetadataProvider(appCtx, null);\n-        actorMdProvider.setConfig(new HashMap<>());\n         return new TestUserActor(\"User: \" + name, actorMdProvider, clusterController);\n     }\n \n@@ -1387,8 +1384,6 @@ public void testSuspendedAllActivities() throws Exception {\n             Mockito.when(ccService.getExecutor()).thenReturn(executor);\n             Mockito.when(ccAppCtx.getStorageComponentProvider()).thenReturn(componentProvider);\n             AlgebricksAbsolutePartitionConstraint locations = new AlgebricksAbsolutePartitionConstraint(nodes);\n-            MetadataProvider metadataProvider = new MetadataProvider(ccAppCtx, null);\n-            metadataProvider.setConfig(new HashMap<>());\n             additionalListeners[i] = listener = new TestEventsListener(clusterController, nodeControllers, jobIdFactory,\n                     entityId, new ArrayList<>(allDatasets), statementExecutor, ccAppCtx, hcc, locations,\n                     new InfiniteRetryPolicyFactory());",
                "raw_url": "https://github.com/apache/asterixdb/raw/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/active/ActiveEventsListenerTest.java",
                "sha": "a256bcf4eb8967c07e1991c217a73440ff86d200",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/asterixdb/blob/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/MetadataProvider.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/MetadataProvider.java?ref=98de3eb14558ee64da8f2bab4a840dab119df401",
                "deletions": 5,
                "filename": "asterixdb/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/MetadataProvider.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -143,10 +144,10 @@\n     private final StorageProperties storageProperties;\n     private final Dataverse defaultDataverse;\n     private final LockList locks;\n+    private final Map<String, String> config;\n \n     private MetadataTransactionContext mdTxnCtx;\n     private boolean isWriteTransaction;\n-    private Map<String, String> config;\n     private IAWriterFactory writerFactory;\n     private FileSplit outputFile;\n     private boolean asyncResults;\n@@ -163,16 +164,13 @@ public MetadataProvider(ICcApplicationContext appCtx, Dataverse defaultDataverse\n         this.storageComponentProvider = appCtx.getStorageComponentProvider();\n         storageProperties = appCtx.getStorageProperties();\n         locks = new LockList();\n+        config = new HashMap<>();\n     }\n \n     public String getPropertyValue(String propertyName) {\n         return config.get(propertyName);\n     }\n \n-    public void setConfig(Map<String, String> config) {\n-        this.config = config;\n-    }\n-\n     public void disableBlockingOperator() {\n         blockingOperatorDisabled = true;\n     }",
                "raw_url": "https://github.com/apache/asterixdb/raw/98de3eb14558ee64da8f2bab4a840dab119df401/asterixdb/asterix-metadata/src/main/java/org/apache/asterix/metadata/declared/MetadataProvider.java",
                "sha": "8971a9064294a60c88a1476458a62c35a109def1",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-2036] Make MetadataProvider Config final\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\ndetails:\n- Make config map final to avoid NullPointerExceptions\n\nChange-Id: I25ed6433a4e1a267deeedbf22d09c119704e8d7d\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1937\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Xikui Wang <xkkwww@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/122a73693e4a6a2484936ed86967e843d4dfa4e1",
        "patched_files": [
            "QueryTranslator.java",
            "MetadataProvider.java",
            "ActiveEntityEventsListener.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "ActiveEventsListenerTest.java",
            "QueryTranslatorTest.java"
        ]
    },
    "asterixdb_9d63f62": {
        "bug_id": "asterixdb_9d63f62",
        "commit": "https://github.com/apache/asterixdb/commit/9d63f629aedd21e891a892d18b22811586da1818",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java",
                "patch": "@@ -2138,6 +2138,12 @@ private void handleConnectFeedStatement(MetadataProvider metadataProvider, State\n             ARecordType outputType = FeedMetadataUtil.getOutputType(feed, feed.getAdapterConfiguration(),\n                     ExternalDataConstants.KEY_TYPE_NAME);\n             List<FunctionSignature> appliedFunctions = cfs.getAppliedFunctions();\n+            for (FunctionSignature func : appliedFunctions) {\n+                if (MetadataManager.INSTANCE.getFunction(mdTxnCtx, func) == null) {\n+                    throw new CompilationException(ErrorCode.FEED_CONNECT_FEED_APPLIED_INVALID_FUNCTION,\n+                            func.getName());\n+                }\n+            }\n             fc = MetadataManager.INSTANCE.getFeedConnection(metadataProvider.getMetadataTxnContext(), dataverseName,\n                     feedName, datasetName);\n             if (fc != null) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/translator/QueryTranslator.java",
                "sha": "6a2b4e0bef609b8dfd0f89c4c0448f345592f995",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.1.ddl.aql",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.1.ddl.aql?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.1.ddl.aql",
                "patch": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+drop dataverse externallibtest if exists;\n+create dataverse externallibtest;\n+use dataverse externallibtest;\n+\n+create type TweetInputType as closed {\n+  id: string,\n+  username : string,\n+  location : string,\n+  text : string,\n+  timestamp : string\n+}\n+\n+create feed TweetFeed\n+using localfs\n+((\"type-name\"=\"TweetInputType\"),\n+(\"path\"=\"asterix_nc1://data/twitter/obamatweets.adm\"),\n+(\"format\"=\"adm\"));\n+\n+create dataset TweetsFeedIngest(TweetInputType)\n+primary key id;",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.1.ddl.aql",
                "sha": "d294772f40f1b318fa695b8675b1614570b50424",
                "status": "added"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.2.update.aql",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.2.update.aql?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.2.update.aql",
                "patch": "@@ -0,0 +1,26 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+use dataverse externallibtest;\n+\n+set wait-for-completion-feed \"true\";\n+\n+connect feed TweetFeed to dataset TweetsFeedIngest\n+apply function function_undefined;\n+\n+start feed TweetFeed;\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/queries/feeds/feed-with-undefined-function/feed-with-undefined-function.2.update.aql",
                "sha": "a2a00babf6fcd35188c8ce90c2721008b468fee6",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/results/feeds/feed-with-undefined-function/feed-with-external-function.1.adm",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/resources/runtimets/results/feeds/feed-with-undefined-function/feed-with-external-function.1.adm?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/test/resources/runtimets/results/feeds/feed-with-undefined-function/feed-with-external-function.1.adm",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/results/feeds/feed-with-undefined-function/feed-with-external-function.1.adm",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "status": "added"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/testsuite.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/resources/runtimets/testsuite.xml?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/test/resources/runtimets/testsuite.xml",
                "patch": "@@ -273,6 +273,12 @@\n         <output-dir compare=\"Text\">record-reader-with-malformed-input-stream</output-dir>\n       </compilation-unit>\n     </test-case>\n+    <test-case FilePath=\"feeds\">\n+      <compilation-unit name=\"feed-with-undefined-function\">\n+        <output-dir compare=\"Text\">feed-with-undefined-function</output-dir>\n+        <expected-error>Cannot find function</expected-error>\n+      </compilation-unit>\n+    </test-case>\n   </test-group>\n   <test-group name=\"upsert\">\n     <test-case FilePath=\"upsert\">",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-app/src/test/resources/runtimets/testsuite.xml",
                "sha": "83d55813439079bf9331a6f02574d084bc95f19e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-common/src/main/java/org/apache/asterix/common/exceptions/ErrorCode.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-common/src/main/java/org/apache/asterix/common/exceptions/ErrorCode.java?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 0,
                "filename": "asterixdb/asterix-common/src/main/java/org/apache/asterix/common/exceptions/ErrorCode.java",
                "patch": "@@ -179,6 +179,7 @@\n     public static final int FEED_METADATA_UTIL_UNEXPECTED_FEED_DATATYPE = 3080;\n     public static final int FEED_METADATA_SOCKET_ADAPTOR_SOCKET_NOT_PROPERLY_CONFIGURED = 3081;\n     public static final int FEED_METADATA_SOCKET_ADAPTOR_SOCKET_INVALID_HOST_NC = 3082;\n+    public static final int FEED_CONNECT_FEED_APPLIED_INVALID_FUNCTION = 3087;\n \n     private ErrorCode() {\n     }",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-common/src/main/java/org/apache/asterix/common/exceptions/ErrorCode.java",
                "sha": "23d67278de9eee563afc5a1d786cf64ed36d1efe",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-common/src/main/resources/asx_errormsg/en.properties",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-common/src/main/resources/asx_errormsg/en.properties?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 1,
                "filename": "asterixdb/asterix-common/src/main/resources/asx_errormsg/en.properties",
                "patch": "@@ -167,4 +167,5 @@\n 3079 = Cannot register runtime, active manager has been shutdown\n 3080 = Unexpected feed datatype '%1$s'\n 3081 = socket is not properly configured\n-3082 = \"Invalid %1$s %2$s as it is not part of the AsterixDB cluster. Valid choices are %3$s\"\n\\ No newline at end of file\n+3082 = \"Invalid %1$s %2$s as it is not part of the AsterixDB cluster. Valid choices are %3$s\"\n+3087 = Cannot find function %1$s\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-common/src/main/resources/asx_errormsg/en.properties",
                "sha": "e1a54cf7024b5c5bc1db6db694a8f115cc8bc771",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-lang-aql/src/main/javacc/AQL.jj",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-lang-aql/src/main/javacc/AQL.jj?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 7,
                "filename": "asterixdb/asterix-lang-aql/src/main/javacc/AQL.jj",
                "patch": "@@ -818,11 +818,10 @@ boolean IfNotExists() throws ParseException:\n     }\n }\n \n-List<FunctionSignature> ApplyFunction() throws ParseException:\n+void ApplyFunction(List<FunctionSignature> funcSigs) throws ParseException:\n {\n   FunctionName functioName = null;\n   String fqFunctionName = null;\n-  List<FunctionSignature> funcSigs = new ArrayList<FunctionSignature>();\n }\n {\n   <APPLY> <FUNCTION> functioName = FunctionName()\n@@ -837,9 +836,6 @@ List<FunctionSignature> ApplyFunction() throws ParseException:\n       funcSigs.add(new FunctionSignature(functioName.dataverse, fqFunctionName, 1));\n     }\n   )*\n-    {\n-        return funcSigs;\n-    }\n }\n \n String GetPolicy() throws ParseException:\n@@ -1167,14 +1163,14 @@ Statement FeedStatement() throws ParseException:\n   Pair<Identifier,Identifier> datasetNameComponents = null;\n \n   Map<String,String> configuration = null;\n-  List<FunctionSignature> appliedFunctions = null;\n+  List<FunctionSignature> appliedFunctions = new ArrayList<FunctionSignature>();\n   Statement stmt = null;\n   String policy = null;\n }\n {\n   (\n     <CONNECT> <FEED> feedNameComponents = QualifiedName() <TO> <DATASET> datasetNameComponents = QualifiedName()\n-    (appliedFunctions = ApplyFunction())? (policy = GetPolicy())?\n+    (ApplyFunction(appliedFunctions))? (policy = GetPolicy())?\n       {\n         stmt = new ConnectFeedStatement(feedNameComponents, datasetNameComponents, appliedFunctions, policy, getVarCounter());\n       }",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-lang-aql/src/main/javacc/AQL.jj",
                "sha": "b92805b76962166fe3ee60ef12fae19f884e3421",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/asterixdb/blob/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-lang-sqlpp/src/main/javacc/SQLPP.jj",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-lang-sqlpp/src/main/javacc/SQLPP.jj?ref=9d63f629aedd21e891a892d18b22811586da1818",
                "deletions": 7,
                "filename": "asterixdb/asterix-lang-sqlpp/src/main/javacc/SQLPP.jj",
                "patch": "@@ -840,11 +840,10 @@ boolean IfNotExists() throws ParseException:\n     }\n }\n \n-List<FunctionSignature> ApplyFunction() throws ParseException:\n+void ApplyFunction(List<FunctionSignature> funcSigs) throws ParseException:\n {\n   FunctionName functioName = null;\n   String fqFunctionName = null;\n-  List<FunctionSignature> funcSigs = new ArrayList<FunctionSignature>();\n }\n {\n   <APPLY> <FUNCTION> functioName = FunctionName()\n@@ -859,9 +858,6 @@ List<FunctionSignature> ApplyFunction() throws ParseException:\n         funcSigs.add(new FunctionSignature(functioName.dataverse, fqFunctionName, 1));\n       }\n   )*\n-    {\n-        return funcSigs;\n-    }\n }\n \n String GetPolicy() throws ParseException:\n@@ -1254,14 +1250,14 @@ Statement ConnectStatement() throws ParseException:\n   Pair<Identifier,Identifier> datasetNameComponents = null;\n \n   Map<String,String> configuration = null;\n-  List<FunctionSignature> appliedFunctions = null;\n+  List<FunctionSignature> appliedFunctions = new ArrayList<FunctionSignature>();\n   Statement stmt = null;\n   String policy = null;\n }\n {\n   (\n     <FEED> feedNameComponents = QualifiedName() <TO> Dataset() datasetNameComponents = QualifiedName()\n-    (appliedFunctions = ApplyFunction())?  (policy = GetPolicy())?\n+    (ApplyFunction(appliedFunctions))?  (policy = GetPolicy())?\n       {\n         stmt = new ConnectFeedStatement(feedNameComponents, datasetNameComponents, appliedFunctions,\n          policy, getVarCounter());",
                "raw_url": "https://github.com/apache/asterixdb/raw/9d63f629aedd21e891a892d18b22811586da1818/asterixdb/asterix-lang-sqlpp/src/main/javacc/SQLPP.jj",
                "sha": "69bfbc5242aa0c5e15f93e3429b32e556f34cdcc",
                "status": "modified"
            }
        ],
        "message": "Add function signature check to Connect Feed\n\n1. Revise the exception info when apply an unknown function to feed.\n2. Fix the possible NPE in connect feed statement.\n3. Add test case for applying undefined function.\n\nChange-Id: I1462b394d84ea7e1eae5a03f98fe8cd39213eb8e\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1674\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: abdullah alamoudi <bamousaa@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/34fc9ef89967fdccdda8d31d571b0f26ae39f569",
        "patched_files": [
            "feed-with-undefined-function.2.update.aql",
            "feed-with-external-function.1.adm",
            "testsuite.xml",
            "feed-with-undefined-function.1.ddl.aql",
            "en.properties",
            "SQLPP.jj",
            "QueryTranslator.java",
            "AQL.jj"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "QueryTranslatorTest.java"
        ]
    },
    "asterixdb_d237f0c": {
        "bug_id": "asterixdb_d237f0c",
        "commit": "https://github.com/apache/asterixdb/commit/d237f0c22db8ee316d553b2254f711f328d5aff8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java",
                "patch": "@@ -54,7 +54,7 @@ public void write(IConnectionWriterState writerState) throws NetException {\n             ecodeSent = true;\n             ccb.reportLocalEOS();\n             adjustChannelWritability();\n-        } else if (eos && !eosSent) {\n+        } else if (isPendingCloseWrite()) {\n             writerState.getCommand().setChannelId(channelId);\n             writerState.getCommand().setCommandType(MuxDemuxCommand.CommandType.CLOSE_CHANNEL);\n             writerState.getCommand().setData(0);",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java",
                "sha": "84f7831d496d428343a381f8aeceee0c244c0e93",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java",
                "patch": "@@ -107,6 +107,7 @@ public synchronized IPartition getPartition(PartitionId pid) {\n     public synchronized void registerPartitionRequest(PartitionId partitionId, NetworkOutputChannel writer) {\n         if (failedJobsCache.getIfPresent(partitionId.getJobId()) != null) {\n             writer.abort(AbstractChannelWriteInterface.REMOTE_ERROR_CODE);\n+            return;\n         }\n         List<IPartition> pList = availablePartitionMap.get(partitionId);\n         if (pList != null && !pList.isEmpty()) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java",
                "sha": "7c8fb3471a0d2465ca6c9ce6d7a2741bfcd4a713",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 1,
                "filename": "hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java",
                "patch": "@@ -75,7 +75,7 @@ private boolean computeWritability() {\n         if (writableDataPresent) {\n             return credits > 0;\n         }\n-        if (eos && !eosSent) {\n+        if (isPendingCloseWrite()) {\n             return true;\n         }\n         return ecode.get() == REMOTE_ERROR_CODE && !ecodeSent;\n@@ -116,6 +116,10 @@ public int getCredits() {\n         return credits;\n     }\n \n+    protected boolean isPendingCloseWrite() {\n+        return eos && !eosSent && !ecodeSent;\n+    }\n+\n     private class CloseableBufferAcceptor implements ICloseableBufferAcceptor {\n         @Override\n         public void accept(ByteBuffer buffer) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java",
                "sha": "5c927f95e3ca1ced89b99f2684e11b6f95f99484",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 1,
                "filename": "hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java",
                "patch": "@@ -58,7 +58,7 @@ public void write(IConnectionWriterState writerState) throws NetException {\n             ecodeSent = true;\n             ccb.reportLocalEOS();\n             adjustChannelWritability();\n-        } else if (eos && !eosSent) {\n+        } else if (isPendingCloseWrite()) {\n             writerState.getCommand().setChannelId(channelId);\n             writerState.getCommand().setCommandType(MuxDemuxCommand.CommandType.CLOSE_CHANNEL);\n             writerState.getCommand().setData(0);",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java",
                "sha": "3f4618bdc9728293345a1688c57975c586dc3a2d",
                "status": "modified"
            }
        ],
        "message": "[NO ISSUE][NET] Ensure CLOSE Is Not Sent After Channel ERROR\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Currently it is possible to send network channel\n  CLOSE command after a channel ERROR was sent. When this\n  happens and the channel was recycled to be reused\n  on the receiver side, the CLOSE command will result\n  in NPE. There is no need to send a CLOSE command\n  after an ERROR command because when an ERROR command\n  is received, it is treated as ERROR + CLOSE on the\n  receiver side.\n- Avoid registering partition requests for failed jobs.\n\nChange-Id: I17a769a46f4d13220adb22dd255e56dc4ccc458d\nReviewed-on: https://asterix-gerrit.ics.uci.edu/2954\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Murtadha Hubail <mhubail@apache.org>\nReviewed-by: Michael Blow <mblow@apache.org>",
        "parent": "https://github.com/apache/asterixdb/commit/3a6846942cde2f6e97ab53e01e14a7025ca04814",
        "patched_files": [
            "PartitionManager.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "PartitionManagerTest.java"
        ]
    },
    "asterixdb_e0e85a3": {
        "bug_id": "asterixdb_e0e85a3",
        "commit": "https://github.com/apache/asterixdb/commit/e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java",
                "patch": "@@ -123,7 +123,7 @@ public void start(IServiceContext serviceCtx, String[] args) throws Exception {\n                 .create(ClusterProperties.INSTANCE.getCluster(), repStrategy, ccServiceCtx);\n         ExternalLibraryUtils.setUpExternaLibraries(libraryManager, false);\n         componentProvider = new StorageComponentProvider();\n-        GlobalRecoveryManager.instantiate((HyracksConnection) getHcc(), componentProvider);\n+        GlobalRecoveryManager.instantiate(ccServiceCtx, getHcc(), componentProvider);\n         appCtx = new CcApplicationContext(ccServiceCtx, getHcc(), libraryManager, resourceIdManager,\n                 () -> MetadataManager.INSTANCE, GlobalRecoveryManager.instance(), ftStrategy,\n                 new ActiveLifecycleListener());",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java",
                "sha": "bf7d5ebfa66547f29f4c653c2d0bd0e819fa3a1d",
                "status": "modified"
            },
            {
                "additions": 126,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java",
                "changes": 250,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 124,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java",
                "patch": "@@ -45,7 +45,8 @@\n import org.apache.asterix.metadata.utils.ExternalIndexingOperations;\n import org.apache.asterix.metadata.utils.MetadataConstants;\n import org.apache.asterix.runtime.utils.ClusterStateManager;\n-import org.apache.hyracks.api.client.HyracksConnection;\n+import org.apache.hyracks.api.application.ICCServiceContext;\n+import org.apache.hyracks.api.client.IHyracksClientConnection;\n import org.apache.hyracks.api.job.JobId;\n import org.apache.hyracks.api.job.JobSpecification;\n \n@@ -55,10 +56,13 @@\n     private static GlobalRecoveryManager instance;\n     private static ClusterState state;\n     private final IStorageComponentProvider componentProvider;\n-    private HyracksConnection hcc;\n+    private final ICCServiceContext ccServiceCtx;\n+    private IHyracksClientConnection hcc;\n \n-    private GlobalRecoveryManager(HyracksConnection hcc, IStorageComponentProvider componentProvider) {\n+    private GlobalRecoveryManager(ICCServiceContext ccServiceCtx, IHyracksClientConnection hcc,\n+                                  IStorageComponentProvider componentProvider) {\n         setState(ClusterState.UNUSABLE);\n+        this.ccServiceCtx = ccServiceCtx;\n         this.hcc = hcc;\n         this.componentProvider = componentProvider;\n     }\n@@ -97,142 +101,140 @@ public void startGlobalRecovery(ICcApplicationContext appCtx) {\n         final ClusterState newState = ClusterStateManager.INSTANCE.getState();\n         boolean needToRecover = !newState.equals(state) && (newState == ClusterState.ACTIVE);\n         if (needToRecover) {\n-            Thread recoveryThread = new Thread(new Runnable() {\n-                @Override\n-                public void run() {\n-                    LOGGER.info(\"Starting AsterixDB's Global Recovery\");\n-                    MetadataTransactionContext mdTxnCtx = null;\n-                    try {\n-                        Thread.sleep(4000);\n-                        MetadataManager.INSTANCE.init();\n-                        // Loop over datasets\n-                        mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n-                        List<Dataverse> dataverses = MetadataManager.INSTANCE.getDataverses(mdTxnCtx);\n-                        for (Dataverse dataverse : dataverses) {\n-                            if (!dataverse.getDataverseName().equals(MetadataConstants.METADATA_DATAVERSE_NAME)) {\n-                                MetadataProvider metadataProvider =\n-                                        new MetadataProvider(appCtx, dataverse, componentProvider);\n-                                try {\n-                                    List<Dataset> datasets = MetadataManager.INSTANCE.getDataverseDatasets(mdTxnCtx,\n-                                            dataverse.getDataverseName());\n-                                    for (Dataset dataset : datasets) {\n-                                        if (dataset.getDatasetType() == DatasetType.EXTERNAL) {\n-                                            // External dataset\n-                                            // Get indexes\n-                                            List<Index> indexes = MetadataManager.INSTANCE.getDatasetIndexes(mdTxnCtx,\n-                                                    dataset.getDataverseName(), dataset.getDatasetName());\n-                                            // Get the state of the dataset\n-                                            ExternalDatasetDetails dsd =\n-                                                    (ExternalDatasetDetails) dataset.getDatasetDetails();\n-                                            TransactionState datasetState = dsd.getState();\n-                                            if (!indexes.isEmpty()) {\n-                                                if (datasetState == TransactionState.BEGIN) {\n-                                                    List<ExternalFile> files = MetadataManager.INSTANCE\n-                                                            .getDatasetExternalFiles(mdTxnCtx, dataset);\n-                                                    // if persumed abort, roll backward\n-                                                    // 1. delete all pending files\n-                                                    for (ExternalFile file : files) {\n-                                                        if (file.getPendingOp() != ExternalFilePendingOp.NO_OP) {\n-                                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n-                                                        }\n-                                                    }\n-                                                }\n-                                                // 2. clean artifacts in NCs\n-                                                metadataProvider.setMetadataTxnContext(mdTxnCtx);\n-                                                JobSpecification jobSpec = ExternalIndexingOperations\n-                                                        .buildAbortOp(dataset, indexes, metadataProvider);\n-                                                executeHyracksJob(jobSpec);\n-                                                // 3. correct the dataset state\n-                                                ((ExternalDatasetDetails) dataset.getDatasetDetails())\n-                                                        .setState(TransactionState.COMMIT);\n-                                                MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n-                                                MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n-                                                mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n-                                            } else if (datasetState == TransactionState.READY_TO_COMMIT) {\n-                                                List<ExternalFile> files = MetadataManager.INSTANCE\n-                                                        .getDatasetExternalFiles(mdTxnCtx, dataset);\n-                                                // if ready to commit, roll forward\n-                                                // 1. commit indexes in NCs\n-                                                metadataProvider.setMetadataTxnContext(mdTxnCtx);\n-                                                JobSpecification jobSpec = ExternalIndexingOperations\n-                                                        .buildRecoverOp(dataset, indexes, metadataProvider);\n-                                                executeHyracksJob(jobSpec);\n-                                                // 2. add pending files in metadata\n-                                                for (ExternalFile file : files) {\n-                                                    if (file.getPendingOp() == ExternalFilePendingOp.ADD_OP) {\n-                                                        MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n-                                                        file.setPendingOp(ExternalFilePendingOp.NO_OP);\n-                                                        MetadataManager.INSTANCE.addExternalFile(mdTxnCtx, file);\n-                                                    } else if (file.getPendingOp() == ExternalFilePendingOp.DROP_OP) {\n-                                                        // find original file\n-                                                        for (ExternalFile originalFile : files) {\n-                                                            if (originalFile.getFileName().equals(file.getFileName())) {\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        file);\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        originalFile);\n-                                                                break;\n-                                                            }\n-                                                        }\n-                                                    } else if (file.getPendingOp() == ExternalFilePendingOp.APPEND_OP) {\n-                                                        // find original file\n-                                                        for (ExternalFile originalFile : files) {\n-                                                            if (originalFile.getFileName().equals(file.getFileName())) {\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        file);\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        originalFile);\n-                                                                originalFile.setSize(file.getSize());\n-                                                                MetadataManager.INSTANCE.addExternalFile(mdTxnCtx,\n-                                                                        originalFile);\n-                                                            }\n-                                                        }\n-                                                    }\n-                                                    // 3. correct the dataset state\n-                                                    ((ExternalDatasetDetails) dataset.getDatasetDetails())\n-                                                            .setState(TransactionState.COMMIT);\n-                                                    MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n-                                                    MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n-                                                    mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n-                                                }\n-                                            }\n-                                        }\n-                                    }\n-                                } finally {\n-                                    metadataProvider.getLocks().unlock();\n-                                }\n-                            }\n-                        }\n-                        MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n-                    } catch (Exception e) {\n-                        // This needs to be fixed <-- Needs to shutdown the system -->\n-                        /*\n-                         * Note: Throwing this illegal state exception will terminate this thread\n-                         * and feeds listeners will not be notified.\n-                         */\n-                        LOGGER.log(Level.SEVERE, \"Global recovery was not completed successfully: \", e);\n+            setState(newState);\n+            ccServiceCtx.getControllerService().getExecutor().submit(() -> {\n+                LOGGER.info(\"Starting Global Recovery\");\n+                MetadataTransactionContext mdTxnCtx = null;\n+                try {\n+                    MetadataManager.INSTANCE.init();\n+                    // Loop over datasets\n+                    mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n+                    for (Dataverse dataverse : MetadataManager.INSTANCE.getDataverses(mdTxnCtx)) {\n+                        mdTxnCtx = recoverDataset(appCtx, mdTxnCtx, dataverse);\n+                    }\n+                    MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n+                } catch (Exception e) {\n+                    // This needs to be fixed <-- Needs to shutdown the system -->\n+                    /*\n+                     * Note: Throwing this illegal state exception will terminate this thread\n+                     * and feeds listeners will not be notified.\n+                     */\n+                    LOGGER.log(Level.SEVERE, \"Global recovery was not completed successfully: \", e);\n+                    if (mdTxnCtx != null) {\n                         try {\n                             MetadataManager.INSTANCE.abortTransaction(mdTxnCtx);\n                         } catch (Exception e1) {\n                             LOGGER.log(Level.SEVERE, \"Exception in aborting\", e1);\n+                            e1.addSuppressed(e);\n                             throw new IllegalStateException(e1);\n                         }\n                     }\n-                    ClusterStateManager.INSTANCE.setGlobalRecoveryCompleted(true);\n-                    LOGGER.info(\"Global Recovery Completed\");\n                 }\n-            }, \"RecoveryThread\");\n-            setState(newState);\n-            recoveryThread.start();\n+                ClusterStateManager.INSTANCE.setGlobalRecoveryCompleted(true);\n+                LOGGER.info(\"Global Recovery Completed\");\n+            });\n+        }\n+    }\n+\n+    private MetadataTransactionContext recoverDataset(ICcApplicationContext appCtx, MetadataTransactionContext mdTxnCtx,\n+                                                      Dataverse dataverse)\n+            throws Exception {\n+        if (!dataverse.getDataverseName().equals(MetadataConstants.METADATA_DATAVERSE_NAME)) {\n+            MetadataProvider metadataProvider = new MetadataProvider(appCtx, dataverse, componentProvider);\n+            try {\n+                List<Dataset> datasets = MetadataManager.INSTANCE.getDataverseDatasets(mdTxnCtx,\n+                        dataverse.getDataverseName());\n+                for (Dataset dataset : datasets) {\n+                    if (dataset.getDatasetType() == DatasetType.EXTERNAL) {\n+                        // External dataset\n+                        // Get indexes\n+                        List<Index> indexes = MetadataManager.INSTANCE.getDatasetIndexes(mdTxnCtx,\n+                                dataset.getDataverseName(), dataset.getDatasetName());\n+                        // Get the state of the dataset\n+                        ExternalDatasetDetails dsd = (ExternalDatasetDetails) dataset.getDatasetDetails();\n+                        TransactionState datasetState = dsd.getState();\n+                        if (!indexes.isEmpty()) {\n+                            if (datasetState == TransactionState.BEGIN) {\n+                                List<ExternalFile> files = MetadataManager.INSTANCE.getDatasetExternalFiles(mdTxnCtx,\n+                                        dataset);\n+                                // if persumed abort, roll backward\n+                                // 1. delete all pending files\n+                                for (ExternalFile file : files) {\n+                                    if (file.getPendingOp() != ExternalFilePendingOp.NO_OP) {\n+                                        MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                    }\n+                                }\n+                            }\n+                            // 2. clean artifacts in NCs\n+                            metadataProvider.setMetadataTxnContext(mdTxnCtx);\n+                            JobSpecification jobSpec = ExternalIndexingOperations.buildAbortOp(dataset, indexes,\n+                                    metadataProvider);\n+                            executeHyracksJob(jobSpec);\n+                            // 3. correct the dataset state\n+                            ((ExternalDatasetDetails) dataset.getDatasetDetails()).setState(TransactionState.COMMIT);\n+                            MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n+                            MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n+                            mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n+                        } else if (datasetState == TransactionState.READY_TO_COMMIT) {\n+                            List<ExternalFile> files = MetadataManager.INSTANCE.getDatasetExternalFiles(mdTxnCtx,\n+                                    dataset);\n+                            // if ready to commit, roll forward\n+                            // 1. commit indexes in NCs\n+                            metadataProvider.setMetadataTxnContext(mdTxnCtx);\n+                            JobSpecification jobSpec = ExternalIndexingOperations.buildRecoverOp(dataset, indexes,\n+                                    metadataProvider);\n+                            executeHyracksJob(jobSpec);\n+                            // 2. add pending files in metadata\n+                            for (ExternalFile file : files) {\n+                                if (file.getPendingOp() == ExternalFilePendingOp.ADD_OP) {\n+                                    MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                    file.setPendingOp(ExternalFilePendingOp.NO_OP);\n+                                    MetadataManager.INSTANCE.addExternalFile(mdTxnCtx, file);\n+                                } else if (file.getPendingOp() == ExternalFilePendingOp.DROP_OP) {\n+                                    // find original file\n+                                    for (ExternalFile originalFile : files) {\n+                                        if (originalFile.getFileName().equals(file.getFileName())) {\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, originalFile);\n+                                            break;\n+                                        }\n+                                    }\n+                                } else if (file.getPendingOp() == ExternalFilePendingOp.APPEND_OP) {\n+                                    // find original file\n+                                    for (ExternalFile originalFile : files) {\n+                                        if (originalFile.getFileName().equals(file.getFileName())) {\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, originalFile);\n+                                            originalFile.setSize(file.getSize());\n+                                            MetadataManager.INSTANCE.addExternalFile(mdTxnCtx, originalFile);\n+                                        }\n+                                    }\n+                                }\n+                                // 3. correct the dataset state\n+                                ((ExternalDatasetDetails) dataset.getDatasetDetails())\n+                                        .setState(TransactionState.COMMIT);\n+                                MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n+                                MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n+                                mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n+                            }\n+                        }\n+                    }\n+                }\n+            } finally {\n+                metadataProvider.getLocks().unlock();\n+            }\n         }\n+\n+        return mdTxnCtx;\n     }\n \n     public static GlobalRecoveryManager instance() {\n         return instance;\n     }\n \n-    public static synchronized void instantiate(HyracksConnection hcc, IStorageComponentProvider componentProvider) {\n-        instance = new GlobalRecoveryManager(hcc, componentProvider);\n+    public static synchronized void instantiate(ICCServiceContext ccServiceCtx, IHyracksClientConnection hcc,\n+                                                IStorageComponentProvider componentProvider) {\n+        instance = new GlobalRecoveryManager(ccServiceCtx, hcc, componentProvider);\n     }\n \n     public static synchronized void setState(ClusterState state) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java",
                "sha": "1816a25292d317ac232f7790cd7bbc6f902bafad",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 4,
                "filename": "asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "patch": "@@ -45,6 +45,7 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n import java.util.regex.Matcher;\n@@ -404,7 +405,12 @@ private static void writeOutputToFile(File actualFile, InputStream resultStream)\n     }\n \n     protected HttpResponse executeAndCheckHttpRequest(HttpUriRequest method) throws Exception {\n-        return checkResponse(executeHttpRequest(method));\n+        return checkResponse(executeHttpRequest(method), code -> code == HttpStatus.SC_OK);\n+    }\n+\n+    protected HttpResponse executeAndCheckHttpRequest(HttpUriRequest method, Predicate<Integer> responseCodeValidator)\n+            throws Exception {\n+        return checkResponse(executeHttpRequest(method), responseCodeValidator);\n     }\n \n     protected HttpResponse executeHttpRequest(HttpUriRequest method) throws Exception {\n@@ -418,8 +424,9 @@ protected HttpResponse executeHttpRequest(HttpUriRequest method) throws Exceptio\n         }\n     }\n \n-    protected HttpResponse checkResponse(HttpResponse httpResponse) throws Exception {\n-        if (httpResponse.getStatusLine().getStatusCode() != HttpStatus.SC_OK) {\n+    protected HttpResponse checkResponse(HttpResponse httpResponse, Predicate<Integer> responseCodeValidator)\n+            throws Exception {\n+        if (!responseCodeValidator.test(httpResponse.getStatusLine().getStatusCode())) {\n             String errorBody = EntityUtils.toString(httpResponse.getEntity());\n             String exceptionMsg;\n             try {\n@@ -582,8 +589,13 @@ public InputStream executeJSONGet(OutputFormat fmt, URI uri) throws Exception {\n     }\n \n     public InputStream executeJSONPost(OutputFormat fmt, URI uri) throws Exception {\n+        return executeJSONPost(fmt, uri, code -> code == HttpStatus.SC_OK);\n+    }\n+\n+    public InputStream executeJSONPost(OutputFormat fmt, URI uri, Predicate<Integer> responseCodeValidator)\n+            throws Exception {\n         HttpUriRequest request = constructPostMethod(uri, fmt, new ArrayList<>());\n-        HttpResponse response = executeAndCheckHttpRequest(request);\n+        HttpResponse response = executeAndCheckHttpRequest(request, responseCodeValidator);\n         return response.getEntity().getContent();\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "sha": "e88f64716a95b13c21ca2a1eeb347c7dbc89374f",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 12,
                "filename": "asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n+import java.util.TreeSet;\n import java.util.concurrent.TimeUnit;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n@@ -41,6 +42,7 @@\n import org.apache.hyracks.api.config.IOption;\n import org.apache.hyracks.api.exceptions.HyracksDataException;\n import org.apache.hyracks.api.exceptions.HyracksException;\n+import org.apache.hyracks.control.cc.ClusterControllerService;\n import org.apache.hyracks.control.common.controllers.NCConfig;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -303,24 +305,27 @@ public synchronized ObjectNode getClusterStateDescription() {\n         stateDescription.put(\"metadata_node\", currentMetadataNode);\n         ArrayNode ncs = om.createArrayNode();\n         stateDescription.set(\"ncs\", ncs);\n-        for (Map.Entry<String, ClusterPartition[]> entry : node2PartitionsMap.entrySet()) {\n+        for (String node : new TreeSet<>(((ClusterControllerService) appCtx.getServiceContext().getControllerService())\n+                .getNodeManager().getAllNodeIds())) {\n             ObjectNode nodeJSON = om.createObjectNode();\n-            nodeJSON.put(\"node_id\", entry.getKey());\n+            nodeJSON.put(\"node_id\", node);\n             boolean allActive = true;\n             boolean anyActive = false;\n             Set<Map<String, Object>> partitions = new HashSet<>();\n-            for (ClusterPartition part : entry.getValue()) {\n-                HashMap<String, Object> partition = new HashMap<>();\n-                partition.put(\"partition_id\", \"partition_\" + part.getPartitionId());\n-                partition.put(\"active\", part.isActive());\n-                partitions.add(partition);\n-                allActive = allActive && part.isActive();\n-                if (allActive) {\n-                    anyActive = true;\n+            if (node2PartitionsMap.containsKey(node)) {\n+                for (ClusterPartition part : node2PartitionsMap.get(node)) {\n+                    HashMap<String, Object> partition = new HashMap<>();\n+                    partition.put(\"partition_id\", \"partition_\" + part.getPartitionId());\n+                    partition.put(\"active\", part.isActive());\n+                    partitions.add(partition);\n+                    allActive = allActive && part.isActive();\n+                    if (allActive) {\n+                        anyActive = true;\n+                    }\n                 }\n             }\n-            nodeJSON.put(\"state\", failedNodes.contains(entry.getKey()) ? \"FAILED\"\n-                    : allActive ? \"ACTIVE\" : anyActive ? \"PARTIALLY_ACTIVE\" : \"INACTIVE\");\n+            nodeJSON.put(\"state\", failedNodes.contains(node) ? \"FAILED\"\n+                    : allActive && anyActive ? \"ACTIVE\" : anyActive ? \"PARTIALLY_ACTIVE\" : \"INACTIVE\");\n             nodeJSON.putPOJO(\"partitions\", partitions);\n             ncs.add(nodeJSON);\n         }",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "sha": "48937f8d9da33688e42a8fff76e7cfc24e001a64",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.hyracks.api.context.ICCContext;\n import org.apache.hyracks.api.job.IJobLifecycleListener;\n+import org.apache.hyracks.api.service.IControllerService;\n \n /**\n  * Service Context at the Cluster Controller for an application.",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java",
                "sha": "94ebcfe01cddf021d02635393f147a6af251d679",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 24,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java",
                "patch": "@@ -43,24 +43,19 @@\n     private boolean configured;\n \n     public LifeCycleComponentManager() {\n-        components = new ArrayList<ILifeCycleComponent>();\n+        components = new ArrayList<>();\n         stopInitiated = false;\n         configured = false;\n         stopped = false;\n     }\n \n     @Override\n     public void uncaughtException(Thread t, Throwable e) {\n-        if (LOGGER.isLoggable(Level.SEVERE)) {\n-            LOGGER.severe(\"Uncaught Exception from thread \" + t.getName() + \" message: \" + e.getMessage());\n-            e.printStackTrace();\n-        }\n+        LOGGER.log(Level.SEVERE, \"Uncaught Exception from thread \" + t.getName(), e);\n         try {\n             stopAll(true);\n         } catch (IOException e1) {\n-            if (LOGGER.isLoggable(Level.SEVERE)) {\n-                LOGGER.severe(\"Exception in stopping Asterix. \" + e1.getMessage());\n-            }\n+            LOGGER.log(Level.SEVERE, \"Exception in stopping instance\", e1);\n         }\n     }\n \n@@ -79,31 +74,25 @@ public void startAll() {\n     @Override\n     public synchronized void stopAll(boolean dumpState) throws IOException {\n         if (LOGGER.isLoggable(Level.INFO)) {\n-            LOGGER.severe(\"Attempting to stop \" + this);\n+            LOGGER.info(\"Attempting to stop \" + this);\n         }\n         if (stopped) {\n-            if (LOGGER.isLoggable(Level.INFO)) {\n-                LOGGER.severe(\"Lifecycle management was already stopped\");\n-            }\n+            LOGGER.info(\"Lifecycle management was already stopped\");\n             return;\n         }\n         if (stopInitiated) {\n-            if (LOGGER.isLoggable(Level.INFO)) {\n-                LOGGER.severe(\"Stop already in progress\");\n-            }\n+            LOGGER.info(\"Stop already in progress\");\n             return;\n         }\n         if (!configured) {\n             if (LOGGER.isLoggable(Level.SEVERE)) {\n-                LOGGER.severe(\"Lifecycle management not configured\" + this);\n+                LOGGER.severe(\"Lifecycle management not configured \" + this);\n             }\n             return;\n         }\n \n         stopInitiated = true;\n-        if (LOGGER.isLoggable(Level.SEVERE)) {\n-            LOGGER.severe(\"Stopping Asterix instance\");\n-        }\n+        LOGGER.severe(\"Stopping instance\");\n \n         FileOutputStream componentDumpStream = null;\n         String componentDumpPath = null;\n@@ -120,14 +109,12 @@ public synchronized void stopAll(boolean dumpState) throws IOException {\n                     componentDumpStream = new FileOutputStream(f);\n                 }\n                 if (LOGGER.isLoggable(Level.INFO)) {\n-                    LOGGER.info(\"Stopping component instance \" + component.getClass().getName() + \" dump state \"\n-                            + dumpState + \" dump path \" + componentDumpPath);\n+                    LOGGER.info(\"Stopping component instance \" + component.getClass().getName() + \"; dump state: \"\n+                            + dumpState + \", dump path: \" + componentDumpPath);\n                 }\n                 component.stop(dumpState, componentDumpStream);\n             } catch (Exception e) {\n-                if (LOGGER.isLoggable(Level.SEVERE)) {\n-                    LOGGER.severe(\"Exception in stopping component \" + component.getClass().getName() + e.getMessage());\n-                }\n+                LOGGER.log(Level.SEVERE, \"Exception in stopping component \" + component.getClass().getName(), e);\n             } finally {\n                 if (componentDumpStream != null) {\n                     componentDumpStream.close();",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java",
                "sha": "4674f9a1b6eb948137cf2593445e8d6d1a7475d7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 5,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java",
                "patch": "@@ -44,12 +44,7 @@ public static void main(String[] args) {\n             application.registerConfig(configManager);\n             NCConfig ncConfig = new NCConfig(nodeId, configManager);\n             final NodeControllerService ncService = new NodeControllerService(ncConfig, application);\n-            if (LOGGER.isLoggable(Level.INFO)) {\n-                LOGGER.info(\"Setting uncaught exception handler \" + ncService.getLifeCycleComponentManager());\n-            }\n-            Thread.currentThread().setUncaughtExceptionHandler(ncService.getLifeCycleComponentManager());\n             ncService.start();\n-            Runtime.getRuntime().addShutdownHook(new NCShutdownHook(ncService));\n             while (true) {\n                 Thread.sleep(10000);\n             }",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java",
                "sha": "11df079a7dbe721077b1160813746e97cccac95d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java",
                "patch": "@@ -256,6 +256,11 @@ private void init() throws Exception {\n     @Override\n     public void start() throws Exception {\n         LOGGER.log(Level.INFO, \"Starting NodeControllerService\");\n+        if (LOGGER.isLoggable(Level.INFO)) {\n+            LOGGER.info(\"Setting uncaught exception handler \" + getLifeCycleComponentManager());\n+        }\n+        Thread.currentThread().setUncaughtExceptionHandler(getLifeCycleComponentManager());\n+        Runtime.getRuntime().addShutdownHook(new NCShutdownHook(this));\n         ipc = new IPCSystem(new InetSocketAddress(ncConfig.getClusterListenAddress(), ncConfig.getClusterListenPort()),\n                 new NodeControllerIPCI(this), new CCNCFunctions.SerializerDeserializer());\n         ipc.start();",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java",
                "sha": "be24dbe8f462581b6e2ce494d4b0ca6f0c9bc85f",
                "status": "modified"
            }
        ],
        "message": "Cleanup Logging, Report Joined Nodes, Misc Cleanup\n\n- Minor refactoring of NodeControllerService startup\n- Cleanup logging in GlobalRecoveryManager / LifeCycleComponentManager\n- Enable TestExecutor to accept non-200 status codes\n- Use ExecutorService for GlobalRecovery thread\n- Eliminate NPE when metadata node goes down before global recovery\n  starts\n\nChange-Id: I87b6b45e1a0cdc7a8b77d80b4e603d927aa60b8a\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1706\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Till Westmann <tillw@apache.org>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>",
        "parent": "https://github.com/apache/asterixdb/commit/6c6479d0e3f3f774703c8e9afa28259d9bd78bf7",
        "patched_files": [
            "ClusterStateManager.java",
            "CCApplication.java",
            "ICCServiceContext.java",
            "GlobalRecoveryManager.java",
            "NCDriver.java",
            "NodeControllerService.java",
            "LifeCycleComponentManager.java"
        ],
        "repo": "asterixdb",
        "unit_tests": [
            "TestExecutor.java"
        ]
    }
}