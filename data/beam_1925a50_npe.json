[
    {
        "repo": "beam",
        "commit": "https://github.com/apache/beam/commit/1925a50d860b7b8f8422f1c2f251d0ea11def736",
        "bug_id": "beam_1925a50",
        "message": "[BEAM-2029] NullPointerException when using multi output ParDo in Spark runner in streaming mode.",
        "parent": "https://github.com/apache/beam/commit/c4936958834aa55f0f1fcb73fe9b8419c4c9eb9b",
        "patched_files": [
            "CreateStream.java",
            "StreamingTransformTranslator.java"
        ],
        "file": [
            {
                "status": "modified",
                "additions": 47,
                "raw_url": "https://github.com/apache/beam/raw/1925a50d860b7b8f8422f1c2f251d0ea11def736/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java?ref=1925a50d860b7b8f8422f1c2f251d0ea11def736",
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java",
                "deletions": 49,
                "sha": "26f0ade7ec9bcf7a443081a2b95d59a6cd3de571",
                "blob_url": "https://github.com/apache/beam/blob/1925a50d860b7b8f8422f1c2f251d0ea11def736/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java",
                "patch": "@@ -385,55 +385,53 @@ public void evaluate(\n         JavaDStream<WindowedValue<InputT>> dStream = unboundedDataset.getDStream();\n \n         final String stepName = context.getCurrentTransform().getFullName();\n-        if (transform.getAdditionalOutputTags().size() == 0) {\n-          JavaPairDStream<TupleTag<?>, WindowedValue<?>> all =\n-              dStream.transformToPair(\n-                  new Function<\n-                      JavaRDD<WindowedValue<InputT>>,\n-                      JavaPairRDD<TupleTag<?>, WindowedValue<?>>>() {\n-                    @Override\n-                    public JavaPairRDD<TupleTag<?>, WindowedValue<?>> call(\n-                        JavaRDD<WindowedValue<InputT>> rdd) throws Exception {\n-                      final Accumulator<NamedAggregators> aggAccum =\n-                          AggregatorsAccumulator.getInstance();\n-                      final Accumulator<SparkMetricsContainer> metricsAccum =\n-                          MetricsAccumulator.getInstance();\n-                      final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>>\n-                          sideInputs =\n-                              TranslationUtils.getSideInputs(\n-                                  transform.getSideInputs(),\n-                                  JavaSparkContext.fromSparkContext(rdd.context()),\n-                                  pviews);\n-                      return rdd.mapPartitionsToPair(\n-                          new MultiDoFnFunction<>(\n-                              aggAccum,\n-                              metricsAccum,\n-                              stepName,\n-                              doFn,\n-                              runtimeContext,\n-                              transform.getMainOutputTag(),\n-                              sideInputs,\n-                              windowingStrategy));\n-                    }\n-                  });\n-          Map<TupleTag<?>, PValue> outputs = context.getOutputs(transform);\n-          if (outputs.size() > 1) {\n-            // cache the DStream if we're going to filter it more than once.\n-            all.cache();\n-          }\n-          for (Map.Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {\n-            @SuppressWarnings(\"unchecked\")\n-            JavaPairDStream<TupleTag<?>, WindowedValue<?>> filtered =\n-                all.filter(new TranslationUtils.TupleTagFilter(output.getKey()));\n-            @SuppressWarnings(\"unchecked\")\n-            // Object is the best we can do since different outputs can have different tags\n-            JavaDStream<WindowedValue<Object>> values =\n-                (JavaDStream<WindowedValue<Object>>)\n-                    (JavaDStream<?>) TranslationUtils.dStreamValues(filtered);\n-            context.putDataset(\n-                output.getValue(),\n-                new UnboundedDataset<>(values, unboundedDataset.getStreamSources()));\n-          }\n+        JavaPairDStream<TupleTag<?>, WindowedValue<?>> all =\n+            dStream.transformToPair(\n+                new Function<\n+                    JavaRDD<WindowedValue<InputT>>,\n+                    JavaPairRDD<TupleTag<?>, WindowedValue<?>>>() {\n+                  @Override\n+                  public JavaPairRDD<TupleTag<?>, WindowedValue<?>> call(\n+                      JavaRDD<WindowedValue<InputT>> rdd) throws Exception {\n+                    final Accumulator<NamedAggregators> aggAccum =\n+                        AggregatorsAccumulator.getInstance();\n+                    final Accumulator<SparkMetricsContainer> metricsAccum =\n+                        MetricsAccumulator.getInstance();\n+                    final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, SideInputBroadcast<?>>>\n+                        sideInputs =\n+                        TranslationUtils.getSideInputs(\n+                            transform.getSideInputs(),\n+                            JavaSparkContext.fromSparkContext(rdd.context()),\n+                            pviews);\n+                    return rdd.mapPartitionsToPair(\n+                        new MultiDoFnFunction<>(\n+                            aggAccum,\n+                            metricsAccum,\n+                            stepName,\n+                            doFn,\n+                            runtimeContext,\n+                            transform.getMainOutputTag(),\n+                            sideInputs,\n+                            windowingStrategy));\n+                  }\n+                });\n+        Map<TupleTag<?>, PValue> outputs = context.getOutputs(transform);\n+        if (outputs.size() > 1) {\n+          // cache the DStream if we're going to filter it more than once.\n+          all.cache();\n+        }\n+        for (Map.Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {\n+          @SuppressWarnings(\"unchecked\")\n+          JavaPairDStream<TupleTag<?>, WindowedValue<?>> filtered =\n+              all.filter(new TranslationUtils.TupleTagFilter(output.getKey()));\n+          @SuppressWarnings(\"unchecked\")\n+          // Object is the best we can do since different outputs can have different tags\n+              JavaDStream<WindowedValue<Object>> values =\n+              (JavaDStream<WindowedValue<Object>>)\n+                  (JavaDStream<?>) TranslationUtils.dStreamValues(filtered);\n+          context.putDataset(\n+              output.getValue(),\n+              new UnboundedDataset<>(values, unboundedDataset.getStreamSources()));\n         }\n       }\n ",
                "changes": 96
            },
            {
                "status": "modified",
                "additions": 50,
                "raw_url": "https://github.com/apache/beam/raw/1925a50d860b7b8f8422f1c2f251d0ea11def736/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/CreateStreamTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/CreateStreamTest.java?ref=1925a50d860b7b8f8422f1c2f251d0ea11def736",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/CreateStreamTest.java",
                "deletions": 0,
                "sha": "dd52c05985eb462e4f897732d513ae20f04c7121",
                "blob_url": "https://github.com/apache/beam/blob/1925a50d860b7b8f8422f1c2f251d0ea11def736/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/CreateStreamTest.java",
                "patch": "@@ -33,8 +33,10 @@\n import org.apache.beam.sdk.testing.PAssert;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.SerializableFunction;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.Values;\n@@ -51,7 +53,10 @@\n import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PCollectionTuple;\n import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.apache.beam.sdk.values.TupleTagList;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.junit.Rule;\n@@ -344,6 +349,51 @@ public void testFlattenedWithWatermarkHold() throws IOException {\n     p.run();\n   }\n \n+  /**\n+   * Test multiple output {@link ParDo} in streaming pipelines.\n+   * This is currently needed as a test for https://issues.apache.org/jira/browse/BEAM-2029 since\n+   * {@link org.apache.beam.sdk.testing.ValidatesRunner} tests do not currently run for Spark runner\n+   * in streaming mode.\n+   */\n+  @Test\n+  public void testMultiOutputParDo() throws IOException {\n+    Pipeline p = pipelineRule.createPipeline();\n+    Instant instant = new Instant(0);\n+    CreateStream<Integer> source1 =\n+        CreateStream.of(VarIntCoder.of(), pipelineRule.batchDuration())\n+            .emptyBatch()\n+            .advanceWatermarkForNextBatch(instant.plus(Duration.standardMinutes(5)))\n+            .nextBatch(\n+                TimestampedValue.of(1, instant),\n+                TimestampedValue.of(2, instant),\n+                TimestampedValue.of(3, instant))\n+            .advanceNextBatchWatermarkToInfinity();\n+\n+    PCollection<Integer> inputs = p.apply(source1);\n+\n+    final TupleTag<Integer> mainTag = new TupleTag<>();\n+    final TupleTag<Integer> additionalTag = new TupleTag<>();\n+\n+    PCollectionTuple outputs = inputs.apply(ParDo.of(new DoFn<Integer, Integer>() {\n+\n+      @SuppressWarnings(\"unused\")\n+      @ProcessElement\n+      public void process(ProcessContext context) {\n+        Integer element = context.element();\n+        context.output(element);\n+        context.output(additionalTag, element + 1);\n+      }\n+    }).withOutputTags(mainTag, TupleTagList.of(additionalTag)));\n+\n+    PCollection<Integer> output1 = outputs.get(mainTag).setCoder(VarIntCoder.of());\n+    PCollection<Integer> output2 = outputs.get(additionalTag).setCoder(VarIntCoder.of());\n+\n+    PAssert.that(output1).containsInAnyOrder(1, 2, 3);\n+    PAssert.that(output2).containsInAnyOrder(2, 3, 4);\n+\n+    p.run();\n+  }\n+\n   @Test\n   public void testElementAtPositiveInfinityThrows() {\n     CreateStream<Integer> source =",
                "changes": 50
            }
        ],
        "unit_tests": [
            "CreateStreamTest.java"
        ]
    },
    {
        "buggy": false,
        "test_file": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/CreateStreamTest.java",
        "buggy_files": [
            "runners/spark/src/main/java/org/apache/beam/runners/spark/io/CreateStream.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java"
        ],
        "fixed": true
    }
]