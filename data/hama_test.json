{
    "hama_583c69b": {
        "bug_id": "hama_583c69b",
        "commit": "https://github.com/apache/hama/commit/583c69b37817f9ee76403805657d6650f2efc6d3",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hama/blob/583c69b37817f9ee76403805657d6650f2efc6d3/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/CHANGES.txt?ref=583c69b37817f9ee76403805657d6650f2efc6d3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -8,6 +8,7 @@ Release 0.7.1 (unreleased changes)\n \n   BUG FIXES\n \n+    HAMA-982: Vertex.read/writeState() method throws NullPointerException (edwardyoon)\n     HAMA-965: Infinite loop because of recursive function call (JongYoon Lim via edwardyoon)\n     HAMA-966: NioServerListener doesn't throw any exceptions (JongYoon Lim via edwardyoon)\n ",
                "raw_url": "https://github.com/apache/hama/raw/583c69b37817f9ee76403805657d6650f2efc6d3/CHANGES.txt",
                "sha": "48048c8a288f56e34b5039371baa19d60d64d31f",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/hama/blob/583c69b37817f9ee76403805657d6650f2efc6d3/examples/src/test/java/org/apache/hama/examples/CustomVertexReadWriteStateTest.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/examples/src/test/java/org/apache/hama/examples/CustomVertexReadWriteStateTest.java?ref=583c69b37817f9ee76403805657d6650f2efc6d3",
                "deletions": 0,
                "filename": "examples/src/test/java/org/apache/hama/examples/CustomVertexReadWriteStateTest.java",
                "patch": "@@ -0,0 +1,70 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hama.examples;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+import junit.framework.TestCase;\n+\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hama.HamaConfiguration;\n+import org.apache.hama.graph.Vertex;\n+\n+public class CustomVertexReadWriteStateTest extends TestCase {\n+  static int initialState = 2;\n+  static int changedState = 4;\n+\n+  public static class TestVertex extends Vertex<Text, IntWritable, IntWritable> {\n+\n+    private static ArrayWritable test = new ArrayWritable(IntWritable.class);\n+\n+    @Override\n+    public void setup(HamaConfiguration conf) {\n+      // Sets the initial state\n+      test.set(new Writable[] { new IntWritable(initialState) });\n+    }\n+\n+    @Override\n+    public void compute(Iterable<IntWritable> messages) throws IOException {\n+      if (this.getSuperstepCount() == 3) {\n+        // change the state\n+        test.set(new Writable[] { new IntWritable(changedState) });\n+      }\n+\n+      if (this.getSuperstepCount() < 3) {\n+        assertEquals(initialState, ((IntWritable) test.get()[0]).get());\n+      } else {\n+        assertEquals(changedState, ((IntWritable) test.get()[0]).get());\n+      }\n+    }\n+\n+    public void readState(DataInput in) throws IOException {\n+      test.readFields(in);\n+    }\n+\n+    public void writeState(DataOutput out) throws IOException {\n+      test.write(out);\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hama/raw/583c69b37817f9ee76403805657d6650f2efc6d3/examples/src/test/java/org/apache/hama/examples/CustomVertexReadWriteStateTest.java",
                "sha": "f753ffb506bb9f32f85b3d4633529b84237549ca",
                "status": "added"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hama/blob/583c69b37817f9ee76403805657d6650f2efc6d3/examples/src/test/java/org/apache/hama/examples/SSSPTest.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/examples/src/test/java/org/apache/hama/examples/SSSPTest.java?ref=583c69b37817f9ee76403805657d6650f2efc6d3",
                "deletions": 6,
                "filename": "examples/src/test/java/org/apache/hama/examples/SSSPTest.java",
                "patch": "@@ -31,7 +31,16 @@\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n import org.apache.hama.HamaConfiguration;\n+import org.apache.hama.bsp.HashPartitioner;\n+import org.apache.hama.bsp.NullOutputFormat;\n+import org.apache.hama.bsp.TextInputFormat;\n+import org.apache.hama.examples.CustomVertexReadWriteStateTest.TestVertex;\n+import org.apache.hama.examples.SSSP.SSSPTextReader;\n+import org.apache.hama.graph.GraphJob;\n import org.junit.Test;\n \n /**\n@@ -61,18 +70,51 @@\n   protected void setUp() throws Exception {\n     super.setUp();\n     fs = FileSystem.get(conf);\n+    generateTestData();\n+  }\n+  \n+  protected void tearDown() throws Exception {\n+    deleteTempDirs();\n+    fs.close();\n   }\n \n   @Test\n   public void testShortestPaths() throws IOException, InterruptedException,\n       ClassNotFoundException, InstantiationException, IllegalAccessException {\n \n-    generateTestData();\n-    try {\n-      SSSP.main(new String[] { \"0\", INPUT, OUTPUT, \"3\" });\n-      verifyResult();\n-    } finally {\n-      deleteTempDirs();\n+    SSSP.main(new String[] { \"0\", INPUT, OUTPUT, \"3\" });\n+    verifyResult();\n+  }\n+\n+  @Test\n+  public void testCustomReadWriteState() throws IOException,\n+      InterruptedException, ClassNotFoundException, InstantiationException,\n+      IllegalAccessException {\n+\n+    HamaConfiguration conf = new HamaConfiguration();\n+    GraphJob job = new GraphJob(conf, CustomVertexReadWriteStateTest.class);\n+    // Set the job name\n+    job.setJobName(\"test custom read/write state\");\n+    job.setInputPath(new Path(INPUT));\n+    job.setNumBspTask(1);\n+    job.setVertexClass(TestVertex.class);\n+    job.setInputFormat(TextInputFormat.class);\n+    job.setInputKeyClass(LongWritable.class);\n+    job.setInputValueClass(Text.class);\n+\n+    job.setPartitioner(HashPartitioner.class);\n+    job.setOutputFormat(NullOutputFormat.class);\n+    job.setVertexInputReaderClass(SSSPTextReader.class);\n+    // Iterate until all the nodes have been reached.\n+    job.setMaxIteration(6);\n+    job.setVertexIDClass(Text.class);\n+    job.setVertexValueClass(IntWritable.class);\n+    job.setEdgeValueClass(IntWritable.class);\n+\n+    long startTime = System.currentTimeMillis();\n+    if (job.waitForCompletion(true)) {\n+      System.out.println(\"Job Finished in \"\n+          + (System.currentTimeMillis() - startTime) / 1000.0 + \" seconds\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hama/raw/583c69b37817f9ee76403805657d6650f2efc6d3/examples/src/test/java/org/apache/hama/examples/SSSPTest.java",
                "sha": "116abbb727cf63b6f020e4f5918acb097c854c3e",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hama/blob/583c69b37817f9ee76403805657d6650f2efc6d3/graph/src/main/java/org/apache/hama/graph/Vertex.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/graph/src/main/java/org/apache/hama/graph/Vertex.java?ref=583c69b37817f9ee76403805657d6650f2efc6d3",
                "deletions": 2,
                "filename": "graph/src/main/java/org/apache/hama/graph/Vertex.java",
                "patch": "@@ -17,8 +17,10 @@\n  */\n package org.apache.hama.graph;\n \n+import java.io.ByteArrayOutputStream;\n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.DataOutputStream;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n@@ -271,7 +273,11 @@ public void readFields(DataInput in) throws IOException {\n       }\n     }\n     votedToHalt = in.readBoolean();\n-    readState(in);\n+\n+    boolean hasMoreContents = in.readBoolean();\n+    if (hasMoreContents) {\n+      readState(in);\n+    }\n   }\n \n   @Override\n@@ -308,8 +314,24 @@ public void write(DataOutput out) throws IOException {\n       }\n     }\n     out.writeBoolean(votedToHalt);\n-    writeState(out);\n \n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    DataOutput customOut = new DataOutputStream(baos);\n+    boolean hasMoreContents = true;\n+    try {\n+      writeState(customOut);\n+    } catch (NullPointerException e) {\n+      // do nothing\n+    }\n+\n+    // if all states are null, set hasContents to false.\n+    if (baos.size() == 0) {\n+      hasMoreContents = false;\n+    }\n+\n+    out.writeBoolean(hasMoreContents);\n+    if (hasMoreContents)\n+      out.write(baos.toByteArray());\n   }\n \n   // compare across the vertex ID",
                "raw_url": "https://github.com/apache/hama/raw/583c69b37817f9ee76403805657d6650f2efc6d3/graph/src/main/java/org/apache/hama/graph/Vertex.java",
                "sha": "cdbf6b51a999c7ae8c2058c6d443c3e8059a3434",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hama/blob/583c69b37817f9ee76403805657d6650f2efc6d3/src/site/xdoc/index.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/site/xdoc/index.xml?ref=583c69b37817f9ee76403805657d6650f2efc6d3",
                "deletions": 2,
                "filename": "src/site/xdoc/index.xml",
                "patch": "@@ -24,7 +24,7 @@ xsi:schemaLocation=\"http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd\n \n     <section name=\"\"></section>\n     <p>\n-    <div style=\"float:left;margin-right:15px;\"><img src=\"./images/hama_paint_logo.png\" style=\"width:130px\" alt=\"\" /></div>\n+    <div style=\"float:left;margin-right:15px;margin-bottom: 10px;\"><img src=\"./images/hama_paint_logo.png\" style=\"width:120px\" alt=\"\" /></div>\n      Apache Hama<sup>TM</sup> is a framework for Big Data analytics which uses the Bulk Synchronous Parallel (BSP) computing model, \n     which was established in 2012 as a Top-Level Project of The Apache Software Foundation. \n     <br/><br/>It provides not only pure BSP programming model \n@@ -49,8 +49,8 @@ xsi:schemaLocation=\"http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd\n \n     <h3 align=\"center\">Recent News</h3>\n       <ul>\n+        <li>Jan 28, 2016: Behroz Sikander was added as a committer and PMC</li>\n         <li>Jun 14, 2015: release 0.7.0 available [<a href=\"downloads.html\">downloads</a>]</li>\n-        <li>Jun 11, 2015: Minho Kim was added as a committer.</li>\n       </ul>\n     </div>\n       ",
                "raw_url": "https://github.com/apache/hama/raw/583c69b37817f9ee76403805657d6650f2efc6d3/src/site/xdoc/index.xml",
                "sha": "16026807765f77f0b6159c5b3860ee5e15174fca",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hama/blob/583c69b37817f9ee76403805657d6650f2efc6d3/src/site/xdoc/team-list.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/site/xdoc/team-list.xml?ref=583c69b37817f9ee76403805657d6650f2efc6d3",
                "deletions": 0,
                "filename": "src/site/xdoc/team-list.xml",
                "patch": "@@ -64,6 +64,12 @@ xsi:schemaLocation=\"http://maven.apache.org/XDOC/2.0 http://maven.apache.org/xsd\n           <td align=\"center\">.</td>\n           <td align=\"center\">committer</td>\n         </tr>\n+       <tr valign=\"top\">\n+          <td align=\"center\">bsikander</td>\n+          <td align=\"center\">Behroz Sikander</td>\n+          <td align=\"center\">Technical University of Munich</td>\n+          <td align=\"center\">PMC member, committer</td>\n+        </tr>\n        <tr valign=\"top\">\n           <td align=\"center\">bsmin</td>\n           <td align=\"center\">Byungseok Min</td>",
                "raw_url": "https://github.com/apache/hama/raw/583c69b37817f9ee76403805657d6650f2efc6d3/src/site/xdoc/team-list.xml",
                "sha": "d4bc37882f8002d970338ce07a4c75ff1b8baa53",
                "status": "modified"
            }
        ],
        "message": "HAMA-982: Vertex.read/writeState() method throws NullPointerException",
        "parent": "https://github.com/apache/hama/commit/09243a3ca1565ba678ee7455a8d25288d95ddf2a",
        "patched_files": [
            "CHANGES.java",
            "SSSP.java",
            "team-list.java",
            "index.java",
            "Vertex.java"
        ],
        "repo": "hama",
        "unit_tests": [
            "CustomVertexReadWriteStateTest.java",
            "SSSPTest.java"
        ]
    },
    "hama_d8f737f": {
        "bug_id": "hama_d8f737f",
        "commit": "https://github.com/apache/hama/commit/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/CHANGES.txt?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -48,5 +48,6 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    HAMA-53: NullPointerException on distributed cluster (edwardyoon)\n     HAMA-26: hama-formatter.xml should be removed (edwardyoon)  \n     HAMA-25: Vector.get() returns double (edwardyoon)",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/CHANGES.txt",
                "sha": "8012c1bfd3d0537c444dbeef9d26f3f40e23d044",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/examples/org/apache/hama/examples/MatrixAddition.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/examples/org/apache/hama/examples/MatrixAddition.java?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 70,
                "filename": "src/examples/org/apache/hama/examples/MatrixAddition.java",
                "patch": "@@ -1,70 +1,74 @@\n-/**\r\n- * Copyright 2007 The Apache Software Foundation\r\n- *\r\n- * Licensed to the Apache Software Foundation (ASF) under one\r\n- * or more contributor license agreements.  See the NOTICE file\r\n- * distributed with this work for additional information\r\n- * regarding copyright ownership.  The ASF licenses this file\r\n- * to you under the Apache License, Version 2.0 (the\r\n- * \"License\"); you may not use this file except in compliance\r\n- * with the License.  You may obtain a copy of the License at\r\n- *\r\n- *     http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-package org.apache.hama.examples;\r\n-\r\n-import java.io.IOException;\r\n-\r\n-import org.apache.hama.DenseMatrix;\r\n-import org.apache.hama.HamaConfiguration;\r\n-import org.apache.hama.Matrix;\r\n-\r\n-public class MatrixAddition {\r\n-\r\n-  public static void main(String[] args) throws IOException {\r\n-    if (args.length < 2) {\r\n-      System.out.println(\"addition <row_m> <column_n>\");\r\n-      System.exit(-1);\r\n-    }\r\n-\r\n-    int row = Integer.parseInt(args[0]);\r\n-    int column = Integer.parseInt(args[1]);\r\n-\r\n-    HamaConfiguration conf = new HamaConfiguration();\r\n-\r\n-    Matrix a = DenseMatrix.random(conf, row, column);\r\n-    Matrix b = DenseMatrix.random(conf, row, column);\r\n-\r\n-    Matrix c = a.add(b);\r\n-\r\n-    System.out.println(\"\\nMatrix A\");\r\n-    System.out.println(\"----------------------\");\r\n-    for(int i =  0; i < row; i++) {\r\n-      for(int j =  0; j < row; j++) {\r\n-        System.out.println(a.get(i, j));\r\n-      }\r\n-    }\r\n-    \r\n-    System.out.println(\"\\nMatrix B\");\r\n-    System.out.println(\"----------------------\");\r\n-    for(int i =  0; i < row; i++) {\r\n-      for(int j =  0; j < row; j++) {\r\n-        System.out.println(b.get(i, j));\r\n-      }\r\n-    }\r\n-    \r\n-    System.out.println(\"\\nC = A + B\");\r\n-    System.out.println(\"----------------------\");\r\n-    for(int i =  0; i < row; i++) {\r\n-      for(int j =  0; j < row; j++) {\r\n-        System.out.println(c.get(i, j));\r\n-      }\r\n-    }\r\n-  }\r\n-}\r\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hama.examples;\n+\n+import java.io.IOException;\n+\n+import org.apache.hama.DenseMatrix;\n+import org.apache.hama.HamaConfiguration;\n+import org.apache.hama.Matrix;\n+\n+public class MatrixAddition {\n+\n+  public static void main(String[] args) throws IOException {\n+    if (args.length < 2) {\n+      System.out.println(\"addition <row_m> <column_n>\");\n+      System.exit(-1);\n+    }\n+\n+    int row = Integer.parseInt(args[0]);\n+    int column = Integer.parseInt(args[1]);\n+\n+    HamaConfiguration conf = new HamaConfiguration();\n+    \n+    conf.set(\"fs.default.name\", \"hdfs://udanax.org:54310\");\n+    conf.set(\"hbase.master\", \"udanax.org:60000\");\n+\n+    Matrix a = DenseMatrix.random(conf, row, column);\n+    Matrix b = DenseMatrix.random(conf, row, column);\n+\n+    System.out.println(\"\\nMatrix A\");\n+    System.out.println(\"----------------------\");\n+    for(int i =  0; i < row; i++) {\n+      for(int j =  0; j < row; j++) {\n+        System.out.println(a.get(i, j));\n+      }\n+    }\n+    \n+    System.out.println(\"\\nMatrix B\");\n+    System.out.println(\"----------------------\");\n+    for(int i =  0; i < row; i++) {\n+      for(int j =  0; j < row; j++) {\n+        System.out.println(b.get(i, j));\n+      }\n+    }\n+    System.out.println();\n+    \n+    Matrix c = a.add(b);\n+    \n+    System.out.println(\"\\nC = A + B\");\n+    System.out.println(\"----------------------\");\n+    for(int i =  0; i < row; i++) {\n+      for(int j =  0; j < row; j++) {\n+        System.out.println(c.get(i, j));\n+      }\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/examples/org/apache/hama/examples/MatrixAddition.java",
                "sha": "dcc35fa598973c2ae9d580483f952c841ef4fa98",
                "status": "modified"
            },
            {
                "additions": 207,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/DenseMatrix.java",
                "changes": 415,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/java/org/apache/hama/DenseMatrix.java?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 208,
                "filename": "src/java/org/apache/hama/DenseMatrix.java",
                "patch": "@@ -1,208 +1,207 @@\n-/**\r\n- * Copyright 2007 The Apache Software Foundation\r\n- *\r\n- * Licensed to the Apache Software Foundation (ASF) under one\r\n- * or more contributor license agreements.  See the NOTICE file\r\n- * distributed with this work for additional information\r\n- * regarding copyright ownership.  The ASF licenses this file\r\n- * to you under the Apache License, Version 2.0 (the\r\n- * \"License\"); you may not use this file except in compliance\r\n- * with the License.  You may obtain a copy of the License at\r\n- *\r\n- *     http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-package org.apache.hama;\r\n-\r\n-import java.io.IOException;\r\n-\r\n-import org.apache.hadoop.hbase.HColumnDescriptor;\r\n-import org.apache.hadoop.hbase.HConstants;\r\n-import org.apache.hadoop.hbase.HTableDescriptor;\r\n-import org.apache.hadoop.hbase.client.HTable;\r\n-import org.apache.hadoop.hbase.client.Scanner;\r\n-import org.apache.hadoop.hbase.io.RowResult;\r\n-import org.apache.hadoop.io.IntWritable;\r\n-import org.apache.hadoop.mapred.JobClient;\r\n-import org.apache.hadoop.mapred.JobConf;\r\n-import org.apache.hama.algebra.AdditionMap;\r\n-import org.apache.hama.algebra.AdditionReduce;\r\n-import org.apache.hama.algebra.MultiplicationMap;\r\n-import org.apache.hama.algebra.MultiplicationReduce;\r\n-import org.apache.hama.io.VectorEntry;\r\n-import org.apache.hama.io.VectorMapWritable;\r\n-import org.apache.hama.mapred.DenseMap;\r\n-import org.apache.hama.mapred.MatrixReduce;\r\n-import org.apache.hama.util.Numeric;\r\n-import org.apache.hama.util.RandomVariable;\r\n-\r\n-public class DenseMatrix extends AbstractMatrix implements Matrix {\r\n-\r\n-  /**\r\n-   * Construct\r\n-   * \r\n-   * @param conf configuration object\r\n-   */\r\n-  public DenseMatrix(HamaConfiguration conf) {\r\n-    setConfiguration(conf);\r\n-  }\r\n-\r\n-  /**\r\n-   * Construct an matrix\r\n-   * \r\n-   * @param conf configuration object\r\n-   * @param matrixName the name of the matrix\r\n-   */\r\n-  public DenseMatrix(HamaConfiguration conf, String matrixName) {\r\n-    try {\r\n-      setConfiguration(conf);\r\n-      this.matrixName = matrixName;\r\n-\r\n-      if (!admin.tableExists(matrixName)) {\r\n-        tableDesc = new HTableDescriptor(matrixName);\r\n-        tableDesc.addFamily(new HColumnDescriptor(Constants.COLUMN));\r\n-        create();\r\n-      }\r\n-\r\n-      table = new HTable(config, matrixName);\r\n-    } catch (Exception e) {\r\n-      e.printStackTrace();\r\n-    }\r\n-  }\r\n-\r\n-  /**\r\n-   * Construct an m-by-n constant matrix.\r\n-   * \r\n-   * @param conf configuration object\r\n-   * @param m the number of rows.\r\n-   * @param n the number of columns.\r\n-   * @param s fill the matrix with this scalar value.\r\n-   */\r\n-  public DenseMatrix(HamaConfiguration conf, int m, int n, double s) {\r\n-    try {\r\n-      setConfiguration(conf);\r\n-      matrixName = RandomVariable.randMatrixName();\r\n-\r\n-      if (!admin.tableExists(matrixName)) {\r\n-        tableDesc = new HTableDescriptor(matrixName);\r\n-        tableDesc.addFamily(new HColumnDescriptor(Constants.COLUMN));\r\n-        create();\r\n-      }\r\n-\r\n-      table = new HTable(config, matrixName);\r\n-\r\n-      for (int i = 0; i < m; i++) {\r\n-        for (int j = 0; j < n; j++) {\r\n-          set(i, j, s);\r\n-        }\r\n-      }\r\n-\r\n-      setDimension(m, n);\r\n-    } catch (IOException e) {\r\n-      e.printStackTrace();\r\n-    }\r\n-  }\r\n-\r\n-  /**\r\n-   * Generate matrix with random elements\r\n-   * \r\n-   * @param conf configuration object\r\n-   * @param m the number of rows.\r\n-   * @param n the number of columns.\r\n-   * @return an m-by-n matrix with uniformly distributed random elements.\r\n-   * @throws IOException\r\n-   */\r\n-  public static Matrix random(HamaConfiguration conf, int m, int n)\r\n-      throws IOException {\r\n-    String name = RandomVariable.randMatrixName();\r\n-    Matrix rand = new DenseMatrix(conf, name);\r\n-    for (int i = 0; i < m; i++) {\r\n-      for (int j = 0; j < n; j++) {\r\n-        rand.set(i, j, RandomVariable.rand());\r\n-      }\r\n-    }\r\n-\r\n-    rand.setDimension(m, n);\r\n-    LOG.info(\"Create the \" + m + \" * \" + n + \" random matrix : \" + name);\r\n-    return rand;\r\n-  }\r\n-\r\n-  public Matrix add(Matrix B) throws IOException {\r\n-    String output = RandomVariable.randMatrixName();\r\n-    Matrix C = new DenseMatrix(config, output);\r\n-\r\n-    JobConf jobConf = new JobConf(config);\r\n-    jobConf.setJobName(\"addition MR job\");\r\n-\r\n-    DenseMap.initJob(this.getName(), B.getName(), AdditionMap.class,\r\n-        IntWritable.class, DenseVector.class, jobConf);\r\n-    MatrixReduce.initJob(C.getName(), AdditionReduce.class, jobConf);\r\n-\r\n-    JobClient.runJob(jobConf);\r\n-    return C;\r\n-  }\r\n-\r\n-  public Matrix add(double alpha, Matrix B) throws IOException {\r\n-    // TODO Auto-generated method stub\r\n-    return null;\r\n-  }\r\n-\r\n-  public DenseVector getRow(int row) throws IOException {\r\n-    return new DenseVector(row, table.getRow(String.valueOf(row)));\r\n-  }\r\n-\r\n-  public Vector getColumn(int column) throws IOException {\r\n-    byte[] columnKey = Numeric.getColumnIndex(column);\r\n-    byte[][] c = { columnKey };\r\n-    Scanner scan = table.getScanner(c, HConstants.EMPTY_START_ROW);\r\n-\r\n-    VectorMapWritable<Integer, VectorEntry> trunk = new VectorMapWritable<Integer, VectorEntry>();\r\n-\r\n-    for (RowResult row : scan) {\r\n-      trunk.put(Numeric.bytesToInt(row.getRow()), new VectorEntry(row\r\n-          .get(columnKey)));\r\n-    }\r\n-\r\n-    return new DenseVector(column, trunk);\r\n-  }\r\n-\r\n-  public Matrix mult(Matrix B) throws IOException {\r\n-    String output = RandomVariable.randMatrixName();\r\n-    Matrix C = new DenseMatrix(config, output);\r\n-\r\n-    JobConf jobConf = new JobConf(config);\r\n-    jobConf.setJobName(\"multiplication MR job\");\r\n-\r\n-    DenseMap.initJob(this.getName(), B.getName(), MultiplicationMap.class,\r\n-        IntWritable.class, DenseVector.class, jobConf);\r\n-    MatrixReduce.initJob(C.getName(), MultiplicationReduce.class, jobConf);\r\n-\r\n-    JobClient.runJob(jobConf);\r\n-    return C;\r\n-  }\r\n-\r\n-  public Matrix multAdd(double alpha, Matrix B, Matrix C) throws IOException {\r\n-    // TODO Auto-generated method stub\r\n-    return null;\r\n-  }\r\n-\r\n-  public double norm(Norm type) throws IOException {\r\n-    // TODO Auto-generated method stub\r\n-    return 0;\r\n-  }\r\n-\r\n-  public Matrix set(double alpha, Matrix B) throws IOException {\r\n-    // TODO Auto-generated method stub\r\n-    return null;\r\n-  }\r\n-\r\n-  public Matrix set(Matrix B) throws IOException {\r\n-    // TODO Auto-generated method stub\r\n-    return null;\r\n-  }\r\n-}\r\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hama;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.hbase.HColumnDescriptor;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.HTableDescriptor;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Scanner;\n+import org.apache.hadoop.hbase.io.RowResult;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hama.algebra.AdditionMap;\n+import org.apache.hama.algebra.AdditionReduce;\n+import org.apache.hama.algebra.MultiplicationMap;\n+import org.apache.hama.algebra.MultiplicationReduce;\n+import org.apache.hama.io.VectorEntry;\n+import org.apache.hama.io.VectorMapWritable;\n+import org.apache.hama.mapred.MatrixReduce;\n+import org.apache.hama.util.Numeric;\n+import org.apache.hama.util.RandomVariable;\n+\n+public class DenseMatrix extends AbstractMatrix implements Matrix {\n+\n+  /**\n+   * Construct\n+   * \n+   * @param conf configuration object\n+   */\n+  public DenseMatrix(HamaConfiguration conf) {\n+    setConfiguration(conf);\n+  }\n+\n+  /**\n+   * Construct an matrix\n+   * \n+   * @param conf configuration object\n+   * @param matrixName the name of the matrix\n+   */\n+  public DenseMatrix(HamaConfiguration conf, String matrixName) {\n+    try {\n+      setConfiguration(conf);\n+      this.matrixName = matrixName;\n+\n+      if (!admin.tableExists(matrixName)) {\n+        tableDesc = new HTableDescriptor(matrixName);\n+        tableDesc.addFamily(new HColumnDescriptor(Constants.COLUMN));\n+        create();\n+      }\n+\n+      table = new HTable(config, matrixName);\n+    } catch (Exception e) {\n+      e.printStackTrace();\n+    }\n+  }\n+\n+  /**\n+   * Construct an m-by-n constant matrix.\n+   * \n+   * @param conf configuration object\n+   * @param m the number of rows.\n+   * @param n the number of columns.\n+   * @param s fill the matrix with this scalar value.\n+   */\n+  public DenseMatrix(HamaConfiguration conf, int m, int n, double s) {\n+    try {\n+      setConfiguration(conf);\n+      matrixName = RandomVariable.randMatrixName();\n+\n+      if (!admin.tableExists(matrixName)) {\n+        tableDesc = new HTableDescriptor(matrixName);\n+        tableDesc.addFamily(new HColumnDescriptor(Constants.COLUMN));\n+        create();\n+      }\n+\n+      table = new HTable(config, matrixName);\n+\n+      for (int i = 0; i < m; i++) {\n+        for (int j = 0; j < n; j++) {\n+          set(i, j, s);\n+        }\n+      }\n+\n+      setDimension(m, n);\n+    } catch (IOException e) {\n+      e.printStackTrace();\n+    }\n+  }\n+\n+  /**\n+   * Generate matrix with random elements\n+   * \n+   * @param conf configuration object\n+   * @param m the number of rows.\n+   * @param n the number of columns.\n+   * @return an m-by-n matrix with uniformly distributed random elements.\n+   * @throws IOException\n+   */\n+  public static Matrix random(HamaConfiguration conf, int m, int n)\n+      throws IOException {\n+    String name = RandomVariable.randMatrixName();\n+    Matrix rand = new DenseMatrix(conf, name);\n+    for (int i = 0; i < m; i++) {\n+      for (int j = 0; j < n; j++) {\n+        rand.set(i, j, RandomVariable.rand());\n+      }\n+    }\n+\n+    rand.setDimension(m, n);\n+    LOG.info(\"Create the \" + m + \" * \" + n + \" random matrix : \" + name);\n+    return rand;\n+  }\n+\n+  public Matrix add(Matrix B) throws IOException {\n+    String output = RandomVariable.randMatrixName();\n+    Matrix C = new DenseMatrix(config, output);\n+\n+    JobConf jobConf = new JobConf(config);\n+    jobConf.setJobName(\"addition MR job\");\n+\n+    AdditionMap.initJob(this.getName(), B.getName(), AdditionMap.class,\n+        IntWritable.class, DenseVector.class, jobConf);\n+    MatrixReduce.initJob(C.getName(), AdditionReduce.class, jobConf);\n+\n+    JobClient.runJob(jobConf);\n+    return C;\n+  }\n+\n+  public Matrix add(double alpha, Matrix B) throws IOException {\n+    // TODO Auto-generated method stub\n+    return null;\n+  }\n+\n+  public DenseVector getRow(int row) throws IOException {\n+    return new DenseVector(row, table.getRow(String.valueOf(row)));\n+  }\n+\n+  public Vector getColumn(int column) throws IOException {\n+    byte[] columnKey = Numeric.getColumnIndex(column);\n+    byte[][] c = { columnKey };\n+    Scanner scan = table.getScanner(c, HConstants.EMPTY_START_ROW);\n+\n+    VectorMapWritable<Integer, VectorEntry> trunk = new VectorMapWritable<Integer, VectorEntry>();\n+\n+    for (RowResult row : scan) {\n+      trunk.put(Numeric.bytesToInt(row.getRow()), new VectorEntry(row\n+          .get(columnKey)));\n+    }\n+\n+    return new DenseVector(column, trunk);\n+  }\n+\n+  public Matrix mult(Matrix B) throws IOException {\n+    String output = RandomVariable.randMatrixName();\n+    Matrix C = new DenseMatrix(config, output);\n+\n+    JobConf jobConf = new JobConf(config);\n+    jobConf.setJobName(\"multiplication MR job\");\n+\n+    MultiplicationMap.initJob(this.getName(), B.getName(), MultiplicationMap.class,\n+        IntWritable.class, DenseVector.class, jobConf);\n+    MatrixReduce.initJob(C.getName(), MultiplicationReduce.class, jobConf);\n+\n+    JobClient.runJob(jobConf);\n+    return C;\n+  }\n+\n+  public Matrix multAdd(double alpha, Matrix B, Matrix C) throws IOException {\n+    // TODO Auto-generated method stub\n+    return null;\n+  }\n+\n+  public double norm(Norm type) throws IOException {\n+    // TODO Auto-generated method stub\n+    return 0;\n+  }\n+\n+  public Matrix set(double alpha, Matrix B) throws IOException {\n+    // TODO Auto-generated method stub\n+    return null;\n+  }\n+\n+  public Matrix set(Matrix B) throws IOException {\n+    // TODO Auto-generated method stub\n+    return null;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/DenseMatrix.java",
                "sha": "65e3da458f38d5d70d0e0c1ef52d00e45b4b8398",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/algebra/AdditionMap.java",
                "changes": 112,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/java/org/apache/hama/algebra/AdditionMap.java?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 43,
                "filename": "src/java/org/apache/hama/algebra/AdditionMap.java",
                "patch": "@@ -1,43 +1,69 @@\n-/**\r\n- * Copyright 2007 The Apache Software Foundation\r\n- *\r\n- * Licensed to the Apache Software Foundation (ASF) under one\r\n- * or more contributor license agreements.  See the NOTICE file\r\n- * distributed with this work for additional information\r\n- * regarding copyright ownership.  The ASF licenses this file\r\n- * to you under the Apache License, Version 2.0 (the\r\n- * \"License\"); you may not use this file except in compliance\r\n- * with the License.  You may obtain a copy of the License at\r\n- *\r\n- *     http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-package org.apache.hama.algebra;\r\n-\r\n-import java.io.IOException;\r\n-\r\n-import org.apache.hadoop.io.IntWritable;\r\n-import org.apache.hadoop.mapred.OutputCollector;\r\n-import org.apache.hadoop.mapred.Reporter;\r\n-import org.apache.hama.DenseVector;\r\n-import org.apache.hama.Vector;\r\n-import org.apache.hama.mapred.DenseMap;\r\n-\r\n-public class AdditionMap extends DenseMap<IntWritable, DenseVector> {\r\n-\r\n-  @Override\r\n-  public void map(IntWritable key, DenseVector value,\r\n-      OutputCollector<IntWritable, DenseVector> output,\r\n-      Reporter reporter) throws IOException {\r\n-\r\n-    Vector v1 = MATRIX_B.getRow(key.get());\r\n-    output.collect(key, (DenseVector) v1.add(value));\r\n-\r\n-  }\r\n-\r\n-}\r\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hama.algebra;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hama.DenseMatrix;\n+import org.apache.hama.DenseVector;\n+import org.apache.hama.HamaConfiguration;\n+import org.apache.hama.Matrix;\n+import org.apache.hama.Vector;\n+import org.apache.hama.mapred.DenseMap;\n+import org.apache.log4j.Logger;\n+\n+public class AdditionMap extends DenseMap<IntWritable, DenseVector> {\n+  static final Logger LOG = Logger.getLogger(AdditionMap.class);\n+  protected Matrix matrix_b;\n+  public static final String MATRIX_B = \"hama.addition.matrix.b\";\n+  \n+  public void configure(JobConf job) {\n+    matrix_b = new DenseMatrix(new HamaConfiguration(), job.get(MATRIX_B, \"\"));\n+  }\n+\n+  public static void initJob(String matrix_a, String matrix_b,\n+      Class<AdditionMap> map, \n+      Class<IntWritable> outputKeyClass, \n+      Class<DenseVector> outputValueClass, \n+      JobConf jobConf) {\n+    \n+    jobConf.setMapOutputValueClass(outputValueClass);\n+    jobConf.setMapOutputKeyClass(outputKeyClass);\n+    jobConf.setMapperClass(map);\n+    jobConf.set(MATRIX_B, matrix_b);\n+    \n+    initJob(matrix_a, map, jobConf);\n+  }\n+\n+  @Override\n+  public void map(IntWritable key, DenseVector value,\n+      OutputCollector<IntWritable, DenseVector> output, Reporter reporter)\n+      throws IOException {\n+\n+    Vector v1 = matrix_b.getRow(key.get());\n+    output.collect(key, (DenseVector) v1.add(value));\n+\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/algebra/AdditionMap.java",
                "sha": "f252fa437ae32fec3f6182204cb7f043d77fcbcc",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/algebra/MultiplicationMap.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/java/org/apache/hama/algebra/MultiplicationMap.java?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 2,
                "filename": "src/java/org/apache/hama/algebra/MultiplicationMap.java",
                "patch": "@@ -23,17 +23,41 @@\n import java.util.Iterator;\n \n import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.OutputCollector;\n import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hama.DenseMatrix;\n import org.apache.hama.DenseVector;\n+import org.apache.hama.HamaConfiguration;\n+import org.apache.hama.Matrix;\n import org.apache.hama.Vector;\n import org.apache.hama.io.VectorEntry;\n import org.apache.hama.mapred.DenseMap;\n import org.apache.log4j.Logger;\n \n public class MultiplicationMap extends DenseMap<IntWritable, DenseVector> {\n   static final Logger LOG = Logger.getLogger(MultiplicationMap.class);\n-\n+  protected Matrix matrix_b;\n+  public static final String MATRIX_B = \"hama.multiplication.matrix.b\";\n+  \n+  public void configure(JobConf job) {\n+    matrix_b = new DenseMatrix(new HamaConfiguration(), job.get(MATRIX_B, \"\"));\n+  }\n+  \n+  public static void initJob(String matrix_a, String matrix_b,\n+      Class<MultiplicationMap> map, \n+      Class<IntWritable> outputKeyClass, \n+      Class<DenseVector> outputValueClass, \n+      JobConf jobConf) {\n+    \n+    jobConf.setMapOutputValueClass(outputValueClass);\n+    jobConf.setMapOutputKeyClass(outputKeyClass);\n+    jobConf.setMapperClass(map);\n+    jobConf.set(MATRIX_B, matrix_b);\n+    \n+    initJob(matrix_a, map, jobConf);\n+  }\n+  \n   @Override\n   public void map(IntWritable key, DenseVector value,\n       OutputCollector<IntWritable, DenseVector> output, Reporter reporter)\n@@ -42,7 +66,7 @@ public void map(IntWritable key, DenseVector value,\n     Iterator<VectorEntry> it = value.iterator();\n     int i = 0;\n     while (it.hasNext()) {\n-      Vector v = MATRIX_B.getRow(i);\n+      Vector v = matrix_b.getRow(i);\n       output.collect(key, (DenseVector) v.scale(it.next().getValue()));\n       i++;\n     }",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/algebra/MultiplicationMap.java",
                "sha": "2d4e78887699f45d08f493d60c16a9b1847adf38",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/mapred/DenseMap.java",
                "changes": 118,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/java/org/apache/hama/mapred/DenseMap.java?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 62,
                "filename": "src/java/org/apache/hama/mapred/DenseMap.java",
                "patch": "@@ -1,62 +1,56 @@\n-/**\r\n- * Copyright 2007 The Apache Software Foundation\r\n- *\r\n- * Licensed to the Apache Software Foundation (ASF) under one\r\n- * or more contributor license agreements.  See the NOTICE file\r\n- * distributed with this work for additional information\r\n- * regarding copyright ownership.  The ASF licenses this file\r\n- * to you under the Apache License, Version 2.0 (the\r\n- * \"License\"); you may not use this file except in compliance\r\n- * with the License.  You may obtain a copy of the License at\r\n- *\r\n- *     http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-package org.apache.hama.mapred;\r\n-\r\n-import java.io.IOException;\r\n-\r\n-import org.apache.hadoop.io.IntWritable;\r\n-import org.apache.hadoop.io.Writable;\r\n-import org.apache.hadoop.io.WritableComparable;\r\n-import org.apache.hadoop.mapred.FileInputFormat;\r\n-import org.apache.hadoop.mapred.JobConf;\r\n-import org.apache.hadoop.mapred.MapReduceBase;\r\n-import org.apache.hadoop.mapred.Mapper;\r\n-import org.apache.hadoop.mapred.OutputCollector;\r\n-import org.apache.hadoop.mapred.Reporter;\r\n-import org.apache.hama.Constants;\r\n-import org.apache.hama.DenseMatrix;\r\n-import org.apache.hama.DenseVector;\r\n-import org.apache.hama.HamaConfiguration;\r\n-import org.apache.hama.Matrix;\r\n-\r\n-@SuppressWarnings(\"unchecked\")\r\n-public abstract class DenseMap<K extends WritableComparable, V extends Writable>\r\n-    extends MapReduceBase implements\r\n-    Mapper<IntWritable, DenseVector, K, V> {\r\n-  public static Matrix MATRIX_B;\r\n-\r\n-  public static void initJob(String matrixA, String matrixB,\r\n-      Class<? extends DenseMap> mapper,\r\n-      Class<? extends WritableComparable> outputKeyClass,\r\n-      Class<? extends Writable> outputValueClass, JobConf job) {\r\n-\r\n-    job.setInputFormat(MatrixInputFormat.class);\r\n-    job.setMapOutputValueClass(outputValueClass);\r\n-    job.setMapOutputKeyClass(outputKeyClass);\r\n-    job.setMapperClass(mapper);\r\n-    FileInputFormat.addInputPaths(job, matrixA);\r\n-\r\n-    MATRIX_B = new DenseMatrix(new HamaConfiguration(), matrixB);\r\n-    job.set(MatrixInputFormat.COLUMN_LIST, Constants.COLUMN);\r\n-  }\r\n-\r\n-  public abstract void map(IntWritable key, DenseVector value,\r\n-      OutputCollector<K, V> output, Reporter reporter) throws IOException;\r\n-}\r\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hama.mapred;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.WritableComparable;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.MapReduceBase;\n+import org.apache.hadoop.mapred.Mapper;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hama.Constants;\n+import org.apache.hama.DenseVector;\n+import org.apache.hama.Matrix;\n+\n+@SuppressWarnings(\"unchecked\")\n+public abstract class DenseMap<K extends WritableComparable, V extends Writable>\n+    extends MapReduceBase implements\n+    Mapper<IntWritable, DenseVector, K, V> {\n+  public static Matrix MATRIX_B;\n+\n+  public static void initJob(String matrixA, \n+      Class<? extends DenseMap> mapper,\n+      JobConf job) {\n+\n+    job.setInputFormat(MatrixInputFormat.class);\n+    job.setMapperClass(mapper);\n+    FileInputFormat.addInputPaths(job, matrixA);\n+\n+    job.set(MatrixInputFormat.COLUMN_LIST, Constants.COLUMN);\n+  }\n+\n+  public abstract void map(IntWritable key, DenseVector value,\n+      OutputCollector<K, V> output, Reporter reporter) throws IOException;\n+}",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/java/org/apache/hama/mapred/DenseMap.java",
                "sha": "53b3a3b0d9a360a76f45da7daeb09fa71ad76f93",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hama/blob/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/test/org/apache/hama/mapred/TestMatrixMapReduce.java",
                "changes": 158,
                "contents_url": "https://api.github.com/repos/apache/hama/contents/src/test/org/apache/hama/mapred/TestMatrixMapReduce.java?ref=d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db",
                "deletions": 79,
                "filename": "src/test/org/apache/hama/mapred/TestMatrixMapReduce.java",
                "patch": "@@ -1,79 +1,79 @@\n-/**\r\n- * Copyright 2007 The Apache Software Foundation\r\n- *\r\n- * Licensed to the Apache Software Foundation (ASF) under one\r\n- * or more contributor license agreements.  See the NOTICE file\r\n- * distributed with this work for additional information\r\n- * regarding copyright ownership.  The ASF licenses this file\r\n- * to you under the Apache License, Version 2.0 (the\r\n- * \"License\"); you may not use this file except in compliance\r\n- * with the License.  You may obtain a copy of the License at\r\n- *\r\n- *     http://www.apache.org/licenses/LICENSE-2.0\r\n- *\r\n- * Unless required by applicable law or agreed to in writing, software\r\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n- * See the License for the specific language governing permissions and\r\n- * limitations under the License.\r\n- */\r\n-package org.apache.hama.mapred;\r\n-\r\n-import java.io.IOException;\r\n-\r\n-import org.apache.hadoop.io.IntWritable;\r\n-import org.apache.hadoop.mapred.JobClient;\r\n-import org.apache.hadoop.mapred.JobConf;\r\n-import org.apache.hama.DenseMatrix;\r\n-import org.apache.hama.DenseVector;\r\n-import org.apache.hama.HCluster;\r\n-import org.apache.hama.Matrix;\r\n-import org.apache.hama.algebra.AdditionMap;\r\n-import org.apache.hama.algebra.AdditionReduce;\r\n-import org.apache.log4j.Logger;\r\n-\r\n-/**\r\n- * Test Matrix Map/Reduce job\r\n- */\r\n-public class TestMatrixMapReduce extends HCluster {\r\n-  static final Logger LOG = Logger.getLogger(TestMatrixMapReduce.class);\r\n-  private String A = \"matrixA\";\r\n-  private String B = \"matrixB\";\r\n-  private String output = \"output\";\r\n-\r\n-  /** constructor */\r\n-  public TestMatrixMapReduce() {\r\n-    super();\r\n-  }\r\n-\r\n-  public void testMatrixMapReduce() throws IOException {\r\n-    Matrix matrixA = new DenseMatrix(conf, A);\r\n-    matrixA.set(0, 0, 1);\r\n-    matrixA.set(0, 1, 0);\r\n-\r\n-    Matrix matrixB = new DenseMatrix(conf, B);\r\n-    matrixB.set(0, 0, 1);\r\n-    matrixB.set(0, 1, 1);\r\n-\r\n-    miniMRJob();\r\n-  }\r\n-\r\n-  public void miniMRJob() throws IOException {\r\n-    Matrix c = new DenseMatrix(conf, output);\r\n-\r\n-    JobConf jobConf = new JobConf(conf, TestMatrixMapReduce.class);\r\n-    jobConf.setJobName(\"test MR job\");\r\n-\r\n-    DenseMap.initJob(A, B, AdditionMap.class, IntWritable.class,\r\n-        DenseVector.class, jobConf);\r\n-    MatrixReduce.initJob(output, AdditionReduce.class, jobConf);\r\n-\r\n-    jobConf.setNumMapTasks(1);\r\n-    jobConf.setNumReduceTasks(1);\r\n-\r\n-    JobClient.runJob(jobConf);\r\n-\r\n-    assertEquals(c.get(0, 0), 2.0);\r\n-    assertEquals(c.get(0, 1), 1.0);\r\n-  }\r\n-}\r\n+/**\n+ * Copyright 2007 The Apache Software Foundation\n+ *\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hama.mapred;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hama.DenseMatrix;\n+import org.apache.hama.DenseVector;\n+import org.apache.hama.HCluster;\n+import org.apache.hama.Matrix;\n+import org.apache.hama.algebra.AdditionMap;\n+import org.apache.hama.algebra.AdditionReduce;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * Test Matrix Map/Reduce job\n+ */\n+public class TestMatrixMapReduce extends HCluster {\n+  static final Logger LOG = Logger.getLogger(TestMatrixMapReduce.class);\n+  private String A = \"matrixA\";\n+  private String B = \"matrixB\";\n+  private String output = \"output\";\n+\n+  /** constructor */\n+  public TestMatrixMapReduce() {\n+    super();\n+  }\n+\n+  public void testMatrixMapReduce() throws IOException {\n+    Matrix matrixA = new DenseMatrix(conf, A);\n+    matrixA.set(0, 0, 1);\n+    matrixA.set(0, 1, 0);\n+\n+    Matrix matrixB = new DenseMatrix(conf, B);\n+    matrixB.set(0, 0, 1);\n+    matrixB.set(0, 1, 1);\n+\n+    miniMRJob();\n+  }\n+\n+  public void miniMRJob() throws IOException {\n+    Matrix c = new DenseMatrix(conf, output);\n+\n+    JobConf jobConf = new JobConf(conf, TestMatrixMapReduce.class);\n+    jobConf.setJobName(\"test MR job\");\n+\n+    AdditionMap.initJob(A, B, AdditionMap.class, IntWritable.class,\n+        DenseVector.class, jobConf);\n+    MatrixReduce.initJob(output, AdditionReduce.class, jobConf);\n+\n+    jobConf.setNumMapTasks(1);\n+    jobConf.setNumReduceTasks(1);\n+\n+    JobClient.runJob(jobConf);\n+\n+    assertEquals(c.get(0, 0), 2.0);\n+    assertEquals(c.get(0, 1), 1.0);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hama/raw/d8f737f269c14a15f50dd0bd9d9fbd878b4ce0db/src/test/org/apache/hama/mapred/TestMatrixMapReduce.java",
                "sha": "64588d147b7f9e7998dd1367d5c84ae2de0dfe8f",
                "status": "modified"
            }
        ],
        "message": "NullPointerException on distributed cluster\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/hama/trunk@693057 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hama/commit/c549c30a2a90cb853520099fe23580ee9d02fa96",
        "patched_files": [
            "CHANGES.java",
            "AdditionMap.java",
            "DenseMatrix.java",
            "DenseMap.java",
            "MultiplicationMap.java",
            "MatrixAddition.java"
        ],
        "repo": "hama",
        "unit_tests": [
            "TestMatrixMapReduce.java"
        ]
    }
}