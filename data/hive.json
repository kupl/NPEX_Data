{
    "hive_01fe664": {
        "bug_id": "hive_01fe664",
        "commit": "https://github.com/apache/hive/commit/01fe6647430ef5dfcc120f93bdcab40677166c7c",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java",
                "patch": "@@ -75,6 +75,21 @@ public void testTxns() throws Exception {\n     Assert.assertFalse(validTxns.isTxnCommitted(4));\n   }\n \n+  @Test\n+  public void testOpenTxnNotExcluded() throws Exception {\n+    List<Long> tids = client.openTxns(\"me\", 3).getTxn_ids();\n+    Assert.assertEquals(1L, (long) tids.get(0));\n+    Assert.assertEquals(2L, (long) tids.get(1));\n+    Assert.assertEquals(3L, (long) tids.get(2));\n+    client.rollbackTxn(1);\n+    client.commitTxn(2);\n+    ValidTxnList validTxns = client.getValidTxns(3);\n+    Assert.assertFalse(validTxns.isTxnCommitted(1));\n+    Assert.assertTrue(validTxns.isTxnCommitted(2));\n+    Assert.assertTrue(validTxns.isTxnCommitted(3));\n+    Assert.assertFalse(validTxns.isTxnCommitted(4));\n+  }\n+\n   @Test\n   public void testTxnRange() throws Exception {\n     ValidTxnList validTxns = client.getValidTxns();",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java",
                "sha": "7f0a6b3b7daecb9cafb5034877d43ee0aeec882b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "patch": "@@ -1689,7 +1689,12 @@ public void cancelDelegationToken(String tokenStrForm) throws MetaException, TEx\n \n   @Override\n   public ValidTxnList getValidTxns() throws TException {\n-    return TxnHandler.createValidTxnList(client.get_open_txns());\n+    return TxnHandler.createValidTxnList(client.get_open_txns(), 0);\n+  }\n+\n+  @Override\n+  public ValidTxnList getValidTxns(long currentTxn) throws TException {\n+    return TxnHandler.createValidTxnList(client.get_open_txns(), currentTxn);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "sha": "94c22450e417b3ffee293aef5a848ddcfc339c49",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 0,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java",
                "patch": "@@ -1085,6 +1085,15 @@ Function getFunction(String dbName, String funcName)\n    */\n   ValidTxnList getValidTxns() throws TException;\n \n+  /**\n+   * Get a structure that details valid transactions.\n+   * @param currentTxn The current transaction of the caller.  This will be removed from the\n+   *                   exceptions list so that the caller sees records from his own transaction.\n+   * @return list of valid transactions\n+   * @throws TException\n+   */\n+  ValidTxnList getValidTxns(long currentTxn) throws TException;\n+\n   /**\n    * Initiate a transaction.\n    * @param user User who is opening this transaction.  This is the Hive user,",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java",
                "sha": "066ab68443fdd1bd04e14c8a3b1b2bbac9130d6c",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "patch": "@@ -233,12 +233,22 @@ public GetOpenTxnsResponse getOpenTxns() throws MetaException {\n     }\n   }\n \n-  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns) {\n+  /**\n+   * Transform a {@link org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse} to a\n+   * {@link org.apache.hadoop.hive.common.ValidTxnList}.\n+   * @param txns txn list from the metastore\n+   * @param currentTxn Current transaction that the user has open.  If this is greater than 0 it\n+   *                   will be removed from the exceptions list so that the user sees his own\n+   *                   transaction as valid.\n+   * @return a valid txn list.\n+   */\n+  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns, long currentTxn) {\n     long highWater = txns.getTxn_high_water_mark();\n     Set<Long> open = txns.getOpen_txns();\n-    long[] exceptions = new long[open.size()];\n+    long[] exceptions = new long[open.size() - (currentTxn > 0 ? 1 : 0)];\n     int i = 0;\n     for(long txn: open) {\n+      if (currentTxn > 0 && currentTxn == txn) continue;\n       exceptions[i++] = txn;\n     }\n     return new ValidTxnListImpl(exceptions, highWater);",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "sha": "6f44169af90e747838bd7e2ccb5652f59ac26475",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 11,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "patch": "@@ -390,6 +390,9 @@ public int compile(String command, boolean resetTaskIds) {\n       tree = ParseUtils.findRootNonNullToken(tree);\n       perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.PARSE);\n \n+      // Initialize the transaction manager.  This must be done before analyze is called\n+      SessionState.get().initTxnMgr(conf);\n+\n       perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ANALYZE);\n       BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);\n       List<HiveSemanticAnalyzerHook> saHooks =\n@@ -889,9 +892,12 @@ private int recordValidTxns() {\n \n   /**\n    * Acquire read and write locks needed by the statement. The list of objects to be locked are\n-   * obtained from he inputs and outputs populated by the compiler. The lock acuisition scheme is\n+   * obtained from the inputs and outputs populated by the compiler. The lock acuisition scheme is\n    * pretty simple. If all the locks cannot be obtained, error out. Deadlock is avoided by making\n    * sure that the locks are lexicographically sorted.\n+   *\n+   * This method also records the list of valid transactions.  This must be done after any\n+   * transactions have been opened and locks acquired.\n    **/\n   private int acquireLocksAndOpenTxn() {\n     PerfLogger perfLogger = PerfLogger.getPerfLogger();\n@@ -931,7 +937,7 @@ private int acquireLocksAndOpenTxn() {\n \n       txnMgr.acquireLocks(plan, ctx, userFromUGI);\n \n-      return 0;\n+      return recordValidTxns();\n     } catch (LockException e) {\n       errorMessage = \"FAILED: Error in acquiring locks: \" + e.getMessage();\n       SQLState = ErrorMsg.findSQLState(e.getMessage());\n@@ -1108,11 +1114,6 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp\n     SessionState ss = SessionState.get();\n     try {\n       ckLock = checkConcurrency();\n-      try {\n-        ss.initTxnMgr(conf);\n-      } catch (LockException e) {\n-        throw new SemanticException(e.getMessage(), e);\n-      }\n     } catch (SemanticException e) {\n       errorMessage = \"FAILED: Error in semantic analysis: \" + e.getMessage();\n       SQLState = ErrorMsg.findSQLState(e.getMessage());\n@@ -1121,11 +1122,8 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp\n           + org.apache.hadoop.util.StringUtils.stringifyException(e));\n       return createProcessorResponse(10);\n     }\n-    int ret = recordValidTxns();\n-    if (ret != 0) {\n-      return createProcessorResponse(ret);\n-    }\n \n+    int ret;\n     if (!alreadyCompiled) {\n       ret = compileInternal(command);\n       if (ret != 0) {",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "sha": "4826abcc4fce6102642480c576bd823ec59f9743",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java",
                "patch": "@@ -286,7 +286,7 @@ public void heartbeat() throws LockException {\n   public ValidTxnList getValidTxns() throws LockException {\n     init();\n     try {\n-      return client.getValidTxns();\n+      return client.getValidTxns(txnId);\n     } catch (TException e) {\n       throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n           e);",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java",
                "sha": "46b441a13d6d981dd6aea5dfc1c0a2abad884cc1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "patch": "@@ -76,7 +76,7 @@ public void run() {\n         // don't doom the entire thread.\n         try {\n           ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());\n-          ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns());\n+          ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n           Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);\n           LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n               \"checking to see if we should compact any of them\");",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "sha": "c1d0fe11c709c90996ce4d18bd41199d3257d780",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "patch": "@@ -120,7 +120,7 @@ public void run() {\n \n         final boolean isMajor = ci.isMajorCompaction();\n         final ValidTxnList txns =\n-            TxnHandler.createValidTxnList(txnHandler.getOpenTxns());\n+            TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n         final StringBuffer jobName = new StringBuffer(name);\n         jobName.append(\"-compactor-\");\n         jobName.append(ci.getFullPartitionName());",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "sha": "249fece426f05b7c2757d8fd52c6959a80941091",
                "status": "modified"
            }
        ],
        "message": "HIVE-8203 ACID operations result in NPE when run through HS2 (Alan Gates, reviewed by Eugene Koifman)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1627914 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/b30756c2dac8b3d9adfdb131e3185498d4dd322b",
        "repo": "hive",
        "unit_tests": [
            "TestWorker.java"
        ]
    },
    "hive_02629e9": {
        "bug_id": "hive_02629e9",
        "commit": "https://github.com/apache/hive/commit/02629e9794e228dcaa8d446423a256d75f71d6dd",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/02629e9794e228dcaa8d446423a256d75f71d6dd/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java?ref=02629e9794e228dcaa8d446423a256d75f71d6dd",
                "deletions": 9,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java",
                "patch": "@@ -33,7 +33,6 @@\n import java.io.DataOutput;\n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Collections;\n import java.util.List;\n \n /**\n@@ -115,11 +114,14 @@\n     private List<Integer> stmtIds;\n     \n     public DeltaMetaData() {\n-      this(0,0,null);\n+      this(0,0,new ArrayList<Integer>());\n     }\n     DeltaMetaData(long minTxnId, long maxTxnId, List<Integer> stmtIds) {\n       this.minTxnId = minTxnId;\n       this.maxTxnId = maxTxnId;\n+      if (stmtIds == null) {\n+        throw new IllegalArgumentException(\"stmtIds == null\");\n+      }\n       this.stmtIds = stmtIds;\n     }\n     long getMinTxnId() {\n@@ -136,9 +138,6 @@ public void write(DataOutput out) throws IOException {\n       out.writeLong(minTxnId);\n       out.writeLong(maxTxnId);\n       out.writeInt(stmtIds.size());\n-      if(stmtIds == null) {\n-        return;\n-      }\n       for(Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n@@ -147,11 +146,8 @@ public void write(DataOutput out) throws IOException {\n     public void readFields(DataInput in) throws IOException {\n       minTxnId = in.readLong();\n       maxTxnId = in.readLong();\n+      stmtIds.clear();\n       int numStatements = in.readInt();\n-      if(numStatements <= 0) {\n-        return;\n-      }\n-      stmtIds = new ArrayList<>();\n       for(int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }",
                "raw_url": "https://github.com/apache/hive/raw/02629e9794e228dcaa8d446423a256d75f71d6dd/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java",
                "sha": "7c7074d156fb5f0ace46fb561e869235024d335a",
                "status": "modified"
            }
        ],
        "message": "HIVE-12202 NPE thrown when reading legacy ACID delta files(Elliot West via Eugene Koifman)",
        "parent": "https://github.com/apache/hive/commit/47617d31f347a0ba78ebfc903738b39dd960b19b",
        "repo": "hive",
        "unit_tests": [
            "TestAcidInputFormat.java"
        ]
    },
    "hive_042b2ef": {
        "bug_id": "hive_042b2ef",
        "commit": "https://github.com/apache/hive/commit/042b2ef7df6af8b93adeb936d94c4079153467ff",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff",
                "deletions": 12,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "patch": "@@ -1352,21 +1352,23 @@ private int getNextFreeListItem(int offset) {\n \n     public void deallocate(LlapAllocatorBuffer buffer, boolean isAfterMove) {\n       assert data != null;\n-      int pos = buffer.byteBuffer.position();\n-      // Note: this is called by someone who has ensured the buffer is not going to be moved.\n-      int headerIx = pos >>> minAllocLog2;\n-      int freeListIx = freeListFromAllocSize(buffer.allocSize);\n-      if (assertsEnabled && !isAfterMove) {\n-        LlapAllocatorBuffer buf = buffers[headerIx];\n-        if (buf != buffer) {\n-          failWithLog(arenaIx + \":\" + headerIx + \" => \"\n+      if (buffer != null && buffer.byteBuffer != null) {\n+        int pos = buffer.byteBuffer.position();\n+        // Note: this is called by someone who has ensured the buffer is not going to be moved.\n+        int headerIx = pos >>> minAllocLog2;\n+        int freeListIx = freeListFromAllocSize(buffer.allocSize);\n+        if (assertsEnabled && !isAfterMove) {\n+          LlapAllocatorBuffer buf = buffers[headerIx];\n+          if (buf != buffer) {\n+            failWithLog(arenaIx + \":\" + headerIx + \" => \"\n               + toDebugString(buffer) + \", \" + toDebugString(buf));\n+          }\n+          assertBufferLooksValid(freeListFromHeader(headers[headerIx]), buf, arenaIx, headerIx);\n+          checkHeader(headerIx, freeListIx, true);\n         }\n-        assertBufferLooksValid(freeListFromHeader(headers[headerIx]), buf, arenaIx, headerIx);\n-        checkHeader(headerIx, freeListIx, true);\n+        buffers[headerIx] = null;\n+        addToFreeListWithMerge(headerIx, freeListIx, buffer, CasLog.Src.DEALLOC);\n       }\n-      buffers[headerIx] = null;\n-      addToFreeListWithMerge(headerIx, freeListIx, buffer, CasLog.Src.DEALLOC);\n     }\n \n     private void addToFreeListWithMerge(int headerIx, int freeListIx,",
                "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "sha": "013f3538b4b75f62f9e4c40ddbdd9d1b0b8ed8b1",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff",
                "deletions": 0,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.orc.impl.RecordReaderUtils;\n \n import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -40,6 +41,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Function;\n+import com.google.common.base.Joiner;\n \n public class LowLevelCacheImpl implements LowLevelCache, BufferUsageManager, LlapIoDebugDump {\n   private static final int DEFAULT_CLEANUP_INTERVAL = 600;\n@@ -457,6 +459,10 @@ public void debugDumpShort(StringBuilder sb) {\n       try {\n         int fileLocked = 0, fileUnlocked = 0, fileEvicted = 0, fileMoving = 0;\n         if (e.getValue().getCache().isEmpty()) continue;\n+        List<LlapDataBuffer> lockedBufs = null;\n+        if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+          lockedBufs = new ArrayList<>();\n+        }\n         for (Map.Entry<Long, LlapDataBuffer> e2 : e.getValue().getCache().entrySet()) {\n           int newRc = e2.getValue().tryIncRef();\n           if (newRc < 0) {\n@@ -470,6 +476,9 @@ public void debugDumpShort(StringBuilder sb) {\n           try {\n             if (newRc > 1) { // We hold one refcount.\n               ++fileLocked;\n+              if (lockedBufs != null) {\n+                lockedBufs.add(e2.getValue());\n+              }\n             } else {\n               ++fileUnlocked;\n             }\n@@ -483,6 +492,9 @@ public void debugDumpShort(StringBuilder sb) {\n         allMoving += fileMoving;\n         sb.append(\"\\n  file \" + e.getKey() + \": \" + fileLocked + \" locked, \" + fileUnlocked\n             + \" unlocked, \" + fileEvicted + \" evicted, \" + fileMoving + \" being moved\");\n+        if (fileLocked > 0 && LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+          LlapIoImpl.LOCKING_LOGGER.trace(\"locked-buffers: {}\", lockedBufs);\n+        }\n       } finally {\n         e.getValue().decRef();\n       }",
                "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "sha": "e012d7dbf9f5dab0bcc88c8cf680dc6ffd8baf71",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "patch": "@@ -1964,7 +1964,9 @@ public void readIndexStreams(OrcIndex index, StripeInformation stripe,\n     } finally {\n       // Release the unreleased buffers. See class comment about refcounts.\n       try {\n-        releaseInitialRefcounts(toRead.next);\n+        if (toRead != null) {\n+          releaseInitialRefcounts(toRead.next);\n+        }\n         releaseBuffers(toRelease.keySet(), true);\n       } catch (Throwable t) {\n         if (!hasError) throw new IOException(t);",
                "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "sha": "759594aea33b468abcf58a4930a8081176c98a51",
                "status": "modified"
            }
        ],
        "message": "HIVE-20249 : LLAP IO: NPE during refCount decrement (Prasanth Jayachandran, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/d4b7b93e27a8a28dde8f4584d883faba86f0203c",
        "repo": "hive",
        "unit_tests": [
            "TestEncodedReaderImpl.java"
        ]
    },
    "hive_0af2fb0": {
        "bug_id": "hive_0af2fb0",
        "commit": "https://github.com/apache/hive/commit/0af2fb0cac270cd6223a4abaf6e410f05450bfef",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/0af2fb0cac270cd6223a4abaf6e410f05450bfef/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java?ref=0af2fb0cac270cd6223a4abaf6e410f05450bfef",
                "deletions": 8,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java",
                "patch": "@@ -1992,17 +1992,18 @@ public static void registerTableFunction(String name, Class<? extends TableFunct\n    * @return true if function is a UDAF, has WindowFunctionDescription annotation and the annotations\n    *         confirms a ranking function, false otherwise\n    */\n-  public static boolean isRankingFunction(String name){\n+  public static boolean isRankingFunction(String name) {\n     FunctionInfo info = getFunctionInfo(name);\n+    if (info == null) {\n+      return false;\n+    }\n     GenericUDAFResolver res = info.getGenericUDAFResolver();\n-    if (res != null){\n-      WindowFunctionDescription desc =\n-          AnnotationUtils.getAnnotation(res.getClass(), WindowFunctionDescription.class);\n-      if (desc != null){\n-        return desc.rankingFunction();\n-      }\n+    if (res == null) {\n+      return false;\n     }\n-    return false;\n+    WindowFunctionDescription desc =\n+        AnnotationUtils.getAnnotation(res.getClass(), WindowFunctionDescription.class);\n+    return (desc != null) && desc.rankingFunction();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/0af2fb0cac270cd6223a4abaf6e410f05450bfef/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java",
                "sha": "451598aa964786e118307643cf92c6f31fcab9aa",
                "status": "modified"
            }
        ],
        "message": "HIVE-8551 : NPE in FunctionRegistry (affects CBO in negative tests) (Sergey Shelukhin, reviewed by Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633692 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/4dd080d286a6db1df33ad643ac618407c1af1a45",
        "repo": "hive",
        "unit_tests": [
            "TestFunctionRegistry.java"
        ]
    },
    "hive_0b81099": {
        "bug_id": "hive_0b81099",
        "commit": "https://github.com/apache/hive/commit/0b810991a12bb7c8d52f64b781772a1deabcbe53",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=0b810991a12bb7c8d52f64b781772a1deabcbe53",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -215,6 +215,7 @@\n   public static final String MAPRED_MAPPER_CLASS = \"mapred.mapper.class\";\n   public static final String MAPRED_REDUCER_CLASS = \"mapred.reducer.class\";\n   public static final String HIVE_ADDED_JARS = \"hive.added.jars\";\n+  public static final String VECTOR_MODE = \"VECTOR_MODE\";\n   public static String MAPNAME = \"Map \";\n   public static String REDUCENAME = \"Reducer \";\n \n@@ -3238,12 +3239,18 @@ private static void resetUmaskInConf(Configuration conf, boolean unsetUmask, Str\n    * but vectorization disallowed eg. for FetchOperator execution.\n    */\n   public static boolean isVectorMode(Configuration conf) {\n-    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) &&\n-        Utilities.getPlanPath(conf) != null && Utilities\n-        .getMapWork(conf).getVectorMode()) {\n-      return true;\n+    if (conf.get(VECTOR_MODE) != null) {\n+      // this code path is necessary, because with HS2 and client\n+      // side split generation we end up not finding the map work.\n+      // This is because of thread local madness (tez split\n+      // generation is multi-threaded - HS2 plan cache uses thread\n+      // locals).\n+      return conf.getBoolean(VECTOR_MODE, false);\n+    } else {\n+      return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED)\n+        && Utilities.getPlanPath(conf) != null\n+        && Utilities.getMapWork(conf).getVectorMode();\n     }\n-    return false;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "fce11c86fcdf1e9c72575ce5025208e1018a3591",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=0b810991a12bb7c8d52f64b781772a1deabcbe53",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -614,6 +614,15 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       }\n     } else {\n       // Setup client side split generation.\n+\n+      // we need to set this, because with HS2 and client side split\n+      // generation we end up not finding the map work. This is\n+      // because of thread local madness (tez split generation is\n+      // multi-threaded - HS2 plan cache uses thread locals). Setting\n+      // VECTOR_MODE causes the split gen code to use the conf instead\n+      // of the map work.\n+      conf.setBoolean(Utilities.VECTOR_MODE, mapWork.getVectorMode());\n+\n       dataSource = MRInputHelpers.configureMRInputWithLegacySplitGeneration(conf, new Path(tezDir,\n           \"split_\" + mapWork.getName().replaceAll(\" \", \"_\")), true);\n       numTasks = dataSource.getNumberOfShards();",
                "raw_url": "https://github.com/apache/hive/raw/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "e8864aee6428d3cc6b165acbc188cac8d22ffd61",
                "status": "modified"
            }
        ],
        "message": "HIVE-12740: NPE with HS2 when using null input format (Vikram Dixit K via Gunther Hagleitner)",
        "parent": "https://github.com/apache/hive/commit/6e513b06c12e508af655a6bea4aef55b86619cc6",
        "repo": "hive",
        "unit_tests": [
            "TestDagUtils.java"
        ]
    },
    "hive_13938db": {
        "bug_id": "hive_13938db",
        "commit": "https://github.com/apache/hive/commit/13938db46ff89fc4a425854b4795df604ced8ba9",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/13938db46ff89fc4a425854b4795df604ced8ba9/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java?ref=13938db46ff89fc4a425854b4795df604ced8ba9",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java",
                "patch": "@@ -3044,8 +3044,13 @@ private void testInsertOverwrite(HiveStatement stmt) throws SQLException {\n   public void testGetQueryId() throws Exception {\n     HiveStatement stmt = (HiveStatement) con.createStatement();\n     HiveStatement stmt1 = (HiveStatement) con.createStatement();\n-    stmt.executeAsync(\"create database query_id_test with dbproperties ('repl.source.for' = '1, 2, 3')\");\n+\n+    // Returns null if no query is running.\n     String queryId = stmt.getQueryId();\n+    assertTrue(queryId == null);\n+\n+    stmt.executeAsync(\"create database query_id_test with dbproperties ('repl.source.for' = '1, 2, 3')\");\n+    queryId = stmt.getQueryId();\n     assertFalse(queryId.isEmpty());\n     stmt.getUpdateCount();\n ",
                "raw_url": "https://github.com/apache/hive/raw/13938db46ff89fc4a425854b4795df604ced8ba9/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java",
                "sha": "14187cc83bfd6f90496477de828dce773de605ac",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/13938db46ff89fc4a425854b4795df604ced8ba9/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java?ref=13938db46ff89fc4a425854b4795df604ced8ba9",
                "deletions": 3,
                "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "patch": "@@ -18,8 +18,8 @@\n \n package org.apache.hive.jdbc;\n \n-import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.codec.binary.Base64;\n+import org.apache.hadoop.hive.common.classification.InterfaceAudience.LimitedPrivate;\n import org.apache.hive.jdbc.logs.InPlaceUpdateStream;\n import org.apache.hive.service.cli.RowSet;\n import org.apache.hive.service.cli.RowSetFactory;\n@@ -37,7 +37,6 @@\n import org.apache.hive.service.rpc.thrift.TGetOperationStatusResp;\n import org.apache.hive.service.rpc.thrift.TGetQueryIdReq;\n import org.apache.hive.service.rpc.thrift.TOperationHandle;\n-import org.apache.hive.service.rpc.thrift.TOperationState;\n import org.apache.hive.service.rpc.thrift.TSessionHandle;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -1007,12 +1006,26 @@ public void setInPlaceUpdateStream(InPlaceUpdateStream stream) {\n     this.inPlaceUpdateStream = stream;\n   }\n \n-  @VisibleForTesting\n+  /**\n+   * Returns the Query ID if it is running.\n+   * This method is a public API for usage outside of Hive, although it is not part of the\n+   * interface java.sql.Statement.\n+   * @return Valid query ID if it is running else returns NULL.\n+   * @throws SQLException If any internal failures.\n+   */\n+  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n   public String getQueryId() throws SQLException {\n+    if (stmtHandle == null) {\n+      // If query is not running or already closed.\n+      return null;\n+    }\n     try {\n       return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n     } catch (TException e) {\n       throw new SQLException(e);\n+    } catch (Exception e) {\n+      // If concurrently the query is closed before we fetch queryID.\n+      return null;\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/13938db46ff89fc4a425854b4795df604ced8ba9/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "sha": "d9b625466ca8880c739977f52eabc92ce44aa98d",
                "status": "modified"
            }
        ],
        "message": "HIVE-21421: HiveStatement.getQueryId throws NPE when query is not running (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/9f2f101feec4283f020b72b918e0961215e76789",
        "repo": "hive",
        "unit_tests": [
            "TestHiveStatement.java"
        ]
    },
    "hive_1a3e4be": {
        "bug_id": "hive_1a3e4be",
        "commit": "https://github.com/apache/hive/commit/1a3e4be3dbd485f2630c7249254727ce58374d1c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/1a3e4be3dbd485f2630c7249254727ce58374d1c/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java?ref=1a3e4be3dbd485f2630c7249254727ce58374d1c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "patch": "@@ -1864,7 +1864,7 @@ public Void call() throws Exception {\n         for (Partition p : partitionsMap.values()) {\n           partNames.add(p.getName());\n         }\n-        metaStoreClient.addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n+        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n           partNames, AcidUtils.toDataOperationType(operation));\n       }\n       LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");",
                "raw_url": "https://github.com/apache/hive/raw/1a3e4be3dbd485f2630c7249254727ce58374d1c/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "sha": "de6adb5047bf6457d031050b6e72d54a6534ccfa",
                "status": "modified"
            }
        ],
        "message": "HIVE-14814: metastoreClient is used directly in Hive cause NPE (Prasanth Jayachandran reviewed by Eugene Koifman)",
        "parent": "https://github.com/apache/hive/commit/c9224d58cce6e0b0520598894e962c48ce9d97e3",
        "repo": "hive",
        "unit_tests": [
            "TestHive.java"
        ]
    },
    "hive_1be4ee5": {
        "bug_id": "hive_1be4ee5",
        "commit": "https://github.com/apache/hive/commit/1be4ee57cd380866f70b22eeea17fc5e896f085d",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/1be4ee57cd380866f70b22eeea17fc5e896f085d/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java?ref=1be4ee57cd380866f70b22eeea17fc5e896f085d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java",
                "patch": "@@ -302,6 +302,11 @@ public int executeInProcess(DriverContext driverContext) {\n     if (work == null) {\n       return -1;\n     }\n+\n+    if (execContext == null) {\n+      execContext = new ExecMapperContext(job);\n+    }\n+\n     memoryMXBean = ManagementFactory.getMemoryMXBean();\n     long startTime = System.currentTimeMillis();\n     console.printInfo(Utilities.now()",
                "raw_url": "https://github.com/apache/hive/raw/1be4ee57cd380866f70b22eeea17fc5e896f085d/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java",
                "sha": "79da5a055dae3d2d100d98050d710a3e246324da",
                "status": "modified"
            }
        ],
        "message": "HIVE-8325: NPE in map join execution (submit via child) (Gunther Hagleitner, reviewed by Szehon Ho)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1628885 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/92ab5a0e15a7edbad6468fcd2263c85b04eb91e3",
        "repo": "hive",
        "unit_tests": [
            "TestMapredLocalTask.java"
        ]
    },
    "hive_1c33fea": {
        "bug_id": "hive_1c33fea",
        "commit": "https://github.com/apache/hive/commit/1c33fea890bc01a85eb336caf5d73a85652f91a3",
        "file": [
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "patch": "@@ -91,6 +91,7 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.junit.Assert;\n \n public class TestReplicationScenarios {\n \n@@ -3185,6 +3186,47 @@ public void testLoadCmPathMissing() throws IOException {\n     fs.create(path, false);\n   }\n \n+  @Test\n+  public void testDumpWithTableDirMissing() throws IOException {\n+    String dbName = createDB(testName.getMethodName(), driver);\n+    run(\"CREATE TABLE \" + dbName + \".normal(a int)\", driver);\n+    run(\"INSERT INTO \" + dbName + \".normal values (1)\", driver);\n+\n+    Path path = new Path(System.getProperty(\"test.warehouse.dir\", \"\"));\n+    path = new Path(path, dbName.toLowerCase() + \".db\");\n+    path = new Path(path, \"normal\");\n+    FileSystem fs = path.getFileSystem(hconf);\n+    fs.delete(path);\n+\n+    advanceDumpDir();\n+    CommandProcessorResponse ret = driver.run(\"REPL DUMP \" + dbName);\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    run(\"DROP TABLE \" + dbName + \".normal\", driver);\n+    run(\"drop database \" + dbName, true, driver);\n+  }\n+\n+  @Test\n+  public void testDumpWithPartitionDirMissing() throws IOException {\n+    String dbName = createDB(testName.getMethodName(), driver);\n+    run(\"CREATE TABLE \" + dbName + \".normal(a int) PARTITIONED BY (part int)\", driver);\n+    run(\"INSERT INTO \" + dbName + \".normal partition (part= 124) values (1)\", driver);\n+\n+    Path path = new Path(System.getProperty(\"test.warehouse.dir\",\"\"));\n+    path = new Path(path, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    path = new Path(path, \"part=124\");\n+    FileSystem fs = path.getFileSystem(hconf);\n+    fs.delete(path);\n+\n+    advanceDumpDir();\n+    CommandProcessorResponse ret = driver.run(\"REPL DUMP \" + dbName);\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    run(\"DROP TABLE \" + dbName + \".normal\", driver);\n+    run(\"drop database \" + dbName, true, driver);\n+  }\n+\n   @Test\n   public void testDumpNonReplDatabase() throws IOException {\n     String dbName = createDBNonRepl(testName.getMethodName(), driver);",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "sha": "46c623d34bb8de2f0ded145b77e52dc34f3e7d49",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java",
                "patch": "@@ -32,6 +32,10 @@\n import org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.CallerArguments;\n import org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.BehaviourInjection;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n import org.junit.rules.TestName;\n import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n@@ -63,10 +67,11 @@\n   protected static final Logger LOG = LoggerFactory.getLogger(TestReplicationScenarios.class);\n   private static WarehouseInstance primary, replica, replicaNonAcid;\n   private String primaryDbName, replicatedDbName;\n+  private static HiveConf conf;\n \n   @BeforeClass\n   public static void classLevelSetup() throws Exception {\n-    Configuration conf = new Configuration();\n+    conf = new HiveConf(TestReplicationScenariosAcidTables.class);\n     conf.set(\"dfs.client.use.datanode.hostname\", \"true\");\n     conf.set(\"hadoop.proxyuser.\" + Utils.getUGI().getShortUserName() + \".hosts\", \"*\");\n     MiniDFSCluster miniDFSCluster =\n@@ -432,4 +437,49 @@ public Boolean apply(@Nullable CallerArguments args) {\n             .run(\"select name from t2 order by name\")\n             .verifyResults(Arrays.asList(\"bob\", \"carl\"));\n   }\n+\n+  @Test\n+  public void testDumpAcidTableWithPartitionDirMissing() throws Throwable {\n+    String dbName = testName.getMethodName();\n+    primary.run(\"CREATE DATABASE \" + dbName + \" WITH DBPROPERTIES ( '\" +\n+            SOURCE_OF_REPLICATION + \"' = '1,2,3')\")\n+    .run(\"CREATE TABLE \" + dbName + \".normal (a int) PARTITIONED BY (part int)\" +\n+            \" STORED AS ORC TBLPROPERTIES ('transactional'='true')\")\n+    .run(\"INSERT INTO \" + dbName + \".normal partition (part= 124) values (1)\");\n+\n+    Path path = new Path(primary.warehouseRoot, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    path = new Path(path, \"part=124\");\n+    FileSystem fs = path.getFileSystem(conf);\n+    fs.delete(path);\n+\n+    CommandProcessorResponse ret = primary.runCommand(\"REPL DUMP \" + dbName +\n+            \" with ('hive.repl.dump.include.acid.tables' = 'true')\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    primary.run(\"DROP TABLE \" + dbName + \".normal\");\n+    primary.run(\"drop database \" + dbName);\n+  }\n+\n+  @Test\n+  public void testDumpAcidTableWithTableDirMissing() throws Throwable {\n+    String dbName = testName.getMethodName();\n+    primary.run(\"CREATE DATABASE \" + dbName + \" WITH DBPROPERTIES ( '\" +\n+            SOURCE_OF_REPLICATION + \"' = '1,2,3')\")\n+            .run(\"CREATE TABLE \" + dbName + \".normal (a int) \" +\n+                    \" STORED AS ORC TBLPROPERTIES ('transactional'='true')\")\n+            .run(\"INSERT INTO \" + dbName + \".normal values (1)\");\n+\n+    Path path = new Path(primary.warehouseRoot, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    FileSystem fs = path.getFileSystem(conf);\n+    fs.delete(path);\n+\n+    CommandProcessorResponse ret = primary.runCommand(\"REPL DUMP \" + dbName +\n+            \" with ('hive.repl.dump.include.acid.tables' = 'true')\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    primary.run(\"DROP TABLE \" + dbName + \".normal\");\n+    primary.run(\"drop database \" + dbName);\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java",
                "sha": "86c040532f0c07a6e79dd2418496a11545beb9f9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 36,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "patch": "@@ -68,6 +68,9 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.junit.Assert;\n \n public class TestReplicationScenariosAcrossInstances {\n   @Rule\n@@ -878,41 +881,6 @@ private void verifyIfSrcOfReplPropMissing(Map<String, String> props) {\n     assertFalse(props.containsKey(SOURCE_OF_REPLICATION));\n   }\n \n-  @Test\n-  public void testIfCkptSetForObjectsByBootstrapReplLoad() throws Throwable {\n-    WarehouseInstance.Tuple tuple = primary\n-            .run(\"use \" + primaryDbName)\n-            .run(\"create table t1 (id int)\")\n-            .run(\"insert into table t1 values (10)\")\n-            .run(\"create table t2 (place string) partitioned by (country string)\")\n-            .run(\"insert into table t2 partition(country='india') values ('bangalore')\")\n-            .run(\"insert into table t2 partition(country='uk') values ('london')\")\n-            .run(\"insert into table t2 partition(country='us') values ('sfo')\")\n-            .dump(primaryDbName, null);\n-\n-    replica.load(replicatedDbName, tuple.dumpLocation)\n-            .run(\"use \" + replicatedDbName)\n-            .run(\"repl status \" + replicatedDbName)\n-            .verifyResult(tuple.lastReplicationId)\n-            .run(\"show tables\")\n-            .verifyResults(new String[] { \"t1\", \"t2\" })\n-            .run(\"select country from t2\")\n-            .verifyResults(Arrays.asList(\"india\", \"uk\", \"us\"));\n-\n-    Database db = replica.getDatabase(replicatedDbName);\n-    verifyIfCkptSet(db.getParameters(), tuple.dumpLocation);\n-    Table t1 = replica.getTable(replicatedDbName, \"t1\");\n-    verifyIfCkptSet(t1.getParameters(), tuple.dumpLocation);\n-    Table t2 = replica.getTable(replicatedDbName, \"t2\");\n-    verifyIfCkptSet(t2.getParameters(), tuple.dumpLocation);\n-    Partition india = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"india\"));\n-    verifyIfCkptSet(india.getParameters(), tuple.dumpLocation);\n-    Partition us = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"us\"));\n-    verifyIfCkptSet(us.getParameters(), tuple.dumpLocation);\n-    Partition uk = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"uk\"));\n-    verifyIfCkptSet(uk.getParameters(), tuple.dumpLocation);\n-  }\n-\n   @Test\n   public void testIncrementalDumpMultiIteration() throws Throwable {\n     WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);\n@@ -1182,7 +1150,9 @@ public Boolean apply(@Nullable CallerArguments args) {\n     assertEquals(0, replica.getForeignKeyList(replicatedDbName, \"t2\").size());\n \n     // Retry with different dump should fail.\n-    replica.loadFailure(replicatedDbName, tuple2.dumpLocation);\n+    CommandProcessorResponse ret = replica.runCommand(\"REPL LOAD \" + replicatedDbName +\n+            \" FROM '\" + tuple2.dumpLocation + \"'\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.getErrorCode());\n \n     // Verify if create table is not called on table t1 but called for t2 and t3.\n     // Also, allow constraint creation only on t1 and t3. Foreign key creation on t2 fails.",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "sha": "08f013031fe9c1e34da14649bb92a65fae562ee0",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "patch": "@@ -77,6 +77,7 @@\n   HiveConf hiveConf;\n   MiniDFSCluster miniDFSCluster;\n   private HiveMetaStoreClient client;\n+  public final Path warehouseRoot;\n \n   private static int uniqueIdentifier = 0;\n \n@@ -90,7 +91,7 @@\n     assert miniDFSCluster.isDataNodeUp();\n     DistributedFileSystem fs = miniDFSCluster.getFileSystem();\n \n-    Path warehouseRoot = mkDir(fs, \"/warehouse\" + uniqueIdentifier);\n+    warehouseRoot = mkDir(fs, \"/warehouse\" + uniqueIdentifier);\n     if (StringUtils.isNotEmpty(keyNameForEncryptedZone)) {\n       fs.createEncryptionZone(warehouseRoot, keyNameForEncryptedZone);\n     }\n@@ -199,6 +200,10 @@ public WarehouseInstance run(String command) throws Throwable {\n     return this;\n   }\n \n+  public CommandProcessorResponse runCommand(String command) throws Throwable {\n+    return driver.run(command);\n+  }\n+\n   WarehouseInstance runFailure(String command) throws Throwable {\n     CommandProcessorResponse ret = driver.run(command);\n     if (ret.getException() == null) {",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "sha": "f666df11415dc238e2cd9f7c190fd16ae7132929",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "patch": "@@ -502,7 +502,8 @@\n   //if the error message is changed for REPL_EVENTS_MISSING_IN_METASTORE, then need modification in getNextNotification\n   //method in HiveMetaStoreClient\n   REPL_EVENTS_MISSING_IN_METASTORE(20016, \"Notification events are missing in the meta store.\"),\n-  REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID(20017, \"Target database is bootstrapped from some other path.\"),\n+  REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID(20017, \"Load path {0} not valid as target database is bootstrapped \" +\n+          \"from some other path : {1}.\"),\n   REPL_FILE_MISSING_FROM_SRC_AND_CM_PATH(20018, \"File is missing from both source and cm path.\"),\n   REPL_LOAD_PATH_NOT_FOUND(20019, \"Load path does not exist.\"),\n   REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION(20020,",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "sha": "b2c9daa436039a0bd2ea57a3cb25c7d09b9c52b6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
                "patch": "@@ -121,6 +121,10 @@ protected int execute(DriverContext driverContext) {\n         lastReplId = incrementalDump(dumpRoot, dmd, cmRoot);\n       }\n       prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)), dumpSchema);\n+    } catch (RuntimeException e) {\n+      LOG.error(\"failed\", e);\n+      setException(e);\n+      return ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode();\n     } catch (Exception e) {\n       LOG.error(\"failed\", e);\n       setException(e);",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
                "sha": "e48657c35d87a52f4269eae15f7146c333c0e125",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java",
                "patch": "@@ -79,7 +79,7 @@ public TaskTracker tasks() throws SemanticException {\n       }\n       return tracker;\n     } catch (Exception e) {\n-      throw new SemanticException(e);\n+      throw new SemanticException(e.getMessage(), e);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java",
                "sha": "0fd305a0f9abb5f19a8bf226fdcfcb056b1286d2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.InvalidOperationException;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.repl.ReplStateLogWork;\n@@ -115,9 +116,8 @@ public static boolean replCkptStatus(String dbName, Map<String, String> props, S\n       if (props.get(REPL_CHECKPOINT_KEY).equals(dumpRoot)) {\n         return true;\n       }\n-      throw new InvalidOperationException(\"REPL LOAD with Dump: \" + dumpRoot\n-              + \" is not allowed as the target DB: \" + dbName\n-              + \" is already bootstrap loaded by another Dump \" + props.get(REPL_CHECKPOINT_KEY));\n+      throw new InvalidOperationException(ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.format(dumpRoot,\n+              props.get(REPL_CHECKPOINT_KEY)));\n     }\n     return false;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
                "sha": "b1f731ff96da06e7ed677d7685b40bc3cae9511e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java",
                "patch": "@@ -20,6 +20,7 @@\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n import org.apache.hadoop.hive.ql.metadata.PartitionIterable;\n import org.apache.hadoop.hive.ql.parse.ReplicationSpec;\n@@ -29,13 +30,15 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.LinkedList;\n import java.util.List;\n import java.util.concurrent.ArrayBlockingQueue;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.Future;\n \n import static org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.Paths;\n \n@@ -70,10 +73,11 @@\n     this.callersSession = SessionState.get();\n   }\n \n-  void write(final ReplicationSpec forReplicationSpec) throws InterruptedException {\n+  void write(final ReplicationSpec forReplicationSpec) throws InterruptedException, HiveException {\n+    List<Future<?>> futures = new LinkedList<>();\n     ExecutorService producer = Executors.newFixedThreadPool(1,\n         new ThreadFactoryBuilder().setNameFormat(\"partition-submitter-thread-%d\").build());\n-    producer.submit(() -> {\n+    futures.add(producer.submit(() -> {\n       SessionState.setCurrentSessionState(callersSession);\n       for (Partition partition : partitionIterable) {\n         try {\n@@ -83,7 +87,7 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n               \"Error while queuing up the partitions for export of data files\", e);\n         }\n       }\n-    });\n+    }));\n     producer.shutdown();\n \n     ThreadFactory namingThreadFactory =\n@@ -102,7 +106,7 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n         continue;\n       }\n       LOG.debug(\"scheduling partition dump {}\", partition.getName());\n-      consumer.submit(() -> {\n+      futures.add(consumer.submit(() -> {\n         String partitionName = partition.getName();\n         String threadName = Thread.currentThread().getName();\n         LOG.debug(\"Thread: {}, start partition dump {}\", threadName, partitionName);\n@@ -115,11 +119,19 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n                   .export(forReplicationSpec);\n           LOG.debug(\"Thread: {}, finish partition dump {}\", threadName, partitionName);\n         } catch (Exception e) {\n-          throw new RuntimeException(\"Error while export of data files\", e);\n+          throw new RuntimeException(e.getMessage(), e);\n         }\n-      });\n+      }));\n     }\n     consumer.shutdown();\n+    for (Future<?> future : futures) {\n+      try {\n+        future.get();\n+      } catch (Exception e) {\n+        LOG.error(\"failed\", e.getCause());\n+        throw new HiveException(e.getCause().getMessage(), e.getCause());\n+      }\n+    }\n     // may be drive this via configuration as well.\n     consumer.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);\n   }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java",
                "sha": "9e2479938278a01abe39e44fc5512e6f1e56525b",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.NotificationEvent;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n@@ -37,6 +38,7 @@\n import org.slf4j.LoggerFactory;\n \n import java.io.DataOutputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.Collection;\n import java.util.Collections;\n@@ -204,7 +206,11 @@ public static boolean shouldReplicate(NotificationEvent tableForEvent,\n   static List<Path> getDataPathList(Path fromPath, ReplicationSpec replicationSpec, HiveConf conf)\n           throws IOException {\n     if (replicationSpec.isTransactionalTableDump()) {\n-      return AcidUtils.getValidDataPaths(fromPath, conf, replicationSpec.getValidWriteIdList());\n+      try {\n+        return AcidUtils.getValidDataPaths(fromPath, conf, replicationSpec.getValidWriteIdList());\n+      } catch (FileNotFoundException e) {\n+        throw new IOException(ErrorMsg.FILE_NOT_FOUND.format(e.getMessage()), e);\n+      }\n     } else {\n       return Collections.singletonList(fromPath);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java",
                "sha": "976104c210d0ef6bf633817a8d53d04a173ad001",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hive.ql.parse.repl.dump.io;\n \n import java.io.BufferedWriter;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.OutputStreamWriter;\n import java.util.ArrayList;\n@@ -46,6 +47,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import static org.apache.hadoop.hive.ql.ErrorMsg.FILE_NOT_FOUND;\n+\n //TODO: this object is created once to call one method and then immediately destroyed.\n //So it's basically just a roundabout way to pass arguments to a static method. Simplify?\n public class FileOperations {\n@@ -156,6 +159,10 @@ private void exportFilesAsList() throws SemanticException, IOException, LoginExc\n         }\n         done = true;\n       } catch (IOException e) {\n+        if (e instanceof FileNotFoundException) {\n+          logger.error(\"exporting data files in dir : \" + dataPathList + \" to \" + exportRootDataDir + \" failed\");\n+          throw new FileNotFoundException(FILE_NOT_FOUND.format(e.getMessage()));\n+        }\n         repeat++;\n         logger.info(\"writeFilesList failed\", e);\n         if (repeat >= FileUtils.MAX_IO_ERROR_RETRY) {",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java",
                "sha": "e8eaae6961e7071e18e559291024ca369bfe5eb5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 5,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "patch": "@@ -125,13 +125,10 @@ public void alterTable(RawStore msdb, Warehouse wh, String catName, String dbnam\n \n     Table oldt = null;\n \n-    List<TransactionalMetaStoreEventListener> transactionalListeners = null;\n-    List<MetaStoreEventListener> listeners = null;\n+    List<TransactionalMetaStoreEventListener> transactionalListeners = handler.getTransactionalListeners();\n+    List<MetaStoreEventListener> listeners = handler.getListeners();\n     Map<String, String> txnAlterTableEventResponses = Collections.emptyMap();\n \n-    transactionalListeners = handler.getTransactionalListeners();\n-    listeners = handler.getListeners();\n-\n     try {\n       boolean rename = false;\n       List<Partition> parts;",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "sha": "93ac74c68b5e14687852a76fdb298557bf79a08a",
                "status": "modified"
            }
        ],
        "message": "HIVE-19970: Replication dump has a NPE when table is empty (Mahesh Kumar Behera, reviewed by Peter Vary, Sankar Hariappan)",
        "parent": "https://github.com/apache/hive/commit/80c3bb58e24f13f82ca698486645c6c72364d75d",
        "repo": "hive",
        "unit_tests": [
            "TestHiveAlterHandler.java"
        ]
    },
    "hive_1d02ab5": {
        "bug_id": "hive_1d02ab5",
        "commit": "https://github.com/apache/hive/commit/1d02ab578dbd47103a70710abd4d949ea8cea9d2",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/1d02ab578dbd47103a70710abd4d949ea8cea9d2/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=1d02ab578dbd47103a70710abd4d949ea8cea9d2",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "patch": "@@ -778,19 +778,18 @@ private DiskRangeList preReadUncompressedStream(long baseOffset,\n     }\n     // Account for maximum cache buffer size.\n     long streamLen = streamEnd - streamOffset;\n-    int partSize = determineUncompressedPartSize(), //\n+    int partSize = determineUncompressedPartSize(),\n         partCount = (int)(streamLen / partSize) + (((streamLen % partSize) != 0) ? 1 : 0);\n \n     CacheChunk lastUncompressed = null;\n     MemoryBuffer[] singleAlloc = new MemoryBuffer[1];\n-    /*\n-Starting pre-read for [12187411,17107411) at start: 12187411 end: 12449555 cache buffer: 0x5f64a8f6(2)\n-Processing uncompressed file data at [12187411, 12449555)\n-  */\n     for (int i = 0; i < partCount; ++i) {\n       long partOffset = streamOffset + (i * partSize),\n            partEnd = Math.min(partOffset + partSize, streamEnd);\n       long hasEntirePartTo = partOffset; // We have 0 bytes of data for this part, for now.\n+      if (current == null) {\n+        break; // We have no data from this point on (could be unneeded), skip.\n+      }\n       assert partOffset <= current.getOffset();\n       if (partOffset == current.getOffset() && current instanceof CacheChunk) {\n         // We assume cache chunks would always match the way we read, so check and skip it.",
                "raw_url": "https://github.com/apache/hive/raw/1d02ab578dbd47103a70710abd4d949ea8cea9d2/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "sha": "a8b51b92e0cab002b83a7538d4304d8ccfa65f12",
                "status": "modified"
            }
        ],
        "message": "HIVE-12532 : LLAP Cache: Uncompressed data cache has NPE (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/a603ed8d73e7f2bf4588a67f1b40709cc54fcbfe",
        "repo": "hive",
        "unit_tests": [
            "TestEncodedReaderImpl.java"
        ]
    },
    "hive_1d61b28": {
        "bug_id": "hive_1d61b28",
        "commit": "https://github.com/apache/hive/commit/1d61b282c01e0927fe43039e7cba4efbba640bd5",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/1d61b282c01e0927fe43039e7cba4efbba640bd5/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java?ref=1d61b282c01e0927fe43039e7cba4efbba640bd5",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "patch": "@@ -65,13 +65,15 @@ static public JSONObject getJSONPlan(PrintStream out, ExplainWork work)\n     }\n \n     // Print out the parse AST\n-    String jsonAST = outputAST(work.getAstStringTree(), out, jsonOutput, 0);\n-    if (out != null) {\n-      out.println();\n-    }\n+    if (work.getAstStringTree() != null) {\n+      String jsonAST = outputAST(work.getAstStringTree(), out, jsonOutput, 0);\n+      if (out != null) {\n+        out.println();\n+      }\n \n-    if (jsonOutput) {\n-      outJSONObject.put(\"ABSTRACT SYNTAX TREE\", jsonAST);\n+      if (jsonOutput) {\n+        outJSONObject.put(\"ABSTRACT SYNTAX TREE\", jsonAST);\n+      }\n     }\n \n     JSONObject jsonDependencies = outputDependencies(out, jsonOutput,",
                "raw_url": "https://github.com/apache/hive/raw/1d61b282c01e0927fe43039e7cba4efbba640bd5/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "sha": "a29efed42ace2816d2747bedb15b0589ea0a5bdb",
                "status": "modified"
            }
        ],
        "message": "HIVE-2581: explain task: getJSONPlan throws a NPE if the ast is null (namit via He Yongqiang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1202820 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/380ac4f45f61105dccbfe6356e560b8d818293cb",
        "repo": "hive",
        "unit_tests": [
            "TestExplainTask.java"
        ]
    },
    "hive_22371f5": {
        "bug_id": "hive_22371f5",
        "commit": "https://github.com/apache/hive/commit/22371f51f365ab609862dc493a86aed17212dac5",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/22371f51f365ab609862dc493a86aed17212dac5/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java?ref=22371f51f365ab609862dc493a86aed17212dac5",
                "deletions": 5,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "patch": "@@ -176,6 +176,8 @@ static void internalBeforeClassSetup(Map<String, String> additionalProperties, b\n     hconf.set(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL.varname,\n         \"org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore\");\n     hconf.setBoolVar(HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES, true);\n+    hconf.setBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER, true);\n+    hconf.setBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE, true);\n     System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, \" \");\n     System.setProperty(HiveConf.ConfVars.POSTEXECHOOKS.varname, \" \");\n \n@@ -850,20 +852,20 @@ public void testIncrementalAdds() throws IOException {\n     // Now, we load data into the tables, and see if an incremental\n     // repl drop/load can duplicate it.\n \n-    run(\"LOAD DATA LOCAL INPATH '\" + unptn_locn + \"' OVERWRITE INTO TABLE \" + dbName + \".unptned\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + unptn_locn + \"' OVERWRITE INTO TABLE \" + dbName + \".unptned\", true, driver);\n     verifySetup(\"SELECT * from \" + dbName + \".unptned\", unptn_data, driver);\n     run(\"CREATE TABLE \" + dbName + \".unptned_late AS SELECT * from \" + dbName + \".unptned\", driver);\n     verifySetup(\"SELECT * from \" + dbName + \".unptned_late\", unptn_data, driver);\n \n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=1)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=1)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned WHERE b=1\", ptn_data_1, driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=2)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=2)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned WHERE b=2\", ptn_data_2, driver);\n \n     run(\"CREATE TABLE \" + dbName + \".ptned_late(a string) PARTITIONED BY (b int) STORED AS TEXTFILE\", driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=1)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=1)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned_late WHERE b=1\",ptn_data_1, driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=2)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=2)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned_late WHERE b=2\", ptn_data_2, driver);\n \n     // Perform REPL-DUMP/LOAD",
                "raw_url": "https://github.com/apache/hive/raw/22371f51f365ab609862dc493a86aed17212dac5/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "sha": "6f47056ca72bb532730a25381a1973be81361ff1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/22371f51f365ab609862dc493a86aed17212dac5/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java?ref=22371f51f365ab609862dc493a86aed17212dac5",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "patch": "@@ -823,7 +823,7 @@ private void blockPartitionLocationChangesOnReplSource(Database db, Table tbl,\n \n     // Do not allow changing location of a managed table as alter event doesn't capture the\n     // new files list. So, it may cause data inconsistency.\n-    if (ec.isSetProperties()) {\n+    if ((ec != null) && ec.isSetProperties()) {\n       String alterType = ec.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n       if (alterType != null && alterType.equalsIgnoreCase(ALTERLOCATION) &&\n               tbl.getTableType().equalsIgnoreCase(TableType.MANAGED_TABLE.name())) {\n@@ -846,7 +846,7 @@ private void validateTableChangesOnReplSource(Database db, Table oldTbl, Table n\n     // Do not allow changing location of a managed table as alter event doesn't capture the\n     // new files list. So, it may cause data inconsistency. We do this whether or not strict\n     // managed is true on the source cluster.\n-    if (ec.isSetProperties()) {\n+    if ((ec != null) && ec.isSetProperties()) {\n         String alterType = ec.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n         if (alterType != null && alterType.equalsIgnoreCase(ALTERLOCATION) &&\n             oldTbl.getTableType().equalsIgnoreCase(TableType.MANAGED_TABLE.name())) {",
                "raw_url": "https://github.com/apache/hive/raw/22371f51f365ab609862dc493a86aed17212dac5/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "sha": "ee0e52879ae4129257e81de4f14cebf3640bb781",
                "status": "modified"
            }
        ],
        "message": "HIVE-21811: Load data into partitioned table throws NPE if DB is enabled for replication (Sankar Hariappan, reviewed by Thejas M Nair)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/81117db0e59aa673adc454d0da40d0a146555ae2",
        "repo": "hive",
        "unit_tests": [
            "TestHiveAlterHandler.java"
        ]
    },
    "hive_27f77f0": {
        "bug_id": "hive_27f77f0",
        "commit": "https://github.com/apache/hive/commit/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hive/blob/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=27f77f054baf2e7cd1ea55282fbbb80c055f5aa9",
                "deletions": 11,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -11831,6 +11831,7 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n       optiqPreCboPlan = applyPreCBOTransforms(optiqGenPlan, HiveDefaultRelMetadataProvider.INSTANCE);\n       List<RelMetadataProvider> list = Lists.newArrayList();\n       list.add(HiveDefaultRelMetadataProvider.INSTANCE);\n+      RelTraitSet desiredTraits = cluster.traitSetOf(HiveRel.CONVENTION, RelCollationImpl.EMPTY);\n \n       if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_GREEDY_JOIN_ORDER)) {\n         planner.registerMetadataProviders(list);\n@@ -11846,18 +11847,15 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n           planner.addRule(HivePullUpProjectsAboveJoinRule.LEFT_PROJECT);\n           planner.addRule(HivePullUpProjectsAboveJoinRule.RIGHT_PROJECT);\n           planner.addRule(HiveMergeProjectRule.INSTANCE);\n+        }\n \n-          RelTraitSet desiredTraits = cluster\n-              .traitSetOf(HiveRel.CONVENTION, RelCollationImpl.EMPTY);\n-\n-          RelNode rootRel = optiqPreCboPlan;\n-          if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n-            rootRel = planner.changeTraits(optiqPreCboPlan, desiredTraits);\n-          }\n-          planner.setRoot(rootRel);\n-\n-          optiqOptimizedPlan = planner.findBestExp();\n+        RelNode rootRel = optiqPreCboPlan;\n+        if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n+          rootRel = planner.changeTraits(optiqPreCboPlan, desiredTraits);\n         }\n+        planner.setRoot(rootRel);\n+\n+        optiqOptimizedPlan = planner.findBestExp();\n       } else {\n         final HepProgram hepPgm = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP)\n             .addRuleInstance(new ConvertMultiJoinRule(HiveJoinRel.class))\n@@ -11869,7 +11867,12 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n         RelMetadataProvider chainedProvider = ChainedRelMetadataProvider.of(list);\n         cluster.setMetadataProvider(new CachingRelMetadataProvider(chainedProvider, hepPlanner));\n \n-        hepPlanner.setRoot(optiqPreCboPlan);\n+        RelNode rootRel = optiqPreCboPlan;\n+        if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n+          rootRel = hepPlanner.changeTraits(optiqPreCboPlan, desiredTraits);\n+        }\n+        hepPlanner.setRoot(rootRel);\n+\n         optiqOptimizedPlan = hepPlanner.findBestExp();\n       }\n ",
                "raw_url": "https://github.com/apache/hive/raw/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "3da2491f5198f3eed9d119545fec81dc72c7f7dc",
                "status": "modified"
            }
        ],
        "message": "HIVE-7515 Fix NPE in CBO (Laljo John Pullokkaran via Harish Butani)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1613482 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/60be7236a80f57cc23a1a9d0dc8b00062ca5397e",
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_2d87c12": {
        "bug_id": "hive_2d87c12",
        "commit": "https://github.com/apache/hive/commit/2d87c12d914044540a4f5ed7fe500e3c245fbead",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/2d87c12d914044540a4f5ed7fe500e3c245fbead/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=2d87c12d914044540a4f5ed7fe500e3c245fbead",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -2024,8 +2024,14 @@ public boolean equals(Object obj) {\n         if (!p1.isSetValues() || !p2.isSetValues()) return p1.isSetValues() == p2.isSetValues();\n         if (p1.getValues().size() != p2.getValues().size()) return false;\n         for (int i = 0; i < p1.getValues().size(); ++i) {\n-          String v1 = p1.getValues().get(i), v2 = p2.getValues().get(i);\n-          if ((v1 == null && v2 != null) || !v1.equals(v2)) return false;\n+          String v1 = p1.getValues().get(i);\n+          String v2 = p2.getValues().get(i);\n+          if (v1 == null && v2 == null) {\n+            continue;\n+          }\n+          if (v1 == null || !v1.equals(v2)) {\n+            return false;\n+          }\n         }\n         return true;\n       }",
                "raw_url": "https://github.com/apache/hive/raw/2d87c12d914044540a4f5ed7fe500e3c245fbead/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "e7960487680dd26059ee512dd28b66c90848aa57",
                "status": "modified"
            }
        ],
        "message": "HIVE-10590 fix potential NPE in HiveMetaStore.equals (Alexander Pivovarov, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/bc0138c436add2335d2045b6c7bf86bc6a15cc27",
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    },
    "hive_36af62a": {
        "bug_id": "hive_36af62a",
        "commit": "https://github.com/apache/hive/commit/36af62a7210b82196133ca7924cf7ba04a249e9d",
        "file": [
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d",
                "deletions": 23,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -82,10 +82,12 @@\n  */\n public class DagUtils {\n \n+  private static final String TEZ_DIR = \"_tez_scratch_dir\";\n+\n   /*\n    * Creates the configuration object necessary to run a specific vertex from\n    * map work. This includes input formats, input processor, etc.\n-=  */\n+   */\n   private static JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {\n     JobConf conf = new JobConf(baseConf);\n \n@@ -124,8 +126,8 @@ private static JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {\n       inpFormat = BucketizedHiveInputFormat.class.getName();\n     }\n \n-    conf.set(MRJobConfig.MAP_CLASS_ATTR, ExecMapper.class.getName());\n-    conf.set(MRJobConfig.INPUT_FORMAT_CLASS_ATTR, inpFormat);\n+    conf.set(\"mapred.mapper.class\", ExecMapper.class.getName());\n+    conf.set(\"mapred.input.format.class\", inpFormat);\n \n     return conf;\n   }\n@@ -141,11 +143,16 @@ private static JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {\n    * @param w The second vertex (sink)\n    * @return\n    */\n-  public static Edge createEdge(JobConf vConf, Vertex v, JobConf wConf, Vertex w) {\n+  public static Edge createEdge(JobConf vConf, Vertex v, JobConf wConf, Vertex w) \n+      throws IOException {\n \n     // Tez needs to setup output subsequent input pairs correctly\n     MultiStageMRConfToTezTranslator.translateVertexConfToTez(wConf, vConf);\n \n+    // update payloads (configuration for the vertices might have changed)\n+    v.getProcessorDescriptor().setUserPayload(MRHelpers.createUserPayloadFromConf(vConf));\n+    w.getProcessorDescriptor().setUserPayload(MRHelpers.createUserPayloadFromConf(wConf));\n+\n     // all edges are of the same type right now\n     EdgeProperty edgeProperty =\n         new EdgeProperty(ConnectionPattern.BIPARTITE, SourceType.STABLE,\n@@ -161,6 +168,8 @@ private static Vertex createVertex(JobConf conf, MapWork mapWork, int seqNo,\n       LocalResource appJarLr, List<LocalResource> additionalLr, FileSystem fs,\n       Path mrScratchDir, Context ctx) throws Exception {\n \n+    Path tezDir = getTezDir(mrScratchDir);\n+\n     // map work can contain localwork, i.e: hashtables for map-side joins\n     Path hashTableArchive = createHashTables(mapWork, conf);\n     LocalResource localWorkLr = null;\n@@ -171,17 +180,24 @@ private static Vertex createVertex(JobConf conf, MapWork mapWork, int seqNo,\n     }\n \n     // write out the operator plan\n-    Path planPath = Utilities.setMapWork(conf, mapWork, mrScratchDir.toUri().toString(), false);\n+    Path planPath = Utilities.setMapWork(conf, mapWork, \n+        mrScratchDir.toUri().toString(), false);\n     LocalResource planLr = createLocalResource(fs,\n         planPath, LocalResourceType.FILE,\n         LocalResourceVisibility.APPLICATION);\n \n     // setup input paths and split info\n-    List<Path> inputPaths = Utilities.getInputPaths(conf, mapWork, mrScratchDir.toUri().toString(), ctx);\n+    List<Path> inputPaths = Utilities.getInputPaths(conf, mapWork, \n+        mrScratchDir.toUri().toString(), ctx);\n     Utilities.setInputPaths(conf, inputPaths);\n \n-    InputSplitInfo inputSplitInfo = MRHelpers.generateInputSplits(conf, mrScratchDir);\n-    MultiStageMRConfToTezTranslator.translateVertexConfToTez(conf, conf);\n+    InputSplitInfo inputSplitInfo = MRHelpers.generateInputSplits(conf, tezDir);\n+\n+    // create the directories FileSinkOperators need\n+    Utilities.createTmpDirs(conf, mapWork);\n+\n+    // Tez ask us to call this even if there's no preceding vertex\n+    MultiStageMRConfToTezTranslator.translateVertexConfToTez(conf, null);\n \n     // finally create the vertex\n     Vertex map = null;\n@@ -229,17 +245,13 @@ private static Path createHashTables(MapWork mapWork, Configuration conf) {\n   private static JobConf initializeVertexConf(JobConf baseConf, ReduceWork reduceWork) {\n     JobConf conf = new JobConf(baseConf);\n \n-    conf.set(MRJobConfig.REDUCE_CLASS_ATTR, ExecReducer.class.getName());\n+    conf.set(\"mapred.reducer.class\", ExecReducer.class.getName());\n \n     boolean useSpeculativeExecReducers = HiveConf.getBoolVar(conf,\n         HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,\n         useSpeculativeExecReducers);\n \n-    // reducers should have been set at planning stage\n-    // job.setNumberOfReducers(rWork.getNumberOfReducers())\n-    conf.set(MRJobConfig.NUM_REDUCES, reduceWork.getNumReduceTasks().toString());\n-\n     return conf;\n   }\n \n@@ -252,10 +264,13 @@ private static Vertex createVertex(JobConf conf, ReduceWork reduceWork, int seqN\n \n     // write out the operator plan\n     Path planPath = Utilities.setReduceWork(conf, reduceWork,\n-        mrScratchDir.getName(), false);\n+        mrScratchDir.toUri().toString(), false);\n     LocalResource planLr = createLocalResource(fs, planPath,\n         LocalResourceType.FILE, LocalResourceVisibility.APPLICATION);\n \n+    // create the directories FileSinkOperators need\n+    Utilities.createTmpDirs(conf, reduceWork);\n+\n     // create the vertex\n     Vertex reducer = new Vertex(\"Reducer \"+seqNo,\n         new ProcessorDescriptor(ReduceProcessor.class.getName(),\n@@ -476,7 +491,7 @@ private static LocalResource localizeResource(Path src, Path dest, Configuration\n    * @throws URISyntaxException when current jar location cannot be determined.\n    */\n   public static LocalResource createHiveExecLocalResource(HiveConf conf)\n-  throws IOException, LoginException, URISyntaxException {\n+      throws IOException, LoginException, URISyntaxException {\n     String hiveJarDir = conf.getVar(HiveConf.ConfVars.HIVE_JAR_DIRECTORY);\n     String currentVersionPathStr = getExecJarPathLocal();\n     String currentJarName = getResourceBaseName(currentVersionPathStr);\n@@ -560,20 +575,17 @@ public static JobConf createConfiguration(HiveConf hiveConf) throws IOException\n       }\n     }\n \n-    conf.set(\"mapreduce.framework.name\",\"yarn-tez\");\n-    conf.set(\"mapreduce.job.output.committer.class\", NullOutputCommitter.class.getName());\n-\n-    conf.setBoolean(MRJobConfig.SETUP_CLEANUP_NEEDED, false);\n-    conf.setBoolean(MRJobConfig.TASK_CLEANUP_NEEDED, false);\n+    conf.set(\"mapred.output.committer.class\", NullOutputCommitter.class.getName());\n \n-    conf.setClass(MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR, HiveOutputFormatImpl.class, OutputFormat.class);\n+    conf.setBoolean(\"mapred.committer.job.setup.cleanup.needed\", false);\n+    conf.setBoolean(\"mapred.committer.job.task.cleanup.needed\", false);\n \n-    conf.set(MRJobConfig.MAP_CLASS_ATTR, ExecMapper.class.getName());\n+    conf.setClass(\"mapred.output.format.class\", HiveOutputFormatImpl.class, OutputFormat.class);\n \n     conf.set(MRJobConfig.OUTPUT_KEY_CLASS, HiveKey.class.getName());\n     conf.set(MRJobConfig.OUTPUT_VALUE_CLASS, BytesWritable.class.getName());\n \n-    conf.set(MRJobConfig.PARTITIONER_CLASS_ATTR, HiveConf.getVar(conf, HiveConf.ConfVars.HIVEPARTITIONER));\n+    conf.set(\"mapred.partitioner.class\", HiveConf.getVar(conf, HiveConf.ConfVars.HIVEPARTITIONER));\n \n     return conf;\n   }\n@@ -631,6 +643,25 @@ public static Vertex createVertex(JobConf conf, BaseWork work,\n     }\n   }\n \n+  /**\n+   * createTezDir creates a temporary directory in the scratchDir folder to\n+   * be used with Tez. Assumes scratchDir exists.\n+   */\n+  public static Path createTezDir(Path scratchDir, Configuration conf) \n+      throws IOException {\n+    Path tezDir = getTezDir(scratchDir);\n+    FileSystem fs = tezDir.getFileSystem(conf);\n+    fs.mkdirs(tezDir);\n+    return tezDir;\n+  }\n+\n+  /**\n+   * Gets the tez dir that belongs to the hive scratch dir\n+   */\n+  public static Path getTezDir(Path scratchDir) {\n+    return new Path(scratchDir, TEZ_DIR);\n+  }\n+\n   private DagUtils() {\n     // don't instantiate\n   }",
                "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "42f63bb53158d13da3baf6a70367823a2058a82b",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d",
                "deletions": 11,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -24,16 +24,20 @@\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.Context;\n import org.apache.hadoop.hive.ql.DriverContext;\n+import org.apache.hadoop.hive.ql.exec.JobCloseFeedBack;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.plan.TezWork;\n import org.apache.hadoop.hive.ql.plan.api.StageType;\n import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.yarn.api.records.LocalResource;\n import org.apache.tez.client.TezClient;\n import org.apache.tez.dag.api.DAG;\n@@ -60,15 +64,26 @@ public TezTask() {\n   @Override\n   public int execute(DriverContext driverContext) {\n     int rc = 1;\n-\n-    Context ctx = driverContext.getCtx();\n+    boolean cleanContext = false;\n+    Context ctx = null;\n+    DAGClient client = null;\n \n     try {\n+      // Get or create Context object. If we create it we have to clean\n+      // it later as well.\n+      ctx = driverContext.getCtx();\n+      if (ctx == null) {\n+        ctx = new Context(conf);\n+        cleanContext = true;\n+      }\n \n       // we will localize all the files (jars, plans, hashtables) to the\n       // scratch dir. let's create this first.\n       Path scratchDir = new Path(ctx.getMRScratchDir());\n \n+      // create the tez tmp dir\n+      DagUtils.createTezDir(scratchDir, conf);\n+\n       // jobConf will hold all the configuration for hadoop, tez, and hive\n       JobConf jobConf = DagUtils.createConfiguration(conf);\n \n@@ -80,21 +95,29 @@ public int execute(DriverContext driverContext) {\n       DAG dag = build(jobConf, work, scratchDir, appJarLr, ctx);\n \n       // submit will send the job to the cluster and start executing\n-      DAGClient client = submit(jobConf, dag, scratchDir, appJarLr);\n+      client = submit(jobConf, dag, scratchDir, appJarLr);\n \n       // finally monitor will print progress until the job is done\n       TezJobMonitor monitor = new TezJobMonitor();\n       rc = monitor.monitorExecution(client);\n \n     } catch (Exception e) {\n       LOG.error(\"Failed to execute tez graph.\", e);\n+      // rc will be 1 at this point indicating failure.\n     } finally {\n       Utilities.clearWork(conf);\n-      try {\n-        ctx.clear();\n-      } catch (Exception e) {\n-        /*best effort*/\n-        LOG.warn(\"Failed to clean up after tez job\");\n+      if (cleanContext) {\n+        try {\n+          ctx.clear();\n+        } catch (Exception e) {\n+          /*best effort*/\n+          LOG.warn(\"Failed to clean up after tez job\");\n+        }\n+      }\n+      // need to either move tmp files or remove them\n+      if (client != null) {\n+        // rc will only be overwritten if close errors out\n+        rc = close(work, rc);\n       }\n     }\n     return rc;\n@@ -115,6 +138,9 @@ private DAG build(JobConf conf, TezWork work, Path scratchDir,\n     List<BaseWork> ws = work.getAllWork();\n     Collections.reverse(ws);\n \n+    Path tezDir = DagUtils.getTezDir(scratchDir);\n+    FileSystem fs = tezDir.getFileSystem(conf);\n+\n     // the name of the dag is what is displayed in the AM/Job UI\n     DAG dag = new DAG(\n         Utilities.abbreviate(HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYSTRING),\n@@ -125,8 +151,8 @@ private DAG build(JobConf conf, TezWork work, Path scratchDir,\n \n       // translate work to vertex\n       JobConf wxConf = DagUtils.initializeVertexConf(conf, w);\n-      Vertex wx = DagUtils.createVertex(wxConf, w, scratchDir, i--,\n-          appJarLr, additionalLr, scratchDir.getFileSystem(conf), ctx);\n+      Vertex wx = DagUtils.createVertex(wxConf, w, tezDir, \n+          i--, appJarLr, additionalLr, fs, ctx);\n       dag.addVertex(wx);\n       workToVertex.put(w, wx);\n       workToConf.put(w, wxConf);\n@@ -155,14 +181,42 @@ private DAGClient submit(JobConf conf, DAG dag, Path scratchDir, LocalResource a\n     Map<String, LocalResource> amLrs = new HashMap<String, LocalResource>();\n     amLrs.put(DagUtils.getBaseName(appJarLr), appJarLr);\n \n+    Path tezDir = DagUtils.getTezDir(scratchDir);\n+\n     // ready to start execution on the cluster\n-    DAGClient dagClient = tezClient.submitDAGApplication(dag, scratchDir,\n+    DAGClient dagClient = tezClient.submitDAGApplication(dag, tezDir,\n         null, \"default\", Collections.singletonList(\"\"), amEnv, amLrs,\n         new TezConfiguration(conf));\n \n     return dagClient;\n   }\n \n+  /*\n+   * close will move the temp files into the right place for the fetch\n+   * task. If the job has failed it will clean up the files.\n+   */\n+  private int close(TezWork work, int rc) {\n+    try {\n+      JobCloseFeedBack feedBack = new JobCloseFeedBack();\n+      List<BaseWork> ws = work.getAllWork();\n+      for (BaseWork w: ws) {\n+        List<Operator<?>> ops = w.getAllOperators();\n+        for (Operator<?> op: ops) {\n+          op.jobClose(conf, rc == 0, feedBack);\n+        }\n+      }\n+    } catch (Exception e) {\n+      // jobClose needs to execute successfully otherwise fail task\n+      if (rc == 0) {\n+        rc = 3;\n+        String mesg = \"Job Commit failed with exception '\" \n+          + Utilities.getNameMessage(e) + \"'\";\n+        console.printError(mesg, \"\\n\" + StringUtils.stringifyException(e));\n+      }\n+    }\n+    return rc;\n+  }\n+\n   @Override\n   public boolean isMapRedTask() {\n     return true;",
                "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "f5d136fd47c5b4c67cde01fbb1baf9b3d0d1e410",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java",
                "patch": "@@ -772,6 +772,21 @@ public static void setTaskPlan(String path, String alias,\n     }\n   }\n \n+  /**\n+   * Set key and value descriptor\n+   * @param work RedueWork\n+   * @param rs ReduceSinkOperator\n+   */\n+  public static void setKeyAndValueDesc(ReduceWork work, ReduceSinkOperator rs) {\n+    work.setKeyDesc(rs.getConf().getKeySerializeInfo());\n+    int tag = Math.max(0, rs.getConf().getTag());\n+    List<TableDesc> tagToSchema = work.getTagToValueDesc();\n+    while (tag + 1 > tagToSchema.size()) {\n+      tagToSchema.add(null);\n+    }\n+    tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());\n+  }\n+\n   /**\n    * set key and value descriptor.\n    *\n@@ -788,13 +803,7 @@ public static void setKeyAndValueDesc(ReduceWork plan,\n \n     if (topOp instanceof ReduceSinkOperator) {\n       ReduceSinkOperator rs = (ReduceSinkOperator) topOp;\n-      plan.setKeyDesc(rs.getConf().getKeySerializeInfo());\n-      int tag = Math.max(0, rs.getConf().getTag());\n-      List<TableDesc> tagToSchema = plan.getTagToValueDesc();\n-      while (tag + 1 > tagToSchema.size()) {\n-        tagToSchema.add(null);\n-      }\n-      tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());\n+      setKeyAndValueDesc(plan, rs);\n     } else {\n       List<Operator<? extends OperatorDesc>> children = topOp.getChildOperators();\n       if (children != null) {",
                "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java",
                "sha": "7433ddcc014742af3fb09942958a18fce7f718f3",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "patch": "@@ -109,6 +109,14 @@ public Object process(Node nd, Stack<Node> stack,\n         = (ReduceSinkOperator)root.getParentOperators().get(0);\n       reduceWork.setNumReduceTasks(reduceSink.getConf().getNumReducers());\n \n+      // need to fill in information about the key and value in the reducer\n+      GenMapRedUtils.setKeyAndValueDesc(reduceWork, reduceSink);\n+\n+      // needs to be fixed in HIVE-5052. This should be driven off of stats\n+      if (reduceWork.getNumReduceTasks() <= 0) {\n+        reduceWork.setNumReduceTasks(1);\n+      }\n+\n       tezWork.add(reduceWork);\n       tezWork.connect(\n           context.preceedingWork,\n@@ -129,13 +137,25 @@ public Object process(Node nd, Stack<Node> stack,\n     // Also note: the concept of leaf and root is reversed in hive for historical\n     // reasons. Roots are data sources, leaves are data sinks. I know.\n     if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n+\n+      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n+\n+      // need to add this branch to the key + value info\n+      assert operator instanceof ReduceSinkOperator \n+        && followingWork instanceof ReduceWork;\n+      ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n+      ReduceWork rWork = (ReduceWork) followingWork;\n+      GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n+\n+      // add dependency between the two work items\n       tezWork.connect(work, context.leafOperatorToFollowingWork.get(operator));\n     }\n \n     // This is where we cut the tree as described above. We also remember that\n     // we might have to connect parent work with this work later.\n     for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n       assert !context.leafOperatorToFollowingWork.containsKey(parent);\n+      assert !(work instanceof MapWork);\n       context.leafOperatorToFollowingWork.put(parent, work);\n       LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n       root.removeParent(parent);",
                "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "sha": "48145ad00c66eeddcedcf22b363ee5125c1d97ab",
                "status": "modified"
            }
        ],
        "message": "HIVE-5058: Fix NPE issue with DAG submission in TEZ (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1513299 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/6b8ea834ed39d7d3e7be3ed390bbcaaa634d8c72",
        "repo": "hive",
        "unit_tests": [
            "TestGenTezWork.java"
        ]
    },
    "hive_48e4e04": {
        "bug_id": "hive_48e4e04",
        "commit": "https://github.com/apache/hive/commit/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java?ref=48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "patch": "@@ -797,10 +797,11 @@ private JSONObject outputPlan(Object work, PrintStream out,\n                 operator.getOperatorId());\n             if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                 && operator instanceof ReduceSinkOperator) {\n-              ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\n-                  OUTPUT_OPERATORS,\n-                  Arrays.toString(((ReduceSinkOperator) operator).getConf().getOutputOperators()\n-                      .toArray()));\n+              List<String> outputOperators = ((ReduceSinkOperator) operator).getConf().getOutputOperators();\n+              if (outputOperators != null) {\n+                ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(OUTPUT_OPERATORS,\n+                    Arrays.toString(outputOperators.toArray()));\n+              }\n             }\n           }\n         }",
                "raw_url": "https://github.com/apache/hive/raw/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "sha": "d35e3ba56a2395ad8665e8df7c29a0ada830a657",
                "status": "modified"
            }
        ],
        "message": "HIVE-16142: ATSHook NPE via LLAP (Pengcheng Xiong, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/35d707950ddd210c37533be3da51cea730bac881",
        "repo": "hive",
        "unit_tests": [
            "TestExplainTask.java"
        ]
    },
    "hive_49d31f8": {
        "bug_id": "hive_49d31f8",
        "commit": "https://github.com/apache/hive/commit/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54",
                "deletions": 7,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "patch": "@@ -394,18 +394,16 @@ protected Void performDataRead() throws IOException {\n             counters.incrCounter(Counter.METADATA_CACHE_MISS);\n             ensureMetadataReader();\n             long startTimeHdfs = counters.startTimeCounter();\n-            stripeMetadata = new OrcStripeMetadata(\n-                stripeKey, metadataReader, stripe, stripeIncludes, sargColumns);\n+            stripeMetadata = new OrcStripeMetadata(new OrcBatchKey(fileId, stripeIx, 0),\n+                metadataReader, stripe, stripeIncludes, sargColumns);\n             counters.incrTimeCounter(Counter.HDFS_TIME_US, startTimeHdfs);\n             if (hasFileId && metadataCache != null) {\n               stripeMetadata = metadataCache.putStripeMetadata(stripeMetadata);\n               if (DebugUtils.isTraceOrcEnabled()) {\n                 LlapIoImpl.LOG.info(\"Caching stripe \" + stripeKey.stripeIx\n                     + \" metadata with includes: \" + DebugUtils.toString(stripeIncludes));\n               }\n-              stripeKey = new OrcBatchKey(fileId, -1, 0);\n             }\n-\n           }\n           consumer.setStripeMetadata(stripeMetadata);\n         }\n@@ -658,16 +656,15 @@ private OrcFileMetadata getOrReadFileMetadata() throws IOException {\n         StripeInformation si = fileMetadata.getStripes().get(stripeIx);\n         if (value == null) {\n           long startTime = counters.startTimeCounter();\n-          value = new OrcStripeMetadata(stripeKey, metadataReader, si, globalInc, sargColumns);\n+          value = new OrcStripeMetadata(new OrcBatchKey(fileId, stripeIx, 0),\n+              metadataReader, si, globalInc, sargColumns);\n           counters.incrTimeCounter(Counter.HDFS_TIME_US, startTime);\n           if (hasFileId && metadataCache != null) {\n             value = metadataCache.putStripeMetadata(value);\n             if (DebugUtils.isTraceOrcEnabled()) {\n               LlapIoImpl.LOG.info(\"Caching stripe \" + stripeKey.stripeIx\n                   + \" metadata with includes: \" + DebugUtils.toString(globalInc));\n             }\n-            // Create new key object to reuse for gets; we've used the old one to put in cache.\n-            stripeKey = new OrcBatchKey(fileId, 0, 0);\n           }\n         }\n         // We might have got an old value from cache; recheck it has indexes.",
                "raw_url": "https://github.com/apache/hive/raw/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "sha": "b36cf6499bcf443bb168c839e486822e05fe14e8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "patch": "@@ -106,7 +106,7 @@ public DiskRangeList createCacheChunk(MemoryBuffer buffer, long offset, long end\n   private ByteBufferAllocatorPool pool;\n   private boolean isDebugTracingEnabled;\n \n-  public EncodedReaderImpl(long fileId, List<OrcProto.Type> types, CompressionCodec codec,\n+  public EncodedReaderImpl(Long fileId, List<OrcProto.Type> types, CompressionCodec codec,\n       int bufferSize, long strideRate, DataCache cache, DataReader dataReader, PoolFactory pf)\n           throws IOException {\n     this.fileId = fileId;",
                "raw_url": "https://github.com/apache/hive/raw/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "sha": "6cec80ee968a503c48cdd487fe11ab31cc2aa592",
                "status": "modified"
            }
        ],
        "message": "HIVE-12990 : LLAP: ORC cache NPE without FileID support (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/38b172cdce32fa0b9014f16b44e8dc96bdc143d3",
        "repo": "hive",
        "unit_tests": [
            "TestEncodedReaderImpl.java"
        ]
    },
    "hive_5708a0b": {
        "bug_id": "hive_5708a0b",
        "commit": "https://github.com/apache/hive/commit/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java?ref=5708a0b797bf12b4f61afaf0d343ea6bd9b237e2",
                "deletions": 13,
                "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "patch": "@@ -812,7 +812,7 @@ private void updateTableColStats(RawStore rawStore, String catName, String dbNam\n       rawStore.openTransaction();\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n-        if (!table.isSetPartitionKeys()) {\n+        if (table != null && !table.isSetPartitionKeys()) {\n           List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n           Deadline.startTimer(\"getTableColumnStatistics\");\n \n@@ -856,18 +856,20 @@ private void updateTablePartitionColStats(RawStore rawStore, String catName, Str\n       rawStore.openTransaction();\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n-        List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n-        List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n-        // Get partition column stats for this table\n-        Deadline.startTimer(\"getPartitionColumnStatistics\");\n-        List<ColumnStatistics> partitionColStats =\n-            rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);\n-        Deadline.stopTimer();\n-        sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);\n-        List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);\n-        // Also save partitions for consistency as they have the stats state.\n-        for (Partition part : parts) {\n-          sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);\n+        if (table != null) {\n+          List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n+          List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n+          // Get partition column stats for this table\n+          Deadline.startTimer(\"getPartitionColumnStatistics\");\n+          List<ColumnStatistics> partitionColStats =\n+                  rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);\n+          Deadline.stopTimer();\n+          sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);\n+          List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);\n+          // Also save partitions for consistency as they have the stats state.\n+          for (Partition part : parts) {\n+            sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);\n+          }\n         }\n         committed = rawStore.commitTransaction();\n       } catch (MetaException | NoSuchObjectException e) {\n@@ -886,6 +888,9 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str\n                                                        String tblName) {\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n+        if (table == null) {\n+          return;\n+        }\n         List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n         List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n         if ((partNames != null) && (partNames.size() > 0)) {",
                "raw_url": "https://github.com/apache/hive/raw/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "sha": "3564efe8ed42c1e5a18787a1012d6e1af90e4416",
                "status": "modified"
            }
        ],
        "message": "HIVE-21479: NPE during metastore cache update (Daniel Dai, reviewed by Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>",
        "parent": "https://github.com/apache/hive/commit/12f83719d940034dc8c6273e2772f6b30d07108e",
        "repo": "hive",
        "unit_tests": [
            "TestCachedStore.java"
        ]
    },
    "hive_597ca1b": {
        "bug_id": "hive_597ca1b",
        "commit": "https://github.com/apache/hive/commit/597ca1bdcd7d662be9cafb5238b6ae402a2972f1",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/597ca1bdcd7d662be9cafb5238b6ae402a2972f1/llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java?ref=597ca1bdcd7d662be9cafb5238b6ae402a2972f1",
                "deletions": 5,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java",
                "patch": "@@ -50,6 +50,7 @@\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n \n+import com.google.common.collect.Maps;\n import org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl;\n import org.apache.hadoop.metrics2.MetricsCollector;\n import org.apache.hadoop.metrics2.MetricsInfo;\n@@ -82,6 +83,8 @@\n   private long maxTimeLost = Long.MIN_VALUE;\n   private long maxTimeToKill = Long.MIN_VALUE;\n \n+  private final Map<String, Integer> executorNames;\n+\n   final MutableGaugeLong[] executorThreadCpuTime;\n   final MutableGaugeLong[] executorThreadUserTime;\n   @Metric\n@@ -152,6 +155,7 @@ private LlapDaemonExecutorMetrics(String displayName, JvmMetrics jm, String sess\n           \"ops\", \"latency\", interval);\n     }\n \n+    this.executorNames = Maps.newHashMap();\n     for (int i = 0; i < numExecutors; i++) {\n       MetricsInfo mic = new LlapDaemonCustomMetricsInfo(ExecutorThreadCPUTime.name() + \"_\" + i,\n           ExecutorThreadCPUTime.description());\n@@ -161,6 +165,7 @@ private LlapDaemonExecutorMetrics(String displayName, JvmMetrics jm, String sess\n       this.userMetricsInfoMap.put(i, miu);\n       this.executorThreadCpuTime[i] = registry.newGauge(mic, 0L);\n       this.executorThreadUserTime[i] = registry.newGauge(miu, 0L);\n+      this.executorNames.put(ContainerRunnerImpl.THREAD_NAME_FORMAT_PREFIX + i, i);\n     }\n   }\n \n@@ -304,13 +309,15 @@ private void updateThreadMetrics(MetricsRecordBuilder rb) {\n       final ThreadInfo[] infos = threadMXBean.getThreadInfo(ids);\n       for (int i = 0; i < ids.length; i++) {\n         ThreadInfo threadInfo = infos[i];\n+        if (threadInfo == null) {\n+          continue;\n+        }\n         String threadName = threadInfo.getThreadName();\n         long threadId = ids[i];\n-        for (int j = 0; j < numExecutors; j++) {\n-          if (threadName.equals(ContainerRunnerImpl.THREAD_NAME_FORMAT_PREFIX + j)) {\n-            executorThreadCpuTime[j].set(threadMXBean.getThreadCpuTime(threadId));\n-            executorThreadUserTime[j].set(threadMXBean.getThreadUserTime(threadId));\n-          }\n+        Integer id = executorNames.get(threadName);\n+        if (id != null) {\n+          executorThreadCpuTime[id].set(threadMXBean.getThreadCpuTime(threadId));\n+          executorThreadUserTime[id].set(threadMXBean.getThreadUserTime(threadId));\n         }\n       }\n ",
                "raw_url": "https://github.com/apache/hive/raw/597ca1bdcd7d662be9cafb5238b6ae402a2972f1/llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java",
                "sha": "92c891343c6294d528aaa44c75e480c81cbbd813",
                "status": "modified"
            }
        ],
        "message": "HIVE-15471: LLAP UI: NPE when getting thread metrics (Rajesh Balamohan, reviewed by Gopal V)",
        "parent": "https://github.com/apache/hive/commit/7299c080f3619a858e56b3826b4f91c0bcf18c6b",
        "repo": "hive",
        "unit_tests": [
            "TestLlapDaemonExecutorMetrics.java"
        ]
    },
    "hive_5db4c77": {
        "bug_id": "hive_5db4c77",
        "commit": "https://github.com/apache/hive/commit/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java?ref=5db4c77699ff2adeb30ef2d8ea038cfbd9035d99",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java",
                "patch": "@@ -305,10 +305,10 @@ private void writeEvent(HiveHookEventProto event) {\n       for (int retryCount = 0; retryCount <= MAX_RETRIES; ++retryCount) {\n         try {\n           if (eventPerFile) {\n-            LOG.debug(\"Event per file enabled. Closing proto event file: {}\", writer.getPath());\n             if (!maybeRolloverWriterForDay()) {\n               writer = logger.getWriter(logFileName + \"_\" + ++logFileCount);\n             }\n+            LOG.debug(\"Event per file enabled. New proto event file: {}\", writer.getPath());\n             writer.writeProto(event);\n             IOUtils.closeQuietly(writer);\n             writer = null;",
                "raw_url": "https://github.com/apache/hive/raw/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java",
                "sha": "ec00ab6d6e9caf8f07169270ae5ba9f306e736e5",
                "status": "modified"
            }
        ],
        "message": "HIVE-21250 : NPE in HiveProtoLoggingHook for eventPerFile mode. (Harish JP, reviewd by Anishek Agarwal)",
        "parent": "https://github.com/apache/hive/commit/793f192de238874d32d4c2f5137e97ebf048cc70",
        "repo": "hive",
        "unit_tests": [
            "TestHiveProtoLoggingHook.java"
        ]
    },
    "hive_6fb647f": {
        "bug_id": "hive_6fb647f",
        "commit": "https://github.com/apache/hive/commit/6fb647f32ce4e393c4bfcd871821d4da166abaa0",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hive/blob/6fb647f32ce4e393c4bfcd871821d4da166abaa0/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java?ref=6fb647f32ce4e393c4bfcd871821d4da166abaa0",
                "deletions": 18,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "patch": "@@ -85,6 +85,7 @@\n import org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy;\n import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.hive.common.util.HiveStringUtils;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -355,10 +356,10 @@ public void run() {\n             }\n           }\n         }\n-      } catch (MetaException e) {\n-        LOG.error(\"Updating CachedStore: error getting database names\", e);\n       } catch (InstantiationException | IllegalAccessException e) {\n         throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n+      } catch (Exception e) {\n+        LOG.error(\"Updating CachedStore: error happen when refresh\", e);\n       } finally {\n         try {\n           if (rawStore != null) {\n@@ -460,15 +461,17 @@ private void updateTableColStats(RawStore rawStore, String dbName, String tblNam\n         ColumnStatistics tableColStats =\n             rawStore.getTableColumnStatistics(dbName, tblName, colNames);\n         Deadline.stopTimer();\n-        if (tableColStatsCacheLock.writeLock().tryLock()) {\n-          // Skip background updates if we detect change\n-          if (isTableColStatsCacheDirty.compareAndSet(true, false)) {\n-            LOG.debug(\"Skipping table column stats cache update; the table column stats list we \"\n-                + \"have is dirty.\");\n-            return;\n+        if (tableColStats != null) {\n+          if (tableColStatsCacheLock.writeLock().tryLock()) {\n+            // Skip background updates if we detect change\n+            if (isTableColStatsCacheDirty.compareAndSet(true, false)) {\n+              LOG.debug(\"Skipping table column stats cache update; the table column stats list we \"\n+                  + \"have is dirty.\");\n+              return;\n+            }\n+            SharedCache.refreshTableColStats(HiveStringUtils.normalizeIdentifier(dbName),\n+                HiveStringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());\n           }\n-          SharedCache.refreshTableColStats(HiveStringUtils.normalizeIdentifier(dbName),\n-              HiveStringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());\n         }\n       } catch (MetaException | NoSuchObjectException e) {\n         LOG.info(\"Updating CachedStore: unable to read table column stats of table: \" + tblName, e);\n@@ -486,15 +489,17 @@ private void updateTablePartitionColStats(RawStore rawStore, String dbName, Stri\n         Map<String, List<ColumnStatisticsObj>> colStatsPerPartition =\n             rawStore.getColStatsForTablePartitions(dbName, tblName);\n         Deadline.stopTimer();\n-        if (partitionColStatsCacheLock.writeLock().tryLock()) {\n-          // Skip background updates if we detect change\n-          if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {\n-            LOG.debug(\"Skipping partition column stats cache update; the partition column stats \"\n-                + \"list we have is dirty.\");\n-            return;\n+        if (colStatsPerPartition != null) {\n+          if (partitionColStatsCacheLock.writeLock().tryLock()) {\n+            // Skip background updates if we detect change\n+            if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {\n+              LOG.debug(\"Skipping partition column stats cache update; the partition column stats \"\n+                  + \"list we have is dirty.\");\n+              return;\n+            }\n+            SharedCache.refreshPartitionColStats(HiveStringUtils.normalizeIdentifier(dbName),\n+                HiveStringUtils.normalizeIdentifier(tblName), colStatsPerPartition);\n           }\n-          SharedCache.refreshPartitionColStats(HiveStringUtils.normalizeIdentifier(dbName),\n-              HiveStringUtils.normalizeIdentifier(tblName), colStatsPerPartition);\n         }\n       } catch (MetaException | NoSuchObjectException e) {\n         LOG.info(\"Updating CachedStore: unable to read partitions column stats of table: \"",
                "raw_url": "https://github.com/apache/hive/raw/6fb647f32ce4e393c4bfcd871821d4da166abaa0/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "sha": "3ac4fe1604c7b0b455894b8e6293484e9226836e",
                "status": "modified"
            }
        ],
        "message": "HIVE-16848: NPE during CachedStore refresh (Daniel Dai, reviewed by Vaibhav Gumashta, Thejas Nair)",
        "parent": "https://github.com/apache/hive/commit/41f72dc3eda0e2744ea3787560ef12ec1d994038",
        "repo": "hive",
        "unit_tests": [
            "TestCachedStore.java"
        ]
    },
    "hive_735ba0d": {
        "bug_id": "hive_735ba0d",
        "commit": "https://github.com/apache/hive/commit/735ba0d872ddfbe0470497576904d721350548a4",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java?ref=735ba0d872ddfbe0470497576904d721350548a4",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java",
                "patch": "@@ -112,7 +112,7 @@ public void collect(HiveKey key, Object value) throws IOException {\n     return partitionKeys;\n   }\n \n-  public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOException {\n+  public void writePartitionKeys(Path path, JobConf job) throws IOException {\n     byte[][] partitionKeys = getPartitionKeys(job.getNumReduceTasks());\n     int numPartition = partitionKeys.length + 1;\n     if (numPartition != job.getNumReduceTasks()) {\n@@ -133,10 +133,11 @@ public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOE\n   }\n \n   // random sampling\n-  public static FetchOperator createSampler(FetchWork work, HiveConf conf, JobConf job,\n+  public static FetchOperator createSampler(FetchWork work, JobConf job,\n       Operator<?> operator) throws HiveException {\n-    int sampleNum = conf.getIntVar(HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n-    float samplePercent = conf.getFloatVar(HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n+    int sampleNum = HiveConf.getIntVar(job, HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n+    float samplePercent =\n+        HiveConf.getFloatVar(job, HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n     if (samplePercent < 0.0 || samplePercent > 1.0) {\n       throw new IllegalArgumentException(\"Percentile value must be within the range of 0 to 1.\");\n     }",
                "raw_url": "https://github.com/apache/hive/raw/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java",
                "sha": "dc1b601b732dc9cda814c41da6ae42d1179d4f56",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=735ba0d872ddfbe0470497576904d721350548a4",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "patch": "@@ -376,7 +376,7 @@ public int execute(DriverContext driverContext) {\n \n       if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n         try {\n-          handleSampling(driverContext, mWork, job, conf);\n+          handleSampling(ctx, mWork, job);\n           job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n         } catch (IllegalStateException e) {\n           console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n@@ -496,7 +496,7 @@ public int execute(DriverContext driverContext) {\n     return (returnVal);\n   }\n \n-  private void handleSampling(DriverContext context, MapWork mWork, JobConf job, HiveConf conf)\n+  private void handleSampling(Context context, MapWork mWork, JobConf job)\n       throws Exception {\n     assert mWork.getAliasToWork().keySet().size() == 1;\n \n@@ -512,7 +512,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n       inputPaths.add(new Path(path));\n     }\n \n-    Path tmpPath = context.getCtx().getExternalTmpPath(inputPaths.get(0));\n+    Path tmpPath = context.getExternalTmpPath(inputPaths.get(0));\n     Path partitionFile = new Path(tmpPath, \".partitions\");\n     ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);\n     PartitionKeySampler sampler = new PartitionKeySampler();\n@@ -541,9 +541,9 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n       fetchWork.setSource(ts);\n \n       // random sampling\n-      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, conf, job, ts);\n+      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, job, ts);\n       try {\n-        ts.initialize(conf, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n+        ts.initialize(job, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n         OperatorUtils.setChildrenCollector(ts.getChildOperators(), sampler);\n         while (fetcher.pushRow()) { }\n       } finally {\n@@ -552,7 +552,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n     } else {\n       throw new IllegalArgumentException(\"Invalid sampling type \" + mWork.getSamplingType());\n     }\n-    sampler.writePartitionKeys(partitionFile, conf, job);\n+    sampler.writePartitionKeys(partitionFile, job);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "sha": "a2cf71281e8cd960e5f297638005da010b74d6ae",
                "status": "modified"
            }
        ],
        "message": "HIVE-10816: NPE in ExecDriver::handleSampling when submitted via child JVM (Rui reviewed by Xuefu)",
        "parent": "https://github.com/apache/hive/commit/8f9d964007f668f11084d55fe5608294edb3434f",
        "repo": "hive",
        "unit_tests": [
            "TestExecDriver.java"
        ]
    },
    "hive_73fb195": {
        "bug_id": "hive_73fb195",
        "commit": "https://github.com/apache/hive/commit/73fb1953c1a5328dbfcf32f545fe74dc47dcc889",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/73fb1953c1a5328dbfcf32f545fe74dc47dcc889/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java?ref=73fb1953c1a5328dbfcf32f545fe74dc47dcc889",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java",
                "patch": "@@ -216,6 +216,11 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n       offset += length;\n     }\n \n+    if (listToRead.get() == null) {\n+      LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n+      return;\n+    }\n+\n     // 2. Now, read all of the ranges from cache or disk.\n     DiskRangeListMutateHelper toRead = new DiskRangeListMutateHelper(listToRead.get());\n     if (DebugUtils.isTraceOrcEnabled()) {",
                "raw_url": "https://github.com/apache/hive/raw/73fb1953c1a5328dbfcf32f545fe74dc47dcc889/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java",
                "sha": "e5e76a0e7a980b453c6606a6f25002d146ae4d0d",
                "status": "modified"
            }
        ],
        "message": "HIVE-10045 : LLAP : NPE in DiskRangeList helper init - initial fix (Sergey Shelukhin)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1668186 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/d9a6435e9cc348c6e85230cc1a290497926f76b4",
        "repo": "hive",
        "unit_tests": [
            "TestEncodedReaderImpl.java"
        ]
    },
    "hive_76f780a": {
        "bug_id": "hive_76f780a",
        "commit": "https://github.com/apache/hive/commit/76f780a545465d2ee674c896a0bd50540edd72e0",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "patch": "@@ -228,7 +228,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     @SuppressWarnings(\"unchecked\")\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n-            isSubQuery, mapJoinConversionPos, mapJoinDesc));\n+            isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n     OpTraits opTraits =\n         new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n             .getSortCols());",
                "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "sha": "f231b0655454085ac447b2df891b1a54e3efed1f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0",
                "deletions": 18,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java",
                "patch": "@@ -22,21 +22,6 @@\n import org.apache.hadoop.hive.ql.plan.TezWork.VertexType;\n \n public class MergeJoinProc implements NodeProcessor {\n-\n-  public Operator<? extends OperatorDesc> getLeafOperator(Operator<? extends OperatorDesc> op) {\n-    for (Operator<? extends OperatorDesc> childOp : op.getChildOperators()) {\n-      // FileSink or ReduceSink operators are used to create vertices. See\n-      // TezCompiler.\n-      if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof FileSinkOperator)) {\n-        return childOp;\n-      } else {\n-        return getLeafOperator(childOp);\n-      }\n-    }\n-\n-    return null;\n-  }\n-\n   @Override\n   public Object\n       process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n@@ -60,13 +45,13 @@\n     // merge work already exists for this merge join operator, add the dummy store work to the\n     // merge work. Else create a merge work, add above work to the merge work\n     MergeJoinWork mergeWork = null;\n-    if (context.opMergeJoinWorkMap.containsKey(getLeafOperator(mergeJoinOp))) {\n+    if (context.opMergeJoinWorkMap.containsKey(mergeJoinOp)) {\n       // we already have the merge work corresponding to this merge join operator\n-      mergeWork = context.opMergeJoinWorkMap.get(getLeafOperator(mergeJoinOp));\n+      mergeWork = context.opMergeJoinWorkMap.get(mergeJoinOp);\n     } else {\n       mergeWork = new MergeJoinWork();\n       tezWork.add(mergeWork);\n-      context.opMergeJoinWorkMap.put(getLeafOperator(mergeJoinOp), mergeWork);\n+      context.opMergeJoinWorkMap.put(mergeJoinOp, mergeWork);\n     }\n \n     mergeWork.setMergeJoinOperator(mergeJoinOp);",
                "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java",
                "sha": "e3c87277c1c3df47bacdd673210cd961409b78ee",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "patch": "@@ -137,15 +137,15 @@ public Object process(Node nd, Stack<Node> stack,\n       // we are currently walking the big table side of the merge join. we need to create or hook up\n       // merge join work.\n       MergeJoinWork mergeJoinWork = null;\n-      if (context.opMergeJoinWorkMap.containsKey(operator)) {\n+      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n         // we have found a merge work corresponding to this closing operator. Hook up this work.\n-        mergeJoinWork = context.opMergeJoinWorkMap.get(operator);\n+        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n       } else {\n         // we need to create the merge join work\n         mergeJoinWork = new MergeJoinWork();\n         mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n         tezWork.add(mergeJoinWork);\n-        context.opMergeJoinWorkMap.put(operator, mergeJoinWork);\n+        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n       }\n       // connect the work correctly.\n       mergeJoinWork.addMergedWork(work, null);\n@@ -334,10 +334,15 @@ public Object process(Node nd, Stack<Node> stack,\n           UnionWork unionWork = (UnionWork) followingWork;\n           int index = getMergeIndex(tezWork, unionWork, rs);\n           // guaranteed to be instance of MergeJoinWork if index is valid\n-          MergeJoinWork mergeJoinWork = (MergeJoinWork) tezWork.getChildren(unionWork).get(index);\n-          // disconnect the connection to union work and connect to merge work\n-          followingWork = mergeJoinWork;\n-          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n+          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n+          if (baseWork instanceof MergeJoinWork) {\n+            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n+            // disconnect the connection to union work and connect to merge work\n+            followingWork = mergeJoinWork;\n+            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n+          } else {\n+            rWork = (ReduceWork) baseWork;\n+          }\n         } else {\n           rWork = (ReduceWork) followingWork;\n         }",
                "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "sha": "59a632776f811dafd2fcd23ec3264b7864535b8f",
                "status": "modified"
            }
        ],
        "message": "HIVE-8563: Running annotate_stats_join_pkfk.q in TestMiniTezCliDriver is causing NPE (Vikram Dixit K via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633986 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/ce7a8b7f53f9cd52ed5e7f0fb427e25ce32fe30f",
        "repo": "hive",
        "unit_tests": [
            "TestGenTezWork.java"
        ]
    },
    "hive_7fa8e37": {
        "bug_id": "hive_7fa8e37",
        "commit": "https://github.com/apache/hive/commit/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef",
                "deletions": 2,
                "filename": "llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "patch": "@@ -541,7 +541,7 @@ public void indicateError(Throwable t) {\n   @Override\n   public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n     String url = \"\";\n-    if (timelineServerUri != null) {\n+    if (timelineServerUri != null && containerNodeId != null) {\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n       BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n       ContainerId containerId = biMap.inverse().get(attemptID);\n@@ -559,7 +559,7 @@ public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerN\n   @Override\n   public String getCompletedLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n     String url = \"\";\n-    if (timelineServerUri != null) {\n+    if (timelineServerUri != null && containerNodeId != null) {\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n       BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n       ContainerId containerId = biMap.inverse().get(attemptID);",
                "raw_url": "https://github.com/apache/hive/raw/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "sha": "3aae7a42fac9f67cd3a7ce9e915b569ffd994115",
                "status": "modified"
            }
        ],
        "message": "HIVE-15992: LLAP: NPE in LlapTaskCommunicator.getCompletedLogsUrl for unsuccessful attempt (Rajesh Balamohan reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/bda64ee87c74a06b3cf19b08c41d67f192f22018",
        "repo": "hive",
        "unit_tests": [
            "TestLlapTaskCommunicator.java"
        ]
    },
    "hive_7faa8a1": {
        "bug_id": "hive_7faa8a1",
        "commit": "https://github.com/apache/hive/commit/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3",
        "file": [
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/hive/blob/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java?ref=7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3",
                "deletions": 0,
                "filename": "itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java",
                "patch": "@@ -0,0 +1,66 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.minikdc;\n+\n+import org.junit.Assert;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hive.service.auth.HiveAuthFactory;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+\n+public class TestHiveAuthFactory {\n+  private static HiveConf hiveConf;\n+  private static MiniHiveKdc miniHiveKdc = null;\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    hiveConf = new HiveConf();\n+    miniHiveKdc = MiniHiveKdc.getMiniHiveKdc(hiveConf);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+  }\n+\n+  /**\n+   * Verify that delegation token manager is started with no exception\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testStartTokenManager() throws Exception {\n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, HiveAuthFactory.AuthTypes.KERBEROS.getAuthName());\n+    String principalName = miniHiveKdc.getFullHiveServicePrincipal();\n+    System.out.println(\"Principal: \" + principalName);\n+    \n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL, principalName);\n+    String keyTabFile = miniHiveKdc.getKeyTabFile(miniHiveKdc.getHiveServicePrincipal());\n+    System.out.println(\"keyTabFile: \" + keyTabFile);\n+    Assert.assertNotNull(keyTabFile);\n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB, keyTabFile);\n+\n+    System.out.println(\"rawStoreClassName =\" +  hiveConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL));\n+\n+    HiveAuthFactory authFactory = new HiveAuthFactory(hiveConf);\n+    Assert.assertNotNull(authFactory);\n+    Assert.assertEquals(\"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory\", \n+        authFactory.getAuthTransFactory().getClass().getName());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java",
                "sha": "a30ec7e9e5e079f7c13456c29082121da9b6c202",
                "status": "added"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java?ref=7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3",
                "deletions": 3,
                "filename": "service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.hive.service.auth;\n \n import java.io.IOException;\n-import java.net.InetAddress;\n import java.net.InetSocketAddress;\n import java.net.UnknownHostException;\n import java.util.ArrayList;\n@@ -33,6 +32,9 @@\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.HiveMetaStore;\n+import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.shims.HadoopShims.KerberosNameShim;\n import org.apache.hadoop.hive.shims.ShimLoader;\n import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;\n@@ -108,8 +110,11 @@ public HiveAuthFactory(HiveConf conf) throws TTransportException {\n                         conf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL));\n         // start delegation token manager\n         try {\n-          saslServer.startDelegationTokenSecretManager(conf, null, ServerMode.HIVESERVER2);\n-        } catch (IOException e) {\n+          HMSHandler baseHandler = new HiveMetaStore.HMSHandler(\n+              \"new db based metaserver\", conf, true);\n+          saslServer.startDelegationTokenSecretManager(conf, baseHandler.getMS(), ServerMode.HIVESERVER2);\n+        }\n+        catch (MetaException|IOException e) {\n           throw new TTransportException(\"Failed to start token manager\", e);\n         }\n       }",
                "raw_url": "https://github.com/apache/hive/raw/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java",
                "sha": "22c309fbfd941fb517370057ef38fbec903ce088",
                "status": "modified"
            }
        ],
        "message": "HIVE-9622 - Getting NPE when trying to restart HS2 when metastore is configured to use org.apache.hadoop.hive.thrift.DBTokenStore (Aihua via Brock)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1659302 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/c09884e0b662ecb635ccdb162d06ade2ea71d8b6",
        "repo": "hive",
        "unit_tests": [
            "TestHiveAuthFactory.java"
        ]
    },
    "hive_82b84ac": {
        "bug_id": "hive_82b84ac",
        "commit": "https://github.com/apache/hive/commit/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7",
                "deletions": 0,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "patch": "@@ -2661,6 +2661,10 @@ public GetHelper(String dbName, String tblName, boolean allowSql, boolean allowJ\n       // the fallback from failed SQL to JDO is not possible.\n       boolean isConfigEnabled = HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL)\n           && (HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL_DDL) || !isInTxn);\n+      if (isConfigEnabled && directSql == null) {\n+        directSql = new MetaStoreDirectSql(pm, getConf());\n+      }\n+\n       if (!allowJdo && isConfigEnabled && !directSql.isCompatibleDatastore()) {\n         throw new MetaException(\"SQL is not operational\"); // test path; SQL is enabled and broken.\n       }",
                "raw_url": "https://github.com/apache/hive/raw/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "sha": "82de857d221889d2c62252aabc0741a83d5f5313",
                "status": "modified"
            }
        ],
        "message": "HIVE-14173: NPE was thrown after enabling directsql in the middle of session (Chaoyu Tang, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/fad946bb2c11a9159b78426865975c0782e5f663",
        "repo": "hive",
        "unit_tests": [
            "TestObjectStore.java"
        ]
    },
    "hive_89c02bf": {
        "bug_id": "hive_89c02bf",
        "commit": "https://github.com/apache/hive/commit/89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java?ref=89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java",
                "patch": "@@ -153,7 +153,7 @@ public static CharTypeInfo getCharTypeInfo(ASTNode node)\n \n   static int getIndex(String[] list, String elem) {\n     for(int i=0; i < list.length; i++) {\n-      if (list[i].toLowerCase().equals(elem)) {\n+      if (list[i] != null && list[i].toLowerCase().equals(elem)) {\n         return i;\n       }\n     }",
                "raw_url": "https://github.com/apache/hive/raw/89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java",
                "sha": "5f13277570900bc9baae8f9fc7935f9984e51476",
                "status": "modified"
            }
        ],
        "message": "HIVE-11433: NPE for a multiple inner join query",
        "parent": "https://github.com/apache/hive/commit/f33e1537292d7f1b1f486d7b31c6485b27ce623e",
        "repo": "hive",
        "unit_tests": [
            "TestParseUtils.java"
        ]
    },
    "hive_89e51e4": {
        "bug_id": "hive_89e51e4",
        "commit": "https://github.com/apache/hive/commit/89e51e4eb47c6b505bce1928d89c0e7e2975431c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 22,
                "filename": "itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java",
                "patch": "@@ -43,7 +43,6 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Deque;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n@@ -64,19 +63,17 @@\n import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n import org.apache.hadoop.hive.cli.CliDriver;\n import org.apache.hadoop.hive.cli.CliSessionState;\n+import org.apache.hadoop.hive.common.io.CachingPrintStream;\n import org.apache.hadoop.hive.common.io.DigestPrintStream;\n import org.apache.hadoop.hive.common.io.SortAndDigestPrintStream;\n import org.apache.hadoop.hive.common.io.SortPrintStream;\n-import org.apache.hadoop.hive.common.io.CachingPrintStream;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n import org.apache.hadoop.hive.metastore.api.Index;\n import org.apache.hadoop.hive.ql.exec.FunctionRegistry;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n-import org.apache.hadoop.hive.ql.exec.vector.util.AllVectorTypesRecord;\n-import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;\n import org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.Table;\n@@ -87,22 +84,14 @@\n import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.session.SessionState;\n-import org.apache.hadoop.hive.serde.serdeConstants;\n-import org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer;\n-import org.apache.hadoop.hive.serde2.thrift.test.Complex;\n import org.apache.hadoop.hive.shims.HadoopShims;\n import org.apache.hadoop.hive.shims.ShimLoader;\n-import org.apache.hadoop.mapred.SequenceFileInputFormat;\n-import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n-import org.apache.hadoop.mapred.TextInputFormat;\n import org.apache.hadoop.util.Shell;\n import org.apache.hive.common.util.StreamPrinter;\n-import org.apache.thrift.protocol.TBinaryProtocol;\n import org.apache.tools.ant.BuildException;\n import org.apache.zookeeper.WatchedEvent;\n import org.apache.zookeeper.Watcher;\n import org.apache.zookeeper.ZooKeeper;\n-import org.junit.Assume;\n \n import com.google.common.collect.ImmutableList;\n \n@@ -145,8 +134,8 @@\n   private QTestSetup setup = null;\n   private boolean isSessionStateStarted = false;\n \n-  private String initScript;\n-  private String cleanupScript;\n+  private final String initScript;\n+  private final String cleanupScript;\n \n   static {\n     for (String srcTable : System.getProperty(\"test.src.tables\", \"\").trim().split(\",\")) {\n@@ -332,14 +321,6 @@ public QTestUtil(String outDir, String logDir, MiniClusterType clusterType,\n     HadoopShims shims = ShimLoader.getHadoopShims();\n     int numberOfDataNodes = 4;\n \n-    // can run tez tests only on hadoop 2\n-    if (clusterType == MiniClusterType.tez) {\n-      Assume.assumeTrue(ShimLoader.getMajorVersion().equals(\"0.23\"));\n-      // this is necessary temporarily - there's a probem with multi datanodes on MiniTezCluster\n-      // will be fixed in 0.3\n-      numberOfDataNodes = 1;\n-    }\n-\n     if (clusterType != MiniClusterType.none) {\n       dfs = shims.getMiniDfs(conf, numberOfDataNodes, true, null);\n       FileSystem fs = dfs.getFileSystem();",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java",
                "sha": "78ea21dbf88ef3c5337b97a31b1422bc1202c374",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -346,7 +346,7 @@ private EdgeProperty createEdgeProperty(TezEdgeProperty edgeProp, Configuration\n \n   /**\n    * Utility method to create a stripped down configuration for the MR partitioner.\n-   * \n+   *\n    * @param partitionerClassName\n    *          the real MR partitioner class name\n    * @param baseConf\n@@ -427,7 +427,7 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n \n     // use tez to combine splits\n     boolean groupSplitsInInputInitializer;\n-    \n+\n     DataSourceDescriptor dataSource;\n \n     int numTasks = -1;\n@@ -462,11 +462,12 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       }\n     }\n \n-    // set up the operator plan. Before setting up Inputs since the config is updated.\n-    Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n-    \n     if (HiveConf.getBoolVar(conf, ConfVars.HIVE_AM_SPLIT_GENERATION)\n         && !mapWork.isUseOneNullRowInputFormat()) {\n+\n+      // set up the operator plan. (before setting up splits on the AM)\n+      Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n+\n       // if we're generating the splits in the AM, we just need to set\n       // the correct plugin.\n       if (groupSplitsInInputInitializer) {\n@@ -484,6 +485,9 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       dataSource = MRInputHelpers.configureMRInputWithLegacySplitGeneration(conf, new Path(tezDir,\n           \"split_\" + mapWork.getName().replaceAll(\" \", \"_\")), true);\n       numTasks = dataSource.getNumberOfShards();\n+\n+      // set up the operator plan. (after generating splits - that changes configs)\n+      Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n     }\n \n     UserPayload serializedConf = TezUtils.createUserPayloadFromConf(conf);",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "1ef6cc5340096bd7b692941151843c0dd8405f69",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 15,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "patch": "@@ -64,6 +64,23 @@\n   protected static final String MAP_PLAN_KEY = \"__MAP_PLAN__\";\n   private MapWork mapWork;\n \n+  public MapRecordProcessor(JobConf jconf) {\n+    ObjectCache cache = ObjectCacheFactory.getCache(jconf);\n+    execContext.setJc(jconf);\n+    // create map and fetch operators\n+    mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n+    if (mapWork == null) {\n+      mapWork = Utilities.getMapWork(jconf);\n+      cache.cache(MAP_PLAN_KEY, mapWork);\n+      l4j.info(\"Plan: \"+mapWork);\n+      for (String s: mapWork.getAliases()) {\n+        l4j.info(\"Alias: \"+s);\n+      }\n+    } else {\n+      Utilities.setMapWork(jconf, mapWork);\n+    }\n+  }\n+\n   @Override\n   void init(JobConf jconf, ProcessorContext processorContext, MRTaskReporter mrReporter,\n       Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs) throws Exception {\n@@ -87,22 +104,7 @@ void init(JobConf jconf, ProcessorContext processorContext, MRTaskReporter mrRep\n       ((TezKVOutputCollector) outMap.get(outputEntry.getKey())).initialize();\n     }\n \n-    ObjectCache cache = ObjectCacheFactory.getCache(jconf);\n     try {\n-\n-      execContext.setJc(jconf);\n-      // create map and fetch operators\n-      mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n-      if (mapWork == null) {\n-        mapWork = Utilities.getMapWork(jconf);\n-        cache.cache(MAP_PLAN_KEY, mapWork);\n-        l4j.info(\"Plan: \"+mapWork);\n-        for (String s: mapWork.getAliases()) {\n-          l4j.info(\"Alias: \"+s);\n-        }\n-      } else {\n-        Utilities.setMapWork(jconf, mapWork);\n-      }\n       if (mapWork.getVectorMode()) {\n         mapOp = new VectorMapOperator();\n       } else {",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "sha": "7556d7bfba81ac67db511e423c634e4125697e10",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "patch": "@@ -130,7 +130,7 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out\n       LOG.info(\"Running task: \" + getContext().getUniqueIdentifier());\n \n       if (isMap) {\n-        rproc = new MapRecordProcessor();\n+        rproc = new MapRecordProcessor(jobConf);\n         MRInputLegacy mrInput = getMRInput(inputs);\n         try {\n           mrInput.init();\n@@ -201,6 +201,7 @@ void initialize() throws Exception {\n       this.writer = (KeyValueWriter) output.getWriter();\n     }\n \n+    @Override\n     public void collect(Object key, Object value) throws IOException {\n       writer.write(key, value);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "sha": "8b023dcd6a27251dda528b7e82946d850e5e1802",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -134,7 +134,7 @@ public int execute(DriverContext driverContext) {\n       }\n \n       List<LocalResource> additionalLr = session.getLocalizedResources();\n-      \n+\n       // log which resources we're adding (apart from the hive exec)\n       if (LOG.isDebugEnabled()) {\n         if (additionalLr == null || additionalLr.size() == 0) {\n@@ -165,7 +165,7 @@ public int execute(DriverContext driverContext) {\n       counters = client.getDAGStatus(statusGetOpts).getDAGCounters();\n       TezSessionPoolManager.getInstance().returnSession(session);\n \n-      if (LOG.isInfoEnabled()) {\n+      if (LOG.isInfoEnabled() && counters != null) {\n         for (CounterGroup group: counters) {\n           LOG.info(group.getDisplayName() +\":\");\n           for (TezCounter counter: group) {",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "f4da332875be221cf668483f4b82bd5ebf32c39e",
                "status": "modified"
            }
        ],
        "message": "HIVE-7891: Fix NPE in split generation on Tez 0.5 (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1619739 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/0754c55b488ba2795b61f1778ec617bd5c88aac8",
        "repo": "hive",
        "unit_tests": [
            "TestTezTask.java"
        ]
    },
    "hive_8b0b83f": {
        "bug_id": "hive_8b0b83f",
        "commit": "https://github.com/apache/hive/commit/8b0b83fd57553b4cb52129ff36c398e18230b649",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/8b0b83fd57553b4cb52129ff36c398e18230b649/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=8b0b83fd57553b4cb52129ff36c398e18230b649",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -4706,7 +4706,8 @@ public boolean drop_partition_by_name_with_environment_context(final String db_n\n     @Override\n     public Index add_index(final Index newIndex, final Table indexTable)\n         throws InvalidObjectException, AlreadyExistsException, MetaException, TException {\n-      startFunction(\"add_index\", \": \" + newIndex.toString() + \" \" + indexTable.toString());\n+      String tableName = indexTable != null ? indexTable.getTableName() : \"\";\n+      startFunction(\"add_index\", \": \" + newIndex.toString() + \" \" + tableName);\n       Index ret = null;\n       Exception ex = null;\n       try {\n@@ -4725,7 +4726,6 @@ public Index add_index(final Index newIndex, final Table indexTable)\n           throw newMetaException(e);\n         }\n       } finally {\n-        String tableName = indexTable != null ? indexTable.getTableName() : null;\n         endFunction(\"add_index\", ret != null, ex, tableName);\n       }\n       return ret;",
                "raw_url": "https://github.com/apache/hive/raw/8b0b83fd57553b4cb52129ff36c398e18230b649/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "58b9044930046758a83ee499692e5593cd82f9e0",
                "status": "modified"
            }
        ],
        "message": "HIVE-10495 : Hive index creation code throws NPE if index table is null (Bing Li via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/6b87af7477219a3b62acb4b8ff4e614d45816d68",
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    },
    "hive_8b2cd2a": {
        "bug_id": "hive_8b2cd2a",
        "commit": "https://github.com/apache/hive/commit/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java?ref=8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "patch": "@@ -569,7 +569,7 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {\n       assert filesIdx == numFiles;\n \n       // in recent hadoop versions, use deleteOnExit to clean tmp files.\n-      if (isNativeTable) {\n+      if (isNativeTable && fs != null && fsp != null) {\n         autoDelete = fs.deleteOnExit(fsp.outPaths[0]);\n       }\n     } catch (Exception e) {",
                "raw_url": "https://github.com/apache/hive/raw/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "sha": "2604d5d82fd5b4b67e6c7cb048c9840c4837d4b2",
                "status": "modified"
            }
        ],
        "message": "HIVE-11380: NPE when FileSinkOperator is not initialized (Yongzhi Chen, reviewed by Sergio Pena)",
        "parent": "https://github.com/apache/hive/commit/251991568c5e9e38b3480e9ef5dc972b9da112db",
        "repo": "hive",
        "unit_tests": [
            "TestFileSinkOperator.java"
        ]
    },
    "hive_a1b07ca": {
        "bug_id": "hive_a1b07ca",
        "commit": "https://github.com/apache/hive/commit/a1b07ca59db6f92bb574e136a580f0b2b6ac1490",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/a1b07ca59db6f92bb574e136a580f0b2b6ac1490/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=a1b07ca59db6f92bb574e136a580f0b2b6ac1490",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -114,7 +114,8 @@ public int execute(DriverContext driverContext) {\n \n       // if we don't have one yet create it.\n       if (session == null) {\n-        ss.setTezSession(new TezSessionState());\n+        session = new TezSessionState();\n+        ss.setTezSession(session);\n       }\n \n       // if it's not running start it.",
                "raw_url": "https://github.com/apache/hive/raw/a1b07ca59db6f92bb574e136a580f0b2b6ac1490/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "c6f431c35f37523983a1e6aa40e57cf8abab9cb7",
                "status": "modified"
            }
        ],
        "message": "HIVE-6231: NPE when switching to Tez execution mode after session has been initialized (Patch by Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1560268 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/45a2616ab18a136b3873fba57c715ece0d09cbbe",
        "repo": "hive",
        "unit_tests": [
            "TestTezTask.java"
        ]
    },
    "hive_a496e58": {
        "bug_id": "hive_a496e58",
        "commit": "https://github.com/apache/hive/commit/a496e581152425773080aac48cf479e493cd5b74",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/a496e581152425773080aac48cf479e493cd5b74/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=a496e581152425773080aac48cf479e493cd5b74",
                "deletions": 12,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "patch": "@@ -117,6 +117,8 @@\n   protected transient JobConf job;\n   public static MemoryMXBean memoryMXBean;\n   protected HadoopJobExecHelper jobExecHelper;\n+  private transient boolean isShutdown = false;\n+  private transient boolean jobKilled = false;\n \n   protected static transient final Logger LOG = LoggerFactory.getLogger(ExecDriver.class);\n \n@@ -413,10 +415,7 @@ public int execute(DriverContext driverContext) {\n \n       if (driverContext.isShutdown()) {\n         LOG.warn(\"Task was cancelled\");\n-        if (rj != null) {\n-          rj.killJob();\n-          rj = null;\n-        }\n+        killJob();\n         return 5;\n       }\n \n@@ -449,7 +448,7 @@ public int execute(DriverContext driverContext) {\n \n         if (rj != null) {\n           if (returnVal != 0) {\n-            rj.killJob();\n+            killJob();\n           }\n           jobID = rj.getID().toString();\n         }\n@@ -857,22 +856,37 @@ public void logPlanProgress(SessionState ss) throws IOException {\n     ss.getHiveHistory().logPlanProgress(queryPlan);\n   }\n \n+  public boolean isTaskShutdown() {\n+    return isShutdown;\n+  }\n+\n   @Override\n   public void shutdown() {\n     super.shutdown();\n-    if (rj != null) {\n+    killJob();\n+    isShutdown = true;\n+  }\n+\n+  @Override\n+  public String getExternalHandle() {\n+    return this.jobID;\n+  }\n+\n+  private void killJob() {\n+    boolean needToKillJob = false;\n+    synchronized(this) {\n+      if (rj != null && !jobKilled) {\n+        jobKilled = true;\n+        needToKillJob = true;\n+      }\n+    }\n+    if (needToKillJob) {\n       try {\n         rj.killJob();\n       } catch (Exception e) {\n         LOG.warn(\"failed to kill job \" + rj.getID(), e);\n       }\n-      rj = null;\n     }\n   }\n-\n-  @Override\n-  public String getExternalHandle() {\n-    return this.jobID;\n-  }\n }\n ",
                "raw_url": "https://github.com/apache/hive/raw/a496e581152425773080aac48cf479e493cd5b74/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "sha": "20ecbcdc6fd7a224316ddfd9d3992c5b9cbe261d",
                "status": "modified"
            }
        ],
        "message": "HIVE-16433: Not nullify variable \"rj\" to avoid NPE due to race condition in ExecDriver (Zhihai Xu via Jimmy Xiang)",
        "parent": "https://github.com/apache/hive/commit/e5a6b30241c166c82d082effd72967dc25804f97",
        "repo": "hive",
        "unit_tests": [
            "TestExecDriver.java"
        ]
    },
    "hive_ae29959": {
        "bug_id": "hive_ae29959",
        "commit": "https://github.com/apache/hive/commit/ae29959d45e3178290a07bcba372e60541dfd4c7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -30,6 +30,9 @@ Trunk -  Unreleased\n     HIVE-1178. enforce bucketing for a table.\n     (Namit Jain via He Yongqiang)\n \n+    HIVE-1188. NPE when TestJdbcDriver/TestHiveServer\n+    (Call Steinbach via Ning Zhang)\n+\n   IMPROVEMENTS\n     HIVE-983. Function from_unixtime takes long.\n     (Ning Zhang via zshao)",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/CHANGES.txt",
                "sha": "125e9810edfc0742fa313d5ee29675149d9edfb1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/negative/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/queries/negative/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "contrib/src/test/queries/negative/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/negative/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/positive/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/queries/positive/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "contrib/src/test/queries/positive/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/positive/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/errors/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/results/compiler/errors/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "contrib/src/test/results/compiler/errors/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/errors/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/parse/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/results/compiler/parse/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "contrib/src/test/results/compiler/parse/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/parse/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/plan/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/results/compiler/plan/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "contrib/src/test/results/compiler/plan/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/plan/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/data/metadb/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/data/metadb/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "data/metadb/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/data/metadb/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/data/warehouse/src/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/data/warehouse/src/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "data/warehouse/src/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/data/warehouse/src/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/metastore/bin/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/bin/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "metastore/bin/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/metastore/bin/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-py/serde/.gitignore",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/gen-py/serde/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7",
                "deletions": 0,
                "filename": "serde/src/gen-py/serde/.gitignore",
                "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory",
                "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-py/serde/.gitignore",
                "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d",
                "status": "added"
            }
        ],
        "message": "HIVE-1188. NPE when running TestJdbcDriver/TestHiveServer\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@915215 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/138a1d5244e6044467180de225b3a1c80ce7b407",
        "repo": "hive",
        "unit_tests": [
            "Test.java",
            "Test.java"
        ]
    },
    "hive_ae82715": {
        "bug_id": "hive_ae82715",
        "commit": "https://github.com/apache/hive/commit/ae82715f1014c4ed514441311b61ed1891e2a12b",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/ae82715f1014c4ed514441311b61ed1891e2a12b/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java?ref=ae82715f1014c4ed514441311b61ed1891e2a12b",
                "deletions": 3,
                "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "patch": "@@ -1023,9 +1023,6 @@ public String getQueryId() throws SQLException {\n       return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n     } catch (TException e) {\n       throw new SQLException(e);\n-    } catch (Exception e) {\n-      // If concurrently the query is closed before we fetch queryID.\n-      return null;\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/ae82715f1014c4ed514441311b61ed1891e2a12b/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "sha": "20e9c3c7e01d2b8300fe92cfca29bf621588801c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/ae82715f1014c4ed514441311b61ed1891e2a12b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java?ref=ae82715f1014c4ed514441311b61ed1891e2a12b",
                "deletions": 0,
                "filename": "service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java",
                "patch": "@@ -854,6 +854,9 @@ public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n       return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n     } catch (HiveSQLException e) {\n       throw new TException(e);\n+    } catch (Exception e) {\n+      // If concurrently the query is closed before we fetch queryID.\n+      return new TGetQueryIdResp((String)null);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/ae82715f1014c4ed514441311b61ed1891e2a12b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java",
                "sha": "48f4fe29ec165424bc5891c6c6980c3e0a85d482",
                "status": "modified"
            }
        ],
        "message": "HIVE-21669: HS2 throws NPE when HiveStatement.getQueryId is invoked and query is closed concurrently (Sankar Hariappan, reviewed by Mahesh Kumar Behera)",
        "parent": "https://github.com/apache/hive/commit/0f8119fe797c5b596d22ec4eaaef8aeeb501ccae",
        "repo": "hive",
        "unit_tests": [
            "ThriftCLIServiceTest.java"
        ]
    },
    "hive_af510fa": {
        "bug_id": "hive_af510fa",
        "commit": "https://github.com/apache/hive/commit/af510fa61b2203aca2c210aee9542f05e2692659",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/af510fa61b2203aca2c210aee9542f05e2692659/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java?ref=af510fa61b2203aca2c210aee9542f05e2692659",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java",
                "patch": "@@ -409,6 +409,9 @@ public void setTaskCounters(String queryId, String taskId, Counters ctrs) {\n \n   public void printRowCount(String queryId) {\n     QueryInfo ji = queryInfoMap.get(queryId);\n+    if (ji == null) {\n+      return;\n+    }\n     for (String tab : ji.rowCountMap.keySet()) {\n       console.printInfo(ji.rowCountMap.get(tab) + \" Rows loaded to \" + tab);\n     }\n@@ -420,7 +423,6 @@ public void printRowCount(String queryId) {\n    * @param queryId\n    */\n   public void endQuery(String queryId) {\n-\n     QueryInfo ji = queryInfoMap.get(queryId);\n     if (ji == null) {\n       return;",
                "raw_url": "https://github.com/apache/hive/raw/af510fa61b2203aca2c210aee9542f05e2692659/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java",
                "sha": "7e80c2d6bd5689a5be4e8ce88be4a64750de7553",
                "status": "modified"
            }
        ],
        "message": "HIVE-3265. HiveHistory.printRowCount() throws NPE (Shreepadma Venugopalan via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1378472 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/d59a476e6aee105d80ecc361f8656dd112f6fbc1",
        "repo": "hive",
        "unit_tests": [
            "TestHiveHistory.java"
        ]
    },
    "hive_b6502b5": {
        "bug_id": "hive_b6502b5",
        "commit": "https://github.com/apache/hive/commit/b6502b5ea35f316ed10e71d845a7b5c6ab4ad151",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/b6502b5ea35f316ed10e71d845a7b5c6ab4ad151/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java?ref=b6502b5ea35f316ed10e71d845a7b5c6ab4ad151",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java",
                "patch": "@@ -63,7 +63,7 @@ public ArrayWritableObjectInspector(final StructTypeInfo rowTypeInfo) {\n \n       final StructFieldImpl field = new StructFieldImpl(name, getObjectInspector(fieldInfo), i);\n       fields.add(field);\n-      fieldsByName.put(name, field);\n+      fieldsByName.put(name.toLowerCase(), field);\n     }\n   }\n \n@@ -158,7 +158,7 @@ public Object getStructFieldData(final Object data, final StructField fieldRef)\n \n   @Override\n   public StructField getStructFieldRef(final String name) {\n-    return fieldsByName.get(name);\n+    return fieldsByName.get(name.toLowerCase());\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/b6502b5ea35f316ed10e71d845a7b5c6ab4ad151/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java",
                "sha": "5f852d0368048e5a5819cf74f6ee5762fa794983",
                "status": "modified"
            }
        ],
        "message": "HIVE-13237: Select parquet struct field with upper case throws NPE (Jimmy, reviewed by Xuefu)",
        "parent": "https://github.com/apache/hive/commit/62bae5e1a5cc563c5ef3f650927f2a63038c5a50",
        "repo": "hive",
        "unit_tests": [
            "TestArrayWritableObjectInspector.java"
        ]
    },
    "hive_b650b79": {
        "bug_id": "hive_b650b79",
        "commit": "https://github.com/apache/hive/commit/b650b798ffb589800e19320595c54f5793cd1e40",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/b650b798ffb589800e19320595c54f5793cd1e40/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java?ref=b650b798ffb589800e19320595c54f5793cd1e40",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java",
                "patch": "@@ -254,9 +254,13 @@ static void dedupLockObjects(List<HiveLockObj> lockObjects) {\n \n   private HiveLockMode getWriteEntityLockMode (WriteEntity we) {\n     HiveLockMode lockMode = we.isComplete() ? HiveLockMode.EXCLUSIVE : HiveLockMode.SHARED;\n-    //but the writeEntity is complete in DDL operations, and we need check its writeType to\n-    //to determine the lockMode\n-    switch (we.getWriteType()) {\n+    //but the writeEntity is complete in DDL operations, instead DDL sets the writeType, so\n+    //we use it to determine its lockMode, and first we check if the writeType was set\n+    WriteEntity.WriteType writeType = we.getWriteType();\n+    if (writeType == null) {\n+      return lockMode;\n+    }\n+    switch (writeType) {\n       case DDL_EXCLUSIVE:\n         return HiveLockMode.EXCLUSIVE;\n       case DDL_SHARED:",
                "raw_url": "https://github.com/apache/hive/raw/b650b798ffb589800e19320595c54f5793cd1e40/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java",
                "sha": "8fdac5e29e723695a00e99751303ed54e6a9e5e5",
                "status": "modified"
            }
        ],
        "message": "HIVE-9330 : DummyTxnManager will throw NPE if WriteEntity writeType has not been set (Chaoyu Tang via Szehon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1652897 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/ec3c2f86bc2399cab3127463846efcd44d7efea2",
        "repo": "hive",
        "unit_tests": [
            "TestDummyTxnManager.java"
        ]
    },
    "hive_bd21f89": {
        "bug_id": "hive_bd21f89",
        "commit": "https://github.com/apache/hive/commit/bd21f890e21f3baa8b715cb4203405de65b2a30b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -555,6 +555,7 @@ minillaplocal.query.files=\\\n   materialized_view_describe.q,\\\n   materialized_view_drop.q,\\\n   materialized_view_rebuild.q,\\\n+  materialized_view_rewrite_empty.q,\\\n   materialized_view_rewrite_1.q,\\\n   materialized_view_rewrite_2.q,\\\n   materialized_view_rewrite_3.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/itests/src/test/resources/testconfiguration.properties",
                "sha": "3ed2cf398a8edde959ccec75d51878724518a72b",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q",
                "patch": "@@ -0,0 +1,28 @@\n+-- SORT_QUERY_RESULTS\n+\n+set hive.vectorized.execution.enabled=false;\n+set hive.support.concurrency=true;\n+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n+set hive.strict.checks.cartesian.product=false;\n+set hive.stats.fetch.column.stats=true;\n+set hive.materializedview.rewriting=true;\n+\n+create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true');\n+analyze table emps_mv_rewrite_empty compute statistics for columns;\n+\n+create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150;\n+\n+explain\n+select * from emps_mv_rewrite_empty where empid < 120;\n+\n+select * from emps_mv_rewrite_empty where empid < 120;\n+\n+drop materialized view emps_mv_rewrite_empty_mv1;\n+drop table emps_mv_rewrite_empty;",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q",
                "sha": "e5daa8dc7820752b4d55cc92f5a6b56bfd6706a4",
                "status": "added"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out",
                "patch": "@@ -0,0 +1,89 @@\n+PREHOOK: query: create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+POSTHOOK: query: create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@emps_mv_rewrite_empty\n+PREHOOK: query: analyze table emps_mv_rewrite_empty compute statistics for columns\n+PREHOOK: type: ANALYZE_TABLE\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table emps_mv_rewrite_empty compute statistics for columns\n+POSTHOOK: type: ANALYZE_TABLE\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+PREHOOK: query: create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150\n+PREHOOK: type: CREATE_MATERIALIZED_VIEW\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: query: create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150\n+POSTHOOK: type: CREATE_MATERIALIZED_VIEW\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: query: explain\n+select * from emps_mv_rewrite_empty where empid < 120\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from emps_mv_rewrite_empty where empid < 120\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: emps_mv_rewrite_empty\n+          Filter Operator\n+            predicate: (empid < 120) (type: boolean)\n+            Select Operator\n+              expressions: empid (type: int), deptno (type: int), name (type: varchar(256)), salary (type: float), commission (type: int)\n+              outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+              ListSink\n+\n+PREHOOK: query: select * from emps_mv_rewrite_empty where empid < 120\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from emps_mv_rewrite_empty where empid < 120\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+PREHOOK: query: drop materialized view emps_mv_rewrite_empty_mv1\n+PREHOOK: type: DROP_MATERIALIZED_VIEW\n+PREHOOK: Input: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: query: drop materialized view emps_mv_rewrite_empty_mv1\n+POSTHOOK: type: DROP_MATERIALIZED_VIEW\n+POSTHOOK: Input: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: query: drop table emps_mv_rewrite_empty\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+POSTHOOK: query: drop table emps_mv_rewrite_empty\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: default@emps_mv_rewrite_empty",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out",
                "sha": "b33d8c3f2d3b0e9eae01b7b878b564350b1a5961",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java",
                "patch": "@@ -360,6 +360,15 @@ private void enrichWithInvalidationInfo(Materialization materialization) {\n \n       final ConcurrentSkipListMap<Long, Long> usedTableModifications =\n           tableModifications.get(qNameTableUsed);\n+      if (usedTableModifications == null) {\n+        // This is not necessarily an error, since the table may be empty. To be safe,\n+        // instead of including this materialized view, we just log the information and\n+        // skip it (if table is really empty, it will not matter for performance anyway).\n+        LOG.warn(\"No information found in invalidation cache for table {}, possible tables are: {}\",\n+            qNameTableUsed, tableModifications.keySet());\n+        materialization.setInvalidationTime(Long.MIN_VALUE);\n+        return;\n+      }\n       final ConcurrentSkipListSet<Long> usedUDTableModifications =\n           updateDeleteTableModifications.get(qNameTableUsed);\n       final Entry<Long, Long> tn = usedTableModifications.higherEntry(tableMaterializationTxnList.getHighWatermark());",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java",
                "sha": "fc644f0b637616a670a3585dc4978fb917828747",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 19,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "patch": "@@ -838,11 +838,7 @@ public void commitTxn(CommitTxnRequest rqst)\n     throws NoSuchTxnException, TxnAbortedException, MetaException {\n     MaterializationsRebuildLockHandler materializationsRebuildLockHandler =\n         MaterializationsRebuildLockHandler.get();\n-    String fullyQualifiedName = null;\n-    String dbName = null;\n-    String tblName = null;\n-    long writeId = 0L;\n-    long timestamp = 0L;\n+    List<TransactionRegistryInfo> txnComponents = new ArrayList<>();\n     boolean isUpdateDelete = false;\n     long txnid = rqst.getTxnid();\n     long sourceTxnId = -1;\n@@ -1007,12 +1003,10 @@ public void commitTxn(CommitTxnRequest rqst)\n         s = \"select ctc_database, ctc_table, ctc_writeid, ctc_timestamp from COMPLETED_TXN_COMPONENTS where ctc_txnid = \" + txnid;\n         LOG.debug(\"Going to extract table modification information for invalidation cache <\" + s + \">\");\n         rs = stmt.executeQuery(s);\n-        if (rs.next()) {\n-          dbName = rs.getString(1);\n-          tblName = rs.getString(2);\n-          fullyQualifiedName = Warehouse.getQualifiedName(dbName, tblName);\n-          writeId = rs.getLong(3);\n-          timestamp = rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"))).getTime();\n+        while (rs.next()) {\n+          // We only enter in this loop if the transaction actually affected any table\n+          txnComponents.add(new TransactionRegistryInfo(rs.getString(1), rs.getString(2),\n+              rs.getLong(3), rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"))).getTime()));\n         }\n         s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n         LOG.debug(\"Going to execute update <\" + s + \">\");\n@@ -1042,18 +1036,22 @@ public void commitTxn(CommitTxnRequest rqst)\n \n         MaterializationsInvalidationCache materializationsInvalidationCache =\n             MaterializationsInvalidationCache.get();\n-        if (materializationsInvalidationCache.containsMaterialization(dbName, tblName) &&\n-            !materializationsRebuildLockHandler.readyToCommitResource(dbName, tblName, txnid)) {\n-          throw new MetaException(\n-              \"Another process is rebuilding the materialized view \" + fullyQualifiedName);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          if (materializationsInvalidationCache.containsMaterialization(info.dbName, info.tblName) &&\n+              !materializationsRebuildLockHandler.readyToCommitResource(info.dbName, info.tblName, txnid)) {\n+            throw new MetaException(\n+                \"Another process is rebuilding the materialized view \" + info.fullyQualifiedName);\n+          }\n         }\n         LOG.debug(\"Going to commit\");\n         close(rs);\n         dbConn.commit();\n \n         // Update registry with modifications\n-        materializationsInvalidationCache.notifyTableModification(\n-            dbName, tblName, writeId, timestamp, isUpdateDelete);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          materializationsInvalidationCache.notifyTableModification(\n+              info.dbName, info.tblName, info.writeId, info.timestamp, isUpdateDelete);\n+        }\n       } catch (SQLException e) {\n         LOG.debug(\"Going to rollback\");\n         rollbackDBConn(dbConn);\n@@ -1064,8 +1062,8 @@ public void commitTxn(CommitTxnRequest rqst)\n         close(commitIdRs);\n         close(lockHandle, stmt, dbConn);\n         unlockInternal();\n-        if (fullyQualifiedName != null) {\n-          materializationsRebuildLockHandler.unlockResource(dbName, tblName, txnid);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          materializationsRebuildLockHandler.unlockResource(info.dbName, info.tblName, txnid);\n         }\n       }\n     } catch (RetryException e) {\n@@ -4783,4 +4781,21 @@ public boolean isWrapperFor(Class<?> iface) throws SQLException {\n       throw new UnsupportedOperationException();\n     }\n   };\n+\n+  private class TransactionRegistryInfo {\n+    final String dbName;\n+    final String tblName;\n+    final String fullyQualifiedName;\n+    final long writeId;\n+    final long timestamp;\n+\n+    public TransactionRegistryInfo (String dbName, String tblName, long writeId, long timestamp) {\n+      this.dbName = dbName;\n+      this.tblName = tblName;\n+      this.fullyQualifiedName = Warehouse.getQualifiedName(dbName, tblName);\n+      this.writeId = writeId;\n+      this.timestamp = timestamp;\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "sha": "361ede54efcd739516285e7b6a85e1122e8b870a",
                "status": "modified"
            }
        ],
        "message": "HIVE-19884 : Invalidation cache may throw NPE when there is no data in table used by materialized view (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/5a9a328a8129eb8bd116158e06cf37a259cf32c6",
        "repo": "hive",
        "unit_tests": [
            "TestTxnHandler.java"
        ]
    },
    "hive_bef879d": {
        "bug_id": "hive_bef879d",
        "commit": "https://github.com/apache/hive/commit/bef879d0a3e1827bffbd278a883e721124ee0eea",
        "file": [
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hive/blob/bef879d0a3e1827bffbd278a883e721124ee0eea/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java?ref=bef879d0a3e1827bffbd278a883e721124ee0eea",
                "deletions": 15,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "patch": "@@ -140,7 +140,9 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {\n       } else if (heapSize == heap.length) {\n         // The buffer is not in the (full) heap. Demote the top item of the heap into the list.\n         LlapCacheableBuffer demoted = heap[0];\n-        synchronized (listLock) {\n+        listLock.lock();\n+        try {\n+          assert demoted.indexInHeap == 0; // Noone could have moved it, we have the heap lock.\n           demoted.indexInHeap = LlapCacheableBuffer.IN_LIST;\n           demoted.prev = null;\n           if (listHead != null) {\n@@ -151,6 +153,8 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {\n             listHead = listTail = demoted;\n             demoted.next = null;\n           }\n+        } finally {\n+          listLock.unlock();\n         }\n         // Now insert the buffer in its place and restore heap property.\n         buffer.indexInHeap = 0;\n@@ -340,44 +344,62 @@ private void removeFromListAndUnlock(LlapCacheableBuffer buffer) {\n   }\n \n   private void removeFromListUnderLock(LlapCacheableBuffer buffer) {\n-    if (buffer == listTail) {\n+    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n+    boolean isTail = buffer == listTail, isHead = buffer == listHead;\n+    if ((isTail != (buffer.next == null)) || (isHead != (buffer.prev == null))) {\n+      debugDumpListOnError(buffer);\n+      throw new AssertionError(\"LRFU list is corrupted.\");\n+    }\n+    if (isTail) {\n       listTail = buffer.prev;\n     } else {\n       buffer.next.prev = buffer.prev;\n     }\n-    if (buffer == listHead) {\n+    if (isHead) {\n       listHead = buffer.next;\n     } else {\n       buffer.prev.next = buffer.next;\n     }\n-    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n   }\n \n   private void removeFromListUnderLockNoStateUpdate(\n       LlapCacheableBuffer from, LlapCacheableBuffer to) {\n-    if (to == listTail) {\n+    boolean isToTail = to == listTail, isFromHead = from == listHead;\n+    if ((isToTail != (to.next == null)) || (isFromHead != (from.prev == null))) {\n+      debugDumpListOnError(from, to);\n+      throw new AssertionError(\"LRFU list is corrupted.\");\n+    }\n+    if (isToTail) {\n       listTail = from.prev;\n     } else {\n       to.next.prev = from.prev;\n     }\n-    if (from == listHead) {\n+    if (isFromHead) {\n       listHead = to.next;\n     } else {\n       from.prev.next = to.next;\n     }\n   }\n \n-  public String debugDumpHeap() {\n-    StringBuilder result = new StringBuilder(\"List: \");\n-    if (listHead == null) {\n-      result.append(\"<empty>\");\n-    } else {\n-      LlapCacheableBuffer listItem = listHead;\n-      while (listItem != null) {\n-        result.append(listItem.toStringForCache()).append(\" -> \");\n-        listItem = listItem.next;\n+  private void debugDumpListOnError(LlapCacheableBuffer... buffers) {\n+    // Hopefully this will be helpful in case of NPEs.\n+    StringBuilder listDump = new StringBuilder(\"Invalid list removal. List: \");\n+    try {\n+      dumpList(listDump, listHead, listTail);\n+      int i = 0;\n+      for (LlapCacheableBuffer buffer : buffers) {\n+        listDump.append(\"; list from the buffer #\").append(i).append(\" being removed: \");\n+        dumpList(listDump, buffer, null);\n       }\n+    } catch (Throwable t) {\n+      LlapIoImpl.LOG.error(\"Error dumping the lists on error\", t);\n     }\n+    LlapIoImpl.LOG.error(listDump.toString());\n+  }\n+\n+  public String debugDumpHeap() {\n+    StringBuilder result = new StringBuilder(\"List: \");\n+    dumpList(result, listHead, listTail);\n     result.append(\"\\nHeap:\");\n     if (heapSize == 0) {\n       result.append(\" <empty>\\n\");\n@@ -421,6 +443,29 @@ public String debugDumpHeap() {\n     return result.toString();\n   }\n \n+  private static void dumpList(StringBuilder result,\n+      LlapCacheableBuffer listHeadLocal, LlapCacheableBuffer listTailLocal) {\n+    if (listHeadLocal == null) {\n+      result.append(\"<empty>\");\n+      return;\n+    }\n+    LlapCacheableBuffer listItem = listHeadLocal;\n+    while (listItem.prev != null) {\n+      listItem = listItem.prev;  // To detect incorrect lists.\n+    }\n+    while (listItem != null) {\n+      result.append(listItem.toStringForCache());\n+      if (listItem == listTailLocal) {\n+        result.append(\"(tail)\"); // To detect incorrect lists.\n+      }\n+      if (listItem == listHeadLocal) {\n+        result.append(\"(head)\"); // To detect incorrect lists.\n+      }\n+      result.append(\" -> \");\n+      listItem = listItem.next;\n+    }\n+  }\n+\n   @Override\n   public String debugDumpForOom() {\n     String result = debugDumpHeap();",
                "raw_url": "https://github.com/apache/hive/raw/bef879d0a3e1827bffbd278a883e721124ee0eea/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "sha": "40cb92d24550489bb9df653e179700594fcb9679",
                "status": "modified"
            }
        ],
        "message": "HIVE-12557 : NPE while removing entry in LRFU cache (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/1d02ab578dbd47103a70710abd4d949ea8cea9d2",
        "repo": "hive",
        "unit_tests": [
            "TestLowLevelLrfuCachePolicy.java"
        ]
    },
    "hive_c48bed2": {
        "bug_id": "hive_c48bed2",
        "commit": "https://github.com/apache/hive/commit/c48bed2e95a60e347b2450c92cfe3581024ea94b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/c48bed2e95a60e347b2450c92cfe3581024ea94b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java?ref=c48bed2e95a60e347b2450c92cfe3581024ea94b",
                "deletions": 1,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java",
                "patch": "@@ -203,8 +203,9 @@ public void unregisterTask(String amLocation, int port) {\n       amNodeInfo = knownAppMasters.get(amNodeId);\n       if (amNodeInfo == null) {\n         LOG.info((\"Ignoring duplicate unregisterRequest for am at: \" + amLocation + \":\" + port));\n+      } else {\n+        amNodeInfo.decrementAndGetTaskCount();\n       }\n-      amNodeInfo.decrementAndGetTaskCount();\n       // Not removing this here. Will be removed when taken off the queue and discovered to have 0\n       // pending tasks.\n     }",
                "raw_url": "https://github.com/apache/hive/raw/c48bed2e95a60e347b2450c92cfe3581024ea94b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java",
                "sha": "2fd2546406c89f14dc903d6b0aa43b9832666100",
                "status": "modified"
            }
        ],
        "message": "HIVE-11260. LLAP: Fix NPE in AMReporter. (Siddharth Seth)",
        "parent": "https://github.com/apache/hive/commit/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c",
        "repo": "hive",
        "unit_tests": [
            "TestAMReporter.java"
        ]
    },
    "hive_c5ae579": {
        "bug_id": "hive_c5ae579",
        "commit": "https://github.com/apache/hive/commit/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision.q",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/decimal_precision.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientnegative/decimal_precision.q",
                "patch": "@@ -0,0 +1,10 @@\n+DROP TABLE IF EXISTS DECIMAL_PRECISION;\n+\n+CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE;\n+\n+SELECT dec * 123456789012345678901234567890.123456789bd FROM DECIMAL_PRECISION;\n+\n+DROP TABLE DECIMAL_PRECISION;",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision.q",
                "sha": "f49649837e2142f89c6ad32e9b5ed70c9f0317e0",
                "status": "added"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision_1.q",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/decimal_precision_1.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientnegative/decimal_precision_1.q",
                "patch": "@@ -0,0 +1,10 @@\n+DROP TABLE IF EXISTS DECIMAL_PRECISION;\n+\n+CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE;\n+\n+SELECT * from DECIMAL_PRECISION WHERE dec > 1234567890123456789.0123456789bd;\n+\n+DROP TABLE DECIMAL_PRECISION;",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision_1.q",
                "sha": "036ff1facc0a1ca8190b295f97b91e7af716b1b1",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_precision.q",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/decimal_precision.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/decimal_precision.q",
                "patch": "@@ -17,4 +17,11 @@ SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec;\n \n SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;\n \n+SELECT dec * cast('123456789012345678901234567890.123456789' as decimal) FROM DECIMAL_PRECISION LIMIT 1;\n+SELECT * from DECIMAL_PRECISION WHERE dec > cast('123456789012345678901234567890.123456789' as decimal) LIMIT 1;\n+SELECT dec * 123456789012345678901234567890.123456789 FROM DECIMAL_PRECISION LIMIT 1;\n+\n+SELECT MIN(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION;\n+SELECT COUNT(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION;\n+\n DROP TABLE DECIMAL_PRECISION;",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_precision.q",
                "sha": "403c2be3fbc10dc6e18444055cea47d061ed4322",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_udf.q",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/decimal_udf.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/decimal_udf.q",
                "patch": "@@ -113,4 +113,16 @@ SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value;\n EXPLAIN SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF; \n SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF; \n \n+-- min\n+EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF;\n+SELECT MIN(key) FROM DECIMAL_UDF;\n+\n+-- max\n+EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF;\n+SELECT MAX(key) FROM DECIMAL_UDF;\n+\n+-- count\n+EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF;\n+SELECT COUNT(key) FROM DECIMAL_UDF;\n+\n DROP TABLE IF EXISTS DECIMAL_UDF;",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_udf.q",
                "sha": "b5ff088d1613a92b41948e7d48b0d042fdd52a37",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision.q.out",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/decimal_precision.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/results/clientnegative/decimal_precision.q.out",
                "patch": "@@ -0,0 +1,16 @@\n+PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@DECIMAL_PRECISION\n+FAILED: SemanticException [Error 10029]: Line 3:13 Invalid numerical constant '123456789012345678901234567890.123456789bd'",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision.q.out",
                "sha": "9eddd38c787ad9f8e9dd99f7831c797c7481db5a",
                "status": "added"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision_1.q.out",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/decimal_precision_1.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/results/clientnegative/decimal_precision_1.q.out",
                "patch": "@@ -0,0 +1,16 @@\n+PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@DECIMAL_PRECISION\n+FAILED: SemanticException [Error 10029]: Line 3:44 Invalid numerical constant '1234567890123456789.0123456789bd'",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision_1.q.out",
                "sha": "21ef65fb27e05873e47c76f17f2f4fb77bc40925",
                "status": "added"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_precision.q.out",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_precision.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/decimal_precision.q.out",
                "patch": "@@ -514,6 +514,50 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@decimal_precision\n #### A masked pattern was here ####\n NULL\tNULL\n+PREHOOK: query: SELECT dec * cast('123456789012345678901234567890.123456789' as decimal) FROM DECIMAL_PRECISION LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT dec * cast('123456789012345678901234567890.123456789' as decimal) FROM DECIMAL_PRECISION LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+NULL\n+PREHOOK: query: SELECT * from DECIMAL_PRECISION WHERE dec > cast('123456789012345678901234567890.123456789' as decimal) LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * from DECIMAL_PRECISION WHERE dec > cast('123456789012345678901234567890.123456789' as decimal) LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+PREHOOK: query: SELECT dec * 123456789012345678901234567890.123456789 FROM DECIMAL_PRECISION LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT dec * 123456789012345678901234567890.123456789 FROM DECIMAL_PRECISION LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+NULL\n+PREHOOK: query: SELECT MIN(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT MIN(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+NULL\n+PREHOOK: query: SELECT COUNT(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+0\n PREHOOK: query: DROP TABLE DECIMAL_PRECISION\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@decimal_precision",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_precision.q.out",
                "sha": "232dace12b8060579fedb025db9841d2569c2b39",
                "status": "modified"
            },
            {
                "additions": 210,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_udf.q.out",
                "changes": 210,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_udf.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/decimal_udf.q.out",
                "patch": "@@ -2482,6 +2482,216 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@decimal_udf\n #### A masked pattern was here ####\n [{\"x\":-1.2345678901234567E9,\"y\":1.0},{\"x\":-148.75058823529412,\"y\":34.0},{\"x\":1.2345678901234567E9,\"y\":1.0}]\n+PREHOOK: query: -- min\n+EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- min\n+EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME DECIMAL_UDF))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION MIN (TOK_TABLE_OR_COL key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        decimal_udf \n+          TableScan\n+            alias: decimal_udf\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: decimal\n+              outputColumnNames: key\n+              Group By Operator\n+                aggregations:\n+                      expr: min(key)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: decimal\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: min(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: decimal\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+-1234567890.123456789\n+PREHOOK: query: -- max\n+EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- max\n+EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME DECIMAL_UDF))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION MAX (TOK_TABLE_OR_COL key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        decimal_udf \n+          TableScan\n+            alias: decimal_udf\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: decimal\n+              outputColumnNames: key\n+              Group By Operator\n+                aggregations:\n+                      expr: max(key)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: decimal\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: max(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: decimal\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+1234567890.12345678\n+PREHOOK: query: -- count\n+EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- count\n+EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME DECIMAL_UDF))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION COUNT (TOK_TABLE_OR_COL key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        decimal_udf \n+          TableScan\n+            alias: decimal_udf\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: decimal\n+              outputColumnNames: key\n+              Group By Operator\n+                aggregations:\n+                      expr: count(key)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: bigint\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: count(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: bigint\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+36\n PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@decimal_udf",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_udf.q.out",
                "sha": "52c8a81f7b8195b7de88b054bd0dd508d0eddde0",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 3,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java",
                "patch": "@@ -276,10 +276,14 @@ public Object convert(Object input) {\n       if (input == null) {\n         return null;\n       }\n-      return outputOI.set(r, PrimitiveObjectInspectorUtils.getHiveDecimal(input,\n-          inputOI));\n+      \n+      try {\n+        return outputOI.set(r, PrimitiveObjectInspectorUtils.getHiveDecimal(input,\n+            inputOI));\n+      } catch (NumberFormatException e) {\n+        return null;\n+      }\n     }\n-\n   }\n \n   public static class BinaryConverter implements Converter{",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java",
                "sha": "742493c82b454d0f35909dcfbc075d06ab1c329f",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java",
                "changes": 86,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633",
                "deletions": 45,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java",
                "patch": "@@ -778,52 +778,48 @@ public static HiveDecimal getHiveDecimal(Object o, PrimitiveObjectInspector oi)\n     }\n \n     HiveDecimal result = null;\n-    try {\n-      switch (oi.getPrimitiveCategory()) {\n-      case VOID:\n-        result = null;\n-        break;\n-      case BOOLEAN:\n-        result = ((BooleanObjectInspector) oi).get(o) ?\n-            HiveDecimal.ONE : HiveDecimal.ZERO;\n-        break;\n-      case BYTE:\n-        result = new HiveDecimal(((ByteObjectInspector) oi).get(o));\n-        break;\n-      case SHORT:\n-        result = new HiveDecimal(((ShortObjectInspector) oi).get(o));\n-        break;\n-      case INT:\n-        result = new HiveDecimal(((IntObjectInspector) oi).get(o));\n-        break;\n-      case LONG:\n-        result = new HiveDecimal(((LongObjectInspector) oi).get(o));\n-        break;\n-      case FLOAT:\n-        Float f = ((FloatObjectInspector) oi).get(o);\n-        result = new HiveDecimal(f.toString());\n-        break;\n-      case DOUBLE:\n-        Double d = ((DoubleObjectInspector) oi).get(o);\n-        result = new HiveDecimal(d.toString());\n-        break;\n-      case STRING:\n-        result = new HiveDecimal(((StringObjectInspector) oi).getPrimitiveJavaObject(o));\n-        break;\n-      case TIMESTAMP:\n-        Double ts = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)\n+    switch (oi.getPrimitiveCategory()) {\n+    case VOID:\n+      result = null;\n+      break;\n+    case BOOLEAN:\n+      result = ((BooleanObjectInspector) oi).get(o) ?\n+        HiveDecimal.ONE : HiveDecimal.ZERO;\n+      break;\n+    case BYTE:\n+      result = new HiveDecimal(((ByteObjectInspector) oi).get(o));\n+      break;\n+    case SHORT:\n+      result = new HiveDecimal(((ShortObjectInspector) oi).get(o));\n+      break;\n+    case INT:\n+      result = new HiveDecimal(((IntObjectInspector) oi).get(o));\n+      break;\n+    case LONG:\n+      result = new HiveDecimal(((LongObjectInspector) oi).get(o));\n+      break;\n+    case FLOAT:\n+      Float f = ((FloatObjectInspector) oi).get(o);\n+      result = new HiveDecimal(f.toString());\n+      break;\n+    case DOUBLE:\n+      Double d = ((DoubleObjectInspector) oi).get(o);\n+      result = new HiveDecimal(d.toString());\n+      break;\n+    case STRING:\n+      result = new HiveDecimal(((StringObjectInspector) oi).getPrimitiveJavaObject(o));\n+      break;\n+    case TIMESTAMP:\n+      Double ts = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)\n         .getDouble();\n-        result = new HiveDecimal(ts.toString());\n-        break;\n-      case DECIMAL:\n-        result = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);\n-        break;\n-      default:\n-        throw new RuntimeException(\"Hive 2 Internal error: unknown type: \"\n-            + oi.getTypeName());\n-      }\n-    } catch(NumberFormatException e) {\n-      // return null\n+      result = new HiveDecimal(ts.toString());\n+      break;\n+    case DECIMAL:\n+      result = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);\n+      break;\n+    default:\n+      throw new RuntimeException(\"Hive 2 Internal error: unknown type: \"\n+                                 + oi.getTypeName());\n     }\n     return result;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java",
                "sha": "94062dd7812036224676767148b68239000af35b",
                "status": "modified"
            }
        ],
        "message": "HIVE-4327 : NPE in constant folding with decimal (Gunther Hagleitner via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1468423 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/0763293314df49c5fd5c628eb22b779e8d9dab3e",
        "repo": "hive",
        "unit_tests": [
            "TestPrimitiveObjectInspectorUtils.java"
        ]
    },
    "hive_c7d2643": {
        "bug_id": "hive_c7d2643",
        "commit": "https://github.com/apache/hive/commit/c7d2643e0025d6fa16744872c7a131f4b5a377e6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java",
                "patch": "@@ -131,6 +131,9 @@ public double getRowCount() {\n       }\n     }\n \n+    if (rowCount == -1)\n+      noColsMissingStats.getAndIncrement();\n+\n     return rowCount;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java",
                "sha": "ddef37be78d2ba64332d6b5eb963b98df4223290",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java",
                "patch": "@@ -86,7 +86,7 @@ public static HiveProjectRel create(RelNode child, List<? extends RexNode> exps,\n     RelOptCluster cluster = child.getCluster();\n \n     // 1 Ensure columnNames are unique - OPTIQ-411\n-    if (!Util.isDistinct(fieldNames)) {\n+    if (fieldNames != null && !Util.isDistinct(fieldNames)) {\n       String msg = \"Select list contains multiple expressions with the same name.\" + fieldNames;\n       throw new OptiqSemanticException(msg);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java",
                "sha": "c643aa46c484210fe348602ae41bfe2deccca2c5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6",
                "deletions": 10,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -12139,6 +12139,7 @@ private boolean canHandleQuery(QB qbToChk, boolean topLevelQB) {\n     // 1. If top level QB is query then everything below it must also be Query\n     // 2. Nested Subquery will return false for qbToChk.getIsQuery()\n     if ((!topLevelQB || qbToChk.getIsQuery())\n+        && (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || conf.getVar(ConfVars.HIVEMAPREDMODE).equalsIgnoreCase(\"nonstrict\"))\n         && (!topLevelQB || (queryProperties.getJoinCount() > 1) || conf.getBoolVar(ConfVars.HIVE_IN_TEST))\n         && !queryProperties.hasClusterBy() && !queryProperties.hasDistributeBy()\n         && !queryProperties.hasSortBy() && !queryProperties.hasPTF()\n@@ -13870,7 +13871,6 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n         aliasToRel.put(tableAlias, op);\n       }\n \n-\n       if (aliasToRel.isEmpty()) {\n         //// This may happen for queries like select 1; (no source table)\n         // We can do following which is same, as what Hive does.\n@@ -13910,7 +13910,15 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n       selectRel = genSelectLogicalPlan(qb, srcRel);\n       srcRel = (selectRel == null) ? srcRel : selectRel;\n \n-      // 6. Incase this QB corresponds to subquery then modify its RR to point\n+      // 6. Build Rel for OB Clause\n+      obRel = genOBLogicalPlan(qb, srcRel);\n+      srcRel = (obRel == null) ? srcRel : obRel;\n+\n+      // 7. Build Rel for Limit Clause\n+      limitRel = genLimitLogicalPlan(qb, srcRel);\n+      srcRel = (limitRel == null) ? srcRel : limitRel;\n+\n+      // 8. Incase this QB corresponds to subquery then modify its RR to point\n       // to subquery alias\n       // TODO: cleanup this\n       if (qb.getParseInfo().getAlias() != null) {\n@@ -13932,14 +13940,6 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n         relToHiveColNameOptiqPosMap.put(srcRel, buildHiveToOptiqColumnMap(newRR, srcRel));\n       }\n \n-      // 7. Build Rel for OB Clause\n-      obRel = genOBLogicalPlan(qb, srcRel);\n-      srcRel = (obRel == null) ? srcRel : obRel;\n-\n-      // 8. Build Rel for Limit Clause\n-      limitRel = genLimitLogicalPlan(qb, srcRel);\n-      srcRel = (limitRel == null) ? srcRel : limitRel;\n-\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Created Plan for Query Block \" + qb.getId());\n       }",
                "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "0daad5bfea23f76c846e17f7c7d5f4940a1cce46",
                "status": "modified"
            }
        ],
        "message": "HIVE-8166 : CBO: 1) Bailout in strict mode 2) OB,LIMIT RR table alias is same as that of sub query 3) If RowCount Not found then fall back to non cbo 4)Fix NPE in unique col name check (John Pullokkaran via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1625852 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/88e790ad42a829ba0492e4ecfaf05dc750798a0c",
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_c935c69": {
        "bug_id": "hive_c935c69",
        "commit": "https://github.com/apache/hive/commit/c935c69ae31a634a2e74d401baf4dea95b7d0878",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java?ref=c935c69ae31a634a2e74d401baf4dea95b7d0878",
                "deletions": 0,
                "filename": "cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java",
                "patch": "@@ -95,6 +95,7 @@ public int processCmd(String cmd) {\n       // if we have come this far - either the previous commands\n       // are all successful or this is command line. in either case\n       // this counts as a successful run\n+      ss.close();\n       System.exit(0);\n \n     } else if (tokens[0].equalsIgnoreCase(\"source\")) {",
                "raw_url": "https://github.com/apache/hive/raw/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java",
                "sha": "1fb11e7abdb2ec6426f9379c85019602104b364f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java?ref=c935c69ae31a634a2e74d401baf4dea95b7d0878",
                "deletions": 3,
                "filename": "cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java",
                "patch": "@@ -103,9 +103,10 @@ public int getPort() {\n \n   public void close() {\n     try {\n-      client.clean();\n-      client.shutdown();\n-      transport.close();\n+      if (remoteMode) {\n+        client.clean();\n+        transport.close();\n+      }\n     } catch (TException e) {\n       e.printStackTrace();\n     }",
                "raw_url": "https://github.com/apache/hive/raw/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java",
                "sha": "c693aeb35a88093abd6ee2dd69cd8da460b0091b",
                "status": "modified"
            }
        ],
        "message": "HIVE-2060:CLI local mode hit NPE when exiting by ^D (Ning Zhang via He Yongqiang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1083640 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/a29e17a29b24e86d305c2eee1dfa76df5142c313",
        "repo": "hive",
        "unit_tests": [
            "TestCliSessionState.java"
        ]
    },
    "hive_cc07902": {
        "bug_id": "hive_cc07902",
        "commit": "https://github.com/apache/hive/commit/cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2/beeline/src/java/org/apache/hive/beeline/Commands.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/Commands.java?ref=cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2",
                "deletions": 1,
                "filename": "beeline/src/java/org/apache/hive/beeline/Commands.java",
                "patch": "@@ -674,7 +674,10 @@ private boolean execute(String line, boolean call) {\n \n     // use multiple lines for statements not terminated by \";\"\n     try {\n-      while (!(line.trim().endsWith(\";\")) && beeLine.getOpts().isAllowMultiLineCommand()) {\n+      //When using -e, console reader is not initialized and command is a single line\n+      while (beeLine.getConsoleReader() != null && !(line.trim().endsWith(\";\"))\n+        && beeLine.getOpts().isAllowMultiLineCommand()) {\n+\n         StringBuilder prompt = new StringBuilder(beeLine.getPrompt());\n         for (int i = 0; i < prompt.length() - 1; i++) {\n           if (prompt.charAt(i) != '>') {\n@@ -691,6 +694,7 @@ private boolean execute(String line, boolean call) {\n       beeLine.handleException(e);\n     }\n \n+\n     if (line.endsWith(\";\")) {\n       line = line.substring(0, line.length() - 1);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2/beeline/src/java/org/apache/hive/beeline/Commands.java",
                "sha": "d2d7fd3bfec9cc279a61323ce695b7e80973d0a6",
                "status": "modified"
            }
        ],
        "message": "HIVE-5765 - Beeline throws NPE when -e option is used (Szehon Ho via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1539694 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/934fad2564e7732fa4210176a3ca2d8dd7933bc8",
        "repo": "hive",
        "unit_tests": [
            "TestCommands.java"
        ]
    },
    "hive_cc61020": {
        "bug_id": "hive_cc61020",
        "commit": "https://github.com/apache/hive/commit/cc61020d7f4adff7905b8301b7908f3737f483a8",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/cc61020d7f4adff7905b8301b7908f3737f483a8/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java?ref=cc61020d7f4adff7905b8301b7908f3737f483a8",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java",
                "patch": "@@ -435,14 +435,16 @@ private void processPaths(JobConf job, CombineFileInputFormatShim combine,\n     List<InputSplitShim> retLists = new ArrayList<InputSplitShim>();\n     Map<String, ArrayList<InputSplitShim>> aliasToSplitList = new HashMap<String, ArrayList<InputSplitShim>>();\n     Map<String, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n+    Map<String, ArrayList<String>> pathToAliasesNoScheme = removeScheme(pathToAliases);\n \n     // Populate list of exclusive splits for every sampled alias\n     //\n     for (InputSplitShim split : splits) {\n       String alias = null;\n       for (Path path : split.getPaths()) {\n+        boolean schemeless = path.toUri().getScheme() == null;\n         List<String> l = HiveFileFormatUtils.doGetAliasesFromPath(\n-            pathToAliases, path);\n+            schemeless ? pathToAliasesNoScheme : pathToAliases, path);\n         // a path for a split unqualified the split from being sampled if:\n         // 1. it serves more than one alias\n         // 2. the alias it serves is not sampled\n@@ -500,6 +502,15 @@ private void processPaths(JobConf job, CombineFileInputFormatShim combine,\n     return retLists;\n   }\n \n+  Map<String, ArrayList<String>> removeScheme(Map<String, ArrayList<String>> pathToAliases) {\n+    Map<String, ArrayList<String>> result = new HashMap<String, ArrayList<String>>();\n+    for (Map.Entry <String, ArrayList<String>> entry : pathToAliases.entrySet()) {\n+      String newKey = new Path(entry.getKey()).toUri().getPath();\n+      result.put(newKey, entry.getValue());\n+    }\n+    return result;\n+  }\n+\n   /**\n    * Create a generic Hive RecordReader than can iterate over all chunks in a\n    * CombinedFileSplit.",
                "raw_url": "https://github.com/apache/hive/raw/cc61020d7f4adff7905b8301b7908f3737f483a8/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java",
                "sha": "6eb51dd41d31d9a0f04f8f4989eaea3080a21366",
                "status": "modified"
            }
        ],
        "message": "HIVE-2778 [jira] Fail on table sampling\n(Navis Ryu via Carl Steinbach)\n\nSummary:\nHIVE-2778 fix NPE on table sampling\n\nTrying table sampling on any non-empty table throws NPE. This does not occur by\ntest on mini-MR.  <div class=\"preformatted panel\" style=\"border-width:\n1px;\"><div class=\"preformattedContent panelContent\"> <pre>select count(*) from\nemp tablesample (0.1 percent);      Total MapReduce jobs = 1 Launching Job 1 out\nof 1 Number of reduce tasks determined at compile time: 1 In order to change the\naverage load for a reducer (in bytes):   set\nhive.exec.reducers.bytes.per.reducer=<number> In order to limit the maximum\nnumber of reducers:   set hive.exec.reducers.max=<number> In order to set a\nconstant number of reducers:   set mapred.reduce.tasks=<number>\njava.lang.NullPointerException \tat\norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(CombineHiveInputFormat.java:450)\n\tat\norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:403)\n\tat org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:971) \tat\norg.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:963) \tat\norg.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170) \tat\norg.apache.hadoop.mapred.JobClient$2.run(JobClient.java:880) \tat\norg.apache.hadoop.mapred.JobClient$2.run(JobClient.java:833) \tat\njava.security.AccessController.doPrivileged(Native Method) \tat\njavax.security.auth.Subject.doAs(Subject.java:396) \tat\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833) \tat\norg.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:807) \tat\norg.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:432) \tat\norg.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:136) \tat\norg.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134) \tat\norg.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57) \tat\norg.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332) \tat\norg.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123) \tat\norg.apache.hadoop.hive.ql.Driver.run(Driver.java:931) \tat\norg.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255) \tat\norg.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212) \tat\norg.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) \tat\norg.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671) \tat\norg.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554) \tat\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597) \tat\norg.apache.hadoop.util.RunJar.main(RunJar.java:186) Job Submission failed with\nexception 'java.lang.NullPointerException(null)' FAILED: Execution Error, return\ncode 1 from org.apache.hadoop.hive.ql.exec.MapRedTask  </pre> </div></div>\n\nTest Plan: EMPTY\n\nReviewers: JIRA, cwsteinbach\n\nReviewed By: cwsteinbach\n\nDifferential Revision: https://reviews.facebook.net/D1593\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1301310 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/c4f4bc8b032d7e04e89668731e7c7d3e3e3b9f03",
        "repo": "hive",
        "unit_tests": [
            "TestCombineHiveInputFormat.java"
        ]
    },
    "hive_cce0e37": {
        "bug_id": "hive_cce0e37",
        "commit": "https://github.com/apache/hive/commit/cce0e3777d178e68f38a0c9335d44a12fff42a6b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/cce0e3777d178e68f38a0c9335d44a12fff42a6b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java?ref=cce0e3777d178e68f38a0c9335d44a12fff42a6b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "patch": "@@ -3291,7 +3291,9 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,\n         try {\n           files = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);\n         } catch (IOException e) {\n-          pool.shutdownNow();\n+          if (null != pool) {\n+            pool.shutdownNow();\n+          }\n           throw new HiveException(e);\n         }\n       } else {",
                "raw_url": "https://github.com/apache/hive/raw/cce0e3777d178e68f38a0c9335d44a12fff42a6b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "sha": "466188130184516459dbd307e9cd0cf22844b46d",
                "status": "modified"
            }
        ],
        "message": "HIVE-19265 : Potential NPE and hiding actual exception in Hive#copyFiles (Igor Kryvenko via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/f0199500f00ae58cf1a9f73f5baebdc5d5eca417",
        "repo": "hive",
        "unit_tests": [
            "TestHive.java"
        ]
    },
    "hive_cf42684": {
        "bug_id": "hive_cf42684",
        "commit": "https://github.com/apache/hive/commit/cf4268487a5d65346b79994a2bfada70b20c428e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java",
                "patch": "@@ -122,6 +122,7 @@ public void testTokenStorage() throws Exception {\n \n     assertTrue(ts.removeToken(tokenId));\n     assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());\n+    assertNull(ts.getToken(tokenId));\n   }\n \n   public void testAclNoAuth() throws Exception {",
                "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java",
                "sha": "65a10e344e3fb24e5efd3b051bb1eaf26b0179a3",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e",
                "deletions": 1,
                "filename": "shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java",
                "patch": "@@ -330,7 +330,6 @@ public void run() {\n       } catch (Throwable t) {\n         LOGGER.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n             + t, t);\n-        Runtime.getRuntime().exit(-1);\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java",
                "sha": "87b418ebf29f8744a57bdb809a1d0fec7ed47ab5",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e",
                "deletions": 0,
                "filename": "shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java",
                "patch": "@@ -396,6 +396,10 @@ public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {\n   @Override\n   public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {\n     byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));\n+    if(tokenBytes == null) {\n+      // The token is already removed.\n+      return null;\n+    }\n     try {\n       return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);\n     } catch (Exception ex) {",
                "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java",
                "sha": "528e55d28cb7f56b0bdc4edb4f829acb6d6a61ca",
                "status": "modified"
            }
        ],
        "message": "HIVE-13090 : Hive metastore crashes on NPE with ZooKeeperTokenStore (Piotr Wikie\u0142, Thejas Nair, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/e732616392ddf5139f4d32bfb9fc51f352114887",
        "repo": "hive",
        "unit_tests": [
            "TestZooKeeperTokenStore.java"
        ]
    },
    "hive_e1ef225": {
        "bug_id": "hive_e1ef225",
        "commit": "https://github.com/apache/hive/commit/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java?ref=e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "patch": "@@ -207,11 +207,17 @@ public StructField getStructFieldRef(String s) {\n \n     @Override\n     public Object getStructFieldData(Object object, StructField field) {\n+      if (object == null) {\n+        return null;\n+      }\n       return ((OrcStruct) object).fields[((Field) field).offset];\n     }\n \n     @Override\n     public List<Object> getStructFieldsDataAsList(Object object) {\n+      if (object == null) {\n+        return null;\n+      }\n       OrcStruct struct = (OrcStruct) object;\n       List<Object> result = new ArrayList<Object>(struct.fields.length);\n       for (Object child: struct.fields) {",
                "raw_url": "https://github.com/apache/hive/raw/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "sha": "685b386606cec0e85eb3bf5fa0e7cf0e4fd6f88f",
                "status": "modified"
            }
        ],
        "message": "HIVE-6716: ORC struct throws NPE for tables with inner structs having null values (Prasanth J via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1581007 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/a39be7254a38c010154e7ebd65e8282db804ba80",
        "repo": "hive",
        "unit_tests": [
            "TestOrcStruct.java"
        ]
    },
    "hive_e91f69e": {
        "bug_id": "hive_e91f69e",
        "commit": "https://github.com/apache/hive/commit/e91f69e213f6ee78ad8299cda81079104f7141bb",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/e91f69e213f6ee78ad8299cda81079104f7141bb/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=e91f69e213f6ee78ad8299cda81079104f7141bb",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -387,7 +387,10 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n \n       path = getPlanPath(conf, name);\n       LOG.info(\"PLAN PATH = \" + path);\n-      assert path != null;\n+      if (path == null) { // Map/reduce plan may not be generated\n+        return null;\n+      }\n+\n       BaseWork gWork = gWorkMap.get(conf).get(path);\n       if (gWork == null) {\n         Path localPath = path;\n@@ -443,12 +446,11 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n       return gWork;\n     } catch (FileNotFoundException fnf) {\n       // happens. e.g.: no reduce work.\n-      LOG.debug(\"File not found: \" + fnf.getMessage());\n-      LOG.info(\"No plan file found: \"+path);\n+      LOG.debug(\"No plan file found: \" + path, fnf);\n       return null;\n     } catch (Exception e) {\n-      String msg = \"Failed to load plan: \" + path + \": \" + e;\n-      LOG.error(msg, e);\n+      String msg = \"Failed to load plan: \" + path;\n+      LOG.error(\"Failed to load plan: \" + path, e);\n       throw new RuntimeException(msg, e);\n     } finally {\n       SerializationUtilities.releaseKryo(kryo);",
                "raw_url": "https://github.com/apache/hive/raw/e91f69e213f6ee78ad8299cda81079104f7141bb/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "528d663e701eaa13e2cd7da328973637457ff642",
                "status": "modified"
            }
        ],
        "message": "HIVE-13855: select INPUT__FILE__NAME throws NPE exception (Aihua Xu, reviewed by Yongzhi Chen)",
        "parent": "https://github.com/apache/hive/commit/e459a67283900393a79e4f69853103cc4fd8a726",
        "repo": "hive",
        "unit_tests": [
            "TestUtilities.java"
        ]
    },
    "hive_eb2b7b8": {
        "bug_id": "hive_eb2b7b8",
        "commit": "https://github.com/apache/hive/commit/eb2b7b81f815238cc2b67d701d45aa7618fc8d13",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/eb2b7b81f815238cc2b67d701d45aa7618fc8d13/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java?ref=eb2b7b81f815238cc2b67d701d45aa7618fc8d13",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "patch": "@@ -1911,6 +1911,9 @@ private boolean isBigTableOnlyResults(MapJoinDesc desc) {\n \n   private boolean onExpressionHasNullSafes(MapJoinDesc desc) {\n     boolean[] nullSafes = desc.getNullSafes();\n+    if (nullSafes == null) {\n+\treturn false;\n+    }\n     for (boolean nullSafe : nullSafes) {\n       if (nullSafe) {\n         return true;",
                "raw_url": "https://github.com/apache/hive/raw/eb2b7b81f815238cc2b67d701d45aa7618fc8d13/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "sha": "ee080aad9c365aa73be4c518fc136e43158edf27",
                "status": "modified"
            }
        ],
        "message": "HIVE-12798 : CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver.vector* queries failures due to NPE in Vectorizer.onExpressionHasNullSafes() (Hari Subramaniyan, reviewed by Matt McCline )",
        "parent": "https://github.com/apache/hive/commit/c8f15f7b802fa0c2f2426a3b29093aba4aebc57f",
        "repo": "hive",
        "unit_tests": [
            "TestVectorizer.java"
        ]
    },
    "hive_ec0eb1b": {
        "bug_id": "hive_ec0eb1b",
        "commit": "https://github.com/apache/hive/commit/ec0eb1b65ff2457537c254d0691bb763677950d6",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ec0eb1b65ff2457537c254d0691bb763677950d6/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=ec0eb1b65ff2457537c254d0691bb763677950d6",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "patch": "@@ -902,7 +902,7 @@ protected void checkRetryable(Connection conn,\n     // so I've tried to capture the different error messages (there appear to be fewer different\n     // error messages than SQL states).\n     // Derby and newer MySQL driver use the new SQLTransactionRollbackException\n-    if (dbProduct == null) {\n+    if (dbProduct == null && conn != null) {\n       determineDatabaseProduct(conn);\n     }\n     if (e instanceof SQLTransactionRollbackException ||",
                "raw_url": "https://github.com/apache/hive/raw/ec0eb1b65ff2457537c254d0691bb763677950d6/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "sha": "ca6464ee52822f63e34f0e14b55ce100f96a6ff0",
                "status": "modified"
            }
        ],
        "message": "HIVE-9404 NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct() (Eugene Koifman, reviewed by Alan Gates)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1653336 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/66054149b8c3d3cefe7a1d15708f4de9b63901c2",
        "repo": "hive",
        "unit_tests": [
            "TestTxnHandler.java"
        ]
    },
    "hive_f3b2c70": {
        "bug_id": "hive_f3b2c70",
        "commit": "https://github.com/apache/hive/commit/f3b2c702238f5592a6743adbe5aee42bbec9f4e6",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/f3b2c702238f5592a6743adbe5aee42bbec9f4e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=f3b2c702238f5592a6743adbe5aee42bbec9f4e6",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -12125,9 +12125,6 @@ private AggregateCall convertAgg(AggInfo agg, RelNode input,\n       List<Integer> argList = new ArrayList<Integer>();\n       RelDataType type = TypeConverter.convert(agg.m_returnType,\n           this.m_cluster.getTypeFactory());\n-      if (aggregation.equals(SqlStdOperatorTable.AVG)) {\n-        type = type.getField(\"sum\", false).getType();\n-      }\n \n       // TODO: Does HQL allows expressions as aggregate args or can it only be\n       // projections from child?",
                "raw_url": "https://github.com/apache/hive/raw/f3b2c702238f5592a6743adbe5aee42bbec9f4e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "c7c76e520d8b4ba3edb5cf883b0f9c5b76dd7bd4",
                "status": "modified"
            }
        ],
        "message": "HIVE-7310: Turning CBO on results in NPE on some queries (Laljo John Pullokkaran via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1606276 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/e34ca1c02edb3b771c2976c75d57dc5212284637",
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_f5ad79b": {
        "bug_id": "hive_f5ad79b",
        "commit": "https://github.com/apache/hive/commit/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java?ref=f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java",
                "patch": "@@ -42,8 +42,8 @@\n   private byte[] bytesValue = null;\n   private String typeString;\n \n-  private transient Type type;\n-  private transient int bytesValueLength = 0;\n+  private Type type;\n+  private int bytesValueLength = 0;\n \n   public ConstantVectorExpression() {\n     super();",
                "raw_url": "https://github.com/apache/hive/raw/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java",
                "sha": "119b4b9d4d6cc0c0d5cf3961009f760dae3558a8",
                "status": "modified"
            }
        ],
        "message": "HIVE-5526 - NPE in ConstantVectorExpression.evaluate(vrg) (Remus Rusanu via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532044 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/7083f4f3cbeecdb9b2ed984b8aa47537c37fe7f6",
        "repo": "hive",
        "unit_tests": [
            "TestConstantVectorExpression.java"
        ]
    },
    "hive_f766b8f": {
        "bug_id": "hive_f766b8f",
        "commit": "https://github.com/apache/hive/commit/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c",
                "deletions": 3,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -4614,9 +4614,12 @@ private boolean drop_index_by_name_core(final RawStore ms,\n           deleteTableData(tblPath);\n           // ok even if the data is not deleted\n         }\n-        for (MetaStoreEventListener listener : listeners) {\n-          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n-          listener.onDropIndex(dropIndexEvent);\n+        // Skip the event listeners if the index is NULL\n+        if (index != null) {\n+          for (MetaStoreEventListener listener : listeners) {\n+            DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n+            listener.onDropIndex(dropIndexEvent);\n+          }\n         }\n       }\n       return success;",
                "raw_url": "https://github.com/apache/hive/raw/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "f8c3c4e48db0df9d6c18801bcd61f9e5dc6eb7c2",
                "status": "modified"
            }
        ],
        "message": "HIVE-15778: DROP INDEX (non-existent) throws NPE when using DbNotificationListener (Vamsee Yarlagadda, reviewed by Aihua Xu)",
        "parent": "https://github.com/apache/hive/commit/7cca0978af944b4a76dd40e014e628f82f43c42f",
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    }
}