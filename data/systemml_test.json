{
    "systemml_129710a": {
        "bug_id": "systemml_129710a",
        "commit": "https://github.com/apache/systemml/commit/129710a01458a9e6bae8806b8d982e580ebb225e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/129710a01458a9e6bae8806b8d982e580ebb225e/src/main/java/org/apache/sysml/runtime/transform/encode/EncoderRecode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/transform/encode/EncoderRecode.java?ref=129710a01458a9e6bae8806b8d982e580ebb225e",
                "deletions": 1,
                "filename": "src/main/java/org/apache/sysml/runtime/transform/encode/EncoderRecode.java",
                "patch": "@@ -59,6 +59,8 @@ public EncoderRecode(JSONObject parsedSpec, String[] colnames, int clen)\n \t}\n \t\n \tprivate long lookupRCDMap(int colID, String key) {\n+\t\tif( !_rcdMaps.containsKey(colID) )\n+\t\t\treturn -1; //empty recode map\n \t\tLong tmp = _rcdMaps.get(colID).get(key);\n \t\treturn (tmp!=null) ? tmp : -1;\n \t}\n@@ -130,7 +132,7 @@ public MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n \t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n \t\t\t\tObject okey = in.get(i, colID-1);\n \t\t\t\tString key = (okey!=null) ? okey.toString() : null;\n-\t\t\t\tlong code = lookupRCDMap(colID, key);\t\t\t\n+\t\t\t\tlong code = lookupRCDMap(colID, key);\n \t\t\t\tout.quickSetValue(i, colID-1,\n \t\t\t\t\t(code >= 0) ? code : Double.NaN);\n \t\t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/129710a01458a9e6bae8806b8d982e580ebb225e/src/main/java/org/apache/sysml/runtime/transform/encode/EncoderRecode.java",
                "sha": "2a7e405f7bacafd20eed4d8d278f6095c8cb427b",
                "status": "modified"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/systemml/blob/129710a01458a9e6bae8806b8d982e580ebb225e/src/test/java/org/apache/sysml/test/integration/functions/transform/TransformApplyEmptyRecodeMapTest.java",
                "changes": 68,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/test/java/org/apache/sysml/test/integration/functions/transform/TransformApplyEmptyRecodeMapTest.java?ref=129710a01458a9e6bae8806b8d982e580ebb225e",
                "deletions": 0,
                "filename": "src/test/java/org/apache/sysml/test/integration/functions/transform/TransformApplyEmptyRecodeMapTest.java",
                "patch": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysml.test.integration.functions.transform;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.apache.sysml.parser.Expression.ValueType;\n+import org.apache.sysml.runtime.DMLRuntimeException;\n+import org.apache.sysml.runtime.matrix.data.FrameBlock;\n+import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n+import org.apache.sysml.runtime.transform.encode.Encoder;\n+import org.apache.sysml.runtime.transform.encode.EncoderFactory;\n+import org.apache.sysml.runtime.util.DataConverter;\n+import org.apache.sysml.test.integration.AutomatedTestBase;\n+import org.apache.sysml.test.utils.TestUtils;\n+\n+public class TransformApplyEmptyRecodeMapTest extends AutomatedTestBase \n+{\n+\tprivate static final int rows = 7;\n+\tprivate static final int cols = 1;\n+\t\n+\t@Override\n+\tpublic void setUp()  {\n+\t\tTestUtils.clearAssertionInformation();\n+\t}\n+\t\n+\t@Test\n+\tpublic void testTransformApplyEmptyRecodeMap() {\n+\t\ttry {\n+\t\t\t//generate input data\n+\t\t\tFrameBlock data = DataConverter.convertToFrameBlock(\n+\t\t\t\tDataConverter.convertToMatrixBlock(getRandomMatrix(rows, cols, 1, 1, 1, 7)));\n+\t\t\tFrameBlock meta = new FrameBlock(new ValueType[]{ValueType.STRING}, new String[]{\"C1\"});\n+\t\t\t\n+\t\t\t//execute transform apply\n+\t\t\tEncoder encoder = EncoderFactory.createEncoder(\n+\t\t\t\t\"{ids:true, recode:[1]}\", data.getColumnNames(), meta.getSchema(), meta);\n+\t\t\tMatrixBlock out = encoder.apply(data, new MatrixBlock(rows, cols, true));\n+\t\t\t\n+\t\t\t//check outputs\n+\t\t\tAssert.assertEquals(rows, out.getNumRows());\n+\t\t\tAssert.assertEquals(cols, out.getNumColumns());\n+\t\t\tfor(int i=0; i<rows; i++)\n+\t\t\t\tfor(int j=0; j<cols; j++)\n+\t\t\t\t\tAssert.assertTrue(Double.isNaN(out.quickGetValue(i, j)));\n+\t\t} \n+\t\tcatch (DMLRuntimeException e) {\n+\t\t\tthrow new RuntimeException(e);\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/systemml/raw/129710a01458a9e6bae8806b8d982e580ebb225e/src/test/java/org/apache/sysml/test/integration/functions/transform/TransformApplyEmptyRecodeMapTest.java",
                "sha": "c694fe4885c0a9a32ba3e93178a89bfbf9115f10",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/129710a01458a9e6bae8806b8d982e580ebb225e/src/test_suites/java/org/apache/sysml/test/integration/functions/transform/ZPackageSuite.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/test_suites/java/org/apache/sysml/test/integration/functions/transform/ZPackageSuite.java?ref=129710a01458a9e6bae8806b8d982e580ebb225e",
                "deletions": 0,
                "filename": "src/test_suites/java/org/apache/sysml/test/integration/functions/transform/ZPackageSuite.java",
                "patch": "@@ -27,6 +27,7 @@\n @RunWith(Suite.class)\n @Suite.SuiteClasses({\n \tFrameCSVReadWriteTest.class,\n+\tTransformApplyEmptyRecodeMapTest.class,\n \tTransformCSVFrameEncodeDecodeTest.class,\n \tTransformCSVFrameEncodeReadTest.class,\n \tTransformEncodeDecodeTest.class,",
                "raw_url": "https://github.com/apache/systemml/raw/129710a01458a9e6bae8806b8d982e580ebb225e/src/test_suites/java/org/apache/sysml/test/integration/functions/transform/ZPackageSuite.java",
                "sha": "2571031328493c6cab210ce6c6c79b5476f31ca2",
                "status": "modified"
            }
        ],
        "message": "[SYSTEMML-1854] Fix NPE on transformapply w/ empty recode map",
        "parent": "https://github.com/apache/systemml/commit/8e06ff3cca487715ae47da25cd896f59a7fcd817",
        "patched_files": [
            "EncoderRecode.java",
            "ZPackageSuite.java"
        ],
        "repo": "systemml",
        "unit_tests": [
            "TransformApplyEmptyRecodeMapTest.java"
        ]
    },
    "systemml_912c655": {
        "bug_id": "systemml_912c655",
        "commit": "https://github.com/apache/systemml/commit/912c65506d626c8b0128ceb80744fde49efd4a1a",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/systemml/blob/912c65506d626c8b0128ceb80744fde49efd4a1a/src/main/java/org/apache/sysml/api/mlcontext/MLContext.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/api/mlcontext/MLContext.java?ref=912c65506d626c8b0128ceb80744fde49efd4a1a",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/api/mlcontext/MLContext.java",
                "patch": "@@ -55,7 +55,7 @@\n \t/**\n \t * Logger for MLContext\n \t */\n-\tpublic static Logger log = Logger.getLogger(MLContext.class);\n+\tprotected static Logger log = Logger.getLogger(MLContext.class);\n \n \t/**\n \t * SparkSession object.\n@@ -665,7 +665,9 @@ public void close() {\n \n \t\t// clear local status, but do not stop sc as it\n \t\t// may be used or stopped externally\n-\t\texecutionScript.clearAll();\n+\t\tif (executionScript != null) {\n+\t\t\texecutionScript.clearAll();\n+\t\t}\n \t\tresetConfig();\n \t\tspark = null;\n \t}\n@@ -693,7 +695,7 @@ public ProjectInfo info() {\n \t */\n \tpublic String version() {\n \t\tif (info() == null) {\n-\t\t\treturn \"Version not available\";\n+\t\t\treturn MLContextUtil.VERSION_NOT_AVAILABLE;\n \t\t}\n \t\treturn info().version();\n \t}\n@@ -705,7 +707,7 @@ public String version() {\n \t */\n \tpublic String buildTime() {\n \t\tif (info() == null) {\n-\t\t\treturn \"Build time not available\";\n+\t\t\treturn MLContextUtil.BUILD_TIME_NOT_AVAILABLE;\n \t\t}\n \t\treturn info().buildTime();\n \t}",
                "raw_url": "https://github.com/apache/systemml/raw/912c65506d626c8b0128ceb80744fde49efd4a1a/src/main/java/org/apache/sysml/api/mlcontext/MLContext.java",
                "sha": "35720a5932a913e7663d529f052ff5067a3778e7",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/912c65506d626c8b0128ceb80744fde49efd4a1a/src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java?ref=912c65506d626c8b0128ceb80744fde49efd4a1a",
                "deletions": 0,
                "filename": "src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java",
                "patch": "@@ -482,6 +482,9 @@ public static FrameObject dataFrameToFrameObject(String variableName, Dataset<Ro\n \t *            the matrix metadata, if available\n \t */\n \tpublic static void determineMatrixFormatIfNeeded(Dataset<Row> dataFrame, MatrixMetadata matrixMetadata) {\n+\t\tif (matrixMetadata == null) {\n+\t\t\treturn;\n+\t\t}\n \t\tMatrixFormat matrixFormat = matrixMetadata.getMatrixFormat();\n \t\tif (matrixFormat != null) {\n \t\t\treturn;",
                "raw_url": "https://github.com/apache/systemml/raw/912c65506d626c8b0128ceb80744fde49efd4a1a/src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java",
                "sha": "3f12aceb4f6af2b186d682c3153b64c50c34757a",
                "status": "modified"
            },
            {
                "additions": 108,
                "blob_url": "https://github.com/apache/systemml/blob/912c65506d626c8b0128ceb80744fde49efd4a1a/src/main/java/org/apache/sysml/api/mlcontext/MLContextUtil.java",
                "changes": 206,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/api/mlcontext/MLContextUtil.java?ref=912c65506d626c8b0128ceb80744fde49efd4a1a",
                "deletions": 98,
                "filename": "src/main/java/org/apache/sysml/api/mlcontext/MLContextUtil.java",
                "patch": "@@ -91,122 +91,32 @@\n  *\n  */\n public final class MLContextUtil {\n-\t\n+\n \t/**\n-\t * Get HOP DAG in dot format for a DML or PYDML Script.\n-\t *\n-\t * @param mlCtx\n-\t *            MLContext object.\n-\t * @param script\n-\t *            The DML or PYDML Script object to execute.\n-\t * @param lines\n-\t *            Only display the hops that have begin and end line number\n-\t *            equals to the given integers.\n-\t * @param performHOPRewrites\n-\t *            should perform static rewrites, perform\n-\t *            intra-/inter-procedural analysis to propagate size information\n-\t *            into functions and apply dynamic rewrites\n-\t * @param withSubgraph\n-\t *            If false, the dot graph will be created without subgraphs for\n-\t *            statement blocks.\n-\t * @return hop DAG in dot format\n-\t * @throws LanguageException\n-\t *             if error occurs\n-\t * @throws DMLRuntimeException\n-\t *             if error occurs\n-\t * @throws HopsException\n-\t *             if error occurs\n+\t * Version not available message.\n \t */\n-\tpublic static String getHopDAG(MLContext mlCtx, Script script, ArrayList<Integer> lines,\n-\t\t\tboolean performHOPRewrites, boolean withSubgraph) throws HopsException, DMLRuntimeException,\n-\t\t\tLanguageException {\n-\t\treturn getHopDAG(mlCtx, script, lines, null, performHOPRewrites, withSubgraph);\n-\t}\n+\tpublic static final String VERSION_NOT_AVAILABLE = \"Version not available\";\n \n \t/**\n-\t * Get HOP DAG in dot format for a DML or PYDML Script.\n-\t *\n-\t * @param mlCtx\n-\t *            MLContext object.\n-\t * @param script\n-\t *            The DML or PYDML Script object to execute.\n-\t * @param lines\n-\t *            Only display the hops that have begin and end line number\n-\t *            equals to the given integers.\n-\t * @param newConf\n-\t *            Spark Configuration.\n-\t * @param performHOPRewrites\n-\t *            should perform static rewrites, perform\n-\t *            intra-/inter-procedural analysis to propagate size information\n-\t *            into functions and apply dynamic rewrites\n-\t * @param withSubgraph\n-\t *            If false, the dot graph will be created without subgraphs for\n-\t *            statement blocks.\n-\t * @return hop DAG in dot format\n-\t * @throws LanguageException\n-\t *             if error occurs\n-\t * @throws DMLRuntimeException\n-\t *             if error occurs\n-\t * @throws HopsException\n-\t *             if error occurs\n+\t * Build time not available message.\n \t */\n-\tpublic static String getHopDAG(MLContext mlCtx, Script script, ArrayList<Integer> lines, SparkConf newConf,\n-\t\t\tboolean performHOPRewrites, boolean withSubgraph) throws HopsException, DMLRuntimeException,\n-\t\t\tLanguageException {\n-\t\tSparkConf oldConf = mlCtx.getSparkSession().sparkContext().getConf();\n-\t\tSparkExecutionContext.SparkClusterConfig systemmlConf = SparkExecutionContext.getSparkClusterConfig();\n-\t\tlong oldMaxMemory = InfrastructureAnalyzer.getLocalMaxMemory();\n-\t\ttry {\n-\t\t\tif (newConf != null) {\n-\t\t\t\tsystemmlConf.analyzeSparkConfiguation(newConf);\n-\t\t\t\tInfrastructureAnalyzer.setLocalMaxMemory(newConf.getSizeAsBytes(\"spark.driver.memory\"));\n-\t\t\t}\n-\t\t\tScriptExecutor scriptExecutor = new ScriptExecutor();\n-\t\t\tscriptExecutor.setExecutionType(mlCtx.getExecutionType());\n-\t\t\tscriptExecutor.setGPU(mlCtx.isGPU());\n-\t\t\tscriptExecutor.setForceGPU(mlCtx.isForceGPU());\n-\t\t\tscriptExecutor.setInit(mlCtx.isInitBeforeExecution());\n-\t\t\tif (mlCtx.isInitBeforeExecution()) {\n-\t\t\t\tmlCtx.setInitBeforeExecution(false);\n-\t\t\t}\n-\t\t\tscriptExecutor.setMaintainSymbolTable(mlCtx.isMaintainSymbolTable());\n-\n-\t\t\tLong time = new Long((new Date()).getTime());\n-\t\t\tif ((script.getName() == null) || (script.getName().equals(\"\"))) {\n-\t\t\t\tscript.setName(time.toString());\n-\t\t\t}\n-\t\t\t\n-\t\t\tmlCtx.setExecutionScript(script);\n-\t\t\tscriptExecutor.compile(script, performHOPRewrites);\n-\t\t\tExplain.reset();\n-\t\t\t// To deal with potential Py4J issues\n-\t\t\tlines = lines.size() == 1 && lines.get(0) == -1 ? new ArrayList<Integer>() : lines;\n-\t\t\treturn Explain.getHopDAG(scriptExecutor.dmlProgram, lines, withSubgraph);\n-\t\t} catch (RuntimeException e) {\n-\t\t\tthrow new MLContextException(\"Exception when compiling script\", e);\n-\t\t} finally {\n-\t\t\tif (newConf != null) {\n-\t\t\t\tsystemmlConf.analyzeSparkConfiguation(oldConf);\n-\t\t\t\tInfrastructureAnalyzer.setLocalMaxMemory(oldMaxMemory);\n-\t\t\t}\n-\t\t}\n-\t}\n+\tpublic static final String BUILD_TIME_NOT_AVAILABLE = \"Build time not available\";\n \n \t/**\n-\t * Basic data types supported by the MLContext API\n+\t * Basic data types supported by the MLContext API.\n \t */\n \t@SuppressWarnings(\"rawtypes\")\n \tpublic static final Class[] BASIC_DATA_TYPES = { Integer.class, Boolean.class, Double.class, String.class };\n \n \t/**\n-\t * Complex data types supported by the MLContext API\n+\t * Complex data types supported by the MLContext API.\n \t */\n \t@SuppressWarnings(\"rawtypes\")\n \tpublic static final Class[] COMPLEX_DATA_TYPES = { JavaRDD.class, RDD.class, Dataset.class, Matrix.class,\n \t\t\tFrame.class, (new double[][] {}).getClass(), MatrixBlock.class, URL.class };\n \n \t/**\n-\t * All data types supported by the MLContext API\n+\t * All data types supported by the MLContext API.\n \t */\n \t@SuppressWarnings(\"rawtypes\")\n \tpublic static final Class[] ALL_SUPPORTED_DATA_TYPES = (Class[]) ArrayUtils.addAll(BASIC_DATA_TYPES,\n@@ -1252,4 +1162,104 @@ private static void deleteRemoveVariableInstructions(ArrayList<Instruction> inst\n \t\t\t}\n \t\t}\n \t}\n+\n+\t/**\n+\t * Get HOP DAG in dot format for a DML or PYDML Script.\n+\t *\n+\t * @param mlCtx\n+\t *            MLContext object.\n+\t * @param script\n+\t *            The DML or PYDML Script object to execute.\n+\t * @param lines\n+\t *            Only display the hops that have begin and end line number\n+\t *            equals to the given integers.\n+\t * @param performHOPRewrites\n+\t *            should perform static rewrites, perform\n+\t *            intra-/inter-procedural analysis to propagate size information\n+\t *            into functions and apply dynamic rewrites\n+\t * @param withSubgraph\n+\t *            If false, the dot graph will be created without subgraphs for\n+\t *            statement blocks.\n+\t * @return hop DAG in dot format\n+\t * @throws LanguageException\n+\t *             if error occurs\n+\t * @throws DMLRuntimeException\n+\t *             if error occurs\n+\t * @throws HopsException\n+\t *             if error occurs\n+\t */\n+\tpublic static String getHopDAG(MLContext mlCtx, Script script, ArrayList<Integer> lines, boolean performHOPRewrites,\n+\t\t\tboolean withSubgraph) throws HopsException, DMLRuntimeException, LanguageException {\n+\t\treturn getHopDAG(mlCtx, script, lines, null, performHOPRewrites, withSubgraph);\n+\t}\n+\n+\t/**\n+\t * Get HOP DAG in dot format for a DML or PYDML Script.\n+\t *\n+\t * @param mlCtx\n+\t *            MLContext object.\n+\t * @param script\n+\t *            The DML or PYDML Script object to execute.\n+\t * @param lines\n+\t *            Only display the hops that have begin and end line number\n+\t *            equals to the given integers.\n+\t * @param newConf\n+\t *            Spark Configuration.\n+\t * @param performHOPRewrites\n+\t *            should perform static rewrites, perform\n+\t *            intra-/inter-procedural analysis to propagate size information\n+\t *            into functions and apply dynamic rewrites\n+\t * @param withSubgraph\n+\t *            If false, the dot graph will be created without subgraphs for\n+\t *            statement blocks.\n+\t * @return hop DAG in dot format\n+\t * @throws LanguageException\n+\t *             if error occurs\n+\t * @throws DMLRuntimeException\n+\t *             if error occurs\n+\t * @throws HopsException\n+\t *             if error occurs\n+\t */\n+\tpublic static String getHopDAG(MLContext mlCtx, Script script, ArrayList<Integer> lines, SparkConf newConf,\n+\t\t\tboolean performHOPRewrites, boolean withSubgraph)\n+\t\t\tthrows HopsException, DMLRuntimeException, LanguageException {\n+\t\tSparkConf oldConf = mlCtx.getSparkSession().sparkContext().getConf();\n+\t\tSparkExecutionContext.SparkClusterConfig systemmlConf = SparkExecutionContext.getSparkClusterConfig();\n+\t\tlong oldMaxMemory = InfrastructureAnalyzer.getLocalMaxMemory();\n+\t\ttry {\n+\t\t\tif (newConf != null) {\n+\t\t\t\tsystemmlConf.analyzeSparkConfiguation(newConf);\n+\t\t\t\tInfrastructureAnalyzer.setLocalMaxMemory(newConf.getSizeAsBytes(\"spark.driver.memory\"));\n+\t\t\t}\n+\t\t\tScriptExecutor scriptExecutor = new ScriptExecutor();\n+\t\t\tscriptExecutor.setExecutionType(mlCtx.getExecutionType());\n+\t\t\tscriptExecutor.setGPU(mlCtx.isGPU());\n+\t\t\tscriptExecutor.setForceGPU(mlCtx.isForceGPU());\n+\t\t\tscriptExecutor.setInit(mlCtx.isInitBeforeExecution());\n+\t\t\tif (mlCtx.isInitBeforeExecution()) {\n+\t\t\t\tmlCtx.setInitBeforeExecution(false);\n+\t\t\t}\n+\t\t\tscriptExecutor.setMaintainSymbolTable(mlCtx.isMaintainSymbolTable());\n+\n+\t\t\tLong time = new Long((new Date()).getTime());\n+\t\t\tif ((script.getName() == null) || (script.getName().equals(\"\"))) {\n+\t\t\t\tscript.setName(time.toString());\n+\t\t\t}\n+\n+\t\t\tmlCtx.setExecutionScript(script);\n+\t\t\tscriptExecutor.compile(script, performHOPRewrites);\n+\t\t\tExplain.reset();\n+\t\t\t// To deal with potential Py4J issues\n+\t\t\tlines = lines.size() == 1 && lines.get(0) == -1 ? new ArrayList<Integer>() : lines;\n+\t\t\treturn Explain.getHopDAG(scriptExecutor.dmlProgram, lines, withSubgraph);\n+\t\t} catch (RuntimeException e) {\n+\t\t\tthrow new MLContextException(\"Exception when compiling script\", e);\n+\t\t} finally {\n+\t\t\tif (newConf != null) {\n+\t\t\t\tsystemmlConf.analyzeSparkConfiguation(oldConf);\n+\t\t\t\tInfrastructureAnalyzer.setLocalMaxMemory(oldMaxMemory);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n }",
                "raw_url": "https://github.com/apache/systemml/raw/912c65506d626c8b0128ceb80744fde49efd4a1a/src/main/java/org/apache/sysml/api/mlcontext/MLContextUtil.java",
                "sha": "03184e37ba28471b5fa8c0aa1129666d96f25d78",
                "status": "modified"
            },
            {
                "additions": 314,
                "blob_url": "https://github.com/apache/systemml/blob/912c65506d626c8b0128ceb80744fde49efd4a1a/src/test/java/org/apache/sysml/test/integration/mlcontext/MLContextTest.java",
                "changes": 314,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/test/java/org/apache/sysml/test/integration/mlcontext/MLContextTest.java?ref=912c65506d626c8b0128ceb80744fde49efd4a1a",
                "deletions": 0,
                "filename": "src/test/java/org/apache/sysml/test/integration/mlcontext/MLContextTest.java",
                "patch": "@@ -42,10 +42,13 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n \n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.ml.linalg.DenseVector;\n import org.apache.spark.ml.linalg.Vector;\n import org.apache.spark.ml.linalg.VectorUDT;\n import org.apache.spark.ml.linalg.Vectors;\n@@ -54,10 +57,12 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.RowFactory;\n import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DoubleType;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n import org.apache.sysml.api.mlcontext.MLContextConversionUtil;\n import org.apache.sysml.api.mlcontext.MLContextException;\n+import org.apache.sysml.api.mlcontext.MLContextUtil;\n import org.apache.sysml.api.mlcontext.MLResults;\n import org.apache.sysml.api.mlcontext.Matrix;\n import org.apache.sysml.api.mlcontext.MatrixFormat;\n@@ -69,11 +74,14 @@\n import org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n+import org.apache.sysml.runtime.util.DataConverter;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import scala.Tuple1;\n import scala.Tuple2;\n import scala.Tuple3;\n+import scala.Tuple4;\n import scala.collection.Iterator;\n import scala.collection.JavaConversions;\n import scala.collection.Seq;\n@@ -2756,4 +2764,310 @@ public void testOutputScalaSeqPYDML() {\n \t\tAssert.assertEquals(3, results.getLong(\"y\"));\n \t}\n \n+\t@Test\n+\tpublic void testOutputDataFrameOfVectorsDML() {\n+\t\tSystem.out.println(\"MLContextTest - output DataFrame of vectors DML\");\n+\n+\t\tString s = \"m=matrix('1 2 3 4',rows=2,cols=2);\";\n+\t\tScript script = dml(s).out(\"m\");\n+\t\tMLResults results = ml.execute(script);\n+\t\tDataset<Row> df = results.getDataFrame(\"m\", true);\n+\t\tDataset<Row> sortedDF = df.sort(RDDConverterUtils.DF_ID_COLUMN);\n+\n+\t\t// verify column types\n+\t\tStructType schema = sortedDF.schema();\n+\t\tStructField[] fields = schema.fields();\n+\t\tStructField idColumn = fields[0];\n+\t\tStructField vectorColumn = fields[1];\n+\t\tAssert.assertTrue(idColumn.dataType() instanceof DoubleType);\n+\t\tAssert.assertTrue(vectorColumn.dataType() instanceof VectorUDT);\n+\n+\t\tList<Row> list = sortedDF.collectAsList();\n+\n+\t\tRow row1 = list.get(0);\n+\t\tAssert.assertEquals(1.0, row1.getDouble(0), 0.0);\n+\t\tVector v1 = (DenseVector) row1.get(1);\n+\t\tdouble[] arr1 = v1.toArray();\n+\t\tAssert.assertArrayEquals(new double[] { 1.0, 2.0 }, arr1, 0.0);\n+\n+\t\tRow row2 = list.get(1);\n+\t\tAssert.assertEquals(2.0, row2.getDouble(0), 0.0);\n+\t\tVector v2 = (DenseVector) row2.get(1);\n+\t\tdouble[] arr2 = v2.toArray();\n+\t\tAssert.assertArrayEquals(new double[] { 3.0, 4.0 }, arr2, 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputDoubleArrayFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output double array from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=2, cols=2);\";\n+\t\tdouble[][] matrix = ml.execute(dml(s).out(\"M\")).getMatrix(\"M\").to2DDoubleArray();\n+\t\tAssert.assertEquals(1.0, matrix[0][0], 0);\n+\t\tAssert.assertEquals(2.0, matrix[0][1], 0);\n+\t\tAssert.assertEquals(3.0, matrix[1][0], 0);\n+\t\tAssert.assertEquals(4.0, matrix[1][1], 0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputDataFrameFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output DataFrame from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=2, cols=2);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tDataset<Row> df = ml.execute(script).getMatrix(\"M\").toDF();\n+\t\tDataset<Row> sortedDF = df.sort(RDDConverterUtils.DF_ID_COLUMN);\n+\t\tList<Row> list = sortedDF.collectAsList();\n+\t\tRow row1 = list.get(0);\n+\t\tAssert.assertEquals(1.0, row1.getDouble(0), 0.0);\n+\t\tAssert.assertEquals(1.0, row1.getDouble(1), 0.0);\n+\t\tAssert.assertEquals(2.0, row1.getDouble(2), 0.0);\n+\n+\t\tRow row2 = list.get(1);\n+\t\tAssert.assertEquals(2.0, row2.getDouble(0), 0.0);\n+\t\tAssert.assertEquals(3.0, row2.getDouble(1), 0.0);\n+\t\tAssert.assertEquals(4.0, row2.getDouble(2), 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputDataFrameDoublesNoIDColumnFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output DataFrame of doubles with no ID column from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=1, cols=4);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tDataset<Row> df = ml.execute(script).getMatrix(\"M\").toDFDoubleNoIDColumn();\n+\t\tList<Row> list = df.collectAsList();\n+\n+\t\tRow row = list.get(0);\n+\t\tAssert.assertEquals(1.0, row.getDouble(0), 0.0);\n+\t\tAssert.assertEquals(2.0, row.getDouble(1), 0.0);\n+\t\tAssert.assertEquals(3.0, row.getDouble(2), 0.0);\n+\t\tAssert.assertEquals(4.0, row.getDouble(3), 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputDataFrameDoublesWithIDColumnFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output DataFrame of doubles with ID column from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=2, cols=2);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tDataset<Row> df = ml.execute(script).getMatrix(\"M\").toDFDoubleWithIDColumn();\n+\t\tDataset<Row> sortedDF = df.sort(RDDConverterUtils.DF_ID_COLUMN);\n+\t\tList<Row> list = sortedDF.collectAsList();\n+\n+\t\tRow row1 = list.get(0);\n+\t\tAssert.assertEquals(1.0, row1.getDouble(0), 0.0);\n+\t\tAssert.assertEquals(1.0, row1.getDouble(1), 0.0);\n+\t\tAssert.assertEquals(2.0, row1.getDouble(2), 0.0);\n+\n+\t\tRow row2 = list.get(1);\n+\t\tAssert.assertEquals(2.0, row2.getDouble(0), 0.0);\n+\t\tAssert.assertEquals(3.0, row2.getDouble(1), 0.0);\n+\t\tAssert.assertEquals(4.0, row2.getDouble(2), 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputDataFrameVectorsNoIDColumnFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output DataFrame of vectors with no ID column from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=1, cols=4);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tDataset<Row> df = ml.execute(script).getMatrix(\"M\").toDFVectorNoIDColumn();\n+\t\tList<Row> list = df.collectAsList();\n+\n+\t\tRow row = list.get(0);\n+\t\tAssert.assertArrayEquals(new double[] { 1.0, 2.0, 3.0, 4.0 }, ((Vector) row.get(0)).toArray(), 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputDataFrameVectorsWithIDColumnFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output DataFrame of vectors with ID column from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=1, cols=4);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tDataset<Row> df = ml.execute(script).getMatrix(\"M\").toDFVectorWithIDColumn();\n+\t\tList<Row> list = df.collectAsList();\n+\n+\t\tRow row = list.get(0);\n+\t\tAssert.assertEquals(1.0, row.getDouble(0), 0.0);\n+\t\tAssert.assertArrayEquals(new double[] { 1.0, 2.0, 3.0, 4.0 }, ((Vector) row.get(1)).toArray(), 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputJavaRDDStringCSVFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output Java RDD String CSV from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=1, cols=4);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tJavaRDD<String> javaRDDStringCSV = ml.execute(script).getMatrix(\"M\").toJavaRDDStringCSV();\n+\t\tList<String> lines = javaRDDStringCSV.collect();\n+\t\tAssert.assertEquals(\"1.0,2.0,3.0,4.0\", lines.get(0));\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputJavaRDDStringIJVFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output Java RDD String IJV from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=2, cols=2);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tMLResults results = ml.execute(script);\n+\t\tJavaRDD<String> javaRDDStringIJV = results.getJavaRDDStringIJV(\"M\");\n+\t\tList<String> lines = javaRDDStringIJV.sortBy(row -> row, true, 1).collect();\n+\t\tAssert.assertEquals(\"1 1 1.0\", lines.get(0));\n+\t\tAssert.assertEquals(\"1 2 2.0\", lines.get(1));\n+\t\tAssert.assertEquals(\"2 1 3.0\", lines.get(2));\n+\t\tAssert.assertEquals(\"2 2 4.0\", lines.get(3));\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputRDDStringCSVFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output RDD String CSV from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=1, cols=4);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tRDD<String> rddStringCSV = ml.execute(script).getMatrix(\"M\").toRDDStringCSV();\n+\t\tIterator<String> iterator = rddStringCSV.toLocalIterator();\n+\t\tAssert.assertEquals(\"1.0,2.0,3.0,4.0\", iterator.next());\n+\t}\n+\n+\t@Test\n+\tpublic void testOutputRDDStringIJVFromMatrixDML() {\n+\t\tSystem.out.println(\"MLContextTest - output RDD String IJV from matrix DML\");\n+\n+\t\tString s = \"M = matrix('1 2 3 4', rows=2, cols=2);\";\n+\t\tScript script = dml(s).out(\"M\");\n+\t\tRDD<String> rddStringIJV = ml.execute(script).getMatrix(\"M\").toRDDStringIJV();\n+\t\tString[] rows = (String[]) rddStringIJV.collect();\n+\t\tArrays.sort(rows);\n+\t\tAssert.assertEquals(\"1 1 1.0\", rows[0]);\n+\t\tAssert.assertEquals(\"1 2 2.0\", rows[1]);\n+\t\tAssert.assertEquals(\"2 1 3.0\", rows[2]);\n+\t\tAssert.assertEquals(\"2 2 4.0\", rows[3]);\n+\t}\n+\n+\t@Test\n+\tpublic void testMLContextVersionMessage() {\n+\t\tSystem.out.println(\"MLContextTest - version message\");\n+\n+\t\tString version = ml.version();\n+\t\t// not available until jar built\n+\t\tAssert.assertEquals(MLContextUtil.VERSION_NOT_AVAILABLE, version);\n+\t}\n+\n+\t@Test\n+\tpublic void testMLContextBuildTimeMessage() {\n+\t\tSystem.out.println(\"MLContextTest - build time message\");\n+\n+\t\tString buildTime = ml.buildTime();\n+\t\t// not available until jar built\n+\t\tAssert.assertEquals(MLContextUtil.BUILD_TIME_NOT_AVAILABLE, buildTime);\n+\t}\n+\n+\t@Test\n+\tpublic void testMLContextCreateAndClose() {\n+\t\t// MLContext created by the @BeforeClass method in MLContextTestBase\n+\t\t// MLContext closed by the @AfterClass method in MLContextTestBase\n+\t\tSystem.out.println(\"MLContextTest - create MLContext and close (without script execution)\");\n+\t}\n+\n+\t@Test\n+\tpublic void testDataFrameToBinaryBlocks() {\n+\t\tSystem.out.println(\"MLContextTest - DataFrame to binary blocks\");\n+\n+\t\tList<String> list = new ArrayList<String>();\n+\t\tlist.add(\"1,2,3\");\n+\t\tlist.add(\"4,5,6\");\n+\t\tlist.add(\"7,8,9\");\n+\t\tJavaRDD<String> javaRddString = sc.parallelize(list);\n+\n+\t\tJavaRDD<Row> javaRddRow = javaRddString.map(new CommaSeparatedValueStringToDoubleArrayRow());\n+\t\tList<StructField> fields = new ArrayList<StructField>();\n+\t\tfields.add(DataTypes.createStructField(\"C1\", DataTypes.DoubleType, true));\n+\t\tfields.add(DataTypes.createStructField(\"C2\", DataTypes.DoubleType, true));\n+\t\tfields.add(DataTypes.createStructField(\"C3\", DataTypes.DoubleType, true));\n+\t\tStructType schema = DataTypes.createStructType(fields);\n+\t\tDataset<Row> dataFrame = spark.createDataFrame(javaRddRow, schema);\n+\n+\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> binaryBlocks = MLContextConversionUtil\n+\t\t\t\t.dataFrameToMatrixBinaryBlocks(dataFrame);\n+\t\tTuple2<MatrixIndexes, MatrixBlock> first = binaryBlocks.first();\n+\t\tMatrixBlock mb = first._2();\n+\t\tdouble[][] matrix = DataConverter.convertToDoubleMatrix(mb);\n+\t\tAssert.assertArrayEquals(new double[] { 1.0, 2.0, 3.0 }, matrix[0], 0.0);\n+\t\tAssert.assertArrayEquals(new double[] { 4.0, 5.0, 6.0 }, matrix[1], 0.0);\n+\t\tAssert.assertArrayEquals(new double[] { 7.0, 8.0, 9.0 }, matrix[2], 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testGetTuple1DML() {\n+\t\tSystem.out.println(\"MLContextTest - Get Tuple1<Matrix> DML\");\n+\t\tJavaRDD<String> javaRddString = sc\n+\t\t\t\t.parallelize(Stream.of(\"1,2,3\", \"4,5,6\", \"7,8,9\").collect(Collectors.toList()));\n+\t\tJavaRDD<Row> javaRddRow = javaRddString.map(new CommaSeparatedValueStringToDoubleArrayRow());\n+\t\tList<StructField> fields = new ArrayList<StructField>();\n+\t\tfields.add(DataTypes.createStructField(\"C1\", DataTypes.DoubleType, true));\n+\t\tfields.add(DataTypes.createStructField(\"C2\", DataTypes.DoubleType, true));\n+\t\tfields.add(DataTypes.createStructField(\"C3\", DataTypes.DoubleType, true));\n+\t\tStructType schema = DataTypes.createStructType(fields);\n+\t\tDataset<Row> df = spark.createDataFrame(javaRddRow, schema);\n+\n+\t\tScript script = dml(\"N=M*2\").in(\"M\", df).out(\"N\");\n+\t\tTuple1<Matrix> tuple = ml.execute(script).getTuple(\"N\");\n+\t\tdouble[][] n = tuple._1().to2DDoubleArray();\n+\t\tAssert.assertEquals(2.0, n[0][0], 0);\n+\t\tAssert.assertEquals(4.0, n[0][1], 0);\n+\t\tAssert.assertEquals(6.0, n[0][2], 0);\n+\t\tAssert.assertEquals(8.0, n[1][0], 0);\n+\t\tAssert.assertEquals(10.0, n[1][1], 0);\n+\t\tAssert.assertEquals(12.0, n[1][2], 0);\n+\t\tAssert.assertEquals(14.0, n[2][0], 0);\n+\t\tAssert.assertEquals(16.0, n[2][1], 0);\n+\t\tAssert.assertEquals(18.0, n[2][2], 0);\n+\t}\n+\n+\t@Test\n+\tpublic void testGetTuple2DML() {\n+\t\tSystem.out.println(\"MLContextTest - Get Tuple2<Matrix,Double> DML\");\n+\n+\t\tdouble[][] m = new double[][] { { 1, 2 }, { 3, 4 } };\n+\n+\t\tScript script = dml(\"N=M*2;s=sum(N)\").in(\"M\", m).out(\"N\", \"s\");\n+\t\tTuple2<Matrix, Double> tuple = ml.execute(script).getTuple(\"N\", \"s\");\n+\t\tdouble[][] n = tuple._1().to2DDoubleArray();\n+\t\tdouble s = tuple._2();\n+\t\tAssert.assertArrayEquals(new double[] { 2, 4 }, n[0], 0.0);\n+\t\tAssert.assertArrayEquals(new double[] { 6, 8 }, n[1], 0.0);\n+\t\tAssert.assertEquals(20.0, s, 0.0);\n+\t}\n+\n+\t@Test\n+\tpublic void testGetTuple3DML() {\n+\t\tSystem.out.println(\"MLContextTest - Get Tuple3<Long,Double,Boolean> DML\");\n+\n+\t\tScript script = dml(\"a=1+2;b=a+0.5;c=TRUE;\").out(\"a\", \"b\", \"c\");\n+\t\tTuple3<Long, Double, Boolean> tuple = ml.execute(script).getTuple(\"a\", \"b\", \"c\");\n+\t\tlong a = tuple._1();\n+\t\tdouble b = tuple._2();\n+\t\tboolean c = tuple._3();\n+\t\tAssert.assertEquals(3, a);\n+\t\tAssert.assertEquals(3.5, b, 0.0);\n+\t\tAssert.assertEquals(true, c);\n+\t}\n+\n+\t@Test\n+\tpublic void testGetTuple4DML() {\n+\t\tSystem.out.println(\"MLContextTest - Get Tuple4<Long,Double,Boolean,String> DML\");\n+\n+\t\tScript script = dml(\"a=1+2;b=a+0.5;c=TRUE;d=\\\"yes it's \\\"+c\").out(\"a\", \"b\", \"c\", \"d\");\n+\t\tTuple4<Long, Double, Boolean, String> tuple = ml.execute(script).getTuple(\"a\", \"b\", \"c\", \"d\");\n+\t\tlong a = tuple._1();\n+\t\tdouble b = tuple._2();\n+\t\tboolean c = tuple._3();\n+\t\tString d = tuple._4();\n+\t\tAssert.assertEquals(3, a);\n+\t\tAssert.assertEquals(3.5, b, 0.0);\n+\t\tAssert.assertEquals(true, c);\n+\t\tAssert.assertEquals(\"yes it's TRUE\", d);\n+\t}\n+\n }",
                "raw_url": "https://github.com/apache/systemml/raw/912c65506d626c8b0128ceb80744fde49efd4a1a/src/test/java/org/apache/sysml/test/integration/mlcontext/MLContextTest.java",
                "sha": "9e4cface892fcb1ecd78db7f621c95eb4ee752d8",
                "status": "modified"
            }
        ],
        "message": "[MINOR] Increase MLContext test coverage\n\nCreate MLContext tests to test previously untested methods.\nUpdate MLContext and MLContextConversionUtil to avoid NPEs.\n\nCloses #649.",
        "parent": "https://github.com/apache/systemml/commit/8dbc93022a01aae309354c7b2b2f0eee9ec11aad",
        "patched_files": [
            "MLContextConversionUtil.java",
            "MLContext.java",
            "MLContextUtil.java"
        ],
        "repo": "systemml",
        "unit_tests": [
            "MLContextTest.java"
        ]
    },
    "systemml_be3e0c9": {
        "bug_id": "systemml_be3e0c9",
        "commit": "https://github.com/apache/systemml/commit/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0",
        "file": [
            {
                "additions": 183,
                "blob_url": "https://github.com/apache/systemml/blob/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/main/java/org/apache/sysml/parser/StatementBlock.java",
                "changes": 380,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/parser/StatementBlock.java?ref=be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0",
                "deletions": 197,
                "filename": "src/main/java/org/apache/sysml/parser/StatementBlock.java",
                "patch": "@@ -545,24 +545,19 @@ public VariableSet validate(DMLProgram dmlProg, VariableSet ids, HashMap<String,\n \t\tthrows LanguageException, ParseException, IOException\n \t{\n \t\t_constVarsIn.putAll(constVars);\n-\t\tHashMap<String, ConstIdentifier> currConstVars = new HashMap<String,ConstIdentifier>();\n-\t\tcurrConstVars.putAll(constVars);\n-\n \t\t_statements = rewriteFunctionCallStatements(dmlProg, _statements);\n \t\t_dmlProg = dmlProg;\n-\n-\t\tfor (Statement current : _statements){\n-\n-\t\t\tif (current instanceof OutputStatement){\n+\t\t\n+\t\tHashMap<String, ConstIdentifier> currConstVars = new HashMap<String,ConstIdentifier>(constVars);\n+\t\tfor (Statement current : _statements) {\n+\t\t\tif (current instanceof OutputStatement) {\n \t\t\t\tOutputStatement os = (OutputStatement)current;\n-\n \t\t\t\t// validate variable being written by output statement exists\n \t\t\t\tDataIdentifier target = (DataIdentifier)os.getIdentifier();\n \t\t\t\tif (ids.getVariable(target.getName()) == null) {\n \t\t\t\t\t//undefined variables are always treated unconditionally as error in order to prevent common script-level bugs\n \t\t\t\t\traiseValidateError(\"Undefined Variable (\" + target.getName() + \") used in statement\", false, LanguageErrorCodes.INVALID_PARAMETERS);\n \t\t\t\t}\n-\n \t\t\t\tif ( ids.getVariable(target.getName()).getDataType() == DataType.SCALAR) {\n \t\t\t\t\tboolean paramsOkay = true;\n \t\t\t\t\tfor (String key : os.getSource().getVarParams().keySet()){\n@@ -577,197 +572,18 @@ public VariableSet validate(DMLProgram dmlProg, VariableSet ids, HashMap<String,\n \t\t\t\tExpression source = os.getSource();\n \t\t\t\tsource.setOutput(target);\n \t\t\t\tsource.validateExpression(ids.getVariables(), currConstVars, conditional);\n-\n \t\t\t\tsetStatementFormatType(os, conditional);\n \t\t\t\ttarget.setDimensionValueProperties(ids.getVariable(target.getName()));\n \t\t\t}\n-\n \t\t\telse if (current instanceof AssignmentStatement){\n-\t\t\t\tAssignmentStatement as = (AssignmentStatement)current;\n-\t\t\t\tDataIdentifier target = as.getTarget();\n-\t\t\t \tExpression source = as.getSource();\n-\n-\t\t\t\tif (source instanceof FunctionCallIdentifier) {\n-\t\t\t\t\t((FunctionCallIdentifier) source).validateExpression(dmlProg, ids.getVariables(),currConstVars, conditional);\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\tif( MLContextProxy.isActive() )\n-\t\t\t\t\t\tMLContextProxy.setAppropriateVarsForRead(source, target._name);\n-\n-\t\t\t\t\tsource.validateExpression(ids.getVariables(), currConstVars, conditional);\n-\t\t\t\t}\n-\n-\t\t\t\tif (source instanceof DataExpression && ((DataExpression)source).getOpCode() == Expression.DataOp.READ)\n-\t\t\t\t\tsetStatementFormatType(as, conditional);\n-\n-\t\t\t\t// Handle const vars: (a) basic constant propagation, and (b) transitive constant propagation over assignments\n-\t\t\t\tif (target != null) {\n-\t\t\t\t\tcurrConstVars.remove(target.getName());\n-\t\t\t\t\tif(source instanceof ConstIdentifier && !(target instanceof IndexedIdentifier)){ //basic\n-\t\t\t\t\t\tcurrConstVars.put(target.getName(), (ConstIdentifier)source);\n-\t\t\t\t\t}\n-\t\t\t\t\tif( source instanceof DataIdentifier && !(target instanceof IndexedIdentifier) ){ //transitive\n-\t\t\t\t\t\tDataIdentifier diSource = (DataIdentifier) source;\n-\t\t\t\t\t\tif( currConstVars.containsKey(diSource.getName()) ){\n-\t\t\t\t\t\t\tcurrConstVars.put(target.getName(), currConstVars.get(diSource.getName()));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tif (source instanceof BuiltinFunctionExpression){\n-\t\t\t\t\tBuiltinFunctionExpression bife = (BuiltinFunctionExpression)source;\n-\t\t\t\t\tif (   bife.getOpCode() == Expression.BuiltinFunctionOp.NROW\n-\t\t\t\t\t\t|| bife.getOpCode() == Expression.BuiltinFunctionOp.NCOL )\n-\t\t\t\t\t{\n-\t\t\t\t\t\tDataIdentifier id = (DataIdentifier)bife.getFirstExpr();\n-\t\t\t\t\t\tDataIdentifier currVal = ids.getVariable(id.getName());\n-\t\t\t\t\t\tif (currVal == null){\n-\t\t\t\t\t\t\t//undefined variables are always treated unconditionally as error in order to prevent common script-level bugs\n-\t\t\t\t\t\t\tbife.raiseValidateError(\"Undefined Variable (\" + id.getName() + \") used in statement\", false, LanguageErrorCodes.INVALID_PARAMETERS);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tIntIdentifier intid = null;\n-\t\t\t\t\t\tif (bife.getOpCode() == Expression.BuiltinFunctionOp.NROW) {\n-\t\t\t\t\t\t\tintid = new IntIdentifier((currVal instanceof IndexedIdentifier)\n-\t\t\t\t\t\t\t\t\t? ((IndexedIdentifier) currVal).getOrigDim1() : currVal.getDim1(), bife);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tintid = new IntIdentifier((currVal instanceof IndexedIdentifier)\n-\t\t\t\t\t\t\t\t\t? ((IndexedIdentifier) currVal).getOrigDim2() : currVal.getDim2(), bife);\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// handle case when nrow / ncol called on variable with size unknown (dims == -1)\n-\t\t\t\t\t\t//\t--> const prop NOT possible\n-\t\t\t\t\t\tif (intid.getValue() != -1){\n-\t\t\t\t\t\t\tcurrConstVars.put(target.getName(), intid);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (target == null) {\n-\t\t\t\t\t// function has no return value\n-\t\t\t\t}\n-\t\t\t\t// CASE: target NOT indexed identifier\n-\t\t\t\telse if (!(target instanceof IndexedIdentifier)){\n-\t\t\t\t\ttarget.setProperties(source.getOutput());\n-\t\t\t\t\tif (source.getOutput() instanceof IndexedIdentifier){\n-\t\t\t\t\t\ttarget.setDimensions(source.getOutput().getDim1(), source.getOutput().getDim2());\n-\t\t\t\t\t}\n-\n-\t\t\t\t}\n-\t\t\t\t// CASE: target is indexed identifier\n-\t\t\t\telse\n-\t\t\t\t{\n-\t\t\t\t\t// process the \"target\" being indexed\n-\t\t\t\t\tDataIdentifier targetAsSeen = ids.getVariable(target.getName());\n-\t\t\t\t\tif (targetAsSeen == null){\n-\t\t\t\t\t\ttarget.raiseValidateError(\"cannot assign value to indexed identifier \" + target.toString() + \" without first initializing \" + target.getName(), conditional);\n-\t\t\t\t\t}\n-\t\t\t\t\ttarget.setProperties(targetAsSeen);\n-\n-\t\t\t\t\t// process the expressions for the indexing\n-\t\t\t\t\tif ( ((IndexedIdentifier)target).getRowLowerBound() != null  )\n-\t\t\t\t\t\t((IndexedIdentifier)target).getRowLowerBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n-\t\t\t\t\tif ( ((IndexedIdentifier)target).getRowUpperBound() != null  )\n-\t\t\t\t\t\t((IndexedIdentifier)target).getRowUpperBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n-\t\t\t\t\tif ( ((IndexedIdentifier)target).getColLowerBound() != null  )\n-\t\t\t\t\t\t((IndexedIdentifier)target).getColLowerBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n-\t\t\t\t\tif ( ((IndexedIdentifier)target).getColUpperBound() != null  )\n-\t\t\t\t\t\t((IndexedIdentifier)target).getColUpperBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n-\n-\t\t\t\t\t// validate that LHS indexed identifier is being assigned a matrix value\n-//\t\t\t\t\tif (source.getOutput().getDataType() != Expression.DataType.MATRIX){\n-//\t\t\t\t\t\tLOG.error(target.printErrorLocation() + \"Indexed expression \" + target.toString() + \" can only be assigned matrix value\");\n-//\t\t\t\t\t\tthrow new LanguageException(target.printErrorLocation() + \"Indexed expression \" + target.toString() + \" can only be assigned matrix value\");\n-//\t\t\t\t\t}\n-\n-\t\t\t\t\t// validate that size of LHS index ranges is being assigned:\n-\t\t\t\t\t//\t(a) a matrix value of same size as LHS\n-\t\t\t\t\t//\t(b) singleton value (semantics: initialize enitre submatrix with this value)\n-\t\t\t\t\tIndexPair targetSize = ((IndexedIdentifier)target).calculateIndexedDimensions(ids.getVariables(), currConstVars, conditional);\n-\n-\t\t\t\t\tif (targetSize._row >= 1 && source.getOutput().getDim1() > 1 && targetSize._row != source.getOutput().getDim1()){\n-\t\t\t\t\t\ttarget.raiseValidateError(\"Dimension mismatch. Indexed expression \" + target.toString() + \" can only be assigned matrix with dimensions \"\n-\t\t\t\t\t\t\t\t+ targetSize._row + \" rows and \" + targetSize._col + \" cols. Attempted to assign matrix with dimensions \"\n-\t\t\t\t\t\t\t\t+ source.getOutput().getDim1() + \" rows and \" + source.getOutput().getDim2() + \" cols \", conditional);\n-\t\t\t\t\t}\n-\n-\t\t\t\t\tif (targetSize._col >= 1 && source.getOutput().getDim2() > 1 && targetSize._col != source.getOutput().getDim2()){\n-\t\t\t\t\t\ttarget.raiseValidateError(\"Dimension mismatch. Indexed expression \" + target.toString() + \" can only be assigned matrix with dimensions \"\n-\t\t\t\t\t\t\t\t+ targetSize._row + \" rows and \" + targetSize._col + \" cols. Attempted to assign matrix with dimensions \"\n-\t\t\t\t\t\t\t\t+ source.getOutput().getDim1() + \" rows and \" + source.getOutput().getDim2() + \" cols \", conditional);\n-\t\t\t\t\t}\n-\n-\t\t\t\t\t((IndexedIdentifier)target).setDimensions(targetSize._row, targetSize._col);\n-\t\t\t\t}\n-\n-\t\t\t\tif (target != null) {\n-\t\t\t\t\tids.addVariable(target.getName(), target);\n-\t\t\t\t}\n-\n+\t\t\t\tvalidateAssignmentStatement(current, dmlProg, ids, currConstVars, conditional);\n \t\t\t}\n-\n \t\t\telse if (current instanceof MultiAssignmentStatement){\n-\t\t\t\tMultiAssignmentStatement mas = (MultiAssignmentStatement) current;\n-\t\t\t\tArrayList<DataIdentifier> targetList = mas.getTargetList();\n-\n-\t\t\t\t// perform validation of source expression\n-\t\t\t\tExpression source = mas.getSource();\n-\t\t\t\t/*\n-\t\t\t\t * MultiAssignmentStatments currently supports only External,\n-\t\t\t\t * User-defined, and Multi-return Builtin function expressions\n-\t\t\t\t */\n-\t\t\t\tif (!(source instanceof DataIdentifier)\n-\t\t\t\t\t\t|| (source instanceof DataIdentifier && !((DataIdentifier)source).multipleReturns()) ) {\n-\t\t\t\t//if (!(source instanceof FunctionCallIdentifier) ) {\n-\t\t\t\t\t\t//|| !(source instanceof BuiltinFunctionExpression && ((BuiltinFunctionExpression)source).isMultiReturnBuiltinFunction()) ){\n-\t\t\t\t\tsource.raiseValidateError(\"can only use user-defined functions with multi-assignment statement\", conditional);\n-\t\t\t\t}\n-\n-\t\t\t\tif ( source instanceof FunctionCallIdentifier) {\n-\t\t\t\t\tFunctionCallIdentifier fci = (FunctionCallIdentifier)source;\n-\t\t\t\t\tfci.validateExpression(dmlProg, ids.getVariables(), currConstVars, conditional);\n-\t\t\t\t}\n-\t\t\t\telse if ( (source instanceof BuiltinFunctionExpression || source instanceof ParameterizedBuiltinFunctionExpression)\n-\t\t\t\t\t\t&& ((DataIdentifier)source).multipleReturns()) {\n-\t\t\t\t\tsource.validateExpression(mas, ids.getVariables(), currConstVars, conditional);\n-\t\t\t\t}\n-\t\t\t\telse\n-\t\t\t\t\tthrow new LanguageException(\"Unexpected error.\");\n-\n-\n-\t\t\t\tif ( source instanceof FunctionCallIdentifier ) {\n-\t\t\t\t\tfor (int j =0; j< targetList.size(); j++){\n-\n-\t\t\t\t\t\tDataIdentifier target = targetList.get(j);\n-\t\t\t\t\t\t\t// set target properties (based on type info in function call statement return params)\n-\t\t\t\t\t\t\tFunctionCallIdentifier fci = (FunctionCallIdentifier)source;\n-\t\t\t\t\t\t\tFunctionStatement fstmt = (FunctionStatement)_dmlProg.getFunctionStatementBlock(fci.getNamespace(), fci.getName()).getStatement(0);\n-\t\t\t\t\t\t\tif (fstmt == null){\n-\t\t\t\t\t\t\t\tfci.raiseValidateError(\" function \" + fci.getName() + \" is undefined in namespace \" + fci.getNamespace(), conditional);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tif (!(target instanceof IndexedIdentifier)){\n-\t\t\t\t\t\t\t\ttarget.setProperties(fstmt.getOutputParams().get(j));\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse{\n-\t\t\t\t\t\t\t\tDataIdentifier targetAsSeen = ids.getVariable(target.getName());\n-\t\t\t\t\t\t\t\tif (targetAsSeen == null){\n-\t\t\t\t\t\t\t\t\traiseValidateError(target.printErrorLocation() + \"cannot assign value to indexed identifier \" + target.toString() + \" without first initializing \" + target.getName(), conditional);\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\ttarget.setProperties(targetAsSeen);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tids.addVariable(target.getName(), target);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\telse if ( source instanceof BuiltinFunctionExpression || source instanceof ParameterizedBuiltinFunctionExpression ) {\n-\t\t\t\t\tIdentifier[] outputs = source.getOutputs();\n-\t\t\t\t\tfor (int j=0; j < targetList.size(); j++) {\n-\t\t\t\t\t\tids.addVariable(targetList.get(j).getName(), (DataIdentifier)outputs[j]);\n-\t\t\t\t\t}\n-\t\t\t\t}\n+\t\t\t\tvalidateMultiAssignmentStatement(current, dmlProg, ids, currConstVars, conditional);\n \t\t\t}\n-\n \t\t\telse if(current instanceof ForStatement || current instanceof IfStatement || current instanceof WhileStatement ){\n \t\t\t\traiseValidateError(\"control statement (WhileStatement, IfStatement, ForStatement) should not be in generic statement block. Likely a parsing error\", conditional);\n \t\t\t}\n-\n \t\t\telse if (current instanceof PrintStatement) {\n \t\t\t\tPrintStatement pstmt = (PrintStatement) current;\n \t\t\t\tList<Expression> expressions = pstmt.getExpressions();\n@@ -782,22 +598,192 @@ else if (current instanceof PrintStatement) {\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n \t\t\t// no work to perform for PathStatement or ImportStatement\n \t\t\telse if (current instanceof PathStatement){}\n \t\t\telse if (current instanceof ImportStatement){}\n-\n-\n \t\t\telse {\n \t\t\t\traiseValidateError(\"cannot process statement of type \" + current.getClass().getSimpleName(), conditional);\n \t\t\t}\n-\n-\t\t} // end for (Statement current : _statements){\n+\t\t}\n \t\t_constVarsOut.putAll(currConstVars);\n \t\treturn ids;\n-\n \t}\n-\n+\t\n+\tprivate void validateAssignmentStatement(Statement current, DMLProgram dmlProg, \n+\t\tVariableSet ids, HashMap<String, ConstIdentifier> currConstVars, boolean conditional) \n+\t\t\tthrows LanguageException, IOException, ParseException \n+\t{\n+\t\tAssignmentStatement as = (AssignmentStatement)current;\n+\t\tDataIdentifier target = as.getTarget();\n+\t\tExpression source = as.getSource();\n+\t\t\n+\t\tif (source instanceof FunctionCallIdentifier) {\n+\t\t\t((FunctionCallIdentifier) source).validateExpression(\n+\t\t\t\tdmlProg, ids.getVariables(),currConstVars, conditional);\n+\t\t}\n+\t\telse { //all builtin functions and expressions\n+\t\t\tif( target == null  )\n+\t\t\t\traiseValidateError(\"Missing variable assignment.\", false);\n+\t\t\t\n+\t\t\tif( MLContextProxy.isActive() )\n+\t\t\t\tMLContextProxy.setAppropriateVarsForRead(source, target._name);\n+\t\t\t\n+\t\t\tsource.validateExpression(ids.getVariables(), currConstVars, conditional);\n+\t\t}\n+\t\t\n+\t\tif (source instanceof DataExpression && ((DataExpression)source).getOpCode() == Expression.DataOp.READ)\n+\t\t\tsetStatementFormatType(as, conditional);\n+\t\t\n+\t\t// Handle const vars: (a) basic constant propagation, and (b) transitive constant propagation over assignments\n+\t\tif (target != null) {\n+\t\t\tcurrConstVars.remove(target.getName());\n+\t\t\tif(source instanceof ConstIdentifier && !(target instanceof IndexedIdentifier)){ //basic\n+\t\t\t\tcurrConstVars.put(target.getName(), (ConstIdentifier)source);\n+\t\t\t}\n+\t\t\tif( source instanceof DataIdentifier && !(target instanceof IndexedIdentifier) ){ //transitive\n+\t\t\t\tDataIdentifier diSource = (DataIdentifier) source;\n+\t\t\t\tif( currConstVars.containsKey(diSource.getName()) ){\n+\t\t\t\t\tcurrConstVars.put(target.getName(), currConstVars.get(diSource.getName()));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\t\n+\t\tif (source instanceof BuiltinFunctionExpression){\n+\t\t\tBuiltinFunctionExpression bife = (BuiltinFunctionExpression)source;\n+\t\t\tif (   bife.getOpCode() == Expression.BuiltinFunctionOp.NROW\n+\t\t\t\t|| bife.getOpCode() == Expression.BuiltinFunctionOp.NCOL )\n+\t\t\t{\n+\t\t\t\tDataIdentifier id = (DataIdentifier)bife.getFirstExpr();\n+\t\t\t\tDataIdentifier currVal = ids.getVariable(id.getName());\n+\t\t\t\tif (currVal == null){\n+\t\t\t\t\t//undefined variables are always treated unconditionally as error in order to prevent common script-level bugs\n+\t\t\t\t\tbife.raiseValidateError(\"Undefined Variable (\" + id.getName() + \") used in statement\", false, LanguageErrorCodes.INVALID_PARAMETERS);\n+\t\t\t\t}\n+\t\t\t\tIntIdentifier intid = null;\n+\t\t\t\tif (bife.getOpCode() == Expression.BuiltinFunctionOp.NROW) {\n+\t\t\t\t\tintid = new IntIdentifier((currVal instanceof IndexedIdentifier)\n+\t\t\t\t\t\t\t? ((IndexedIdentifier) currVal).getOrigDim1() : currVal.getDim1(), bife);\n+\t\t\t\t} else {\n+\t\t\t\t\tintid = new IntIdentifier((currVal instanceof IndexedIdentifier)\n+\t\t\t\t\t\t\t? ((IndexedIdentifier) currVal).getOrigDim2() : currVal.getDim2(), bife);\n+\t\t\t\t}\n+\t\t\t\t\n+\t\t\t\t// handle case when nrow / ncol called on variable with size unknown (dims == -1)\n+\t\t\t\t//\t--> const prop NOT possible\n+\t\t\t\tif (intid.getValue() != -1)\n+\t\t\t\t\tcurrConstVars.put(target.getName(), intid);\n+\t\t\t}\n+\t\t}\n+\t\tif (target == null) {\n+\t\t\t// function has no return value\n+\t\t}\n+\t\t// CASE: target NOT indexed identifier\n+\t\telse if (!(target instanceof IndexedIdentifier)){\n+\t\t\ttarget.setProperties(source.getOutput());\n+\t\t\tif (source.getOutput() instanceof IndexedIdentifier)\n+\t\t\t\ttarget.setDimensions(source.getOutput().getDim1(), source.getOutput().getDim2());\n+\t\t}\n+\t\t// CASE: target is indexed identifier\n+\t\telse\n+\t\t{\n+\t\t\t// process the \"target\" being indexed\n+\t\t\tDataIdentifier targetAsSeen = ids.getVariable(target.getName());\n+\t\t\tif (targetAsSeen == null){\n+\t\t\t\ttarget.raiseValidateError(\"cannot assign value to indexed identifier \" + target.toString() + \" without first initializing \" + target.getName(), conditional);\n+\t\t\t}\n+\t\t\ttarget.setProperties(targetAsSeen);\n+\t\t\t\n+\t\t\t// process the expressions for the indexing\n+\t\t\tif ( ((IndexedIdentifier)target).getRowLowerBound() != null  )\n+\t\t\t\t((IndexedIdentifier)target).getRowLowerBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n+\t\t\tif ( ((IndexedIdentifier)target).getRowUpperBound() != null  )\n+\t\t\t\t((IndexedIdentifier)target).getRowUpperBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n+\t\t\tif ( ((IndexedIdentifier)target).getColLowerBound() != null  )\n+\t\t\t\t((IndexedIdentifier)target).getColLowerBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n+\t\t\tif ( ((IndexedIdentifier)target).getColUpperBound() != null  )\n+\t\t\t\t((IndexedIdentifier)target).getColUpperBound().validateExpression(ids.getVariables(), currConstVars, conditional);\n+\t\t\t\n+\t\t\t// validate that size of LHS index ranges is being assigned:\n+\t\t\t//\t(a) a matrix value of same size as LHS\n+\t\t\t//\t(b) singleton value (semantics: initialize enitre submatrix with this value)\n+\t\t\tIndexPair targetSize = ((IndexedIdentifier)target).calculateIndexedDimensions(ids.getVariables(), currConstVars, conditional);\n+\t\t\t\n+\t\t\tif (targetSize._row >= 1 && source.getOutput().getDim1() > 1 && targetSize._row != source.getOutput().getDim1()){\n+\t\t\t\ttarget.raiseValidateError(\"Dimension mismatch. Indexed expression \" + target.toString() + \" can only be assigned matrix with dimensions \"\n+\t\t\t\t\t\t+ targetSize._row + \" rows and \" + targetSize._col + \" cols. Attempted to assign matrix with dimensions \"\n+\t\t\t\t\t\t+ source.getOutput().getDim1() + \" rows and \" + source.getOutput().getDim2() + \" cols \", conditional);\n+\t\t\t}\n+\t\t\t\n+\t\t\tif (targetSize._col >= 1 && source.getOutput().getDim2() > 1 && targetSize._col != source.getOutput().getDim2()){\n+\t\t\t\ttarget.raiseValidateError(\"Dimension mismatch. Indexed expression \" + target.toString() + \" can only be assigned matrix with dimensions \"\n+\t\t\t\t\t\t+ targetSize._row + \" rows and \" + targetSize._col + \" cols. Attempted to assign matrix with dimensions \"\n+\t\t\t\t\t\t+ source.getOutput().getDim1() + \" rows and \" + source.getOutput().getDim2() + \" cols \", conditional);\n+\t\t\t}\n+\t\t\t((IndexedIdentifier)target).setDimensions(targetSize._row, targetSize._col);\n+\t\t}\n+\t\t\n+\t\tif (target != null)\n+\t\t\tids.addVariable(target.getName(), target);\n+\t}\n+\t\n+\tprivate void validateMultiAssignmentStatement(Statement current, DMLProgram dmlProg, \n+\t\tVariableSet ids, HashMap<String, ConstIdentifier> currConstVars, boolean conditional) \n+\t\t\tthrows LanguageException, IOException \n+\t{\n+\t\tMultiAssignmentStatement mas = (MultiAssignmentStatement) current;\n+\t\tArrayList<DataIdentifier> targetList = mas.getTargetList();\n+\t\tExpression source = mas.getSource();\n+\t\t\n+\t\t//MultiAssignmentStatments currently supports only External,\n+\t\t//User-defined, and Multi-return Builtin function expressions\n+\t\tif (!(source instanceof DataIdentifier)\n+\t\t\t\t|| (source instanceof DataIdentifier && !((DataIdentifier)source).multipleReturns()) ) {\n+\t\t\tsource.raiseValidateError(\"can only use user-defined functions with multi-assignment statement\", conditional);\n+\t\t}\n+\t\tif ( source instanceof FunctionCallIdentifier) {\n+\t\t\tFunctionCallIdentifier fci = (FunctionCallIdentifier)source;\n+\t\t\tfci.validateExpression(dmlProg, ids.getVariables(), currConstVars, conditional);\n+\t\t}\n+\t\telse if ( (source instanceof BuiltinFunctionExpression || source instanceof ParameterizedBuiltinFunctionExpression)\n+\t\t\t\t&& ((DataIdentifier)source).multipleReturns()) {\n+\t\t\tsource.validateExpression(mas, ids.getVariables(), currConstVars, conditional);\n+\t\t}\n+\t\telse\n+\t\t\tthrow new LanguageException(\"Unexpected error.\");\n+\t\t\n+\t\tif ( source instanceof FunctionCallIdentifier ) {\n+\t\t\tfor (int j =0; j< targetList.size(); j++) {\n+\t\t\t\tDataIdentifier target = targetList.get(j);\n+\t\t\t\t// set target properties (based on type info in function call statement return params)\n+\t\t\t\tFunctionCallIdentifier fci = (FunctionCallIdentifier)source;\n+\t\t\t\tFunctionStatement fstmt = (FunctionStatement)_dmlProg\n+\t\t\t\t\t.getFunctionStatementBlock(fci.getNamespace(), fci.getName()).getStatement(0);\n+\t\t\t\tif (fstmt == null){\n+\t\t\t\t\tfci.raiseValidateError(\" function \" + fci.getName() \n+\t\t\t\t\t\t+ \" is undefined in namespace \" + fci.getNamespace(), conditional);\n+\t\t\t\t}\n+\t\t\t\tif (!(target instanceof IndexedIdentifier)){\n+\t\t\t\t\ttarget.setProperties(fstmt.getOutputParams().get(j));\n+\t\t\t\t}\n+\t\t\t\telse{\n+\t\t\t\t\tDataIdentifier targetAsSeen = ids.getVariable(target.getName());\n+\t\t\t\t\tif (targetAsSeen == null){\n+\t\t\t\t\t\traiseValidateError(target.printErrorLocation() + \"cannot assign value to indexed identifier \" \n+\t\t\t\t\t\t\t+ target.toString() + \" without first initializing \" + target.getName(), conditional);\n+\t\t\t\t\t}\n+\t\t\t\t\ttarget.setProperties(targetAsSeen);\n+\t\t\t\t}\n+\t\t\t\tids.addVariable(target.getName(), target);\n+\t\t\t}\n+\t\t}\n+\t\telse if ( source instanceof BuiltinFunctionExpression || source instanceof ParameterizedBuiltinFunctionExpression ) {\n+\t\t\tIdentifier[] outputs = source.getOutputs();\n+\t\t\tfor (int j=0; j < targetList.size(); j++) {\n+\t\t\t\tids.addVariable(targetList.get(j).getName(), (DataIdentifier)outputs[j]);\n+\t\t\t}\n+\t\t}\n+\t}\n+\t\n \tpublic void setStatementFormatType(OutputStatement s, boolean conditionalValidate)\n \t\tthrows LanguageException, ParseException\n \t{",
                "raw_url": "https://github.com/apache/systemml/raw/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/main/java/org/apache/sysml/parser/StatementBlock.java",
                "sha": "d901119c121b6583a582d8f68a9b926a59a1d549",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/systemml/blob/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/test/java/org/apache/sysml/test/integration/functions/misc/InvalidBuiltinFunctionCallTest.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/test/java/org/apache/sysml/test/integration/functions/misc/InvalidBuiltinFunctionCallTest.java?ref=be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0",
                "deletions": 0,
                "filename": "src/test/java/org/apache/sysml/test/integration/functions/misc/InvalidBuiltinFunctionCallTest.java",
                "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysml.test.integration.functions.misc;\n+\n+import org.junit.Test;\n+\n+import org.apache.sysml.api.DMLException;\n+import org.apache.sysml.test.integration.AutomatedTestBase;\n+import org.apache.sysml.test.integration.TestConfiguration;\n+import org.apache.sysml.test.utils.TestUtils;\n+\n+public class InvalidBuiltinFunctionCallTest extends AutomatedTestBase\n+{\n+\tprivate final static String TEST_DIR = \"functions/misc/\";\n+\tprivate final static String TEST_NAME1 = \"InvalidBuiltinFunctionCallTest1\";\n+\tprivate final static String TEST_CLASS_DIR = TEST_DIR + InvalidBuiltinFunctionCallTest.class.getSimpleName() + \"/\";\n+\t\n+\t@Override\n+\tpublic void setUp() {\n+\t\tTestUtils.clearAssertionInformation();\n+\t\taddTestConfiguration(TEST_NAME1, new TestConfiguration(TEST_CLASS_DIR, TEST_NAME1, new String[] {}));\n+\t}\n+\t\n+\t@Test\n+\tpublic void testInvalidBuiltinFunctionCall1() { \n+\t\trunTest( TEST_NAME1, true ); \n+\t}\n+\t\n+\tprivate void runTest( String testName, boolean expected ) \n+\t{\n+\t\tTestConfiguration config = getTestConfiguration(testName);\n+\t\tloadTestConfiguration(config);\n+\t\t\n+\t\tString HOME = SCRIPT_DIR + TEST_DIR;\n+\t\tfullDMLScriptName = HOME + testName + \".dml\";\n+\t\tprogramArgs = new String[]{};\n+\t\t\n+\t\t//run tests\n+\t\trunTest(true, expected, DMLException.class, -1);\n+\t}\n+}",
                "raw_url": "https://github.com/apache/systemml/raw/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/test/java/org/apache/sysml/test/integration/functions/misc/InvalidBuiltinFunctionCallTest.java",
                "sha": "f755752c680fb1f92f8b5d90adf2f3bd63d02b64",
                "status": "added"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/systemml/blob/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/test/scripts/functions/misc/InvalidBuiltinFunctionCallTest1.dml",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/test/scripts/functions/misc/InvalidBuiltinFunctionCallTest1.dml?ref=be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0",
                "deletions": 0,
                "filename": "src/test/scripts/functions/misc/InvalidBuiltinFunctionCallTest1.dml",
                "patch": "@@ -0,0 +1,24 @@\n+#-------------------------------------------------------------\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+# \n+#   http://www.apache.org/licenses/LICENSE-2.0\n+# \n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+#-------------------------------------------------------------\n+\n+X = rand(rows=1006, cols=784, sparsity=0.001);\n+removeEmpty(target=X, margin=\"rows\");\n+print(\"nrow(X): \" + nrow(X));",
                "raw_url": "https://github.com/apache/systemml/raw/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/test/scripts/functions/misc/InvalidBuiltinFunctionCallTest1.dml",
                "sha": "5ddbbe06d9d426b66e9f0f6ffab6b8e0092816cf",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/test_suites/java/org/apache/sysml/test/integration/functions/misc/ZPackageSuite.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/test_suites/java/org/apache/sysml/test/integration/functions/misc/ZPackageSuite.java?ref=be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0",
                "deletions": 0,
                "filename": "src/test_suites/java/org/apache/sysml/test/integration/functions/misc/ZPackageSuite.java",
                "patch": "@@ -32,6 +32,7 @@\n \tFunctionInliningTest.class,\n \tFunctionNamespaceTest.class,\n \tIfTest.class,\n+\tInvalidBuiltinFunctionCallTest.class,\n \tInvalidFunctionAssignmentTest.class,\n \tInvalidFunctionSignatureTest.class,\n \tIPAConstantFoldingScalarVariablePropagationTest.class,",
                "raw_url": "https://github.com/apache/systemml/raw/be3e0c993d2aa59f80f7c39ec1fd1f17d7f730b0/src/test_suites/java/org/apache/sysml/test/integration/functions/misc/ZPackageSuite.java",
                "sha": "e3833f4f3cb17a29513baa28210df8b0b384864d",
                "status": "modified"
            }
        ],
        "message": "[SYSTEMML-1949] Fix robustness builtin functions w/ missing assignment\n\nThis patch fixes NPE issues on compiling scripts that call builtin\nfunctions but miss the assignment of the expression output to\nleft-hand-side variables. Furthermore, this also includes some minor\nrefactoring and additional tests.",
        "parent": "https://github.com/apache/systemml/commit/d9c09e77196516c17c80813c7ed60378a1515f51",
        "patched_files": [
            "InvalidBuiltinFunctionCallTest1.java",
            "StatementBlock.java",
            "ZPackageSuite.java"
        ],
        "repo": "systemml",
        "unit_tests": [
            "InvalidBuiltinFunctionCallTest.java"
        ]
    },
    "systemml_c3fdbb4": {
        "bug_id": "systemml_c3fdbb4",
        "commit": "https://github.com/apache/systemml/commit/c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java",
                "patch": "@@ -65,7 +65,6 @@\n import org.apache.sysml.runtime.matrix.data.Pair;\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.collection.JavaConversions;\n import scala.reflect.ClassTag;\n@@ -300,7 +299,7 @@ public static FrameObject binaryBlocksToFrameObject(JavaPairRDD<Long, FrameBlock\n \t\t\tframeMetadata.asMatrixCharacteristics() : new MatrixCharacteristics();\n \t\tValueType[] schema = (frameMetadata != null) ?\n \t\t\tframeMetadata.getFrameSchema().getSchema().toArray(new ValueType[0]) : \n-\t\t\tUtilFunctions.nCopies(IntUtils.toInt(mc.getCols()), ValueType.STRING);\n+\t\t\tUtilFunctions.nCopies((int)mc.getCols(), ValueType.STRING);\n \t\t\n \t\tFrameObject frameObject = new FrameObject(OptimizerUtils.getUniqueTempFileName(),\n \t\t\tnew MetaDataFormat(mc, OutputInfo.BinaryBlockOutputInfo, InputInfo.BinaryBlockInputInfo), schema);\n@@ -690,7 +689,7 @@ public static FrameObject javaRDDStringIJVToFrameObject(JavaRDD<String> javaRDD,\n \t\ttry {\n \t\t\tValueType[] lschema = null;\n \t\t\tif (lschema == null)\n-\t\t\t\tlschema = UtilFunctions.nCopies(IntUtils.toInt(mc.getCols()), ValueType.STRING);\n+\t\t\t\tlschema = UtilFunctions.nCopies((int) mc.getCols(), ValueType.STRING);\n \t\t\trdd = FrameRDDConverterUtils.textCellToBinaryBlock(jsc(), javaPairRDDText, mc, lschema);\n \t\t} catch (DMLRuntimeException e) {\n \t\t\te.printStackTrace();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/api/mlcontext/MLContextConversionUtil.java",
                "sha": "5d2478fe39df9237e9641b9866a8f6500131f110",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/DnnOp.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/hops/DnnOp.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 9,
                "filename": "src/main/java/org/apache/sysml/hops/DnnOp.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.sysml.runtime.instructions.gpu.context.GPUContextPool;\n import org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n import org.apache.sysml.runtime.matrix.data.DnnParameters;\n-import org.apache.sysml.utils.IntUtils;\n \n import java.util.ArrayList;\n \n@@ -380,18 +379,18 @@ protected double computeOutputMemEstimate( long dim1, long dim2, long nnz )\n \tprivate static class IntermediateDimensions {\n \t\tint dim1; int dim2; double sp;\n \t\tpublic IntermediateDimensions(DnnOp h, String dim1Str, String dim2Str, double sp) {\n-\t\t\tdim1 = IntUtils.toInt(h.getDim(dim1Str));\n-\t\t\tdim2 = IntUtils.toInt(h.getDim(dim2Str));\n+\t\t\tdim1 = (int) h.getDim(dim1Str);\n+\t\t\tdim2 = (int) h.getDim(dim2Str);\n \t\t\tthis.sp = sp;\n \t\t}\n \t\tpublic IntermediateDimensions(DnnOp h, String dim1Str, String dim2Str) {\n-\t\t\tdim1 = IntUtils.toInt(h.getDim(dim1Str));\n-\t\t\tdim2 = IntUtils.toInt(h.getDim(dim2Str));\n+\t\t\tdim1 = (int) h.getDim(dim1Str);\n+\t\t\tdim2 = (int) h.getDim(dim2Str);\n \t\t\tsp = 1;\n \t\t}\n \t\tpublic IntermediateDimensions(DnnOp h, int dim1, String dim2Str) {\n \t\t\tthis.dim1 = dim1;\n-\t\t\tdim2 = IntUtils.toInt(h.getDim(dim2Str));\n+\t\t\tdim2 = (int) h.getDim(dim2Str);\n \t\t\tsp = 1;\n \t\t}\n \t\t\n@@ -450,7 +449,7 @@ private double computeIntermediateMemEstimateHelper(\n \t\t\tArrayList<IntermediateDimensions> gpuIntermediates,\n \t\t\tArrayList<IntermediateDimensions> cpIntermediates) {\n \t\t// Since CP operators use row-level parallelism by default\n-\t\tint numWorkers = IntUtils.toInt(Math.min(OptimizerUtils.getConstrainedNumThreads(_maxNumThreads), Math.max(getDim(\"N\"), 1)));\n+\t\tint numWorkers = (int) Math.min(OptimizerUtils.getConstrainedNumThreads(_maxNumThreads), Math.max(getDim(\"N\"), 1));\n \t\tif(ConfigurationManager.isGPU()) {\n \t\t\t// Account for potential sparse-to-dense conversion\n \t\t\tdouble gpuMemBudget = IntermediateDimensions.addEstimateSizes(gpuIntermediates, 1);\n@@ -678,10 +677,10 @@ DnnParameters parseInput() {\n \t\t// P = as.integer(floor((H + 2*pad_h - R)/stride_h + 1))\n \t\t// Q = as.integer(floor((W + 2*pad_w - S)/stride_w + 1))\n \t\tif(_cachedParams.P < 0 && _cachedParams.H >= 0 && _cachedParams.R >= 0 && _cachedParams.stride_h >= 0 && _cachedParams.pad_h >= 0) {\n-\t\t\t_cachedParams.P = IntUtils.toInt(org.apache.sysml.runtime.util.DnnUtils.getP(_cachedParams.H, _cachedParams.R, _cachedParams.stride_h, _cachedParams.pad_h));\n+\t\t\t_cachedParams.P = (int) org.apache.sysml.runtime.util.DnnUtils.getP(_cachedParams.H, _cachedParams.R, _cachedParams.stride_h, _cachedParams.pad_h);\n \t\t}\n \t\tif(_cachedParams.Q < 0 && _cachedParams.W >= 0 && _cachedParams.S >= 0 && _cachedParams.stride_w >= 0 && _cachedParams.pad_w >= 0) {\n-\t\t\t_cachedParams.Q = IntUtils.toInt(org.apache.sysml.runtime.util.DnnUtils.getQ(_cachedParams.W, _cachedParams.S, _cachedParams.stride_w, _cachedParams.pad_w));\n+\t\t\t_cachedParams.Q = (int) org.apache.sysml.runtime.util.DnnUtils.getQ(_cachedParams.W, _cachedParams.S, _cachedParams.stride_w, _cachedParams.pad_w);\n \t\t}\n \t\t\n \t\treturn _cachedParams;",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/DnnOp.java",
                "sha": "a948eede459cd5f019d7a684fbae8b60427a127d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/IndexingOp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/hops/IndexingOp.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/hops/IndexingOp.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.sysml.parser.Expression.DataType;\n import org.apache.sysml.parser.Expression.ValueType;\n import org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n-import org.apache.sysml.utils.IntUtils;\n \n //for now only works for range based indexing op\n public class IndexingOp extends Hop \n@@ -348,8 +347,8 @@ private boolean isBlockAligned() {\n \t\tlong ru = (input3 instanceof LiteralOp) ? (HopRewriteUtils.getIntValueSafe((LiteralOp)input3)) : -1;\n \t\tlong cl = (input4 instanceof LiteralOp) ? (HopRewriteUtils.getIntValueSafe((LiteralOp)input4)) : -1;\n \t\tlong cu = (input5 instanceof LiteralOp) ? (HopRewriteUtils.getIntValueSafe((LiteralOp)input5)) : -1;\n-\t\tint brlen = input1.getRowsInBlock();\n-\t\tint bclen = input1.getColsInBlock();\n+\t\tint brlen = (int)input1.getRowsInBlock();\n+\t\tint bclen = (int)input1.getColsInBlock();\n \t\t\n \t\treturn OptimizerUtils.isIndexingRangeBlockAligned(rl, ru, cl, cu, brlen, bclen);\n \t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/IndexingOp.java",
                "sha": "1091027139174f2de3e412cb343f8306f85a4faa",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/OptimizerUtils.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/hops/OptimizerUtils.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 7,
                "filename": "src/main/java/org/apache/sysml/hops/OptimizerUtils.java",
                "patch": "@@ -57,7 +57,6 @@\n import org.apache.sysml.runtime.matrix.data.SparseBlock;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.yarn.ropt.YarnClusterAnalyzer;\n \n public class OptimizerUtils \n@@ -520,7 +519,7 @@ public static int getNumReducers( boolean configOnly )\n \t\t\t\n \t\t\t//correction max number of reducers on yarn clusters\n \t\t\tif( InfrastructureAnalyzer.isYarnEnabled() )\n-\t\t\t\tret = IntUtils.toInt(Math.max( ret, YarnClusterAnalyzer.getNumCores()/2 ));\n+\t\t\t\tret = (int)Math.max( ret, YarnClusterAnalyzer.getNumCores()/2 );\n \t\t}\n \t\t\n \t\treturn ret;\n@@ -535,7 +534,7 @@ public static int getNumMappers()\n \t\t\t\n \t\t//correction max number of reducers on yarn clusters\n \t\tif( InfrastructureAnalyzer.isYarnEnabled() )\n-\t\t\tret = IntUtils.toInt(Math.max( ret, YarnClusterAnalyzer.getNumCores() ));\n+\t\t\tret = (int)Math.max( ret, YarnClusterAnalyzer.getNumCores() );\n \t\t\n \t\treturn ret;\n \t}\n@@ -583,7 +582,7 @@ public static int getParallelTextReadParallelism()\n \t\t//compute degree of parallelism for parallel text read\n \t\tdouble dop = InfrastructureAnalyzer.getLocalParallelism()\n \t\t\t\t     * PARALLEL_CP_READ_PARALLELISM_MULTIPLIER;\n-\t\treturn IntUtils.toInt( Math.round(dop) );\n+\t\treturn (int) Math.round(dop);\n \t}\n \n \tpublic static int getParallelBinaryReadParallelism()\n@@ -594,7 +593,7 @@ public static int getParallelBinaryReadParallelism()\n \t\t//compute degree of parallelism for parallel text read\n \t\tdouble dop = InfrastructureAnalyzer.getLocalParallelism()\n \t\t\t\t     * PARALLEL_CP_READ_PARALLELISM_MULTIPLIER;\n-\t\treturn IntUtils.toInt( Math.round(dop) );\n+\t\treturn (int) Math.round(dop);\n \t}\n \t\n \t/**\n@@ -613,7 +612,7 @@ public static int getParallelTextWriteParallelism()\n \t\t//compute degree of parallelism for parallel text read\n \t\tdouble dop = InfrastructureAnalyzer.getLocalParallelism()\n \t\t\t\t     * PARALLEL_CP_WRITE_PARALLELISM_MULTIPLIER;\n-\t\treturn IntUtils.toInt( Math.round(dop) );\n+\t\treturn (int) Math.round(dop);\n \t}\n \n \tpublic static int getParallelBinaryWriteParallelism()\n@@ -624,7 +623,7 @@ public static int getParallelBinaryWriteParallelism()\n \t\t//compute degree of parallelism for parallel text read\n \t\tdouble dop = InfrastructureAnalyzer.getLocalParallelism()\n \t\t\t\t     * PARALLEL_CP_WRITE_PARALLELISM_MULTIPLIER;\n-\t\treturn IntUtils.toInt( Math.round(dop) );\n+\t\treturn (int) Math.round(dop);\n \t}\n \t\n \t////////////////////////",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/OptimizerUtils.java",
                "sha": "a43abb3266fc1c36f23a032da76ce6146bc4d24a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/codegen/cplan/CNodeNary.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/hops/codegen/cplan/CNodeNary.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/hops/codegen/cplan/CNodeNary.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.sysml.parser.Expression.DataType;\n import org.apache.sysml.runtime.util.DnnUtils;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CNodeNary extends CNode\n {\n@@ -231,8 +230,8 @@ private static String getDnnParameterString(List<CNode> inputs, boolean unary) {\n \t\tint K = Integer.parseInt(inputs.get(off+9).getVarname());\n \t\tint R = Integer.parseInt(inputs.get(off+11).getVarname());\n \t\tint S = Integer.parseInt(inputs.get(off+12).getVarname());\n-\t\tint P = IntUtils.toInt(DnnUtils.getP(H, R, 1, 0));\n-\t\tint Q = IntUtils.toInt(DnnUtils.getQ(W, S, 1, 0));\n+\t\tint P = (int) DnnUtils.getP(H, R, 1, 0);\n+\t\tint Q = (int) DnnUtils.getQ(W, S, 1, 0);\n \t\t\n \t\t//construct parameter string\n \t\treturn \"rix, \" + StringUtils.join(",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/codegen/cplan/CNodeNary.java",
                "sha": "1a717d3d0a8615f91a8ba54c93c9b9c176e7e393",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/recompile/LiteralReplacement.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/hops/recompile/LiteralReplacement.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 5,
                "filename": "src/main/java/org/apache/sysml/hops/recompile/LiteralReplacement.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.sysml.runtime.instructions.cp.ScalarObject;\n import org.apache.sysml.runtime.instructions.cp.ScalarObjectFactory;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public class LiteralReplacement \n@@ -248,7 +247,7 @@ private static LiteralOp replaceLiteralValueTypeCastRightIndexing( Hop c, LocalV\n \t\t\t\tif( mo.getNumRows()*mo.getNumColumns() < REPLACE_LITERALS_MAX_MATRIX_SIZE )\n \t\t\t\t{\n \t\t\t\t\tMatrixBlock mBlock = mo.acquireRead();\n-\t\t\t\t\tdouble value = mBlock.getValue(IntUtils.toInt(rlval-1),IntUtils.toInt(clval-1));\n+\t\t\t\t\tdouble value = mBlock.getValue((int)rlval-1,(int)clval-1);\n \t\t\t\t\tmo.release();\n \t\t\t\t\t\n \t\t\t\t\t//literal substitution (always double)\n@@ -322,7 +321,7 @@ private static LiteralOp replaceLiteralFullUnaryAggregateRightIndexing( Hop c, L\n \t\t\t\tif( mo.getNumRows()*mo.getNumColumns() < REPLACE_LITERALS_MAX_MATRIX_SIZE )\n \t\t\t\t{\n \t\t\t\t\tMatrixBlock mBlock = mo.acquireRead();\n-\t\t\t\t\tMatrixBlock mBlock2 = mBlock.slice(IntUtils.toInt(rlval-1), IntUtils.toInt(ruval-1), IntUtils.toInt(clval-1), IntUtils.toInt(cuval-1), new MatrixBlock());\n+\t\t\t\t\tMatrixBlock mBlock2 = mBlock.slice((int)(rlval-1), (int)(ruval-1), (int)(clval-1), (int)(cuval-1), new MatrixBlock());\n \t\t\t\t\tdouble value = replaceUnaryAggregate((AggUnaryOp)c, mBlock2);\n \t\t\t\t\tmo.release();\n \t\t\t\t\t\t\n@@ -370,7 +369,7 @@ private static DataOp replaceTReadMatrixLookupFromList( Hop c, LocalVariableMap\n \t\t\t\tString varname = Dag.getNextUniqueVarname(DataType.MATRIX);\n \t\t\t\tLiteralOp lit = (LiteralOp) ix.getInput().get(1);\n \t\t\t\tMatrixObject mo = (MatrixObject) (!lit.getValueType().isNumeric() ?\n-\t\t\t\t\tlist.slice(lit.getName()) : list.slice(IntUtils.toInt(lit.getLongValue()-1)));\n+\t\t\t\t\tlist.slice(lit.getName()) : list.slice((int)lit.getLongValue()-1));\n \t\t\t\tvars.put(varname, mo);\n \t\t\t\tret = HopRewriteUtils.createTransientRead(varname, c);\n \t\t\t}\n@@ -392,7 +391,7 @@ private static LiteralOp replaceTReadScalarLookupFromList( Hop c, LocalVariableM\n \t\t\t\tListObject list = (ListObject)vars.get(ixIn.getName());\n \t\t\t\tLiteralOp lit = (LiteralOp) ix.getInput().get(1);\n \t\t\t\tScalarObject so = (ScalarObject) (!lit.getValueType().isNumeric() ?\n-\t\t\t\t\tlist.slice(lit.getName()) : list.slice(IntUtils.toInt(lit.getLongValue()-1)));\n+\t\t\t\t\tlist.slice(lit.getName()) : list.slice((int)lit.getLongValue()-1));\n \t\t\t\treturn ScalarObjectFactory.createLiteralOp(so);\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/hops/recompile/LiteralReplacement.java",
                "sha": "c81848a4d7a96bec4e51836e8f05182d3ab2f68f",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/lops/compile/Dag.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/lops/compile/Dag.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 8,
                "filename": "src/main/java/org/apache/sysml/lops/compile/Dag.java",
                "patch": "@@ -83,7 +83,6 @@\n import org.apache.sysml.runtime.matrix.data.InputInfo;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.matrix.sort.PickFromCompactInputFormat;\n-import org.apache.sysml.utils.IntUtils;\n \n \n /**\n@@ -2339,8 +2338,8 @@ else if(!(node instanceof FunctionCallCP)) //general case\n \t\t\t\t// generate an instruction that creates a symbol table entry for the new variable\n \t\t\t\t//String createInst = prepareVariableInstruction(\"createvar\", node);\n \t\t\t\t//out.addPreInstruction(CPInstructionParser.parseSingleInstruction(createInst));\n-\t\t\t\tint rpb = IntUtils.toInt(oparams.getRowsInBlock());\n-\t\t\t\tint cpb = IntUtils.toInt(oparams.getColsInBlock());\n+\t\t\t\tint rpb = (int) oparams.getRowsInBlock();\n+\t\t\t\tint cpb = (int) oparams.getColsInBlock();\n \t\t\t\tInstruction createvarInst = VariableCPInstruction.prepareCreateVariableInstruction(\n \t\t\t\t\t\t\t\t\t        oparams.getLabel(),\n \t\t\t\t\t\t\t\t\t\t\toparams.getFile_name(), \n@@ -2378,7 +2377,7 @@ else if(!(node instanceof FunctionCallCP)) //general case\n \t\t\t\t\t\t\t\tgetFilePath() + fnOutParams.getLabel(), \n \t\t\t\t\t\t\t\ttrue, fnOut.getDataType(),\n \t\t\t\t\t\t\t\tOutputInfo.outputInfoToString(getOutputInfo(fnOut, false)),\n-\t\t\t\t\t\t\t\tnew MatrixCharacteristics(fnOutParams.getNumRows(), fnOutParams.getNumCols(), IntUtils.toInt(fnOutParams.getRowsInBlock()), IntUtils.toInt(fnOutParams.getColsInBlock()), fnOutParams.getNnz()),\n+\t\t\t\t\t\t\t\tnew MatrixCharacteristics(fnOutParams.getNumRows(), fnOutParams.getNumCols(), (int)fnOutParams.getRowsInBlock(), (int)fnOutParams.getColsInBlock(), fnOutParams.getNnz()),\n \t\t\t\t\t\t\t\toparams.getUpdateType()\n \t\t\t\t\t\t\t);\n \t\t\t\t\t\t\n@@ -2484,8 +2483,8 @@ else if ( !node.getInputs().isEmpty() )\n \t\t\t\t\t\tString tempVarName = oparams.getLabel() + \"temp\";\n \t\t\t\t\t\tString tempFileName = getNextUniqueFilename();\n \t\t\t\t\t\t\n-\t\t\t\t\t\tint rpb = IntUtils.toInt( oparams.getRowsInBlock() );\n-\t\t\t\t\t\tint cpb = IntUtils.toInt( oparams.getColsInBlock() );\n+\t\t\t\t\t\tint rpb = (int) oparams.getRowsInBlock();\n+\t\t\t\t\t\tint cpb = (int) oparams.getColsInBlock();\n \t\t\t\t\t\tInstruction createvarInst = VariableCPInstruction.prepareCreateVariableInstruction(\n \t\t\t\t\t\t\t\t\t\t\t\t\ttempVarName, \n \t\t\t\t\t\t\t\t\t\t\t\t\ttempFileName, \n@@ -2540,8 +2539,8 @@ else if ( !node.getInputs().isEmpty() )\n \t\t\t\t\t\t// create a variable to hold the result produced by this \"rootNode\"\n \t\t\t\t\t\toparams.setLabel(\"pVar\" + var_index.getNextID() );\n \t\t\t\t\t\t\n-\t\t\t\t\t\tint rpb = IntUtils.toInt( oparams.getRowsInBlock() );\n-\t\t\t\t\t\tint cpb = IntUtils.toInt( oparams.getColsInBlock() );\n+\t\t\t\t\t\tint rpb = (int) oparams.getRowsInBlock();\n+\t\t\t\t\t\tint cpb = (int) oparams.getColsInBlock();\n \t\t\t\t\t\tLop fnameLop = ((Data)node).getNamedInputLop(DataExpression.IO_FILENAME);\n \t\t\t\t\t\tString fnameStr = (fnameLop instanceof Data && ((Data)fnameLop).isLiteral()) ? \n \t\t\t\t\t\t\t\t           fnameLop.getOutputParameters().getLabel() ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/lops/compile/Dag.java",
                "sha": "47b497c902ce3a265638b2436af06848af9177a0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/lops/runtime/RunMRJobs.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/lops/runtime/RunMRJobs.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/lops/runtime/RunMRJobs.java",
                "patch": "@@ -67,7 +67,6 @@\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.matrix.data.RandomMatrixGenerator;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n \n@@ -493,7 +492,7 @@ private static JobReturn executeInMemoryDataGenOperations( MRJobInstruction inst\n \t\t\t\tRandInstruction lrand = (RandInstruction)ldgInst; \n \t\t\t\tRandomMatrixGenerator rgen = LibMatrixDatagen.createRandomMatrixGenerator(\n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlrand.getProbabilityDensityFunction(), \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIntUtils.toInt(lrand.getRows()), IntUtils.toInt(lrand.getCols()), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(int)lrand.getRows(), (int)lrand.getCols(), \n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlrand.getRowsInBlock(), lrand.getColsInBlock(), \n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlrand.getSparsity(), lrand.getMinValue(), lrand.getMaxValue(), \n \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlrand.getPdfParams());",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/lops/runtime/RunMRJobs.java",
                "sha": "e8145ac2ed9ef2655a32a1e9368503f629687cb1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/ParForProgramBlock.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/ParForProgramBlock.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 10,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/ParForProgramBlock.java",
                "patch": "@@ -111,7 +111,6 @@\n import org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n import org.apache.sysml.yarn.ropt.YarnClusterAnalyzer;\n \n@@ -889,7 +888,7 @@ private void executeRemoteMRParFor( ExecutionContext ec, IntObject itervar, IntO\n \t\tString resultFile = constructResultFileName();\n \t\t\n \t\tlong numIterations = partitioner.getNumIterations();\n-\t\tint maxDigits = IntUtils.toInt(Math.log10(to.getLongValue()) + 1);\n+\t\tint maxDigits = (int)Math.log10(to.getLongValue()) + 1;\n \t\tlong numCreatedTasks = -1;\n \t\tif( USE_STREAMING_TASK_CREATION ) {\n \t\t\tLocalTaskQueue<Task> queue = new LocalTaskQueue<>();\n@@ -1454,7 +1453,7 @@ private DataPartitioner createDataPartitioner(PartitionFormat dpf, PDataPartitio\n \t\tint maxNumRed = InfrastructureAnalyzer.getRemoteParallelReduceTasks();\n \t\t//correction max number of reducers on yarn clusters\n \t\tif( InfrastructureAnalyzer.isYarnEnabled() )\n-\t\t\tmaxNumRed = IntUtils.toInt(Math.max( maxNumRed, YarnClusterAnalyzer.getNumCores()/2 ));\n+\t\t\tmaxNumRed = (int)Math.max( maxNumRed, YarnClusterAnalyzer.getNumCores()/2 );\n \t\tint numRed = Math.min(numReducers,maxNumRed);\n \t\t\n \t\t//create data partitioner\n@@ -1485,7 +1484,7 @@ private ResultMerge createResultMerge( PResultMerge prm, MatrixObject out, Matri\n \t\t//determine degree of parallelism\n \t\tint maxMap = -1, maxRed = -1;\n \t\tif( OptimizerUtils.isSparkExecutionMode() ) {\n-\t\t\tmaxMap = IntUtils.toInt( SparkExecutionContext.getDefaultParallelism(true));\n+\t\t\tmaxMap = (int) SparkExecutionContext.getDefaultParallelism(true);\n \t\t\tmaxRed = maxMap; //equal map/reduce\n \t\t}\n \t\telse {\n@@ -1495,8 +1494,8 @@ private ResultMerge createResultMerge( PResultMerge prm, MatrixObject out, Matri\n \t\t\t\tInfrastructureAnalyzer.getRemoteParallelReduceTasks());\n \t\t\t//correction max number of reducers on yarn clusters\n \t\t\tif( InfrastructureAnalyzer.isYarnEnabled() ) {\n-\t\t\t\tmaxMap = IntUtils.toInt(Math.max( maxMap, YarnClusterAnalyzer.getNumCores() ));\n-\t\t\t\tmaxRed = IntUtils.toInt(Math.max( maxRed, YarnClusterAnalyzer.getNumCores()/2 ));\n+\t\t\t\tmaxMap = (int)Math.max( maxMap, YarnClusterAnalyzer.getNumCores() );\n+\t\t\t\tmaxRed = (int)Math.max( maxRed, YarnClusterAnalyzer.getNumCores()/2 );\n \t\t\t}\n \t\t}\n \t\tint numMap = Math.max(_numThreads, maxMap);\n@@ -1633,8 +1632,8 @@ private void consolidateAndCheckResults(ExecutionContext ec, long expIters, long\n \t\t\tint par = Math.min( _resultVars.size(), \n \t\t\t\t\t            InfrastructureAnalyzer.getLocalParallelism() );\n \t\t\tif( InfrastructureAnalyzer.isLocalMode() ) {\n-\t\t\t\tint parmem = IntUtils.toInt(Math.floor(OptimizerUtils.getLocalMemBudget() / \n-\t\t\t\t\t\tInfrastructureAnalyzer.getRemoteMaxMemorySortBuffer()));\n+\t\t\t\tint parmem = (int)Math.floor(OptimizerUtils.getLocalMemBudget() / \n+\t\t\t\t\t\tInfrastructureAnalyzer.getRemoteMaxMemorySortBuffer());\n \t\t\t\tpar = Math.min(par, Math.max(parmem, 1)); //reduce k if necessary\n \t\t\t}\n \t\t\t\n@@ -1750,7 +1749,7 @@ private void setParForProgramBlockIDs(int IDPrefix) {\n \t\tif( _IDPrefix == -1 ) //not specified\n \t\t\t_ID = _pfIDSeq.getNextID(); //generated new ID\n \t\telse //remote case (further nested parfors are all in one JVM)\n-\t\t\t_ID = IDHandler.concatIntIDsToLong(_IDPrefix, IntUtils.toInt(_pfIDSeq.getNextID()));\t\n+\t\t\t_ID = IDHandler.concatIntIDsToLong(_IDPrefix, (int)_pfIDSeq.getNextID());\t\n \t}\n \t\n \t/**\n@@ -1770,7 +1769,7 @@ private void setLocalParWorkerIDs()\n \t\t\tif(_IDPrefix == -1)\n \t\t\t\t_pwIDs[i] = _pwIDSeq.getNextID();\n \t\t\telse\n-\t\t\t\t_pwIDs[i] = IDHandler.concatIntIDsToLong(_IDPrefix,IntUtils.toInt(_pwIDSeq.getNextID()));\n+\t\t\t\t_pwIDs[i] = IDHandler.concatIntIDsToLong(_IDPrefix,(int)_pwIDSeq.getNextID());\n \t\t\t\n \t\t\tif( _monitor ) \n \t\t\t\tStatisticMonitor.putPfPwMapping(_ID, _pwIDs[i]);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/ParForProgramBlock.java",
                "sha": "a851a4da7682b65ccaf0c758bdd54f4d45676459",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/ByteBuffer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/caching/ByteBuffer.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/caching/ByteBuffer.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.matrix.data.FrameBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.util.LocalFileUtils;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Wrapper for WriteBuffer byte array per matrix/frame in order to\n@@ -62,9 +61,9 @@ public void serializeBlock( CacheBlock cb )\n \t\t\t{\n \t\t\t\t//deep serialize (for compression)\n \t\t\t\tif( CacheableData.CACHING_BUFFER_PAGECACHE )\n-\t\t\t\t\t_bdata = PageCache.getPage(IntUtils.toInt(_size));\n+\t\t\t\t\t_bdata = PageCache.getPage((int)_size);\n \t\t\t\tif( _bdata==null )\n-\t\t\t\t\t_bdata = new byte[IntUtils.toInt(_size)];\n+\t\t\t\t\t_bdata = new byte[(int)_size];\n \t\t\t\tDataOutput dout = new CacheDataOutput(_bdata);\n \t\t\t\tcb.write(dout);\n \t\t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/ByteBuffer.java",
                "sha": "a87e4b491a642629ac2e5df3d4b015c13d56e182",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/CacheStatistics.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/caching/CacheStatistics.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 7,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/caching/CacheStatistics.java",
                "patch": "@@ -86,7 +86,7 @@ public static void incrementMemHits() {\n \t\t_numHitsMem.increment();\n \t}\n \t\n-\tpublic static void incrementMemHits(long delta) {\n+\tpublic static void incrementMemHits(int delta) {\n \t\t_numHitsMem.add(delta);\n \t}\n \t\n@@ -98,7 +98,7 @@ public static void incrementFSBuffHits() {\n \t\t_numHitsFSBuff.increment();\n \t}\n \t\n-\tpublic static void incrementFSBuffHits( long delta ) {\n+\tpublic static void incrementFSBuffHits( int delta ) {\n \t\t_numHitsFSBuff.add(delta);\n \t}\n \t\n@@ -110,7 +110,7 @@ public static void incrementFSHits() {\n \t\t_numHitsFS.increment();\n \t}\n \t\n-\tpublic static void incrementFSHits(long delta) {\n+\tpublic static void incrementFSHits(int delta) {\n \t\t_numHitsFS.add(delta);\n \t}\n \t\n@@ -122,7 +122,7 @@ public static void incrementHDFSHits() {\n \t\t_numHitsHDFS.increment();\n \t}\n \t\n-\tpublic static void incrementHDFSHits(long delta) {\n+\tpublic static void incrementHDFSHits(int delta) {\n \t\t_numHitsHDFS.add(delta);\n \t}\n \t\n@@ -134,7 +134,7 @@ public static void incrementFSBuffWrites() {\n \t\t_numWritesFSBuff.increment();\n \t}\n \t\n-\tpublic static void incrementFSBuffWrites(long delta) {\n+\tpublic static void incrementFSBuffWrites(int delta) {\n \t\t_numWritesFSBuff.add(delta);\n \t}\n \t\n@@ -146,7 +146,7 @@ public static void incrementFSWrites() {\n \t\t_numWritesFS.increment();\n \t}\n \t\n-\tpublic static void incrementFSWrites(long delta) {\n+\tpublic static void incrementFSWrites(int delta) {\n \t\t_numWritesFS.add(delta);\n \t}\n \t\n@@ -158,7 +158,7 @@ public static void incrementHDFSWrites() {\n \t\t_numWritesHDFS.increment();\n \t}\n \t\n-\tpublic static void incrementHDFSWrites(long delta) {\n+\tpublic static void incrementHDFSWrites(int delta) {\n \t\t_numWritesHDFS.add(delta);\n \t}\n \t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/CacheStatistics.java",
                "sha": "c569787129e3cecc2c9b6279a696fd4eadfc1b7c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/FrameObject.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/caching/FrameObject.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 8,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/caching/FrameObject.java",
                "patch": "@@ -43,7 +43,6 @@\n import org.apache.sysml.runtime.matrix.data.InputInfo;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class FrameObject extends CacheableData<FrameBlock>\n {\n@@ -108,14 +107,14 @@ public FrameObject(FrameObject fo) {\n \t */\n \tpublic ValueType[] mergeSchemas(FrameObject fo) {\n \t\treturn (ValueType[]) ArrayUtils.addAll(\n-\t\t\t(_schema!=null) ? _schema : UtilFunctions.nCopies(IntUtils.toInt(getNumColumns()), ValueType.STRING), \n-\t\t\t(fo._schema!=null) ? fo._schema : UtilFunctions.nCopies(IntUtils.toInt(fo.getNumColumns()), ValueType.STRING));\n+\t\t\t(_schema!=null) ? _schema : UtilFunctions.nCopies((int)getNumColumns(), ValueType.STRING), \n+\t\t\t(fo._schema!=null) ? fo._schema : UtilFunctions.nCopies((int)fo.getNumColumns(), ValueType.STRING));\n \t} \n \t\n \tpublic void setSchema(String schema) {\n \t\tif( schema.equals(\"*\") ) {\n \t\t\t//populate default schema\n-\t\t\tint clen = IntUtils.toInt( getNumColumns() );\n+\t\t\tint clen = (int) getNumColumns();\n \t\t\tif( clen >= 0 ) //known number of cols\n \t\t\t\t_schema = UtilFunctions.nCopies(clen, ValueType.STRING);\n \t\t}\n@@ -170,7 +169,7 @@ protected FrameBlock readBlobFromHDFS(String fname, long rlen, long clen)\n \t\t\n \t\t//handle missing schema if necessary\n \t\tValueType[] lschema = (_schema!=null) ? _schema : \n-\t\t\tUtilFunctions.nCopies(clen>=1 ? IntUtils.toInt(clen) : 1, ValueType.STRING);\n+\t\t\tUtilFunctions.nCopies(clen>=1 ? (int)clen : 1, ValueType.STRING);\n \t\t\n \t\t//read the frame block\n \t\tFrameBlock data = null;\n@@ -202,12 +201,12 @@ protected FrameBlock readBlobFromRDD(RDDObject rdd, MutableBoolean status)\n \t\t\n \t\tMetaDataFormat iimd = (MetaDataFormat) _metaData;\n \t\tMatrixCharacteristics mc = iimd.getMatrixCharacteristics();\n-\t\tint rlen = IntUtils.toInt(mc.getRows());\n-\t\tint clen = IntUtils.toInt(mc.getCols());\n+\t\tint rlen = (int)mc.getRows();\n+\t\tint clen = (int)mc.getCols();\n \t\t\n \t\t//handle missing schema if necessary\n \t\tValueType[] lschema = (_schema!=null) ? _schema : \n-\t\t\tUtilFunctions.nCopies(clen>=1 ? IntUtils.toInt(clen) : 1, ValueType.STRING);\n+\t\t\tUtilFunctions.nCopies(clen>=1 ? (int)clen : 1, ValueType.STRING);\n \t\t\n \t\tFrameBlock fb = null;\n \t\ttry  {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/FrameObject.java",
                "sha": "b953906b89c72686429466940ddc57ca131bc6bb",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/MatrixObject.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/caching/MatrixObject.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 10,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/caching/MatrixObject.java",
                "patch": "@@ -43,7 +43,6 @@\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n \n \n /**\n@@ -314,7 +313,7 @@ public synchronized MatrixBlock readMatrixPartition( IndexRange pred ) {\n \t\t\t\t\tmb = readBlobFromHDFS( fname, rows, cols );\n \t\t\t\telse\n \t\t\t\t{\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(rows), IntUtils.toInt(cols), true);\n+\t\t\t\t\tmb = new MatrixBlock((int)rows, (int)cols, true);\n \t\t\t\t\tLOG.warn(\"Reading empty matrix partition \"+fname);\n \t\t\t\t}\n \t\t\t}\n@@ -328,13 +327,13 @@ public synchronized MatrixBlock readMatrixPartition( IndexRange pred ) {\n \t\t\t\t\n \t\t\t\tif( _partitionFormat == PDataPartitionFormat.ROW_BLOCK_WISE )\n \t\t\t\t{\n-\t\t\t\t\tint rix = IntUtils.toInt((pred.rowStart-1)%brlen);\n-\t\t\t\t\tmb = mb.slice(rix, rix, IntUtils.toInt(pred.colStart-1), IntUtils.toInt(pred.colEnd-1), new MatrixBlock());\n+\t\t\t\t\tint rix = (int)((pred.rowStart-1)%brlen);\n+\t\t\t\t\tmb = mb.slice(rix, rix, (int)(pred.colStart-1), (int)(pred.colEnd-1), new MatrixBlock());\n \t\t\t\t}\n \t\t\t\tif( _partitionFormat == PDataPartitionFormat.COLUMN_BLOCK_WISE )\n \t\t\t\t{\n-\t\t\t\t\tint cix = IntUtils.toInt((pred.colStart-1)%bclen);\n-\t\t\t\t\tmb = mb.slice(IntUtils.toInt(pred.rowStart-1), IntUtils.toInt(pred.rowEnd-1), cix, cix, new MatrixBlock());\n+\t\t\t\t\tint cix = (int)((pred.colStart-1)%bclen);\n+\t\t\t\t\tmb = mb.slice((int)(pred.rowStart-1), (int)(pred.rowEnd-1), cix, cix, new MatrixBlock());\n \t\t\t\t}\n \t\t\t}\n \t\t\t\n@@ -468,10 +467,10 @@ protected MatrixBlock readBlobFromRDD(RDDObject rdd, MutableBoolean writeStatus)\n \t\t\t}\n \t\t\t\n \t\t\t//obtain matrix block from RDD\n-\t\t\tint rlen = IntUtils.toInt(mc.getRows());\n-\t\t\tint clen = IntUtils.toInt(mc.getCols());\n-\t\t\tint brlen = mc.getRowsPerBlock();\n-\t\t\tint bclen = mc.getColsPerBlock();\n+\t\t\tint rlen = (int)mc.getRows();\n+\t\t\tint clen = (int)mc.getCols();\n+\t\t\tint brlen = (int)mc.getRowsPerBlock();\n+\t\t\tint bclen = (int)mc.getColsPerBlock();\n \t\t\tlong nnz = mc.getNonZerosBound();\n \t\t\t\n \t\t\t//guarded rdd collect ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/caching/MatrixObject.java",
                "sha": "4ab0f34c99962addcb1b2dc907098af37ea772a2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/context/ExecutionContext.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/context/ExecutionContext.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/context/ExecutionContext.java",
                "patch": "@@ -60,7 +60,6 @@\n import org.apache.sysml.runtime.matrix.data.Pair;\n import org.apache.sysml.runtime.util.MapReduceTool;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n \n@@ -289,7 +288,7 @@ public void setMetaData(String varName, long nrows, long ncols) {\n \t\tif( oldMetaData == null || !(oldMetaData instanceof MetaDataFormat) )\n \t\t\tthrow new DMLRuntimeException(\"Metadata not available\");\n \t\tMatrixCharacteristics mc = new MatrixCharacteristics(nrows, ncols,\n-\t\t\t\tIntUtils.toInt(mo.getNumRowsPerBlock()), IntUtils.toInt(mo.getNumColumnsPerBlock()));\n+\t\t\t(int) mo.getNumRowsPerBlock(), (int)mo.getNumColumnsPerBlock());\n \t\tmo.setMetaData(new MetaDataFormat(mc, \n \t\t\t((MetaDataFormat)oldMetaData).getOutputInfo(),\n \t\t\t((MetaDataFormat)oldMetaData).getInputInfo()));",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/context/ExecutionContext.java",
                "sha": "3e8636bc80bc2c9a67c86825605550137314c471",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/context/SparkExecutionContext.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/context/SparkExecutionContext.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 16,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/context/SparkExecutionContext.java",
                "patch": "@@ -84,7 +84,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.MapReduceTool;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.MLContextProxy;\n import org.apache.sysml.utils.Statistics;\n \n@@ -377,7 +376,7 @@ else if( mo.isDirty() || mo.isCached(false) )\n \t\t\t}\n \t\t\telse { //default case\n \t\t\t\tMatrixBlock mb = mo.acquireRead(); //pin matrix in memory\n-\t\t\t\trdd = toMatrixJavaPairRDD(sc, mb, IntUtils.toInt(mo.getNumRowsPerBlock()), IntUtils.toInt(mo.getNumColumnsPerBlock()), numParts, inclEmpty);\n+\t\t\t\trdd = toMatrixJavaPairRDD(sc, mb, (int)mo.getNumRowsPerBlock(), (int)mo.getNumColumnsPerBlock(), numParts, inclEmpty);\n \t\t\t\tmo.release(); //unpin matrix\n \t\t\t\t_parRDDs.registerRDD(rdd.id(), OptimizerUtils.estimatePartitionedSizeExactSparsity(mc), true);\n \t\t\t}\n@@ -574,8 +573,8 @@ else if(inputInfo2 == InputInfo.BinaryCellInputInfo) {\n \t\t\t\tCacheableData.addBroadcastSize(-mo.getBroadcastHandle().getPartitionedBroadcastSize());\n \n \t\t\t//obtain meta data for matrix\n-\t\t\tint brlen = IntUtils.toInt( mo.getNumRowsPerBlock() );\n-\t\t\tint bclen = IntUtils.toInt( mo.getNumColumnsPerBlock() );\n+\t\t\tint brlen = (int) mo.getNumRowsPerBlock();\n+\t\t\tint bclen = (int) mo.getNumColumnsPerBlock();\n \n \t\t\t//create partitioned matrix block and release memory consumed by input\n \t\t\tMatrixBlock mb = mo.acquireRead();\n@@ -584,7 +583,7 @@ else if(inputInfo2 == InputInfo.BinaryCellInputInfo) {\n \n \t\t\t//determine coarse-grained partitioning\n \t\t\tint numPerPart = PartitionedBroadcast.computeBlocksPerPartition(mo.getNumRows(), mo.getNumColumns(), brlen, bclen);\n-\t\t\tint numParts = IntUtils.toInt( Math.ceil((double) pmb.getNumRowBlocks() * pmb.getNumColumnBlocks() / numPerPart) );\n+\t\t\tint numParts = (int) Math.ceil((double) pmb.getNumRowBlocks() * pmb.getNumColumnBlocks() / numPerPart);\n \t\t\tBroadcast<PartitionedBlock<MatrixBlock>>[] ret = new Broadcast[numParts];\n \n \t\t\t//create coarse-grained partitioned broadcasts\n@@ -639,7 +638,7 @@ else if(inputInfo2 == InputInfo.BinaryCellInputInfo) {\n \t\t\t\tCacheableData.addBroadcastSize(-fo.getBroadcastHandle().getPartitionedBroadcastSize());\n \n \t\t\t//obtain meta data for frame\n-\t\t\tint bclen = IntUtils.toInt( fo.getNumColumns() );\n+\t\t\tint bclen = (int) fo.getNumColumns();\n \t\t\tint brlen = OptimizerUtils.getDefaultFrameSize();\n \n \t\t\t//create partitioned frame block and release memory consumed by input\n@@ -649,7 +648,7 @@ else if(inputInfo2 == InputInfo.BinaryCellInputInfo) {\n \n \t\t\t//determine coarse-grained partitioning\n \t\t\tint numPerPart = PartitionedBroadcast.computeBlocksPerPartition(fo.getNumRows(), fo.getNumColumns(), brlen, bclen);\n-\t\t\tint numParts = IntUtils.toInt( Math.ceil((double) pmb.getNumRowBlocks() * pmb.getNumColumnBlocks() / numPerPart) );\n+\t\t\tint numParts = (int) Math.ceil((double) pmb.getNumRowBlocks() * pmb.getNumColumnBlocks() / numPerPart);\n \t\t\tBroadcast<PartitionedBlock<FrameBlock>>[] ret = new Broadcast[numParts];\n \n \t\t\t//create coarse-grained partitioned broadcasts\n@@ -745,8 +744,8 @@ public void setRDDHandleForVariable(String varname, JavaPairRDD<?,?> rdd) {\n \t\t\tint maxCol = UtilFunctions.computeBlockSize(mc.getCols(), blockCol+1, mc.getColsPerBlock());\n \t\t\t//copy sub-matrix to block\n \t\t\tMatrixBlock block = new MatrixBlock(maxRow, maxCol, mb.isInSparseFormat());\n-\t\t\tint row_offset = IntUtils.toInt(blockRow*mc.getRowsPerBlock());\n-\t\t\tint col_offset = IntUtils.toInt(blockCol*mc.getColsPerBlock());\n+\t\t\tint row_offset = (int)blockRow*mc.getRowsPerBlock();\n+\t\t\tint col_offset = (int)blockCol*mc.getColsPerBlock();\n \t\t\tblock = mb.slice( row_offset, row_offset+maxRow-1,\n \t\t\t\tcol_offset, col_offset+maxCol-1, block );\n \t\t\t//create key-value pair\n@@ -763,7 +762,7 @@ public void setRDDHandleForVariable(String varname, JavaPairRDD<?,?> rdd) {\n \n \t\t//create and write subblocks of matrix\n \t\tint blksize = ConfigurationManager.getBlocksize();\n-\t\tfor(int blockRow = 0; blockRow < IntUtils.toInt(Math.ceil(src.getNumRows()/(double)blksize)); blockRow++)\n+\t\tfor(int blockRow = 0; blockRow < (int)Math.ceil(src.getNumRows()/(double)blksize); blockRow++)\n \t\t{\n \t\t\tint maxRow = (blockRow*blksize + blksize < src.getNumRows()) ? blksize : src.getNumRows() - blockRow*blksize;\n \t\t\tint roffset = blockRow*blksize;\n@@ -864,8 +863,8 @@ else if( list.size()==1 )\n \t\t\t\tMatrixBlock block = keyval._2();\n \t\t\t\t\n \t\t\t\t//compute row/column block offsets\n-\t\t\t\tint row_offset = IntUtils.toInt(ix.getRowIndex()-1)*brlen;\n-\t\t\t\tint col_offset = IntUtils.toInt(ix.getColumnIndex()-1)*bclen;\n+\t\t\t\tint row_offset = (int)(ix.getRowIndex()-1)*brlen;\n+\t\t\t\tint col_offset = (int)(ix.getColumnIndex()-1)*bclen;\n \t\t\t\tint rows = block.getNumRows();\n \t\t\t\tint cols = block.getNumColumns();\n \t\t\t\t\n@@ -945,7 +944,7 @@ public static MatrixBlock toMatrixBlock(JavaPairRDD<MatrixIndexes, MatrixCell> r\n \n \t\t\t//append cell to dense/sparse target in order to avoid shifting for sparse\n \t\t\t//note: this append requires a final sort of sparse rows\n-\t\t\tout.appendValue(IntUtils.toInt(ix.getRowIndex()-1), IntUtils.toInt(ix.getColumnIndex()-1), cell.getValue());\n+\t\t\tout.appendValue((int)ix.getRowIndex()-1, (int)ix.getColumnIndex()-1, cell.getValue());\n \t\t}\n \n \t\t//post-processing output matrix\n@@ -976,7 +975,7 @@ public static MatrixBlock toMatrixBlock(JavaPairRDD<MatrixIndexes, MatrixCell> r\n \t\t\t//unpack index-block pair\n \t\t\tMatrixIndexes ix = keyval._1();\n \t\t\tMatrixBlock block = keyval._2();\n-\t\t\tout.setBlock(IntUtils.toInt(ix.getRowIndex()), IntUtils.toInt(ix.getColumnIndex()), block);\n+\t\t\tout.setBlock((int)ix.getRowIndex(), (int)ix.getColumnIndex(), block);\n \t\t}\n \n \t\tif (ConfigurationManager.isStatistics()) {\n@@ -1009,7 +1008,7 @@ public static FrameBlock toFrameBlock(JavaPairRDD<Long,FrameBlock> rdd, ValueTyp\n \t\tfor( Tuple2<Long,FrameBlock> keyval : list )\n \t\t{\n \t\t\t//unpack index-block pair\n-\t\t\tint ix = IntUtils.toInt(keyval._1() - 1);\n+\t\t\tint ix = (int)(keyval._1() - 1);\n \t\t\tFrameBlock block = keyval._2();\n \n \t\t\t//copy into output frame\n@@ -1315,7 +1314,7 @@ private static synchronized int allocSchedulerPoolName() {\n \t\tif( pool < 0 ) {\n \t\t\tpool = _poolBuff.length;\n \t\t\t_poolBuff = Arrays.copyOf(_poolBuff,\n-\t\t\t\t\tIntUtils.toInt(Math.min(2L*pool, Integer.MAX_VALUE)));\n+\t\t\t\t\t(int)Math.min(2L*pool, Integer.MAX_VALUE));\n \t\t}\n \t\t//mark pool name for in use\n \t\t_poolBuff[pool] = true;",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/context/SparkExecutionContext.java",
                "sha": "0ed86f148ef2eddc9e74a62040568ecf36332473",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/LocalPSWorker.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/LocalPSWorker.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/LocalPSWorker.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.sysml.runtime.controlprogram.context.ExecutionContext;\n import org.apache.sysml.runtime.controlprogram.parfor.stat.Timing;\n import org.apache.sysml.runtime.instructions.cp.ListObject;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public class LocalPSWorker extends PSWorker implements Callable<Void> {\n@@ -54,7 +53,7 @@ public Void call() throws Exception {\n \t\tincWorkerNumber();\n \t\ttry {\n \t\t\tlong dataSize = _features.getNumRows();\n-\t\t\tint batchIter = IntUtils.toInt( Math.ceil((double) dataSize / _batchSize) );\n+\t\t\tint batchIter = (int) Math.ceil((double) dataSize / _batchSize);\n \n \t\t\tswitch (_freq) {\n \t\t\t\tcase BATCH:",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/LocalPSWorker.java",
                "sha": "df9c92535758c436917009e0aa0513e617a7fd37",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/ParamservUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/ParamservUtils.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/ParamservUtils.java",
                "patch": "@@ -71,7 +71,6 @@\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.matrix.operators.BinaryOperator;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n import scala.Tuple2;\n@@ -195,7 +194,7 @@ public static MatrixObject sliceMatrix(MatrixObject mo, long rl, long rh) {\n \t * @return new sliced matrix block\n \t */\n \tpublic static MatrixBlock sliceMatrixBlock(MatrixBlock mb, long rl, long rh) {\n-\t\treturn mb.slice(IntUtils.toInt( rl - 1 ), IntUtils.toInt( rh - 1 ));\n+\t\treturn mb.slice((int) rl - 1, (int) rh - 1);\n \t}\n \n \t/**\n@@ -390,7 +389,7 @@ public static MatrixBlock cbindMatrix(MatrixBlock left, MatrixBlock right) {\n \t\tJavaPairRDD<MatrixIndexes, MatrixBlock> labelsRDD = (JavaPairRDD<MatrixIndexes, MatrixBlock>)\n \t\t\tsec.getRDDHandleForMatrixObject(labels, InputInfo.BinaryBlockInputInfo);\n \n-\t\tDataPartitionerSparkMapper mapper = new DataPartitionerSparkMapper(scheme, workerNum, sec, IntUtils.toInt( features.getNumRows()));\n+\t\tDataPartitionerSparkMapper mapper = new DataPartitionerSparkMapper(scheme, workerNum, sec, (int) features.getNumRows());\n \t\tJavaPairRDD<Integer, Tuple2<MatrixBlock, MatrixBlock>> result = ParamservUtils\n \t\t\t.assembleTrainingData(featuresRDD, labelsRDD) // Combine features and labels into a pair (rowBlockID => (features, labels))\n \t\t\t.flatMapToPair(mapper) // Do the data partitioning on spark (workerID => (rowBlockID, (single row features, single row labels))",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/ParamservUtils.java",
                "sha": "58bf311c1f4bef95344c4aec7978f1afcd32b6d7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRLocalScheme.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRLocalScheme.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRLocalScheme.java",
                "patch": "@@ -29,7 +29,6 @@\n import org.apache.sysml.runtime.controlprogram.paramserv.ParamservUtils;\n import org.apache.sysml.runtime.instructions.InstructionUtils;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Data partitioner Disjoint_Random:\n@@ -40,7 +39,7 @@\n public class DRLocalScheme extends DataPartitionLocalScheme {\n \n \tprivate List<MatrixBlock> partition(int k, MatrixBlock mb, MatrixBlock permutation) {\n-\t\tint batchSize = IntUtils.toInt(Math.ceil((double) mb.getNumRows() / k));\n+\t\tint batchSize = (int) Math.ceil((double) mb.getNumRows() / k);\n \t\treturn IntStream.range(0, k).mapToObj(i -> {\n \t\t\tint begin = i * batchSize;\n \t\t\tint end = Math.min((i + 1) * batchSize, mb.getNumRows());",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRLocalScheme.java",
                "sha": "464be9904ce70fa00c31a037ce93c706b696d5d2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRSparkScheme.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRSparkScheme.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRSparkScheme.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.apache.sysml.hops.OptimizerUtils;\n import org.apache.sysml.runtime.controlprogram.paramserv.ParamservUtils;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -59,10 +58,10 @@ public Result doPartitioning(int numWorkers, int rblkID, MatrixBlock features, M\n \t\t\tlong shiftedPosition = (long) partialPerm.getValue(r, 0);\n \n \t\t\t// Get the shifted block and position\n-\t\t\tint shiftedBlkID =  IntUtils.toInt(shiftedPosition / OptimizerUtils.DEFAULT_BLOCKSIZE + 1);\n+\t\t\tint shiftedBlkID = (int) (shiftedPosition / OptimizerUtils.DEFAULT_BLOCKSIZE + 1);\n \n \t\t\tMatrixBlock indicator = _workerIndicator.getBlock(shiftedBlkID, 1);\n-\t\t\tint workerID = IntUtils.toInt( indicator.getValue(IntUtils.toInt( shiftedPosition / OptimizerUtils.DEFAULT_BLOCKSIZE), 0));\n+\t\t\tint workerID = (int) indicator.getValue((int) shiftedPosition / OptimizerUtils.DEFAULT_BLOCKSIZE, 0);\n \t\t\treturn new Tuple2<>(workerID, new Tuple2<>(shiftedPosition, rowMB));\n \t\t}).collect(Collectors.toList());\n \t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DRSparkScheme.java",
                "sha": "df61af9e90bc4b384ff9746154465f83adcfd43c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionSparkScheme.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionSparkScheme.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionSparkScheme.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.controlprogram.paramserv.ParamservUtils;\n import org.apache.sysml.runtime.instructions.spark.data.PartitionedBroadcast;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -66,7 +65,7 @@ protected void setWorkerIndicator(PartitionedBroadcast<MatrixBlock> wi) {\n \tprotected List<Tuple2<Integer, Tuple2<Long, MatrixBlock>>> nonShuffledPartition(int rblkID, MatrixBlock mb) {\n \t\tMatrixBlock indicator = _workerIndicator.getBlock(rblkID, 1);\n \t\treturn LongStream.range(0, mb.getNumRows()).mapToObj(r -> {\n-\t\t\tint workerID = IntUtils.toInt( indicator.getValue(IntUtils.toInt( r ), 0) );\n+\t\t\tint workerID = (int) indicator.getValue((int) r, 0);\n \t\t\tMatrixBlock rowMB = ParamservUtils.sliceMatrixBlock(mb, r + 1, r + 1);\n \t\t\tlong shiftedPosition = r + (rblkID - 1) * OptimizerUtils.DEFAULT_BLOCKSIZE;\n \t\t\treturn new Tuple2<>(workerID, new Tuple2<>(shiftedPosition, rowMB));",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionSparkScheme.java",
                "sha": "7992ac8ecd0a66196cb4ff5452fcb74ed0d6e7b8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionerSparkAggregator.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionerSparkAggregator.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 5,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionerSparkAggregator.java",
                "patch": "@@ -25,7 +25,6 @@\n import org.apache.spark.api.java.function.PairFunction;\n import org.apache.sysml.runtime.controlprogram.caching.MatrixObject;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -52,15 +51,15 @@ public DataPartitionerSparkAggregator(long fcol, long lcol) {\n \t */\n \t@Override\n \tpublic Tuple2<Integer, Tuple2<MatrixBlock, MatrixBlock>> call(Tuple2<Integer, LinkedList<Tuple2<Long, Tuple2<MatrixBlock, MatrixBlock>>>> input) throws Exception {\n-\t\tMatrixBlock fmb = new MatrixBlock(input._2.size(), IntUtils.toInt( _fcol ), false);\n-\t\tMatrixBlock lmb = new MatrixBlock(input._2.size(), IntUtils.toInt( _lcol ), false);\n+\t\tMatrixBlock fmb = new MatrixBlock(input._2.size(), (int) _fcol, false);\n+\t\tMatrixBlock lmb = new MatrixBlock(input._2.size(), (int) _lcol, false);\n \n \t\tfor (int i = 0; i < input._2.size(); i++) {\n \t\t\tMatrixBlock tmpFMB = input._2.get(i)._2._1;\n \t\t\tMatrixBlock tmpLMB = input._2.get(i)._2._2;\n \t\t\t// Row-wise aggregation\n-\t\t\tfmb = fmb.leftIndexingOperations(tmpFMB, i, i, 0, IntUtils.toInt( _fcol - 1 ), fmb, MatrixObject.UpdateType.INPLACE_PINNED);\n-\t\t\tlmb = lmb.leftIndexingOperations(tmpLMB, i, i, 0, IntUtils.toInt( _lcol - 1 ), lmb, MatrixObject.UpdateType.INPLACE_PINNED);\n+\t\t\tfmb = fmb.leftIndexingOperations(tmpFMB, i, i, 0, (int) _fcol - 1, fmb, MatrixObject.UpdateType.INPLACE_PINNED);\n+\t\t\tlmb = lmb.leftIndexingOperations(tmpLMB, i, i, 0, (int) _lcol - 1, lmb, MatrixObject.UpdateType.INPLACE_PINNED);\n \t\t}\n \t\treturn new Tuple2<>(input._1, new Tuple2<>(fmb, lmb));\n \t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/DataPartitionerSparkAggregator.java",
                "sha": "0314ccf528bc843d2ca498eed95034f204e538f6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/SparkDataPartitioner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/SparkDataPartitioner.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/SparkDataPartitioner.java",
                "patch": "@@ -33,7 +33,6 @@\n import org.apache.sysml.runtime.instructions.spark.data.PartitionedBroadcast;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.util.DataConverter;\n-import org.apache.sysml.utils.IntUtils;\n \n public class SparkDataPartitioner implements Serializable {\n \n@@ -75,7 +74,7 @@ private void createDRIndicator(SparkExecutionContext sec, int numWorkers, int nu\n \n \tprivate void createDCIndicator(SparkExecutionContext sec, int numWorkers, int numEntries) {\n \t\tdouble[] vector = new double[numEntries];\n-\t\tint batchSize = IntUtils.toInt( Math.ceil((double) numEntries / numWorkers) );\n+\t\tint batchSize = (int) Math.ceil((double) numEntries / numWorkers);\n \t\tfor (int i = 1; i < numWorkers; i++) {\n \t\t\tint begin = batchSize * i;\n \t\t\tint end = Math.min(begin + batchSize, numEntries);\n@@ -91,7 +90,7 @@ private void createGlobalPermutations(SparkExecutionContext sec, int numEntries,\n \t\t\t// Create the source-target id vector from the permutation ranging from 1 to number of entries\n \t\t\tdouble[] vector = new double[numEntries];\n \t\t\tfor (int j = 0; j < perm.getDenseBlockValues().length; j++) {\n-\t\t\t\tvector[IntUtils.toInt( perm.getDenseBlockValues()[j] - 1)] = j;\n+\t\t\t\tvector[(int) perm.getDenseBlockValues()[j] - 1] = j;\n \t\t\t}\n \t\t\tMatrixBlock vectorMB = DataConverter.convertToMatrixBlock(vector, true);\n \t\t\treturn sec.getBroadcastForMatrixObject(ParamservUtils.newMatrixObject(vectorMB));\n@@ -102,6 +101,6 @@ private void createGlobalPermutations(SparkExecutionContext sec, int numEntries,\n \tpublic DataPartitionSparkScheme.Result doPartitioning(int numWorkers, MatrixBlock features, MatrixBlock labels,\n \t\t\tlong rowID) {\n \t\t// Set the rowID in order to get the according permutation\n-\t\treturn _scheme.doPartitioning(numWorkers, IntUtils.toInt( rowID ), features, labels);\n+\t\treturn _scheme.doPartitioning(numWorkers, (int) rowID, features, labels);\n \t}\n }",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/dp/SparkDataPartitioner.java",
                "sha": "031150b9b6fd62820750ebbe4e6c6a89ef85eb8e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/rpc/PSRpcObject.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/rpc/PSRpcObject.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/rpc/PSRpcObject.java",
                "patch": "@@ -33,7 +33,6 @@\n import org.apache.sysml.runtime.instructions.cp.ListObject;\n import org.apache.sysml.runtime.io.IOUtilFunctions;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysml.utils.IntUtils;\n \n public abstract class PSRpcObject {\n \n@@ -94,7 +93,7 @@ protected int getExactSerializedSize(ListObject lo) {\n \t\t\t((MatrixObject)d).acquireReadAndRelease().getExactSizeOnDisk()).sum();\n \t\tif( result > Integer.MAX_VALUE )\n \t\t\tthrow new DMLRuntimeException(\"Serialized size (\"+result+\") larger than Integer.MAX_VALUE.\");\n-\t\treturn IntUtils.toInt( result );\n+\t\treturn (int) result;\n \t}\n \n \tprivate void validateListObject(ListObject lo) {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/paramserv/rpc/PSRpcObject.java",
                "sha": "38d80a2fea691975590214abf8c88840a4aa37b4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitioner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitioner.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitioner.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n \n \n /**\n@@ -150,7 +149,7 @@ public MatrixObject createPartitionedMatrixObject( MatrixObject in, MatrixObject\n \t\t//create output matrix object\n \t\tout.setPartitioned( _format, _n ); \n \t\t\n-\t\tMatrixCharacteristics mcNew = new MatrixCharacteristics( rows, cols, IntUtils.toInt(brlen), IntUtils.toInt(bclen) ); \n+\t\tMatrixCharacteristics mcNew = new MatrixCharacteristics( rows, cols, (int)brlen, (int)bclen ); \n \t\tmcNew.setNonZeros( nonZeros );\n \t\tif( convertBlock2Cell )\n \t\t\tii = InputInfo.BinaryCellInputInfo;\n@@ -177,11 +176,11 @@ public static MatrixBlock createReuseMatrixBlock( PDataPartitionFormat dpf, int\n \t\t{\n \t\t\tcase ROW_WISE:\n \t\t\t\t//default assumption sparse, but reset per input block anyway\n-\t\t\t\ttmp = new MatrixBlock( 1, IntUtils.toInt(cols), true, IntUtils.toInt(cols*0.1) );\n+\t\t\t\ttmp = new MatrixBlock( 1, (int)cols, true, (int)(cols*0.1) );\n \t\t\t\tbreak;\n \t\t\tcase COLUMN_WISE:\n \t\t\t\t//default dense because single column alwyas below SKINNY_MATRIX_TURN_POINT\n-\t\t\t\ttmp = new MatrixBlock( IntUtils.toInt(rows), 1, false );\n+\t\t\t\ttmp = new MatrixBlock( (int)rows, 1, false );\n \t\t\t\tbreak;\n \t\t\tdefault:\n \t\t\t\t//do nothing",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitioner.java",
                "sha": "3d12949d3a367a521cc09d2123cc2500e7a7d660",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerLocal.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerLocal.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 13,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerLocal.java",
                "patch": "@@ -56,7 +56,6 @@\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.FastStringTokenizer;\n import org.apache.sysml.runtime.util.LocalFileUtils;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Partitions a given matrix into row or column partitions with a two pass-approach.\n@@ -190,8 +189,8 @@ private void partitionTextCell( String fname, String fnameStaging, String fnameN\n \t\t\t\tThread[] threads = new Thread[len];\n \t\t\t\tfor( int i=0;i<len;i++ )\n \t\t\t\t{\n-\t\t\t\t\tint start = i*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len));\n-\t\t\t\t\tint end = (i+1)*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len)-1);\n+\t\t\t\t\tint start = i*(int)Math.ceil(((double)fnamesPartitions.length)/len);\n+\t\t\t\t\tint end = (i+1)*(int)Math.ceil(((double)fnamesPartitions.length)/len)-1;\n \t\t\t\t\tend = Math.min(end, fnamesPartitions.length-1);\n \t\t\t\t\tthreads[i] = new Thread(new DataPartitionerWorkerTextCell(job, fnameNew, fnameStaging, fnamesPartitions, start, end));\n \t\t\t\t\tthreads[i].start();\n@@ -277,8 +276,8 @@ private void partitionBinaryCell( String fname, String fnameStaging, String fnam\n \t\t\t\tThread[] threads = new Thread[len];\n \t\t\t\tfor( int i=0;i<len;i++ )\n \t\t\t\t{\n-\t\t\t\t\tint start = i*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len));\n-\t\t\t\t\tint end = (i+1)*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len)-1);\n+\t\t\t\t\tint start = i*(int)Math.ceil(((double)fnamesPartitions.length)/len);\n+\t\t\t\t\tint end = (i+1)*(int)Math.ceil(((double)fnamesPartitions.length)/len)-1;\n \t\t\t\t\tend = Math.min(end, fnamesPartitions.length-1);\n \t\t\t\t\tthreads[i] = new Thread(new DataPartitionerWorkerBinaryCell(job, fnameNew, fnameStaging, fnamesPartitions, start, end));\n \t\t\t\t\tthreads[i].start();\n@@ -359,8 +358,8 @@ private void partitionBinaryBlock( String fname, String fnameStaging, String fna\n \t\t\t\tThread[] threads = new Thread[len];\n \t\t\t\tfor( int i=0;i<len;i++ )\n \t\t\t\t{\n-\t\t\t\t\tint start = i*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len));\n-\t\t\t\t\tint end = (i+1)*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len)-1);\n+\t\t\t\t\tint start = i*(int)Math.ceil(((double)fnamesPartitions.length)/len);\n+\t\t\t\t\tint end = (i+1)*(int)Math.ceil(((double)fnamesPartitions.length)/len)-1;\n \t\t\t\t\tend = Math.min(end, fnamesPartitions.length-1);\n \t\t\t\t\tthreads[i] = new Thread(new DataPartitionerWorkerBinaryBlock(job, fnameNew, fnameStaging, fnamesPartitions, start, end));\n \t\t\t\t\tthreads[i].start();\n@@ -463,8 +462,8 @@ private void partitionBinaryBlock2BinaryCell( String fname, String fnameStaging,\n \t\t\t\tThread[] threads = new Thread[len];\n \t\t\t\tfor( int i=0;i<len;i++ )\n \t\t\t\t{\n-\t\t\t\t\tint start = i*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len));\n-\t\t\t\t\tint end = (i+1)*IntUtils.toInt(Math.ceil(((double)fnamesPartitions.length)/len)-1);\n+\t\t\t\t\tint start = i*(int)Math.ceil(((double)fnamesPartitions.length)/len);\n+\t\t\t\t\tint end = (i+1)*(int)Math.ceil(((double)fnamesPartitions.length)/len)-1;\n \t\t\t\t\tend = Math.min(end, fnamesPartitions.length-1);\n \t\t\t\t\tthreads[i] = new Thread(new DataPartitionerWorkerBinaryCell(job, fnameNew, fnameStaging, fnamesPartitions, start, end));\n \t\t\t\t\tthreads[i].start();\n@@ -497,12 +496,12 @@ private void appendBlockToStagingArea( String dir, MatrixBlock mb, long row_offs\n \n \t\tif( _format == PDataPartitionFormat.ROW_WISE ) \n \t\t{\t\n-\t\t\t_reuseBlk.reset( 1, IntUtils.toInt(cols), sparse, IntUtils.toInt(cols*sparsity) );\n+\t\t\t_reuseBlk.reset( 1, (int)cols, sparse, (int) (cols*sparsity) );\n \t\t\tfor( int i=0; i<rows; i++ )\n \t\t\t{\n \t\t\t\tString pdir = LocalFileUtils.checkAndCreateStagingDir(dir+\"/\"+(row_offset+1+i));\n \t\t\t\tString pfname = pdir+\"/\"+\"block_\"+(col_offset/bclen+1);\n-\t\t\t\tmb.slice(i, i, 0, IntUtils.toInt(cols-1), _reuseBlk);\n+\t\t\t\tmb.slice(i, i, 0, (int)(cols-1), _reuseBlk);\n \t\t\t\tLocalFileUtils.writeMatrixBlockToLocal(pfname, _reuseBlk);\n \t\t\t\t_reuseBlk.reset();\n \t\t\t}\n@@ -516,13 +515,13 @@ else if( _format == PDataPartitionFormat.ROW_BLOCK_WISE )\n \t\telse if( _format == PDataPartitionFormat.COLUMN_WISE )\n \t\t{\n \t\t\t//create object for reuse\n-\t\t\t_reuseBlk.reset( IntUtils.toInt(rows), 1, false );\n+\t\t\t_reuseBlk.reset( (int)rows, 1, false );\n \t\t\t\n \t\t\tfor( int i=0; i<cols; i++ )\n \t\t\t{\n \t\t\t\tString pdir = LocalFileUtils.checkAndCreateStagingDir(dir+\"/\"+(col_offset+1+i));\n \t\t\t\tString pfname = pdir+\"/\"+\"block_\"+(row_offset/brlen+1); \t\t\t\n-\t\t\t\tmb.slice(0, IntUtils.toInt(rows-1), i, i, _reuseBlk);\n+\t\t\t\tmb.slice(0, (int)(rows-1), i, i, _reuseBlk);\n \t\t\t\tLocalFileUtils.writeMatrixBlockToLocal(pfname, _reuseBlk);\n \t\t\t\t_reuseBlk.reset();\n \t\t\t}\t\t\t\t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerLocal.java",
                "sha": "ecb5ea06ee9472fff5d8184a5814e7a137d0edba",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMR.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMR.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMR.java",
                "patch": "@@ -38,7 +38,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRConfigurationNames;\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n import org.apache.sysml.yarn.DMLAppMasterUtils;\n \n@@ -150,7 +149,7 @@ else if ( oi == OutputInfo.BinaryBlockOutputInfo )\n \t\t\t    default:\n \t\t\t\t\t//do nothing\n \t\t    }\n-\t\t    job.setNumReduceTasks( IntUtils.toInt(Math.min( _numReducers, reducerGroups)) ); \t\n+\t\t    job.setNumReduceTasks( (int)Math.min( _numReducers, reducerGroups) ); \t\n \n \t\t\t\n \t\t\t//disable automatic tasks timeouts and speculative task exec",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMR.java",
                "sha": "8195d91b556e1e4eff3677a682364c3af1cb9189",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMapper.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMapper.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 7,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMapper.java",
                "patch": "@@ -41,7 +41,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.FastStringTokenizer;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Remote data partitioner mapper implementation that does the actual\n@@ -336,12 +335,12 @@ protected void processKeyValue(Writable key, Writable value, OutputCollector<Wri\n \t\t\t\tswitch( _pdf )\n \t\t\t\t{\n \t\t\t\t\tcase ROW_WISE:\n-\t\t\t\t\t\t_reuseBlk.reset(1, IntUtils.toInt(cols), sparse, IntUtils.toInt(cols*sparsity));\t\t\t\t\t\t\t\t\n+\t\t\t\t\t\t_reuseBlk.reset(1, (int)cols, sparse, (int)(cols*sparsity));\t\t\t\t\t\t\t\t\n \t\t\t\t\t\tfor( int i=0; i<rows; i++ )\n \t\t\t\t\t\t{\n \t\t\t\t\t\t\t_longKey.set(row_offset+1+i);\n \t\t\t\t\t\t\t_pairKey.setIndexes(1, (col_offset/_bclen+1) );\t\n-\t\t\t\t\t\t\tvalue2.slice(i, i, 0, IntUtils.toInt(cols-1), _reuseBlk);\n+\t\t\t\t\t\t\tvalue2.slice(i, i, 0, (int)(cols-1), _reuseBlk);\n \t\t\t\t\t\t\tout.collect(_longKey, _pair);\n \t\t\t\t\t\t\t_reuseBlk.reset();\n \t\t\t\t\t\t}\n@@ -362,12 +361,12 @@ protected void processKeyValue(Writable key, Writable value, OutputCollector<Wri\n \t\t\t\t\t\tout.collect(_longKey, _pair);\n \t\t\t\t\t\tbreak;\n \t\t\t\t\tcase COLUMN_WISE:\n-\t\t\t\t\t\t_reuseBlk.reset(IntUtils.toInt(rows), 1, false);\n+\t\t\t\t\t\t_reuseBlk.reset((int)rows, 1, false);\n \t\t\t\t\t\tfor( int i=0; i<cols; i++ )\n \t\t\t\t\t\t{\n \t\t\t\t\t\t\t_longKey.set(col_offset+1+i);\n \t\t\t\t\t\t\t_pairKey.setIndexes(row_offset/_brlen+1, 1);\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\tvalue2.slice(0, IntUtils.toInt(rows-1), i, i, _reuseBlk);\n+\t\t\t\t\t\t\tvalue2.slice(0, (int)(rows-1), i, i, _reuseBlk);\n \t\t\t\t\t\t\tout.collect(_longKey, _pair );\n \t\t\t\t\t\t\t_reuseBlk.reset();\n \t\t\t\t\t\t}\t\n@@ -672,9 +671,9 @@ protected void close()\n \t\t\t\tlong numPartitions = -1;\n \t\t\t\tswitch( _pdf ){\n \t\t\t\t\tcase ROW_WISE: \t\t\t numPartitions = _rlen; break;\n-\t\t\t\t\tcase ROW_BLOCK_WISE:     numPartitions = IntUtils.toInt(Math.ceil(_rlen/(double)_brlen)); break;\n+\t\t\t\t\tcase ROW_BLOCK_WISE:     numPartitions = (int)Math.ceil(_rlen/(double)_brlen); break;\n \t\t\t\t\tcase COLUMN_WISE:        numPartitions = _clen; break;\n-\t\t\t\t\tcase COLUMN_BLOCK_WISE:  numPartitions = IntUtils.toInt(Math.ceil(_clen/(double)_bclen)); break;\n+\t\t\t\t\tcase COLUMN_BLOCK_WISE:  numPartitions = (int)Math.ceil(_clen/(double)_bclen); break;\n \t\t\t\t\tdefault:\n \t\t\t\t\t\t//do nothing\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteMapper.java",
                "sha": "f535b325699e50e7473ea59fc2879e8dfd0c258e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSpark.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSpark.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSpark.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n /**\n@@ -75,7 +74,7 @@ protected void partitionMatrix(MatrixObject in, String fnameNew, InputInfo ii, O\n \t\t\t\n \t\t\t//determine degree of parallelism\n \t\t\tMatrixCharacteristics mc = in.getMatrixCharacteristics();\n-\t\t\tint numRed = IntUtils.toInt(determineNumReducers(inRdd, mc, _numRed));\n+\t\t\tint numRed = (int)determineNumReducers(inRdd, mc, _numRed);\n \t\n \t\t\t//run spark remote data partition job \n \t\t\tDataPartitionerRemoteSparkMapper dpfun = new DataPartitionerRemoteSparkMapper(mc, ii, oi, _format, _n);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSpark.java",
                "sha": "daab642c5683b2619baf11b2fefe87d404e39914",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSparkMapper.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSparkMapper.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSparkMapper.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.DataConverter;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -118,7 +117,7 @@ public DataPartitionerRemoteSparkMapper(MatrixCharacteristics mc, InputInfo ii,\n \t\t\t\t\tfor( int i=0; i<rows; i+=_n ) {\n \t\t\t\t\t\tPairWritableBlock tmp = new PairWritableBlock();\n \t\t\t\t\t\ttmp.indexes = new MatrixIndexes(1, col_offset/_bclen+1);\n-\t\t\t\t\t\ttmp.block = value2.slice(i, Math.min(i+IntUtils.toInt(_n)-1, value2.getNumRows()-1));\n+\t\t\t\t\t\ttmp.block = value2.slice(i, Math.min(i+(int)_n-1, value2.getNumRows()-1));\n \t\t\t\t\t\tret.add(new Tuple2<Long,Writable>(new Long((row_offset+i)/_n+1),tmp));\n \t\t\t\t\t}\n \t\t\t\t}\n@@ -153,7 +152,7 @@ public DataPartitionerRemoteSparkMapper(MatrixCharacteristics mc, InputInfo ii,\n \t\t\t\t\t\tPairWritableBlock tmp = new PairWritableBlock();\n \t\t\t\t\t\ttmp.indexes = new MatrixIndexes(row_offset/_brlen+1, 1);\n \t\t\t\t\t\ttmp.block = value2.slice(0, value2.getNumRows()-1, \n-\t\t\t\t\t\t\t\ti, Math.min(i+IntUtils.toInt(_n)-1, value2.getNumColumns()-1), new MatrixBlock());\n+\t\t\t\t\t\t\t\ti, Math.min(i+(int)_n-1, value2.getNumColumns()-1), new MatrixBlock());\n \t\t\t\t\t\tret.add(new Tuple2<Long,Writable>(new Long((col_offset+i)/_n+1),tmp));\n \t\t\t\t\t}\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/DataPartitionerRemoteSparkMapper.java",
                "sha": "b5f09567a0428e9ea57ba0d8d1afd8f594525645",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForMR.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForMR.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 12,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForMR.java",
                "patch": "@@ -57,7 +57,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.MapReduceTool;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n import org.apache.sysml.yarn.DMLAppMasterUtils;\n \n@@ -100,8 +99,8 @@ public static RemoteParForJobReturn runJob(long pfid, String itervar, String mat\n \t\t\tPath path = new Path( input.getFileName() );\n \t\t\tlong rlen = input.getNumRows();\n \t\t\tlong clen = input.getNumColumns();\n-\t\t\tint brlen = IntUtils.toInt( input.getNumRowsPerBlock());\n-\t\t\tint bclen = IntUtils.toInt( input.getNumColumnsPerBlock());\n+\t\t\tint brlen = (int) input.getNumRowsPerBlock();\n+\t\t\tint bclen = (int) input.getNumColumnsPerBlock();\n \t\t\tMRJobConfiguration.setPartitioningInfo(job, rlen, clen, brlen, bclen, InputInfo.BinaryBlockInputInfo, \n \t\t\t\t\toi, dpf._dpf, dpf._N, input.getFileName(), itervar, matrixvar, tSparseCol);\n \t\t\tjob.setInputFormat(InputInfo.BinaryBlockInputInfo.inputFormatClass);\n@@ -173,20 +172,20 @@ else if( oi == OutputInfo.BinaryCellOutputInfo )\n \t\t\t// Process different counters \n \t\t\tStatistics.incrementNoOfExecutedMRJobs();\n \t\t\tGroup pgroup = runjob.getCounters().getGroup(ParForProgramBlock.PARFOR_COUNTER_GROUP_NAME);\n-\t\t\tint numTasks = IntUtils.toInt(pgroup.getCounter( Stat.PARFOR_NUMTASKS.toString() ));\n-\t\t\tint numIters = IntUtils.toInt(pgroup.getCounter( Stat.PARFOR_NUMITERS.toString() ));\n+\t\t\tint numTasks = (int)pgroup.getCounter( Stat.PARFOR_NUMTASKS.toString() );\n+\t\t\tint numIters = (int)pgroup.getCounter( Stat.PARFOR_NUMITERS.toString() );\n \t\t\tif( ConfigurationManager.isStatistics() && !InfrastructureAnalyzer.isLocalMode() ) {\n \t\t\t\tStatistics.incrementJITCompileTime( pgroup.getCounter( Stat.PARFOR_JITCOMPILE.toString() ) );\n \t\t\t\tStatistics.incrementJVMgcCount( pgroup.getCounter( Stat.PARFOR_JVMGC_COUNT.toString() ) );\n \t\t\t\tStatistics.incrementJVMgcTime( pgroup.getCounter( Stat.PARFOR_JVMGC_TIME.toString() ) );\n \t\t\t\tGroup cgroup = runjob.getCounters().getGroup(CacheableData.CACHING_COUNTER_GROUP_NAME.toString());\n-\t\t\t\tCacheStatistics.incrementMemHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_MEM.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSBuffHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FSBUFF.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FS.toString() ));\n-\t\t\t\tCacheStatistics.incrementHDFSHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_HDFS.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSBuffWrites(cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FSBUFF.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSWrites(cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FS.toString() ));\n-\t\t\t\tCacheStatistics.incrementHDFSWrites(cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_HDFS.toString() ));\n+\t\t\t\tCacheStatistics.incrementMemHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_MEM.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSBuffHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FSBUFF.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FS.toString() ));\n+\t\t\t\tCacheStatistics.incrementHDFSHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_HDFS.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSBuffWrites((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FSBUFF.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSWrites((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FS.toString() ));\n+\t\t\t\tCacheStatistics.incrementHDFSWrites((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_HDFS.toString() ));\n \t\t\t\tCacheStatistics.incrementAcquireRTime(cgroup.getCounter( CacheStatistics.Stat.CACHE_TIME_ACQR.toString() ));\n \t\t\t\tCacheStatistics.incrementAcquireMTime(cgroup.getCounter( CacheStatistics.Stat.CACHE_TIME_ACQM.toString() ));\n \t\t\t\tCacheStatistics.incrementReleaseTime(cgroup.getCounter( CacheStatistics.Stat.CACHE_TIME_RLS.toString() ));",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForMR.java",
                "sha": "56238279a03b8067c3461e2fbb7e70656a094b2d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSpark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSpark.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSpark.java",
                "patch": "@@ -57,7 +57,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n /**\n@@ -91,7 +90,7 @@ public static RemoteParForJobReturn runJob(long pfid, String itervar, String mat\n \n \t\t//compute number of reducers (to avoid OOMs and reduce memory pressure)\n \t\tint numParts = SparkUtils.getNumPreferredPartitions(mc, in);\n-\t\tint numReducers2 = Math.max(numReducers, Math.min(numParts, IntUtils.toInt(dpf.getNumParts(mc))));\n+\t\tint numReducers2 = Math.max(numReducers, Math.min(numParts, (int)dpf.getNumParts(mc)));\n \t\t\n \t\t//core parfor datapartition-execute (w/ or w/o shuffle, depending on data characteristics)\n \t\tRemoteDPParForSparkWorker efun = new RemoteDPParForSparkWorker(program, clsMap, \n@@ -224,7 +223,7 @@ public DataFrameToRowBinaryBlockFunction(long clen, boolean containsID, boolean\n \t\t\tint off = _containsID ? 1: 0;\n \t\t\tObject obj = _isVector ? arg0._1().get(off) : arg0._1();\n \t\t\tboolean sparse = (obj instanceof SparseVector);\n-\t\t\tMatrixBlock mb = new MatrixBlock(1, IntUtils.toInt(_clen), sparse);\n+\t\t\tMatrixBlock mb = new MatrixBlock(1, (int)_clen, sparse);\n \t\t\t\n \t\t\tif( _isVector ) {\n \t\t\t\tVector vect = (Vector) obj;",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSpark.java",
                "sha": "866e456b61471164aa4768b554779f02ac8ad35c",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSparkWorker.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSparkWorker.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 10,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSparkWorker.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -85,8 +84,8 @@ public RemoteDPParForSparkWorker(String program, HashMap<String, byte[]> clsMap,\n \t\t_aIters = aiters;\n \t\t\n \t\t//setup matrix block partition meta data\n-\t\t_rlen = IntUtils.toInt(dpf.getNumRows(mc));\n-\t\t_clen = IntUtils.toInt(dpf.getNumColumns(mc));\n+\t\t_rlen = (int)dpf.getNumRows(mc);\n+\t\t_clen = (int)dpf.getNumColumns(mc);\n \t\t_brlen = mc.getRowsPerBlock();\n \t\t_bclen = mc.getColsPerBlock();\n \t\t_tSparseCol = tSparseCol;\n@@ -126,7 +125,7 @@ public RemoteDPParForSparkWorker(String program, HashMap<String, byte[]> clsMap,\n \t\t\t\n \t\t\t//maintain accumulators\n \t\t\t_aTasks.add( 1 );\n-\t\t\t_aIters.add( IntUtils.toInt(getExecutedIterations()-numIter) );\n+\t\t\t_aIters.add( (int)(getExecutedIterations()-numIter) );\n \t\t}\n \t\t\n \t\t//write output if required (matrix indexed write)\n@@ -144,7 +143,7 @@ private void configureWorker( long ID )\n \t\t\tCodegenUtils.getClassSync(e.getKey(), e.getValue());\n \t\n \t\t//parse and setup parfor body program\n-\t\tParForBody body = ProgramConverter.parseParForBody(_prog, IntUtils.toInt(_workerID), true);\n+\t\tParForBody body = ProgramConverter.parseParForBody(_prog, (int)_workerID, true);\n \t\t_childBlocks = body.getChildBlocks();\n \t\t_ec          = body.getEc();\n \t\t_resultVars  = body.getResultVariables();\n@@ -200,8 +199,8 @@ else if( partition!=null )\n \t\t\tlong lnnz = 0;\n \t\t\tfor( Writable val : valueList ) {\n \t\t\t\tPairWritableBlock pval = (PairWritableBlock) val;\n-\t\t\t\tint row_offset = IntUtils.toInt(pval.indexes.getRowIndex()-1)*_brlen;\n-\t\t\t\tint col_offset = IntUtils.toInt(pval.indexes.getColumnIndex()-1)*_bclen;\n+\t\t\t\tint row_offset = (int)(pval.indexes.getRowIndex()-1)*_brlen;\n+\t\t\t\tint col_offset = (int)(pval.indexes.getColumnIndex()-1)*_bclen;\n \t\t\t\tif( !partition.isInSparseFormat() ) //DENSE\n \t\t\t\t\tpartition.copy( row_offset, row_offset+pval.block.getNumRows()-1, \n \t\t\t\t\t\tcol_offset, col_offset+pval.block.getNumColumns()-1,\n@@ -255,7 +254,7 @@ private MatrixBlock collectBinaryCellInput( Iterable<Writable> valueList )\n \t\t\t\t\tPairWritableCell pairValue = (PairWritableCell)valueList.iterator().next();\n \t\t\t\t\tif( pairValue.indexes.getColumnIndex()<0 )\n \t\t\t\t\t\tcontinue; //cells used to ensure empty partitions\n-\t\t\t\t\tpartition.quickSetValue(0, IntUtils.toInt(pairValue.indexes.getColumnIndex()-1), pairValue.cell.getValue());\n+\t\t\t\t\tpartition.quickSetValue(0, (int)pairValue.indexes.getColumnIndex()-1, pairValue.cell.getValue());\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t\tcase COLUMN_WISE:\n@@ -265,9 +264,9 @@ private MatrixBlock collectBinaryCellInput( Iterable<Writable> valueList )\n \t\t\t\t\tif( pairValue.indexes.getRowIndex()<0 )\n \t\t\t\t\t\tcontinue; //cells used to ensure empty partitions\n \t\t\t\t\tif( _tSparseCol )\n-\t\t\t\t\t\tpartition.appendValue(0,IntUtils.toInt(pairValue.indexes.getRowIndex()-1), pairValue.cell.getValue());\n+\t\t\t\t\t\tpartition.appendValue(0,(int)pairValue.indexes.getRowIndex()-1, pairValue.cell.getValue());\n \t\t\t\t\telse\n-\t\t\t\t\t\tpartition.quickSetValue(IntUtils.toInt(pairValue.indexes.getRowIndex()-1), 0, pairValue.cell.getValue());\n+\t\t\t\t\t\tpartition.quickSetValue((int)pairValue.indexes.getRowIndex()-1, 0, pairValue.cell.getValue());\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t\tdefault: ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParForSparkWorker.java",
                "sha": "66b42839fe5f635ceb8a6344d1ad754479f376b1",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParWorkerReducer.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParWorkerReducer.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 11,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParWorkerReducer.java",
                "patch": "@@ -48,7 +48,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.LocalFileUtils;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public class RemoteDPParWorkerReducer extends ParWorker\n@@ -121,18 +120,18 @@ public void configure(JobConf job)\n \t\t_dpf = MRJobConfiguration.getPartitioningFormat( job );\n \t\tMatrixCharacteristics mc = MRJobConfiguration.getPartitionedMatrixSize(job);\n \t\tPartitionFormat pf = new PartitionFormat(_dpf, MRJobConfiguration.getPartitioningSizeN(job));\n-\t\t_rlen = IntUtils.toInt(pf.getNumRows(mc));\n-\t\t_clen = IntUtils.toInt(pf.getNumColumns(mc));\n+\t\t_rlen = (int)pf.getNumRows(mc);\n+\t\t_clen = (int)pf.getNumColumns(mc);\n \t\t_brlen = mc.getRowsPerBlock();\n \t\t_bclen = mc.getColsPerBlock();\n \t\t_iterVar = MRJobConfiguration.getPartitioningItervar( job );\n \t\t_inputVar = MRJobConfiguration.getPartitioningMatrixvar( job );\n \t\t_info = MRJobConfiguration.getPartitioningOutputInfo( job );\n \t\t_tSparseCol = MRJobConfiguration.getPartitioningTransposedCol( job ); \n \t\tif( _tSparseCol )\n-\t\t\t_partition = new MatrixBlock(IntUtils.toInt(_clen), _rlen, true);\n+\t\t\t_partition = new MatrixBlock((int)_clen, _rlen, true);\n \t\telse\n-\t\t\t_partition = new MatrixBlock(IntUtils.toInt(_rlen), _clen, false);\n+\t\t\t_partition = new MatrixBlock((int)_rlen, _clen, false);\n \n \t\t//Step 1: configure parworker\n \t\tString taskID = job.get(MRConfigurationNames.MR_TASK_ID);\n@@ -153,7 +152,7 @@ public void configure(JobConf job)\n \t\t\t\n \t\t\t//create local runtime program\n \t\t\tString in = MRJobConfiguration.getProgramBlocks(job);\n-\t\t\tParForBody body = ProgramConverter.parseParForBody(in, IntUtils.toInt(_workerID));\n+\t\t\tParForBody body = ProgramConverter.parseParForBody(in, (int)_workerID);\n \t\t\t_childBlocks = body.getChildBlocks();\n \t\t\t_ec          = body.getEc();\n \t\t\t_resultVars  = body.getResultVariables();\n@@ -243,8 +242,8 @@ private MatrixBlock collectBinaryBlock( Iterator<Writable> valueList )\n \t\t\twhile( valueList.hasNext() )\n \t\t\t{\n \t\t\t\tPairWritableBlock pairValue = (PairWritableBlock)valueList.next();\n-\t\t\t\tint row_offset = IntUtils.toInt(pairValue.indexes.getRowIndex()-1)*_brlen;\n-\t\t\t\tint col_offset = IntUtils.toInt(pairValue.indexes.getColumnIndex()-1)*_bclen;\n+\t\t\t\tint row_offset = (int)(pairValue.indexes.getRowIndex()-1)*_brlen;\n+\t\t\t\tint col_offset = (int)(pairValue.indexes.getColumnIndex()-1)*_bclen;\n \t\t\t\tMatrixBlock block = pairValue.block;\n \t\t\t\tif( !_partition.isInSparseFormat() ) //DENSE\n \t\t\t\t{\n@@ -298,7 +297,7 @@ private MatrixBlock collectBinaryCellInput( Iterator<Writable> valueList )\n \t\t\t\t\tPairWritableCell pairValue = (PairWritableCell)valueList.next();\n \t\t\t\t\tif( pairValue.indexes.getColumnIndex()<0 )\n \t\t\t\t\t\tcontinue; //cells used to ensure empty partitions\n-\t\t\t\t\t_partition.quickSetValue(0, IntUtils.toInt(pairValue.indexes.getColumnIndex()-1), pairValue.cell.getValue());\n+\t\t\t\t\t_partition.quickSetValue(0, (int)pairValue.indexes.getColumnIndex()-1, pairValue.cell.getValue());\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t\tcase COLUMN_WISE:\n@@ -308,9 +307,9 @@ private MatrixBlock collectBinaryCellInput( Iterator<Writable> valueList )\n \t\t\t\t\tif( pairValue.indexes.getRowIndex()<0 )\n \t\t\t\t\t\tcontinue; //cells used to ensure empty partitions\n \t\t\t\t\tif( _tSparseCol )\n-\t\t\t\t\t\t_partition.appendValue(0,IntUtils.toInt(pairValue.indexes.getRowIndex()-1), pairValue.cell.getValue());\n+\t\t\t\t\t\t_partition.appendValue(0,(int)pairValue.indexes.getRowIndex()-1, pairValue.cell.getValue());\n \t\t\t\t\telse\n-\t\t\t\t\t\t_partition.quickSetValue(IntUtils.toInt(pairValue.indexes.getRowIndex()-1), 0, pairValue.cell.getValue());\n+\t\t\t\t\t\t_partition.quickSetValue((int)pairValue.indexes.getRowIndex()-1, 0, pairValue.cell.getValue());\n \t\t\t\t}\n \t\t\t\tbreak;\n \t\t\tdefault: ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteDPParWorkerReducer.java",
                "sha": "11d1ed9de6e80862408ad97bf1ba87dcf276bda5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForColocatedNLineInputFormat.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForColocatedNLineInputFormat.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForColocatedNLineInputFormat.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.sysml.runtime.controlprogram.ParForProgramBlock.PartitionFormat;\n import org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Specific extension of NLineInputFormat in order to ensure data colocation\n@@ -49,7 +48,7 @@\n \t\tMatrixCharacteristics mc = MRJobConfiguration.getPartitionedMatrixSize(job);\n \t\tPDataPartitionFormat dpf = MRJobConfiguration.getPartitioningFormat(job);\n \t\tPartitionFormat pf = new PartitionFormat(dpf, -1);\n-\t\tint blen = IntUtils.toInt(pf.isRowwise() ? pf.getNumRows(mc) : pf.getNumColumns(mc));\n+\t\tint blen = (int) (pf.isRowwise() ? pf.getNumRows(mc) : pf.getNumColumns(mc));\n \t\tString fname = MRJobConfiguration.getPartitioningFilename(job);\n \n \t\t//create wrapper splits ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForColocatedNLineInputFormat.java",
                "sha": "a2a231d6e1dc1895fcccd2f21798ca6daaae850e",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForMR.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForMR.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 10,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForMR.java",
                "patch": "@@ -54,7 +54,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.MapReduceTool;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n import org.apache.sysml.yarn.DMLAppMasterUtils;\n \n@@ -183,20 +182,20 @@ public static RemoteParForJobReturn runJob(long pfid, String program, String tas\n \t\t\t// Process different counters \n \t\t\tStatistics.incrementNoOfExecutedMRJobs();\n \t\t\tGroup pgroup = runjob.getCounters().getGroup(ParForProgramBlock.PARFOR_COUNTER_GROUP_NAME);\n-\t\t\tint numTasks = IntUtils.toInt(pgroup.getCounter( Stat.PARFOR_NUMTASKS.toString() ));\n-\t\t\tint numIters = IntUtils.toInt(pgroup.getCounter( Stat.PARFOR_NUMITERS.toString() ));\n+\t\t\tint numTasks = (int)pgroup.getCounter( Stat.PARFOR_NUMTASKS.toString() );\n+\t\t\tint numIters = (int)pgroup.getCounter( Stat.PARFOR_NUMITERS.toString() );\n \t\t\tif( ConfigurationManager.isStatistics() && !InfrastructureAnalyzer.isLocalMode() ) {\n \t\t\t\tStatistics.incrementJITCompileTime( pgroup.getCounter( Stat.PARFOR_JITCOMPILE.toString() ) );\n \t\t\t\tStatistics.incrementJVMgcCount( pgroup.getCounter( Stat.PARFOR_JVMGC_COUNT.toString() ) );\n \t\t\t\tStatistics.incrementJVMgcTime( pgroup.getCounter( Stat.PARFOR_JVMGC_TIME.toString() ) );\n \t\t\t\tGroup cgroup = runjob.getCounters().getGroup(CacheableData.CACHING_COUNTER_GROUP_NAME.toString());\n-\t\t\t\tCacheStatistics.incrementMemHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_MEM.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSBuffHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FSBUFF.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FS.toString() ));\n-\t\t\t\tCacheStatistics.incrementHDFSHits(cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_HDFS.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSBuffWrites(cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FSBUFF.toString() ));\n-\t\t\t\tCacheStatistics.incrementFSWrites(cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FS.toString() ));\n-\t\t\t\tCacheStatistics.incrementHDFSWrites(cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_HDFS.toString() ));\n+\t\t\t\tCacheStatistics.incrementMemHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_MEM.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSBuffHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FSBUFF.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_FS.toString() ));\n+\t\t\t\tCacheStatistics.incrementHDFSHits((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_HITS_HDFS.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSBuffWrites((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FSBUFF.toString() ));\n+\t\t\t\tCacheStatistics.incrementFSWrites((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_FS.toString() ));\n+\t\t\t\tCacheStatistics.incrementHDFSWrites((int)cgroup.getCounter( CacheStatistics.Stat.CACHE_WRITES_HDFS.toString() ));\n \t\t\t\tCacheStatistics.incrementAcquireRTime(cgroup.getCounter( CacheStatistics.Stat.CACHE_TIME_ACQR.toString() ));\n \t\t\t\tCacheStatistics.incrementAcquireMTime(cgroup.getCounter( CacheStatistics.Stat.CACHE_TIME_ACQM.toString() ));\n \t\t\t\tCacheStatistics.incrementReleaseTime(cgroup.getCounter( CacheStatistics.Stat.CACHE_TIME_RLS.toString() ));",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForMR.java",
                "sha": "63e4b401bc4ba47408e7962a4f39dcd869f5c0c9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForSparkWorker.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForSparkWorker.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForSparkWorker.java",
                "patch": "@@ -39,7 +39,6 @@\n import org.apache.sysml.runtime.controlprogram.parfor.stat.InfrastructureAnalyzer;\n import org.apache.sysml.runtime.util.ProgramConverter;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -91,7 +90,7 @@ public RemoteParForSparkWorker(long jobid, String program, HashMap<String, byte[\n \t\t\n \t\t//maintain accumulators\n \t\t_aTasks.add( 1 );\n-\t\t_aIters.add( IntUtils.toInt(getExecutedIterations()-numIter) );\n+\t\t_aIters.add( (int)(getExecutedIterations()-numIter) );\n \t\t\n \t\t//cleanup remaining intermediate variables from buffer pool\n \t\t_ec.getVariables().keySet().stream().filter(v -> !inVars.contains(v))\n@@ -115,7 +114,7 @@ private void configureWorker(long taskID)\n \t\t\tCodegenUtils.getClassSync(e.getKey(), e.getValue());\n \t\n \t\t//parse and setup parfor body program\n-\t\tParForBody body = ProgramConverter.parseParForBody(_prog, IntUtils.toInt(_workerID), true);\n+\t\tParForBody body = ProgramConverter.parseParForBody(_prog, (int)_workerID, true);\n \t\t_childBlocks = body.getChildBlocks();\n \t\t_ec          = body.getEc();\n \t\t_resultVars  = body.getResultVariables();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParForSparkWorker.java",
                "sha": "fd0b9eb1a90e853051ed05cca4246158b1b89a0c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParWorkerMapper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParWorkerMapper.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParWorkerMapper.java",
                "patch": "@@ -42,7 +42,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.LocalFileUtils;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n /**\n@@ -163,7 +162,7 @@ public void configure(JobConf job)\n \t\t\t\t\n \t\t\t\t//create local runtime program\n \t\t\t\tString in = MRJobConfiguration.getProgramBlocks(job);\n-\t\t\t\tParForBody body = ProgramConverter.parseParForBody(in, IntUtils.toInt(_workerID));\n+\t\t\t\tParForBody body = ProgramConverter.parseParForBody(in, (int)_workerID);\n \t\t\t\t_childBlocks = body.getChildBlocks();\n \t\t\t\t_ec          = body.getEc();\n \t\t\t\t_resultVars  = body.getResultVariables();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/RemoteParWorkerMapper.java",
                "sha": "7db5bcdf1c8c6e367ab01ac05157a9a44c3b41a1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalFile.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalFile.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalFile.java",
                "patch": "@@ -60,7 +60,6 @@\n import org.apache.sysml.runtime.util.FastStringTokenizer;\n import org.apache.sysml.runtime.util.LocalFileUtils;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * \n@@ -707,8 +706,8 @@ private void createBinaryBlockResultFile( String fnameStaging, String fnameStagi\n \t\t\t\t\t}\n \t\t\t\t\telse {\n \t\t\t\t\t\t//NOTE: whenever runtime does not need all blocks anymore, this can be removed\n-\t\t\t\t\t\tint maxRow = IntUtils.toInt(((brow-1)*brlen + brlen < rlen) ? brlen : rlen - (brow-1)*brlen);\n-\t\t\t\t\t\tint maxCol = IntUtils.toInt(((bcol-1)*bclen + bclen < clen) ? bclen : clen - (bcol-1)*bclen);\n+\t\t\t\t\t\tint maxRow = (int)(((brow-1)*brlen + brlen < rlen) ? brlen : rlen - (brow-1)*brlen);\n+\t\t\t\t\t\tint maxCol = (int)(((bcol-1)*bclen + bclen < clen) ? bclen : clen - (bcol-1)*bclen);\n \t\t\t\t\t\tmb = new MatrixBlock(maxRow, maxCol, true);\n \t\t\t\t\t}\n \t\t\t\t\t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalFile.java",
                "sha": "4f7fac5f4bdb983cfd051c87dd828500bd2500d4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalMemory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalMemory.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalMemory.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.util.DataConverter;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Local in-memory realization of result merge. If the resulting matrix is\n@@ -166,8 +165,8 @@ public MatrixObject executeParallelMerge( int par )\n \t\t\t\t//NOTE: always in dense representation in order to allow for parallel unsynchronized access \n \t\t\t\tlong rows = outMB.getNumRows();\n \t\t\t\tlong cols = outMB.getNumColumns();\n-\t\t\t\tMatrixBlock outMBNew = new MatrixBlock(IntUtils.toInt(rows), IntUtils.toInt(cols), false);\n-\t\t\t\toutMBNew.allocateDenseBlockUnsafe(IntUtils.toInt(rows), IntUtils.toInt(cols));\n+\t\t\t\tMatrixBlock outMBNew = new MatrixBlock((int)rows, (int)cols, false);\n+\t\t\t\toutMBNew.allocateDenseBlockUnsafe((int)rows, (int)cols);\n \t\t\t\t\n \t\t\t\t//create compare matrix if required (existing data in result)\n \t\t\t\t_compare = getCompareMatrix(outMB);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeLocalMemory.java",
                "sha": "c76b3f9a42c37452d2ad1c17656d28748654422e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteMR.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteMR.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteMR.java",
                "patch": "@@ -50,7 +50,6 @@\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n import org.apache.sysml.runtime.util.LocalFileUtils;\n import org.apache.sysml.runtime.util.MapReduceTool;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n /**\n@@ -257,7 +256,7 @@ else if ( oi == OutputInfo.BinaryBlockOutputInfo )\n \t\t    \treducerGroups = Math.max(rlen/brlen,1) * Math.max(clen/bclen, 1); \n \t\t    else //textcell/binarycell\n \t\t    \treducerGroups = Math.max((rlen*clen)/StagingFileUtils.CELL_BUFFER_SIZE, 1);\n-\t\t\tjob.setNumReduceTasks( IntUtils.toInt(Math.min( _numReducers, reducerGroups) )); \t\n+\t\t\tjob.setNumReduceTasks( (int)Math.min( _numReducers, reducerGroups) ); \t\n \n \t\t\t\n \t\t\t//disable automatic tasks timeouts and speculative task exec",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteMR.java",
                "sha": "2884c32911cb94a827d490d37ae3673221e1afd5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemotePartitioning.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemotePartitioning.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemotePartitioning.java",
                "patch": "@@ -25,7 +25,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.data.TaggedMatrixBlock;\n import org.apache.sysml.runtime.matrix.mapred.MRJobConfiguration;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ResultMergeRemotePartitioning implements Partitioner<ResultMergeTaggedMatrixIndexes, TaggedMatrixBlock> \n {\n@@ -46,7 +45,7 @@ public int getPartition(ResultMergeTaggedMatrixIndexes key, TaggedMatrixBlock va\n     \t//the assumption that there is no sparsity skew between blocks.\n     \t\n     \tMatrixIndexes ix = key.getIndexes();\n-    \tint blockid = IntUtils.toInt(ix.getRowIndex() * _numColBlocks + ix.getColumnIndex());\n+    \tint blockid = (int) (ix.getRowIndex() * _numColBlocks + ix.getColumnIndex());\n     \tint partition = blockid % numPartitions;\n     \t\n         //int hash = key.getIndexes().hashCode();\n@@ -60,7 +59,7 @@ public void configure(JobConf job)\n \t{\n \t\tlong[] tmp = MRJobConfiguration.getResultMergeMatrixCharacteristics( job );\n \t\tlong clen = tmp[1]; \n-\t\tint bclen = IntUtils.toInt(tmp[3]);\n+\t\tint bclen = (int) tmp[3];\n \t\t_numColBlocks = clen/bclen + ((clen%bclen!=0)? 1 : 0);\n \t}\n }",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemotePartitioning.java",
                "sha": "93c39b09552323004f3903b8f0660479301147cc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteSpark.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteSpark.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteSpark.java",
                "patch": "@@ -41,7 +41,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public class ResultMergeRemoteSpark extends ResultMerge\n@@ -121,7 +120,7 @@ protected RDDObject executeMerge(MatrixObject compare, MatrixObject[] inputs, lo\n \t\tRDDObject ret = null;\n \t\t\n \t\t//determine degree of parallelism\n-\t\tint numRed = IntUtils.toInt(determineNumReducers(rlen, clen, brlen, bclen, _numReducers));\n+\t\tint numRed = (int)determineNumReducers(rlen, clen, brlen, bclen, _numReducers);\n \t\t\n \t\t//sanity check for empty src files\n \t\tif( inputs == null || inputs.length==0  )\n@@ -198,7 +197,7 @@ protected RDDObject executeMerge(MatrixObject compare, MatrixObject[] inputs, lo\n \tprivate static int determineNumReducers(long rlen, long clen, int brlen, int bclen, long numRed) {\n \t\t//set the number of mappers and reducers \n \t\tlong reducerGroups = Math.max(rlen/brlen,1) * Math.max(clen/bclen, 1);\n-\t\treturn IntUtils.toInt(Math.min( numRed, reducerGroups ));\n+\t\treturn (int)Math.min( numRed, reducerGroups );\n \t}\n \t\n \t@SuppressWarnings(\"unchecked\")",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/ResultMergeRemoteSpark.java",
                "sha": "8055c2ba8b6641cf5365ecb87b3a365a4815dd5f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/TaskPartitionerStatic.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/TaskPartitionerStatic.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/controlprogram/parfor/TaskPartitionerStatic.java",
                "patch": "@@ -20,7 +20,6 @@\n package org.apache.sysml.runtime.controlprogram.parfor;\n \n import org.apache.sysml.runtime.instructions.cp.IntObject;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * This static task partitioner virtually iterates over the given FOR loop (from, to, incr),\n@@ -36,6 +35,6 @@ public TaskPartitionerStatic( long taskSize, int numThreads, String iterVarName,\n \t\tsuper(taskSize, iterVarName, fromVal, toVal, incrVal);\n \t\n \t\t_taskSize = _numIter / numThreads;\n-\t\t_firstnPlus1 = IntUtils.toInt(_numIter % numThreads);\n+\t\t_firstnPlus1 = (int)_numIter % numThreads;\n \t}\t\n }",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/controlprogram/parfor/TaskPartitionerStatic.java",
                "sha": "9f68768d0b86407be392370c0526134361fe92a2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/CPInstructionParser.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/CPInstructionParser.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 1,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/CPInstructionParser.java",
                "patch": "@@ -393,13 +393,15 @@ public static CPInstruction parseSingleInstruction ( CPType cptype, String str )\n \t\t\tcase Builtin: \n \t\t\t\tString []parts = InstructionUtils.getInstructionPartsWithValueType(str);\n \t\t\t\tif ( parts[0].equals(\"log\") || parts[0].equals(\"log_nz\") ) {\n-\t\t\t\t\tif ( parts.length == 3 || (parts.length == 4 &&\n+\t\t\t\t\tif ( parts.length == 3 || (parts.length == 5 &&\n \t\t\t\t\t\tUtilFunctions.isIntegerNumber(parts[3])) ) {\n \t\t\t\t\t\t// B=log(A), y=log(x)\n \t\t\t\t\t\treturn UnaryCPInstruction.parseInstruction(str);\n \t\t\t\t\t} else if ( parts.length == 4 ) {\n \t\t\t\t\t\t// B=log(A,10), y=log(x,10)\n \t\t\t\t\t\treturn BinaryCPInstruction.parseInstruction(str);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tthrow new DMLRuntimeException(\"Error parsing the instruction: \" + str);\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/CPInstructionParser.java",
                "sha": "ea01dc22cf6d2c2f115331108e1a81add4ba9ca4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/SPInstructionParser.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/SPInstructionParser.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 1,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/SPInstructionParser.java",
                "patch": "@@ -73,6 +73,7 @@\n import org.apache.sysml.runtime.instructions.spark.RmmSPInstruction;\n import org.apache.sysml.runtime.instructions.spark.SPInstruction;\n import org.apache.sysml.runtime.instructions.spark.SPInstruction.SPType;\n+import org.apache.sysml.runtime.util.UtilFunctions;\n import org.apache.sysml.runtime.instructions.spark.SpoofSPInstruction;\n import org.apache.sysml.runtime.instructions.spark.CtableSPInstruction;\n import org.apache.sysml.runtime.instructions.spark.Tsmm2SPInstruction;\n@@ -405,12 +406,15 @@ public static SPInstruction parseSingleInstruction ( SPType sptype, String str )\n \t\t\tcase Builtin: \n \t\t\t\tparts = InstructionUtils.getInstructionPartsWithValueType(str);\n \t\t\t\tif ( parts[0].equals(\"log\") || parts[0].equals(\"log_nz\") ) {\n-\t\t\t\t\tif ( parts.length == 3 ) {\n+\t\t\t\t\tif ( parts.length == 3 || (parts.length == 5 &&\n+\t\t\t\t\t\t\tUtilFunctions.isIntegerNumber(parts[3])) ) {\n \t\t\t\t\t\t// B=log(A), y=log(x)\n \t\t\t\t\t\treturn UnaryMatrixSPInstruction.parseInstruction(str);\n \t\t\t\t\t} else if ( parts.length == 4 ) {\n \t\t\t\t\t\t// B=log(A,10), y=log(x,10)\n \t\t\t\t\t\treturn BinarySPInstruction.parseInstruction(str);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tthrow new DMLRuntimeException(\"Error parsing the instruction: \" + str);\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/SPInstructionParser.java",
                "sha": "036320c2d91ffcca3cce3814f62cade2d8bf3b1f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/CentralMomentCPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/CentralMomentCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/CentralMomentCPInstruction.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.operators.CMOperator;\n import org.apache.sysml.runtime.matrix.operators.CMOperator.AggregateOperationTypes;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CentralMomentCPInstruction extends AggregateUnaryCPInstruction {\n \n@@ -105,7 +104,7 @@ public void processInstruction( ExecutionContext ec ) {\n \t\t\n \t\tCMOperator cm_op = ((CMOperator)_optr); \n \t\tif ( cm_op.getAggOpType() == AggregateOperationTypes.INVALID )\n-\t\t\tcm_op = cm_op.setCMAggOp(IntUtils.toInt(order.getLongValue()));\n+\t\t\tcm_op = cm_op.setCMAggOp((int)order.getLongValue());\n \t\t\n \t\tCM_COV_Object cmobj = null; \n \t\tif (input3 == null ) {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/CentralMomentCPInstruction.java",
                "sha": "4f13f1f3f48d4ef2387900099b155045e6593524",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/CtableCPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/CtableCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/CtableCPInstruction.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.sysml.runtime.matrix.operators.SimpleOperator;\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.LongLongDoubleHashMap.EntryType;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CtableCPInstruction extends ComputationCPInstruction {\n \tprivate final String _outDim1;\n@@ -111,7 +110,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t//only create result block if dense; it is important not to aggregate on sparse result\n \t\t\t//blocks because it would implicitly turn the O(N) algorithm into O(N log N). \n \t\t\tif( !sparse )\n-\t\t\t\tresultBlock = new MatrixBlock(IntUtils.toInt(outputDim1), IntUtils.toInt(outputDim2), false); \n+\t\t\t\tresultBlock = new MatrixBlock((int)outputDim1, (int)outputDim2, false); \n \t\t}\n \t\tif( _isExpand ){\n \t\t\tresultBlock = new MatrixBlock( matBlock1.getNumRows(), Integer.MAX_VALUE, true );\n@@ -165,7 +164,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t//we need to respect potentially specified output dimensions here, because we might have \n \t\t\t//decided for hash-aggregation just to prevent inefficiency in case of sparse outputs.  \n \t\t\tif( outputDimsKnown )\n-\t\t\t\tresultBlock = DataConverter.convertToMatrixBlock( resultMap, IntUtils.toInt(outputDim1), IntUtils.toInt(outputDim2) );\n+\t\t\t\tresultBlock = DataConverter.convertToMatrixBlock( resultMap, (int)outputDim1, (int)outputDim2 );\n \t\t\telse\n \t\t\t\tresultBlock = DataConverter.convertToMatrixBlock( resultMap );\n \t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/CtableCPInstruction.java",
                "sha": "7b907b3f502c34a0ee67f3922dbcc2795193ebd3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/DataGenCPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/DataGenCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/DataGenCPInstruction.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.sysml.runtime.matrix.data.RandomMatrixGenerator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class DataGenCPInstruction extends UnaryCPInstruction {\n \n@@ -218,7 +217,7 @@ public void processInstruction( ExecutionContext ec )\n \t\t\t\tLOG.trace(\"Process DataGenCPInstruction rand with seed = \"+lSeed+\".\");\n \t\t\t\n \t\t\tRandomMatrixGenerator rgen = LibMatrixDatagen.createRandomMatrixGenerator(\n-\t\t\t\tpdf, IntUtils.toInt( lrows ), IntUtils.toInt( lcols ), rowsInBlock, colsInBlock, sparsity, minValue, maxValue, pdfParams);\n+\t\t\t\tpdf, (int) lrows, (int) lcols, rowsInBlock, colsInBlock, sparsity, minValue, maxValue, pdfParams);\n \t\t\tsoresBlock = MatrixBlock.randOperations(rgen, seed, numThreads);\n \t\t}\n \t\telse if ( method == DataGenMethod.SEQ ) \n@@ -247,7 +246,7 @@ else if ( method == DataGenMethod.SAMPLE )\n \t\t\tif ( range < lrows && !replace )\n \t\t\t\tthrow new DMLRuntimeException(\"Sample (size=\" + lrows + \") larger than population (size=\" + range + \") can only be generated with replacement.\");\n \t\t\t\n-\t\t\tsoresBlock = MatrixBlock.sampleOperations(range, IntUtils.toInt(lrows), replace, seed);\n+\t\t\tsoresBlock = MatrixBlock.sampleOperations(range, (int)lrows, replace, seed);\n \t\t}\n \t\t\n \t\t//guarded sparse block representation change",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/DataGenCPInstruction.java",
                "sha": "7e0d5c1b8760fa3037fcc9b808721de3d0e4d964",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/DataPartitionCPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/DataPartitionCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/DataPartitionCPInstruction.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class DataPartitionCPInstruction extends UnaryCPInstruction {\n \n@@ -75,10 +74,10 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t//write matrix partitions to hdfs\n \t\t\tWriterBinaryBlock.writePartitionedBinaryBlockMatrixToHDFS(new Path(fname), \n \t\t\t\tnew JobConf(ConfigurationManager.getCachedJobConf()), mb, moIn.getNumRows(), moIn.getNumColumns(),\n-\t\t\t\tIntUtils.toInt(moIn.getNumRowsPerBlock()), IntUtils.toInt(moIn.getNumColumnsPerBlock()), _pformat);\n+\t\t\t\t(int)moIn.getNumRowsPerBlock(), (int)moIn.getNumColumnsPerBlock(), _pformat);\n \t\t\t\n \t\t\t//ensure correctness of output characteristics (required if input unknown during compile and no recompile)\n-\t\t\tMatrixCharacteristics mc = new MatrixCharacteristics(moIn.getNumRows(), moIn.getNumColumns(), IntUtils.toInt(moIn.getNumRowsPerBlock()), IntUtils.toInt(moIn.getNumColumnsPerBlock()), moIn.getNnz()); \n+\t\t\tMatrixCharacteristics mc = new MatrixCharacteristics(moIn.getNumRows(), moIn.getNumColumns(), (int)moIn.getNumRowsPerBlock(), (int)moIn.getNumColumnsPerBlock(), moIn.getNnz()); \n \t\t\tMetaDataFormat meta = new MetaDataFormat(mc, OutputInfo.BinaryBlockOutputInfo, InputInfo.BinaryBlockInputInfo);\n \t\t\tmoOut.setMetaData(meta);\n \t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/DataPartitionCPInstruction.java",
                "sha": "e7caa68dedc4ef175f160da18f8b872901c8ee87",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/DnnCPInstruction.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/DnnCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 6,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/DnnCPInstruction.java",
                "patch": "@@ -33,7 +33,6 @@\n import org.apache.sysml.runtime.matrix.data.LibMatrixNative;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.util.DnnUtils;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.NativeHelper;\n \n public class DnnCPInstruction extends UnaryCPInstruction {\n@@ -268,8 +267,8 @@ else if (opcode.equalsIgnoreCase(\"batch_norm2d_backward\")) {\n \t}\n \n \tprivate static int getScalarInput(ExecutionContext ec, ArrayList<CPOperand> aL, int index) {\n-\t\treturn IntUtils.toInt( ec.getScalarInput(aL.get(index).getName(),\n-\t\t\taL.get(index).getValueType(), aL.get(index).isLiteral()).getLongValue());\n+\t\treturn (int) ec.getScalarInput(aL.get(index).getName(),\n+\t\t\taL.get(index).getValueType(), aL.get(index).isLiteral()).getLongValue();\n \t}\n \t\n \tpublic void processReluBackwardInstruction(ExecutionContext ec) {\n@@ -455,8 +454,8 @@ else if (instOpcode.equalsIgnoreCase(\"batch_norm2d_backward\")) {\n \t\t\n \t\tint R = getScalarInput(ec, _filter_shape, 2);\n \t\tint S = getScalarInput(ec, _filter_shape, 3);\n-\t\tint P = IntUtils.toInt( DnnUtils.getP(H, R, stride_h, pad_h) );\n-\t\tint Q = IntUtils.toInt( DnnUtils.getQ(W, S, stride_w, pad_w) );\n+\t\tint P = (int) DnnUtils.getP(H, R, stride_h, pad_h);\n+\t\tint Q = (int) DnnUtils.getQ(W, S, stride_w, pad_w);\n \t\t\n \t\tDnnParameters params = new DnnParameters(N, C, H, W, K, R, S, stride_h, stride_w, pad_h, pad_w, _numThreads);\n \t\tparams.enableNative = NativeHelper.isNativeLibraryLoaded();\n@@ -592,7 +591,7 @@ else if (instOpcode.equalsIgnoreCase(\"conv2d_backward_data\")) {\n \tprivate void resetNumThreads(DnnParameters params, int numRows, int numCols, double sparsity) {\n \t\tif(ConfigurationManager.isGPU()) {\n \t\t\tdouble memBudget1Thread = OptimizerUtils.estimateSizeExactSparsity(numRows, numCols, sparsity);\n-\t\t\tint limitedDegreeOfParallelism = IntUtils.toInt( Math.floor(_intermediateMemoryBudget / memBudget1Thread) );\n+\t\t\tint limitedDegreeOfParallelism = (int) Math.floor(_intermediateMemoryBudget / memBudget1Thread);\n \t\t\tif(params.numThreads > limitedDegreeOfParallelism) {\n \t\t\t\tparams.numThreads = limitedDegreeOfParallelism;\n \t\t\t\tif(!warnedUnderUtilitization)",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/DnnCPInstruction.java",
                "sha": "7bed33f8e5679608049106517ed04da8d48439f2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/FrameIndexingCPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/FrameIndexingCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/FrameIndexingCPInstruction.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.apache.sysml.runtime.controlprogram.context.ExecutionContext;\n import org.apache.sysml.runtime.matrix.data.FrameBlock;\n import org.apache.sysml.runtime.util.IndexRange;\n-import org.apache.sysml.utils.IntUtils;\n \n public final class FrameIndexingCPInstruction extends IndexingCPInstruction {\n \n@@ -72,7 +71,7 @@ else if ( opcode.equalsIgnoreCase(LeftIndex.OPCODE)) {\n \t\t\t\t\tthrow new DMLRuntimeException(\"Invalid index range of scalar leftindexing: \"+ixrange.toString()+\".\" );\n \t\t\t\tScalarObject scalar = ec.getScalarInput(input2);\n \t\t\t\tout = new FrameBlock(lin);\n-\t\t\t\tout.set(IntUtils.toInt(ixrange.rowStart), IntUtils.toInt(ixrange.colStart), scalar.getStringValue());\n+\t\t\t\tout.set((int)ixrange.rowStart, (int)ixrange.colStart, scalar.getStringValue());\n \t\t\t}\n \n \t\t\t//unpin lhs input",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/FrameIndexingCPInstruction.java",
                "sha": "52e05b67e92689e9313d32ac6e865b36f99ad261",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/IndexingCPInstruction.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/IndexingCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 5,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/IndexingCPInstruction.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.apache.sysml.runtime.controlprogram.context.ExecutionContext;\n import org.apache.sysml.runtime.instructions.InstructionUtils;\n import org.apache.sysml.runtime.util.IndexRange;\n-import org.apache.sysml.utils.IntUtils;\n \n public abstract class IndexingCPInstruction extends UnaryCPInstruction {\n \tprotected final CPOperand rowLower, rowUpper, colLower, colUpper;\n@@ -51,10 +50,10 @@ protected IndexingCPInstruction(CPOperand lhsInput, CPOperand rhsInput, CPOperan\n \n \tprotected IndexRange getIndexRange(ExecutionContext ec) {\n \t\treturn new IndexRange( //rl, ru, cl, ru\n-\t\t\tIntUtils.toInt(ec.getScalarInput(rowLower).getLongValue()-1),\n-\t\t\tIntUtils.toInt(ec.getScalarInput(rowUpper).getLongValue()-1),\n-\t\t\tIntUtils.toInt(ec.getScalarInput(colLower).getLongValue()-1),\n-\t\t\tIntUtils.toInt(ec.getScalarInput(colUpper).getLongValue()-1));\n+\t\t\t(int)(ec.getScalarInput(rowLower).getLongValue()-1),\n+\t\t\t(int)(ec.getScalarInput(rowUpper).getLongValue()-1),\n+\t\t\t(int)(ec.getScalarInput(colLower).getLongValue()-1),\n+\t\t\t(int)(ec.getScalarInput(colUpper).getLongValue()-1));\n \t}\n \n \tpublic static IndexingCPInstruction parseInstruction ( String str ) {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/IndexingCPInstruction.java",
                "sha": "515084721658ed204c99ebff21f54bf79ca2557e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ListIndexingCPInstruction.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/ListIndexingCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 5,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/ListIndexingCPInstruction.java",
                "patch": "@@ -25,7 +25,6 @@\n import org.apache.sysml.runtime.DMLRuntimeException;\n import org.apache.sysml.runtime.controlprogram.caching.CacheableData;\n import org.apache.sysml.runtime.controlprogram.context.ExecutionContext;\n-import org.apache.sysml.utils.IntUtils;\n \n public final class ListIndexingCPInstruction extends IndexingCPInstruction {\n \n@@ -56,7 +55,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t}\n \t\t\telse {\n \t\t\t\tec.setVariable(output.getName(),\n-\t\t\t\t\tlist.slice(IntUtils.toInt(rl.getLongValue()-1), IntUtils.toInt(ru.getLongValue()-1)));\n+\t\t\t\t\tlist.slice((int)rl.getLongValue()-1, (int)ru.getLongValue()-1));\n \t\t\t}\n \t\t}\n \t\t//left indexing\n@@ -69,22 +68,22 @@ else if ( opcode.equalsIgnoreCase(LeftIndex.OPCODE)) {\n \t\t\t\tif( rl.getValueType()==ValueType.STRING || ru.getValueType()==ValueType.STRING  )\n \t\t\t\t\tec.setVariable(output.getName(), lin.copy().set(rl.getStringValue(), ru.getStringValue(), rin));\n \t\t\t\telse\n-\t\t\t\t\tec.setVariable(output.getName(), lin.copy().set(IntUtils.toInt(rl.getLongValue()-1), IntUtils.toInt(ru.getLongValue()-1), rin));\n+\t\t\t\t\tec.setVariable(output.getName(), lin.copy().set((int)rl.getLongValue()-1, (int)ru.getLongValue()-1, rin));\n \t\t\t}\n \t\t\telse if( input2.getDataType().isScalar() ) { //LIST <- SCALAR\n \t\t\t\tScalarObject scalar = ec.getScalarInput(input2);\n \t\t\t\tif( rl.getValueType()==ValueType.STRING )\n \t\t\t\t\tec.setVariable(output.getName(), lin.copy().set(rl.getStringValue(), scalar));\n \t\t\t\telse\n-\t\t\t\t\tec.setVariable(output.getName(), lin.copy().set(IntUtils.toInt(rl.getLongValue()-1), scalar));\n+\t\t\t\t\tec.setVariable(output.getName(), lin.copy().set((int)rl.getLongValue()-1, scalar));\n \t\t\t}\n \t\t\telse if( input2.getDataType().isMatrix() ) { //LIST <- MATRIX/FRAME\n \t\t\t\tCacheableData<?> dat = ec.getCacheableData(input2);\n \t\t\t\tdat.enableCleanup(false);\n \t\t\t\tif( rl.getValueType()==ValueType.STRING )\n \t\t\t\t\tec.setVariable(output.getName(), lin.copy().set(rl.getStringValue(), dat));\n \t\t\t\telse\n-\t\t\t\t\tec.setVariable(output.getName(), lin.copy().set(IntUtils.toInt(rl.getLongValue()-1), dat));\n+\t\t\t\t\tec.setVariable(output.getName(), lin.copy().set((int)rl.getLongValue()-1, dat));\n \t\t\t}\n \t\t\telse {\n \t\t\t\tthrow new DMLRuntimeException(\"Unsupported list \"",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ListIndexingCPInstruction.java",
                "sha": "9e53700b5e4b5112c179baf7772ecbd0f4a2150a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ListObject.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/ListObject.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/ListObject.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.sysml.parser.Expression.ValueType;\n import org.apache.sysml.runtime.DMLRuntimeException;\n import org.apache.sysml.runtime.controlprogram.caching.CacheableData;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ListObject extends Data {\n \tprivate static final long serialVersionUID = 3652422061598967358L;\n@@ -53,8 +52,8 @@ public ListObject(List<Data> data, List<String> names, ValueType vt) {\n \t\tsuper(DataType.LIST, vt);\n \t\t_data = data;\n \t\t_names = names;\n-\t\t_nCacheable = IntUtils.toInt( data.stream().filter(\n-\t\t\td -> d instanceof CacheableData).count() );\n+\t\t_nCacheable = (int) data.stream().filter(\n+\t\t\td -> d instanceof CacheableData).count();\n \t}\n \t\n \tpublic ListObject(ListObject that) {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ListObject.java",
                "sha": "576e57c00cd12686001cd7477030df3bd9796e96",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixIndexingCPInstruction.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixIndexingCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixIndexingCPInstruction.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.sysml.runtime.controlprogram.context.ExecutionContext;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.util.IndexRange;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public final class MatrixIndexingCPInstruction extends IndexingCPInstruction {\n@@ -65,8 +64,8 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t\t//execute right indexing operation (with shallow row copies for range\n \t\t\t\t//of entire sparse rows, which is safe due to copy on update)\n \t\t\t\tMatrixBlock matBlock = ec.getMatrixInput(input1.getName(), getExtendedOpcode());\n-\t\t\t\tresultBlock = matBlock.slice(IntUtils.toInt(ixrange.rowStart), IntUtils.toInt(ixrange.rowEnd), \n-\t\t\t\t\t\tIntUtils.toInt(ixrange.colStart), IntUtils.toInt(ixrange.colEnd), false, new MatrixBlock());\n+\t\t\t\tresultBlock = matBlock.slice((int)ixrange.rowStart, (int)ixrange.rowEnd, \n+\t\t\t\t\t(int)ixrange.colStart, (int)ixrange.colEnd, false, new MatrixBlock());\n \t\t\t\t\n \t\t\t\t//unpin rhs input\n \t\t\t\tec.releaseMatrixInput(input1.getName(), getExtendedOpcode());\n@@ -102,7 +101,7 @@ else if ( opcode.equalsIgnoreCase(LeftIndex.OPCODE))\n \t\t\t\t\tthrow new DMLRuntimeException(\"Invalid index range of scalar leftindexing: \"+ixrange.toString()+\".\" );\n \t\t\t\tScalarObject scalar = ec.getScalarInput(input2.getName(), ValueType.DOUBLE, input2.isLiteral());\n \t\t\t\tresultBlock = (MatrixBlock) matBlock.leftIndexingOperations(scalar, \n-\t\t\t\t\t\tIntUtils.toInt(ixrange.rowStart), IntUtils.toInt(ixrange.colStart), new MatrixBlock(), updateType);\n+\t\t\t\t\t(int)ixrange.rowStart, (int)ixrange.colStart, new MatrixBlock(), updateType);\n \t\t\t}\n \n \t\t\t//unpin lhs input",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixIndexingCPInstruction.java",
                "sha": "34a747638323dbdfb9a5e78274df5fb1b18b4717",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixReshapeCPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixReshapeCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixReshapeCPInstruction.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.apache.sysml.runtime.matrix.data.LibMatrixReorg;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixReshapeCPInstruction extends UnaryCPInstruction {\n \n@@ -61,8 +60,8 @@ public static MatrixReshapeCPInstruction parseInstruction ( String str ) {\n \tpublic void processInstruction(ExecutionContext ec) {\n \t\t//get inputs\n \t\tMatrixBlock in = ec.getMatrixInput(input1.getName(), getExtendedOpcode());\n-\t\tint rows = IntUtils.toInt(ec.getScalarInput(_opRows).getLongValue()); //save cast\n-\t\tint cols = IntUtils.toInt(ec.getScalarInput(_opCols).getLongValue()); //save cast\n+\t\tint rows = (int)ec.getScalarInput(_opRows).getLongValue(); //save cast\n+\t\tint cols = (int)ec.getScalarInput(_opCols).getLongValue(); //save cast\n \t\tBooleanObject byRow = (BooleanObject) ec.getScalarInput(_opByRow.getName(), ValueType.BOOLEAN, _opByRow.isLiteral());\n \n \t\t//execute operations ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/MatrixReshapeCPInstruction.java",
                "sha": "1f7b0531b771a8461a29e36c88b8e299c33d14ab",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/PMMJCPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/PMMJCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/PMMJCPInstruction.java",
                "patch": "@@ -24,7 +24,6 @@\n import org.apache.sysml.runtime.instructions.InstructionUtils;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class PMMJCPInstruction extends ComputationCPInstruction {\n \n@@ -56,7 +55,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t//get inputs\n \t\tMatrixBlock matBlock1 = ec.getMatrixInput(input1.getName(), getExtendedOpcode());\n \t\tMatrixBlock matBlock2 = ec.getMatrixInput(input2.getName(), getExtendedOpcode());\n-\t\tint rlen = IntUtils.toInt(ec.getScalarInput(input3.getName(), input3.getValueType(), input3.isLiteral()).getLongValue());\n+\t\tint rlen = (int)ec.getScalarInput(input3.getName(), input3.getValueType(), input3.isLiteral()).getLongValue();\n \t\t//execute operations\n \t\tMatrixBlock ret = new MatrixBlock(rlen, matBlock2.getNumColumns(), matBlock2.isInSparseFormat());\n \t\tmatBlock1.permutationMatrixMultOperations(matBlock2, ret, null, _numThreads);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/PMMJCPInstruction.java",
                "sha": "93742dd29fda6b4d20d8a4c758514fad955d69be",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java",
                "patch": "@@ -50,7 +50,6 @@\n import org.apache.sysml.runtime.transform.encode.EncoderFactory;\n import org.apache.sysml.runtime.transform.meta.TfMetaUtils;\n import org.apache.sysml.runtime.util.DataConverter;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ParameterizedBuiltinCPInstruction extends ComputationCPInstruction {\n \tprivate static final int TOSTRING_MAXROWS = 100;\n@@ -176,7 +175,7 @@ else if ( opcode.equalsIgnoreCase(\"groupedagg\") ) {\n \t\t\t\n \t\t\tint ngroups = -1;\n \t\t\tif ( params.get(Statement.GAGG_NUM_GROUPS) != null) {\n-\t\t\t\tngroups = IntUtils.toInt( Double.parseDouble(params.get(Statement.GAGG_NUM_GROUPS)));\n+\t\t\t\tngroups = (int) Double.parseDouble(params.get(Statement.GAGG_NUM_GROUPS));\n \t\t\t}\n \t\t\t\n \t\t\t// compute the result",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java",
                "sha": "8b76ad790a6f738e1efb5cb5eb5c1941138f39a3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ParamservBuiltinCPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/ParamservBuiltinCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/ParamservBuiltinCPInstruction.java",
                "patch": "@@ -76,7 +76,6 @@\n import org.apache.sysml.runtime.controlprogram.parfor.stat.Timing;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.ProgramConverter;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public class ParamservBuiltinCPInstruction extends ParameterizedBuiltinCPInstruction {\n@@ -287,7 +286,7 @@ private int getEpochs() {\n \t}\n \n \tprivate int getParLevel(int workerNum) {\n-\t\treturn Math.max(IntUtils.toInt(Math.ceil((double)getRemainingCores()/workerNum)), 1);\n+\t\treturn Math.max((int)Math.ceil((double)getRemainingCores()/workerNum), 1);\n \t}\n \n \tprivate PSUpdateType getUpdateType() {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ParamservBuiltinCPInstruction.java",
                "sha": "bc3ca2463040f2538ecf50e754592757e562777b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ReorgCPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/ReorgCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/ReorgCPInstruction.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.runtime.util.DataConverter;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ReorgCPInstruction extends UnaryCPInstruction {\n \t// sort-specific attributes (to enable variable attributes)\n@@ -131,7 +130,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\tif( r_op.fn instanceof SortIndex ) {\n \t\t\t//additional attributes for sort\n \t\t\tint[] cols = _col.getDataType().isMatrix() ? DataConverter.convertToIntVector(ec.getMatrixInput(_col.getName())) :\n-\t\t\t\tnew int[]{IntUtils.toInt(ec.getScalarInput(_col).getLongValue())};\n+\t\t\t\tnew int[]{(int)ec.getScalarInput(_col).getLongValue()};\n \t\t\tboolean desc = ec.getScalarInput(_desc).getBooleanValue();\n \t\t\tboolean ixret = ec.getScalarInput(_ixret).getBooleanValue();\n \t\t\tr_op = r_op.setFn(new SortIndex(cols, desc, ixret));",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/ReorgCPInstruction.java",
                "sha": "5eeda8a90df19492b646b09a1422623081f54cd7",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/StringInitCPInstruction.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/cp/StringInitCPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/cp/StringInitCPInstruction.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.instructions.InstructionUtils;\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class StringInitCPInstruction extends UnaryCPInstruction {\n \tpublic static final String DELIM = \" \";\n@@ -73,7 +72,7 @@ public static StringInitCPInstruction parseInstruction(String str) {\n \tpublic void processInstruction( ExecutionContext ec ) {\n \t\t//setup output matrix\n \t\tString outName = output.getName();\n-\t\tMatrixBlock outBlk = new MatrixBlock(IntUtils.toInt(_rlen), IntUtils.toInt(_clen), false);\n+\t\tMatrixBlock outBlk = new MatrixBlock((int)_rlen, (int)_clen, false);\n \t\t\n \t\t//init tokenizer \n \t\tStringTokenizer st = new StringTokenizer(_data, DELIM);\n@@ -87,8 +86,8 @@ public void processInstruction( ExecutionContext ec ) {\n \t\tfor( int i=0; i<len; i++ ){\n \t\t\tString sval = st.nextToken();\n \t\t\tDouble dval = Double.parseDouble(sval);\n-\t\t\tint rix = IntUtils.toInt(i / _clen);\n-\t\t\tint cix = IntUtils.toInt(i % _clen);\n+\t\t\tint rix = (int) (i / _clen);\n+\t\t\tint cix = (int) (i % _clen);\n \t\t\toutBlk.quickSetValue(rix, cix, dval);\n \t\t}\n \t\t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/cp/StringInitCPInstruction.java",
                "sha": "2e82ab244323b74af5364613d7711cb0f6b00912",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateBinaryGPUInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateBinaryGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateBinaryGPUInstruction.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class AggregateBinaryGPUInstruction extends GPUInstruction {\n \tprivate CPOperand _input1 = null;\n@@ -79,8 +78,8 @@ public void processInstruction(ExecutionContext ec) {\n \t\tMatrixObject m1 = getMatrixInputForGPUInstruction(ec, _input1.getName());\n \t\tMatrixObject m2 = getMatrixInputForGPUInstruction(ec, _input2.getName());\n \t\t//compute matrix multiplication\n-\t\tint rlen = IntUtils.toInt(_isLeftTransposed ? m1.getNumColumns() : m1.getNumRows());\n-\t\tint clen = IntUtils.toInt(_isRightTransposed ? m2.getNumRows() : m2.getNumColumns());\n+\t\tint rlen = (int) (_isLeftTransposed ? m1.getNumColumns() : m1.getNumRows());\n+\t\tint clen = (int) (_isRightTransposed ? m2.getNumRows() : m2.getNumColumns());\n \t\tec.setMetaData(_output.getName(), rlen, clen);\n \t\tLibMatrixCuMatMult.matmult(ec, ec.getGPUContext(0), getExtendedOpcode(), m1, m2, _output.getName(), _isLeftTransposed, _isRightTransposed);\n \t\t//release inputs/outputs",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateBinaryGPUInstruction.java",
                "sha": "1d8f7fe88b91ba041daf0f6aec1c9d40af7b94b6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateUnaryGPUInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateUnaryGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateUnaryGPUInstruction.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateUnaryOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Implements aggregate unary instructions for CUDA\n@@ -78,8 +77,8 @@ public void processInstruction(ExecutionContext ec) {\n     //get inputs\n     MatrixObject in1 = getMatrixInputForGPUInstruction(ec, _input1.getName());\n \n-    int rlen = IntUtils.toInt(in1.getNumRows());\n-    int clen = IntUtils.toInt(in1.getNumColumns());\n+    int rlen = (int)in1.getNumRows();\n+    int clen = (int)in1.getNumColumns();\n \n     IndexFunction indexFunction = ((AggregateUnaryOperator) _optr).indexFn;\n     if (indexFunction instanceof ReduceRow){  // COL{SUM, MAX...}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/AggregateUnaryGPUInstruction.java",
                "sha": "1d5802bc532dadf5ca4fc454cec0903ed0704279",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/DnnGPUInstruction.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/DnnGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 26,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/DnnGPUInstruction.java",
                "patch": "@@ -36,7 +36,6 @@\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.runtime.util.DnnUtils;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class DnnGPUInstruction extends GPUInstruction {\n \t\n@@ -444,8 +443,8 @@ private void processInverseVarianceInstruction(String instOpcode, ExecutionConte\n \t\ttry(GPUDenseInputPointerFetcher fetcher = new GPUDenseInputPointerFetcher(ec, gCtx, instName, _output)) {\n \t\t\tfetcher.add(\"X\", _input1).addScalar(\"eps\", _input2);\n \t\t\t\n-\t\t\tint rows = IntUtils.toInt(fetcher.getInputNumRows(\"X\"));\n-\t\t\tint cols = IntUtils.toInt(fetcher.getInputNumColumns(\"X\"));\n+\t\t\tint rows = LibMatrixCUDA.toInt(fetcher.getInputNumRows(\"X\"));\n+\t\t\tint cols = LibMatrixCUDA.toInt(fetcher.getInputNumColumns(\"X\"));\n \t\t\t\n \t\t\t// invVar(X, C, eps, size);\n \t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"invVar\", \n@@ -517,8 +516,8 @@ private void processEMAInstruction(ExecutionContext ec) {\n \t\t\tfetcher.add(\"ema_mean\", _input1).add(\"mean\", _input2).addScalar(\"mu\", _input3);\n \t\t\tdouble mu = fetcher.getDouble(\"mu\");\n \t\t\t\n-\t\t\tint rows = IntUtils.toInt(fetcher.getInputNumRows(\"ema_mean\"));\n-\t\t\tint cols = IntUtils.toInt(fetcher.getInputNumColumns(\"ema_mean\"));\n+\t\t\tint rows = LibMatrixCUDA.toInt(fetcher.getInputNumRows(\"ema_mean\"));\n+\t\t\tint cols = LibMatrixCUDA.toInt(fetcher.getInputNumColumns(\"ema_mean\"));\n \t\t\t\n \t\t\tfetcher.validateDimensions(\"mean\", rows, cols);\n \t\t\t\n@@ -537,8 +536,8 @@ private void processReshapeColMeansInstruction(ExecutionContext ec) {\n \t\t\tint C = fetcher.getInteger(\"C\");\n \t\t\tint HW = fetcher.getInteger(\"HW\");\n \t\t\tfetcher.validateDimensions(\"X\", -1, C*HW);\n-\t\t\tint rows = IntUtils.toInt(fetcher.getInputNumRows(\"X\"));\n-\t\t\tint cols = IntUtils.toInt(fetcher.getInputNumColumns(\"X\"));\n+\t\t\tint rows = LibMatrixCUDA.toInt(fetcher.getInputNumRows(\"X\"));\n+\t\t\tint cols = LibMatrixCUDA.toInt(fetcher.getInputNumColumns(\"X\"));\n \t\t\t// output = matrix(colMeans(X), rows=C, cols=Hin*Win)\n \t\t\tLibMatrixCUDA.colMeans(gCtx, instName,  \n \t\t\t\t\tfetcher.getInputPointer(\"X\"), \n@@ -566,7 +565,7 @@ private void processUpdateEMAVarInstruction(ExecutionContext ec) {\n \t\t\tPointer subgrp_vars = gCtx.allocate(instName, C*HW*LibMatrixCUDA.sizeOfDataType);\n \t\t\t// subgrp_vars <- colVars(X)\n \t\t\tLibMatrixCUDA.colVars(gCtx, instName, fetcher.getInputPointer(\"X\"), subgrp_vars, \n-\t\t\t\t\tIntUtils.toInt(fetcher.getInputNumRows(\"X\")), C*HW);\n+\t\t\t\t\tLibMatrixCUDA.toInt(fetcher.getInputNumRows(\"X\")), C*HW);\n \t\t\t\n \t\t\t// tmp1 <- rowMeans(subgrp_vars)\n \t\t\tPointer tmp1 = gCtx.allocate(instName, C*LibMatrixCUDA.sizeOfDataType);\n@@ -593,11 +592,11 @@ private void processNesterovUpdateInstruction(ExecutionContext ec) {\n \t\t\tfetcher.add(\"input\", _input1).add(\"v\", _input2).add(\"v_prev\", _input3)\n \t\t\t.addScalar(\"mu\", _input4);\n \t\t\tMatrixObject input = fetcher.getInputMatrixObject(\"input\");\n-\t\t\tint rows = IntUtils.toInt(input.getNumRows());\n-\t\t\tint cols = IntUtils.toInt(input.getNumColumns());\n+\t\t\tint rows = LibMatrixCUDA.toInt(input.getNumRows());\n+\t\t\tint cols = LibMatrixCUDA.toInt(input.getNumColumns());\n \t\t\t\n \t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"update_nesterov_x\", \n-\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(IntUtils.toInt(rows*cols)),\n+\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(LibMatrixCUDA.toInt(rows*cols)),\n \t\t\t\t\tfetcher.getInputPointer(\"input\"), \n \t\t\t\t\tfetcher.getInputPointer(\"v\"),\n \t\t\t\t\tfetcher.getInputPointer(\"v_prev\"),\n@@ -628,21 +627,28 @@ private void processBatchNorm2dBackwardDGammaInstruction(ExecutionContext ec) {\n \t\t\t\tPointer tmp = gCtx.allocate(instName, N*CHW*LibMatrixCUDA.sizeOfDataType);\n \t\t\t\t// jcuda.runtime.JCuda.cudaDeviceSynchronize();\n \t\t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"backward_dgamma_tmp\", \n-\t\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(IntUtils.toInt(N*CHW)),\n+\t\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(LibMatrixCUDA.toInt(N*CHW)),\n \t\t\t\t\t\tfetcher.getInputPointer(\"ema_mean\"), \n \t\t\t\t\t\tfetcher.getInputPointer(\"dout\"),\n \t\t\t\t\t\tfetcher.getInputPointer(\"X\"),\n \t\t\t\t\t\tfetcher.getInputPointer(\"ema_var\"),\n \t\t\t\t\t\ttmp,\n \t\t\t\t\t\t// N, C, HW, CHW, NCHW\n-\t\t\t\t\t\tIntUtils.toInt(N), IntUtils.toInt(C), IntUtils.toInt(HW), IntUtils.toInt(CHW), N*CHW);\n+\t\t\t\t\t\ttoInt(N), toInt(C), toInt(HW), toInt(CHW), N*CHW);\n \t\t\t\t\n \t\t\t\tLibMatrixCUDA.channelSums(gCtx, instName, \n \t\t\t\t\t\ttmp, fetcher.getOutputPointer(C, 1), N, C, HW);\n \t\t\t\tgCtx.cudaFreeHelper(instName, tmp, gCtx.EAGER_CUDA_FREE);\n \t\t\t}\n \t\t}\n \t\n+\tprivate static int toInt(long num) throws DMLRuntimeException {\n+\t\tif(num >= Integer.MAX_VALUE || num <= Integer.MIN_VALUE) {\n+\t\t\tthrow new DMLRuntimeException(\"GPU : Exceeded supported size \" + num);\n+\t\t}\n+\t\treturn (int)num;\n+\t}\n+\t\n \tpublic static long getMemRequiredForCuDNNLSTMBackward(long N, long T, long M, long D, boolean return_sequences) {\n \t\tdouble memRequired = (D+M)*4*M // sysmlWPointer\n \t\t\t\t+ 2*(D+M+2)*(4*M) // cudnnWPointer and cudnnDwPointer\n@@ -666,9 +672,9 @@ private void processLstmBackwardInstruction(ExecutionContext ec) throws DMLRunti\n \t\tlong numRowsW = W.getNumRows();\n \t\tlong D = numRowsW - M; // since W:(D+M, 4M) ... numFeatures\n \t\tMatrixObject X = getMatrixInputForGPUInstruction(ec, _input1.getName());\n-\t\tint N = IntUtils.toInt(X.getNumRows()); // batchSize .. since X:(N, T*D)\n+\t\tint N = toInt(X.getNumRows()); // batchSize .. since X:(N, T*D)\n \t\tlong numColsX = X.getNumColumns();\n-\t\tint T = IntUtils.toInt(numColsX/ D); // since X:(N, T*D) ... seqLength\n+\t\tint T = toInt(numColsX/ D); // since X:(N, T*D) ... seqLength\n \t\tboolean return_sequences = ec.getScalarInput(_input6.getName(), _input6.getValueType(), _input6.isLiteral()).getBooleanValue();\n \t\t\n \t\t// long memRequired = getMemRequiredForCuDNNLSTMBackward(N, T, M, D, return_sequences);\n@@ -697,14 +703,14 @@ private void processLstmBackwardInstruction(ExecutionContext ec) throws DMLRunti\n \t\t\tPointer sysmlBiasPointer = LibMatrixCuDNN.getDensePointerForCuDNN(gCtx, bias, instName, 1, 4*M);\n \t\t\tPointer cudnnWPointer = gCtx.allocate(instName, (D+M+2)*(4*M)*LibMatrixCUDA.sizeOfDataType);\n \t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"prepare_lstm_weight\",\n-\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(IntUtils.toInt((D+M+2)*(4*M))),\n+\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(toInt((D+M+2)*(4*M))),\n \t\t\t\t\tsysmlWPointer, sysmlBiasPointer, cudnnWPointer, D, M);\n \t\t\tec.releaseMatrixInputForGPUInstruction(_input2.getName());\n \t\t\tec.releaseMatrixInputForGPUInstruction(_input3.getName());\n \t\t\tPointer xPointer = LibMatrixCUDA.getDensePointer(gCtx, X, instName); \n \t\t\tPointer cudnnInput = gCtx.allocate(instName, (N*T*D)*LibMatrixCUDA.sizeOfDataType);\n \t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"prepare_lstm_input\",\n-\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(IntUtils.toInt(N*T*D)),\n+\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(toInt(N*T*D)),\n \t\t\t\t\txPointer, cudnnInput, N, D, T*D, N*T*D);\n \t\t\tec.releaseMatrixInputForGPUInstruction(_input1.getName());\n \t\t\tPointer c0Pointer = LibMatrixCUDA.getDensePointer(gCtx, getMatrixInputForGPUInstruction(ec, _input5.getName()), instName);\n@@ -809,20 +815,20 @@ private void processLstmInstruction(ExecutionContext ec) throws DMLRuntimeExcept\n \t\t\tPointer sysmlBiasPointer = LibMatrixCuDNN.getDensePointerForCuDNN(gCtx, bias, instName, 1, 4*M);\n \t\t\tPointer cudnnWPointer = gCtx.allocate(instName, (D+M+2)*(4*M)*LibMatrixCUDA.sizeOfDataType);\n \t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"prepare_lstm_weight\",\n-\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(IntUtils.toInt((D+M+2)*(4*M))),\n-\t\t\t\t\tsysmlWPointer, sysmlBiasPointer, cudnnWPointer, IntUtils.toInt(D), IntUtils.toInt(M));\n+\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(toInt((D+M+2)*(4*M))),\n+\t\t\t\t\tsysmlWPointer, sysmlBiasPointer, cudnnWPointer, toInt(D), toInt(M));\n \t\t\tec.releaseMatrixInputForGPUInstruction(_input2.getName()); // W\n \t\t\tec.releaseMatrixInputForGPUInstruction(_input3.getName()); // bias\n \t\t\t// Beause the matrices are released immediately, the output for transpose need not be taken into account\n \t\t\tPointer xPointer = LibMatrixCUDA.getDensePointer(gCtx, X, instName); \n \t\t\tPointer cudnnInput = gCtx.allocate(instName, (N*T*D)*LibMatrixCUDA.sizeOfDataType);\n \t\t\tLibMatrixCUDA.getCudaKernels(gCtx).launchKernel(\"prepare_lstm_input\",\n-\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(IntUtils.toInt(N*T*D)),\n-\t\t\t\t\txPointer, cudnnInput, IntUtils.toInt(N), IntUtils.toInt(D), IntUtils.toInt(T*D), IntUtils.toInt(N*T*D));\n+\t\t\t\t\tExecutionConfig.getConfigForSimpleVectorOperations(toInt(N*T*D)),\n+\t\t\t\t\txPointer, cudnnInput, toInt(N), toInt(D), toInt(T*D), toInt(N*T*D));\n \t\t\tec.releaseMatrixInputForGPUInstruction(_input1.getName());\n \t\t\tPointer c0Pointer = LibMatrixCUDA.getDensePointer(gCtx, getMatrixInputForGPUInstruction(ec, _input5.getName()), instName); \n \t\t\tLibMatrixCuDNN.cuDNNLstm(ec, gCtx, instName, cudnnInput, cudnnWPointer, out0Pointer, c0Pointer, return_sequences, _output.getName(), _output2.getName(), \n-\t\t\t\t\tIntUtils.toInt(N), IntUtils.toInt(M), IntUtils.toInt(D), IntUtils.toInt(T));\n+\t\t\t\t\ttoInt(N), toInt(M), toInt(D), toInt(T));\n \t\t\tgCtx.cudaFreeHelper(instName, cudnnWPointer, gCtx.EAGER_CUDA_FREE);\n \t\t\tgCtx.cudaFreeHelper(instName, cudnnInput, gCtx.EAGER_CUDA_FREE);\n \t\t}\n@@ -1010,8 +1016,8 @@ else if (instOpcode.equalsIgnoreCase(\"maxpooling\") || instOpcode.equalsIgnoreCas\n \t\t\t\tlong CHW = ((long)C)*((long)H)*((long)W);\n \t\t\t\tx = gCtx.allocate(instName, ((long)N)*CHW*LibMatrixCUDA.sizeOfDataType);\n \t\t\t\tLibMatrixCuDNN.getCudaKernels(gCtx).launchKernel(\"relu\",\n-\t\t\t\t\t\tExecutionConfig.getConfigForSimpleMatrixOperations(IntUtils.toInt(N), IntUtils.toInt(CHW)),\n-\t\t\t\t\t\ttmpX, x, N, IntUtils.toInt(CHW));\n+\t\t\t\t\t\tExecutionConfig.getConfigForSimpleMatrixOperations(toInt(N), toInt(CHW)),\n+\t\t\t\t\t\ttmpX, x, N, toInt(CHW));\n \t\t\t\tec.releaseMatrixInputForGPUInstruction(_input1.getName());\n \t\t\t}\n \n@@ -1040,8 +1046,8 @@ else if (instOpcode.equalsIgnoreCase(\"maxpooling_backward\") || instOpcode.equals\n \t\t\t\tlong CHW = ((long)C)*((long)H)*((long)W);\n \t\t\t\tx = gCtx.allocate(instName, ((long)N)*CHW*LibMatrixCUDA.sizeOfDataType);\n \t\t\t\tLibMatrixCuDNN.getCudaKernels(gCtx).launchKernel(\"relu\",\n-\t\t\t\t\t\tExecutionConfig.getConfigForSimpleMatrixOperations(IntUtils.toInt(N), IntUtils.toInt(CHW)),\n-\t\t\t\t\t\ttmpX, x, N, IntUtils.toInt(CHW));\n+\t\t\t\t\t\tExecutionConfig.getConfigForSimpleMatrixOperations(toInt(N), toInt(CHW)),\n+\t\t\t\t\t\ttmpX, x, N, toInt(CHW));\n \t\t\t\tec.releaseMatrixInputForGPUInstruction(_input1.getName());\n \t\t\t}\n \t\t\t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/DnnGPUInstruction.java",
                "sha": "35c9591a6634863c4bc4720bd39e6fa1aed020a1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MMTSJGPUInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/MMTSJGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/MMTSJGPUInstruction.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.matrix.data.LibMatrixCUDA;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MMTSJGPUInstruction extends GPUInstruction {\n \tprivate MMTSJType _type = null;\n@@ -80,7 +79,7 @@ public void processInstruction(ExecutionContext ec) {\n                 GPUStatistics.incrementNoOfExecutedGPUInst();\n                 MatrixObject mat = getMatrixInputForGPUInstruction(ec, _input.getName());\n                 boolean isLeftTransposed = ( _type == MMTSJType.LEFT);\n-                int rlen = IntUtils.toInt(isLeftTransposed? mat.getNumColumns() : mat.getNumRows());\n+                int rlen = (int) (isLeftTransposed? mat.getNumColumns() : mat.getNumRows());\n                 int clen = rlen;\n                 //execute operations \n                 ec.setMetaData(_output.getName(), rlen, clen);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MMTSJGPUInstruction.java",
                "sha": "310e35811e6ee3a4e04c0db04e1ec391a74871c7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixIndexingGPUInstruction.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixIndexingGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 5,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixIndexingGPUInstruction.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.sysml.runtime.matrix.operators.SimpleOperator;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixIndexingGPUInstruction extends GPUInstruction {\n \tCPOperand rowLower, rowUpper, colLower, colUpper;\n@@ -136,9 +135,9 @@ public void processInstruction(ExecutionContext ec) {\n \t\n \tIndexRange getIndexRange(ExecutionContext ec) {\n \t\treturn new IndexRange( //rl, ru, cl, ru\n-\t\t\tIntUtils.toInt(ec.getScalarInput(rowLower.getName(), rowLower.getValueType(), rowLower.isLiteral()).getLongValue()-1),\n-\t\t\tIntUtils.toInt(ec.getScalarInput(rowUpper.getName(), rowUpper.getValueType(), rowUpper.isLiteral()).getLongValue()-1),\n-\t\t\tIntUtils.toInt(ec.getScalarInput(colLower.getName(), colLower.getValueType(), colLower.isLiteral()).getLongValue()-1),\n-\t\t\tIntUtils.toInt(ec.getScalarInput(colUpper.getName(), colUpper.getValueType(), colUpper.isLiteral()).getLongValue()-1));\n+\t\t\t(int)(ec.getScalarInput(rowLower.getName(), rowLower.getValueType(), rowLower.isLiteral()).getLongValue()-1),\n+\t\t\t(int)(ec.getScalarInput(rowUpper.getName(), rowUpper.getValueType(), rowUpper.isLiteral()).getLongValue()-1),\n+\t\t\t(int)(ec.getScalarInput(colLower.getName(), colLower.getValueType(), colLower.isLiteral()).getLongValue()-1),\n+\t\t\t(int)(ec.getScalarInput(colUpper.getName(), colUpper.getValueType(), colUpper.isLiteral()).getLongValue()-1));\n \t}\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixIndexingGPUInstruction.java",
                "sha": "6e1ac2c730d8077502bcca957f204e4e55219d9c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixArithmeticGPUInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixArithmeticGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixArithmeticGPUInstruction.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.apache.sysml.runtime.matrix.operators.BinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixMatrixArithmeticGPUInstruction extends ArithmeticBinaryGPUInstruction {\n \n@@ -61,7 +60,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\tclen = clen1 > clen2 ? clen1 : clen2;\n \t\t}\n \n-\t\tec.setMetaData(_output.getName(), IntUtils.toInt(rlen), IntUtils.toInt(clen));\n+\t\tec.setMetaData(_output.getName(), (int)rlen, (int)clen);\n \t\t\n \t\tBinaryOperator bop = (BinaryOperator) _optr;\n \t\tLibMatrixCUDA.matrixMatrixArithmetic(ec, ec.getGPUContext(0), getExtendedOpcode(), in1, in2, _output.getName(), isLeftTransposed, isRightTransposed, bop);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixArithmeticGPUInstruction.java",
                "sha": "9f4f2c58ea02a9315584cb205c0bd2f603835872",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixAxpyGPUInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixAxpyGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixAxpyGPUInstruction.java",
                "patch": "@@ -29,7 +29,6 @@\n import org.apache.sysml.runtime.matrix.data.LibMatrixCUDA;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixMatrixAxpyGPUInstruction extends ArithmeticBinaryGPUInstruction {\n \tCPOperand constant = null;\n@@ -90,7 +89,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\tlong rlen2 = in2.getNumRows();\n \t\tlong clen2 = in2.getNumColumns();\n \t\tif(isValidMMOperation(rlen1, rlen2, clen1, clen2) || isValidMVOperation(rlen1, rlen2, clen1, clen2)) {\n-\t\t\tec.setMetaData(_output.getName(), IntUtils.toInt(rlen1), IntUtils.toInt(clen1));\n+\t\t\tec.setMetaData(_output.getName(), (int)rlen1, (int)clen1);\n \t\t}\n \t\telse { \n \t\t\tthrow new DMLRuntimeException(\"Incorrect dimensions of inputs in GPU axpy operation. input1:\" + rlen1 + \" X \" + clen1 +",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixAxpyGPUInstruction.java",
                "sha": "a4eea05767b727dfa6566c0f7af0196dbd8ebd95",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixRelationalBinaryGPUInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixRelationalBinaryGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixRelationalBinaryGPUInstruction.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.apache.sysml.runtime.matrix.operators.BinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixMatrixRelationalBinaryGPUInstruction extends RelationalBinaryGPUInstruction {\n \n@@ -57,7 +56,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\tclen = clen1 > clen2 ? clen1 : clen2;\n \t\t}\n \n-\t\tec.setMetaData(_output.getName(), IntUtils.toInt(rlen), IntUtils.toInt(clen));\n+\t\tec.setMetaData(_output.getName(), (int)rlen, (int)clen);\n \n \t\tBinaryOperator bop = (BinaryOperator) _optr;\n \t\tLibMatrixCUDA.matrixMatrixRelational(ec, ec.getGPUContext(0), getExtendedOpcode(), in1, in2, _output.getName(), bop);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixMatrixRelationalBinaryGPUInstruction.java",
                "sha": "df64f78d8dba6ec927547b6608d52e1dfe58437d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixReshapeGPUInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixReshapeGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixReshapeGPUInstruction.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n import jcuda.Pointer;\n \n@@ -71,8 +70,8 @@ public static MatrixReshapeGPUInstruction parseInstruction ( String str ) {\n \n \t@Override\n \tpublic void processInstruction(ExecutionContext ec) {\n-\t\tint rows = IntUtils.toInt(ec.getScalarInput(_opRows.getName(), _opRows.getValueType(), _opRows.isLiteral()).getLongValue()); //save cast\n-\t\tint cols = IntUtils.toInt(ec.getScalarInput(_opCols.getName(), _opCols.getValueType(), _opCols.isLiteral()).getLongValue()); //save cast\n+\t\tint rows = (int)ec.getScalarInput(_opRows.getName(), _opRows.getValueType(), _opRows.isLiteral()).getLongValue(); //save cast\n+\t\tint cols = (int)ec.getScalarInput(_opCols.getName(), _opCols.getValueType(), _opCols.isLiteral()).getLongValue(); //save cast\n \t\tBooleanObject byRow = (BooleanObject) ec.getScalarInput(_opByRow.getName(), ValueType.BOOLEAN, _opByRow.isLiteral());\n \t\t\n \t\tGPUStatistics.incrementNoOfExecutedGPUInst();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/MatrixReshapeGPUInstruction.java",
                "sha": "ee2166ef5db03b4c3fbef529b4a7768aa96528e6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/ReorgGPUInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/ReorgGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/ReorgGPUInstruction.java",
                "patch": "@@ -29,7 +29,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ReorgGPUInstruction extends GPUInstruction {\n \tprivate CPOperand _input;\n@@ -72,8 +71,8 @@ public static ReorgGPUInstruction parseInstruction ( String str ) {\n \tpublic void processInstruction(ExecutionContext ec) {\n \t\tGPUStatistics.incrementNoOfExecutedGPUInst();\n \t\tMatrixObject mat = getMatrixInputForGPUInstruction(ec, _input.getName());\n-\t\tint rlen = IntUtils.toInt(mat.getNumColumns());\n-\t\tint clen = IntUtils.toInt(mat.getNumRows());\n+\t\tint rlen = (int) mat.getNumColumns();\n+\t\tint clen = (int) mat.getNumRows();\n \t\t//execute operation\n \t\tec.setMetaData(_output.getName(), rlen, clen);\n \t\tLibMatrixCUDA.transpose(ec, ec.getGPUContext(0), getExtendedOpcode(), mat, _output.getName());",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/ReorgGPUInstruction.java",
                "sha": "f191d2a26aa4115f2d4ae69b5c60754ea1b8830d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixArithmeticGPUInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixArithmeticGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixArithmeticGPUInstruction.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ScalarOperator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ScalarMatrixArithmeticGPUInstruction extends ArithmeticBinaryGPUInstruction {\n \n@@ -47,8 +46,8 @@ public void processInstruction(ExecutionContext ec) {\n \t\tScalarObject constant = (ScalarObject) ec.getScalarInput(scalar.getName(), scalar.getValueType(), scalar.isLiteral());\n \t\t\n \t\tboolean isTransposed = false;\n-\t\tint rlen = IntUtils.toInt(isTransposed ?  in1.getNumColumns() :  in1.getNumRows());\n-\t\tint clen = IntUtils.toInt(isTransposed ?  in1.getNumRows() :  in1.getNumColumns());\n+\t\tint rlen = isTransposed ? (int) in1.getNumColumns() : (int) in1.getNumRows();\n+\t\tint clen = isTransposed ? (int) in1.getNumRows() : (int) in1.getNumColumns();\n \t\t\n \t\tec.setMetaData(_output.getName(), rlen, clen);\n \t\t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixArithmeticGPUInstruction.java",
                "sha": "2593837c9b7dea83efecadefd97841fa0300a2ed",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixRelationalBinaryGPUInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixRelationalBinaryGPUInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixRelationalBinaryGPUInstruction.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ScalarOperator;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ScalarMatrixRelationalBinaryGPUInstruction extends RelationalBinaryGPUInstruction {\n \n@@ -46,8 +45,8 @@ public void processInstruction(ExecutionContext ec) {\n \t\tMatrixObject in1 = getMatrixInputForGPUInstruction(ec, mat.getName());\n \t\tScalarObject constant = (ScalarObject) ec.getScalarInput(scalar.getName(), scalar.getValueType(), scalar.isLiteral());\n \n-\t\tint rlen = IntUtils.toInt(in1.getNumRows());\n-\t\tint clen = IntUtils.toInt(in1.getNumColumns());\n+\t\tint rlen = (int) in1.getNumRows();\n+\t\tint clen = (int) in1.getNumColumns();\n \t\tec.setMetaData(_output.getName(), rlen, clen);\n \n \t\tScalarOperator sc_op = (ScalarOperator) _optr;",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/gpu/ScalarMatrixRelationalBinaryGPUInstruction.java",
                "sha": "bf5b017df694a19b0254376f501283df0af02dd0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/mr/AggregateBinaryInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/mr/AggregateBinaryInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/mr/AggregateBinaryInstruction.java",
                "patch": "@@ -39,7 +39,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateBinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class AggregateBinaryInstruction extends BinaryMRInstructionBase implements IDistributedCacheConsumer {\n \tprivate String _opcode = null;\n@@ -192,7 +191,7 @@ private void processMapMultInstruction(Class<? extends MatrixValue> valueClass,\n \t\t\t\t// Matrix multiply A[i,k] %*% B[k,bid]\n \t\t\t\t\n \t\t\t\t// Setup input2 block\n-\t\t\t\tIndexedMatrixValue in2Block = dcInput.getDataBlock(IntUtils.toInt(in1.getIndexes().getColumnIndex()), bidx);\n+\t\t\t\tIndexedMatrixValue in2Block = dcInput.getDataBlock((int)in1.getIndexes().getColumnIndex(), bidx);\n \t\t\t\t\n \t\t\t\tMatrixValue in2BlockValue = in2Block.getValue(); \n \t\t\t\tMatrixIndexes in2BlockIndex = in2Block.getIndexes();\n@@ -219,7 +218,7 @@ private void processMapMultInstruction(Class<? extends MatrixValue> valueClass,\n \t\t\t\t// Matrix multiply A[i,k] %*% B[k,bid]\n \t\t\t\t\n \t\t\t\t// Setup input2 block\n-\t\t\t\tIndexedMatrixValue in1Block = dcInput.getDataBlock(bidx, IntUtils.toInt(in2.getIndexes().getRowIndex()));\n+\t\t\t\tIndexedMatrixValue in1Block = dcInput.getDataBlock(bidx, (int)in2.getIndexes().getRowIndex());\n \t\t\t\t\n \t\t\t\tMatrixValue in1BlockValue = in1Block.getValue(); \n \t\t\t\tMatrixIndexes in1BlockIndex = in1Block.getIndexes();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/mr/AggregateBinaryInstruction.java",
                "sha": "3c4a10c35af11f06f9aaa2298527fe1c015439cc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/AppendGSPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/AppendGSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/AppendGSPInstruction.java",
                "patch": "@@ -41,7 +41,6 @@\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class AppendGSPInstruction extends BinarySPInstruction {\n \tprivate boolean _cbind = true;\n@@ -165,8 +164,8 @@ public ShiftMatrix(MatrixCharacteristics mc1, MatrixCharacteristics mc2, boolean\n \t\t\t_cbind = cbind;\n \t\t\t_startIx = cbind ? UtilFunctions.computeBlockIndex(mc1.getCols(), mc1.getColsPerBlock()) :\n \t\t\t\tUtilFunctions.computeBlockIndex(mc1.getRows(), mc1.getRowsPerBlock());\n-\t\t\t_blen = IntUtils.toInt(cbind ? mc1.getColsPerBlock() : mc1.getRowsPerBlock());\n-\t\t\t_shiftBy = IntUtils.toInt(cbind ? mc1.getCols()%_blen : mc1.getRows()%_blen); \n+\t\t\t_blen = (int) (cbind ? mc1.getColsPerBlock() : mc1.getRowsPerBlock());\n+\t\t\t_shiftBy = (int) (cbind ? mc1.getCols()%_blen : mc1.getRows()%_blen); \n \t\t\t_outlen = cbind ? mc1.getCols()+mc2.getCols() : mc1.getRows()+mc2.getRows();\n \t\t}\n ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/AppendGSPInstruction.java",
                "sha": "3ecb91fb39219f50b1a6179d26e091fda5dc03a2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CastSPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/CastSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/CastSPInstruction.java",
                "patch": "@@ -36,7 +36,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CastSPInstruction extends UnarySPInstruction {\n \n@@ -87,7 +86,7 @@ else if( opcode.equals(UnaryCP.CAST_AS_FRAME_OPCODE) ) {\n \t\t//update schema information for output frame\n \t\tif( opcode.equals(UnaryCP.CAST_AS_FRAME_OPCODE) ) {\n \t\t\tsec.getFrameObject(output.getName()).setSchema(\n-\t\t\t\tUtilFunctions.nCopies(IntUtils.toInt(mcIn.getCols()), ValueType.DOUBLE));\n+\t\t\t\tUtilFunctions.nCopies((int)mcIn.getCols(), ValueType.DOUBLE));\n \t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CastSPInstruction.java",
                "sha": "3ff878a634be027556df1fd6eccd353b726f53df",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CentralMomentSPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/CentralMomentSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/CentralMomentSPInstruction.java",
                "patch": "@@ -40,7 +40,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.CMOperator;\n import org.apache.sysml.runtime.matrix.operators.CMOperator.AggregateOperationTypes;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CentralMomentSPInstruction extends UnarySPInstruction {\n \n@@ -105,7 +104,7 @@ public void processInstruction( ExecutionContext ec ) {\n \t\tScalarObject order = ec.getScalarInput(scalarInput.getName(), scalarInput.getValueType(), scalarInput.isLiteral()); \n \t\tCMOperator cop = ((CMOperator)_optr); \n \t\tif ( cop.getAggOpType() == AggregateOperationTypes.INVALID ) {\n-\t\t\tcop.setCMAggOp(IntUtils.toInt(order.getLongValue()));\n+\t\t\tcop.setCMAggOp((int)order.getLongValue());\n \t\t}\n \t\t\n \t\t//get input",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CentralMomentSPInstruction.java",
                "sha": "f25899ff5ca327a606766bae941d70230c7d29a8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CpmmSPInstruction.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/CpmmSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/CpmmSPInstruction.java",
                "patch": "@@ -49,7 +49,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Cpmm: cross-product matrix multiplication operation (distributed matrix multiply\n@@ -166,9 +165,9 @@ private static int getPreferredParJoin(MatrixCharacteristics mc1, MatrixCharacte\n \t}\n \t\n \tprivate static int getMaxParJoin(MatrixCharacteristics mc1, MatrixCharacteristics mc2) {\n-\t\treturn IntUtils.toInt(mc1.colsKnown() ? mc1.getNumColBlocks() :\n-\t\t\tmc2.rowsKnown() ? mc2.getNumRowBlocks() :\n-\t\t\tInteger.MAX_VALUE);\n+\t\treturn mc1.colsKnown() ? (int)mc1.getNumColBlocks() :\n+\t\t\tmc2.rowsKnown() ? (int)mc2.getNumRowBlocks() :\n+\t\t\tInteger.MAX_VALUE;\n \t}\n \n \tprivate static class CpmmIndexFunction implements PairFunction<Tuple2<MatrixIndexes, MatrixBlock>, Long, IndexedMatrixValue>",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CpmmSPInstruction.java",
                "sha": "308e60ff05a33d5d6fa62ff6b2fc24fe6186bd84",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeAggregateSPInstruction.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeAggregateSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeAggregateSPInstruction.java",
                "patch": "@@ -39,7 +39,6 @@\n import org.apache.sysml.runtime.matrix.data.OperationsOnMatrixValues;\n import org.apache.sysml.runtime.matrix.operators.AggregateUnaryOperator;\n import org.apache.sysml.runtime.matrix.operators.UnaryOperator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CumulativeAggregateSPInstruction extends AggregateUnarySPInstruction {\n \n@@ -77,7 +76,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t//merge partial aggregates, adjusting for correct number of partitions\n \t\t//as size can significant shrink (1K) but also grow (sparse-dense)\n \t\tint numParts = SparkUtils.getNumPreferredPartitions(mcOut);\n-\t\tint minPar = IntUtils.toInt(Math.min(SparkExecutionContext.getDefaultParallelism(true), mcOut.getNumBlocks()));\n+\t\tint minPar = (int)Math.min(SparkExecutionContext.getDefaultParallelism(true), mcOut.getNumBlocks());\n \t\tout = RDDAggregateUtils.mergeByKey(out, Math.max(numParts, minPar), false);\n \t\t\n \t\t//put output handle in symbol table\n@@ -134,9 +133,9 @@ public RDDCumAggFunction( AggregateUnaryOperator op, long rlen, int brlen, int b\n \t\t\t//cumsum expand partial aggregates\n \t\t\tlong rlenOut = (long)Math.ceil((double)_rlen/_brlen);\n \t\t\tlong rixOut = (long)Math.ceil((double)ixIn.getRowIndex()/_brlen);\n-\t\t\tint rlenBlk = IntUtils.toInt( Math.min(rlenOut-(rixOut-1)*_brlen, _brlen));\n+\t\t\tint rlenBlk = (int) Math.min(rlenOut-(rixOut-1)*_brlen, _brlen);\n \t\t\tint clenBlk = blkOut.getNumColumns();\n-\t\t\tint posBlk = IntUtils.toInt((ixIn.getRowIndex()-1) % _brlen);\n+\t\t\tint posBlk = (int) ((ixIn.getRowIndex()-1) % _brlen);\n \t\t\t\n \t\t\t//construct sparse output blocks (single row in target block size)\n \t\t\tMatrixBlock blkOut2 = new MatrixBlock(rlenBlk, clenBlk, true);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeAggregateSPInstruction.java",
                "sha": "1da64284ee5d796170ab97e442d927bc6c444f10",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeOffsetSPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeOffsetSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeOffsetSPInstruction.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.sysml.runtime.matrix.operators.UnaryOperator;\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class CumulativeOffsetSPInstruction extends BinarySPInstruction {\n \tprivate UnaryOperator _uop = null;\n@@ -201,7 +200,7 @@ public RDDCumSplitLookupFunction(PartitionedBroadcast<MatrixBlock> pbc, double i\n \t\t\t\n \t\t\t//lookup offset row and return joined output\n \t\t\tMatrixBlock off = (ixIn.getRowIndex() == 1) ? new MatrixBlock(1, blkIn.getNumColumns(), _initValue) :\n-\t\t\t\t_pbc.getBlock(IntUtils.toInt(brix), IntUtils.toInt(ixIn.getColumnIndex())).slice(rix, rix);\n+\t\t\t\t_pbc.getBlock((int)brix, (int)ixIn.getColumnIndex()).slice(rix, rix);\n \t\t\treturn new Tuple2<MatrixIndexes, Tuple2<MatrixBlock,MatrixBlock>>(ixIn, new Tuple2<>(blkIn,off));\n \t\t}\n \t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/CumulativeOffsetSPInstruction.java",
                "sha": "1b2606001747b8f477567d35f4fc34227c825bc8",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/DnnSPInstruction.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/DnnSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 7,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/DnnSPInstruction.java",
                "patch": "@@ -47,7 +47,6 @@\n import org.apache.sysml.runtime.matrix.data.OutputInfo;\n import org.apache.sysml.runtime.matrix.operators.ReorgOperator;\n import org.apache.sysml.runtime.util.DnnUtils;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.NativeHelper;\n \n import scala.Tuple2;\n@@ -215,7 +214,7 @@ else if (opcode.equalsIgnoreCase(\"bias_add\")) {\n \t\tMatrixCharacteristics mcRdd = sec.getMatrixCharacteristics(name);\n \t\tif(mcRdd.getColsPerBlock() < mcRdd.getCols() || mcRdd.getRowsPerBlock() != 1) {\n \t\t\tMatrixCharacteristics mcOut = new MatrixCharacteristics(mcRdd);\n-\t\t\tmcOut.setColsPerBlock(IntUtils.toInt(mcRdd.getCols()));\n+\t\t\tmcOut.setColsPerBlock((int)mcRdd.getCols());\n \t\t\tmcOut.setRowsPerBlock(numRowsPerBlock); \n \t\t\tin1 = RDDAggregateUtils.mergeByKey(in1.flatMapToPair(new ExtractBlockForBinaryReblock(mcRdd, mcOut)));\n \t\t\t// TODO: Inject checkpoint to avoid doing this repeated for validation set\n@@ -267,8 +266,8 @@ else if(instOpcode.equalsIgnoreCase(\"conv2d_bias_add\")) {\n \t\t\tint K = getScalarInput(ec, _filter_shape, 0);\n \t\t\tint R = getScalarInput(ec, _filter_shape, 2);\n \t\t\tint S = getScalarInput(ec, _filter_shape, 3);\n-\t\t\tint P = IntUtils.toInt(DnnUtils.getP(H, R, stride_h, pad_h));\n-\t\t\tint Q = IntUtils.toInt(DnnUtils.getQ(W, S, stride_w, pad_w));\n+\t\t\tint P = (int) DnnUtils.getP(H, R, stride_h, pad_h);\n+\t\t\tint Q = (int) DnnUtils.getQ(W, S, stride_w, pad_w);\n \t\t\t\n \t\t\tDnnParameters params = new DnnParameters(numRowsPerBlock, C, H, W, K, R, S, stride_h, stride_w, pad_h, pad_w, 1);\n \t\t\tboolean enableNativeBLAS = NativeHelper.isNativeLibraryLoaded(); \n@@ -287,16 +286,16 @@ else if(instOpcode.equalsIgnoreCase(\"conv2d_bias_add\")) {\n \t\t\t\tthrow new DMLRuntimeException(\"The current operator doesnot support large outputs.\");\n \t\t\t}\n \t\t\tsec.setMetaData(output.getName(), \n-\t\t\t\t\tnew MetaDataFormat(new MatrixCharacteristics(mcRdd.getRows(), numCols, numRowsPerBlock, IntUtils.toInt(numCols), nnz), OutputInfo.BinaryBlockOutputInfo, InputInfo.BinaryBlockInputInfo));\n+\t\t\t\t\tnew MetaDataFormat(new MatrixCharacteristics(mcRdd.getRows(), numCols, numRowsPerBlock, (int)numCols, nnz), OutputInfo.BinaryBlockOutputInfo, InputInfo.BinaryBlockInputInfo));\n \t\t}\n \t\telse {\n \t\t\tthrow new DMLRuntimeException(\"Not implemented: \" + instOpcode);\n \t\t}\n \t}\n \n \tprivate static int getScalarInput(ExecutionContext ec, ArrayList<CPOperand> aL, int index) {\n-\t\treturn IntUtils.toInt( ec.getScalarInput(aL.get(index).getName(),\n-\t\t\taL.get(index).getValueType(), aL.get(index).isLiteral()).getLongValue());\n+\t\treturn (int) ec.getScalarInput(aL.get(index).getName(),\n+\t\t\taL.get(index).getValueType(), aL.get(index).isLiteral()).getLongValue();\n \t}\n \t\n \tprivate static class RDDConv2dMapMMFunction implements PairFlatMapFunction<Iterator<Tuple2<MatrixIndexes, MatrixBlock>>, MatrixIndexes, MatrixBlock> {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/DnnSPInstruction.java",
                "sha": "fbb214ecae55423fa204ced8d3ca2d72ed64719d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/FrameIndexingSPInstruction.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/FrameIndexingSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/FrameIndexingSPInstruction.java",
                "patch": "@@ -47,7 +47,6 @@\n import org.apache.sysml.runtime.matrix.data.Pair;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * This class implements the frame indexing functionality inside Spark.\n@@ -104,7 +103,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t\n \t\t\t//update schema of output with subset of input schema\n \t\t\tsec.getFrameObject(output.getName()).setSchema(\n-\t\t\t\tsec.getFrameObject(input1.getName()).getSchema(IntUtils.toInt(cl), IntUtils.toInt(cu)));\n+\t\t\t\tsec.getFrameObject(input1.getName()).getSchema((int)cl, (int)cu));\n \t\t}\n \t\t//left indexing\n \t\telse if ( opcode.equalsIgnoreCase(LeftIndex.OPCODE) || opcode.equalsIgnoreCase(\"mapLeftIndex\"))\n@@ -187,8 +186,8 @@ public SliceRHSForLeftIndexing(IndexRange ixrange, MatrixCharacteristics mcLeft)\n \t\t\t_ixrange = ixrange;\n \t\t\t_rlen = mcLeft.getRows();\n \t\t\t_clen = mcLeft.getCols();\n-\t\t\t_brlen = IntUtils.toInt( Math.min(OptimizerUtils.getDefaultFrameSize(), _rlen));\n-\t\t\t_bclen = IntUtils.toInt(mcLeft.getCols());\n+\t\t\t_brlen = (int) Math.min(OptimizerUtils.getDefaultFrameSize(), _rlen);\n+\t\t\t_bclen = (int) mcLeft.getCols();\n \t\t}\n \n \t\t@Override",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/FrameIndexingSPInstruction.java",
                "sha": "f74f29327ed98a6c8847359ebae8f6cefe457471",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmChainSPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmChainSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmChainSPInstruction.java",
                "patch": "@@ -38,7 +38,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MapmmChainSPInstruction extends SPInstruction {\n \tprivate ChainType _chainType = null;\n@@ -169,7 +168,7 @@ public MatrixBlock call( Tuple2<MatrixIndexes, MatrixBlock> arg0 ) {\n \t\t\tMatrixBlock pmV = _pmV.getBlock(1, 1);\n \t\t\tMatrixIndexes ixIn = arg0._1();\n \t\t\tMatrixBlock blkIn = arg0._2();\n-\t\t\tint rowIx = IntUtils.toInt(ixIn.getRowIndex());\n+\t\t\tint rowIx = (int)ixIn.getRowIndex();\n \t\t\t//execute mapmmchain operation\n \t\t\treturn blkIn.chainMatrixMultOperations(pmV, \n \t\t\t\t\t_pmW.getBlock(rowIx,1), new MatrixBlock(), _chainType);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmChainSPInstruction.java",
                "sha": "c9566a237e7142aa482d7dd995478e69c297e1da",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmSPInstruction.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 10,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmSPInstruction.java",
                "patch": "@@ -54,7 +54,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateBinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MapmmSPInstruction extends BinarySPInstruction {\n \tprivate CacheType _type = null;\n@@ -239,7 +238,7 @@ private static int getNumRepartitioning( CacheType type, MatrixCharacteristics m\n \t\t\t\tisLeft?mcRdd.getCols():mcBc.getCols(), isLeft?mcBc.getRowsPerBlock():mcRdd.getRowsPerBlock(),\n \t\t\t\tisLeft?mcRdd.getColsPerBlock():mcBc.getColsPerBlock(), 1.0)); \n \t\tlong numParts = sizeOutput / InfrastructureAnalyzer.getHDFSBlockSize();\n-\t\treturn IntUtils.toInt(Math.min(numParts, (isLeft?mcRdd.getNumColBlocks():mcRdd.getNumRowBlocks())));\n+\t\treturn (int)Math.min(numParts, (isLeft?mcRdd.getNumColBlocks():mcRdd.getNumRowBlocks()));\n \t}\n \n \tprivate static class RDDMapMMFunction implements PairFunction<Tuple2<MatrixIndexes, MatrixBlock>, MatrixIndexes, MatrixBlock> \n@@ -273,7 +272,7 @@ public RDDMapMMFunction( CacheType type, PartitionedBroadcast<MatrixBlock> binpu\n \t\t\tif( _type == CacheType.LEFT )\n \t\t\t{\n \t\t\t\t//get the right hand side matrix\n-\t\t\t\tMatrixBlock left = _pbc.getBlock(1, IntUtils.toInt(ixIn.getRowIndex()));\n+\t\t\t\tMatrixBlock left = _pbc.getBlock(1, (int)ixIn.getRowIndex());\n \t\t\t\t\n \t\t\t\t//execute matrix-vector mult\n \t\t\t\tOperationsOnMatrixValues.matMult(new MatrixIndexes(1,ixIn.getRowIndex()),\n@@ -282,7 +281,7 @@ public RDDMapMMFunction( CacheType type, PartitionedBroadcast<MatrixBlock> binpu\n \t\t\telse //if( _type == CacheType.RIGHT )\n \t\t\t{\n \t\t\t\t//get the right hand side matrix\n-\t\t\t\tMatrixBlock right = _pbc.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), 1);\n+\t\t\t\tMatrixBlock right = _pbc.getBlock((int)ixIn.getColumnIndex(), 1);\n \t\t\t\t\n \t\t\t\t//execute matrix-vector mult\n \t\t\t\tOperationsOnMatrixValues.matMult(ixIn, blkIn,\n@@ -325,7 +324,7 @@ public MatrixBlock call( Tuple2<MatrixIndexes, MatrixBlock> arg0 )\n \t\t\tif( _type == CacheType.LEFT )\n \t\t\t{\n \t\t\t\t//get the right hand side matrix\n-\t\t\t\tMatrixBlock left = _pbc.getBlock(1, IntUtils.toInt(ixIn.getRowIndex()));\n+\t\t\t\tMatrixBlock left = _pbc.getBlock(1, (int)ixIn.getRowIndex());\n \t\t\t\t\n \t\t\t\t//execute matrix-vector mult\n \t\t\t\treturn OperationsOnMatrixValues.matMult( \n@@ -334,7 +333,7 @@ public MatrixBlock call( Tuple2<MatrixIndexes, MatrixBlock> arg0 )\n \t\t\telse //if( _type == CacheType.RIGHT )\n \t\t\t{\n \t\t\t\t//get the right hand side matrix\n-\t\t\t\tMatrixBlock right = _pbc.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), 1);\n+\t\t\t\tMatrixBlock right = _pbc.getBlock((int)ixIn.getColumnIndex(), 1);\n \t\t\t\t\n \t\t\t\t//execute matrix-vector mult\n \t\t\t\treturn OperationsOnMatrixValues.matMult(\n@@ -390,15 +389,15 @@ public MapMMPartitionIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> in) {\n \t\t\t\tif( _type == CacheType.LEFT )\n \t\t\t\t{\n \t\t\t\t\t//get the right hand side matrix\n-\t\t\t\t\tMatrixBlock left = _pbc.getBlock(1, IntUtils.toInt(ixIn.getRowIndex()));\n+\t\t\t\t\tMatrixBlock left = _pbc.getBlock(1, (int)ixIn.getRowIndex());\n \t\t\t\t\t\n \t\t\t\t\t//execute index preserving matrix multiplication\n \t\t\t\t\tOperationsOnMatrixValues.matMult(left, blkIn, blkOut, _op);\n \t\t\t\t}\n \t\t\t\telse //if( _type == CacheType.RIGHT )\n \t\t\t\t{\n \t\t\t\t\t//get the right hand side matrix\n-\t\t\t\t\tMatrixBlock right = _pbc.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), 1);\n+\t\t\t\t\tMatrixBlock right = _pbc.getBlock((int)ixIn.getColumnIndex(), 1);\n \n \t\t\t\t\t//execute index preserving matrix multiplication\n \t\t\t\t\tOperationsOnMatrixValues.matMult(blkIn, right, blkOut, _op);\n@@ -438,14 +437,14 @@ public RDDFlatMapMMFunction( CacheType type, PartitionedBroadcast<MatrixBlock> b\n \t\t\t\t//for all matching left-hand-side blocks, returned as lazy iterator\n \t\t\t\treturn IntStream.range(1, _pbc.getNumRowBlocks()+1).mapToObj(i ->\n \t\t\t\t\tnew Tuple2<>(new MatrixIndexes(i, ixIn.getColumnIndex()),\n-\t\t\t\t\tOperationsOnMatrixValues.matMult(_pbc.getBlock(i, IntUtils.toInt(ixIn.getRowIndex())), blkIn,\n+\t\t\t\t\tOperationsOnMatrixValues.matMult(_pbc.getBlock(i, (int)ixIn.getRowIndex()), blkIn,\n \t\t\t\t\t\tnew MatrixBlock(), _op))).iterator();\n \t\t\t}\n \t\t\telse { //RIGHT\n \t\t\t\t//for all matching right-hand-side blocks, returned as lazy iterator\n \t\t\t\treturn IntStream.range(1, _pbc.getNumColumnBlocks()+1).mapToObj(j ->\n \t\t\t\t\tnew Tuple2<>(new MatrixIndexes(ixIn.getRowIndex(), j),\n-\t\t\t\t\tOperationsOnMatrixValues.matMult(blkIn, _pbc.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), j),\n+\t\t\t\t\tOperationsOnMatrixValues.matMult(blkIn, _pbc.getBlock((int)ixIn.getColumnIndex(), j),\n \t\t\t\t\t\tnew MatrixBlock(), _op))).iterator();\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MapmmSPInstruction.java",
                "sha": "7c8d60657adc0b621d98b2f75e26833e0e999274",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixAppendMSPInstruction.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixAppendMSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 7,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixAppendMSPInstruction.java",
                "patch": "@@ -39,7 +39,6 @@\n import org.apache.sysml.runtime.matrix.data.OperationsOnMatrixValues;\n import org.apache.sysml.runtime.matrix.mapred.IndexedMatrixValue;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixAppendMSPInstruction extends AppendMSPInstruction {\n \n@@ -139,11 +138,11 @@ else if( _cbind && in1.getValue().getNumColumns() == _bclen\n \t\t\t\t//output shallow copy of rhs block\n \t\t\t\tif( _cbind ) {\n \t\t\t\t\tret.add( new Tuple2<>(new MatrixIndexes(ix.getRowIndex(), ix.getColumnIndex()+1),\n-\t\t\t\t\t\t_pm.getBlock(IntUtils.toInt(ix.getRowIndex()), 1)) );\n+\t\t\t\t\t\t_pm.getBlock((int)ix.getRowIndex(), 1)) );\n \t\t\t\t}\n \t\t\t\telse { //rbind\n \t\t\t\t\tret.add( new Tuple2<>(new MatrixIndexes(ix.getRowIndex()+1, ix.getColumnIndex()),\n-\t\t\t\t\t\t_pm.getBlock(1, IntUtils.toInt(ix.getColumnIndex()))) );\n+\t\t\t\t\t\t_pm.getBlock(1, (int)ix.getColumnIndex())) );\n \t\t\t\t}\n \t\t\t}\n \t\t\t//case 3: append operation on boundary block\n@@ -156,15 +155,15 @@ else if( _cbind && in1.getValue().getNumColumns() == _bclen\n \t\t\t\t\n \t\t\t\tMatrixBlock value_in2 = null;\n \t\t\t\tif( _cbind ) {\n-\t\t\t\t\tvalue_in2 = _pm.getBlock(IntUtils.toInt(ix.getRowIndex()), 1);\n+\t\t\t\t\tvalue_in2 = _pm.getBlock((int)ix.getRowIndex(), 1);\n \t\t\t\t\tif(in1.getValue().getNumColumns()+value_in2.getNumColumns()>_bclen) {\n \t\t\t\t\t\tIndexedMatrixValue second=new IndexedMatrixValue(new MatrixIndexes(), new MatrixBlock());\n \t\t\t\t\t\tsecond.getIndexes().setIndexes(ix.getRowIndex(), ix.getColumnIndex()+1);\n \t\t\t\t\t\toutlist.add(second);\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse { //rbind\n-\t\t\t\t\tvalue_in2 = _pm.getBlock(1, IntUtils.toInt(ix.getColumnIndex()));\n+\t\t\t\t\tvalue_in2 = _pm.getBlock(1, (int)ix.getColumnIndex());\n \t\t\t\t\tif(in1.getValue().getNumRows()+value_in2.getNumRows()>_brlen) {\n \t\t\t\t\t\tIndexedMatrixValue second=new IndexedMatrixValue(new MatrixIndexes(), new MatrixBlock());\n \t\t\t\t\t\tsecond.getIndexes().setIndexes(ix.getRowIndex()+1, ix.getColumnIndex());\n@@ -229,8 +228,8 @@ public MapAppendPartitionIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> i\n \t\t\t\t}\n \t\t\t\t//case 3: append operation on boundary block\n \t\t\t\telse {\n-\t\t\t\t\tint rowix = _cbind ? IntUtils.toInt(ix.getRowIndex()) : 1;\n-\t\t\t\t\tint colix = _cbind ? 1 : IntUtils.toInt(ix.getColumnIndex());\n+\t\t\t\t\tint rowix = _cbind ? (int)ix.getRowIndex() : 1;\n+\t\t\t\t\tint colix = _cbind ? 1 : (int)ix.getColumnIndex();\n \t\t\t\t\tMatrixBlock in2 = _pm.getBlock(rowix, colix);\n \t\t\t\t\tMatrixBlock out = in1.append(in2, new MatrixBlock(), _cbind);\n \t\t\t\t\treturn new Tuple2<>(ix, out);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixAppendMSPInstruction.java",
                "sha": "3019a78a9fa2c74463352d3167f6d6fdc8b9c730",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixIndexingSPInstruction.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixIndexingSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 10,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixIndexingSPInstruction.java",
                "patch": "@@ -58,7 +58,6 @@\n import org.apache.sysml.runtime.matrix.mapred.IndexedMatrixValue;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * This class implements the matrix indexing functionality inside CP.\n@@ -204,8 +203,8 @@ private static MatrixBlock multiBlockIndexing(JavaPairRDD<MatrixIndexes,MatrixBl\n \t\t\t\t .mapToPair(new SliceBlock2(ixrange, mcOut));       //slice relevant blocks\n \t\t\n \t\t//collect output without shuffle to avoid side-effects with custom PartitionPruningRDD\n-\t\tMatrixBlock mbout = SparkExecutionContext.toMatrixBlock(out, IntUtils.toInt(mcOut.getRows()), \n-\t\t\t\tIntUtils.toInt(mcOut.getCols()), mcOut.getRowsPerBlock(), mcOut.getColsPerBlock(), -1);\n+\t\tMatrixBlock mbout = SparkExecutionContext.toMatrixBlock(out, (int)mcOut.getRows(), \n+\t\t\t\t(int)mcOut.getCols(), mcOut.getRowsPerBlock(), mcOut.getColsPerBlock(), -1);\n \t\treturn mbout;\n \t}\n \t\n@@ -411,11 +410,11 @@ public LeftIndexPartitionIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> i\n \t\t\t\t\tMatrixBlock right = arg._2();\n \n \t\t\t\t\tint rl = UtilFunctions.computeCellInBlock(_ixrange.rowStart, _brlen);\n-\t\t\t\t\tint ru = IntUtils.toInt(Math.min(_ixrange.rowEnd, rl+right.getNumRows())-1);\n+\t\t\t\t\tint ru = (int)Math.min(_ixrange.rowEnd, rl+right.getNumRows())-1;\n \t\t\t\t\tint cl = UtilFunctions.computeCellInBlock(_ixrange.colStart, _brlen);\n-\t\t\t\t\tint cu = IntUtils.toInt(Math.min(_ixrange.colEnd, cl+right.getNumColumns())-1);\n+\t\t\t\t\tint cu = (int)Math.min(_ixrange.colEnd, cl+right.getNumColumns())-1;\n \t\t\t\t\t\n-\t\t\t\t\tMatrixBlock left = _binput.getBlock(IntUtils.toInt(ix.getRowIndex()), IntUtils.toInt(ix.getColumnIndex()));\n+\t\t\t\t\tMatrixBlock left = _binput.getBlock((int)ix.getRowIndex(), (int)ix.getColumnIndex());\n \t\t\t\t\tMatrixBlock tmp = left.leftIndexingOperations(right, \n \t\t\t\t\t\t\trl, ru, cl, cu, new MatrixBlock(), UpdateType.COPY);\n \t\t\t\t\t\n@@ -475,10 +474,10 @@ public SliceSingleBlock(IndexRange ixrange, MatrixCharacteristics mcOut) {\n \t\t\t//compute local index range \n \t\t\tlong grix = UtilFunctions.computeCellIndex(ix.getRowIndex(), _brlen, 0);\n \t\t\tlong gcix = UtilFunctions.computeCellIndex(ix.getColumnIndex(), _bclen, 0);\n-\t\t\tint lrl = IntUtils.toInt((_ixrange.rowStart<grix) ? 0 : _ixrange.rowStart - grix);\n-\t\t\tint lcl = IntUtils.toInt((_ixrange.colStart<gcix) ? 0 : _ixrange.colStart - gcix);\n-\t\t\tint lru = IntUtils.toInt(Math.min(block.getNumRows()-1, _ixrange.rowEnd - grix));\n-\t\t\tint lcu = IntUtils.toInt(Math.min(block.getNumColumns()-1, _ixrange.colEnd - gcix));\n+\t\t\tint lrl = (int)((_ixrange.rowStart<grix) ? 0 : _ixrange.rowStart - grix);\n+\t\t\tint lcl = (int)((_ixrange.colStart<gcix) ? 0 : _ixrange.colStart - gcix);\n+\t\t\tint lru = (int)Math.min(block.getNumRows()-1, _ixrange.rowEnd - grix);\n+\t\t\tint lcu = (int)Math.min(block.getNumColumns()-1, _ixrange.colEnd - gcix);\n \t\t\t\n \t\t\t//compute output index\n \t\t\tMatrixIndexes ixOut = new MatrixIndexes(",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MatrixIndexingSPInstruction.java",
                "sha": "d30f330b63c6a08e16b47039a9a1a35250e7ed71",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java",
                "patch": "@@ -64,7 +64,6 @@\n import org.apache.sysml.runtime.transform.encode.EncoderMVImpute.MVMethod;\n import org.apache.sysml.runtime.transform.meta.TfMetaUtils;\n import org.apache.sysml.runtime.transform.meta.TfOffsetMap;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -116,7 +115,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t\t\t\n \t\t\t//step 1: build transform meta data\n \t\t\tEncoder encoderBuild = EncoderFactory.createEncoder(spec, colnames,\n-\t\t\t\t\tfo.getSchema(), IntUtils.toInt(fo.getNumColumns()), null);\n+\t\t\t\t\tfo.getSchema(), (int)fo.getNumColumns(), null);\n \t\t\t\n \t\t\tMaxLongAccumulator accMax = registerMaxLongAccumulator(sec.getSparkContext()); \n \t\t\tJavaRDD<String> rcMaps = in\n@@ -147,7 +146,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t\n \t\t\t//create encoder broadcast (avoiding replication per task) \n \t\t\tEncoder encoder = EncoderFactory.createEncoder(spec, colnames,\n-\t\t\t\t\tfo.getSchema(), IntUtils.toInt(fo.getNumColumns()), meta);\n+\t\t\t\t\tfo.getSchema(), (int)fo.getNumColumns(), meta);\n \t\t\tmcOut.setDimension(mcIn.getRows()-((omap!=null)?omap.getNumRmRows():0), encoder.getNumCols()); \n \t\t\tBroadcast<Encoder> bmeta = sec.getSparkContext().broadcast(encoder);\n \t\t\tBroadcast<TfOffsetMap> bomap = (omap!=null) ? sec.getSparkContext().broadcast(omap) : null;",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java",
                "sha": "cd5e2bad24b6cd192865a05a3c79bb24a84284f0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/PMapmmSPInstruction.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/PMapmmSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/PMapmmSPInstruction.java",
                "patch": "@@ -49,7 +49,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateBinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * This pmapmm matrix multiplication instruction is still experimental\n@@ -106,8 +105,8 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t\t\t.filter(new IsBlockInRange(i+1, i+NUM_ROWBLOCKS*mc1.getRowsPerBlock(), 1, mc1.getCols(), mc1))\n \t\t\t\t\t.mapToPair(new PMapMMRebaseBlocksFunction(i/mc1.getRowsPerBlock()));\n \t\t\t\n-\t\t\tint rlen = IntUtils.toInt(Math.min(mc1.getRows()-i, NUM_ROWBLOCKS*mc1.getRowsPerBlock()));\n-\t\t\tPartitionedBlock<MatrixBlock> pmb = SparkExecutionContext.toPartitionedMatrixBlock(rdd, rlen, IntUtils.toInt(mc1.getCols()), mc1.getRowsPerBlock(), mc1.getColsPerBlock(), -1L);\n+\t\t\tint rlen = (int)Math.min(mc1.getRows()-i, NUM_ROWBLOCKS*mc1.getRowsPerBlock());\n+\t\t\tPartitionedBlock<MatrixBlock> pmb = SparkExecutionContext.toPartitionedMatrixBlock(rdd, rlen, (int)mc1.getCols(), mc1.getRowsPerBlock(), mc1.getColsPerBlock(), -1L);\n \t\t\tBroadcast<PartitionedBlock<MatrixBlock>> bpmb = sec.getSparkContext().broadcast(pmb);\n \t\t\t\n \t\t\t//matrix multiplication\n@@ -191,7 +190,7 @@ public PMapMMFunction( Broadcast<PartitionedBlock<MatrixBlock>> binput, long off\n \t\t\t\n \t\t\t//get the right hand side matrix\n \t\t\tfor( int i=1; i<=pm.getNumRowBlocks(); i++ ) {\n-\t\t\t\tMatrixBlock left = pm.getBlock(i, IntUtils.toInt(ixIn.getRowIndex()));\n+\t\t\t\tMatrixBlock left = pm.getBlock(i, (int)ixIn.getRowIndex());\n \t\t\t\n \t\t\t\t//execute matrix-vector mult\n \t\t\t\tOperationsOnMatrixValues.matMult(new MatrixIndexes(i,ixIn.getRowIndex()),",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/PMapmmSPInstruction.java",
                "sha": "1b6435b1886fcdd0abfb5f8e532c1e3c881359f3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 16,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java",
                "patch": "@@ -83,7 +83,6 @@\n import org.apache.sysml.runtime.transform.meta.TfOffsetMap;\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ParameterizedBuiltinSPInstruction extends ComputationSPInstruction {\n \tprotected HashMap<String, String> params;\n@@ -241,7 +240,7 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\tMatrixCharacteristics mc1 = sec.getMatrixCharacteristics( targetVar );\n \t\t\tMatrixCharacteristics mcOut = sec.getMatrixCharacteristics(output.getName());\n \t\t\n-\t\t\tint ngroups = IntUtils.toInt(getLongParam(ec, Statement.GAGG_NUM_GROUPS));\n+\t\t\tint ngroups = (int) getLongParam(ec, Statement.GAGG_NUM_GROUPS);\n \t\t\t\n \t\t\t//single-block aggregation\n \t\t\tif( ngroups <= mc1.getRowsPerBlock() && mc1.getCols() <= mc1.getColsPerBlock() ) {\n@@ -402,12 +401,12 @@ else if ( opcode.equalsIgnoreCase(\"rmempty\") )\n \t\t\t\t\n \t\t\t\t//update output statistics (required for correctness)\n \t\t\t\tMatrixCharacteristics mcOut = sec.getMatrixCharacteristics(output.getName());\n-\t\t\t\tmcOut.set(rows?maxDim:mcIn.getRows(), rows?mcIn.getCols():maxDim, IntUtils.toInt(brlen), IntUtils.toInt(bclen), mcIn.getNonZeros());\n+\t\t\t\tmcOut.set(rows?maxDim:mcIn.getRows(), rows?mcIn.getCols():maxDim, (int)brlen, (int)bclen, mcIn.getNonZeros());\n \t\t\t}\n \t\t\telse //special case: empty output (ensure valid dims)\n \t\t\t{\n \t\t\t\tint n = emptyReturn ? 1 : 0;\n-\t\t\t\tMatrixBlock out = new MatrixBlock(rows?n:IntUtils.toInt(mcIn.getRows()), rows?IntUtils.toInt(mcIn.getCols()):n, true); \n+\t\t\t\tMatrixBlock out = new MatrixBlock(rows?n:(int)mcIn.getRows(), rows?(int)mcIn.getCols():n, true); \n \t\t\t\tsec.setMatrixOutput(output.getName(), out, getExtendedOpcode());\n \t\t\t}\n \t\t}\n@@ -467,8 +466,8 @@ else if ( opcode.equalsIgnoreCase(\"rexpand\") )\n \t\t\t//repartition input vector for higher degree of parallelism \n \t\t\t//(avoid scenarios where few input partitions create huge outputs)\n \t\t\tMatrixCharacteristics mcTmp = new MatrixCharacteristics(dirRows?lmaxVal:mcIn.getRows(),\n-\t\t\t\t\tdirRows?mcIn.getRows():lmaxVal, IntUtils.toInt(brlen), IntUtils.toInt(bclen), mcIn.getRows());\n-\t\t\tint numParts = IntUtils.toInt(Math.min(SparkUtils.getNumPreferredPartitions(mcTmp, in), mcIn.getNumBlocks()));\n+\t\t\t\t\tdirRows?mcIn.getRows():lmaxVal, (int)brlen, (int)bclen, mcIn.getRows());\n+\t\t\tint numParts = (int)Math.min(SparkUtils.getNumPreferredPartitions(mcTmp, in), mcIn.getNumBlocks());\n \t\t\tif( numParts > in.getNumPartitions()*2 )\n \t\t\t\tin = in.repartition(numParts);\n \t\t\t\n@@ -483,7 +482,7 @@ else if ( opcode.equalsIgnoreCase(\"rexpand\") )\n \t\t\t\n \t\t\t//update output statistics (required for correctness)\n \t\t\tMatrixCharacteristics mcOut = sec.getMatrixCharacteristics(output.getName());\n-\t\t\tmcOut.set(dirRows?lmaxVal:mcIn.getRows(), dirRows?mcIn.getRows():lmaxVal, IntUtils.toInt(brlen), IntUtils.toInt(bclen), -1);\n+\t\t\tmcOut.set(dirRows?lmaxVal:mcIn.getRows(), dirRows?mcIn.getRows():lmaxVal, (int)brlen, (int)bclen, -1);\n \t\t}\n \t\telse if ( opcode.equalsIgnoreCase(\"transformapply\") ) \n \t\t{\n@@ -506,7 +505,7 @@ else if ( opcode.equalsIgnoreCase(\"transformapply\") )\n \t\t\t\n \t\t\t//create encoder broadcast (avoiding replication per task) \n \t\t\tEncoder encoder = EncoderFactory.createEncoder(params.get(\"spec\"), colnames,\n-\t\t\t\tfo.getSchema(), IntUtils.toInt(fo.getNumColumns()), meta);\n+\t\t\t\tfo.getSchema(), (int)fo.getNumColumns(), meta);\n \t\t\tmcOut.setDimension(mcIn.getRows()-((omap!=null)?omap.getNumRmRows():0), encoder.getNumCols()); \n \t\t\tBroadcast<Encoder> bmeta = sec.getSparkContext().broadcast(encoder);\n \t\t\tBroadcast<TfOffsetMap> bomap = (omap!=null) ? sec.getSparkContext().broadcast(omap) : null;\n@@ -533,7 +532,7 @@ else if ( opcode.equalsIgnoreCase(\"transformdecode\") )\n \t\t\t//reblock if necessary (clen > bclen)\n \t\t\tif( mc.getCols() > mc.getNumColBlocks() ) {\n \t\t\t\tin = in.mapToPair(new RDDTransformDecodeExpandFunction(\n-\t\t\t\t\t\tIntUtils.toInt(mc.getCols()), mc.getColsPerBlock()));\n+\t\t\t\t\t\t(int)mc.getCols(), mc.getColsPerBlock()));\n \t\t\t\tin = RDDAggregateUtils.mergeByKey(in, false);\n \t\t\t}\n \t\t\t\n@@ -680,8 +679,8 @@ public RDDRemoveEmptyFunctionInMem(boolean rmRows, long len, long brlen, long bc\n \t\t\t//prepare inputs (for internal api compatibility)\n \t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(),arg0._2());\n \t\t\tIndexedMatrixValue offsets = _rmRows ?\n-\t\t\t\tSparkUtils.toIndexedMatrixBlock(arg0._1(), _off.getBlock(IntUtils.toInt(arg0._1().getRowIndex()), 1)) :\n-\t\t\t\tSparkUtils.toIndexedMatrixBlock(arg0._1(), _off.getBlock(1, IntUtils.toInt(arg0._1().getColumnIndex())));\n+\t\t\t\tSparkUtils.toIndexedMatrixBlock(arg0._1(), _off.getBlock((int)arg0._1().getRowIndex(), 1)) :\n+\t\t\t\tSparkUtils.toIndexedMatrixBlock(arg0._1(), _off.getBlock(1, (int)arg0._1().getColumnIndex()));\n \t\t\t\n \t\t\t//execute remove empty operations\n \t\t\tArrayList<IndexedMatrixValue> out = new ArrayList<>();\n@@ -755,7 +754,7 @@ public RDDMapGroupedAggFunction(PartitionedBroadcast<MatrixBlock> pbm, Operator\n \t\t\t//get all inputs\n \t\t\tMatrixIndexes ix = arg0._1();\n \t\t\tMatrixBlock target = arg0._2();\t\t\n-\t\t\tMatrixBlock groups = _pbm.getBlock(IntUtils.toInt(ix.getRowIndex()), 1);\n+\t\t\tMatrixBlock groups = _pbm.getBlock((int)ix.getRowIndex(), 1);\n \t\t\t\n \t\t\t//execute map grouped aggregate operations\n \t\t\tIndexedMatrixValue in1 = SparkUtils.toIndexedMatrixBlock(ix, target);\n@@ -791,7 +790,7 @@ public MatrixBlock call(Tuple2<MatrixIndexes, MatrixBlock> arg0)\n \t\t\t//get all inputs\n \t\t\tMatrixIndexes ix = arg0._1();\n \t\t\tMatrixBlock target = arg0._2();\n-\t\t\tMatrixBlock groups = _pbm.getBlock(IntUtils.toInt(ix.getRowIndex()), 1);\n+\t\t\tMatrixBlock groups = _pbm.getBlock((int)ix.getRowIndex(), 1);\n \t\t\t\n \t\t\t//execute map grouped aggregate operations\n \t\t\treturn groups.groupedAggOperations(target, null, new MatrixBlock(), _ngroups, _op);\n@@ -966,8 +965,8 @@ public RDDTransformDecodeExpandFunction(int clen, int bclen) {\n \t\t\tMatrixBlock inBlk = in._2();\n \t\t\t\n \t\t\t//construct expanded block via leftindexing\n-\t\t\tint cl = IntUtils.toInt(UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _bclen, 0)-1);\n-\t\t\tint cu = IntUtils.toInt(UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _bclen, inBlk.getNumColumns()-1)-1);\n+\t\t\tint cl = (int)UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _bclen, 0)-1;\n+\t\t\tint cu = (int)UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _bclen, inBlk.getNumColumns()-1)-1;\n \t\t\tMatrixBlock out = new MatrixBlock(inBlk.getNumRows(), _clen, false);\n \t\t\tout = out.leftIndexingOperations(inBlk, 0, inBlk.getNumRows()-1, cl, cu, null, UpdateType.INPLACE_PINNED);\n \t\t\t\n@@ -982,7 +981,7 @@ public void setOutputCharacteristicsForGroupedAgg(MatrixCharacteristics mc1, Mat\n \t\t\t}\n \t\t\t\n \t\t\tif ( params.get(Statement.GAGG_NUM_GROUPS) != null) {\n-\t\t\t\tint ngroups = IntUtils.toInt( Double.parseDouble(params.get(Statement.GAGG_NUM_GROUPS)));\n+\t\t\t\tint ngroups = (int) Double.parseDouble(params.get(Statement.GAGG_NUM_GROUPS));\n \t\t\t\tmcOut.set(ngroups, mc1.getCols(), -1, -1); //grouped aggregate with cell output\n \t\t\t}\n \t\t\telse {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java",
                "sha": "4a1c7103af5a1cee32d1691504a42e988b527925",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/PmmSPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/PmmSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/PmmSPInstruction.java",
                "patch": "@@ -47,7 +47,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class PmmSPInstruction extends BinarySPInstruction {\n \tprivate CacheType _type = null;\n@@ -126,7 +125,7 @@ public RDDPMMFunction( CacheType type, PartitionedBroadcast<MatrixBlock> binput,\n \t\t\tMatrixBlock mb2 = arg0._2();\n \t\t\t\n \t\t\t//get the right hand side matrix\n-\t\t\tMatrixBlock mb1 = _pmV.getBlock(IntUtils.toInt(ixIn.getRowIndex()), 1);\n+\t\t\tMatrixBlock mb1 = _pmV.getBlock((int)ixIn.getRowIndex(), 1);\n \t\t\t\n \t\t\t//compute target block indexes\n \t\t\tlong minPos = UtilFunctions.toLong( mb1.minNonZero() );",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/PmmSPInstruction.java",
                "sha": "3914c550b400e7dae28a65b5ef4f73b154e40106",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/QuantilePickSPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/QuantilePickSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/QuantilePickSPInstruction.java",
                "patch": "@@ -47,7 +47,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class QuantilePickSPInstruction extends BinarySPInstruction {\n \tprivate OperationTypes _type = null;\n@@ -221,7 +220,7 @@ private static double lookupKey(JavaPairRDD<MatrixIndexes,MatrixBlock> in, long\n \t\tif( tmp.getNumRows() <= pos )\n \t\t\tthrow new DMLRuntimeException(\"Invalid key lookup for \" +\n \t\t\t\tpos + \" in block of size \" + tmp.getNumRows()+\"x\"+tmp.getNumColumns());\n-\t\treturn val.get(0).quickGetValue(IntUtils.toInt(pos), 0);\n+\t\treturn val.get(0).quickGetValue((int)pos, 0);\n \t}\n \t\n \tprivate static class FilterFunction implements Function<Tuple2<MatrixIndexes,MatrixBlock>, Boolean> \n@@ -338,7 +337,7 @@ public ExtractWeightedQuantileFunction(MatrixCharacteristics mc, double[] qdKeys\n \t\t\t\treturn Collections.emptyIterator();\n \t\t\t\n \t\t\t//determine which quantiles are active\n-\t\t\tint qlen = IntUtils.toInt(Arrays.stream(_qPIDs).filter(i -> i==v1).count());\n+\t\t\tint qlen = (int)Arrays.stream(_qPIDs).filter(i -> i==v1).count();\n \t\t\tint[] qix = new int[qlen];\n \t\t\tfor(int i=0, pos=0; i<_qPIDs.length; i++)\n \t\t\t\tif( _qPIDs[i]==v1 )",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/QuantilePickSPInstruction.java",
                "sha": "18f0bef32d011c96b9973de614ac0f382e184a9d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/QuaternarySPInstruction.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/QuaternarySPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 7,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/QuaternarySPInstruction.java",
                "patch": "@@ -58,7 +58,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.matrix.operators.QuaternaryOperator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class QuaternarySPInstruction extends ComputationSPInstruction {\n \tprivate CPOperand _input4 = null;\n@@ -375,8 +374,8 @@ public RDDQuaternaryPartitionIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock\n \t\t\t\tMatrixIndexes ixIn = arg._1();\n \t\t\t\tMatrixBlock blkIn = arg._2();\n \t\t\t\tMatrixBlock blkOut = new MatrixBlock();\n-\t\t\t\tMatrixBlock mbU = _pmU.getBlock(IntUtils.toInt(ixIn.getRowIndex()), 1);\n-\t\t\t\tMatrixBlock mbV = _pmV.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), 1);\n+\t\t\t\tMatrixBlock mbU = _pmU.getBlock((int)ixIn.getRowIndex(), 1);\n+\t\t\t\tMatrixBlock mbV = _pmV.getBlock((int)ixIn.getColumnIndex(), 1);\n \t\t\t\t\n \t\t\t\t//execute core operation\n \t\t\t\tblkIn.quaternaryOperations(_qop, mbU, mbV, null, blkOut);\n@@ -403,8 +402,8 @@ public RDDQuaternaryFunction2( QuaternaryOperator qop, PartitionedBroadcast<Matr\n \t\t\tMatrixBlock blkIn1 = arg0._2()._1();\n \t\t\tMatrixBlock blkIn2 = arg0._2()._2();\n \t\t\tMatrixBlock blkOut = new MatrixBlock();\n-\t\t\tMatrixBlock mbU = (_pmU!=null)?_pmU.getBlock(IntUtils.toInt(ixIn.getRowIndex()), 1) : blkIn2;\n-\t\t\tMatrixBlock mbV = (_pmV!=null)?_pmV.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), 1) : blkIn2;\n+\t\t\tMatrixBlock mbU = (_pmU!=null)?_pmU.getBlock((int)ixIn.getRowIndex(), 1) : blkIn2;\n+\t\t\tMatrixBlock mbV = (_pmV!=null)?_pmV.getBlock((int)ixIn.getColumnIndex(), 1) : blkIn2;\n \t\t\tMatrixBlock mbW = (_qop.hasFourInputs()) ? blkIn2 : null;\n \t\t\t\n \t\t\t//execute core operation\n@@ -432,8 +431,8 @@ public RDDQuaternaryFunction3( QuaternaryOperator qop, PartitionedBroadcast<Matr\n \t\t\tMatrixBlock blkIn2 = arg0._2()._1()._2();\n \t\t\tMatrixBlock blkIn3 = arg0._2()._2();\n \t\t\tMatrixBlock blkOut = new MatrixBlock();\n-\t\t\tMatrixBlock mbU = (_pmU!=null)?_pmU.getBlock(IntUtils.toInt(ixIn.getRowIndex()), 1) : blkIn2;\n-\t\t\tMatrixBlock mbV = (_pmV!=null)?_pmV.getBlock(IntUtils.toInt(ixIn.getColumnIndex()), 1) : \n+\t\t\tMatrixBlock mbU = (_pmU!=null)?_pmU.getBlock((int)ixIn.getRowIndex(), 1) : blkIn2;\n+\t\t\tMatrixBlock mbV = (_pmV!=null)?_pmV.getBlock((int)ixIn.getColumnIndex(), 1) : \n \t\t\t\t              (_pmU!=null)? blkIn2 : blkIn3;\n \t\t\tMatrixBlock mbW = (_qop.hasFourInputs())? blkIn3 : null;\n \t\t\t",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/QuaternarySPInstruction.java",
                "sha": "a5a5d94d662e6d9a707ff1b06bb320ed45dafbcc",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/RandSPInstruction.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/RandSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 8,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/RandSPInstruction.java",
                "patch": "@@ -65,7 +65,6 @@\n import org.apache.sysml.runtime.matrix.data.RandomMatrixGenerator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n public class RandSPInstruction extends UnarySPInstruction {\n@@ -259,7 +258,7 @@ private void generateRandData(SparkExecutionContext sec) {\n \t\t\t&&  ConfigurationManager.getExecutionMode() != RUNTIME_PLATFORM.SPARK )\n \t\t{\n \t\t\tRandomMatrixGenerator rgen = LibMatrixDatagen.createRandomMatrixGenerator(\n-\t\t\t\tpdf, IntUtils.toInt(lrows), IntUtils.toInt(lcols), rowsInBlock, colsInBlock, \n+\t\t\t\tpdf, (int)lrows, (int)lcols, rowsInBlock, colsInBlock, \n \t\t\t\tsparsity, minValue, maxValue, pdfParams);\n \t\t\tMatrixBlock mb = MatrixBlock.randOperations(rgen, lSeed);\n \t\t\t\n@@ -291,7 +290,7 @@ private void generateRandData(SparkExecutionContext sec) {\n \t\t\t}\n \t\t\t\n \t\t\t//for load balancing: degree of parallelism such that ~128MB per partition\n-\t\t\tint numPartitions = IntUtils.toInt( Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1));\n+\t\t\tint numPartitions = (int) Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1);\n \t\t\t\n \t\t\t//create seeds rdd \n \t\t\tseedsRDD = sec.getSparkContext().parallelizePairs(seeds, numPartitions);\n@@ -324,7 +323,7 @@ private void generateRandData(SparkExecutionContext sec) {\n \t\t\t}\n \t\t\t\n \t\t\t//for load balancing: degree of parallelism such that ~128MB per partition\n-\t\t\tint numPartitions = IntUtils.toInt( Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1));\n+\t\t\tint numPartitions = (int) Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1);\n \t\t\t\n \t\t\t//create seeds rdd \n \t\t\tseedsRDD = sec.getSparkContext()\n@@ -382,7 +381,7 @@ private void generateSequence(SparkExecutionContext sec) {\n \t\t\t}\n \t\t\t\n \t\t\t//for load balancing: degree of parallelism such that ~128MB per partition\n-\t\t\tint numPartitions = IntUtils.toInt( Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1));\n+\t\t\tint numPartitions = (int) Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1);\n \t\t\t\n \t\t\t//create offset rdd\n \t\t\toffsetsRDD = sec.getSparkContext().parallelize(offsets, numPartitions);\n@@ -409,7 +408,7 @@ private void generateSequence(SparkExecutionContext sec) {\n \t\t\t}\n \t\t\t\n \t\t\t//for load balancing: degree of parallelism such that ~128MB per partition\n-\t\t\tint numPartitions = IntUtils.toInt( Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1));\n+\t\t\tint numPartitions = (int) Math.max(Math.min(totalSize/hdfsBlkSize, numBlocks), 1);\n \t\t\t\n \t\t\t//create seeds rdd \n \t\t\toffsetsRDD = sec.getSparkContext()\n@@ -443,14 +442,14 @@ private void generateSample(SparkExecutionContext sec) {\n \t\t\tLOG.trace(\"Process RandSPInstruction sample with range=\"+ maxValue +\", size=\"+ lrows +\", replace=\"+ replace + \", seed=\" + seed);\n \t\t\n \t\t// sampling rate that guarantees a sample of size >= sampleSizeLowerBound 99.99% of the time.\n-\t\tdouble fraction = SamplingUtils.computeFractionForSampleSize(IntUtils.toInt(lrows), UtilFunctions.toLong(maxValue), replace);\n+\t\tdouble fraction = SamplingUtils.computeFractionForSampleSize((int)lrows, UtilFunctions.toLong(maxValue), replace);\n \t\t\n \t\tWell1024a bigrand = LibMatrixDatagen.setupSeedsForRand(seed);\n \n \t\t// divide the population range across numPartitions by creating SampleTasks\n \t\tdouble hdfsBlockSize = InfrastructureAnalyzer.getHDFSBlockSize();\n \t\tlong outputSize = MatrixBlock.estimateSizeDenseInMemory(lrows,1);\n-\t\tint numPartitions = IntUtils.toInt( Math.ceil((double)outputSize/hdfsBlockSize));\n+\t\tint numPartitions = (int) Math.ceil((double)outputSize/hdfsBlockSize);\n \t\tlong partitionSize = (long) Math.ceil(maxValue/numPartitions);\n \n \t\tArrayList<SampleTask> offsets = new ArrayList<>();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/RandSPInstruction.java",
                "sha": "2b9adcb532476e5d4c55a3920764e58493860c45",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/ReorgSPInstruction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/ReorgSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/ReorgSPInstruction.java",
                "patch": "@@ -58,7 +58,6 @@\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ReorgSPInstruction extends UnarySPInstruction {\n \t// sort-specific attributes (to enable variable attributes)\n@@ -170,7 +169,7 @@ else if ( opcode.equalsIgnoreCase(\"rsort\") ) //ORDER\n \t\t\t\t// extract column (if necessary) and sort \n \t\t\t\tif( !singleCol )\n \t\t\t\t\tout = out.filter(new IsBlockInRange(1, mcIn.getRows(), cols[0], cols[0], mcIn))\n-\t\t\t\t\t\t.mapValues(new ExtractColumn(IntUtils.toInt(UtilFunctions.computeCellInBlock(cols[0], mcIn.getColsPerBlock()))));\n+\t\t\t\t\t\t.mapValues(new ExtractColumn((int)UtilFunctions.computeCellInBlock(cols[0], mcIn.getColsPerBlock())));\n \t\t\t\t\n \t\t\t\t//actual index/data sort operation\n \t\t\t\tif( ixret ) //sort indexes \n@@ -269,7 +268,7 @@ public RDDDiagV2MFunction(MatrixCharacteristics mcIn) {\n \t\t\tret.add(new Tuple2<>(ixOut,blkOut));\n \t\t\t\n \t\t\t// insert newly created empty blocks for entire row\n-\t\t\tint numBlocks = IntUtils.toInt( Math.ceil((double)_mcIn.getRows()/_mcIn.getRowsPerBlock()));\n+\t\t\tint numBlocks = (int) Math.ceil((double)_mcIn.getRows()/_mcIn.getRowsPerBlock());\n \t\t\tfor(int i = 1; i <= numBlocks; i++) {\n \t\t\t\tif(i != ixOut.getColumnIndex()) {\n \t\t\t\t\tint lrlen = UtilFunctions.computeBlockSize(_mcIn.getRows(), rix, _mcIn.getRowsPerBlock());",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/ReorgSPInstruction.java",
                "sha": "3e09d17448bf6c5d76cd17720539638f15679517",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/RmmSPInstruction.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/RmmSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/RmmSPInstruction.java",
                "patch": "@@ -48,7 +48,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateBinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class RmmSPInstruction extends BinarySPInstruction {\n \n@@ -115,7 +114,7 @@ private static int getNumJoinPartitions(MatrixCharacteristics mc1, MatrixCharact\n \t\t\t* ((long) Math.ceil((double)mc2.getCols()/mc2.getColsPerBlock()));\n \t\tdouble matrix2PSize = OptimizerUtils.estimatePartitionedSizeExactSparsity(mc2)\n \t\t\t* ((long) Math.ceil((double)mc1.getRows()/mc1.getRowsPerBlock()));\n-\t\treturn IntUtils.toInt( Math.max(Math.ceil((matrix1PSize+matrix2PSize)/hdfsBlockSize), 1));\n+\t\treturn (int) Math.max(Math.ceil((matrix1PSize+matrix2PSize)/hdfsBlockSize), 1);\n \t}\n \n \tprivate static class RmmReplicateFunction implements PairFlatMapFunction<Tuple2<MatrixIndexes, MatrixBlock>, TripleIndexes, MatrixBlock> ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/RmmSPInstruction.java",
                "sha": "90e539662da08a23d6226a1fdd43fee03b02703f",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/SpoofSPInstruction.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/SpoofSPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 9,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/SpoofSPInstruction.java",
                "patch": "@@ -65,7 +65,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -145,7 +144,7 @@ else if(_in[i].getDataType()==DataType.SCALAR) {\n \t\t\t\t\tlong numBlocks = (op.getCellType()==CellType.ROW_AGG ) ? \n \t\t\t\t\t\tmcIn.getNumRowBlocks() : mcIn.getNumColBlocks();\n \t\t\t\t\tout = RDDAggregateUtils.aggByKeyStable(out, aggop,\n-\t\t\t\t\t\t\tIntUtils.toInt(Math.min(out.getNumPartitions(), numBlocks)), false);\n+\t\t\t\t\t\t(int)Math.min(out.getNumPartitions(), numBlocks), false);\n \t\t\t\t}\n \t\t\t\tsec.setRDDHandleForVariable(_out.getName(), out);\n \t\t\t\t\n@@ -184,7 +183,7 @@ else if(_class.getSuperclass() == SpoofOuterProduct.class) //OUTER\n \t\t\t\tif(type == OutProdType.LEFT_OUTER_PRODUCT || type == OutProdType.RIGHT_OUTER_PRODUCT ) {\n \t\t\t\t\tlong numBlocks = mcOut.getNumRowBlocks() * mcOut.getNumColBlocks();\n \t\t\t\t\tout = RDDAggregateUtils.sumByKeyStable(out,\n-\t\t\t\t\t\t\tIntUtils.toInt(Math.min(out.getNumPartitions(), numBlocks)), false);\n+\t\t\t\t\t\t(int)Math.min(out.getNumPartitions(), numBlocks), false);\n \t\t\t\t}\n \t\t\t\tsec.setRDDHandleForVariable(_out.getName(), out);\n \t\t\t\t\n@@ -207,7 +206,7 @@ else if( _class.getSuperclass() == SpoofRowwise.class ) { //ROW\n \t\t\tlong clen2 = op.getRowType().isConstDim2(op.getConstDim2()) ? op.getConstDim2() :\n \t\t\t\top.getRowType().isRowTypeB1() ? sec.getMatrixCharacteristics(_in[1].getName()).getCols() : -1;\n \t\t\tRowwiseFunction fmmc = new RowwiseFunction(_class.getName(), _classBytes, bcVect2,\n-\t\t\t\tbcMatrices, scalars, mcIn.getRowsPerBlock(), IntUtils.toInt(mcIn.getCols()), IntUtils.toInt(clen2));\n+\t\t\t\tbcMatrices, scalars, mcIn.getRowsPerBlock(), (int)mcIn.getCols(), (int)clen2);\n \t\t\tout = in.mapPartitionsToPair(fmmc, op.getRowType()==RowType.ROW_AGG\n \t\t\t\t\t|| op.getRowType() == RowType.NO_AGG);\n \t\t\t\n@@ -223,7 +222,7 @@ else if( _class.getSuperclass() == SpoofRowwise.class ) { //ROW\n \t\t\t{\n \t\t\t\tif( op.getRowType()==RowType.ROW_AGG && mcIn.getCols() > mcIn.getColsPerBlock() ) {\n \t\t\t\t\tout = RDDAggregateUtils.sumByKeyStable(out,\n-\t\t\t\t\t\t\tIntUtils.toInt(Math.min(out.getNumPartitions(), mcIn.getNumRowBlocks())), false);\n+\t\t\t\t\t\t(int)Math.min(out.getNumPartitions(), mcIn.getNumRowBlocks()), false);\n \t\t\t\t}\n \t\t\t\tsec.setRDDHandleForVariable(_out.getName(), out);\n \t\t\t\t\n@@ -264,8 +263,8 @@ else if( _class.getSuperclass() == SpoofRowwise.class ) { //ROW\n \t}\n \t\n \tprivate static boolean[] getMatrixBroadcastVector(SparkExecutionContext sec, CPOperand[] inputs, boolean[] bcVect) {\n-\t\tint numMtx = IntUtils.toInt(Arrays.stream(inputs)\n-\t\t\t.filter(in -> in.getDataType().isMatrix()).count());\n+\t\tint numMtx = (int) Arrays.stream(inputs)\n+\t\t\t.filter(in -> in.getDataType().isMatrix()).count();\n \t\tboolean[] ret = new boolean[numMtx];\n \t\tfor(int i=0, pos=0; i<inputs.length; i++)\n \t\t\tif( inputs[i].getDataType().isMatrix() )\n@@ -381,9 +380,9 @@ protected SpoofFunction(String className, byte[] classBytes, boolean[] bcInd, Ar\n \t\t\tfor( int i=0, posRdd=0, posBc=0; i<_bcInd.length; i++ ) {\n \t\t\t\tif( _bcInd[i] ) {\n \t\t\t\t\tPartitionedBroadcast<MatrixBlock> pb = _inputs.get(posBc++);\n-\t\t\t\t\tint rowIndex = IntUtils.toInt((outer && i==2) ? ixIn.getColumnIndex() : \n+\t\t\t\t\tint rowIndex = (int) ((outer && i==2) ? ixIn.getColumnIndex() : \n \t\t\t\t\t\t(pb.getNumRowBlocks()>=ixIn.getRowIndex())?ixIn.getRowIndex():1);\n-\t\t\t\t\tint colIndex = IntUtils.toInt((outer && i==2) ? 1 : \n+\t\t\t\t\tint colIndex = (int) ((outer && i==2) ? 1 : \n \t\t\t\t\t\t(pb.getNumColumnBlocks()>=ixIn.getColumnIndex())?ixIn.getColumnIndex():1);\n \t\t\t\t\tret.add(pb.getBlock(rowIndex, colIndex));\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/SpoofSPInstruction.java",
                "sha": "8f63427b59292bfed5a090241730560592670a54",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/Tsmm2SPInstruction.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/Tsmm2SPInstruction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 14,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/Tsmm2SPInstruction.java",
                "patch": "@@ -50,7 +50,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateBinaryOperator;\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -89,17 +88,17 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t\t\t_type.isLeft() ? mc.getColsPerBlock()+1 : 1, mc.getCols(), mc))\n \t\t\t  .mapToPair(new ShiftTSMMIndexesFunction(_type));\t\t\n \t\tPartitionedBlock<MatrixBlock> pmb = SparkExecutionContext.toPartitionedMatrixBlock(tmp1, \n-\t\t\t\tIntUtils.toInt(_type.isLeft() ? mc.getRows() : mc.getRows() - mc.getRowsPerBlock()), \n-\t\t\t\tIntUtils.toInt(_type.isLeft() ? mc.getCols()-mc.getColsPerBlock() : mc.getCols()), \n+\t\t\t\t(int)(_type.isLeft() ? mc.getRows() : mc.getRows() - mc.getRowsPerBlock()), \n+\t\t\t\t(int)(_type.isLeft() ? mc.getCols()-mc.getColsPerBlock() : mc.getCols()), \n \t\t\t\tmc.getRowsPerBlock(), mc.getColsPerBlock(), -1L);\n \t\tBroadcast<PartitionedBlock<MatrixBlock>> bpmb = sec.getSparkContext().broadcast(pmb);\n \t\t\n \t\t//step 2: second pass of X, compute tsmm/mapmm and aggregate result blocks\n-\t\tint outputDim = IntUtils.toInt(_type.isLeft() ? mc.getCols() : mc.getRows());\n+\t\tint outputDim = (int) (_type.isLeft() ? mc.getCols() : mc.getRows());\n \t\tif( OptimizerUtils.estimateSize(outputDim, outputDim) <= 32*1024*1024 ) { //default: <=32MB\n \t\t\t//output large blocks and reduceAll to avoid skew on combineByKey\n \t\t\tJavaRDD<MatrixBlock> tmp2 = in.map(\n-\t\t\t\t\tnew RDDTSMM2ExtFunction(bpmb, _type, outputDim, IntUtils.toInt(mc.getRowsPerBlock())));\n+\t\t\t\t\tnew RDDTSMM2ExtFunction(bpmb, _type, outputDim, (int)mc.getRowsPerBlock()));\n \t\t\tMatrixBlock out = RDDAggregateUtils.sumStable(tmp2);\n \t\t      \n \t\t\t//put output block into symbol table (no lineage because single block)\n@@ -151,8 +150,8 @@ public RDDTSMM2Function( Broadcast<PartitionedBlock<MatrixBlock>> pb, MMTSJType\n \t\t\tif( _type.isLeft() ? ixin.getColumnIndex() == 1 : ixin.getRowIndex() == 1 ) {\n \t\t\t\t//execute block mapmm operation for full block only (output two blocks, due to symmetry)\n \t\t\t\tMatrixBlock mbin2 = _pb.getValue().getBlock( //lookup broadcast block\n-\t\t\t\t\t\tIntUtils.toInt(_type.isLeft()?ixin.getRowIndex():1), \n-\t\t\t\t\t\tIntUtils.toInt(_type.isLeft()?1:ixin.getColumnIndex()));\n+\t\t\t\t\t\t(int)(_type.isLeft()?ixin.getRowIndex():1), \n+\t\t\t\t\t\t(int)(_type.isLeft()?1:ixin.getColumnIndex()));\n \t\t\t\tMatrixBlock mbin2t = transpose(mbin2, new MatrixBlock()); //prep for transpose rewrite mm\n \t\t\t\t\n \t\t\t\tMatrixBlock out2 = (MatrixBlock) OperationsOnMatrixValues.matMult( //mm\n@@ -206,25 +205,25 @@ public MatrixBlock call(Tuple2<MatrixIndexes, MatrixBlock> arg0)\n \t\t\t\n \t\t\t//execute block tsmm operation\n \t\t\tMatrixBlock out1 = mbin.transposeSelfMatrixMultOperations(new MatrixBlock(), _type);\n-\t\t\tint ix = IntUtils.toInt((_type.isLeft() ? ixin.getColumnIndex() : ixin.getRowIndex())-1) * _blen;\n+\t\t\tint ix = (int) ((_type.isLeft() ? ixin.getColumnIndex() : ixin.getRowIndex())-1) * _blen;\n \t\t\tout.copy(ix, ix+out1.getNumRows()-1, ix, ix+out1.getNumColumns()-1, out1, true);\n \t\t\t\n \t\t\tif( fullBlock ) {\n \t\t\t\t//execute block mapmm operation for full block only (output two blocks, due to symmetry)\n \t\t\t\tMatrixBlock mbin2 = _pb.getValue().getBlock( //lookup broadcast block\n-\t\t\t\t\t\tIntUtils.toInt(_type.isLeft()?ixin.getRowIndex():1), \n-\t\t\t\t\t\tIntUtils.toInt(_type.isLeft()?1:ixin.getColumnIndex()));\n+\t\t\t\t\t\t(int)(_type.isLeft()?ixin.getRowIndex():1), \n+\t\t\t\t\t\t(int)(_type.isLeft()?1:ixin.getColumnIndex()));\n \t\t\t\tMatrixBlock mbin2t = transpose(mbin2, new MatrixBlock()); //prep for transpose rewrite mm\n \t\t\t\t\n \t\t\t\tMatrixBlock out2 = OperationsOnMatrixValues.matMult( //mm\n \t\t\t\t\t\t_type.isLeft() ? mbin2t : mbin, _type.isLeft() ? mbin : mbin2t, new MatrixBlock(), _op);\n \t\t\t\t\n \t\t\t\tMatrixIndexes ixout2 = _type.isLeft() ? new MatrixIndexes(2,1) : new MatrixIndexes(1,2);\n-\t\t\t\tout.copy(IntUtils.toInt(ixout2.getRowIndex()-1)*_blen, IntUtils.toInt(ixout2.getRowIndex()-1)*_blen+out2.getNumRows()-1, \n-\t\t\t\t\t\tIntUtils.toInt(ixout2.getColumnIndex()-1)*_blen, IntUtils.toInt(ixout2.getColumnIndex()-1)*_blen+out2.getNumColumns()-1, out2, true);\n+\t\t\t\tout.copy((int)(ixout2.getRowIndex()-1)*_blen, (int)(ixout2.getRowIndex()-1)*_blen+out2.getNumRows()-1, \n+\t\t\t\t\t\t(int)(ixout2.getColumnIndex()-1)*_blen, (int)(ixout2.getColumnIndex()-1)*_blen+out2.getNumColumns()-1, out2, true);\n \t\t\t\tMatrixBlock out3 = transpose(out2, new MatrixBlock()); \n-\t\t\t\tout.copy(IntUtils.toInt(ixout2.getColumnIndex()-1)*_blen, IntUtils.toInt(ixout2.getColumnIndex()-1)*_blen+out3.getNumRows()-1, \n-\t\t\t\t\t\tIntUtils.toInt(ixout2.getRowIndex()-1)*_blen, IntUtils.toInt(ixout2.getRowIndex()-1)*_blen+out3.getNumColumns()-1, out3, true);\n+\t\t\t\tout.copy((int)(ixout2.getColumnIndex()-1)*_blen, (int)(ixout2.getColumnIndex()-1)*_blen+out3.getNumRows()-1, \n+\t\t\t\t\t\t(int)(ixout2.getRowIndex()-1)*_blen, (int)(ixout2.getRowIndex()-1)*_blen+out3.getNumColumns()-1, out3, true);\n \t\t\t}\n \t\t\t\n \t\t\treturn out;",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/Tsmm2SPInstruction.java",
                "sha": "5bb686b5601a41e844de752780c1f45d15720a19",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/data/BlockPartitioner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/data/BlockPartitioner.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/data/BlockPartitioner.java",
                "patch": "@@ -25,7 +25,6 @@\n \n import org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * Default partitioner used for all binary block rdd operations in order\n@@ -76,7 +75,7 @@ else if( ncblks < dimBlks ) { //tall and skinny\n \t\t}\n \t\t\n \t\t//compute meta data for runtime\n-\t\t_ncparts = IntUtils.toInt(Math.ceil((double)ncblks/_cbPerPart));\n+\t\t_ncparts = (int)Math.ceil((double)ncblks/_cbPerPart);\n \t\t_numParts = numParts;\n \t}\n \t\n@@ -91,8 +90,8 @@ public int getPartition(Object arg0)\n \t\t\t\n \t\t//get partition id\n \t\tMatrixIndexes ix = (MatrixIndexes) arg0;\n-\t\tint ixr = IntUtils.toInt((ix.getRowIndex()-1)/_rbPerPart);\n-\t\tint ixc = IntUtils.toInt((ix.getColumnIndex()-1)/_cbPerPart);\n+\t\tint ixr = (int)((ix.getRowIndex()-1)/_rbPerPart);\n+\t\tint ixc = (int)((ix.getColumnIndex()-1)/_cbPerPart);\n \t\tint id = ixr * _ncparts + ixc;\n \t\t\n \t\t//ensure valid range",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/data/BlockPartitioner.java",
                "sha": "677fbbf1ad0991c1dc12eeae3909de2da82dc618",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBlock.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBlock.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBlock.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.sysml.runtime.controlprogram.caching.CacheBlockFactory;\n import org.apache.sysml.runtime.util.FastBufferedDataInputStream;\n import org.apache.sysml.runtime.util.FastBufferedDataOutputStream;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * This class is for partitioned matrix/frame blocks, to be used as broadcasts. \n@@ -133,11 +132,11 @@ public long getNumColumnsPerBlock() {\n \t}\n \n \tpublic int getNumRowBlocks() {\n-\t\treturn IntUtils.toInt(Math.ceil((double)_rlen/_brlen));\n+\t\treturn (int)Math.ceil((double)_rlen/_brlen);\n \t}\n \n \tpublic int getNumColumnBlocks() {\n-\t\treturn IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\treturn (int)Math.ceil((double)_clen/_bclen);\n \t}\n \n \t@SuppressWarnings(\"unchecked\")",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBlock.java",
                "sha": "8a4999b6ccdfcb84b79efb860fd7a95abe887c5a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBroadcast.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBroadcast.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBroadcast.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.sysml.runtime.matrix.data.OperationsOnMatrixValues;\n import org.apache.sysml.runtime.matrix.data.Pair;\n import org.apache.sysml.runtime.util.IndexRange;\n-import org.apache.sysml.utils.IntUtils;\n \n /**\n  * This class is a wrapper around an array of broadcasts of partitioned matrix/frame blocks,\n@@ -72,11 +71,11 @@ public long getNumCols() {\n \t}\n \n \tpublic int getNumRowBlocks() {\n-\t\treturn IntUtils.toInt(_mc.getNumRowBlocks());\n+\t\treturn (int)_mc.getNumRowBlocks();\n \t}\n \t\n \tpublic int getNumColumnBlocks() {\n-\t\treturn IntUtils.toInt(_mc.getNumColBlocks());\n+\t\treturn (int)_mc.getNumColBlocks();\n \t}\n \n \tpublic static int computeBlocksPerPartition(long rlen, long clen, long brlen, long bclen) {",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/data/PartitionedBroadcast.java",
                "sha": "ab29c4cc9af36beb7b9faf92ed9a6db6839be4e6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractBlockForBinaryReblock.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractBlockForBinaryReblock.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractBlockForBinaryReblock.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class ExtractBlockForBinaryReblock implements PairFlatMapFunction<Tuple2<MatrixIndexes,MatrixBlock>, MatrixIndexes, MatrixBlock> \n {\n@@ -97,8 +96,8 @@ public ExtractBlockForBinaryReblock(MatrixCharacteristics mcIn, MatrixCharacteri\n \t\t\t\t\tblk.setNonZeros(in.getNonZeros());\n \t\t\t\t}\n \t\t\t\telse { //general case\n-\t\t\t\t\tfor(int i2 = 0; i2 <= IntUtils.toInt(rowUpper-rowLower); i2++)\n-\t\t\t\t\t\tfor(int j2 = 0; j2 <= IntUtils.toInt(colUpper-colLower); j2++)\n+\t\t\t\t\tfor(int i2 = 0; i2 <= (int)(rowUpper-rowLower); i2++)\n+\t\t\t\t\t\tfor(int j2 = 0; j2 <= (int)(colUpper-colLower); j2++)\n \t\t\t\t\t\t\tblk.appendValue(cixi+i2, cixj+j2, in.quickGetValue(aixi+i2, aixj+j2));\n \t\t\t\t}\n \t\t\t\tretVal.add(new Tuple2<>(indx, blk));",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractBlockForBinaryReblock.java",
                "sha": "0ce654b205b84a79361333e7245312a5eb6af01e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractGroup.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractGroup.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractGroup.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.sysml.runtime.matrix.operators.AggregateOperator;\n import org.apache.sysml.runtime.matrix.operators.Operator;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public abstract class ExtractGroup implements Serializable \n {\n@@ -66,7 +65,7 @@ public ExtractGroup( long bclen, long ngroups, Operator op ) {\n \t\tif(_op instanceof AggregateOperator && _ngroups > 0 \n \t\t\t&& OptimizerUtils.isValidCPDimensions(_ngroups, target.getNumColumns()) ) \n \t\t{\n-\t\t\tMatrixBlock tmp = group.groupedAggOperations(target, null, new MatrixBlock(), IntUtils.toInt(_ngroups), _op);\n+\t\t\tMatrixBlock tmp = group.groupedAggOperations(target, null, new MatrixBlock(), (int)_ngroups, _op);\n \t\t\t\n \t\t\tfor(int i=0; i<tmp.getNumRows(); i++) {\n \t\t\t\tfor( int j=0; j<tmp.getNumColumns(); j++ ) {\n@@ -140,7 +139,7 @@ public ExtractGroupBroadcast( PartitionedBroadcast<MatrixBlock> pbm, long bclen,\n \t\t\t\tthrows Exception \n \t\t{\n \t\t\tMatrixIndexes ix = arg._1;\n-\t\t\tMatrixBlock group = _pbm.getBlock(IntUtils.toInt(ix.getRowIndex()), 1);\n+\t\t\tMatrixBlock group = _pbm.getBlock((int)ix.getRowIndex(), 1);\n \t\t\tMatrixBlock target = arg._2;\n \t\t\t\n \t\t\treturn execute(ix, group, target).iterator();",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/ExtractGroup.java",
                "sha": "3e6908be53b5130827a03e5a0a48102271c10752",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/MatrixVectorBinaryOpPartitionFunction.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/MatrixVectorBinaryOpPartitionFunction.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 3,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/functions/MatrixVectorBinaryOpPartitionFunction.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.sysml.runtime.matrix.data.MatrixBlock;\n import org.apache.sysml.runtime.matrix.data.MatrixIndexes;\n import org.apache.sysml.runtime.matrix.operators.BinaryOperator;\n-import org.apache.sysml.utils.IntUtils;\n \n public class MatrixVectorBinaryOpPartitionFunction implements PairFlatMapFunction<Iterator<Tuple2<MatrixIndexes,MatrixBlock>>, MatrixIndexes,MatrixBlock>\n {\n@@ -75,8 +74,8 @@ public MapBinaryPartitionIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> i\n \t\t\tMatrixBlock in1 = arg._2();\n \t\t\t\n \t\t\t//get the rhs block \n-\t\t\tint rix= IntUtils.toInt((_vtype==VectorType.COL_VECTOR) ? ix.getRowIndex() : 1);\n-\t\t\tint cix= IntUtils.toInt((_vtype==VectorType.COL_VECTOR) ? 1 : ix.getColumnIndex());\n+\t\t\tint rix= (int)((_vtype==VectorType.COL_VECTOR) ? ix.getRowIndex() : 1);\n+\t\t\tint cix= (int)((_vtype==VectorType.COL_VECTOR) ? 1 : ix.getColumnIndex());\n \t\t\tMatrixBlock in2 = _pmV.getBlock(rix, cix);\n \t\t\t\t\n \t\t\t//execute the binary operation",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/functions/MatrixVectorBinaryOpPartitionFunction.java",
                "sha": "afd2e1a23713dd7674c4f6c36f13083e32369760",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/FrameRDDConverterUtils.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/FrameRDDConverterUtils.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 23,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/utils/FrameRDDConverterUtils.java",
                "patch": "@@ -67,7 +67,6 @@\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.FastStringTokenizer;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -102,7 +101,7 @@\n \t\t\n \t\t//prepare default schema if needed\n \t\tif( schema == null || schema.length==1 )\n-\t\t\tschema = UtilFunctions.nCopies(IntUtils.toInt(mc.getCols()), ValueType.STRING);\n+\t\t\tschema = UtilFunctions.nCopies((int)mc.getCols(), ValueType.STRING);\n \n \t\t//convert csv rdd to binary block rdd (w/ partial blocks)\n \t\tJavaPairRDD<Long, FrameBlock> out = prepinput.mapPartitionsToPair(\n@@ -154,7 +153,7 @@\n \t\t\tJavaPairRDD<Long, Text> input, MatrixCharacteristics mc, ValueType[] schema ) {\n \t\t//prepare default schema if needed\n \t\tif( schema == null || schema.length==1 ) {\n-\t\t\tschema = UtilFunctions.nCopies(IntUtils.toInt(mc.getCols()), \n+\t\t\tschema = UtilFunctions.nCopies((int)mc.getCols(), \n \t\t\t\t(schema!=null) ? schema[0] : ValueType.STRING);\n \t\t}\n \t\t\n@@ -190,7 +189,7 @@\n \t\tif(mcIn.getCols() > mcIn.getColsPerBlock()) {\n \t\t\t//split matrix blocks into extended matrix blocks \n \t\t\tin = in.flatMapToPair(new MatrixFrameReblockFunction(mcIn));\n-\t\t\tmc.setBlockSize(MatrixFrameReblockFunction.computeBlockSize(mc), IntUtils.toInt(mc.getCols()));\n+\t\t\tmc.setBlockSize(MatrixFrameReblockFunction.computeBlockSize(mc), (int)mc.getCols());\n \t\t\t\n \t\t\t//shuffle matrix blocks (instead of frame blocks) in order to exploit \n \t\t\t//sparse formats (for sparse or wide matrices) during shuffle\n@@ -239,8 +238,8 @@\n \t\t\t\tdf.javaRDD().zipWithIndex(); //zip row index\n \n \t\t//convert data frame to frame schema (prepare once)\n-\t\tString[] colnames = new String[IntUtils.toInt(mc.getCols())];\n-\t\tValueType[] fschema = new ValueType[IntUtils.toInt(mc.getCols())];\n+\t\tString[] colnames = new String[(int)mc.getCols()];\n+\t\tValueType[] fschema = new ValueType[(int)mc.getCols()];\n \t\tint colVect = convertDFSchemaToFrameSchema(df.schema(), colnames, fschema, containsID);\n \t\tout.set(colnames, fschema); //make schema available\n \t\t\n@@ -261,7 +260,7 @@\n \t\t\t\t\n \t\t//create data frame schema\n \t\tif( schema == null )\n-\t\t\tschema = UtilFunctions.nCopies(IntUtils.toInt(mc.getCols()), ValueType.STRING);\n+\t\t\tschema = UtilFunctions.nCopies((int)mc.getCols(), ValueType.STRING);\n \t\tStructType dfSchema = convertFrameSchemaToDFSchema(schema, true);\n \t\n \t\t//rdd to data frame conversion\n@@ -553,7 +552,7 @@ public CSVToBinaryBlockFunction(MatrixCharacteristics mc, ValueType[] schema, bo\n \t\t\t_schema = schema;\n \t\t\t_hasHeader = hasHeader;\n \t\t\t_delim = delim;\n-\t\t\t_maxRowsPerBlock = Math.max(IntUtils.toInt(FrameBlock.BUFFER_SIZE/_clen), 1);\n+\t\t\t_maxRowsPerBlock = Math.max((int) (FrameBlock.BUFFER_SIZE/_clen), 1);\n \t\t}\n \n \t\t@Override\n@@ -564,7 +563,7 @@ public CSVToBinaryBlockFunction(MatrixCharacteristics mc, ValueType[] schema, bo\n \n \t\t\tlong ix = -1;\n \t\t\tFrameBlock fb = null;\n-\t\t\tString[] tmprow = new String[IntUtils.toInt(_clen)]; \n+\t\t\tString[] tmprow = new String[(int)_clen]; \n \t\t\t\n \t\t\twhile( arg0.hasNext() )\n \t\t\t{\n@@ -576,11 +575,11 @@ public CSVToBinaryBlockFunction(MatrixCharacteristics mc, ValueType[] schema, bo\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \t\t\t\tif( row.startsWith(TfUtils.TXMTD_MVPREFIX) ) {\n-\t\t\t\t\t_mvMeta = Arrays.asList(Arrays.copyOfRange(IOUtilFunctions.splitCSV(row, _delim), 1, IntUtils.toInt(_clen+1)));\n+\t\t\t\t\t_mvMeta = Arrays.asList(Arrays.copyOfRange(IOUtilFunctions.splitCSV(row, _delim), 1, (int)_clen+1));\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \t\t\t\telse if( row.startsWith(TfUtils.TXMTD_NDPREFIX) ) {\n-\t\t\t\t\t_ndMeta = Arrays.asList(Arrays.copyOfRange(IOUtilFunctions.splitCSV(row, _delim), 1, IntUtils.toInt(_clen+1)));\n+\t\t\t\t\t_ndMeta = Arrays.asList(Arrays.copyOfRange(IOUtilFunctions.splitCSV(row, _delim), 1, (int)_clen+1));\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \t\t\t\t\n@@ -717,7 +716,7 @@ public DataFrameToBinaryBlockFunction(MatrixCharacteristics mc, String[] colname\n \t\t\t_schema = schema;\n \t\t\t_containsID = containsID;\n \t\t\t_colVect = colVect;\n-\t\t\t_maxRowsPerBlock = Math.max(IntUtils.toInt(FrameBlock.BUFFER_SIZE/_clen), 1);\n+\t\t\t_maxRowsPerBlock = Math.max((int) (FrameBlock.BUFFER_SIZE/_clen), 1);\n \t\t}\n \t\t\n \t\t@Override\n@@ -728,7 +727,7 @@ public DataFrameToBinaryBlockFunction(MatrixCharacteristics mc, String[] colname\n \n \t\t\tlong ix = -1;\n \t\t\tFrameBlock fb = null;\n-\t\t\tObject[] tmprow = new Object[IntUtils.toInt(_clen)];\n+\t\t\tObject[] tmprow = new Object[(int)_clen];\n \t\t\t\n \t\t\twhile( arg0.hasNext() )\n \t\t\t{\n@@ -816,7 +815,7 @@ protected CellToBinaryBlockFunction(MatrixCharacteristics mc)\n \t\t\t_clen = mc.getCols();\n \t\t\t\n \t\t\t//determine upper bounded buffer len\n-\t\t\t_bufflen = IntUtils.toInt(Math.min(_rlen*_clen, FrameBlock.BUFFER_SIZE));\n+\t\t\t_bufflen = (int) Math.min(_rlen*_clen, FrameBlock.BUFFER_SIZE);\n \t\t}\n \n \t\tprotected void flushBufferToList( FrameReblockBuffer rbuff,  ArrayList<Tuple2<Long,FrameBlock>> ret ) \n@@ -858,7 +857,7 @@ protected TextToBinaryBlockFunction(MatrixCharacteristics mc, ValueType[] schema\n \t\t\t\tst.reset( strVal );\n \t\t\t\tlong row = st.nextLong();\n \t\t\t\tlong col = st.nextLong();\n-\t\t\t\tObject val = UtilFunctions.stringToObject(_schema[IntUtils.toInt(col-1)], st.nextToken());\n+\t\t\t\tObject val = UtilFunctions.stringToObject(_schema[(int)col-1], st.nextToken());\n \t\t\t\t\n \t\t\t\t//flush buffer if necessary\n \t\t\t\tif( rbuff.getSize() >= rbuff.getCapacity() )\n@@ -909,16 +908,16 @@ public MatrixFrameReblockFunction(MatrixCharacteristics mc) {\n \t\t\tlong rowix = (ix.getRowIndex()-1)*_brlen+1;\n \n \t\t\t//global index within frame block (0-based)\n-\t\t\tlong cl = IntUtils.toInt((ix.getColumnIndex()-1)*_bclen);\n+\t\t\tlong cl = (int)((ix.getColumnIndex()-1)*_bclen);\n \t\t\tlong cu = Math.min(cl+mb.getNumColumns()-1, _clen);\n \n \t\t\t//prepare output frame blocks \n \t\t\tfor( int i=0; i<mb.getNumRows(); i+=_maxRowsPerBlock ) {\n \t\t\t\tint ru = Math.min(i+_maxRowsPerBlock, mb.getNumRows())-1;\n \t\t\t\tlong rix = UtilFunctions.computeBlockIndex(rowix+i, _maxRowsPerBlock);\n \t\t\t\tMatrixIndexes ixout = new MatrixIndexes(rix, 1);\n-\t\t\t\tMatrixBlock out = new MatrixBlock(ru-i+1, IntUtils.toInt(_clen), sparse);\n-\t\t\t\tout.copy(0, out.getNumRows()-1, IntUtils.toInt(cl), IntUtils.toInt(cu), \n+\t\t\t\tMatrixBlock out = new MatrixBlock(ru-i+1, (int)_clen, sparse);\n+\t\t\t\tout.copy(0, out.getNumRows()-1, (int)cl, (int)cu, \n \t\t\t\t\tmb.slice(i, ru, 0, mb.getNumColumns()-1, mbreuse), true);\n \t\t\t\tout.examSparsity();\n \t\t\t\tret.add(new Tuple2<>(ixout,out));\n@@ -936,8 +935,8 @@ public MatrixFrameReblockFunction(MatrixCharacteristics mc) {\n \t\t */\n \t\tpublic static int computeBlockSize(MatrixCharacteristics mc) {\n \t\t\tint brlen = mc.getRowsPerBlock();\n-\t\t\tint basic = Math.max(IntUtils.toInt(FrameBlock.BUFFER_SIZE/mc.getCols()), 1);\n-\t\t\tint div = IntUtils.toInt(Math.ceil((double)brlen/basic));\n+\t\t\tint basic = Math.max((int)(FrameBlock.BUFFER_SIZE/mc.getCols()), 1);\n+\t\t\tint div = (int)Math.ceil((double)brlen/basic);\n \t\t\twhile( brlen % div != 0 ) \n \t\t\t\tdiv++;\n \t\t\treturn brlen / div;\n@@ -996,16 +995,16 @@ public BinaryBlockToMatrixBlockFunction(MatrixCharacteristics mcIn, MatrixCharac\n \t\t\tfor( long rix=rstartix; rix<=rendix; rix++ ) { //for all row blocks\n \t\t\t\tlong rpos = UtilFunctions.computeCellIndex(rix, brlen, 0);\n \t\t\t\tint lrlen = UtilFunctions.computeBlockSize(rlen, rix, brlen);\n-\t\t\t\tint fix = IntUtils.toInt((rpos-rowIndex>=0) ? rpos-rowIndex : 0);\n-\t\t\t\tint fix2 = IntUtils.toInt(Math.min(rpos+lrlen-rowIndex-1,blk.getNumRows()-1));\n+\t\t\t\tint fix = (int)((rpos-rowIndex>=0) ? rpos-rowIndex : 0);\n+\t\t\t\tint fix2 = (int)Math.min(rpos+lrlen-rowIndex-1,blk.getNumRows()-1);\n \t\t\t\tint mix = UtilFunctions.computeCellInBlock(rowIndex+fix, brlen);\n \t\t\t\tint mix2 = mix + (fix2-fix);\n \t\t\t\tfor( long cix=1; cix<=cendix; cix++ ) { //for all column blocks\n \t\t\t\t\tlong cpos = UtilFunctions.computeCellIndex(cix, bclen, 0);\n \t\t\t\t\tint lclen = UtilFunctions.computeBlockSize(clen, cix, bclen);\n \t\t\t\t\tMatrixBlock matrix = new MatrixBlock(lrlen, lclen, false);\n \t\t\t\t\tFrameBlock frame = blk.slice(fix, fix2, \n-\t\t\t\t\t\t\tIntUtils.toInt(cpos-1), IntUtils.toInt(cpos+lclen-2), new FrameBlock());\n+\t\t\t\t\t\t\t(int)cpos-1, (int)cpos+lclen-2, new FrameBlock());\n \t\t\t\t\tMatrixBlock mframe = DataConverter.convertToMatrixBlock(frame);\n \t\t\t\t\tret.add(new Tuple2<>(new MatrixIndexes(rix, cix), \n \t\t\t\t\t\t\tmatrix.leftIndexingOperations(mframe, mix, mix2, 0, lclen-1, ",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/FrameRDDConverterUtils.java",
                "sha": "c0fc34a24a696b77197bce503466b2e902ec15aa",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtils.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtils.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 26,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtils.java",
                "patch": "@@ -71,7 +71,6 @@\n import org.apache.sysml.runtime.util.FastStringTokenizer;\n import org.apache.sysml.runtime.util.MapReduceTool;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -272,7 +271,7 @@\n \t\t//slice blocks into rows, align and convert into data frame rows\n \t\tJavaRDD<Row> rowsRDD = in\n \t\t\t.flatMapToPair(new SliceBinaryBlockToRowsFunction(mc.getRowsPerBlock()))\n-\t\t\t.groupByKey().map(new ConvertRowBlocksToRows(IntUtils.toInt(mc.getCols()), mc.getColsPerBlock(), toVector));\n+\t\t\t.groupByKey().map(new ConvertRowBlocksToRows((int)mc.getCols(), mc.getColsPerBlock(), toVector));\n \t\t\n \t\t//create data frame schema\n \t\tList<StructField> fields = new ArrayList<>();\n@@ -323,7 +322,7 @@ public static void libsvmToBinaryBlock(JavaSparkContext sc, String pathIn,\n \t\t\tMapReduceTool.deleteFileIfExistOnHDFS(pathY);\n \t\t\t\n \t\t\t//convert libsvm to labeled points\n-\t\t\tint numFeatures = IntUtils.toInt( mcOutX.getCols() );\n+\t\t\tint numFeatures = (int) mcOutX.getCols();\n \t\t\tint numPartitions = SparkUtils.getNumPreferredPartitions(mcOutX, null);\n \t\t\tJavaRDD<org.apache.spark.mllib.regression.LabeledPoint> lpoints = \n \t\t\t\t\tMLUtils.loadLibSVMFile(sc.sc(), pathIn, numFeatures, numPartitions).toJavaRDD();\n@@ -486,7 +485,7 @@ protected CellToBinaryBlockFunction(MatrixCharacteristics mc)\n \t\t\t_bclen = mc.getColsPerBlock();\n \t\t\t\n \t\t\t//determine upper bounded buffer len\n-\t\t\t_bufflen = IntUtils.toInt( Math.min(_rlen*_clen, BUFFER_SIZE) );\n+\t\t\t_bufflen = (int) Math.min(_rlen*_clen, BUFFER_SIZE);\n \t\t}\n \n \t\tprotected void flushBufferToList( ReblockBuffer rbuff,  ArrayList<Tuple2<MatrixIndexes,MatrixBlock>> ret ) \n@@ -703,7 +702,7 @@ public CSVToBinaryBlockFunction(MatrixCharacteristics mc, boolean sparse, boolea\n \t\t{\n \t\t\tArrayList<Tuple2<MatrixIndexes,MatrixBlock>> ret = new ArrayList<>();\n \n-\t\t\tint ncblks = IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\t\tint ncblks = (int)Math.ceil((double)_clen/_bclen);\n \t\t\tMatrixIndexes[] ix = new MatrixIndexes[ncblks];\n \t\t\tMatrixBlock[] mb = new MatrixBlock[ncblks];\n \t\t\t\n@@ -725,15 +724,15 @@ public CSVToBinaryBlockFunction(MatrixCharacteristics mc, boolean sparse, boolea\n \t\t\t\t\tif( ix[0] !=null )\n \t\t\t\t\t\tflushBlocksToList(ix, mb, ret);\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n-\t\t\t\t\tcreateBlocks(rowix, IntUtils.toInt(len), ix, mb);\n+\t\t\t\t\tcreateBlocks(rowix, (int)len, ix, mb);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\t//process row data\n \t\t\t\tString[] parts = IOUtilFunctions.split(row, _delim);\n \t\t\t\tboolean emptyFound = false;\n \t\t\t\tfor( int cix=1, pix=0; cix<=ncblks; cix++ ) \n \t\t\t\t{\n-\t\t\t\t\tint lclen = IntUtils.toInt(UtilFunctions.computeBlockSize(_clen, cix, _bclen));\n+\t\t\t\t\tint lclen = (int)UtilFunctions.computeBlockSize(_clen, cix, _bclen);\n \t\t\t\t\tif( mb[cix-1].isInSparseFormat() ) {\n \t\t\t\t\t\t//allocate row once (avoid re-allocations)\n \t\t\t\t\t\tint lnnz = IOUtilFunctions.countNnz(parts, pix, lclen);\n@@ -763,13 +762,13 @@ private void createBlocks(long rowix, int lrlen, MatrixIndexes[] ix, MatrixBlock\n \t\t{\n \t\t\t//compute row block index and number of column blocks\n \t\t\tlong rix = UtilFunctions.computeBlockIndex(rowix, _brlen);\n-\t\t\tint ncblks = IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\t\tint ncblks = (int)Math.ceil((double)_clen/_bclen);\n \t\t\t\n \t\t\t//create all column blocks (assume dense since csv is dense text format)\n \t\t\tfor( int cix=1; cix<=ncblks; cix++ ) {\n-\t\t\t\tint lclen = IntUtils.toInt(UtilFunctions.computeBlockSize(_clen, cix, _bclen));\t\t\t\t\n+\t\t\t\tint lclen = (int)UtilFunctions.computeBlockSize(_clen, cix, _bclen);\t\t\t\t\n \t\t\t\tix[cix-1] = new MatrixIndexes(rix, cix);\n-\t\t\t\tmb[cix-1] = new MatrixBlock(lrlen, lclen, _sparse, IntUtils.toInt(lrlen*lclen*_sparsity));\n+\t\t\t\tmb[cix-1] = new MatrixBlock(lrlen, lclen, _sparse, (int)(lrlen*lclen*_sparsity));\n \t\t\t\tmb[cix-1].allocateBlock();\n \t\t\t}\n \t\t}\n@@ -814,7 +813,7 @@ public LabeledPointToBinaryBlockFunction(MatrixCharacteristics mc, boolean label\n \t\t{\n \t\t\tArrayList<Tuple2<MatrixIndexes,MatrixBlock>> ret = new ArrayList<>();\n \n-\t\t\tint ncblks = IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\t\tint ncblks = (int)Math.ceil((double)_clen/_bclen);\n \t\t\tMatrixIndexes[] ix = new MatrixIndexes[ncblks];\n \t\t\tMatrixBlock[] mb = new MatrixBlock[ncblks];\n \t\t\t\n@@ -834,7 +833,7 @@ public LabeledPointToBinaryBlockFunction(MatrixCharacteristics mc, boolean label\n \t\t\t\t\tif( ix[0] !=null )\n \t\t\t\t\t\tflushBlocksToList(ix, mb, ret);\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n-\t\t\t\t\tcreateBlocks(rowix, IntUtils.toInt(len), ix, mb, lsparse);\n+\t\t\t\t\tcreateBlocks(rowix, (int)len, ix, mb, lsparse);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\t//process row data\n@@ -851,14 +850,14 @@ public LabeledPointToBinaryBlockFunction(MatrixCharacteristics mc, boolean label\n \t\t\t\t\t\t\t\t(org.apache.spark.mllib.linalg.SparseVector) row.features();\n \t\t\t\t\t\tfor( int k=0; k<lnnz; k++ ) {\n \t\t\t\t\t\t\tint gix = srow.indices()[k]+1;\n-\t\t\t\t\t\t\tint cix = IntUtils.toInt(UtilFunctions.computeBlockIndex(gix, _bclen));\n+\t\t\t\t\t\t\tint cix = (int)UtilFunctions.computeBlockIndex(gix, _bclen);\n \t\t\t\t\t\t\tint j = UtilFunctions.computeCellInBlock(gix, _bclen);\n \t\t\t\t\t\t\tmb[cix-1].appendValue(pos, j, srow.values()[k]);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t\telse { //dense\n \t\t\t\t\t\tfor( int cix=1, pix=0; cix<=ncblks; cix++ ) {\n-\t\t\t\t\t\t\tint lclen = IntUtils.toInt(UtilFunctions.computeBlockSize(_clen, cix, _bclen));\n+\t\t\t\t\t\t\tint lclen = (int)UtilFunctions.computeBlockSize(_clen, cix, _bclen);\n \t\t\t\t\t\t\tfor( int j=0; j<lclen; j++ )\n \t\t\t\t\t\t\t\tmb[cix-1].appendValue(pos, j, row.features().apply(pix++));\n \t\t\t\t\t\t}\n@@ -878,11 +877,11 @@ private void createBlocks(long rowix, int lrlen, MatrixIndexes[] ix, MatrixBlock\n \t\t{\n \t\t\t//compute row block index and number of column blocks\n \t\t\tlong rix = UtilFunctions.computeBlockIndex(rowix, _brlen);\n-\t\t\tint ncblks = IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\t\tint ncblks = (int)Math.ceil((double)_clen/_bclen);\n \t\t\t\n \t\t\t//create all column blocks (assume dense since csv is dense text format)\n \t\t\tfor( int cix=1; cix<=ncblks; cix++ ) {\n-\t\t\t\tint lclen = IntUtils.toInt(UtilFunctions.computeBlockSize(_clen, cix, _bclen));\n+\t\t\t\tint lclen = (int)UtilFunctions.computeBlockSize(_clen, cix, _bclen);\n \t\t\t\tix[cix-1] = new MatrixIndexes(rix, cix);\n \t\t\t\tmb[cix-1] = new MatrixBlock(lrlen, lclen, lsparse);\n \t\t\t\tmb[cix-1].allocateBlock();\n@@ -986,7 +985,7 @@ public SliceBinaryBlockToRowsFunction(int brlen) {\n \t\tpublic ConcatenateBlocksFunction(long clen, int bclen) {\n \t\t\t_clen = clen;\n \t\t\t_bclen = bclen;\n-\t\t\t_ncblks = IntUtils.toInt(Math.ceil((double)clen/bclen));\n+\t\t\t_ncblks = (int)Math.ceil((double)clen/bclen);\n \t\t}\n \t\t\n \t\t@Override\n@@ -1002,9 +1001,9 @@ public ConcatenateBlocksFunction(long clen, int bclen) {\n \t\t\t\ttmpBlks[entry._1().intValue()-1] = entry._2();\n \t\t\t}\n \t\t\t//concatenate blocks\n-\t\t\tMatrixBlock out = new MatrixBlock(1,IntUtils.toInt(_clen), tmpBlks[0].isInSparseFormat());\n+\t\t\tMatrixBlock out = new MatrixBlock(1,(int)_clen, tmpBlks[0].isInSparseFormat());\n \t\t\tfor( int i=0; i<_ncblks; i++ )\n-\t\t\t\tout.copy(0, 0, i*_bclen, IntUtils.toInt(Math.min((i+1)*_bclen, _clen)-1), tmpBlks[i], false);\n+\t\t\t\tout.copy(0, 0, i*_bclen, (int)Math.min((i+1)*_bclen, _clen)-1, tmpBlks[i], false);\n \t\t\tout.recomputeNonZeros();\n \t\t\t//output row block\n \t\t\treturn new Tuple2<>(new MatrixIndexes(rowIndex, 1),out);\n@@ -1044,7 +1043,7 @@ public DataFrameToBinaryBlockFunction(MatrixCharacteristics mc, boolean sparse,\n \t\t{\n \t\t\tArrayList<Tuple2<MatrixIndexes,MatrixBlock>> ret = new ArrayList<>();\n \t\t\t\n-\t\t\tint ncblks = IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\t\tint ncblks = (int)Math.ceil((double)_clen/_bclen);\n \t\t\tMatrixIndexes[] ix = new MatrixIndexes[ncblks];\n \t\t\tMatrixBlock[] mb = new MatrixBlock[ncblks];\n \t\t\t\n@@ -1061,15 +1060,15 @@ public DataFrameToBinaryBlockFunction(MatrixCharacteristics mc, boolean sparse,\n \t\t\t\t\tif( ix[0] !=null )\n \t\t\t\t\t\tflushBlocksToList(ix, mb, ret);\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n-\t\t\t\t\tcreateBlocks(rowix, IntUtils.toInt(len), ix, mb);\n+\t\t\t\t\tcreateBlocks(rowix, (int)len, ix, mb);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\t//process row data\n \t\t\t\tint off = _containsID ? 1 : 0;\n \t\t\t\tObject obj = _isVector ? tmp._1().get(off) : tmp._1();\n \t\t\t\tfor( int cix=1, pix=_isVector?0:off; cix<=ncblks; cix++ ) {\n-\t\t\t\t\tint lclen = IntUtils.toInt(UtilFunctions.computeBlockSize(_clen, cix, _bclen));\n-\t\t\t\t\tint cu = IntUtils.toInt( Math.min(_clen, cix*_bclen) + (_isVector?0:off) );\n+\t\t\t\t\tint lclen = (int)UtilFunctions.computeBlockSize(_clen, cix, _bclen);\n+\t\t\t\t\tint cu = (int) Math.min(_clen, cix*_bclen) + (_isVector?0:off);\n \t\t\t\t\t//allocate sparse row once (avoid re-allocations)\n \t\t\t\t\tif( mb[cix-1].isInSparseFormat() ) {\n \t\t\t\t\t\tint lnnz = countNnz(obj, _isVector, pix, cu);\n@@ -1110,13 +1109,13 @@ private void createBlocks(long rowix, int lrlen, MatrixIndexes[] ix, MatrixBlock\n \t\t{\n \t\t\t//compute row block index and number of column blocks\n \t\t\tlong rix = UtilFunctions.computeBlockIndex(rowix, _brlen);\n-\t\t\tint ncblks = IntUtils.toInt(Math.ceil((double)_clen/_bclen));\n+\t\t\tint ncblks = (int)Math.ceil((double)_clen/_bclen);\n \t\t\t\n \t\t\t//create all column blocks (assume dense since csv is dense text format)\n \t\t\tfor( int cix=1; cix<=ncblks; cix++ ) {\n-\t\t\t\tint lclen = IntUtils.toInt(UtilFunctions.computeBlockSize(_clen, cix, _bclen));\n+\t\t\t\tint lclen = (int)UtilFunctions.computeBlockSize(_clen, cix, _bclen);\n \t\t\t\tix[cix-1] = new MatrixIndexes(rix, cix);\n-\t\t\t\tmb[cix-1] = new MatrixBlock(lrlen, lclen, _sparse,IntUtils.toInt(lrlen*lclen*_sparsity));\n+\t\t\t\tmb[cix-1] = new MatrixBlock(lrlen, lclen, _sparse,(int)(lrlen*lclen*_sparsity));\n \t\t\t\tmb[cix-1].allocateBlock();\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtils.java",
                "sha": "f775e92eb193f232fe2583b805973d13042ff140",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtilsExt.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtilsExt.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 8,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtilsExt.java",
                "patch": "@@ -54,7 +54,6 @@\n import org.apache.sysml.runtime.matrix.mapred.IndexedMatrixValue;\n import org.apache.sysml.runtime.matrix.mapred.ReblockBuffer;\n import org.apache.sysml.runtime.util.FastStringTokenizer;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -128,15 +127,15 @@\n \t}\n \n \tpublic static MatrixBlock convertPy4JArrayToMB(byte [] data, long rlen, long clen) {\n-\t\treturn convertPy4JArrayToMB(data, IntUtils.toInt(rlen), IntUtils.toInt(clen), false);\n+\t\treturn convertPy4JArrayToMB(data, (int)rlen, (int)clen, false);\n \t}\n \n \tpublic static MatrixBlock convertPy4JArrayToMB(byte [] data, int rlen, int clen) {\n \t\treturn convertPy4JArrayToMB(data, rlen, clen, false);\n \t}\n \n \tpublic static MatrixBlock convertSciPyCOOToMB(byte [] data, byte [] row, byte [] col, long rlen, long clen, long nnz) {\n-\t\treturn convertSciPyCOOToMB(data, row, col, IntUtils.toInt(rlen), IntUtils.toInt(clen), IntUtils.toInt(nnz));\n+\t\treturn convertSciPyCOOToMB(data, row, col, (int)rlen, (int)clen, (int)nnz);\n \t}\n \n \tpublic static MatrixBlock convertSciPyCOOToMB(byte [] data, byte [] row, byte [] col, int rlen, int clen, int nnz) {\n@@ -160,7 +159,7 @@ public static MatrixBlock convertSciPyCOOToMB(byte [] data, byte [] row, byte []\n \t}\n \n \tpublic static MatrixBlock convertPy4JArrayToMB(byte [] data, long rlen, long clen, boolean isSparse) {\n-\t\treturn convertPy4JArrayToMB(data, IntUtils.toInt( rlen), IntUtils.toInt(clen), isSparse);\n+\t\treturn convertPy4JArrayToMB(data, (int) rlen, (int) clen, isSparse);\n \t}\n \n \tpublic static MatrixBlock allocateDenseOrSparse(int rlen, int clen, boolean isSparse) {\n@@ -187,7 +186,7 @@ public static void copyRowBlocks(MatrixBlock mb, int rowIndex, MatrixBlock ret,\n \tpublic static void copyRowBlocks(MatrixBlock mb, long rowIndex, MatrixBlock ret, long numRowsPerBlock, long rlen, long clen) {\n \t\t// TODO: Double-check if synchronization is required here.\n \t\t// synchronized (RDDConverterUtilsExt.class) {\n-\t\t\tret.copy(IntUtils.toInt(rowIndex*numRowsPerBlock), IntUtils.toInt(Math.min((rowIndex+1)*numRowsPerBlock-1, rlen-1)), 0, IntUtils.toInt(clen-1), mb, false);\n+\t\t\tret.copy((int)(rowIndex*numRowsPerBlock), (int)Math.min((rowIndex+1)*numRowsPerBlock-1, rlen-1), 0, (int)(clen-1), mb, false);\n \t\t// }\n \t}\n \n@@ -205,7 +204,7 @@ public static MatrixBlock convertPy4JArrayToMB(byte [] data, int rlen, int clen,\n \t\t\tlong limit = rlen*clen;\n \t\t\tif( limit > Integer.MAX_VALUE )\n \t\t\t\tthrow new DMLRuntimeException(\"Dense NumPy array of size \" + limit + \" cannot be converted to MatrixBlock\");\n-\t\t\tdouble [] denseBlock = new double[IntUtils.toInt(limit)];\n+\t\t\tdouble [] denseBlock = new double[(int) limit];\n \t\t\tByteBuffer buf = ByteBuffer.wrap(data);\n \t\t\tbuf.order(ByteOrder.nativeOrder());\n \t\t\tfor(int i = 0; i < rlen*clen; i++) {\n@@ -228,7 +227,7 @@ public static MatrixBlock convertPy4JArrayToMB(byte [] data, int rlen, int clen,\n \t\tint times = Double.SIZE / Byte.SIZE;\n \t\tif( limit > Integer.MAX_VALUE / times )\n \t\t\tthrow new DMLRuntimeException(\"MatrixBlock of size \" + limit + \" cannot be converted to dense numpy array\");\n-\t\tret = new byte[IntUtils.toInt(limit * times)];\n+\t\tret = new byte[(int) (limit * times)];\n \n \t\tdouble [] denseBlock = mb.getDenseBlockValues();\n \t\tif(mb.isEmptyBlock()) {\n@@ -319,7 +318,7 @@ public IJVToBinaryBlockFunctionHelper(MatrixCharacteristics mc) {\n \t\t\t_brlen = mc.getRowsPerBlock();\n \t\t\t_bclen = mc.getColsPerBlock();\n \t\t\t//determine upper bounded buffer len\n-\t\t\t_bufflen = IntUtils.toInt( Math.min(_rlen*_clen, BUFFER_SIZE));\n+\t\t\t_bufflen = (int) Math.min(_rlen*_clen, BUFFER_SIZE);\n \t\t}\n \n \t\t// ----------------------------------------------------",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDConverterUtilsExt.java",
                "sha": "4871aee0cba6971a9715902040b0101af9e42817",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDSortUtils.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDSortUtils.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 14,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDSortUtils.java",
                "patch": "@@ -48,7 +48,6 @@\n import org.apache.sysml.runtime.util.DataConverter;\n import org.apache.sysml.runtime.util.SortUtils;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n public class RDDSortUtils \n {\n@@ -61,7 +60,7 @@\n \t\n \t\t//sort (creates sorted range per partition)\n \t\tlong hdfsBlocksize = InfrastructureAnalyzer.getHDFSBlockSize();\n-\t\tint numPartitions = IntUtils.toInt(Math.ceil(((double)rlen*8)/hdfsBlocksize));\n+\t\tint numPartitions = (int)Math.ceil(((double)rlen*8)/hdfsBlocksize);\n \t\tJavaRDD<Double> sdvals = dvals\n \t\t\t\t.sortBy(new CreateDoubleKeyFunction(), true, numPartitions);\n \t\t\n@@ -83,7 +82,7 @@\n \t\n \t\t//sort (creates sorted range per partition)\n \t\tlong hdfsBlocksize = InfrastructureAnalyzer.getHDFSBlockSize();\n-\t\tint numPartitions = IntUtils.toInt(Math.ceil(((double)rlen*8)/hdfsBlocksize));\n+\t\tint numPartitions = (int)Math.ceil(((double)rlen*8)/hdfsBlocksize);\n \t\tJavaRDD<DoublePair> sdvals = dvals\n \t\t\t.sortBy(new CreateDoubleKeyFunction2(), true, numPartitions);\n \n@@ -127,7 +126,7 @@\n \t\t\n \t\t//sort (creates sorted range per partition)\n \t\tlong hdfsBlocksize = InfrastructureAnalyzer.getHDFSBlockSize();\n-\t\tint numPartitions = IntUtils.toInt(Math.ceil(((double)rlen*16)/hdfsBlocksize));\n+\t\tint numPartitions = (int)Math.ceil(((double)rlen*16)/hdfsBlocksize);\n \t\tJavaRDD<ValueIndexPair> sdvals = dvals\n \t\t\t.sortByKey(new IndexComparator(asc), true, numPartitions)\n \t\t\t.keys(); //workaround for index comparator\n@@ -173,7 +172,7 @@\n \t\t\n \t\t//sort (creates sorted range per partition)\n \t\tlong hdfsBlocksize = InfrastructureAnalyzer.getHDFSBlockSize();\n-\t\tint numPartitions = IntUtils.toInt(Math.ceil(((double)rlen*16)/hdfsBlocksize));\n+\t\tint numPartitions = (int)Math.ceil(((double)rlen*16)/hdfsBlocksize);\n \t\tJavaRDD<ValueIndexPair> sdvals = dvals\n \t\t\t.sortByKey(new IndexComparator(asc), true, numPartitions)\n \t\t\t.keys(); //workaround for index comparator\n@@ -250,7 +249,7 @@\n \t{\n \t\t//collect orderby column for in-memory sorting\n \t\tMatrixBlock inMatBlock = SparkExecutionContext\n-\t\t\t\t.toMatrixBlock(val, IntUtils.toInt(rlen), 1, brlen, bclen, -1);\n+\t\t\t\t.toMatrixBlock(val, (int)rlen, 1, brlen, bclen, -1);\n \n \t\t//in-memory sort operation (w/ index return: source index in target position)\n \t\tReorgOperator lrop = new ReorgOperator(new SortIndex(1, !asc, true));\n@@ -260,7 +259,7 @@\n \t\t//flip sort indices from <source ix in target pos> to <target ix in source pos>\n \t\tMatrixBlock sortedIxSrc = new MatrixBlock(sortedIx.getNumRows(), 1, false); \n \t\tfor (int i=0; i < sortedIx.getNumRows(); i++) \n-\t\t\tsortedIxSrc.quickSetValue(IntUtils.toInt(sortedIx.quickGetValue(i,0)-1), 0, i+1);\n+\t\t\tsortedIxSrc.quickSetValue((int)sortedIx.quickGetValue(i,0)-1, 0, i+1);\n \n \t\t//broadcast index vector\n \t\tPartitionedBlock<MatrixBlock> pmb = new PartitionedBlock<>(sortedIxSrc, brlen, bclen);\n@@ -462,7 +461,7 @@ public ConvertToBinaryBlockFunction(long rlen, int brlen)\n \t\t\t\t\t\tret.add(new Tuple2<>(ix,mb));\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n \t\t\t\t\tix = new MatrixIndexes(rix,1);\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(len), 1, false);\t\n+\t\t\t\t\tmb = new MatrixBlock((int)len, 1, false);\t\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tmb.quickSetValue(pos, 0, val._1);\n@@ -509,7 +508,7 @@ public ConvertToBinaryBlockFunction2(long rlen, int brlen)\n \t\t\t\t\t\tret.add(new Tuple2<>(ix,mb));\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n \t\t\t\t\tix = new MatrixIndexes(rix,1);\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(len), 2, false);\n+\t\t\t\t\tmb = new MatrixBlock((int)len, 2, false);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tmb.quickSetValue(pos, 0, val._1.val1);\n@@ -558,7 +557,7 @@ public ConvertToBinaryBlockFunction3(long rlen, int brlen)\n \t\t\t\t\t\tret.add(new Tuple2<>(ix,mb));\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n \t\t\t\t\tix = new MatrixIndexes(rix,1);\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(len), 1, false);\t\n+\t\t\t\t\tmb = new MatrixBlock((int)len, 1, false);\t\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tmb.quickSetValue(pos, 0, val._1.ix);\n@@ -606,7 +605,7 @@ public ConvertToBinaryBlockFunction4(long rlen, int brlen)\n \t\t\t\t\t\tret.add(new Tuple2<>(ix,mb));\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n \t\t\t\t\tix = new MatrixIndexes(rix,1);\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(len), 1, false);\t\n+\t\t\t\t\tmb = new MatrixBlock((int)len, 1, false);\t\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tmb.quickSetValue(pos, 0, val._2+1);\n@@ -652,7 +651,7 @@ public ConvertToBinaryBlockFunction5(long rlen, int brlen)\n \t\t\t\t\t\tret.add(new Tuple2<>(ix,mb));\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n \t\t\t\t\tix = new MatrixIndexes(rix,1);\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(len), val._1.getNumColumns(), false);\n+\t\t\t\t\tmb = new MatrixBlock((int)len, val._1.getNumColumns(), false);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tmb.leftIndexingOperations(val._1, pos, pos, 0, val._1.getNumColumns()-1, mb, UpdateType.INPLACE);\n@@ -698,7 +697,7 @@ public ConvertToBinaryBlockFunction6(long rlen, int brlen)\n \t\t\t\t\t\tret.add(new Tuple2<>(ix,mb));\n \t\t\t\t\tlong len = UtilFunctions.computeBlockSize(_rlen, rix, _brlen);\n \t\t\t\t\tix = new MatrixIndexes(rix,1);\n-\t\t\t\t\tmb = new MatrixBlock(IntUtils.toInt(len), 1, false);\n+\t\t\t\t\tmb = new MatrixBlock((int)len, 1, false);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\tmb.quickSetValue(pos, 0, val._1.ix);\n@@ -858,7 +857,7 @@ public boolean hasNext() {\n \t\t\t\t\t//produce next output tuple\n \t\t\t\t\tMatrixIndexes ixmap = _currBlk._1();\n \t\t\t\t\tMatrixBlock data = _currBlk._2();\n-\t\t\t\t\tMatrixBlock mbTargetIndex = _pmb.value().getBlock(IntUtils.toInt(ixmap.getRowIndex()), 1);\n+\t\t\t\t\tMatrixBlock mbTargetIndex = _pmb.value().getBlock((int)ixmap.getRowIndex(), 1);\n \t\t\t\t\t\n \t\t\t\t\tlong valix = (long) mbTargetIndex.getValue(_currPos, 0);\n \t\t\t\t\tlong rix = UtilFunctions.computeBlockIndex(valix, _brlen);",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/RDDSortUtils.java",
                "sha": "01bdef855886ea47c0eb1c7a4e751362bfd98634",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/SparkUtils.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/SparkUtils.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 4,
                "filename": "src/main/java/org/apache/sysml/runtime/instructions/spark/utils/SparkUtils.java",
                "patch": "@@ -49,7 +49,6 @@\n import org.apache.sysml.runtime.matrix.data.Pair;\n import org.apache.sysml.runtime.matrix.mapred.IndexedMatrixValue;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n \n import scala.Tuple2;\n \n@@ -121,7 +120,7 @@ public static int getNumPreferredPartitions(MatrixCharacteristics mc) {\n \t\t\treturn SparkExecutionContext.getDefaultParallelism(true);\n \t\tdouble hdfsBlockSize = InfrastructureAnalyzer.getHDFSBlockSize();\n \t\tdouble matrixPSize = OptimizerUtils.estimatePartitionedSizeExactSparsity(mc);\n-\t\treturn IntUtils.toInt( Math.max(Math.ceil(matrixPSize/hdfsBlockSize), 1));\n+\t\treturn (int) Math.max(Math.ceil(matrixPSize/hdfsBlockSize), 1);\n \t}\n \t\n \t/**\n@@ -188,8 +187,8 @@ public static String getPrefixFromSparkDebugInfo(String line) {\n \t\t//compute degree of parallelism and block ranges\n \t\tlong size = mc.getNumBlocks() * OptimizerUtils.estimateSizeEmptyBlock(Math.min(\n \t\t\t\tMath.max(mc.getRows(),1), mc.getRowsPerBlock()), Math.min(Math.max(mc.getCols(),1), mc.getColsPerBlock()));\n-\t\tint par = IntUtils.toInt(Math.min(4*Math.max(SparkExecutionContext.getDefaultParallelism(true),\n-\t\t\t\tMath.ceil(size/InfrastructureAnalyzer.getHDFSBlockSize())), mc.getNumBlocks()));\n+\t\tint par = (int) Math.min(4*Math.max(SparkExecutionContext.getDefaultParallelism(true),\n+\t\t\t\tMath.ceil(size/InfrastructureAnalyzer.getHDFSBlockSize())), mc.getNumBlocks());\n \t\tlong pNumBlocks = (long)Math.ceil((double)mc.getNumBlocks()/par);\n \t\t\n \t\t//generate block offsets per partition",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/instructions/spark/utils/SparkUtils.java",
                "sha": "952135e36b312b22e9b0c1ec0b57a68780e2f817",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/matrix/data/LibMatrixCUDA.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/matrix/data/LibMatrixCUDA.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 2,
                "filename": "src/main/java/org/apache/sysml/runtime/matrix/data/LibMatrixCUDA.java",
                "patch": "@@ -79,7 +79,6 @@\n import org.apache.sysml.runtime.matrix.operators.ScalarOperator;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.utils.GPUStatistics;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.Statistics;\n \n import jcuda.Pointer;\n@@ -1878,7 +1877,10 @@ public static void transpose(ExecutionContext ec, GPUContext gCtx, String instNa\n \t//********************************************************************/\n \n \tpublic static int toInt(long num) {\n-\t\treturn IntUtils.toInt(num);\n+\t\tif(num >= Integer.MAX_VALUE || num <= Integer.MIN_VALUE) {\n+\t\t\tthrow new DMLRuntimeException(\"GPU : Exceeded supported size \" + num);\n+\t\t}\n+\t\treturn (int)num;\n \t}\n \n \t//********************************************************************/",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/matrix/data/LibMatrixCUDA.java",
                "sha": "fd06578eb4260fbbedcde9dcfc174b8c564c25fa",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/systemml/blob/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/matrix/data/MatrixBlock.java",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/runtime/matrix/data/MatrixBlock.java?ref=c3fdbb4da7c6cac4d363b31366ae21ef976cde92",
                "deletions": 47,
                "filename": "src/main/java/org/apache/sysml/runtime/matrix/data/MatrixBlock.java",
                "patch": "@@ -97,7 +97,6 @@\n import org.apache.sysml.runtime.util.FastBufferedDataOutputStream;\n import org.apache.sysml.runtime.util.IndexRange;\n import org.apache.sysml.runtime.util.UtilFunctions;\n-import org.apache.sysml.utils.IntUtils;\n import org.apache.sysml.utils.NativeHelper;\n \n \n@@ -256,7 +255,7 @@ private void reset(int rl, int cl, boolean sp, long estnnz, double val) {\n \t\tsparse = (val == 0) ? sp : false;\n \t\tnonZeros = (val == 0) ? 0 : (long)rl*cl;\n \t\testimatedNNzsPerRow = (estnnz < 0 || !sparse) ? -1 :\n-\t\t\tIntUtils.toInt(Math.ceil((double)estnnz/(double)rlen));\n+\t\t\t(int)Math.ceil((double)estnnz/(double)rlen);\n \t\t\n \t\t//reset sparse/dense blocks\n \t\tif( sparse )\n@@ -1091,7 +1090,7 @@ private void denseToSparse(boolean allowCSR)\n \t\t\n \t\tif( allowCSR && nonZeros <= Integer.MAX_VALUE ) {\n \t\t\t//allocate target in memory-efficient CSR format\n-\t\t\tint lnnz = IntUtils.toInt(nonZeros);\n+\t\t\tint lnnz = (int) nonZeros;\n \t\t\tint[] rptr = new int[m+1];\n \t\t\tint[] indexes = new int[lnnz];\n \t\t\tdouble[] values = new double[lnnz];\n@@ -1292,7 +1291,7 @@ public void copy(MatrixValue thatValue, boolean sp)\n \t\tthis.rlen=that.rlen;\n \t\tthis.clen=that.clen;\n \t\tthis.sparse=sp;\n-\t\testimatedNNzsPerRow=IntUtils.toInt(Math.ceil((double)thatValue.getNonZeros()/(double)rlen));\n+\t\testimatedNNzsPerRow=(int)Math.ceil((double)thatValue.getNonZeros()/(double)rlen);\n \t\tif(this.sparse && that.sparse)\n \t\t\tcopySparseToSparse(that);\n \t\telse if(this.sparse && !that.sparse)\n@@ -1937,7 +1936,7 @@ private void readUltraSparseBlock(DataInput in)\n \t\t\t//block: read ijv-triples (ordered by row and column) via custom \n \t\t\t//init to avoid repeated updates of row pointers per append\n \t\t\tSparseBlockCSR sblockCSR = (SparseBlockCSR) sparseBlock;\n-\t\t\tsblockCSR.initUltraSparse(IntUtils.toInt(nonZeros), in);\n+\t\t\tsblockCSR.initUltraSparse((int)nonZeros, in);\n \t\t}\n \t\telse { //ULTRA-SPARSE COL\n \t\t\t//col: read iv-pairs (should never happen since always dense)\n@@ -2265,7 +2264,7 @@ private void writeNnzInfo( DataOutput out, boolean ultrasparse )\n \t\t\tout.writeLong( nonZeros ); \n \t\t}\n \t\telse {\n-\t\t\tout.writeInt( IntUtils.toInt(nonZeros) );\n+\t\t\tout.writeInt( (int)nonZeros );\n \t\t}\n \t}\n \t\n@@ -3476,7 +3475,7 @@ else if(nnz != 0) //SPARSE\n \t\t\t\tSparseBlock sblock = result.getSparseBlock();\n \t\t\t\tfor( int i=0; i<result.rlen; i++ ) {\n \t\t\t\t\tfinal int row = i; //workaround for lambda compile issue\n-\t\t\t\t\tint lnnz = IntUtils.toInt(this.recomputeNonZeros(i, i, 0, this.clen-1) + Arrays.stream(that)\n+\t\t\t\t\tint lnnz = (int) (this.recomputeNonZeros(i, i, 0, this.clen-1) + Arrays.stream(that)\n \t\t\t\t\t\t.mapToLong(mb -> mb.recomputeNonZeros(row, row, 0, mb.clen-1)).sum());\n \t\t\t\t\tsblock.allocate(i, lnnz);\n \t\t\t\t}\n@@ -3691,8 +3690,8 @@ public void permutationMatrixMultOperations( MatrixValue m2Val, MatrixValue out1\n \t\n \tpublic final MatrixBlock leftIndexingOperations(MatrixBlock rhsMatrix,\n \t\t\tIndexRange ixrange, MatrixBlock ret, UpdateType update) {\n-\t\treturn leftIndexingOperations(rhsMatrix, IntUtils.toInt(ixrange.rowStart),\n-\t\t\t\tIntUtils.toInt(ixrange.rowEnd), IntUtils.toInt(ixrange.colStart), IntUtils.toInt(ixrange.colEnd), ret, update);\n+\t\treturn leftIndexingOperations(rhsMatrix, (int)ixrange.rowStart,\n+\t\t\t\t(int)ixrange.rowEnd, (int)ixrange.colStart, (int)ixrange.colEnd, ret, update);\n \t}\n \n \tpublic MatrixBlock leftIndexingOperations(MatrixBlock rhsMatrix,\n@@ -3818,8 +3817,8 @@ public MatrixBlock leftIndexingOperations(ScalarObject scalar, int rl, int cl, M\n \n \tpublic MatrixBlock slice(IndexRange ixrange, MatrixBlock ret) {\n \t\treturn slice(\n-\t\t\t\tIntUtils.toInt(ixrange.rowStart), IntUtils.toInt(ixrange.rowEnd), \n-\t\t\t\tIntUtils.toInt(ixrange.colStart), IntUtils.toInt(ixrange.colEnd), true, ret);\n+\t\t\t\t(int)ixrange.rowStart, (int)ixrange.rowEnd, \n+\t\t\t\t(int)ixrange.colStart, (int)ixrange.colEnd, true, ret);\n \t}\n \t\n \tpublic MatrixBlock slice(int rl, int ru) {\n@@ -3970,44 +3969,44 @@ public void slice(ArrayList<IndexedMatrixValue> outlist, IndexRange range, int r\n \t\tif(colCut>range.colEnd)\n \t\t\tblockColFactor=boundaryClen;\n \t\t\n-\t\tint minrowcut=IntUtils.toInt(Math.min(rowCut,range.rowEnd));\n-\t\tint mincolcut=IntUtils.toInt(Math.min(colCut, range.colEnd));\n-\t\tint maxrowcut=IntUtils.toInt(Math.max(rowCut, range.rowStart));\n-\t\tint maxcolcut=IntUtils.toInt(Math.max(colCut, range.colStart));\n+\t\tint minrowcut=(int)Math.min(rowCut,range.rowEnd);\n+\t\tint mincolcut=(int)Math.min(colCut, range.colEnd);\n+\t\tint maxrowcut=(int)Math.max(rowCut, range.rowStart);\n+\t\tint maxcolcut=(int)Math.max(colCut, range.colStart);\n \t\t\n \t\tif(range.rowStart<rowCut && range.colStart<colCut)\n \t\t{\n \t\t\ttopleft=(MatrixBlock) p.next().getValue();\n \t\t\t//topleft.reset(blockRowFactor, blockColFactor, \n-\t\t\t//\t\tcheckSparcityOnSlide(rowCut-IntUtils.toInt(range.rowStart), colCut-IntUtils.toInt(range.colStart), blockRowFactor, blockColFactor));\n+\t\t\t//\t\tcheckSparcityOnSlide(rowCut-(int)range.rowStart, colCut-(int)range.colStart, blockRowFactor, blockColFactor));\n \t\t\t\n \t\t\ttopleft.reset(blockRowFactor, blockColFactor, \n-\t\t\t\t\testimateSparsityOnSlice(minrowcut-IntUtils.toInt(range.rowStart), mincolcut-IntUtils.toInt(range.colStart), blockRowFactor, blockColFactor));\n+\t\t\t\t\testimateSparsityOnSlice(minrowcut-(int)range.rowStart, mincolcut-(int)range.colStart, blockRowFactor, blockColFactor));\n \t\t}\n \t\tif(range.rowStart<rowCut && range.colEnd>=colCut)\n \t\t{\n \t\t\ttopright=(MatrixBlock) p.next().getValue();\n \t\t\ttopright.reset(blockRowFactor, boundaryClen, \n-\t\t\t\t\testimateSparsityOnSlice(minrowcut-IntUtils.toInt(range.rowStart), IntUtils.toInt(range.colEnd)-maxcolcut+1, blockRowFactor, boundaryClen));\n+\t\t\t\t\testimateSparsityOnSlice(minrowcut-(int)range.rowStart, (int)range.colEnd-maxcolcut+1, blockRowFactor, boundaryClen));\n \t\t}\n \t\tif(range.rowEnd>=rowCut && range.colStart<colCut)\n \t\t{\n \t\t\tbottomleft=(MatrixBlock) p.next().getValue();\n \t\t\tbottomleft.reset(boundaryRlen, blockColFactor, \n-\t\t\t\t\testimateSparsityOnSlice(IntUtils.toInt(range.rowEnd)-maxrowcut+1, mincolcut-IntUtils.toInt(range.colStart), boundaryRlen, blockColFactor));\n+\t\t\t\t\testimateSparsityOnSlice((int)range.rowEnd-maxrowcut+1, mincolcut-(int)range.colStart, boundaryRlen, blockColFactor));\n \t\t}\n \t\tif(range.rowEnd>=rowCut && range.colEnd>=colCut)\n \t\t{\n \t\t\tbottomright=(MatrixBlock) p.next().getValue();\n \t\t\tbottomright.reset(boundaryRlen, boundaryClen, \n-\t\t\t\t\testimateSparsityOnSlice(IntUtils.toInt(range.rowEnd)-maxrowcut+1, IntUtils.toInt(range.colEnd)-maxcolcut+1, boundaryRlen, boundaryClen));\n+\t\t\t\t\testimateSparsityOnSlice((int)range.rowEnd-maxrowcut+1, (int)range.colEnd-maxcolcut+1, boundaryRlen, boundaryClen));\n \t\t}\n \t\t\n \t\tif(sparse)\n \t\t{\n \t\t\tif(sparseBlock!=null)\n \t\t\t{\n-\t\t\t\tint r=IntUtils.toInt(range.rowStart);\n+\t\t\t\tint r=(int)range.rowStart;\n \t\t\t\tfor(; r<Math.min(Math.min(rowCut, sparseBlock.numRows()), range.rowEnd+1); r++)\n \t\t\t\t\tsliceHelp(r, range, colCut, topleft, topright, normalBlockRowFactor-rowCut, normalBlockRowFactor, normalBlockColFactor);\n \t\t\t\t\n@@ -4019,11 +4018,11 @@ public void slice(ArrayList<IndexedMatrixValue> outlist, IndexRange range, int r\n \t\t\tif(denseBlock!=null)\n \t\t\t{\n \t\t\t\tdouble[] a = getDenseBlockValues();\n-\t\t\t\tint i=(IntUtils.toInt(range.rowStart))*clen;\n-\t\t\t\tint r=IntUtils.toInt(range.rowStart);\n+\t\t\t\tint i=((int)range.rowStart)*clen;\n+\t\t\t\tint r=(int) range.rowStart;\n \t\t\t\tfor(; r<Math.min(rowCut, range.rowEnd+1); r++)\n \t\t\t\t{\n-\t\t\t\t\tint c=IntUtils.toInt(range.colStart);\n+\t\t\t\t\tint c=(int) range.colStart;\n \t\t\t\t\tfor(; c<Math.min(colCut, range.colEnd+1); c++)\n \t\t\t\t\t\ttopleft.appendValue(r+normalBlockRowFactor-rowCut, c+normalBlockColFactor-colCut, a[i+c]);\n \t\t\t\t\tfor(; c<=range.colEnd; c++)\n@@ -4033,7 +4032,7 @@ public void slice(ArrayList<IndexedMatrixValue> outlist, IndexRange range, int r\n \t\t\t\t\n \t\t\t\tfor(; r<=range.rowEnd; r++)\n \t\t\t\t{\n-\t\t\t\t\tint c=IntUtils.toInt(range.colStart);\n+\t\t\t\t\tint c=(int) range.colStart;\n \t\t\t\t\tfor(; c<Math.min(colCut, range.colEnd+1); c++)\n \t\t\t\t\t\tbottomleft.appendValue(r-rowCut, c+normalBlockColFactor-colCut, a[i+c]);\n \t\t\t\t\tfor(; c<=range.colEnd; c++)\n@@ -4051,10 +4050,10 @@ private void sliceHelp(int r, IndexRange range, int colCut, MatrixBlock left, Ma\n \t\t\n \t\tint[] cols=sparseBlock.indexes(r);\n \t\tdouble[] values=sparseBlock.values(r);\n-\t\tint start=sparseBlock.posFIndexGTE(r, IntUtils.toInt(range.colStart));\n+\t\tint start=sparseBlock.posFIndexGTE(r, (int)range.colStart);\n \t\tif(start<0) \n \t\t\treturn;\n-\t\tint end=sparseBlock.posFIndexLTE(r, IntUtils.toInt(range.colEnd));\n+\t\tint end=sparseBlock.posFIndexLTE(r, (int)range.colEnd);\n \t\tif(end<0 || start>end) \n \t\t\treturn;\n \t\t\n@@ -4124,9 +4123,9 @@ public MatrixValue zeroOutOperations(MatrixValue result, IndexRange range, boole\n \t\tboolean lsparse = evalSparseFormatInMemory(rlen, clen, (long)(estimatedSps*rlen*clen));\n \t\t\n \t\tif(result==null)\n-\t\t\tresult=new MatrixBlock(rlen, clen, lsparse, IntUtils.toInt(estimatedSps*rlen*clen));\n+\t\t\tresult=new MatrixBlock(rlen, clen, lsparse, (int)(estimatedSps*rlen*clen));\n \t\telse\n-\t\t\tresult.reset(rlen, clen, lsparse, IntUtils.toInt(estimatedSps*rlen*clen));\n+\t\t\tresult.reset(rlen, clen, lsparse, (int)(estimatedSps*rlen*clen));\n \t\t\n \t\t\n \t\tif(sparse)\n@@ -4135,12 +4134,12 @@ public MatrixValue zeroOutOperations(MatrixValue result, IndexRange range, boole\n \t\t\t{\n \t\t\t\tif(!complementary)//if zero out\n \t\t\t\t{\n-\t\t\t\t\tfor(int r=0; r<Math.min(IntUtils.toInt(range.rowStart), sparseBlock.numRows()); r++)\n+\t\t\t\t\tfor(int r=0; r<Math.min((int)range.rowStart, sparseBlock.numRows()); r++)\n \t\t\t\t\t\t((MatrixBlock) result).appendRow(r, sparseBlock.get(r));\n-\t\t\t\t\tfor(int r=Math.min(IntUtils.toInt(range.rowEnd+1), sparseBlock.numRows()); r<Math.min(rlen, sparseBlock.numRows()); r++)\n+\t\t\t\t\tfor(int r=Math.min((int)range.rowEnd+1, sparseBlock.numRows()); r<Math.min(rlen, sparseBlock.numRows()); r++)\n \t\t\t\t\t\t((MatrixBlock) result).appendRow(r, sparseBlock.get(r));\n \t\t\t\t}\n-\t\t\t\tfor(int r=IntUtils.toInt(range.rowStart); r<=Math.min(range.rowEnd, sparseBlock.numRows()-1); r++)\n+\t\t\t\tfor(int r=(int)range.rowStart; r<=Math.min(range.rowEnd, sparseBlock.numRows()-1); r++)\n \t\t\t\t{\n \t\t\t\t\tif(sparseBlock.isEmpty(r)) \n \t\t\t\t\t\tcontinue;\n@@ -4149,9 +4148,9 @@ public MatrixValue zeroOutOperations(MatrixValue result, IndexRange range, boole\n \t\t\t\t\t\n \t\t\t\t\tif(complementary)//if selection\n \t\t\t\t\t{\n-\t\t\t\t\t\tint start=sparseBlock.posFIndexGTE(r,IntUtils.toInt(range.colStart));\n+\t\t\t\t\t\tint start=sparseBlock.posFIndexGTE(r,(int)range.colStart);\n \t\t\t\t\t\tif(start<0) continue;\n-\t\t\t\t\t\tint end=sparseBlock.posFIndexGT(r,IntUtils.toInt(range.colEnd));\n+\t\t\t\t\t\tint end=sparseBlock.posFIndexGT(r,(int)range.colEnd);\n \t\t\t\t\t\tif(end<0 || start>end) \n \t\t\t\t\t\t\tcontinue;\n \t\t\t\t\t\t\n@@ -4163,9 +4162,9 @@ public MatrixValue zeroOutOperations(MatrixValue result, IndexRange range, boole\n \t\t\t\t\t{\n \t\t\t\t\t\tint pos = sparseBlock.pos(r);\n \t\t\t\t\t\tint len = sparseBlock.size(r);\n-\t\t\t\t\t\tint start=sparseBlock.posFIndexGTE(r,IntUtils.toInt(range.colStart));\n+\t\t\t\t\t\tint start=sparseBlock.posFIndexGTE(r,(int)range.colStart);\n \t\t\t\t\t\tif(start<0) start=pos+len;\n-\t\t\t\t\t\tint end=sparseBlock.posFIndexGT(r,IntUtils.toInt(range.colEnd));\n+\t\t\t\t\t\tint end=sparseBlock.posFIndexGT(r,(int)range.colEnd);\n \t\t\t\t\t\tif(end<0) end=pos+len;\n \t\t\t\t\t\t\n \t\t\t\t\t\tfor(int i=pos; i<start; i++)\n@@ -4186,26 +4185,26 @@ public MatrixValue zeroOutOperations(MatrixValue result, IndexRange range, boole\n \t\t\t\tdouble[] a = getDenseBlockValues();\n \t\t\t\tif(complementary)//if selection\n \t\t\t\t{\n-\t\t\t\t\tint offset=(IntUtils.toInt(range.rowStart))*clen;\n-\t\t\t\t\tfor(int r=IntUtils.toInt(range.rowStart); r<=range.rowEnd; r++)\n+\t\t\t\t\tint offset=((int)range.rowStart)*clen;\n+\t\t\t\t\tfor(int r=(int) range.rowStart; r<=range.rowEnd; r++)\n \t\t\t\t\t{\n-\t\t\t\t\t\tfor(int c=IntUtils.toInt(range.colStart); c<=range.colEnd; c++)\n+\t\t\t\t\t\tfor(int c=(int) range.colStart; c<=range.colEnd; c++)\n \t\t\t\t\t\t\t((MatrixBlock) result).appendValue(r, c, a[offset+c]);\n \t\t\t\t\t\toffset+=clen;\n \t\t\t\t\t}\n \t\t\t\t}else\n \t\t\t\t{\n \t\t\t\t\tint offset=0;\n \t\t\t\t\tint r=0;\n-\t\t\t\t\tfor(; r<IntUtils.toInt(range.rowStart); r++)\n+\t\t\t\t\tfor(; r<(int)range.rowStart; r++)\n \t\t\t\t\t\tfor(int c=0; c<clen; c++, offset++)\n \t\t\t\t\t\t\t((MatrixBlock) result).appendValue(r, c, a[offset]);\n \t\t\t\t\t\n-\t\t\t\t\tfor(; r<=IntUtils.toInt(range.rowEnd); r++)\n+\t\t\t\t\tfor(; r<=(int)range.rowEnd; r++)\n \t\t\t\t\t{\n-\t\t\t\t\t\tfor(int c=0; c<IntUtils.toInt(range.colStart); c++)\n+\t\t\t\t\t\tfor(int c=0; c<(int)range.colStart; c++)\n \t\t\t\t\t\t\t((MatrixBlock) result).appendValue(r, c, a[offset+c]);\n-\t\t\t\t\t\tfor(int c=IntUtils.toInt(range.colEnd)+1; c<clen; c++)\n+\t\t\t\t\t\tfor(int c=(int)range.colEnd+1; c<clen; c++)\n \t\t\t\t\t\t\t((MatrixBlock) result).appendValue(r, c, a[offset+c]);\n \t\t\t\t\t\toffset+=clen;\n \t\t\t\t\t}\n@@ -4628,7 +4627,7 @@ public MatrixValue sortOperations(MatrixValue weights, MatrixValue result) {\n \t\t\n \t\t// prepare result, currently always dense\n \t\t// #rows in temp matrix = 1 + #nnz in the input ( 1 is for the \"zero\" value)\n-\t\tint dim1 = IntUtils.toInt(1+this.getNonZeros());\n+\t\tint dim1 = (int) (1+this.getNonZeros());\n \t\tif(result==null)\n \t\t\tresult=new MatrixBlock(dim1, 2, false);\n \t\telse\n@@ -4944,7 +4943,7 @@ public MatrixBlock groupedAggOperations(MatrixValue tgt, MatrixValue wghts, Matr\n \t\t\t\tthrow new DMLRuntimeException(\"Invalid value (\" + min + \") encountered in 'groups' while computing groupedAggregate\");\n \t\t\tif ( max <= 0 )\n \t\t\t\tthrow new DMLRuntimeException(\"Invalid value (\" + max + \") encountered in 'groups' while computing groupedAggregate.\");\n-\t\t\tngroups = IntUtils.toInt( max );\n+\t\t\tngroups = (int) max;\n \t\t}\n \t\n \t\t// Allocate result matrix\n@@ -5192,7 +5191,7 @@ public void ctableOperations(Operator op, MatrixIndexes ix1, double scalarThat,\n \t{\t\n \t\tCTable ctable = CTable.getCTableFnObject();\n \t\tdouble w = scalarThat;\n-\t\tint offset = IntUtils.toInt((ix1.getRowIndex()-1)*brlen); \n+\t\tint offset = (int) ((ix1.getRowIndex()-1)*brlen);  \n \t\t\n \t\t//sparse-unsafe ctable execution\n \t\t//(because input values of 0 are invalid and have to result in errors) \n@@ -5377,7 +5376,7 @@ else if( qop.wtype2 != null || qop.wtype5 != null )\n \t\t\tR.reset(rlen, clen, sparse);\n \t\telse if( qop.wtype3 != null ) {\n \t\t\tMatrixCharacteristics mc = qop.wtype3.computeOutputCharacteristics(X.rlen, X.clen, U.clen);\n-\t\t\tR.reset( IntUtils.toInt(mc.getRows()), IntUtils.toInt(mc.getCols()), qop.wtype3.isBasic()?X.isInSparseFormat():false);\n+\t\t\tR.reset( (int)mc.getRows(), (int)mc.getCols(), qop.wtype3.isBasic()?X.isInSparseFormat():false);\n \t\t}\n \t\t\n \t\t//core block operation",
                "raw_url": "https://github.com/apache/systemml/raw/c3fdbb4da7c6cac4d363b31366ae21ef976cde92/src/main/java/org/apache/sysml/runtime/matrix/data/MatrixBlock.java",
                "sha": "7e935981e76f7c2ce396194d441c3a4a0bbf28d1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/systemml/blob/25a10f412614235d8974f371a2bb07bc08c88cee/src/main/java/org/apache/sysml/utils/IntUtils.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/systemml/contents/src/main/java/org/apache/sysml/utils/IntUtils.java?ref=25a10f412614235d8974f371a2bb07bc08c88cee",
                "deletions": 34,
                "filename": "src/main/java/org/apache/sysml/utils/IntUtils.java",
                "patch": "@@ -1,34 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.sysml.utils;\n-import static java.lang.Math.toIntExact;\n-\n-\n-public class IntUtils {\n-\n-\tpublic static int toInt(long val) {\n-\t\treturn toIntExact(val);\n-\t}\n-\t\n-\tpublic static int toInt(double val) {\n-\t\tlong val1 = (long) val;\n-\t\treturn toIntExact(val1);\n-\t}\n-\t\n-}",
                "raw_url": "https://github.com/apache/systemml/raw/25a10f412614235d8974f371a2bb07bc08c88cee/src/main/java/org/apache/sysml/utils/IntUtils.java",
                "sha": "aa1f96324c283f1e7cf186206fd7262c090d6e5a",
                "status": "removed"
            }
        ],
        "message": "[BUGFIX] Revert all the files of commit 95cbbd6\n\n- This also contains a instruction parsing-related bugfix introduced by the commit 25a10f4\n- However, there is also NPE error from recent commits related to Spark bcumoffk instruction. This error will be fixed in later commits as it is independent of the commit 95cbbd6.\n\nCloses #851.",
        "parent": "https://github.com/apache/systemml/commit/25a10f412614235d8974f371a2bb07bc08c88cee",
        "patched_files": [
            "SparkDataPartitioner.java",
            "RDDConverterUtilsExt.java"
        ],
        "repo": "systemml",
        "unit_tests": [
            "SparkDataPartitionerTest.java",
            "RDDConverterUtilsExtTest.java"
        ]
    }
}