{
    "hadoop-common_006ffd5": {
        "bug_id": "hadoop-common_006ffd5",
        "commit": "https://github.com/apache/hadoop-common/commit/006ffd5207ff585047ed8bb522e5e516997397f5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=006ffd5207ff585047ed8bb522e5e516997397f5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -179,6 +179,8 @@ Release 2.1.2 - UNRELEASED\n     YARN-1273. Fixed Distributed-shell to account for containers that failed\n     to start. (Hitesh Shah via vinodkv)\n \n+    YARN-1032. Fixed NPE in RackResolver. (Lohit Vijayarenu via acmurthy)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/CHANGES.txt",
                "sha": "0c3a0307fbf19277a23271d1b9cc3981adc9398f",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java?ref=006ffd5207ff585047ed8bb522e5e516997397f5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.CachedDNSToSwitchMapping;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.net.ScriptBasedMapping;\n@@ -98,8 +99,15 @@ private static Node coreResolve(String hostName) {\n     List <String> tmpList = new ArrayList<String>(1);\n     tmpList.add(hostName);\n     List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n-    String rName = rNameList.get(0);\n-    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    String rName = null;\n+    if (rNameList == null || rNameList.get(0) == null) {\n+      rName = NetworkTopology.DEFAULT_RACK;\n+      LOG.info(\"Couldn't resolve \" + hostName + \". Falling back to \"\n+          + NetworkTopology.DEFAULT_RACK);\n+    } else {\n+      rName = rNameList.get(0);\n+      LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    }\n     return new NodeBase(hostName, rName);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "sha": "cc2a56c3be6819cfaa5a05b6ccc38a0b54079607",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java?ref=006ffd5207ff585047ed8bb522e5e516997397f5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "patch": "@@ -28,13 +28,16 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.junit.Assert;\n import org.junit.Test;\n \n public class TestRackResolver {\n \n   private static Log LOG = LogFactory.getLog(TestRackResolver.class);\n+  private static final String invalidHost = \"invalidHost\";\n+\n \n   public static final class MyResolver implements DNSToSwitchMapping {\n \n@@ -50,6 +53,11 @@\n       if (hostList.isEmpty()) {\n         return returnList;\n       }\n+      if (hostList.get(0).equals(invalidHost)) {\n+        // Simulate condition where resolving host returns null\n+        return null; \n+      }\n+        \n       LOG.info(\"Received resolve request for \"\n           + hostList.get(0));\n       if (hostList.get(0).equals(\"host1\")\n@@ -90,6 +98,8 @@ public void testCaching() {\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n     node = RackResolver.resolve(\"host1\");\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n+    node = RackResolver.resolve(invalidHost);\n+    Assert.assertEquals(NetworkTopology.DEFAULT_RACK, node.getNetworkLocation());\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/006ffd5207ff585047ed8bb522e5e516997397f5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "sha": "70ca23c3a2e0b97fd7100ce0519947a806b43a79",
                "status": "modified"
            }
        ],
        "message": "YARN-1032. Fixed NPE in RackResolver. Contributed by Lohit Vijayarenu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/692a361f14da347dc06a2fc924516be9943dbdc5",
        "patched_files": [
            "RackResolver.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRackResolver.java"
        ]
    },
    "hadoop-common_03bf467": {
        "bug_id": "hadoop-common_03bf467",
        "commit": "https://github.com/apache/hadoop-common/commit/03bf4679e2705ffd70452a1b7b0e29b058cae290",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/03bf4679e2705ffd70452a1b7b0e29b058cae290/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=03bf4679e2705ffd70452a1b7b0e29b058cae290",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -471,6 +471,9 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-1615. Fix compilation of TestSubmitJob. (cdouglas)\n \n+    MAPREDUCE-1508. Protect against NPE in TestMultipleLevelCaching. (Aaron\n+    Kimball via cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/03bf4679e2705ffd70452a1b7b0e29b058cae290/CHANGES.txt",
                "sha": "0c5c780698ceb83acf0a4dff998dba0bc609b0c1",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/03bf4679e2705ffd70452a1b7b0e29b058cae290/src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java?ref=03bf4679e2705ffd70452a1b7b0e29b058cae290",
                "deletions": 2,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java",
                "patch": "@@ -119,8 +119,11 @@ private void testCachingAtLevel(int level) throws IOException {\n     \t\t  testName, mr, fileSys, inDir, outputPath, 1, 1, 0, 0);\n       mr.shutdown();\n     } finally {\n-      fileSys.delete(inDir, true);\n-      fileSys.delete(outputPath, true);\n+      if (null != fileSys) {\n+        // inDir, outputPath only exist if fileSys is valid.\n+        fileSys.delete(inDir, true);\n+        fileSys.delete(outputPath, true);\n+      }\n       if (dfs != null) { \n         dfs.shutdown(); \n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/03bf4679e2705ffd70452a1b7b0e29b058cae290/src/test/mapred/org/apache/hadoop/mapred/TestMultipleLevelCaching.java",
                "sha": "6b767fc12380f79f27cb27b1beabfed9d1802e24",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1508. Protect against NPE in TestMultipleLevelCaching. Contributed by Aaron Kimball\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@925554 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/67166032772b591062b187be3e284cc5ed8f2ac2",
        "patched_files": [
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestMultipleLevelCaching.java"
        ]
    },
    "hadoop-common_0432804": {
        "bug_id": "hadoop-common_0432804",
        "commit": "https://github.com/apache/hadoop-common/commit/04328044e122dbd0c44e534a99a01b05fa43021d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -174,6 +174,7 @@ Release 0.23.2 - UNRELEASED\n     (sharad, todd via todd)\n \n   BUG FIXES\n+    HADOOP-8054 NPE with FilterFileSystem (Daryn Sharp via bobby)\n \n     HADOOP-8042  When copying a file out of HDFS, modifying it, and uploading\n     it back into HDFS, the put fails due to a CRC mismatch",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "a199c6e1b88bf677994949ae87c9cf694836c645",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "patch": "@@ -80,6 +80,11 @@ public FileSystem getRawFileSystem() {\n    */\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n+    // this is less than ideal, but existing filesystems sometimes neglect\n+    // to initialize the embedded filesystem\n+    if (fs.getConf() == null) {\n+      fs.initialize(name, conf);\n+    }\n     String scheme = name.getScheme();\n     if (!scheme.equals(fs.getUri().getScheme())) {\n       swapScheme = scheme;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "sha": "91ee2ae710566d54b4950413f17e01051ca0f618",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "patch": "@@ -48,13 +48,6 @@ public LocalFileSystem(FileSystem rawLocalFileSystem) {\n     super(rawLocalFileSystem);\n   }\n     \n-  @Override\n-  public void initialize(URI uri, Configuration conf) throws IOException {\n-    super.initialize(uri, conf);\n-    // ctor didn't initialize the filtered fs\n-    getRawFileSystem().initialize(uri, conf);\n-  }\n-  \n   /** Convert a path to a File. */\n   public File pathToFile(Path path) {\n     return ((RawLocalFileSystem)fs).pathToFile(path);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "sha": "4aff81114b9befe0dc7daf6677c91e65c09356db",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop-common/blob/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java?ref=04328044e122dbd0c44e534a99a01b05fa43021d",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "patch": "@@ -18,24 +18,39 @@\n \n package org.apache.hadoop.fs;\n \n+import static org.junit.Assert.*;\n+import static org.mockito.Matchers.*;\n+import static org.mockito.Mockito.*;\n+\n import java.io.IOException;\n import java.lang.reflect.Method;\n import java.lang.reflect.Modifier;\n+import java.net.URI;\n import java.util.EnumSet;\n import java.util.Iterator;\n \n-import junit.framework.TestCase;\n import org.apache.commons.logging.Log;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.Options.CreateOpts;\n import org.apache.hadoop.fs.Options.Rename;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.util.Progressable;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n \n-public class TestFilterFileSystem extends TestCase {\n+public class TestFilterFileSystem {\n \n   private static final Log LOG = FileSystem.LOG;\n+  private static final Configuration conf = new Configuration();\n \n+  @BeforeClass\n+  public static void setup() {\n+    conf.set(\"fs.flfs.impl\", FilterLocalFileSystem.class.getName());\n+    conf.setBoolean(\"fs.flfs.impl.disable.cache\", true);\n+    conf.setBoolean(\"fs.file.impl.disable.cache\", true);\n+  }\n+  \n   public static class DontCheck {\n     public BlockLocation[] getFileBlockLocations(Path p, \n         long start, long len) { return null; }\n@@ -153,6 +168,7 @@ public void primitiveMkdir(Path f, FsPermission absolutePermission,\n     \n   }\n   \n+  @Test\n   public void testFilterFileSystem() throws Exception {\n     for (Method m : FileSystem.class.getDeclaredMethods()) {\n       if (Modifier.isStatic(m.getModifiers()))\n@@ -176,4 +192,109 @@ public void testFilterFileSystem() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testFilterEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new FilterFileSystem(mockFs), true);\n+  }\n+\n+  @Test\n+  public void testFilterEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new FilterFileSystem(mockFs), false);\n+  }\n+\n+  @Test\n+  public void testLocalEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new LocalFileSystem(mockFs), true);\n+  }  \n+  \n+  @Test\n+  public void testLocalEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new LocalFileSystem(mockFs), false);\n+  }\n+  \n+  private FileSystem createMockFs(boolean useConf) {\n+    FileSystem mockFs = mock(FileSystem.class);\n+    when(mockFs.getUri()).thenReturn(URI.create(\"mock:/\"));\n+    when(mockFs.getConf()).thenReturn(useConf ? conf : null);\n+    return mockFs;\n+  }\n+\n+  @Test\n+  public void testGetLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = FileSystem.getLocal(conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testGetFilterLocalFsSetsConfs() throws Exception {\n+    FilterFileSystem flfs =\n+        (FilterFileSystem) FileSystem.get(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    lfs.initialize(lfs.getUri(), conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testInitFilterFsSetsEmbedConf() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    FilterFileSystem ffs = new FilterFileSystem(lfs);\n+    assertEquals(lfs, ffs.getRawFileSystem());\n+    checkFsConf(ffs, null, 3);\n+    ffs.initialize(URI.create(\"filter:/\"), conf);\n+    checkFsConf(ffs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitFilterLocalFsSetsEmbedConf() throws Exception {\n+    FilterFileSystem flfs = new FilterLocalFileSystem();\n+    assertEquals(LocalFileSystem.class, flfs.getRawFileSystem().getClass());\n+    checkFsConf(flfs, null, 3);\n+    flfs.initialize(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  private void checkInit(FilterFileSystem fs, boolean expectInit)\n+      throws Exception {\n+    URI uri = URI.create(\"filter:/\");\n+    fs.initialize(uri, conf);\n+    \n+    FileSystem embedFs = fs.getRawFileSystem();\n+    if (expectInit) {\n+      verify(embedFs, times(1)).initialize(eq(uri), eq(conf));\n+    } else {\n+      verify(embedFs, times(0)).initialize(any(URI.class), any(Configuration.class));\n+    }\n+  }\n+\n+  // check the given fs's conf, and all its filtered filesystems\n+  private void checkFsConf(FileSystem fs, Configuration conf, int expectDepth) {\n+    int depth = 0;\n+    while (true) {\n+      depth++; \n+      assertFalse(\"depth \"+depth+\">\"+expectDepth, depth > expectDepth);\n+      assertEquals(conf, fs.getConf());\n+      if (!(fs instanceof FilterFileSystem)) {\n+        break;\n+      }\n+      fs = ((FilterFileSystem) fs).getRawFileSystem();\n+    }\n+    assertEquals(expectDepth, depth);\n+  }\n+  \n+  private static class FilterLocalFileSystem extends FilterFileSystem {\n+    FilterLocalFileSystem() {\n+      super(new LocalFileSystem());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/04328044e122dbd0c44e534a99a01b05fa43021d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "sha": "364f46d2a40652beb7d20a27c3755376ba95d9b8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8054. NPE with FilterFileSystem (Daryn Sharp via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1245637 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/585a0bf5128d1477d06b6cb708aedfebc6dc4b04",
        "patched_files": [
            "FilterFileSystem.java",
            "LocalFileSystem.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFilterFileSystem.java",
            "TestLocalFileSystem.java"
        ]
    },
    "hadoop-common_0727aa6": {
        "bug_id": "hadoop-common_0727aa6",
        "commit": "https://github.com/apache/hadoop-common/commit/0727aa6e8cb2499562ae1d35f7a4e5a923083571",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -163,6 +163,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-2539. Fixed NPE in getMapTaskReports in JobClient. (Robert Evans via\n+    acmurthy) \n+\n     MAPREDUCE-2531. Fixed jobcontrol to downgrade JobID. (Robert Evans via\n     acmurthy) \n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/CHANGES.txt",
                "sha": "493d13037e920b46fb0b710e743a3eaf1d5f3b8f",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "deletions": 20,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -576,6 +576,8 @@ public RunningJob getJob(String jobid) throws IOException {\n     return getJob(JobID.forName(jobid));\n   }\n   \n+  private static final TaskReport[] EMPTY_TASK_REPORTS = new TaskReport[0];\n+  \n   /**\n    * Get the information of the current state of the map tasks of a job.\n    * \n@@ -584,9 +586,16 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */\n   public TaskReport[] getMapTaskReports(JobID jobId) throws IOException {\n+    return getTaskReports(jobId, TaskType.MAP);\n+  }\n+  \n+  private TaskReport[] getTaskReports(JobID jobId, TaskType type) throws IOException {\n     try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.MAP));\n+      Job j = cluster.getJob(jobId);\n+      if(j == null) {\n+        return EMPTY_TASK_REPORTS;\n+      }\n+      return TaskReport.downgradeArray(j.getTaskReports(type));\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n@@ -606,12 +615,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getReduceTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.REDUCE));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.REDUCE);\n   }\n \n   /**\n@@ -622,12 +626,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getCleanupTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.JOB_CLEANUP));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.JOB_CLEANUP);\n   }\n \n   /**\n@@ -638,12 +637,7 @@ public RunningJob getJob(String jobid) throws IOException {\n    * @throws IOException\n    */    \n   public TaskReport[] getSetupTaskReports(JobID jobId) throws IOException {\n-    try {\n-      return TaskReport.downgradeArray(\n-        cluster.getJob(jobId).getTaskReports(TaskType.JOB_SETUP));\n-    } catch (InterruptedException ie) {\n-      throw new IOException(ie);\n-    }\n+    return getTaskReports(jobId, TaskType.JOB_SETUP);\n   }\n \n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "245e362866fe446422f09c9425a1f3e6651e7879",
                "status": "modified"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=0727aa6e8cb2499562ae1d35f7a4e5a923083571",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.junit.Test;\n+\n+public class JobClientUnitTest {\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testMapTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getMapTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testReduceTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getReduceTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testSetupTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getSetupTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+  \n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testCleanupTaskReportsWithNullJob() throws Exception {\n+    JobClient client = new JobClient();\n+    Cluster mockCluster = mock(Cluster.class);\n+    client.cluster = mockCluster;\n+    JobID id = new JobID(\"test\",0);\n+    \n+    when(mockCluster.getJob(id)).thenReturn(null);\n+    \n+    TaskReport[] result = client.getCleanupTaskReports(id);\n+    assertEquals(0, result.length);\n+    \n+    verify(mockCluster).getJob(id);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0727aa6e8cb2499562ae1d35f7a4e5a923083571/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "sha": "11873c14233814e67b9a766c486e4f8b775edd10",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2539. Fixed NPE in getMapTaskReports in JobClient. Contributed by Robert Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1130994 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/34fc2ec906ba5602d4a65b0bb2ec9fc7c4cb855f",
        "patched_files": [
            "JobClient.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java",
            "JobClientUnitTest.java"
        ]
    },
    "hadoop-common_076d285": {
        "bug_id": "hadoop-common_076d285",
        "commit": "https://github.com/apache/hadoop-common/commit/076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -558,6 +558,9 @@ Release 0.23.4 - UNRELEASED\n     MAPREDUCE-4691. Historyserver can report \"Unknown job\" after RM says job\n     has completed (Robert Joseph Evans via jlowe)\n \n+    MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE\n+    (jlowe via bobby)\n+\n Release 0.23.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "9e98ac96e1d250ab9b6265c43322bbd37c6f80f3",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java?ref=076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "patch": "@@ -42,6 +42,8 @@\n \n public class CompletedTask implements Task {\n \n+  private static final Counters EMPTY_COUNTERS = new Counters();\n+\n   private final TaskId taskId;\n   private final TaskInfo taskInfo;\n   private TaskReport report;\n@@ -124,7 +126,11 @@ private void constructTaskReport() {\n     report.setFinishTime(taskInfo.getFinishTime());\n     report.setTaskState(getState());\n     report.setProgress(getProgress());\n-    report.setCounters(TypeConverter.toYarn(getCounters()));\n+    Counters counters = getCounters();\n+    if (counters == null) {\n+      counters = EMPTY_COUNTERS;\n+    }\n+    report.setCounters(TypeConverter.toYarn(counters));\n     if (successfulAttempt != null) {\n       report.setSuccessfulAttempt(successfulAttempt);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "sha": "830b64f1ad364a1d9982e3dad1b0bd37f2175732",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hadoop-common/blob/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java?ref=076d285ac838dbae1ee28b93a4b7e492cc31f4ff",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "patch": "@@ -49,6 +49,7 @@\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId;\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskState;\n import org.apache.hadoop.mapreduce.v2.app.MRApp;\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n@@ -402,6 +403,63 @@ public void testHistoryParsingForFailedAttempts() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testCountersForFailedTask() throws Exception {\n+    LOG.info(\"STARTING testCountersForFailedTask\");\n+    try {\n+    Configuration conf = new Configuration();\n+    conf\n+        .setClass(\n+            CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+            MyResolver.class, DNSToSwitchMapping.class);\n+    RackResolver.init(conf);\n+    MRApp app = new MRAppWithHistoryWithFailedTask(2, 1, true,\n+        this.getClass().getName(), true);\n+    app.submit(conf);\n+    Job job = app.getContext().getAllJobs().values().iterator().next();\n+    JobId jobId = job.getID();\n+    app.waitForState(job, JobState.FAILED);\n+\n+    // make sure all events are flushed\n+    app.waitForState(Service.STATE.STOPPED);\n+\n+    String jobhistoryDir = JobHistoryUtils\n+        .getHistoryIntermediateDoneDirForUser(conf);\n+    JobHistory jobHistory = new JobHistory();\n+    jobHistory.init(conf);\n+\n+    JobIndexInfo jobIndexInfo = jobHistory.getJobFileInfo(jobId)\n+        .getJobIndexInfo();\n+    String jobhistoryFileName = FileNameIndexUtils\n+        .getDoneFileName(jobIndexInfo);\n+\n+    Path historyFilePath = new Path(jobhistoryDir, jobhistoryFileName);\n+    FSDataInputStream in = null;\n+    FileContext fc = null;\n+    try {\n+      fc = FileContext.getFileContext(conf);\n+      in = fc.open(fc.makeQualified(historyFilePath));\n+    } catch (IOException ioe) {\n+      LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\n+      throw (new Exception(\"Can not open History File\"));\n+    }\n+\n+    JobHistoryParser parser = new JobHistoryParser(in);\n+    JobInfo jobInfo = parser.parse();\n+    Exception parseException = parser.getParseException();\n+    Assert.assertNull(\"Caught an expected exception \" + parseException,\n+        parseException);\n+    for (Map.Entry<TaskID,TaskInfo> entry : jobInfo.getAllTasks().entrySet()) {\n+      TaskId yarnTaskID = TypeConverter.toYarn(entry.getKey());\n+      CompletedTask ct = new CompletedTask(yarnTaskID, entry.getValue());\n+      Assert.assertNotNull(\"completed task report has null counters\",\n+          ct.getReport().getCounters());\n+    }\n+    } finally {\n+      LOG.info(\"FINISHED testCountersForFailedTask\");\n+    }\n+  }\n+\n   static class MRAppWithHistoryWithFailedAttempt extends MRAppWithHistory {\n \n     public MRAppWithHistoryWithFailedAttempt(int maps, int reduces, boolean autoComplete,\n@@ -422,6 +480,26 @@ protected void attemptLaunched(TaskAttemptId attemptID) {\n     }\n   }\n \n+  static class MRAppWithHistoryWithFailedTask extends MRAppWithHistory {\n+\n+    public MRAppWithHistoryWithFailedTask(int maps, int reduces, boolean autoComplete,\n+        String testName, boolean cleanOnStart) {\n+      super(maps, reduces, autoComplete, testName, cleanOnStart);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Override\n+    protected void attemptLaunched(TaskAttemptId attemptID) {\n+      if (attemptID.getTaskId().getId() == 0) {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_FAILMSG));\n+      } else {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));\n+      }\n+    }\n+  }\n+\n   public static void main(String[] args) throws Exception {\n     TestJobHistoryParsing t = new TestJobHistoryParsing();\n     t.testHistoryParsing();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/076d285ac838dbae1ee28b93a4b7e492cc31f4ff/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "sha": "f9acb1a38212081cd4aa43beb959dc58d3cd310d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE (jlowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1391679 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9d91daeefc9e42ed5848f214c5774bdde6c9c6bb",
        "patched_files": [
            "CompletedTask.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobHistoryParsing.java"
        ]
    },
    "hadoop-common_0933689": {
        "bug_id": "hadoop-common_0933689",
        "commit": "https://github.com/apache/hadoop-common/commit/09336893d22e678afe600dd606849df36812af7e",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -220,9 +220,6 @@ Release 0.23.3 - UNRELEASED\n     HADOOP-8163. Improve ActiveStandbyElector to provide hooks for\n     fencing old active. (todd)\n \n-    HADOOP-8193. Refactor FailoverController/HAAdmin code to add an abstract\n-    class for \"target\" services. (todd)\n-\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "f3fda7668c5ee52db3dacbecaf62f2f9cfa5b5bd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "patch": "@@ -19,16 +19,11 @@\n \n import java.io.IOException;\n \n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-\n /**\n  * Indicates that the operator has specified an invalid configuration\n  * for fencing methods.\n  */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public class BadFencingConfigurationException extends IOException {\n+class BadFencingConfigurationException extends IOException {\n   private static final long serialVersionUID = 1L;\n \n   public BadFencingConfigurationException(String msg) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "sha": "3d3b1ba53cca506a3ef8d28ade71a18c25747777",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 22,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.ha;\n \n import java.io.IOException;\n+import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -50,21 +51,21 @@\n    * allow it to become active, eg because it triggers a log roll\n    * so the standby can learn about new blocks and leave safemode.\n    *\n-   * @param target service to make active\n+   * @param toSvc service to make active\n+   * @param toSvcName name of service to make active\n    * @param forceActive ignore toSvc if it reports that it is not ready\n    * @throws FailoverFailedException if we should avoid failover\n    */\n-  private static void preFailoverChecks(HAServiceTarget target,\n+  private static void preFailoverChecks(HAServiceProtocol toSvc,\n+                                        InetSocketAddress toSvcAddr,\n                                         boolean forceActive)\n       throws FailoverFailedException {\n     HAServiceStatus toSvcStatus;\n-    HAServiceProtocol toSvc;\n \n     try {\n-      toSvc = target.getProxy();\n       toSvcStatus = toSvc.getServiceStatus();\n     } catch (IOException e) {\n-      String msg = \"Unable to get service state for \" + target;\n+      String msg = \"Unable to get service state for \" + toSvcAddr;\n       LOG.error(msg, e);\n       throw new FailoverFailedException(msg, e);\n     }\n@@ -78,7 +79,7 @@ private static void preFailoverChecks(HAServiceTarget target,\n       String notReadyReason = toSvcStatus.getNotReadyReason();\n       if (!forceActive) {\n         throw new FailoverFailedException(\n-            target + \" is not ready to become active: \" +\n+            toSvcAddr + \" is not ready to become active: \" +\n             notReadyReason);\n       } else {\n         LOG.warn(\"Service is not ready to become active, but forcing: \" +\n@@ -102,72 +103,77 @@ private static void preFailoverChecks(HAServiceTarget target,\n    * then try to failback.\n    *\n    * @param fromSvc currently active service\n+   * @param fromSvcAddr addr of the currently active service\n    * @param toSvc service to make active\n+   * @param toSvcAddr addr of the service to make active\n+   * @param fencer for fencing fromSvc\n    * @param forceFence to fence fromSvc even if not strictly necessary\n    * @param forceActive try to make toSvc active even if it is not ready\n    * @throws FailoverFailedException if the failover fails\n    */\n-  public static void failover(HAServiceTarget fromSvc,\n-                              HAServiceTarget toSvc,\n+  public static void failover(HAServiceProtocol fromSvc,\n+                              InetSocketAddress fromSvcAddr,\n+                              HAServiceProtocol toSvc,\n+                              InetSocketAddress toSvcAddr,\n+                              NodeFencer fencer,\n                               boolean forceFence,\n                               boolean forceActive)\n       throws FailoverFailedException {\n-    Preconditions.checkArgument(fromSvc.getFencer() != null,\n-        \"failover requires a fencer\");\n-    preFailoverChecks(toSvc, forceActive);\n+    Preconditions.checkArgument(fencer != null, \"failover requires a fencer\");\n+    preFailoverChecks(toSvc, toSvcAddr, forceActive);\n \n     // Try to make fromSvc standby\n     boolean tryFence = true;\n     try {\n-      HAServiceProtocolHelper.transitionToStandby(fromSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToStandby(fromSvc);\n       // We should try to fence if we failed or it was forced\n       tryFence = forceFence ? true : false;\n     } catch (ServiceFailedException sfe) {\n-      LOG.warn(\"Unable to make \" + fromSvc + \" standby (\" +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr + \" standby (\" +\n           sfe.getMessage() + \")\");\n     } catch (IOException ioe) {\n-      LOG.warn(\"Unable to make \" + fromSvc +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr +\n           \" standby (unable to connect)\", ioe);\n     }\n \n     // Fence fromSvc if it's required or forced by the user\n     if (tryFence) {\n-      if (!fromSvc.getFencer().fence(fromSvc)) {\n+      if (!fencer.fence(fromSvcAddr)) {\n         throw new FailoverFailedException(\"Unable to fence \" +\n-            fromSvc + \". Fencing failed.\");\n+            fromSvcAddr + \". Fencing failed.\");\n       }\n     }\n \n     // Try to make toSvc active\n     boolean failed = false;\n     Throwable cause = null;\n     try {\n-      HAServiceProtocolHelper.transitionToActive(toSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToActive(toSvc);\n     } catch (ServiceFailedException sfe) {\n-      LOG.error(\"Unable to make \" + toSvc + \" active (\" +\n+      LOG.error(\"Unable to make \" + toSvcAddr + \" active (\" +\n           sfe.getMessage() + \"). Failing back.\");\n       failed = true;\n       cause = sfe;\n     } catch (IOException ioe) {\n-      LOG.error(\"Unable to make \" + toSvc +\n+      LOG.error(\"Unable to make \" + toSvcAddr +\n           \" active (unable to connect). Failing back.\", ioe);\n       failed = true;\n       cause = ioe;\n     }\n \n     // We failed to make toSvc active\n     if (failed) {\n-      String msg = \"Unable to failover to \" + toSvc;\n+      String msg = \"Unable to failover to \" + toSvcAddr;\n       // Only try to failback if we didn't fence fromSvc\n       if (!tryFence) {\n         try {\n           // Unconditionally fence toSvc in case it is still trying to\n           // become active, eg we timed out waiting for its response.\n           // Unconditionally force fromSvc to become active since it\n           // was previously active when we initiated failover.\n-          failover(toSvc, fromSvc, true, true);\n+          failover(toSvc, toSvcAddr, fromSvc, fromSvcAddr, fencer, true, true);\n         } catch (FailoverFailedException ffe) {\n-          msg += \". Failback to \" + fromSvc +\n+          msg += \". Failback to \" + fromSvcAddr +\n             \" failed (\" + ffe.getMessage() + \")\";\n           LOG.fatal(msg);\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "sha": "c8878e8b73951f0f7a3290757615241c8f989d08",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n+\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configurable;\n@@ -60,6 +62,6 @@\n    * @throws BadFencingConfigurationException if the configuration was\n    *         determined to be invalid only at runtime\n    */\n-  public boolean tryFence(HAServiceTarget target, String args)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String args)\n     throws BadFencingConfigurationException;\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "sha": "d8bda1402fa928ae1001a2a705709aaad03d70fb",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import java.io.IOException;\n import java.io.PrintStream;\n+import java.net.InetSocketAddress;\n import java.util.Map;\n \n import org.apache.commons.cli.Options;\n@@ -27,8 +28,11 @@\n import org.apache.commons.cli.GnuParser;\n import org.apache.commons.cli.ParseException;\n \n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -73,8 +77,6 @@\n   protected PrintStream errOut = System.err;\n   PrintStream out = System.out;\n \n-  protected abstract HAServiceTarget resolveTarget(String string);\n-\n   protected String getUsageString() {\n     return \"Usage: HAAdmin\";\n   }\n@@ -107,7 +109,7 @@ private int transitionToActive(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToActive(proto);\n     return 0;\n   }\n@@ -120,13 +122,14 @@ private int transitionToStandby(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToStandby(proto);\n     return 0;\n   }\n \n   private int failover(final String[] argv)\n       throws IOException, ServiceFailedException {\n+    Configuration conf = getConf();\n     boolean forceFence = false;\n     boolean forceActive = false;\n \n@@ -159,12 +162,29 @@ private int failover(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceTarget fromNode = resolveTarget(args[0]);\n-    HAServiceTarget toNode = resolveTarget(args[1]);\n-    \n+    NodeFencer fencer;\n     try {\n-      FailoverController.failover(fromNode, toNode,\n-          forceFence, forceActive); \n+      fencer = NodeFencer.create(conf);\n+    } catch (BadFencingConfigurationException bfce) {\n+      errOut.println(\"failover: incorrect fencing configuration: \" + \n+          bfce.getLocalizedMessage());\n+      return -1;\n+    }\n+    if (fencer == null) {\n+      errOut.println(\"failover: no fencer configured\");\n+      return -1;\n+    }\n+\n+    InetSocketAddress addr1 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[0]));\n+    InetSocketAddress addr2 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[1]));\n+    HAServiceProtocol proto1 = getProtocol(args[0]);\n+    HAServiceProtocol proto2 = getProtocol(args[1]);\n+\n+    try {\n+      FailoverController.failover(proto1, addr1, proto2, addr2,\n+          fencer, forceFence, forceActive); \n       out.println(\"Failover from \"+args[0]+\" to \"+args[1]+\" successful\");\n     } catch (FailoverFailedException ffe) {\n       errOut.println(\"Failover failed: \" + ffe.getLocalizedMessage());\n@@ -181,7 +201,7 @@ private int checkHealth(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     try {\n       HAServiceProtocolHelper.monitorHealth(proto);\n     } catch (HealthCheckFailedException e) {\n@@ -199,7 +219,7 @@ private int getServiceState(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     out.println(proto.getServiceStatus().getState());\n     return 0;\n   }\n@@ -212,6 +232,16 @@ protected String getServiceAddr(String serviceId) {\n     return serviceId;\n   }\n \n+  /**\n+   * Return a proxy to the specified target service.\n+   */\n+  protected HAServiceProtocol getProtocol(String serviceId)\n+      throws IOException {\n+    String serviceAddr = getServiceAddr(serviceId);\n+    InetSocketAddress addr = NetUtils.createSocketAddr(serviceAddr);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, getConf());\n+  }\n+\n   @Override\n   public int run(String[] argv) throws Exception {\n     try {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "sha": "a16ffb4c4004bdf89f821920605e17b78d9fe4a1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java?ref=b1f1f5d93b21d98be1e61b777994322f57e3fda8",
                "deletions": 74,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "patch": "@@ -1,74 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n-import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n-\n-/**\n- * Represents a target of the client side HA administration commands.\n- */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public abstract class HAServiceTarget {\n-\n-  /**\n-   * @return the IPC address of the target node.\n-   */\n-  public abstract InetSocketAddress getAddress();\n-\n-  /**\n-   * @return a Fencer implementation configured for this target node\n-   */\n-  public abstract NodeFencer getFencer();\n-  \n-  /**\n-   * @throws BadFencingConfigurationException if the fencing configuration\n-   * appears to be invalid. This is divorced from the above\n-   * {@link #getFencer()} method so that the configuration can be checked\n-   * during the pre-flight phase of failover.\n-   */\n-  public abstract void checkFencingConfigured()\n-      throws BadFencingConfigurationException;\n-  \n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public HAServiceProtocol getProxy(Configuration conf, int timeoutMs)\n-      throws IOException {\n-    Configuration confCopy = new Configuration(conf);\n-    // Lower the timeout so we quickly fail to connect\n-    confCopy.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n-    return new HAServiceProtocolClientSideTranslatorPB(\n-        getAddress(),\n-        confCopy, null, timeoutMs);\n-  }\n-\n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public final HAServiceProtocol getProxy() throws IOException {\n-    return getProxy(new Configuration(), 0); // default conf, timeout\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "sha": "78a2f2e4d98ab1b14b3042af340caa3d1c0d751b",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Matcher;\n@@ -90,14 +91,14 @@ public static NodeFencer create(Configuration conf)\n     return new NodeFencer(conf);\n   }\n \n-  public boolean fence(HAServiceTarget fromSvc) {\n+  public boolean fence(InetSocketAddress serviceAddr) {\n     LOG.info(\"====== Beginning Service Fencing Process... ======\");\n     int i = 0;\n     for (FenceMethodWithArg method : methods) {\n       LOG.info(\"Trying method \" + (++i) + \"/\" + methods.size() +\": \" + method);\n       \n       try {\n-        if (method.method.tryFence(fromSvc, method.arg)) {\n+        if (method.method.tryFence(serviceAddr, method.arg)) {\n           LOG.info(\"====== Fencing successful by method \" + method + \" ======\");\n           return true;\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "sha": "34a2c8b823a3ee2943d18c94431babe22275ef2d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "patch": "@@ -75,8 +75,7 @@ public void checkArgs(String args) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String cmd) {\n-    InetSocketAddress serviceAddr = target.getAddress();\n+  public boolean tryFence(InetSocketAddress serviceAddr, String cmd) {\n     List<String> cmdList = Arrays.asList(cmd.split(\"\\\\s+\"));\n \n     // Create arg list with service as the first argument",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "sha": "ca81f23a1878fa7fea3247b8e197e1150209a53c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "patch": "@@ -79,11 +79,10 @@ public void checkArgs(String argStr) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String argsStr)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String argsStr)\n       throws BadFencingConfigurationException {\n \n     Args args = new Args(argsStr);\n-    InetSocketAddress serviceAddr = target.getAddress();\n     String host = serviceAddr.getHostName();\n     \n     Session session;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "sha": "00b9a83a572a3ad8a14cf2f7eab8cd013501a2ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java?ref=b1f1f5d93b21d98be1e61b777994322f57e3fda8",
                "deletions": 94,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "patch": "@@ -1,94 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n-import org.apache.hadoop.security.AccessControlException;\n-import org.mockito.Mockito;\n-\n-/**\n- * Test-only implementation of {@link HAServiceTarget}, which returns\n- * a mock implementation.\n- */\n-class DummyHAService extends HAServiceTarget {\n-  HAServiceState state;\n-  HAServiceProtocol proxy;\n-  NodeFencer fencer;\n-  InetSocketAddress address;\n-\n-  DummyHAService(HAServiceState state, InetSocketAddress address) {\n-    this.state = state;\n-    this.proxy = makeMock();\n-    this.fencer = Mockito.mock(NodeFencer.class);\n-    this.address = address;\n-  }\n-  \n-  private HAServiceProtocol makeMock() {\n-    return Mockito.spy(new HAServiceProtocol() {\n-      @Override\n-      public void monitorHealth() throws HealthCheckFailedException,\n-          AccessControlException, IOException {\n-      }\n-\n-      @Override\n-      public void transitionToActive() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.ACTIVE;\n-      }\n-\n-      @Override\n-      public void transitionToStandby() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.STANDBY;\n-      }\n-\n-      @Override\n-      public HAServiceStatus getServiceStatus() throws IOException {\n-        HAServiceStatus ret = new HAServiceStatus(state);\n-        if (state == HAServiceState.STANDBY) {\n-          ret.setReadyToBecomeActive();\n-        }\n-        return ret;\n-      }\n-    });\n-  }\n-\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return address;\n-  }\n-\n-  @Override\n-  public HAServiceProtocol getProxy(Configuration conf, int timeout)\n-      throws IOException {\n-    return proxy;\n-  }\n-\n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "sha": "69c4a6fde41141b8c43c94a4efe41348718f9a32",
                "status": "removed"
            },
            {
                "additions": 214,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "changes": 358,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 144,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "patch": "@@ -24,85 +24,124 @@\n import static org.mockito.Mockito.verify;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysFailFencer;\n import static org.apache.hadoop.ha.TestNodeFencer.setupFencer;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n \n import org.junit.Test;\n-import org.mockito.Mockito;\n-import org.mockito.internal.stubbing.answers.ThrowsException;\n-import org.mockito.stubbing.Answer;\n-\n import static org.junit.Assert.*;\n \n public class TestFailoverController {\n+\n   private InetSocketAddress svc1Addr = new InetSocketAddress(\"svc1\", 1234); \n-  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678);\n+  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678); \n+\n+  private class DummyService implements HAServiceProtocol {\n+    HAServiceState state;\n+\n+    DummyService(HAServiceState state) {\n+      this.state = state;\n+    }\n+\n+    @Override\n+    public void monitorHealth() throws HealthCheckFailedException, IOException {\n+      // Do nothing\n+    }\n+\n+    @Override\n+    public void transitionToActive() throws ServiceFailedException, IOException {\n+      state = HAServiceState.ACTIVE;\n+    }\n \n-  HAServiceStatus STATE_NOT_READY = new HAServiceStatus(HAServiceState.STANDBY)\n-      .setNotReadyToBecomeActive(\"injected not ready\");\n+    @Override\n+    public void transitionToStandby() throws ServiceFailedException, IOException {\n+      state = HAServiceState.STANDBY;\n+    }\n \n+    @Override\n+    public HAServiceStatus getServiceStatus() throws IOException {\n+      HAServiceStatus ret = new HAServiceStatus(state);\n+      if (state == HAServiceState.STANDBY) {\n+        ret.setReadyToBecomeActive();\n+      }\n+      return ret;\n+    }\n+    \n+    private HAServiceState getServiceState() {\n+      return state;\n+    }\n+  }\n+  \n   @Test\n   public void testFailoverAndFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc1, svc2, false, false);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc2, svc1, false, false);\n+    FailoverController.failover(svc2, svc2Addr, svc1, svc1Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromStandbyToStandby() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.STANDBY, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.STANDBY);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n-    FailoverController.failover(svc1, svc2, false, false);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromActiveToActive() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.ACTIVE, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.ACTIVE);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to an already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverWithoutPermission() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc1.proxy).getServiceStatus();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover when access is denied\");\n     } catch (FailoverFailedException ffe) {\n       assertTrue(ffe.getCause().getMessage().contains(\"Access denied\"));\n@@ -112,13 +151,19 @@ public void testFailoverWithoutPermission() throws Exception {\n \n   @Test\n   public void testFailoverToUnreadyService() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doReturn(STATE_NOT_READY).when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        HAServiceStatus ret = new HAServiceStatus(HAServiceState.STANDBY);\n+        ret.setNotReadyToBecomeActive(\"injected not ready\");\n+        return ret;\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to a service that's not ready\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -127,88 +172,95 @@ public void testFailoverToUnreadyService() throws Exception {\n       }\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n \n     // Forcing it means we ignore readyToBecomeActive\n-    FailoverController.failover(svc1, svc2, false, true);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, true);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToUnhealthyServiceFailsAndFailsback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new HealthCheckFailedException(\"Failed!\"))\n-        .when(svc2.proxy).monitorHealth();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void monitorHealth() throws HealthCheckFailedException {\n+        throw new HealthCheckFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to unhealthy service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceSucceeds() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Faulty active prevented failover\");\n     }\n \n     // svc1 still thinks it's active, that's OK, it was fenced\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc1, AlwaysSucceedFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceFencingFailure() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over even though fencing failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFencingFailureDuringFailover() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over even though fencing requested and failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -217,105 +269,115 @@ public void testFencingFailureDuringFailover() throws Exception {\n     // If fencing was requested and it failed we don't try to make\n     // svc2 active anyway, and we don't failback to svc1.\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n   \n+  private HAServiceProtocol getProtocol(String target)\n+      throws IOException {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(target);\n+    Configuration conf = new Configuration();\n+    // Lower the timeout so we quickly fail to connect\n+    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, conf);\n+  }\n+\n   @Test\n   public void testFailoverFromNonExistantServiceWithFencer() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(null, svc1Addr));\n-    // Getting a proxy to a dead server will throw IOException on call,\n-    // not on creation of the proxy.\n-    HAServiceProtocol errorThrowingProxy = Mockito.mock(HAServiceProtocol.class,\n-        new ThrowsException(new IOException(\"Could not connect to host\")));\n-    Mockito.doReturn(errorThrowingProxy).when(svc1).getProxy();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    HAServiceProtocol svc1 = getProtocol(\"localhost:1234\");\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Non-existant active prevented failover\");\n     }\n \n     // Don't check svc1 because we can't reach it, but that's OK, it's been fenced.\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToNonExistantServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = spy(new DummyHAService(null, svc2Addr));\n-    Mockito.doThrow(new IOException(\"Failed to connect\"))\n-      .when(svc2).getProxy(Mockito.<Configuration>any(),\n-          Mockito.anyInt());\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    HAServiceProtocol svc2 = getProtocol(\"localhost:1234\");\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to a non-existant standby\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToFaultyServiceFailsbackOK() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(HAServiceState.ACTIVE, svc1Addr));\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = spy(new DummyService(HAServiceState.ACTIVE));\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // svc1 went standby then back to active\n-    verify(svc1.proxy).transitionToStandby();\n-    verify(svc1.proxy).transitionToActive();\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    verify(svc1).transitionToStandby();\n+    verify(svc1).transitionToActive();\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeDontFailbackIfActiveWasFenced() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // We failed to failover and did not failback because we fenced\n     // svc1 (we forced it), therefore svc1 and svc2 should be standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n     AlwaysSucceedFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -324,22 +386,25 @@ public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n     // We failed to failover. We did not fence svc1 because it cooperated\n     // and we didn't force it, so we failed back to svc1 and fenced svc2.\n     // Note svc2 still thinks it's active, that's OK, we fenced it.\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc2, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysSucceedFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new IOException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n     AlwaysFailFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -348,30 +413,35 @@ public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n     // We did not fence svc1 because it cooperated and we didn't force it, \n     // we failed to failover so we fenced svc2, we failed to fence svc2\n     // so we did not failback to svc1, ie it's still standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc2, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysFailFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailbackToFaultyServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToActive();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1, svc1Addr, svc2, svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "sha": "6dec32c636de5317a679eb3bc609faa8168e0922",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "patch": "@@ -22,15 +22,14 @@\n import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n import java.io.PrintStream;\n-import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n \n import org.junit.Before;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n import com.google.common.base.Charsets;\n import com.google.common.base.Joiner;\n@@ -41,14 +40,15 @@\n   private HAAdmin tool;\n   private ByteArrayOutputStream errOutBytes = new ByteArrayOutputStream();\n   private String errOutput;\n-\n+  private HAServiceProtocol mockProtocol;\n+  \n   @Before\n   public void setup() throws IOException {\n+    mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new HAAdmin() {\n       @Override\n-      protected HAServiceTarget resolveTarget(String target) {\n-        return new DummyHAService(HAServiceState.STANDBY,\n-            new InetSocketAddress(\"dummy\", 12345));\n+      protected HAServiceProtocol getProtocol(String target) throws IOException {\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(new Configuration());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "sha": "7f885d8bc2535d2b34eb4fe9e5dd961da12be685",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 28,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "patch": "@@ -26,35 +26,26 @@\n import org.apache.hadoop.conf.Configured;\n import org.junit.Before;\n import org.junit.Test;\n-import org.mockito.Mockito;\n \n import com.google.common.collect.Lists;\n \n public class TestNodeFencer {\n \n-  private HAServiceTarget MOCK_TARGET;\n-  \n-\n   @Before\n   public void clearMockState() {\n     AlwaysSucceedFencer.fenceCalled = 0;\n     AlwaysSucceedFencer.callArgs.clear();\n     AlwaysFailFencer.fenceCalled = 0;\n     AlwaysFailFencer.callArgs.clear();\n-    \n-    MOCK_TARGET = Mockito.mock(HAServiceTarget.class);\n-    Mockito.doReturn(\"my mock\").when(MOCK_TARGET).toString();\n-    Mockito.doReturn(new InetSocketAddress(\"host\", 1234))\n-        .when(MOCK_TARGET).getAddress();\n   }\n \n   @Test\n   public void testSingleFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n   \n@@ -63,7 +54,7 @@ public void testMultipleFencers() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar)\\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // Only one call, since the first fencer succeeds\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n@@ -77,12 +68,12 @@ public void testWhitespaceAndCommentsInConfig()\n         \" # the next one will always fail\\n\" +\n         \" \" + AlwaysFailFencer.class.getName() + \"(foo) # <- fails\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar) \\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysFailFencer.fencedSvc);\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysFailFencer.callArgs.get(0));\n     assertEquals(\"bar\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n@@ -91,41 +82,41 @@ public void testWhitespaceAndCommentsInConfig()\n   public void testArglessFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName());\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(null, AlwaysSucceedFencer.callArgs.get(0));\n   }\n \n   @Test\n   public void testShortNameShell() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"shell(true)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSsh() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUser() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUserPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   public static NodeFencer setupFencer(String confStr)\n@@ -142,12 +133,12 @@ public static NodeFencer setupFencer(String confStr)\n   public static class AlwaysSucceedFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return true;\n@@ -164,12 +155,12 @@ public void checkArgs(String args) {\n   public static class AlwaysFailFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return false;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "sha": "5508547c0a52daff57ae2400b14a0f30c946d699",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "patch": "@@ -22,7 +22,6 @@\n import java.net.InetSocketAddress;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.util.StringUtils;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -33,9 +32,6 @@\n \n public class TestShellCommandFencer {\n   private ShellCommandFencer fencer = createFencer();\n-  private static final HAServiceTarget TEST_TARGET =\n-      new DummyHAService(HAServiceState.ACTIVE,\n-          new InetSocketAddress(\"host\", 1234));\n   \n   @BeforeClass\n   public static void setupLogSpy() {\n@@ -61,10 +57,11 @@ private static ShellCommandFencer createFencer() {\n    */\n   @Test\n   public void testBasicSuccessFailure() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo\"));\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"exit 1\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo\"));\n+    assertFalse(fencer.tryFence(addr, \"exit 1\"));\n     // bad path should also fail\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"xxxxxxxxxxxx\"));\n+    assertFalse(fencer.tryFence(addr, \"xxxxxxxxxxxx\"));\n   }\n   \n   @Test\n@@ -101,7 +98,8 @@ public void testCheckParensNoArgs() {\n    */\n   @Test\n   public void testStdoutLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello\"));\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo hello: host:1234 hello\"));\n   }\n@@ -112,7 +110,8 @@ public void testStdoutLogging() {\n    */\n   @Test\n   public void testStderrLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello >&2\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello >&2\"));\n     Mockito.verify(ShellCommandFencer.LOG).warn(\n         Mockito.endsWith(\"echo hello >&2: host:1234 hello\"));\n   }\n@@ -123,7 +122,8 @@ public void testStderrLogging() {\n    */\n   @Test\n   public void testConfAsEnvironment() {\n-    fencer.tryFence(TEST_TARGET, \"echo $in_fencing_tests\");\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    fencer.tryFence(addr, \"echo $in_fencing_tests\");\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo $in...ing_tests: host:1234 yessir\"));\n   }\n@@ -136,7 +136,8 @@ public void testConfAsEnvironment() {\n    */\n   @Test(timeout=10000)\n   public void testSubprocessInputIsClosed() {\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"read\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertFalse(fencer.tryFence(addr, \"read\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "sha": "49bae039eccbcc7b7f43a73e64e2084bc66c9a2d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 19,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "patch": "@@ -23,7 +23,6 @@\n \n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.SshFenceByTcpPort.Args;\n import org.apache.log4j.Level;\n import org.junit.Assume;\n@@ -35,25 +34,12 @@\n     ((Log4JLogger)SshFenceByTcpPort.LOG).getLogger().setLevel(Level.ALL);\n   }\n   \n-  private static String TEST_FENCING_HOST = System.getProperty(\n+  private String TEST_FENCING_HOST = System.getProperty(\n       \"test.TestSshFenceByTcpPort.host\", \"localhost\");\n-  private static final String TEST_FENCING_PORT = System.getProperty(\n+  private String TEST_FENCING_PORT = System.getProperty(\n       \"test.TestSshFenceByTcpPort.port\", \"8020\");\n-  private static final String TEST_KEYFILE = System.getProperty(\n+  private final String TEST_KEYFILE = System.getProperty(\n       \"test.TestSshFenceByTcpPort.key\");\n-  \n-  private static final InetSocketAddress TEST_ADDR =\n-    new InetSocketAddress(TEST_FENCING_HOST,\n-      Integer.valueOf(TEST_FENCING_PORT));\n-  private static final HAServiceTarget TEST_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE, TEST_ADDR);\n-  \n-  /**\n-   *  Connect to Google's DNS server - not running ssh!\n-   */\n-  private static final HAServiceTarget UNFENCEABLE_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE,\n-        new InetSocketAddress(\"8.8.8.8\", 1234));\n \n   @Test(timeout=20000)\n   public void testFence() throws BadFencingConfigurationException {\n@@ -63,7 +49,8 @@ public void testFence() throws BadFencingConfigurationException {\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n     assertTrue(fence.tryFence(\n-        TEST_TARGET,\n+        new InetSocketAddress(TEST_FENCING_HOST,\n+                              Integer.valueOf(TEST_FENCING_PORT)),\n         null));\n   }\n \n@@ -78,7 +65,8 @@ public void testConnectTimeout() throws BadFencingConfigurationException {\n     conf.setInt(SshFenceByTcpPort.CONF_CONNECT_TIMEOUT_KEY, 3000);\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n-    assertFalse(fence.tryFence(UNFENCEABLE_TARGET, \"\"));\n+    // Connect to Google's DNS server - not running ssh!\n+    assertFalse(fence.tryFence(new InetSocketAddress(\"8.8.8.8\", 1234), \"\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "sha": "554a7abca5fdd33e027dfbb305c85e2d4a1b9ba3",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.ha.HAAdmin;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -65,9 +65,15 @@ public void setConf(Configuration conf) {\n    * Try to map the given namenode ID to its service address.\n    */\n   @Override\n-  protected HAServiceTarget resolveTarget(String nnId) {\n+  protected String getServiceAddr(String nnId) {\n     HdfsConfiguration conf = (HdfsConfiguration)getConf();\n-    return new NNHAServiceTarget(conf, nameserviceId, nnId);\n+    String serviceAddr = \n+      DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, nnId);\n+    if (serviceAddr == null) {\n+      throw new IllegalArgumentException(\n+          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n+    }\n+    return serviceAddr;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "sha": "13bde2ae53391ee734fd84e669950a2eb1355f4b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java?ref=b1f1f5d93b21d98be1e61b777994322f57e3fda8",
                "deletions": 84,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "patch": "@@ -1,84 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hdfs.tools;\n-\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.ha.BadFencingConfigurationException;\n-import org.apache.hadoop.ha.HAServiceTarget;\n-import org.apache.hadoop.ha.NodeFencer;\n-import org.apache.hadoop.hdfs.DFSUtil;\n-import org.apache.hadoop.hdfs.HdfsConfiguration;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n-import org.apache.hadoop.net.NetUtils;\n-\n-/**\n- * One of the NN NameNodes acting as the target of an administrative command\n- * (e.g. failover).\n- */\n-@InterfaceAudience.Private\n-public class NNHAServiceTarget extends HAServiceTarget {\n-\n-  private final InetSocketAddress addr;\n-  private NodeFencer fencer;\n-  private BadFencingConfigurationException fenceConfigError;\n-\n-  public NNHAServiceTarget(HdfsConfiguration conf,\n-      String nsId, String nnId) {\n-    String serviceAddr = \n-      DFSUtil.getNamenodeServiceAddr(conf, nsId, nnId);\n-    if (serviceAddr == null) {\n-      throw new IllegalArgumentException(\n-          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n-    }\n-    this.addr = NetUtils.createSocketAddr(serviceAddr,\n-        NameNode.DEFAULT_PORT);\n-    try {\n-      this.fencer = NodeFencer.create(conf);\n-    } catch (BadFencingConfigurationException e) {\n-      this.fenceConfigError = e;\n-    }\n-  }\n-\n-  /**\n-   * @return the NN's IPC address.\n-   */\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return addr;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-    if (fenceConfigError != null) {\n-      throw fenceConfigError;\n-    }\n-  }\n-  \n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-  \n-  @Override\n-  public String toString() {\n-    return \"NameNode at \" + addr;\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b1f1f5d93b21d98be1e61b777994322f57e3fda8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "sha": "9e8c239e7e8782fffff27109a1c2c2f0f8700028",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java?ref=09336893d22e678afe600dd606849df36812af7e",
                "deletions": 12,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.HAServiceStatus;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.NodeFencer;\n \n@@ -80,18 +79,10 @@ private HdfsConfiguration getHAConf() {\n   public void setup() throws IOException {\n     mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new DFSHAAdmin() {\n-\n       @Override\n-      protected HAServiceTarget resolveTarget(String nnId) {\n-        HAServiceTarget target = super.resolveTarget(nnId);\n-        HAServiceTarget spy = Mockito.spy(target);\n-        // OVerride the target to return our mock protocol\n-        try {\n-          Mockito.doReturn(mockProtocol).when(spy).getProxy();\n-        } catch (IOException e) {\n-          throw new AssertionError(e); // mock setup doesn't really throw\n-        }\n-        return spy;\n+      protected HAServiceProtocol getProtocol(String serviceId) throws IOException {\n+        getServiceAddr(serviceId);\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(getHAConf());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09336893d22e678afe600dd606849df36812af7e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "sha": "c5ba0eb7e589257272defe6615228876daebf439",
                "status": "modified"
            }
        ],
        "message": "Revert HADOOP-8193 from r1304967. Patch introduced some NPEs in a test case.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1305152 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b1f1f5d93b21d98be1e61b777994322f57e3fda8",
        "patched_files": [
            "BadFencingConfigurationException.java",
            "HAServiceTarget.java",
            "CHANGES.txt",
            "DFSHAAdmin.java",
            "ShellCommandFencer.java",
            "DummyHAService.java",
            "NNHAServiceTarget.java",
            "SshFenceByTcpPort.java",
            "FenceMethod.java",
            "FailoverController.java",
            "NodeFencer.java",
            "HAAdmin.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFailoverController.java",
            "TestNodeFencer.java",
            "TestShellCommandFencer.java",
            "TestHAAdmin.java",
            "TestSshFenceByTcpPort.java",
            "TestDFSHAAdmin.java"
        ]
    },
    "hadoop-common_098aac1": {
        "bug_id": "hadoop-common_098aac1",
        "commit": "https://github.com/apache/hadoop-common/commit/098aac1a89eae4c7b9091c22199098ed379dd212",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/098aac1a89eae4c7b9091c22199098ed379dd212/common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/CHANGES.txt?ref=098aac1a89eae4c7b9091c22199098ed379dd212",
                "deletions": 0,
                "filename": "common/CHANGES.txt",
                "patch": "@@ -246,6 +246,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    HADOOP-7327. FileSystem.listStatus() throws NullPointerException instead of\n+    IOException upon access permission failure. (mattf)\n+\n     HADOOP-7015. RawLocalFileSystem#listStatus does not deal with a directory\n     whose entries are changing (e.g. in a multi-thread or multi-process\n     environment). (Sanjay Radia via eli)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/098aac1a89eae4c7b9091c22199098ed379dd212/common/CHANGES.txt",
                "sha": "a74fa5daddc6d721f84d53e0b8e918efe0ad4df4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/098aac1a89eae4c7b9091c22199098ed379dd212/common/src/java/org/apache/hadoop/fs/FileSystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/java/org/apache/hadoop/fs/FileSystem.java?ref=098aac1a89eae4c7b9091c22199098ed379dd212",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/fs/FileSystem.java",
                "patch": "@@ -1151,6 +1151,9 @@ public boolean accept(Path file) {\n   private void listStatus(ArrayList<FileStatus> results, Path f,\n       PathFilter filter) throws FileNotFoundException, IOException {\n     FileStatus listing[] = listStatus(f);\n+    if (listing == null) {\n+      throw new IOException(\"Error accessing \" + f);\n+    }\n \n     for (int i = 0; i < listing.length; i++) {\n       if (filter.accept(listing[i].getPath())) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/098aac1a89eae4c7b9091c22199098ed379dd212/common/src/java/org/apache/hadoop/fs/FileSystem.java",
                "sha": "63954910962f851b19c9b06026dcdca22d0b7856",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop-common/blob/098aac1a89eae4c7b9091c22199098ed379dd212/common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java?ref=098aac1a89eae4c7b9091c22199098ed379dd212",
                "deletions": 4,
                "filename": "common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java",
                "patch": "@@ -25,6 +25,7 @@\n \n \n import org.apache.hadoop.fs.Options.Rename;\n+import org.apache.hadoop.fs.permission.FsPermission;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -252,8 +253,9 @@ public void testGetFileStatusThrowsExceptionForNonExistentFile()\n     }\n   } \n   \n+  @Test\n   public void testListStatusThrowsExceptionForNonExistentFile()\n-                                                    throws Exception {\n+  throws Exception {\n     try {\n       fSys.listStatus(getTestRootPath(fSys, \"test/hadoop/file\"));\n       Assert.fail(\"Should throw FileNotFoundException\");\n@@ -262,6 +264,27 @@ public void testListStatusThrowsExceptionForNonExistentFile()\n     }\n   }\n   \n+  // TODO: update after fixing HADOOP-7352\n+  @Test\n+  public void testListStatusThrowsExceptionForUnreadableDir()\n+  throws Exception {\n+    Path testRootDir = getTestRootPath(fSys, \"test/hadoop/dir\");\n+    Path obscuredDir = new Path(testRootDir, \"foo\");\n+    Path subDir = new Path(obscuredDir, \"bar\"); //so foo is non-empty\n+    fSys.mkdirs(subDir);\n+    fSys.setPermission(obscuredDir, new FsPermission((short)0)); //no access\n+    try {\n+      fSys.listStatus(obscuredDir);\n+      Assert.fail(\"Should throw IOException\");\n+    } catch (IOException ioe) {\n+      // expected\n+    } finally {\n+      // make sure the test directory can be deleted\n+      fSys.setPermission(obscuredDir, new FsPermission((short)0755)); //default\n+    }\n+  }\n+\n+\n   @Test\n   public void testListStatus() throws Exception {\n     Path[] testDirs = {\n@@ -315,6 +338,7 @@ public void testListStatusFilterWithNoMatches() throws Exception {\n     \n   }\n   \n+  @Test\n   public void testListStatusFilterWithSomeMatches() throws Exception {\n     Path[] testDirs = {\n         getTestRootPath(fSys, TEST_DIR_AAA),\n@@ -919,12 +943,13 @@ public void testRenameDirectoryToNonExistentParent() throws Exception {\n \n   @Test\n   public void testRenameDirectoryAsNonExistentDirectory() throws Exception {\n-    testRenameDirectoryAsNonExistentDirectory(Rename.NONE);\n+    doTestRenameDirectoryAsNonExistentDirectory(Rename.NONE);\n     tearDown();\n-    testRenameDirectoryAsNonExistentDirectory(Rename.OVERWRITE);\n+    doTestRenameDirectoryAsNonExistentDirectory(Rename.OVERWRITE);\n   }\n \n-  private void testRenameDirectoryAsNonExistentDirectory(Rename... options) throws Exception {\n+  private void doTestRenameDirectoryAsNonExistentDirectory(Rename... options) \n+  throws Exception {\n     if (!renameSupported()) return;\n     \n     Path src = getTestRootPath(fSys, \"test/hadoop/dir\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/098aac1a89eae4c7b9091c22199098ed379dd212/common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java",
                "sha": "6a9079c8ecef1b6fd7cdc9f6a504bb9ce3906f69",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7327. FileSystem.listStatus() throws NullPointerException instead of IOException upon access permission failure. Contributed by Matt Foley.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1143491 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/878f30fd844a211ff5fcc12d1c44d8f67f215d35",
        "patched_files": [
            "FileSystem.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "FSMainOperationsBaseTest.java",
            "TestFileSystem.java"
        ]
    },
    "hadoop-common_09feec5": {
        "bug_id": "hadoop-common_09feec5",
        "commit": "https://github.com/apache/hadoop-common/commit/09feec50847386ddfb9d1976077bd7a5d006da1c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09feec50847386ddfb9d1976077bd7a5d006da1c/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=09feec50847386ddfb9d1976077bd7a5d006da1c",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -81,6 +81,9 @@ Trunk (unreleased changes)\n     HADOOP-6603. Provide workaround for issue with Kerberos not resolving \n     cross-realm principal (Kan Zhang and Jitendra Pandey via jghoman)\n \n+    HADOOP-6620. NPE if renewer is passed as null in getDelegationToken.\n+    (Jitendra Pandey via jghoman)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09feec50847386ddfb9d1976077bd7a5d006da1c/CHANGES.txt",
                "sha": "b54723d9bcc63a66f19a9a9369792b956995e7f4",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09feec50847386ddfb9d1976077bd7a5d006da1c/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java?ref=09feec50847386ddfb9d1976077bd7a5d006da1c",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java",
                "patch": "@@ -49,8 +49,16 @@ public AbstractDelegationTokenIdentifier() {\n   }\n   \n   public AbstractDelegationTokenIdentifier(Text owner, Text renewer, Text realUser) {\n-    this.owner = owner;\n-    this.renewer = renewer;\n+    if (owner == null) {\n+      this.owner = new Text();\n+    } else {\n+      this.owner = owner;\n+    }\n+    if (renewer == null) {\n+      this.renewer = new Text();\n+    } else {\n+      this.renewer = renewer;\n+    }\n     if (realUser == null) {\n       this.realUser = new Text();\n     } else {\n@@ -170,4 +178,14 @@ public void write(DataOutput out) throws IOException {\n     WritableUtils.writeVInt(out, sequenceNumber);\n     WritableUtils.writeVInt(out, masterKeyId);\n   }\n+  \n+  public String toString() {\n+    StringBuilder buffer = new StringBuilder();\n+    buffer\n+        .append(\"owner=\" + owner + \", renewer=\" + renewer + \", realUser=\"\n+            + realUser + \", issueDate=\" + issueDate + \", maxDate=\" + maxDate\n+            + \", sequenceNumber=\" + sequenceNumber + \", masterKeyId=\"\n+            + masterKeyId);\n+    return buffer.toString();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09feec50847386ddfb9d1976077bd7a5d006da1c/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java",
                "sha": "d29ea324197a444d0bd7dbc5f58bfe1dea85035e",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09feec50847386ddfb9d1976077bd7a5d006da1c/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java?ref=09feec50847386ddfb9d1976077bd7a5d006da1c",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java",
                "patch": "@@ -178,6 +178,7 @@ private synchronized void removeExpiredKeys() {\n   \n   @Override\n   protected synchronized byte[] createPassword(TokenIdent identifier) {\n+    LOG.info(\"Creating password for identifier: \"+identifier);\n     int sequenceNum;\n     long now = System.currentTimeMillis();\n     sequenceNum = ++delegationTokenSequenceNumber;\n@@ -220,12 +221,13 @@ public synchronized long renewToken(Token<TokenIdent> token,\n     DataInputStream in = new DataInputStream(buf);\n     TokenIdent id = createIdentifier();\n     id.readFields(in);\n-\n+    LOG.info(\"Token renewal requested for identifier: \"+id);\n+    \n     if (id.getMaxDate() < now) {\n       throw new InvalidToken(\"User \" + renewer + \n                              \" tried to renew an expired token\");\n     }\n-    if (id.getRenewer() == null) {\n+    if ((id.getRenewer() == null) || (\"\".equals(id.getRenewer().toString()))) {\n       throw new AccessControlException(\"User \" + renewer + \n                                        \" tried to renew a token without \" +\n                                        \"a renewer\");\n@@ -271,13 +273,16 @@ public synchronized TokenIdent cancelToken(Token<TokenIdent> token,\n     DataInputStream in = new DataInputStream(buf);\n     TokenIdent id = createIdentifier();\n     id.readFields(in);\n+    LOG.info(\"Token cancelation requested for identifier: \"+id);\n+    \n     if (id.getUser() == null) {\n       throw new InvalidToken(\"Token with no owner\");\n     }\n     String owner = id.getUser().getUserName();\n     Text renewer = id.getRenewer();\n     if (!canceller.equals(owner)\n-        && (renewer == null || !canceller.equals(renewer.toString()))) {\n+        && (renewer == null || \"\".equals(renewer.toString()) || !canceller\n+            .equals(renewer.toString()))) {\n       throw new AccessControlException(canceller\n           + \" is not authorized to cancel the token\");\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09feec50847386ddfb9d1976077bd7a5d006da1c/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java",
                "sha": "555dcff91126e91782c24b1d2eea0dc76624707e",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/09feec50847386ddfb9d1976077bd7a5d006da1c/src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java?ref=09feec50847386ddfb9d1976077bd7a5d006da1c",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "patch": "@@ -365,4 +365,24 @@ public void run() {\n       dtSecretManager.stopThreads();\n     }\n   }\n+  \n+  @Test \n+  public void testDelegationTokenNullRenewer() throws Exception {\n+    TestDelegationTokenSecretManager dtSecretManager = \n+      new TestDelegationTokenSecretManager(24*60*60*1000,\n+        10*1000,1*1000,3600000);\n+    dtSecretManager.startThreads();\n+    TestDelegationTokenIdentifier dtId = new TestDelegationTokenIdentifier(new Text(\n+        \"theuser\"), null, null);\n+    Token<TestDelegationTokenIdentifier> token = new Token<TestDelegationTokenIdentifier>(\n+        dtId, dtSecretManager);\n+    Assert.assertTrue(token != null);\n+    try {\n+      dtSecretManager.renewToken(token, \"\");\n+      Assert.fail(\"Renewal must not succeed\");\n+    } catch (IOException e) {\n+      //PASS\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/09feec50847386ddfb9d1976077bd7a5d006da1c/src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "sha": "22cfb3d84c24597849f6a45866ec0bb4165652ba",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6620. NPE if renewer is passed as null in getDelegationToken. Contributed by Jitendra Pandey.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@953896 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9e4ea2ffe6a2302527d40b180721c8c11836e589",
        "patched_files": [
            "AbstractDelegationTokenSecretManager.java",
            "AbstractDelegationTokenIdentifier.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDelegationToken.java"
        ]
    },
    "hadoop-common_0a0dc3f": {
        "bug_id": "hadoop-common_0a0dc3f",
        "commit": "https://github.com/apache/hadoop-common/commit/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -289,6 +289,9 @@ Trunk (unreleased changes)\n     HADOOP-6991.  Fix SequenceFile::Reader to honor file lengths and call\n     openFile (cdouglas via omalley)\n \n+    HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.\n+    (Aaron T. Myers via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/CHANGES.txt",
                "sha": "e607a0c1062c4eef025df0df61256d286a37a9af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/src/java/org/apache/hadoop/security/KerberosName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/KerberosName.java?ref=0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/security/KerberosName.java",
                "patch": "@@ -399,9 +399,10 @@ static void printRules() throws IOException {\n   }\n \n   public static void main(String[] args) throws Exception {\n+    setConfiguration(new Configuration());\n     for(String arg: args) {\n       KerberosName name = new KerberosName(arg);\n       System.out.println(\"Name: \" + name + \" to \" + name.getShortName());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/src/java/org/apache/hadoop/security/KerberosName.java",
                "sha": "b533cd22f77d6dbf0aaa0a42edf6338dc6d987c6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.  Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028938 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/22f7d55eb833d49fa0cba03b5275d4462d5b2c97",
        "patched_files": [
            "KerberosName.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKerberosName.java"
        ]
    },
    "hadoop-common_0edd4fd": {
        "bug_id": "hadoop-common_0edd4fd",
        "commit": "https://github.com/apache/hadoop-common/commit/0edd4fd75717ad2e5846012e164249eb1cc07178",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0edd4fd75717ad2e5846012e164249eb1cc07178/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=0edd4fd75717ad2e5846012e164249eb1cc07178",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -347,6 +347,9 @@ Release 2.4.1 - UNRELEASED\n     HDFS-6198. DataNode rolling upgrade does not correctly identify current\n     block pool directory and replace with trash on Windows. (cnauroth)\n \n+    HDFS-6206. Fix NullPointerException in DFSUtil.substituteForWildcardAddress.\n+    (szetszwo) \n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0edd4fd75717ad2e5846012e164249eb1cc07178/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3d2675c1e915d6ef8280c62b184e1929d97b476c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0edd4fd75717ad2e5846012e164249eb1cc07178/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=0edd4fd75717ad2e5846012e164249eb1cc07178",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -37,6 +37,7 @@\n import java.io.IOException;\n import java.io.PrintStream;\n import java.io.UnsupportedEncodingException;\n+import java.net.InetAddress;\n import java.net.InetSocketAddress;\n import java.net.URI;\n import java.net.URISyntaxException;\n@@ -1100,7 +1101,8 @@ static String substituteForWildcardAddress(String configuredAddress,\n     InetSocketAddress sockAddr = NetUtils.createSocketAddr(configuredAddress);\n     InetSocketAddress defaultSockAddr = NetUtils.createSocketAddr(defaultHost\n         + \":0\");\n-    if (sockAddr.getAddress().isAnyLocalAddress()) {\n+    final InetAddress addr = sockAddr.getAddress();\n+    if (addr != null && addr.isAnyLocalAddress()) {\n       if (UserGroupInformation.isSecurityEnabled() &&\n           defaultSockAddr.getAddress().isAnyLocalAddress()) {\n         throw new IOException(\"Cannot use a wildcard address with security. \" +",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0edd4fd75717ad2e5846012e164249eb1cc07178/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "347237bea6edddf3f9e8812f94fc7fb505a22d57",
                "status": "modified"
            }
        ],
        "message": "HDFS-6206. Fix NullPointerException in DFSUtil.substituteForWildcardAddress.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1586034 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0ffacefaf99443054e7ed1a1873c714dc742a35e",
        "patched_files": [
            "DFSUtil.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop-common_0f85e82": {
        "bug_id": "hadoop-common_0f85e82",
        "commit": "https://github.com/apache/hadoop-common/commit/0f85e824d9f26adff5b48bc78de88f63db5d3de5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0f85e824d9f26adff5b48bc78de88f63db5d3de5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=0f85e824d9f26adff5b48bc78de88f63db5d3de5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -391,6 +391,8 @@ Release 2.0.5-beta - UNRELEASED\n     HDFS-4571. WebHDFS should not set the service hostname on the server side. \n     (tucu)\n \n+    HDFS-4013. TestHftpURLTimeouts throws NPE. (Chao Shi via suresh)\n+\n Release 2.0.4-alpha - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0f85e824d9f26adff5b48bc78de88f63db5d3de5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "f306aae88ac38abb9dadc6d3a2fdf0d498e55414",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0f85e824d9f26adff5b48bc78de88f63db5d3de5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java?ref=0f85e824d9f26adff5b48bc78de88f63db5d3de5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hdfs;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n import java.io.IOException;\n@@ -39,7 +40,7 @@\n public class TestHftpURLTimeouts {\n   @BeforeClass\n   public static void setup() {\n-    URLUtils.SOCKET_TIMEOUT = 1;\n+    URLUtils.SOCKET_TIMEOUT = 5;\n   }\n   \n   @Test\n@@ -116,6 +117,7 @@ private boolean checkConnectTimeout(HftpFileSystem fs, boolean ignoreReadTimeout\n           conns.add(fs.openConnection(\"/\", \"\"));\n         } catch (SocketTimeoutException ste) {\n           String message = ste.getMessage();\n+          assertNotNull(message);\n           // https will get a read timeout due to SSL negotiation, but\n           // a normal http will not, so need to ignore SSL read timeouts\n           // until a connect timeout occurs",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0f85e824d9f26adff5b48bc78de88f63db5d3de5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java",
                "sha": "d9a22c10111e557bcd8bc5200256b93c2899fdea",
                "status": "modified"
            }
        ],
        "message": "HDFS-4013. TestHftpURLTimeouts throws NPE. Contributed by Chao Shi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1455755 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/03307a90e48d6511d8da73085fc5161e77cce911",
        "patched_files": [
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHftpURLTimeouts.java"
        ]
    },
    "hadoop-common_0fd4ce1": {
        "bug_id": "hadoop-common_0fd4ce1",
        "commit": "https://github.com/apache/hadoop-common/commit/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -435,6 +435,9 @@ Release 2.1.2 - UNRELEASED\n     HADOOP-10003. HarFileSystem.listLocatedStatus() fails.\n     (Jason Dere and suresh via suresh)\n \n+    HADOOP-10017. Fix NPE in DFSClient#getDelegationToken when doing Distcp \n+    from a secured cluster to an insecured cluster. (Haohui Mai via jing9)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "12911a3cdbc528e0535921ded9b761ad2c277e33",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -900,10 +900,15 @@ public String getCanonicalServiceName() {\n     assert dtService != null;\n     Token<DelegationTokenIdentifier> token =\n       namenode.getDelegationToken(renewer);\n-    token.setService(this.dtService);\n \n-    LOG.info(\"Created \" + DelegationTokenIdentifier.stringifyToken(token));\n+    if (token != null) {\n+      token.setService(this.dtService);\n+      LOG.info(\"Created \" + DelegationTokenIdentifier.stringifyToken(token));\n+    } else {\n+      LOG.info(\"Cannot get delegation token from \" + renewer);\n+    }\n     return token;\n+\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "1e1b9861dbeef0a68a7465450f7065f99d41df77",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -94,6 +94,19 @@ private HdfsConfiguration getTestConfiguration() {\n     return conf;\n   }\n \n+  @Test\n+  public void testEmptyDelegationToken() throws IOException {\n+    Configuration conf = getTestConfiguration();\n+    MiniDFSCluster cluster = null;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n+      FileSystem fileSys = cluster.getFileSystem();\n+      fileSys.getDelegationToken(\"\");\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n+\n   @Test\n   public void testFileSystemCloseAll() throws Exception {\n     Configuration conf = getTestConfiguration();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0fd4ce1aa1d5765bf37aa50c06185bbade1ac8f8/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "0a97a62001458290acdbe4c8786f9c5dabfceb92",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10017. Fix NPE in DFSClient#getDelegationToken when doing Distcp from a secured cluster to an insecured cluster. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529571 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/2a8181d1206dc2e65de2ff582cc7c75cfbac7989",
        "patched_files": [
            "DistributedFileSystem.java",
            "CHANGES.txt",
            "DFSClient.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop-common_1073a1c": {
        "bug_id": "hadoop-common_1073a1c",
        "commit": "https://github.com/apache/hadoop-common/commit/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7342. Add an utility API in FileUtil for JDK File.list\n+    avoid NPEs on File.list() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7322. Adding a util method in FileUtil for directory listing,\n     avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/CHANGES.txt",
                "sha": "16152a6c9f688f892ae747b874e7f6f7105ac83b",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -728,4 +728,23 @@ public static void replaceFile(File src, File target) throws IOException {\n     }\n     return files;\n   }  \n+  \n+  /**\n+   * A wrapper for {@link File#list()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#list() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of file names or empty string list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static String[] list(File dir) throws IOException {\n+    String[] fileNames = dir.list();\n+    if(fileNames == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return fileNames;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "86956e368c079cfa9e221c8317550404fe53a29e",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=1073a1c10b125805ab301e73d1a4c0a1e1d4e34e",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -143,6 +143,33 @@ public void testListFiles() throws IOException {\n     \t//Expected an IOException\n     }\n   }\n+\n+  @Test\n+  public void testListAPI() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    String[] files = FileUtil.list(partitioned);\n+    Assert.assertEquals(\"Unexpected number of pre-existing files\", 2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.list(newDir);\n+    Assert.assertEquals(\"New directory unexpectedly contains files\", 0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.list(newDir);\n+      Assert.fail(\"IOException expected on list() for non-existent dir \"\n+          + newDir.toString());\n+    } catch(IOException ioe) {\n+      //Expected an IOException\n+    }\n+  }\n   \n   @After\n   public void tearDown() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1073a1c10b125805ab301e73d1a4c0a1e1d4e34e/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "65c43435b75b9fb79d8be4e247a5f8175da3bf22",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7342. Add an utility API in FileUtil for JDK File.list avoid NPEs on File.list().  Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1131330 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f14ab45505c3eedc78f9671c9e43a221d28dba8d",
        "patched_files": [
            "FileUtil.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop-common_10aa608": {
        "bug_id": "hadoop-common_10aa608",
        "commit": "https://github.com/apache/hadoop-common/commit/10aa6081cb5ac21b390b313fbb973ce978507f52",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/10aa6081cb5ac21b390b313fbb973ce978507f52/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=10aa6081cb5ac21b390b313fbb973ce978507f52",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -89,6 +89,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-3664. Federation Documentation has incorrect configuration example.\n     (Brandon Li via jitendra)\n \n+    MAPREDUCE-1740. NPE in getMatchingLevelForNodes when node locations are \n+    variable depth (ahmed via tucu) [IMPORTANT: this is dead code in trunk]\n+\n Release 0.23.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/10aa6081cb5ac21b390b313fbb973ce978507f52/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d5993698def544f85b305cd8afee5516bf4389a4",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/10aa6081cb5ac21b390b313fbb973ce978507f52/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java?ref=10aa6081cb5ac21b390b313fbb973ce978507f52",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java",
                "patch": "@@ -1596,16 +1596,37 @@ public synchronized Task obtainNewReduceTask(TaskTrackerStatus tts,\n   \n   // returns the (cache)level at which the nodes matches\n   private int getMatchingLevelForNodes(Node n1, Node n2) {\n+    return getMatchingLevelForNodes(n1, n2, this.maxLevel);\n+  }\n+\n+  static int getMatchingLevelForNodes(Node n1, Node n2, int maxLevel) {\n     int count = 0;\n+\n+    // In the case that the two nodes are at different levels in the\n+    // node heirarchy, walk upwards on the deeper one until the\n+    // levels are equal. Each of these counts as \"distance\" since it\n+    // assumedly is going through another rack.\n+    int level1=n1.getLevel(), level2=n2.getLevel();\n+    while(n1!=null && level1>level2) {\n+      n1 = n1.getParent();\n+      level1--;\n+      count++;\n+    }\n+    while(n2!=null && level2>level1) {\n+      n2 = n2.getParent();\n+      level2--;\n+      count++;\n+    }\n+\n     do {\n-      if (n1.equals(n2)) {\n-        return count;\n+      if (n1.equals(n2) || count >= maxLevel) {\n+        return Math.min(count, maxLevel);\n       }\n       ++count;\n       n1 = n1.getParent();\n       n2 = n2.getParent();\n     } while (n1 != null);\n-    return this.maxLevel;\n+    return maxLevel;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/10aa6081cb5ac21b390b313fbb973ce978507f52/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java",
                "sha": "9f92707077b56ee905db372e676d2e8b71678827",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hadoop-common/blob/10aa6081cb5ac21b390b313fbb973ce978507f52/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java?ref=10aa6081cb5ac21b390b313fbb973ce978507f52",
                "deletions": 24,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java",
                "patch": "@@ -29,10 +29,9 @@\n import java.util.Map;\n import java.util.Set;\n \n-import junit.extensions.TestSetup;\n-import junit.framework.Test;\n-import junit.framework.TestCase;\n-import junit.framework.TestSuite;\n+import static org.junit.Assert.*;\n+import org.junit.Test;\n+import org.junit.BeforeClass;\n import static org.mockito.Mockito.*;\n \n import org.apache.commons.logging.Log;\n@@ -47,11 +46,13 @@\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.mapreduce.split.JobSplit.TaskSplitMetaInfo;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n+import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.net.StaticMapping;\n \n @SuppressWarnings(\"deprecation\")\n-public class TestJobInProgress extends TestCase {\n+public class TestJobInProgress {\n   static final Log LOG = LogFactory.getLog(TestJobInProgress.class);\n \n   static FakeJobTracker jobTracker;\n@@ -75,25 +76,21 @@\n   static int numUniqueHosts = hosts.length;\n   static int clusterSize = trackers.length;\n \n-  public static Test suite() {\n-    TestSetup setup = new TestSetup(new TestSuite(TestJobInProgress.class)) {\n-      protected void setUp() throws Exception {\n-        JobConf conf = new JobConf();\n-        conf.set(JTConfig.JT_IPC_ADDRESS, \"localhost:0\");\n-        conf.set(JTConfig.JT_HTTP_ADDRESS, \"0.0.0.0:0\");\n-        conf.setClass(\"topology.node.switch.mapping.impl\", \n-            StaticMapping.class, DNSToSwitchMapping.class);\n-        jobTracker = new FakeJobTracker(conf, new FakeClock(), trackers);\n-        // Set up the Topology Information\n-        for (int i = 0; i < hosts.length; i++) {\n-          StaticMapping.addNodeToRack(hosts[i], racks[i]);\n-        }\n-        for (String s: trackers) {\n-          FakeObjectUtilities.establishFirstContact(jobTracker, s);\n-        }\n-      }\n-    };\n-    return setup;\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    JobConf conf = new JobConf();\n+    conf.set(JTConfig.JT_IPC_ADDRESS, \"localhost:0\");\n+    conf.set(JTConfig.JT_HTTP_ADDRESS, \"0.0.0.0:0\");\n+    conf.setClass(\"topology.node.switch.mapping.impl\",\n+                  StaticMapping.class, DNSToSwitchMapping.class);\n+    jobTracker = new FakeJobTracker(conf, new FakeClock(), trackers);\n+    // Set up the Topology Information\n+    for (int i = 0; i < hosts.length; i++) {\n+      StaticMapping.addNodeToRack(hosts[i], racks[i]);\n+    }\n+    for (String s: trackers) {\n+      FakeObjectUtilities.establishFirstContact(jobTracker, s);\n+    }\n   }\n \n   static class MyFakeJobInProgress extends FakeJobInProgress {\n@@ -157,6 +154,7 @@ public TaskAttemptID findAndRunNewTask(boolean isMap,\n     }\n   }\n \n+  //@Test\n   public void testPendingMapTaskCount() throws Exception {\n \n     int numMaps = 4;\n@@ -259,6 +257,7 @@ static void testRunningTaskCount(boolean speculation)  throws Exception {\n \n   }\n \n+  //@Test\n   public void testRunningTaskCount() throws Exception {\n     // test with spec = false \n     testRunningTaskCount(false);\n@@ -287,6 +286,7 @@ static void checkTaskCounts(JobInProgress jip, int runningMaps,\n     assertEquals(pendingReduces, jip.pendingReduces());\n   }\n \n+  //@Test\n   public void testJobSummary() throws Exception {\n     int numMaps = 2;\n     int numReds = 2;\n@@ -341,4 +341,35 @@ public void testJobSummary() throws Exception {\n     assertEquals(\"firstReduceTaskLaunchTime\", 3,\n         jspy.getFirstTaskLaunchTimes().get(TaskType.REDUCE).longValue());\n   }\n+\n+  @Test\n+  public void testLocality() throws Exception {\n+    NetworkTopology nt = new NetworkTopology();\n+\n+    Node r1n1 = new NodeBase(\"/default/rack1/node1\");\n+    nt.add(r1n1);\n+    Node r1n2 = new NodeBase(\"/default/rack1/node2\");\n+    nt.add(r1n2);\n+\n+    Node r2n3 = new NodeBase(\"/default/rack2/node3\");\n+    nt.add(r2n3);\n+\n+    Node r2n4 = new NodeBase(\"/default/rack2/s1/node4\");\n+    nt.add(r2n4);\n+\n+    LOG.debug(\"r1n1 parent: \" + r1n1.getParent() + \"\\n\" +\n+              \"r1n2 parent: \" + r1n2.getParent() + \"\\n\" +\n+              \"r2n3 parent: \" + r2n3.getParent() + \"\\n\" +\n+              \"r2n4 parent: \" + r2n4.getParent());\n+\n+    // Same host\n+    assertEquals(0, JobInProgress.getMatchingLevelForNodes(r1n1, r1n1, 3));\n+    // Same rack\n+    assertEquals(1, JobInProgress.getMatchingLevelForNodes(r1n1, r1n2, 3));\n+    // Different rack\n+    assertEquals(2, JobInProgress.getMatchingLevelForNodes(r1n1, r2n3, 3));\n+    // Different rack at different depth\n+    assertEquals(3, JobInProgress.getMatchingLevelForNodes(r1n1, r2n4, 3));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/10aa6081cb5ac21b390b313fbb973ce978507f52/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java",
                "sha": "ea100aab08f5e0308763313332365114c0464c68",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1740. NPE in getMatchingLevelForNodes when node locations are variable depth (ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303076 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/aa5d4b0a901c60c0232f681a74bed89f706184c2",
        "patched_files": [
            "JobInProgress.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobInProgress.java"
        ]
    },
    "hadoop-common_11131d7": {
        "bug_id": "hadoop-common_11131d7",
        "commit": "https://github.com/apache/hadoop-common/commit/11131d72040c250f897a8d1c130b83ce1b065050",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -2198,3 +2198,5 @@ Release 0.21.0 - 2010-08-13\n \n     MAPREDUCE-1856. Extract a subset of tests for smoke (DOA) validation (cos)\n \n+    MAPREDUCE-2317. Fix a NPE in HadoopArchives.  (Devaraj K via szetszwo)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "sha": "9f26015d60ab79f23ebb727a7c2cdc543fbbf831",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/tools/org/apache/hadoop/tools/HadoopArchives.java?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "deletions": 9,
                "filename": "src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "patch": "@@ -367,16 +367,18 @@ private void writeTopLevelDirs(SequenceFile.Writer srcWriter,\n         }\n         else {\n           Path parent = p.getParent();\n-          if (allpaths.containsKey(parent.toString())) {\n-            HashSet<String> children = allpaths.get(parent.toString());\n-            children.add(p.getName());\n-          }\n-          else {\n-            HashSet<String> children = new HashSet<String>();\n-            children.add(p.getName());\n-            allpaths.put(parent.toString(), children);\n+          if (null != parent) {\n+            if (allpaths.containsKey(parent.toString())) {\n+              HashSet<String> children = allpaths.get(parent.toString());\n+              children.add(p.getName());\n+            } \n+            else {\n+              HashSet<String> children = new HashSet<String>();\n+              children.add(p.getName());\n+              allpaths.put(parent.toString(), children);\n+            }\n+            parents.add(parent);\n           }\n-          parents.add(parent);\n         }\n       }\n       justDirs = parents;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "sha": "41d149ea952f98e877d56de2deb3c5ba482ae120",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2317. Fix a NPE in HadoopArchives.  Contributed by Devaraj K\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1096022 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ac3445f84c3699f5032374587c3c80087b598e0f",
        "patched_files": [
            "HadoopArchives.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHadoopArchives.java"
        ]
    },
    "hadoop-common_12bf844": {
        "bug_id": "hadoop-common_12bf844",
        "commit": "https://github.com/apache/hadoop-common/commit/12bf8447ccb4be21e9634dde97407c45edcdbb28",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/12bf8447ccb4be21e9634dde97407c45edcdbb28/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=12bf8447ccb4be21e9634dde97407c45edcdbb28",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -155,6 +155,9 @@ Release 2.0.3-alpha - Unreleased\n     YARN-283. Fair scheduler fails to get queue info without root prefix. \n     (sandyr via tucu)\n \n+    YARN-192. Node update causes NPE in the fair scheduler.\n+    (Sandy Ryza via tomwhite)\n+\n Release 2.0.2-alpha - 2012-09-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/12bf8447ccb4be21e9634dde97407c45edcdbb28/hadoop-yarn-project/CHANGES.txt",
                "sha": "baa0d4ef5e43a20fb4c53bda061dc604968b072a",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/12bf8447ccb4be21e9634dde97407c45edcdbb28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java?ref=12bf8447ccb4be21e9634dde97407c45edcdbb28",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java",
                "patch": "@@ -18,6 +18,9 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;\n \n+import java.util.Arrays;\n+import java.util.Collection;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience.Private;\n@@ -293,12 +296,16 @@ public Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     } else {\n       // If this app is over quota, don't schedule anything\n       if (!(getRunnable())) { return Resources.none(); }\n-\n     }\n+\n+    Collection<Priority> prioritiesToTry = (reserved) ? \n+        Arrays.asList(node.getReservedContainer().getReservedPriority()) : \n+        app.getPriorities();\n+    \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n-    for (Priority priority : app.getPriorities()) {\n+    for (Priority priority : prioritiesToTry) {\n       app.addSchedulingOpportunity(priority);\n       NodeType allowedLocality = app.getAllowedLocalityLevel(priority,\n           scheduler.getNumClusterNodes(), scheduler.getNodeLocalityThreshold(),",
                "raw_url": "https://github.com/apache/hadoop-common/raw/12bf8447ccb4be21e9634dde97407c45edcdbb28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java",
                "sha": "d26e7cad6fb6ff7bd48fe6a77e795ec2e12941b3",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/hadoop-common/blob/12bf8447ccb4be21e9634dde97407c45edcdbb28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=12bf8447ccb4be21e9634dde97407c45edcdbb28",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.io.IOException;\n import java.io.PrintWriter;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collection;\n import java.util.LinkedList;\n import java.util.List;\n@@ -53,6 +54,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent;\n@@ -1187,4 +1189,58 @@ public void testUserMaxRunningApps() throws Exception {\n     // Request should be fulfilled\n     assertEquals(2, scheduler.applications.get(attId1).getLiveContainers().size());\n   }\n+  \n+  @Test\n+  public void testReservationWhileMultiplePriorities() {\n+    // Add a node\n+    RMNode node1 = MockNodes.newNodeInfo(1, Resources.createResource(1024));\n+    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n+    scheduler.handle(nodeEvent1);\n+\n+    ApplicationAttemptId attId = createSchedulingRequest(1024, \"queue1\",\n+        \"user1\", 1, 2);\n+    scheduler.update();\n+    NodeUpdateSchedulerEvent updateEvent = new NodeUpdateSchedulerEvent(node1,\n+      new ArrayList<ContainerStatus>(), new ArrayList<ContainerStatus>());\n+    scheduler.handle(updateEvent);\n+    \n+    FSSchedulerApp app = scheduler.applications.get(attId);\n+    assertEquals(1, app.getLiveContainers().size());\n+    \n+    ContainerId containerId = scheduler.applications.get(attId)\n+        .getLiveContainers().iterator().next().getContainerId();\n+\n+    // Cause reservation to be created\n+    createSchedulingRequestExistingApplication(1024, 2, attId);\n+    scheduler.update();\n+    scheduler.handle(updateEvent);\n+\n+    assertEquals(1, app.getLiveContainers().size());\n+    \n+    // Create request at higher priority\n+    createSchedulingRequestExistingApplication(1024, 1, attId);\n+    scheduler.update();\n+    scheduler.handle(updateEvent);\n+    \n+    assertEquals(1, app.getLiveContainers().size());\n+    // Reserved container should still be at lower priority\n+    for (RMContainer container : app.getReservedContainers()) {\n+      assertEquals(2, container.getReservedPriority().getPriority());\n+    }\n+    \n+    // Complete container\n+    scheduler.allocate(attId, new ArrayList<ResourceRequest>(),\n+        Arrays.asList(containerId));\n+    \n+    // Schedule at opening\n+    scheduler.update();\n+    scheduler.handle(updateEvent);\n+    \n+    // Reserved container (at lower priority) should be run\n+    Collection<RMContainer> liveContainers = app.getLiveContainers();\n+    assertEquals(1, liveContainers.size());\n+    for (RMContainer liveContainer : liveContainers) {\n+      Assert.assertEquals(2, liveContainer.getContainer().getPriority().getPriority());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/12bf8447ccb4be21e9634dde97407c45edcdbb28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "47a35378e595bb714b6d572c7728ff08f0551ab6",
                "status": "modified"
            }
        ],
        "message": "YARN-192. Node update causes NPE in the fair scheduler. Contributed by Sandy Ryza\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1428314 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d1b1a2986deb59981c327115736dd0044759d2d6",
        "patched_files": [
            "AppSchedulable.java",
            "FairScheduler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_1615886": {
        "bug_id": "hadoop-common_1615886",
        "commit": "https://github.com/apache/hadoop-common/commit/1615886bbd9ece187b7754d1a138fb7074a955c8",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1615886bbd9ece187b7754d1a138fb7074a955c8/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1615886bbd9ece187b7754d1a138fb7074a955c8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -854,3 +854,7 @@ Release 0.21.0 - Unreleased\n \n     MAPREDUCE-1163. Remove unused, hard-coded paths from libhdfs. (Allen\n     Wittenauer via cdouglas)\n+\n+    MAPREDUCE-962. Fix a NullPointerException while killing task process \n+    trees. (Ravi Gummadi via yhemanth)\n+    ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1615886bbd9ece187b7754d1a138fb7074a955c8/CHANGES.txt",
                "sha": "1c473e8de64a12b4d3cced2670d4bb939610b1be",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1615886bbd9ece187b7754d1a138fb7074a955c8/src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java?ref=1615886bbd9ece187b7754d1a138fb7074a955c8",
                "deletions": 12,
                "filename": "src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java",
                "patch": "@@ -228,12 +228,19 @@ public boolean isAnyProcessInTreeAlive() {\n \n   /** Verify that the given process id is same as its process group id.\n    * @param pidStr Process id of the to-be-verified-process\n+   * @param procfsDir  Procfs root dir\n    */\n-  private static boolean assertPidPgrpidForMatch(String pidStr) {\n+  static boolean checkPidPgrpidForMatch(String pidStr, String procfsDir) {\n     Integer pId = Integer.parseInt(pidStr);\n     // Get information for this process\n     ProcessInfo pInfo = new ProcessInfo(pId);\n-    pInfo = constructProcessInfo(pInfo);\n+    pInfo = constructProcessInfo(pInfo, procfsDir);\n+    if (pInfo == null) {\n+      // process group leader may have finished execution, but we still need to\n+      // kill the subProcesses in the process group.\n+      return true;\n+    }\n+\n     //make sure that pId and its pgrpId match\n     if (!pInfo.getPgrpId().equals(pId)) {\n       LOG.warn(\"Unexpected: Process with PID \" + pId +\n@@ -258,7 +265,7 @@ public static void assertAndDestroyProcessGroup(String pgrpId, long interval,\n                        boolean inBackground)\n          throws IOException {\n     // Make sure that the pid given is a process group leader\n-    if (!assertPidPgrpidForMatch(pgrpId)) {\n+    if (!checkPidPgrpidForMatch(pgrpId, PROCFS)) {\n       throw new IOException(\"Process with PID \" + pgrpId  +\n                           \" is not a process group leader.\");\n     }\n@@ -390,15 +397,6 @@ private static Integer getValidPID(String pid) {\n     return processList;\n   }\n \n-  /**\n-   * \n-   * Construct the ProcessInfo using the process' PID and procfs and return the\n-   * same. Returns null on failing to read from procfs,\n-   */\n-  private static ProcessInfo constructProcessInfo(ProcessInfo pinfo) {\n-    return constructProcessInfo(pinfo, PROCFS);\n-  }\n-\n   /**\n    * Construct the ProcessInfo using the process' PID and procfs rooted at the\n    * specified directory and return the same. It is provided mainly to assist\n@@ -422,6 +420,8 @@ private static ProcessInfo constructProcessInfo(ProcessInfo pinfo,\n       in = new BufferedReader(fReader);\n     } catch (FileNotFoundException f) {\n       // The process vanished in the interim!\n+      LOG.warn(\"The process \" + pinfo.getPid()\n+          + \" may have finished in the interim.\");\n       return ret;\n     }\n \n@@ -436,6 +436,11 @@ private static ProcessInfo constructProcessInfo(ProcessInfo pinfo,\n             .parseInt(m.group(4)), Integer.parseInt(m.group(5)), Long\n             .parseLong(m.group(7)));\n       }\n+      else {\n+        LOG.warn(\"Unexpected: procfs stat file is not in the expected format\"\n+            + \" for process with pid \" + pinfo.getPid());\n+        ret = null;\n+      }\n     } catch (IOException io) {\n       LOG.warn(\"Error reading the stream \" + io);\n       ret = null;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1615886bbd9ece187b7754d1a138fb7074a955c8/src/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java",
                "sha": "c3ced7964bd06e643a49a25279f4e217d1c3965d",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1615886bbd9ece187b7754d1a138fb7074a955c8/src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java?ref=1615886bbd9ece187b7754d1a138fb7074a955c8",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java",
                "patch": "@@ -422,6 +422,34 @@ public void testVMemForOlderProcesses() throws IOException {\n     }\n   }\n \n+  /**\n+   * Verifies ProcfsBasedProcessTree.checkPidPgrpidForMatch() in case of\n+   * 'constructProcessInfo() returning null' by not writing stat file for the\n+   * mock process\n+   * @throws IOException if there was a problem setting up the\n+   *                      fake procfs directories or files.\n+   */\n+  public void testDestroyProcessTree() throws IOException {\n+    // test process\n+    String pid = \"100\";\n+    // create the fake procfs root directory. \n+    File procfsRootDir = new File(TEST_ROOT_DIR, \"proc\");\n+\n+    try {\n+      setupProcfsRootDir(procfsRootDir);\n+      \n+      // crank up the process tree class.\n+      ProcfsBasedProcessTree processTree = new ProcfsBasedProcessTree(\n+                        pid, true, 100L, procfsRootDir.getAbsolutePath());\n+\n+      // Let us not create stat file for pid 100.\n+      assertTrue(ProcfsBasedProcessTree.checkPidPgrpidForMatch(\n+                            pid, procfsRootDir.getAbsolutePath()));\n+    } finally {\n+      FileUtil.fullyDelete(procfsRootDir);\n+    }\n+  }\n+  \n   /**\n    * Test the correctness of process-tree dump.\n    * ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1615886bbd9ece187b7754d1a138fb7074a955c8/src/test/mapred/org/apache/hadoop/mapreduce/util/TestProcfsBasedProcessTree.java",
                "sha": "cc785062e3c6ffd1baa3537f64d4fda3ec0580f1",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-962. Fix a NullPointerException while killing task process trees. Contributed by Ravi Gummadi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@833002 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/80540f15210c86c549e0ffe9854ce152806d499f",
        "patched_files": [
            "ProcfsBasedProcessTree.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestProcfsBasedProcessTree.java"
        ]
    },
    "hadoop-common_17a47c1": {
        "bug_id": "hadoop-common_17a47c1",
        "commit": "https://github.com/apache/hadoop-common/commit/17a47c189460dc5183582ceb370b3f3f226fb1ae",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=17a47c189460dc5183582ceb370b3f3f226fb1ae",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -88,6 +88,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2321. NodeManager web UI can incorrectly report Pmem enforcement\n     (Leitao Guo via jlowe)\n \n+    YARN-2273. NPE in ContinuousScheduling thread when we lose a node. \n+    (Wei Yan via kasha)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/CHANGES.txt",
                "sha": "2e69a756c6415598c7f26f304b969407987ee68e",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hadoop-common/blob/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=17a47c189460dc5183582ceb370b3f3f226fb1ae",
                "deletions": 30,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -970,37 +970,27 @@ private synchronized void nodeUpdate(RMNode nm) {\n     }\n   }\n \n-  private void continuousScheduling() {\n-    while (true) {\n-      List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n-      // Sort the nodes by space available on them, so that we offer\n-      // containers on emptier nodes first, facilitating an even spread. This\n-      // requires holding the scheduler lock, so that the space available on a\n-      // node doesn't change during the sort.\n-      synchronized (this) {\n-        Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n-      }\n+  void continuousSchedulingAttempt() {\n+    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n+    // Sort the nodes by space available on them, so that we offer\n+    // containers on emptier nodes first, facilitating an even spread. This\n+    // requires holding the scheduler lock, so that the space available on a\n+    // node doesn't change during the sort.\n+    synchronized (this) {\n+      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n+    }\n \n-      // iterate all nodes\n-      for (NodeId nodeId : nodeIdList) {\n-        if (nodes.containsKey(nodeId)) {\n-          FSSchedulerNode node = getFSSchedulerNode(nodeId);\n-          try {\n-            if (Resources.fitsIn(minimumAllocation,\n-                    node.getAvailableResource())) {\n-              attemptScheduling(node);\n-            }\n-          } catch (Throwable ex) {\n-            LOG.warn(\"Error while attempting scheduling for node \" + node +\n-                    \": \" + ex.toString(), ex);\n-          }\n-        }\n-      }\n+    // iterate all nodes\n+    for (NodeId nodeId : nodeIdList) {\n+      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n       try {\n-        Thread.sleep(getContinuousSchedulingSleepMs());\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"Error while doing sleep in continuous scheduling: \" +\n-                e.toString(), e);\n+        if (node != null && Resources.fitsIn(minimumAllocation,\n+            node.getAvailableResource())) {\n+          attemptScheduling(node);\n+        }\n+      } catch (Throwable ex) {\n+        LOG.error(\"Error while attempting scheduling for node \" + node +\n+            \": \" + ex.toString(), ex);\n       }\n     }\n   }\n@@ -1010,6 +1000,12 @@ private void continuousScheduling() {\n \n     @Override\n     public int compare(NodeId n1, NodeId n2) {\n+      if (!nodes.containsKey(n1)) {\n+        return 1;\n+      }\n+      if (!nodes.containsKey(n2)) {\n+        return -1;\n+      }\n       return RESOURCE_CALCULATOR.compare(clusterResource,\n               nodes.get(n2).getAvailableResource(),\n               nodes.get(n1).getAvailableResource());\n@@ -1234,7 +1230,16 @@ private synchronized void initScheduler(Configuration conf)\n           new Runnable() {\n             @Override\n             public void run() {\n-              continuousScheduling();\n+              while (!Thread.currentThread().isInterrupted()) {\n+                try {\n+                  continuousSchedulingAttempt();\n+                  Thread.sleep(getContinuousSchedulingSleepMs());\n+                } catch (InterruptedException e) {\n+                  LOG.error(\"Continuous scheduling thread interrupted. Exiting. \",\n+                      e);\n+                  return;\n+                }\n+              }\n             }\n           }\n       );",
                "raw_url": "https://github.com/apache/hadoop-common/raw/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "c0687bcbc249c470465c73ebeda16c64c623b9c6",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop-common/blob/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=17a47c189460dc5183582ceb370b3f3f226fb1ae",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -2763,7 +2763,43 @@ public void testContinuousScheduling() throws Exception {\n     Assert.assertEquals(2, nodes.size());\n   }\n \n-  \n+  @Test\n+  public void testContinuousSchedulingWithNodeRemoved() throws Exception {\n+    // Disable continuous scheduling, will invoke continuous scheduling once manually\n+    scheduler.init(conf);\n+    scheduler.start();\n+    Assert.assertTrue(\"Continuous scheduling should be disabled.\",\n+        !scheduler.isContinuousSchedulingEnabled());\n+\n+    // Add two nodes\n+    RMNode node1 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 1,\n+            \"127.0.0.1\");\n+    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n+    scheduler.handle(nodeEvent1);\n+    RMNode node2 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 2,\n+            \"127.0.0.2\");\n+    NodeAddedSchedulerEvent nodeEvent2 = new NodeAddedSchedulerEvent(node2);\n+    scheduler.handle(nodeEvent2);\n+    Assert.assertEquals(\"We should have two alive nodes.\",\n+        2, scheduler.getNumClusterNodes());\n+\n+    // Remove one node\n+    NodeRemovedSchedulerEvent removeNode1 = new NodeRemovedSchedulerEvent(node1);\n+    scheduler.handle(removeNode1);\n+    Assert.assertEquals(\"We should only have one alive node.\",\n+        1, scheduler.getNumClusterNodes());\n+\n+    // Invoke the continuous scheduling once\n+    try {\n+      scheduler.continuousSchedulingAttempt();\n+    } catch (Exception e) {\n+      fail(\"Exception happened when doing continuous scheduling. \" +\n+        e.toString());\n+    }\n+  }\n+\n   @Test\n   public void testDontAllowUndeclaredPools() throws Exception{\n     conf.setBoolean(FairSchedulerConfiguration.ALLOW_UNDECLARED_POOLS, false);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/17a47c189460dc5183582ceb370b3f3f226fb1ae/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "df157e75001fd51b7687096383deb70b4eb25d4b",
                "status": "modified"
            }
        ],
        "message": "YARN-2273. NPE in ContinuousScheduling thread when we lose a node. (Wei Yan via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612720 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0470b4182114fd42c01517ff2ef0eacca340ae1f",
        "patched_files": [
            "FairScheduler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_196d459": {
        "bug_id": "hadoop-common_196d459",
        "commit": "https://github.com/apache/hadoop-common/commit/196d459a3cf830529b32779e5cfb5b38ff32417c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/196d459a3cf830529b32779e5cfb5b38ff32417c/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=196d459a3cf830529b32779e5cfb5b38ff32417c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -253,6 +253,9 @@ Release 2.1.1-beta - UNRELEASED\n \n     MAPREDUCE-5475. MRClientService does not verify ACLs properly (jlowe)\n \n+    MAPREDUCE-5414. TestTaskAttempt fails in JDK7 with NPE (Nemon Lou via \n+    devaraj)\n+\n Release 2.1.0-beta - 2013-08-22\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/196d459a3cf830529b32779e5cfb5b38ff32417c/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "13f0079fc3c4b3ec7c06a736953a4ea3d63de510",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/196d459a3cf830529b32779e5cfb5b38ff32417c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java?ref=196d459a3cf830529b32779e5cfb5b38ff32417c",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "patch": "@@ -343,7 +343,7 @@ public void testLaunchFailedWhileKilling() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), null);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n@@ -399,7 +399,7 @@ public void testContainerCleanedWhileRunning() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.2\", 0);\n@@ -456,7 +456,7 @@ public void testContainerCleanedWhileCommitting() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n@@ -516,7 +516,7 @@ public void testDoubleTooManyFetchFailure() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n@@ -582,7 +582,7 @@ public void testAppDiognosticEventOnUnassignedTask() throws Exception {\n \n     TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler,\n         jobFile, 1, splits, jobConf, taListener,\n-        mock(Token.class), new Credentials(), new SystemClock(), appCtx);\n+        new Token(), new Credentials(), new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n     ContainerId contId = ContainerId.newInstance(appAttemptId, 3);\n@@ -631,7 +631,7 @@ public void testAppDiognosticEventOnNewTask() throws Exception {\n \n     TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler,\n         jobFile, 1, splits, jobConf, taListener,\n-        mock(Token.class), new Credentials(), new SystemClock(), appCtx);\n+        new Token(), new Credentials(), new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n     ContainerId contId = ContainerId.newInstance(appAttemptId, 3);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/196d459a3cf830529b32779e5cfb5b38ff32417c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "sha": "1129c2fcfc4469f14a9ec29499d3e417758fc6ed",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5414. TestTaskAttempt fails in JDK7 with NPE. Contributed by Nemon Lou.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1520964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0f862fa035d7e36513ec559943949ef346292515",
        "patched_files": [
            "TaskAttempt.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTaskAttempt.java"
        ]
    },
    "hadoop-common_1a84d98": {
        "bug_id": "hadoop-common_1a84d98",
        "commit": "https://github.com/apache/hadoop-common/commit/1a84d983db2623af64c7d1b4c2cfcff276873609",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -123,6 +123,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-1876. Fixes TaskAttemptStartedEvent to correctly log event type\n     for all task types. (Amar Kamat via amareshwari)\n \n+    MAPREDUCE-1863. Fix NPE in Rumen when processing null CDF for failed task\n+    attempts. (Amar Kamat via cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/CHANGES.txt",
                "sha": "46294b6aa914b6c135c86cae792cd25bbbe2f4e5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java",
                "patch": "@@ -214,7 +214,7 @@ public void testHadoop20JHParser() throws Exception {\n             lfs.getUri(), lfs.getWorkingDirectory());\n \n     final Path rootInputPath = new Path(rootInputDir, \"rumen/small-trace-test\");\n-    final Path tempDir = new Path(rootTempDir, \"TestRumenViaDispatch\");\n+    final Path tempDir = new Path(rootTempDir, \"TestHadoop20JHParser\");\n     lfs.delete(tempDir, true);\n \n     final Path inputPath = new Path(rootInputPath, \"v20-single-input-log.gz\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/mapred/org/apache/hadoop/tools/rumen/TestRumenJobTraces.java",
                "sha": "c58d801d8bdef43c51254a86ba04b562e257fd53",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 0,
                "filename": "src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/counters-test-trace.json.gz",
                "sha": "fdf9b353d38d5a0cdee8e94f601ec5de9bd16c2b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 0,
                "filename": "src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/test/tools/data/rumen/small-trace-test/dispatch-trace-output.json.gz",
                "sha": "8ca1f06c60e952914aeaaca74d379b3796dd3b35",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1a84d983db2623af64c7d1b4c2cfcff276873609/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java?ref=1a84d983db2623af64c7d1b4c2cfcff276873609",
                "deletions": 2,
                "filename": "src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                "patch": "@@ -357,8 +357,6 @@ public LoggedJob build() {\n         100);\n     result.setSuccessfulReduceAttemptCDF(succReduce);\n \n-    result.setFailedMapAttemptCDFs(null);\n-\n     long totalSuccessfulAttempts = 0L;\n     long maxTriesToSucceed = 0L;\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1a84d983db2623af64c7d1b4c2cfcff276873609/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                "sha": "2877eb7f06c61208a3588191ab6b9d92e9ddabfd",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1863. Fix NPE in Rumen when processing null CDF for failed task attempts. Contributed by Amar Kamat\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@958841 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d3fc6b3d90714c96988009bc5edcc29be800dbb0",
        "patched_files": [
            "counters-test-trace.json.gz",
            "JobBuilder.java",
            "CHANGES.txt",
            "dispatch-trace-output.json.gz"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRumenJobTraces.java"
        ]
    },
    "hadoop-common_1bd18c3": {
        "bug_id": "hadoop-common_1bd18c3",
        "commit": "https://github.com/apache/hadoop-common/commit/1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -37,6 +37,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are\n+    scheduled but not running. (Todd Lipcon via matei)\n+\n     MAPREDUCE-1014. Fix the libraries for common and hdfs. (omalley)\n \n     MAPREDUCE-1111. JT Jetty UI not working if we run mumak.sh ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "sha": "0571389832cb58552b200f56fa0479f7524b073d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "deletions": 1,
                "filename": "src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "patch": "@@ -831,7 +831,11 @@ protected int tasksToPreempt(PoolSchedulable sched, long curTime) {\n     List<TaskStatus> statuses = new ArrayList<TaskStatus>();\n     for (TaskInProgress tip: tips) {\n       for (TaskAttemptID id: tip.getActiveTasks().keySet()) {\n-        statuses.add(tip.getTaskStatus(id));\n+        TaskStatus stat = tip.getTaskStatus(id);\n+        // status is null when the task has been scheduled but not yet running\n+        if (stat != null) {\n+          statuses.add(stat);\n+        }\n       }\n     }\n     return statuses;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "sha": "ae0930c46581cef366f6fcd59d3ca7408b5383e3",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are  \nscheduled but not running. Contributed by Todd Lipcon.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@830821 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/a98327008f5b124724f3577bde06f2b4ddec6a18",
        "patched_files": [
            "FairScheduler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_1db2b08": {
        "bug_id": "hadoop-common_1db2b08",
        "commit": "https://github.com/apache/hadoop-common/commit/1db2b0851b5de7631427c9983dfd689bde9a5bbf",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1db2b0851b5de7631427c9983dfd689bde9a5bbf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=1db2b0851b5de7631427c9983dfd689bde9a5bbf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -452,6 +452,9 @@ Release 0.23.3 - UNRELEASED\n \n     MAPREDUCE-4163. consistently set the bind address (Daryn Sharp via bobby)\n \n+    MAPREDUCE-4048. NullPointerException exception while accessing the\n+    Application Master UI (Devaraj K via bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1db2b0851b5de7631427c9983dfd689bde9a5bbf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c4046c64b6901ef7c971ea909c42d1fa5763ba49",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1db2b0851b5de7631427c9983dfd689bde9a5bbf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java?ref=1db2b0851b5de7631427c9983dfd689bde9a5bbf",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                "patch": "@@ -27,6 +27,8 @@\n import javax.servlet.http.HttpServletResponse;\n \n import org.apache.commons.lang.StringUtils;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobACL;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n@@ -47,6 +49,8 @@\n  * This class renders the various pages that the web app supports.\n  */\n public class AppController extends Controller implements AMParams {\n+  private static final Log LOG = LogFactory.getLog(AppController.class);\n+  \n   protected final App app;\n   \n   protected AppController(App app, Configuration conf, RequestContext ctx,\n@@ -220,6 +224,8 @@ public void tasks() {\n             toString().toLowerCase(Locale.US));\n         setTitle(join(tt, \" Tasks for \", $(JOB_ID)));\n       } catch (Exception e) {\n+        LOG.error(\"Failed to render tasks page with task type : \"\n+            + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\n         badRequest(e.getMessage());\n       }\n     }\n@@ -283,6 +289,8 @@ public void attempts() {\n \n         render(attemptsPage());\n       } catch (Exception e) {\n+        LOG.error(\"Failed to render attempts page with task type : \"\n+            + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\n         badRequest(e.getMessage());\n       }\n     }\n@@ -316,7 +324,8 @@ public void conf() {\n    */\n   void badRequest(String s) {\n     setStatus(HttpServletResponse.SC_BAD_REQUEST);\n-    setTitle(join(\"Bad request: \", s));\n+    String title = \"Bad request: \";\n+    setTitle((s != null) ? join(title, s) : title);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1db2b0851b5de7631427c9983dfd689bde9a5bbf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                "sha": "da537e5bc71adb18ccbfd1016b6e9af40f8b505e",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1db2b0851b5de7631427c9983dfd689bde9a5bbf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java?ref=1db2b0851b5de7631427c9983dfd689bde9a5bbf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java",
                "patch": "@@ -0,0 +1,71 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapreduce.v2.app.webapp;\n+\n+import static org.mockito.Matchers.anyString;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.v2.app.AppContext;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.util.Records;\n+import org.apache.hadoop.yarn.webapp.Controller.RequestContext;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class TestAppController {\n+\n+  private AppController appController;\n+  private RequestContext ctx;\n+\n+  @Before\n+  public void setUp() {\n+    AppContext context = mock(AppContext.class);\n+    when(context.getApplicationID()).thenReturn(\n+        Records.newRecord(ApplicationId.class));\n+    App app = new App(context);\n+    Configuration conf = new Configuration();\n+    ctx = mock(RequestContext.class);\n+    appController = new AppController(app, conf, ctx);\n+  }\n+\n+  @Test\n+  public void testBadRequest() {\n+    String message = \"test string\";\n+    appController.badRequest(message);\n+    verifyExpectations(message);\n+  }\n+\n+  @Test\n+  public void testBadRequestWithNullMessage() {\n+    // It should not throw NullPointerException\n+    appController.badRequest(null);\n+    verifyExpectations(StringUtils.EMPTY);\n+  }\n+\n+  private void verifyExpectations(String message) {\n+    verify(ctx).setStatus(400);\n+    verify(ctx).set(\"app.id\", \"application_0_0000\");\n+    verify(ctx).set(eq(\"rm.web\"), anyString());\n+    verify(ctx).set(\"title\", \"Bad request: \" + message);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1db2b0851b5de7631427c9983dfd689bde9a5bbf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java",
                "sha": "4fcb4755736662a8eb47e90fb23944f633913e7d",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-4048. NullPointerException exception while accessing the Application Master UI (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1334013 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/98c8413de63579494085846cf78af764bfcc570a",
        "patched_files": [
            "AppController.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestAppController.java"
        ]
    },
    "hadoop-common_1f04f4b": {
        "bug_id": "hadoop-common_1f04f4b",
        "commit": "https://github.com/apache/hadoop-common/commit/1f04f4baf46b45468f0d15d16c71c769c4820827",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -324,6 +324,9 @@ Release 2.0.4-beta - UNRELEASED\n     but not in dfs.namenode.edits.dir are silently ignored.  (Arpit Agarwal\n     via szetszwo)\n \n+    HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race \n+    between delete and replication of same file. (umamahesh)\n+\n Release 2.0.3-alpha - 2013-02-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3b1a1e36bceb5c82791b0db68a2ddae8471b5c0a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1343,6 +1343,11 @@ static String getFullPathName(INode inode) {\n \n     // fill up the inodes in the path from this inode to root\n     for (int i = 0; i < depth; i++) {\n+      if (inode == null) {\n+        NameNode.stateChangeLog.warn(\"Could not get full path.\"\n+            + \" Corresponding file might have deleted already.\");\n+        return null;\n+      }\n       inodes[depth-i-1] = inode;\n       inode = inode.parent;\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "b11059a4bb47c865d14e439b1bbabfa418b90e2b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -282,7 +282,11 @@ String getLocalName() {\n \n   String getLocalParentDir() {\n     INode inode = isRoot() ? this : getParent();\n-    return (inode != null) ? inode.getFullPathName() : \"\";\n+    String parentDir = \"\";\n+    if (inode != null) {\n+      parentDir = inode.getFullPathName();\n+    }\n+    return (parentDir != null) ? parentDir : \"\";\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "b407a62da97b1808bbbc8d7169ecd48a9b05292a",
                "status": "modified"
            }
        ],
        "message": "HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race between delete and replication of same file. Contributed by Uma Maheswara Rao G.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448708 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/57181e75b9305bf18ed50eb905d7f7a6c013b340",
        "patched_files": [
            "INode.java",
            "FSDirectory.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestINode.java",
            "TestFSDirectory.java"
        ]
    },
    "hadoop-common_1f653c8": {
        "bug_id": "hadoop-common_1f653c8",
        "commit": "https://github.com/apache/hadoop-common/commit/1f653c81fac98662ff276f4d65c8eeb0cecbeced",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=1f653c81fac98662ff276f4d65c8eeb0cecbeced",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -413,6 +413,8 @@ Release 2.4.0 - UNRELEASED\n     YARN-1768. Fixed error message being too verbose when killing a non-existent\n     application\n     \n+    YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and\n+    Karthik Kambatla via kasha)\n \n Release 2.3.1 - UNRELEASED\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/CHANGES.txt",
                "sha": "9286a7ead8254cf637f41654d9c50c6958e7e261",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=1f653c81fac98662ff276f4d65c8eeb0cecbeced",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -611,9 +611,6 @@ protected synchronized void addApplication(ApplicationId applicationId,\n     RMApp rmApp = rmContext.getRMApps().get(applicationId);\n     FSLeafQueue queue = assignToQueue(rmApp, queueName, user);\n     if (queue == null) {\n-      rmContext.getDispatcher().getEventHandler().handle(\n-          new RMAppRejectedEvent(applicationId,\n-              \"Application rejected by queue placement policy\"));\n       return;\n     }\n \n@@ -679,27 +676,43 @@ protected synchronized void addApplicationAttempt(\n         new RMAppAttemptEvent(applicationAttemptId,\n             RMAppAttemptEventType.ATTEMPT_ADDED));\n   }\n-  \n+\n+  /**\n+   * Helper method that attempts to assign the app to a queue. The method is\n+   * responsible to call the appropriate event-handler if the app is rejected.\n+   */\n   @VisibleForTesting\n   FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     FSLeafQueue queue = null;\n+    String appRejectMsg = null;\n+\n     try {\n       QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();\n       queueName = placementPolicy.assignAppToQueue(queueName, user);\n       if (queueName == null) {\n-        return null;\n+        appRejectMsg = \"Application rejected by queue placement policy\";\n+      } else {\n+        queue = queueMgr.getLeafQueue(queueName, true);\n+        if (queue == null) {\n+          appRejectMsg = queueName + \" is not a leaf queue\";\n+        }\n       }\n-      queue = queueMgr.getLeafQueue(queueName, true);\n-    } catch (IOException ex) {\n-      LOG.error(\"Error assigning app to queue, rejecting\", ex);\n+    } catch (IOException ioe) {\n+      appRejectMsg = \"Error assigning app to queue \" + queueName;\n     }\n-    \n+\n+    if (appRejectMsg != null && rmApp != null) {\n+      LOG.error(appRejectMsg);\n+      rmContext.getDispatcher().getEventHandler().handle(\n+          new RMAppRejectedEvent(rmApp.getApplicationId(), appRejectMsg));\n+      return null;\n+    }\n+\n     if (rmApp != null) {\n       rmApp.setQueue(queue.getName());\n     } else {\n-      LOG.warn(\"Couldn't find RM app to set queue name on\");\n+      LOG.error(\"Couldn't find RM app to set queue name on\");\n     }\n-    \n     return queue;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "3cdff7f405563bc14e3c7ac7b5522ca977a25a54",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=1f653c81fac98662ff276f4d65c8eeb0cecbeced",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -704,6 +704,22 @@ public void testAssignToQueue() throws Exception {\n     assertEquals(rmApp2.getQueue(), queue2.getName());\n     assertEquals(\"root.notdefault\", rmApp2.getQueue());\n   }\n+\n+  @Test\n+  public void testAssignToNonLeafQueueReturnsNull() throws Exception {\n+    conf.set(FairSchedulerConfiguration.USER_AS_DEFAULT_QUEUE, \"true\");\n+    scheduler.reinitialize(conf, resourceManager.getRMContext());\n+\n+    scheduler.getQueueManager().getLeafQueue(\"root.child1.granchild\", true);\n+    scheduler.getQueueManager().getLeafQueue(\"root.child2\", true);\n+\n+    RMApp rmApp1 = new MockRMApp(0, 0, RMAppState.NEW);\n+    RMApp rmApp2 = new MockRMApp(1, 1, RMAppState.NEW);\n+\n+    // Trying to assign to non leaf queue would return null\n+    assertNull(scheduler.assignToQueue(rmApp1, \"root.child1\", \"tintin\"));\n+    assertNotNull(scheduler.assignToQueue(rmApp2, \"root.child2\", \"snowy\"));\n+  }\n   \n   @Test\n   public void testQueuePlacementWithPolicy() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f653c81fac98662ff276f4d65c8eeb0cecbeced/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "d1052bb165e6ae3798697208b56814da897157ba",
                "status": "modified"
            }
        ],
        "message": "YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and Karthik Kambatla via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575415 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9dae58de3bfd4941f8ec1fc353055298b399d915",
        "patched_files": [
            "FairScheduler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_2426d84": {
        "bug_id": "hadoop-common_2426d84",
        "commit": "https://github.com/apache/hadoop-common/commit/2426d84a3a13421abb9f48c81e479e935733c3f8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -390,6 +390,8 @@ Release 0.21.0 - Unreleased\n \n     HDFS-665. TestFileAppend2 sometimes hangs. (hairong)\n \n+    HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery() (shv)\n+\n Release 0.20.1 - 2009-09-01\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/CHANGES.txt",
                "sha": "5abba3a61bbc02969fd3476b07cd9bfd7ae376a4",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 19,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "patch": "@@ -1982,19 +1982,21 @@ static ReplicaRecoveryInfo initReplicaRecovery(\n     return rur.createInfo();\n   }\n \n-  /** Update a replica of a block. */\n-  synchronized void updateReplica(final Block block, final long recoveryId,\n-      final long newlength) throws IOException {\n+  @Override // FSDatasetInterface\n+  public synchronized ReplicaInfo updateReplicaUnderRecovery(\n+                                    final Block oldBlock,\n+                                    final long recoveryId,\n+                                    final long newlength) throws IOException {\n     //get replica\n-    final ReplicaInfo replica = volumeMap.get(block.getBlockId());\n-    DataNode.LOG.info(\"updateReplica: block=\" + block\n+    final ReplicaInfo replica = volumeMap.get(oldBlock.getBlockId());\n+    DataNode.LOG.info(\"updateReplica: block=\" + oldBlock\n         + \", recoveryId=\" + recoveryId\n         + \", length=\" + newlength\n         + \", replica=\" + replica);\n \n     //check replica\n     if (replica == null) {\n-      throw new ReplicaNotFoundException(block);\n+      throw new ReplicaNotFoundException(oldBlock);\n     }\n \n     //check replica state\n@@ -2007,26 +2009,18 @@ synchronized void updateReplica(final Block block, final long recoveryId,\n     checkReplicaFiles(replica);\n \n     //update replica\n-    final ReplicaInfo finalized = (ReplicaInfo)updateReplicaUnderRecovery(\n-                                    replica, recoveryId, newlength);\n+    final FinalizedReplica finalized = updateReplicaUnderRecovery(\n+        (ReplicaUnderRecovery)replica, recoveryId, newlength);\n \n     //check replica files after update\n     checkReplicaFiles(finalized);\n+    return finalized;\n   }\n \n-  @Override // FSDatasetInterface\n-  public synchronized FinalizedReplica updateReplicaUnderRecovery(\n-                                          Block oldBlock,\n+  private FinalizedReplica updateReplicaUnderRecovery(\n+                                          ReplicaUnderRecovery rur,\n                                           long recoveryId,\n                                           long newlength) throws IOException {\n-    Replica r = getReplica(oldBlock.getBlockId());\n-    if(r.getState() != ReplicaState.RUR)\n-      throw new IOException(\"Replica \" + r + \" must be under recovery.\");\n-    ReplicaUnderRecovery rur = (ReplicaUnderRecovery)r;\n-    DataNode.LOG.info(\"updateReplicaUnderRecovery: recoveryId=\" + recoveryId\n-        + \", newlength=\" + newlength\n-        + \", rur=\" + rur);\n-\n     //check recovery id\n     if (rur.getRecoveryID() != recoveryId) {\n       throw new IOException(\"rur.getRecoveryID() != recoveryId = \" + recoveryId",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "sha": "12d08168eac8d3840933a776ea336865423b2838",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "patch": "@@ -350,7 +350,8 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n   /**\n    * Update replica's generation stamp and length and finalize it.\n    */\n-  public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n+  public ReplicaInfo updateReplicaUnderRecovery(\n+                                          Block oldBlock,\n                                           long recoveryId,\n                                           long newLength) throws IOException;\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/java/org/apache/hadoop/hdfs/server/datanode/FSDatasetInterface.java",
                "sha": "462e1fffc573d6230dc402b6e34066fd7dd325f1",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -804,10 +804,10 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n     return new ReplicaRecoveryInfo(rBlock.getBlock(), ReplicaState.FINALIZED);\n   }\n \n-  @Override\n+  @Override // FSDatasetInterface\n   public FinalizedReplica updateReplicaUnderRecovery(Block oldBlock,\n-                                          long recoveryId,\n-                                          long newlength) throws IOException {\n+                                        long recoveryId,\n+                                        long newlength) throws IOException {\n     return new FinalizedReplica(\n         oldBlock.getBlockId(), newlength, recoveryId, null, null);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "1d9fe16525aab45d39a39a2ff7358ab4650c3c0e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java?ref=2426d84a3a13421abb9f48c81e479e935733c3f8",
                "deletions": 3,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "patch": "@@ -234,9 +234,8 @@ public void testUpdateReplicaUnderRecovery() throws IOException {\n       FSDataset.checkReplicaFiles(rur);\n \n       //update\n-      final ReplicaInfo finalized = \n-        (ReplicaInfo)fsdataset.updateReplicaUnderRecovery(\n-            rur, recoveryid, newlength);\n+      final ReplicaInfo finalized = fsdataset.updateReplicaUnderRecovery(\n+                                                rur, recoveryid, newlength);\n \n       //check meta data after update\n       FSDataset.checkReplicaFiles(finalized);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2426d84a3a13421abb9f48c81e479e935733c3f8/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestInterDatanodeProtocol.java",
                "sha": "40d15a6f02c0cd211df29bfad0ebed066a4c5b4c",
                "status": "modified"
            }
        ],
        "message": "HDFS-676. Fix NPE in FSDataset.updateReplicaUnderRecovery(). Contributed by Konstantin Shvachko.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@823732 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f2809529a9d80906c53ae6931e84255f89ff1f8b",
        "patched_files": [
            "InterDatanodeProtocol.java",
            "FSDatasetInterface.java",
            "CHANGES.txt",
            "SimulatedFSDataset.java",
            "FSDataset.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestSimulatedFSDataset.java",
            "TestInterDatanodeProtocol.java"
        ]
    },
    "hadoop-common_24275fd": {
        "bug_id": "hadoop-common_24275fd",
        "commit": "https://github.com/apache/hadoop-common/commit/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -656,6 +656,9 @@ Release 2.5.0 - UNRELEASED\n     HDFS-6552. add DN storage to a BlockInfo will not replace the different\n     storage from same DN. (Amir Langer via Arpit Agarwal)\n \n+    HDFS-6551. Rename with OVERWRITE option may throw NPE when the target\n+    file/directory is a reference INode. (jing9)\n+\n   BREAKDOWN OF HDFS-2006 SUBTASKS AND RELATED JIRAS\n \n     HDFS-6299. Protobuf for XAttr and client-side implementation. (Yi Liu via umamahesh)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "f0a84bd7d224030b1d5c5518bd0a997e33a2152d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.hadoop.fs.XAttrSetFlag;\n import org.apache.hadoop.fs.permission.AclEntry;\n import org.apache.hadoop.fs.permission.AclStatus;\n-import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n@@ -891,9 +890,10 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n     \n     boolean undoRemoveDst = false;\n     INode removedDst = null;\n+    long removedNum = 0;\n     try {\n       if (dstInode != null) { // dst exists remove it\n-        if (removeLastINode(dstIIP) != -1) {\n+        if ((removedNum = removeLastINode(dstIIP)) != -1) {\n           removedDst = dstIIP.getLastINode();\n           undoRemoveDst = true;\n         }\n@@ -933,13 +933,15 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n         long filesDeleted = -1;\n         if (removedDst != null) {\n           undoRemoveDst = false;\n-          BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n-          List<INode> removedINodes = new ChunkedArrayList<INode>();\n-          filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n-              dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes, true)\n-              .get(Quota.NAMESPACE);\n-          getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n-              removedINodes);\n+          if (removedNum > 0) {\n+            BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n+            List<INode> removedINodes = new ChunkedArrayList<INode>();\n+            filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n+                dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes,\n+                true).get(Quota.NAMESPACE);\n+            getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n+                removedINodes);\n+          }\n         }\n \n         if (snapshottableDirs.size() > 0) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "dc553ab73b96d1c636cdf5fb23a33ebf4ccfec8c",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop-common/blob/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java?ref=24275fdc0f5dddee3cccadf570f7361a8cd8d5e3",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "patch": "@@ -171,8 +171,6 @@ public void testRenameFromSDir2NonSDir() throws Exception {\n   private static boolean existsInDiffReport(List<DiffReportEntry> entries,\n       DiffType type, String relativePath) {\n     for (DiffReportEntry entry : entries) {\n-      System.out.println(\"DiffEntry is:\" + entry.getType() + \"\\\"\"\n-          + new String(entry.getRelativePath()) + \"\\\"\");\n       if ((entry.getType() == type)\n           && ((new String(entry.getRelativePath())).compareTo(relativePath) == 0)) {\n         return true;\n@@ -2374,4 +2372,46 @@ public void testAppendFileAfterRenameInSnapshot() throws Exception {\n     // save namespace and restart\n     restartClusterAndCheckImage(true);\n   }\n+\n+  @Test\n+  public void testRenameWithOverWrite() throws Exception {\n+    final Path root = new Path(\"/\");\n+    final Path foo = new Path(root, \"foo\");\n+    final Path file1InFoo = new Path(foo, \"file1\");\n+    final Path file2InFoo = new Path(foo, \"file2\");\n+    final Path file3InFoo = new Path(foo, \"file3\");\n+    DFSTestUtil.createFile(hdfs, file1InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file2InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file3InFoo, 1L, REPL, SEED);\n+    final Path bar = new Path(root, \"bar\");\n+    hdfs.mkdirs(bar);\n+\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n+    // move file1 from foo to bar\n+    final Path fileInBar = new Path(bar, \"file1\");\n+    hdfs.rename(file1InFoo, fileInBar);\n+    // rename bar to newDir\n+    final Path newDir = new Path(root, \"newDir\");\n+    hdfs.rename(bar, newDir);\n+    // move file2 from foo to newDir\n+    final Path file2InNewDir = new Path(newDir, \"file2\");\n+    hdfs.rename(file2InFoo, file2InNewDir);\n+    // move file3 from foo to newDir and rename it to file1, this will overwrite\n+    // the original file1\n+    final Path file1InNewDir = new Path(newDir, \"file1\");\n+    hdfs.rename(file3InFoo, file1InNewDir, Rename.OVERWRITE);\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n+\n+    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\");\n+    LOG.info(\"DiffList is \\n\\\"\" + report.toString() + \"\\\"\");\n+    List<DiffReportEntry> entries = report.getDiffList();\n+    assertEquals(7, entries.size());\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, \"\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, foo.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, bar.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.CREATE, newDir.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file1\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file2\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file3\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/24275fdc0f5dddee3cccadf570f7361a8cd8d5e3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "sha": "c88aaf2410e8b34fd0303b6546aeb9249a2e51b5",
                "status": "modified"
            }
        ],
        "message": "HDFS-6551. Rename with OVERWRITE option may throw NPE when the target file/directory is a reference INode. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603612 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f7e0a9792a93d8f8bf982295b92003f186024301",
        "patched_files": [
            "FSDirectory.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSDirectory.java",
            "TestRenameWithSnapshots.java"
        ]
    },
    "hadoop-common_279d851": {
        "bug_id": "hadoop-common_279d851",
        "commit": "https://github.com/apache/hadoop-common/commit/279d851c47c6fc994affcbaf04c49ff102e96b5c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -69,3 +69,5 @@ HDFS-5535 subtasks:\n     HDFS-5987. Fix findbugs warnings in Rolling Upgrade branch. (seztszwo via\n     Arpit Agarwal)\n \n+    HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for\n+    TestOfflineEditsViewer.  (szetszwo)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "4c567902c863adaa5719b632106cb992a0247b66",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n@@ -72,10 +73,6 @@ public static void verifySavedMD5(File dataFile, MD5Hash expectedMD5)\n    *   where group(1) is the md5 string and group(2) is the data file path.\n    */\n   private static Matcher readStoredMd5(File md5File) throws IOException {\n-    if (!md5File.exists()) {\n-      return null;\n-    }\n-    \n     BufferedReader reader =\n         new BufferedReader(new InputStreamReader(new FileInputStream(\n             md5File), Charsets.UTF_8));\n@@ -105,6 +102,10 @@ private static Matcher readStoredMd5(File md5File) throws IOException {\n    */\n   public static MD5Hash readStoredMd5ForFile(File dataFile) throws IOException {\n     final File md5File = getDigestFileForFile(dataFile);\n+    if (!md5File.exists()) {\n+      return null;\n+    }\n+\n     final Matcher matcher = readStoredMd5(md5File);\n     String storedHash = matcher.group(1);\n     File referencedFile = new File(matcher.group(2));\n@@ -165,6 +166,10 @@ private static void saveMD5File(File dataFile, String digestString)\n   public static void renameMD5File(File oldDataFile, File newDataFile)\n       throws IOException {\n     final File fromFile = getDigestFileForFile(oldDataFile);\n+    if (!fromFile.exists()) {\n+      throw new FileNotFoundException(fromFile + \" does not exist.\");\n+    }\n+\n     final String digestString = readStoredMd5(fromFile).group(1);\n     saveMD5File(newDataFile, digestString);\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "sha": "d87ffbf3154236c316020cdb0520469379d87d12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "patch": "@@ -130,7 +130,10 @@ private CheckpointSignature runOperations() throws IOException {\n     DFSTestUtil.runOperations(cluster, dfs, cluster.getConfiguration(0),\n         dfs.getDefaultBlockSize(), 0);\n \n+    // OP_ROLLING_UPGRADE_START\n     cluster.getNamesystem().getEditLog().logStartRollingUpgrade(Time.now());\n+    // OP_ROLLING_UPGRADE_FINALIZE\n+    cluster.getNamesystem().getEditLog().logFinalizeRollingUpgrade(Time.now());\n \n     // Force a roll so we get an OP_END_LOG_SEGMENT txn\n     return cluster.getNameNodeRpc().rollEditLog();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "sha": "c6364b174d0b0763c271e09fff004d0b2abdcab0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "patch": "@@ -95,6 +95,7 @@ public void testGenerated() throws IOException {\n     // edits generated by nnHelper (MiniDFSCluster), should have all op codes\n     // binary, XML, reparsed binary\n     String edits = nnHelper.generateEdits();\n+    LOG.info(\"Generated edits=\" + edits);\n     String editsParsedXml = folder.newFile(\"editsParsed.xml\").getAbsolutePath();\n     String editsReparsed = folder.newFile(\"editsParsed\").getAbsolutePath();\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "sha": "19d98ab6988a92753296f65452c519f0fb878653",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "sha": "6c17f4af4a9ec863a56a6b27dec867a38eb6210e",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/hadoop-common/blob/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "changes": 578,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml?ref=279d851c47c6fc994affcbaf04c49ff102e96b5c",
                "deletions": 476,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "patch": "@@ -1,10 +1,6 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <EDITS>\n-<<<<<<< .working\n-  <EDITS_VERSION>-50</EDITS_VERSION>\n-=======\n-  <EDITS_VERSION>-51</EDITS_VERSION>\n->>>>>>> .merge-right.r1559304\n+  <EDITS_VERSION>-55</EDITS_VERSION>\n   <RECORD>\n     <OPCODE>OP_START_LOG_SEGMENT</OPCODE>\n     <DATA>\n@@ -17,13 +13,8 @@\n       <TXID>2</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>1</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314720</EXPIRY_DATE>\n-        <KEY>d2a03d66ebfac521</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460949</EXPIRY_DATE>\n-        <KEY>dc8d30edc97df67d</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283650</EXPIRY_DATE>\n+        <KEY>76e6d2854a753680</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -33,13 +24,8 @@\n       <TXID>3</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>2</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314722</EXPIRY_DATE>\n-        <KEY>ef94532092f55aef</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460952</EXPIRY_DATE>\n-        <KEY>096bc20b6debed03</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283653</EXPIRY_DATE>\n+        <KEY>939fb7b875c956cd</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -51,36 +37,18 @@\n       <INODEID>16386</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115316</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828264873</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084379</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>6</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>7</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -91,22 +59,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115327</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828265699</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084397</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -119,15 +78,9 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115331</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>8</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265705</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>11</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084400</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>9</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -136,15 +89,9 @@\n       <TXID>7</TXID>\n       <LENGTH>0</LENGTH>\n       <PATH>/file_moved</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115336</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265712</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>12</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084413</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>10</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -154,17 +101,9 @@\n       <LENGTH>0</LENGTH>\n       <INODEID>16387</INODEID>\n       <PATH>/directory_mkdir</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115342</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265722</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084419</TIMESTAMP>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>493</MODE>\n       </PERMISSION_STATUS>\n@@ -197,13 +136,8 @@\n       <TXID>12</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot1</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>14</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>15</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -213,13 +147,8 @@\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTOLDNAME>snapshot1</SNAPSHOTOLDNAME>\n       <SNAPSHOTNEWNAME>snapshot2</SNAPSHOTNEWNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>15</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>18</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>16</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -228,13 +157,8 @@\n       <TXID>14</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot2</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>16</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>19</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>17</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -245,36 +169,18 @@\n       <INODEID>16388</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115362</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265757</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084440</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>20</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>18</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -285,22 +191,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115363</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265759</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084441</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -356,19 +253,10 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115378</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265782</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084455</TIMESTAMP>\n       <OPTIONS>NONE</OPTIONS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>24</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>27</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>25</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -379,36 +267,18 @@\n       <INODEID>16389</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115382</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828265787</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084459</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>26</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>29</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>27</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -513,13 +383,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115461</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828266540</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084525</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -539,11 +404,7 @@\n         <GENSTAMP>1003</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -557,36 +418,18 @@\n       <INODEID>16390</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115463</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266544</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084527</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>39</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>41</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>40</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -691,13 +534,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115477</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266569</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084542</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -717,11 +555,7 @@\n         <GENSTAMP>1006</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -735,36 +569,18 @@\n       <INODEID>16391</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115479</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266572</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084544</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>51</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>53</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>52</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -869,13 +685,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115495</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266599</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084559</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -895,11 +706,7 @@\n         <GENSTAMP>1009</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -911,22 +718,13 @@\n       <TXID>56</TXID>\n       <LENGTH>0</LENGTH>\n       <TRG>/file_concat_target</TRG>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115498</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828266603</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084561</TIMESTAMP>\n       <SOURCES>\n         <SOURCE1>/file_concat_0</SOURCE1>\n         <SOURCE2>/file_concat_1</SOURCE2>\n       </SOURCES>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>62</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>64</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>63</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -937,190 +735,37 @@\n       <INODEID>16392</INODEID>\n       <PATH>/file_symlink</PATH>\n       <VALUE>/file_concat_target</VALUE>\n-<<<<<<< .working\n-      <MTIME>1388730115502</MTIME>\n-      <ATIME>1388730115502</ATIME>\n-=======\n-      <MTIME>1389828266633</MTIME>\n-      <ATIME>1389828266633</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084564</MTIME>\n+      <ATIME>1392957084564</ATIME>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>511</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>63</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>66</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>64</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_ADD</OPCODE>\n     <DATA>\n       <TXID>58</TXID>\n-<<<<<<< .working\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515505</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_RENEW_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>59</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515564</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_CANCEL_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>60</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>61</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>szetszwo</OWNERNAME>\n-      <GROUPNAME>staff</GROUPNAME>\n-      <MODE>493</MODE>\n-      <LIMIT>9223372036854775807</LIMIT>\n-      <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>62</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>carlton</OWNERNAME>\n-      <GROUPNAME>party</GROUPNAME>\n-      <MODE>448</MODE>\n-      <LIMIT>1989</LIMIT>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>68</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>63</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar</PATH>\n-      <REPLICATION>1</REPLICATION>\n-      <POOL>poolparty</POOL>\n-      <EXPIRATION>2305844397943809533</EXPIRATION>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>69</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>64</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar2</PATH>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>70</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>65</TXID>\n-      <ID>1</ID>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>71</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>66</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>72</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD</OPCODE>\n-    <DATA>\n-      <TXID>67</TXID>\n-=======\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>16393</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115596</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828266637</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084567</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>73</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>65</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1175,38 +820,22 @@\n   <RECORD>\n     <OPCODE>OP_REASSIGN_LEASE</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>73</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_381408282_1</LEASEHOLDER>\n-=======\n       <TXID>64</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_16108824_1</LEASEHOLDER>\n->>>>>>> .merge-right.r1559304\n+      <LEASEHOLDER>DFSClient_NONMAPREDUCE_-1178237747_1</LEASEHOLDER>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <NEWHOLDER>HDFS_NameNode</NEWHOLDER>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_CLOSE</OPCODE>\n-    <DATA>\n-      <TXID>74</TXID>\n-=======\n     <OPCODE>OP_CLOSE</OPCODE>\n     <DATA>\n       <TXID>65</TXID>\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>0</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730118281</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828269751</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957087263</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -1216,36 +845,24 @@\n         <GENSTAMP>1011</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_UPGRADE_MARKER</OPCODE>\n-    <DATA>\n-      <TXID>75</TXID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-=======\n     <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>66</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <OWNERNAME>jing</OWNERNAME>\n+      <OWNERNAME>szetszwo</OWNERNAME>\n       <GROUPNAME>staff</GROUPNAME>\n       <MODE>493</MODE>\n       <LIMIT>9223372036854775807</LIMIT>\n       <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>74</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>72</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1254,8 +871,8 @@\n       <TXID>67</TXID>\n       <POOLNAME>pool1</POOLNAME>\n       <LIMIT>99</LIMIT>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>75</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>73</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1266,9 +883,9 @@\n       <PATH>/path</PATH>\n       <REPLICATION>1</REPLICATION>\n       <POOL>pool1</POOL>\n-      <EXPIRATION>2305844399041964876</EXPIRATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>76</RPC_CALLID>\n+      <EXPIRATION>2305844402170781554</EXPIRATION>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>74</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1277,44 +894,53 @@\n       <TXID>69</TXID>\n       <ID>1</ID>\n       <REPLICATION>2</REPLICATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>77</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>75</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n     <DATA>\n       <TXID>70</TXID>\n       <ID>1</ID>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>78</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>76</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>71</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>79</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>77</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n->>>>>>> .merge-right.r1559304\n-    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <OPCODE>OP_SET_ACL</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>76</TXID>\n-=======\n       <TXID>72</TXID>\n->>>>>>> .merge-right.r1559304\n+      <SRC>/file_concat_target</SRC>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-    <OPCODE>OP_SET_ACL</OPCODE>\n+    <OPCODE>OP_ROLLING_UPGRADE_START</OPCODE>\n     <DATA>\n       <TXID>73</TXID>\n-      <SRC>/file_set_acl</SRC>\n+      <STARTTIME>1392957087621</STARTTIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_ROLLING_UPGRADE_FINALIZE</OPCODE>\n+    <DATA>\n+      <TXID>74</TXID>\n+      <FINALIZETIME>1392957087621</FINALIZETIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <DATA>\n+      <TXID>75</TXID>\n     </DATA>\n   </RECORD>\n </EDITS>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/279d851c47c6fc994affcbaf04c49ff102e96b5c/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "sha": "b3115591d0327d924a4eb098147c8da374327d5b",
                "status": "modified"
            }
        ],
        "message": "HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for TestOfflineEditsViewer.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1570690 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/65c28e1a6fea59574d6f8f6a9688df3361d56562",
        "patched_files": [
            "editsStored",
            "editsStored.xml",
            "CHANGES_HDFS-5535.txt",
            "OfflineEditsViewerHelper.java",
            "MD5FileUtils.java",
            "OfflineEditsViewer.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestOfflineEditsViewer.java",
            "TestMD5FileUtils.java"
        ]
    },
    "hadoop-common_28ab08c": {
        "bug_id": "hadoop-common_28ab08c",
        "commit": "https://github.com/apache/hadoop-common/commit/28ab08c53d70d5e79139d5dc288866c6142d8d37",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/28ab08c53d70d5e79139d5dc288866c6142d8d37/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=28ab08c53d70d5e79139d5dc288866c6142d8d37",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -183,6 +183,8 @@ Release 2.5.0 - UNRELEASED\n     YARN-2103. Inconsistency between viaProto flag and initial value of \n     SerializedExceptionProto.Builder (Binglin Chang via junping_du)\n \n+    YARN-1550. NPE in FairSchedulerAppsBlock#render. (Anubhav Dhoot via kasha)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/28ab08c53d70d5e79139d5dc288866c6142d8d37/hadoop-yarn-project/CHANGES.txt",
                "sha": "84563027cbec909539a2a57376d83158af716c40",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/28ab08c53d70d5e79139d5dc288866c6142d8d37/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java?ref=28ab08c53d70d5e79139d5dc288866c6142d8d37",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "patch": "@@ -25,6 +25,8 @@\n \n import java.util.Collection;\n import java.util.HashSet;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.lang.StringEscapeUtils;\n@@ -35,6 +37,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerInfo;\n@@ -60,7 +63,14 @@\n     super(ctx);\n     FairScheduler scheduler = (FairScheduler) rm.getResourceScheduler();\n     fsinfo = new FairSchedulerInfo(scheduler);\n-    apps = rmContext.getRMApps();\n+    apps = new ConcurrentHashMap<ApplicationId, RMApp>();\n+    for (Map.Entry<ApplicationId, RMApp> entry : rmContext.getRMApps().entrySet()) {\n+      if (!(RMAppState.NEW.equals(entry.getValue().getState())\n+          || RMAppState.NEW_SAVING.equals(entry.getValue().getState())\n+          || RMAppState.SUBMITTED.equals(entry.getValue().getState()))) {\n+        apps.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n     this.conf = conf;\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/28ab08c53d70d5e79139d5dc288866c6142d8d37/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "sha": "b1aff9078ca9dea312d3ced677ebfcd3afe37508",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop-common/blob/28ab08c53d70d5e79139d5dc288866c6142d8d37/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java?ref=28ab08c53d70d5e79139d5dc288866c6142d8d37",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "patch": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager.webapp;\n+\n+import com.google.common.collect.Maps;\n+import com.google.inject.Binder;\n+import com.google.inject.Injector;\n+import com.google.inject.Module;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n+import org.apache.hadoop.yarn.webapp.test.WebAppTests;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentMap;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class TestRMWebAppFairScheduler {\n+\n+  @Test\n+  public void testFairSchedulerWebAppPage() {\n+    List<RMAppState> appStates = Arrays.asList(RMAppState.NEW,\n+        RMAppState.NEW_SAVING, RMAppState.SUBMITTED);\n+    final RMContext rmContext = mockRMContext(appStates);\n+    Injector injector = WebAppTests.createMockInjector(RMContext.class,\n+        rmContext,\n+        new Module() {\n+          @Override\n+          public void configure(Binder binder) {\n+            try {\n+              ResourceManager mockRmWithFairScheduler =\n+                  mockRm(rmContext);\n+              binder.bind(ResourceManager.class).toInstance\n+                  (mockRmWithFairScheduler);\n+\n+            } catch (IOException e) {\n+              throw new IllegalStateException(e);\n+            }\n+          }\n+        });\n+    FairSchedulerPage fsViewInstance = injector.getInstance(FairSchedulerPage\n+        .class);\n+    fsViewInstance.render();\n+    WebAppTests.flushOutput(injector);\n+  }\n+\n+  private static RMContext mockRMContext(List<RMAppState> states) {\n+    final ConcurrentMap<ApplicationId, RMApp> applicationsMaps = Maps\n+        .newConcurrentMap();\n+    int i = 0;\n+    for (RMAppState state : states) {\n+      MockRMApp app = new MockRMApp(i, i, state);\n+      applicationsMaps.put(app.getApplicationId(), app);\n+      i++;\n+    }\n+\n+    return new RMContextImpl(null, null, null, null,\n+        null, null, null, null, null, null) {\n+      @Override\n+      public ConcurrentMap<ApplicationId, RMApp> getRMApps() {\n+        return applicationsMaps;\n+      }\n+    };\n+  }\n+\n+  private static ResourceManager mockRm(RMContext rmContext) throws\n+      IOException {\n+    ResourceManager rm = mock(ResourceManager.class);\n+    ResourceScheduler rs = mockFairScheduler();\n+    when(rm.getResourceScheduler()).thenReturn(rs);\n+    when(rm.getRMContext()).thenReturn(rmContext);\n+    return rm;\n+  }\n+\n+  private static FairScheduler mockFairScheduler() throws IOException {\n+    FairScheduler fs = new FairScheduler();\n+    FairSchedulerConfiguration conf = new FairSchedulerConfiguration();\n+    fs.setRMContext(new RMContextImpl(null, null, null, null, null,\n+        null, new RMContainerTokenSecretManager(conf),\n+        new NMTokenSecretManagerInRM(conf),\n+        new ClientToAMTokenSecretManagerInRM(), null));\n+    fs.init(conf);\n+    return fs;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/28ab08c53d70d5e79139d5dc288866c6142d8d37/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "sha": "1de64896c55c3d684d568814af455366605c6032",
                "status": "added"
            }
        ],
        "message": "YARN-1550. NPE in FairSchedulerAppsBlock#render. (Anubhav Dhoot via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1599345 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b3495abb4d0011cb65a184f67271cf9111cc8158",
        "patched_files": [
            "FairSchedulerAppsBlock.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMWebAppFairScheduler.java"
        ]
    },
    "hadoop-common_28c1b2a": {
        "bug_id": "hadoop-common_28c1b2a",
        "commit": "https://github.com/apache/hadoop-common/commit/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -930,6 +930,9 @@ Release 0.22.0 - Unreleased\n     HDFS-1544. Ivy resolve force mode should be turned off by default.\n     (Luke Lu via tomwhite)\n \n+    HDFS-1615. seek() on closed DFS input stream throws NullPointerException\n+    (Scott Carey via todd)\n+\n Release 0.21.1 - Unreleased\n     HDFS-1466. TestFcHdfsSymlink relies on /tmp/test not existing. (eli)\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/CHANGES.txt",
                "sha": "fda22ec7b56dd6259763dc18fe543538ce17f4fc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -741,6 +741,9 @@ public synchronized void seek(long targetPos) throws IOException {\n     if (targetPos > getFileLength()) {\n       throw new IOException(\"Cannot seek after EOF\");\n     }\n+    if (closed) {\n+      throw new IOException(\"Stream is closed!\");\n+    }\n     boolean done = false;\n     if (pos <= targetPos && targetPos <= blockEnd) {\n       //",
                "raw_url": "https://github.com/apache/hadoop-common/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "b74b55dc2226ce1404d5478214bd56fb3180248c",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop-common/blob/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=28c1b2a50acab2b3815ec0c3c2ea76eae94b91df",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -100,6 +100,45 @@ public void testDFSClose() throws Exception {\n       if (cluster != null) {cluster.shutdown();}\n     }\n   }\n+  \n+  @Test\n+  public void testDFSSeekExceptions() throws IOException {\n+    Configuration conf = getTestConfiguration();\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    FileSystem fileSys = cluster.getFileSystem();\n+\n+    try {\n+      String file = \"/test/fileclosethenseek/file-0\";\n+      Path path = new Path(file);\n+      // create file\n+      FSDataOutputStream output = fileSys.create(path);\n+      output.writeBytes(\"Some test data to write longer than 10 bytes\");\n+      output.close();\n+      FSDataInputStream input = fileSys.open(path);\n+      input.seek(10);\n+      boolean threw = false;\n+      try {\n+        input.seek(100);\n+      } catch (IOException e) {\n+        // success\n+        threw = true;\n+      }\n+      assertTrue(\"Failed to throw IOE when seeking past end\", threw);\n+      input.close();\n+      threw = false;\n+      try {\n+        input.seek(1);\n+      } catch (IOException e) {\n+        //success\n+        threw = true;\n+      }\n+      assertTrue(\"Failed to throw IOE when seeking after close\", threw);\n+      fileSys.close();\n+    }\n+    finally {\n+      if (cluster != null) {cluster.shutdown();}\n+    }\n+  }\n \n   @Test\n   public void testDFSClient() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/28c1b2a50acab2b3815ec0c3c2ea76eae94b91df/src/test/hdfs/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "5184ceb782d552c16fb66f76c3649110ae1c30a6",
                "status": "modified"
            }
        ],
        "message": "HDFS-1615. seek() on closed DFS input stream throws NullPointerException. Contributed by Scott Carey.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1102094 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/1abbfe982f977c605db25b1bd9933e1d1a251de8",
        "patched_files": [
            "DistributedFileSystem.java",
            "DFSInputStream.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop-common_29f5383": {
        "bug_id": "hadoop-common_29f5383",
        "commit": "https://github.com/apache/hadoop-common/commit/29f5383330910e589757db7293dd6b3a094805a9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/CHANGES.txt?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 0,
                "filename": "common/CHANGES.txt",
                "patch": "@@ -346,6 +346,9 @@ Trunk (unreleased changes)\n     HADOOP-7090. Fix resource leaks in s3.INode, BloomMapFile, WritableUtils\n     and CBZip2OutputStream.  (Uma Maheswara Rao G via szetszwo)\n \n+    HADOOP-7440. HttpServer.getParameterValues throws NPE for missing\n+    parameters. (Uma Maheswara Rao G and todd via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/CHANGES.txt",
                "sha": "c84dc559c8557a4434498e93f6e7e12e1e42daef",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/java/org/apache/hadoop/http/HttpServer.java?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/http/HttpServer.java",
                "patch": "@@ -800,6 +800,9 @@ public String getParameter(String name) {\n       public String[] getParameterValues(String name) {\n         String unquoteName = HtmlQuoting.unquoteHtmlChars(name);\n         String[] unquoteValue = rawRequest.getParameterValues(unquoteName);\n+        if (unquoteValue == null) {\n+          return null;\n+        }\n         String[] result = new String[unquoteValue.length];\n         for(int i=0; i < result.length; ++i) {\n           result[i] = HtmlQuoting.quoteHtmlChars(unquoteValue[i]);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "sha": "6d6864c63dcb62429fd757215868e1cae03c94fd",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 3,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "patch": "@@ -17,11 +17,12 @@\n  */\n package org.apache.hadoop.http;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n+\n+import javax.servlet.http.HttpServletRequest;\n \n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHtmlQuoting {\n \n@@ -62,4 +63,28 @@ private void runRoundTrip(String str) throws Exception {\n     }\n     runRoundTrip(buffer.toString());\n   }\n+  \n+\n+  @Test\n+  public void testRequestQuoting() throws Exception {\n+    HttpServletRequest mockReq = Mockito.mock(HttpServletRequest.class);\n+    HttpServer.QuotingInputFilter.RequestQuoter quoter =\n+      new HttpServer.QuotingInputFilter.RequestQuoter(mockReq);\n+    \n+    Mockito.doReturn(\"a<b\").when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test simple param quoting\",\n+        \"a&lt;b\", quoter.getParameter(\"x\"));\n+    \n+    Mockito.doReturn(null).when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test that missing parameters dont cause NPE\",\n+        null, quoter.getParameter(\"x\"));\n+\n+    Mockito.doReturn(new String[]{\"a<b\", \"b\"}).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test escaping of an array\",\n+        new String[]{\"a&lt;b\", \"b\"}, quoter.getParameterValues(\"x\"));\n+\n+    Mockito.doReturn(null).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test that missing parameters dont cause NPE for array\",\n+        null, quoter.getParameterValues(\"x\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "sha": "9fc53a3b6fb9d50beafba1b66721310239b18b8b",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-common/blob/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java?ref=29f5383330910e589757db7293dd6b3a094805a9",
                "deletions": 0,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "patch": "@@ -45,16 +45,20 @@\n import javax.servlet.http.HttpServletRequestWrapper;\n import javax.servlet.http.HttpServletResponse;\n \n+import junit.framework.Assert;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.http.HttpServer.QuotingInputFilter.RequestQuoter;\n import org.apache.hadoop.security.Groups;\n import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;\n import org.apache.hadoop.security.authorize.AccessControlList;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHttpServer extends HttpServerFunctionalTest {\n   private static HttpServer server;\n@@ -379,4 +383,26 @@ public void testAuthorizationOfDefaultServlets() throws Exception {\n     }\n     myServer.stop();\n   }\n+  \n+  @Test\n+  public void testRequestQuoterWithNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    Mockito.doReturn(null).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertEquals(\"It should return null \"\n+        + \"when there are no values for the parameter\", null, parameterValues);\n+  }\n+\n+  @Test\n+  public void testRequestQuoterWithNotNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    String[] values = new String[] { \"abc\", \"def\" };\n+    Mockito.doReturn(values).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertTrue(\"It should return Parameter Values\", Arrays.equals(\n+        values, parameterValues));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/29f5383330910e589757db7293dd6b3a094805a9/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "sha": "0a25236fc8381e3ece6424ab16a94c890afc2f0d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7440. HttpServer.getParameterValues throws NPE for missing parameters. Contributed by Uma Maheswara Rao G and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1143212 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9906f9949e30d3226bddc31c295d0f9fefe52986",
        "patched_files": [
            "HtmlQuoting.java",
            "CHANGES.txt",
            "HttpServer.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHtmlQuoting.java",
            "TestHttpServer.java"
        ]
    },
    "hadoop-common_2c80616": {
        "bug_id": "hadoop-common_2c80616",
        "commit": "https://github.com/apache/hadoop-common/commit/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -232,6 +232,8 @@ Trunk (unreleased changes)\n     MAPREDUCE-2495. exit() the TaskTracker when the distributed cache cleanup\n     thread dies. (Robert Joseph Evans via cdouglas)\n \n+    MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters. (Robert Joseph Evans\n+    via cdouglas)\n \n Release 0.22.0 - Unreleased\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/CHANGES.txt",
                "sha": "267bbd5f6761ad691d7e255f6a8f1b766e31c5d1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -42,8 +42,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RemoteException;\n-import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.util.Tool;\n@@ -149,7 +147,7 @@\n    * a JobProfile object to provide some info, and interacts with the\n    * remote service to provide certain functionality.\n    */\n-  class NetworkedJob implements RunningJob {\n+  static class NetworkedJob implements RunningJob {\n     Job job;\n     /**\n      * We store a JobProfile and a timestamp for when we last\n@@ -158,7 +156,7 @@\n      * has completely forgotten about the job.  (eg, 24 hours after the\n      * job completes.)\n      */\n-    public NetworkedJob(JobStatus status) throws IOException {\n+    public NetworkedJob(JobStatus status, Cluster cluster) throws IOException {\n       job = Job.getInstance(cluster, status, new JobConf(status.getJobFile()));\n     }\n \n@@ -380,7 +378,12 @@ public String toString() {\n      */\n     public Counters getCounters() throws IOException {\n       try { \n-        return Counters.downgrade(job.getCounters());\n+        Counters result = null;\n+        org.apache.hadoop.mapreduce.Counters temp = job.getCounters();\n+        if(temp != null) {\n+          result = Counters.downgrade(temp);\n+        }\n+        return result;\n       } catch (InterruptedException ie) {\n         throw new IOException(ie);\n       }\n@@ -557,7 +560,7 @@ public RunningJob getJob(JobID jobid) throws IOException {\n       if (job != null) {\n         JobStatus status = JobStatus.downgrade(job.getStatus());\n         if (status != null) {\n-          return new NetworkedJob(status);\n+          return new NetworkedJob(status, cluster);\n         } \n       }\n     } catch (InterruptedException ie) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "3b5f84bfe1fb6d4c0920e78cfbc20d3de32c28e1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/RunningJob.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/RunningJob.java",
                "patch": "@@ -193,7 +193,7 @@\n   /**\n    * Gets the counters for this job.\n    * \n-   * @return the counters for this job.\n+   * @return the counters for this job or null if the job has been retired.\n    * @throws IOException\n    */\n   public Counters getCounters() throws IOException;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "sha": "b3af4f6c98d5ebe15971387e23173e2e4a0513a8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "patch": "@@ -230,7 +230,7 @@ public Counters getJobCounters(JobID jobid)\n   \n   /**\n    * Get task completion events for the jobid, starting from fromEventId. \n-   * Returns empty aray if no events are available. \n+   * Returns empty array if no events are available. \n    * @param jobid job id \n    * @param fromEventId event id to start from.\n    * @param maxEvents the max number of events we want to look at ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "sha": "80e556bac2f3f4929cd313ea3e21c4a4839719c9",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.Test;\n+import static org.mockito.Mockito.*;\n+\n+\n+public class TestNetworkedJob {\n+\n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testGetNullCounters() throws Exception {\n+    //mock creation\n+    Job mockJob = mock(Job.class);\n+    RunningJob underTest = new JobClient.NetworkedJob(mockJob); \n+\n+    when(mockJob.getCounters()).thenReturn(null);\n+    assertNull(underTest.getCounters());\n+    //verification\n+    verify(mockJob).getCounters();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "sha": "b6565e28956d7446628aae1da838b930a83313db",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java?ref=2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3",
                "deletions": 5,
                "filename": "src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "patch": "@@ -129,7 +129,7 @@ public void testFailedTaskJobStatus()\n     }\n     Assert.assertTrue(\"Task has not been started for 1 min.\", counter != 60);\n \n-    NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());\n+    NetworkedJob networkJob = new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster);\n     TaskID tID = TaskID.downgrade(taskInfo.getTaskID());\n     TaskAttemptID taskAttID = new TaskAttemptID(tID, 0);\n     networkJob.killTask(taskAttID, false);\n@@ -245,7 +245,7 @@ public void testDirCleanupAfterTaskKilled()\n       filesStatus = ttClient.listStatus(localTaskDir, true);\n       if (filesStatus.length > 0) {\n         isTempFolderExists = true;\n-        NetworkedJob networkJob = jobClient.new NetworkedJob(jInfo.getStatus());\n+        NetworkedJob networkJob = new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster);\n         networkJob.killTask(taskAttID, false);\n         break;\n       }\n@@ -558,7 +558,7 @@ public void testAllTaskAttemptKill() throws Exception {\n             taskIdKilled = taskid.toString();\n             taskAttemptID = new TaskAttemptID(taskid, i);\n             LOG.info(\"taskAttemptid going to be killed is : \" + taskAttemptID);\n-            (jobClient.new NetworkedJob(jInfo.getStatus())).killTask(\n+            (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(\n                 taskAttemptID, true);\n             checkTaskCompletionEvent(taskAttemptID, jInfo);\n             break;\n@@ -568,7 +568,7 @@ public void testAllTaskAttemptKill() throws Exception {\n               LOG\n                   .info(\"taskAttemptid going to be killed is : \"\n                       + taskAttemptID);\n-              (jobClient.new NetworkedJob(jInfo.getStatus())).killTask(\n+              (new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)).killTask(\n                   taskAttemptID, true);\n               checkTaskCompletionEvent(taskAttemptID, jInfo);\n               break;\n@@ -611,7 +611,7 @@ public void checkTaskCompletionEvent(\n     int count = 0;\n     while (!match) {\n       TaskCompletionEvent[] taskCompletionEvents =\n-          jobClient.new NetworkedJob(jInfo.getStatus())\n+          new JobClient.NetworkedJob(jInfo.getStatus(),jobClient.cluster)\n               .getTaskCompletionEvents(0);\n       for (TaskCompletionEvent taskCompletionEvent : taskCompletionEvents) {\n         if ((taskCompletionEvent.getTaskAttemptId().toString())",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2c80616ea8adcfe3e2db598eb6991d84e0dcfbc3/src/test/system/test/org/apache/hadoop/mapred/TestTaskKilling.java",
                "sha": "d84f41a547b710bcec7bebea5f1ed74266917589",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters.\nContributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1127444 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/64b55c391e197a8a72d40da7be50fdcb9f8748b9",
        "patched_files": [
            "JobClient.java",
            "ClientProtocol.java",
            "CHANGES.txt",
            "RunningJob.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java",
            "TestTaskKilling.java",
            "TestNetworkedJob.java"
        ]
    },
    "hadoop-common_2d5ca82": {
        "bug_id": "hadoop-common_2d5ca82",
        "commit": "https://github.com/apache/hadoop-common/commit/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -663,6 +663,8 @@ Release 2.1.0-beta - 2013-07-02\n     mechanisms are enabled and thus fix YARN/MR test failures after HADOOP-9421.\n     (Daryn Sharp and Vinod Kumar Vavilapalli via vinodkv)\n \n+    YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n+\n   BREAKDOWN OF HADOOP-8562 SUBTASKS AND RELATED JIRAS\n \n     YARN-158. Yarn creating package-info.java must not depend on sh.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/CHANGES.txt",
                "sha": "a0e3a9b75e4a6f22677ddb2bea8c2e4a0fc80283",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -801,9 +801,10 @@ private synchronized FiCaSchedulerApp getApplication(\n     if (reservedContainer != null) {\n       FiCaSchedulerApp application = \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return \n-          assignReservedContainer(application, node, reservedContainer, \n-              clusterResource); \n+      synchronized (application) {\n+        return assignReservedContainer(application, node, reservedContainer,\n+          clusterResource);\n+      }\n     }\n     \n     // Try to assign containers to applications in order",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dbfa7444183198dd512254b8ff5daaa6b7208180",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=2d5ca8201ccaeca6b6edb4a314e0bd01317f3710",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceRequest;\n+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger;\n@@ -426,6 +427,16 @@ public synchronized void unreserve(FiCaSchedulerNode node, Priority priority) {\n       this.reservedContainers.remove(priority);\n     }\n     \n+    // reservedContainer should not be null here\n+    if (reservedContainer == null) {\n+      String errorMesssage =\n+          \"Application \" + getApplicationId() + \" is trying to unreserve \"\n+              + \" on node \" + node + \", currently has \"\n+              + reservedContainers.size() + \" at priority \" + priority\n+              + \"; currentReservation \" + currentReservation;\n+      LOG.warn(errorMesssage);\n+      throw new YarnRuntimeException(errorMesssage);\n+    }\n     // Reset the re-reservation count\n     resetReReservations(priority);\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2d5ca8201ccaeca6b6edb4a314e0bd01317f3710/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "8e2020abc79259fa89228678556aead46ddddbc6",
                "status": "modified"
            }
        ],
        "message": "YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499886 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e0b0ed18e41da6362ad88ba7c86f3aedfa47e6bd",
        "patched_files": [
            "LeafQueue.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop-common_2f4e2a8": {
        "bug_id": "hadoop-common_2f4e2a8",
        "commit": "https://github.com/apache/hadoop-common/commit/2f4e2a8c45c764553db8cf17aeb074384cfa433e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2f4e2a8c45c764553db8cf17aeb074384cfa433e/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=2f4e2a8c45c764553db8cf17aeb074384cfa433e",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -96,6 +96,9 @@ Trunk (unreleased changes)\n \n     HDFS-825. Build fails to pull latest hadoop-core-* artifacts (cos)\n \n+    HDFS-812. FSNamesystem#internalReleaseLease throws NullPointerException on\n+    a single-block file's lease recovery. (cos)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2f4e2a8c45c764553db8cf17aeb074384cfa433e/CHANGES.txt",
                "sha": "8ed2191be9822ec88aba585b448686f8ee36fe02",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=2f4e2a8c45c764553db8cf17aeb074384cfa433e",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1953,8 +1953,17 @@ boolean internalReleaseLease(\n     BlockInfoUnderConstruction lastBlock = pendingFile.getLastBlock();\n     BlockUCState lastBlockState = lastBlock.getBlockUCState();\n     BlockInfo penultimateBlock = pendingFile.getPenultimateBlock();\n-    BlockUCState penultimateBlockState = (penultimateBlock == null ?\n-        BlockUCState.COMPLETE : penultimateBlock.getBlockUCState());\n+    boolean penultimateBlockMinReplication;\n+    BlockUCState penultimateBlockState;\n+    if (penultimateBlock == null) {\n+      penultimateBlockState = BlockUCState.COMPLETE;\n+      // If penultimate block doesn't exist then its minReplication is met\n+      penultimateBlockMinReplication = true;\n+    } else {\n+      penultimateBlockState = BlockUCState.COMMITTED;\n+      penultimateBlockMinReplication = \n+        blockManager.checkMinReplication(penultimateBlock);\n+    }\n     assert penultimateBlockState == BlockUCState.COMPLETE ||\n            penultimateBlockState == BlockUCState.COMMITTED :\n            \"Unexpected state of penultimate block in \" + src;\n@@ -1965,7 +1974,7 @@ boolean internalReleaseLease(\n       break;\n     case COMMITTED:\n       // Close file if committed blocks are minimally replicated\n-      if(blockManager.checkMinReplication(penultimateBlock) &&\n+      if(penultimateBlockMinReplication &&\n           blockManager.checkMinReplication(lastBlock)) {\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n         NameNode.stateChangeLog.warn(\"BLOCK*\"",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "29d51bf3257ee747b01c4b2c25e9bede58679940",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java?ref=2f4e2a8c45c764553db8cf17aeb074384cfa433e",
                "deletions": 13,
                "filename": "src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java",
                "patch": "@@ -157,6 +157,27 @@ public void testInternalReleaseLease_COMM_COMM () throws IOException {\n     assertTrue(\"FSNamesystem.internalReleaseLease suppose to throw \" +\n       \"AlreadyBeingCreatedException here\", false);\n   }\n+\n+  /**\n+   * Mocks FSNamesystem instance, adds an empty file with 0 blocks\n+   * and invokes lease recovery method. \n+   * \n+   */\n+  @Test\n+  public void testInternalReleaseLease_0blocks () throws IOException {\n+    LOG.debug(\"Running \" + GenericTestUtils.getMethodName());\n+    LeaseManager.Lease lm = mock(LeaseManager.Lease.class);\n+    Path file = \n+      spy(new Path(\"/\" + GenericTestUtils.getMethodName() + \"_test.dat\"));\n+    DatanodeDescriptor dnd = mock(DatanodeDescriptor.class);\n+    PermissionStatus ps =\n+      new PermissionStatus(\"test\", \"test\", new FsPermission((short)0777));\n+\n+    mockFileBlocks(0, null, null, file, dnd, ps, false);\n+\n+    assertTrue(\"True has to be returned in this case\",\n+      fsn.internalReleaseLease(lm, file.toString(), null));\n+  }\n   \n   /**\n    * Mocks FSNamesystem instance, adds an empty file with 1 block\n@@ -346,16 +367,6 @@ private void mockFileBlocks(int fileBlocksNumber,\n     when(b.getBlockUCState()).thenReturn(penUltState);\n     when(b1.getBlockUCState()).thenReturn(lastState);\n     BlockInfo[] blocks;\n-    switch (fileBlocksNumber) {\n-      case 0:\n-        blocks = new BlockInfo[0];\n-        break;\n-      case 1:\n-        blocks = new BlockInfo[]{b1};\n-        break;\n-      default:\n-        blocks = new BlockInfo[]{b, b1};\n-    }\n \n     FSDirectory fsDir = mock(FSDirectory.class);\n     INodeFileUnderConstruction iNFmock = mock(INodeFileUnderConstruction.class);\n@@ -368,14 +379,29 @@ private void mockFileBlocks(int fileBlocksNumber,\n     when(fsn.getFSImage().getEditLog()).thenReturn(editLog);\n     fsn.getFSImage().setFSNamesystem(fsn);\n     \n+    switch (fileBlocksNumber) {\n+      case 0:\n+        blocks = new BlockInfo[0];\n+        break;\n+      case 1:\n+        blocks = new BlockInfo[]{b1};\n+        when(iNFmock.getLastBlock()).thenReturn(b1);\n+        break;\n+      default:\n+        when(iNFmock.getPenultimateBlock()).thenReturn(b);\n+        when(iNFmock.getLastBlock()).thenReturn(b1);\n+        blocks = new BlockInfo[]{b, b1};\n+    }\n+    \n     when(iNFmock.getBlocks()).thenReturn(blocks);\n-    when(iNFmock.numBlocks()).thenReturn(2);\n-    when(iNFmock.getPenultimateBlock()).thenReturn(b);\n-    when(iNFmock.getLastBlock()).thenReturn(b1);\n+    when(iNFmock.numBlocks()).thenReturn(blocks.length);\n     when(iNFmock.isUnderConstruction()).thenReturn(true);\n+    when(iNFmock.convertToInodeFile()).thenReturn(iNFmock);    \n     fsDir.addFile(file.toString(), ps, (short)3, 1l, \"test\", \n       \"test-machine\", dnd, 1001l);\n \n+    fsn.leaseManager = mock(LeaseManager.class);\n+    fsn.leaseManager.addLease(\"mock-lease\", file.toString());\n     if (setStoredBlock) {\n       when(b1.getINode()).thenReturn(iNFmock);\n       fsn.blockManager.blocksMap.addINode(b1, iNFmock);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2f4e2a8c45c764553db8cf17aeb074384cfa433e/src/test/unit/org/apache/hadoop/hdfs/server/namenode/TestNNLeaseRecovery.java",
                "sha": "f6125b328d109087f3549a1f4a5617a2c87b7c0d",
                "status": "modified"
            }
        ],
        "message": "HDFS-812. FSNamesystem#internalReleaseLease throws NullPointerException on a single-block file's lease recovery. Contributed by Konstantin Boudnik\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@891106 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/c344d269c457117eae9de38a6e91b60310f9a655",
        "patched_files": [
            "CHANGES.txt",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNNLeaseRecovery.java"
        ]
    },
    "hadoop-common_320308a": {
        "bug_id": "hadoop-common_320308a",
        "commit": "https://github.com/apache/hadoop-common/commit/320308a92cc4f4ad118ad3703c97b0cfc34a4539",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/320308a92cc4f4ad118ad3703c97b0cfc34a4539/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=320308a92cc4f4ad118ad3703c97b0cfc34a4539",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -207,6 +207,9 @@ Trunk (unreleased changes)\n     HADOOP-6549. TestDoAsEffectiveUser should use ip address of the host\n      for superuser ip check(jnp via boryas)\n \n+    HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns\n+    null. (hairong)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/320308a92cc4f4ad118ad3703c97b0cfc34a4539/CHANGES.txt",
                "sha": "7354c4cec03c5288df758a25a5ba3d2d7bfb07fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/java/org/apache/hadoop/ipc/RPC.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/ipc/RPC.java?ref=320308a92cc4f4ad118ad3703c97b0cfc34a4539",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/ipc/RPC.java",
                "patch": "@@ -244,8 +244,9 @@ public static Object getProxy(Class protocol, long clientVersion,\n    * @param proxy the proxy to be stopped\n    */\n   public static void stopProxy(Object proxy) {\n-    if (proxy!=null) {\n-      getProxyEngine(proxy).stopProxy(proxy);\n+    RpcEngine rpcEngine;\n+    if (proxy!=null && (rpcEngine = getProxyEngine(proxy)) != null) {\n+      rpcEngine.stopProxy(proxy);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/java/org/apache/hadoop/ipc/RPC.java",
                "sha": "36874c511dcca84e6dfa25fd0967d8e5af2a6cab",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/ipc/TestRPC.java?ref=320308a92cc4f4ad118ad3703c97b0cfc34a4539",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "patch": "@@ -39,6 +39,8 @@\n import org.apache.hadoop.security.authorize.Service;\n import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;\n \n+import static org.mockito.Mockito.*;\n+\n /** Unit tests for RPC. */\n public class TestRPC extends TestCase {\n   private static final String ADDRESS = \"0.0.0.0\";\n@@ -392,6 +394,14 @@ public void testNoPings() throws Exception {\n     conf.setBoolean(\"ipc.client.ping\", false);\n     new TestRPC(\"testnoPings\").testCalls(conf);\n   }\n+\n+  /**\n+   * Test stopping a non-registered proxy\n+   * @throws Exception\n+   */\n+  public void testStopNonRegisteredProxy() throws Exception {\n+    RPC.stopProxy(mock(TestProtocol.class));\n+  }\n   \n   public static void main(String[] args) throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/320308a92cc4f4ad118ad3703c97b0cfc34a4539/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "sha": "0bb3f8dc5c0b3c0820081e6e849e2e1f18e2add8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns null. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@911134 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/178095ad514d5b41bb0215b7d7d6ef8b852049b9",
        "patched_files": [
            "RPC.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRPC.java"
        ]
    },
    "hadoop-common_332f922": {
        "bug_id": "hadoop-common_332f922",
        "commit": "https://github.com/apache/hadoop-common/commit/332f9226ab505b8b1c32887f1bea66c6c36fdc1e",
        "file": [
            {
                "additions": 161,
                "blob_url": "https://github.com/apache/hadoop-common/blob/332f9226ab505b8b1c32887f1bea66c6c36fdc1e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java",
                "changes": 161,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java?ref=332f9226ab505b8b1c32887f1bea66c6c36fdc1e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java",
                "patch": "@@ -0,0 +1,161 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io.compress;\n+\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.DataInputBuffer;\n+import org.apache.hadoop.io.DataOutputBuffer;\n+import org.apache.hadoop.io.RandomDatum;\n+import org.apache.hadoop.io.compress.zlib.ZlibFactory;\n+import org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionLevel;\n+import org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionStrategy;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import junit.framework.TestCase;\n+\n+public class TestCompressionStreamReuse extends TestCase {\n+  private static final Log LOG = LogFactory\n+      .getLog(TestCompressionStreamReuse.class);\n+\n+  private Configuration conf = new Configuration();\n+  private int count = 10000;\n+  private int seed = new Random().nextInt();\n+\n+  public void testBZip2Codec() throws IOException {\n+    resetStateTest(conf, seed, count,\n+        \"org.apache.hadoop.io.compress.BZip2Codec\");\n+  }\n+\n+  public void testGzipCompressStreamReuse() throws IOException {\n+    resetStateTest(conf, seed, count,\n+        \"org.apache.hadoop.io.compress.GzipCodec\");\n+  }\n+\n+  public void testGzipCompressStreamReuseWithParam() throws IOException {\n+    Configuration conf = new Configuration(this.conf);\n+    ZlibFactory\n+        .setCompressionLevel(conf, CompressionLevel.BEST_COMPRESSION);\n+    ZlibFactory.setCompressionStrategy(conf,\n+        CompressionStrategy.HUFFMAN_ONLY);\n+    resetStateTest(conf, seed, count,\n+        \"org.apache.hadoop.io.compress.GzipCodec\");\n+  }\n+\n+  private static void resetStateTest(Configuration conf, int seed, int count,\n+      String codecClass) throws IOException {\n+    // Create the codec\n+    CompressionCodec codec = null;\n+    try {\n+      codec = (CompressionCodec) ReflectionUtils.newInstance(conf\n+          .getClassByName(codecClass), conf);\n+    } catch (ClassNotFoundException cnfe) {\n+      throw new IOException(\"Illegal codec!\");\n+    }\n+    LOG.info(\"Created a Codec object of type: \" + codecClass);\n+\n+    // Generate data\n+    DataOutputBuffer data = new DataOutputBuffer();\n+    RandomDatum.Generator generator = new RandomDatum.Generator(seed);\n+    for (int i = 0; i < count; ++i) {\n+      generator.next();\n+      RandomDatum key = generator.getKey();\n+      RandomDatum value = generator.getValue();\n+\n+      key.write(data);\n+      value.write(data);\n+    }\n+    LOG.info(\"Generated \" + count + \" records\");\n+\n+    // Compress data\n+    DataOutputBuffer compressedDataBuffer = new DataOutputBuffer();\n+    DataOutputStream deflateOut = new DataOutputStream(\n+        new BufferedOutputStream(compressedDataBuffer));\n+    CompressionOutputStream deflateFilter = codec\n+        .createOutputStream(deflateOut);\n+    deflateFilter.write(data.getData(), 0, data.getLength());\n+    deflateFilter.finish();\n+    deflateFilter.flush();\n+    LOG.info(\"Finished compressing data\");\n+\n+    // reset deflator\n+    deflateFilter.resetState();\n+    LOG.info(\"Finished reseting deflator\");\n+\n+    // re-generate data\n+    data.reset();\n+    generator = new RandomDatum.Generator(seed);\n+    for (int i = 0; i < count; ++i) {\n+      generator.next();\n+      RandomDatum key = generator.getKey();\n+      RandomDatum value = generator.getValue();\n+\n+      key.write(data);\n+      value.write(data);\n+    }\n+    DataInputBuffer originalData = new DataInputBuffer();\n+    DataInputStream originalIn = new DataInputStream(\n+        new BufferedInputStream(originalData));\n+    originalData.reset(data.getData(), 0, data.getLength());\n+\n+    // re-compress data\n+    compressedDataBuffer.reset();\n+    deflateOut = new DataOutputStream(new BufferedOutputStream(\n+        compressedDataBuffer));\n+    deflateFilter = codec.createOutputStream(deflateOut);\n+\n+    deflateFilter.write(data.getData(), 0, data.getLength());\n+    deflateFilter.finish();\n+    deflateFilter.flush();\n+    LOG.info(\"Finished re-compressing data\");\n+\n+    // De-compress data\n+    DataInputBuffer deCompressedDataBuffer = new DataInputBuffer();\n+    deCompressedDataBuffer.reset(compressedDataBuffer.getData(), 0,\n+        compressedDataBuffer.getLength());\n+    CompressionInputStream inflateFilter = codec\n+        .createInputStream(deCompressedDataBuffer);\n+    DataInputStream inflateIn = new DataInputStream(\n+        new BufferedInputStream(inflateFilter));\n+\n+    // Check\n+    for (int i = 0; i < count; ++i) {\n+      RandomDatum k1 = new RandomDatum();\n+      RandomDatum v1 = new RandomDatum();\n+      k1.readFields(originalIn);\n+      v1.readFields(originalIn);\n+\n+      RandomDatum k2 = new RandomDatum();\n+      RandomDatum v2 = new RandomDatum();\n+      k2.readFields(inflateIn);\n+      v2.readFields(inflateIn);\n+      assertTrue(\n+          \"original and compressed-then-decompressed-output not equal\",\n+          k1.equals(k2) && v1.equals(v2));\n+    }\n+    LOG.info(\"SUCCESS! Completed checking \" + count + \" records\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/332f9226ab505b8b1c32887f1bea66c6c36fdc1e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java",
                "sha": "2c285944539da847caa071b80c54e4fa01314872",
                "status": "added"
            }
        ],
        "message": "HADOOP-8419. Fixed GzipCode NPE reset for IBM JDK. (Yu Li via eyang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431740 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/df593f55a37c2b7b60a7c5284395d439c6f97812",
        "patched_files": [],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCompressionStreamReuse.java"
        ]
    },
    "hadoop-common_3407cdc": {
        "bug_id": "hadoop-common_3407cdc",
        "commit": "https://github.com/apache/hadoop-common/commit/3407cdcbe9b272421574682332bb634900497ed2",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/3407cdcbe9b272421574682332bb634900497ed2/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=3407cdcbe9b272421574682332bb634900497ed2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -426,6 +426,8 @@ Release 0.22.0 - Unreleased\n \n     HDFS-1523. TestLargeBlock is failing on trunk. (cos)\n \n+    HDFS-1502. TestBlockRecovery triggers NPE in assert. (hairong via cos)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/3407cdcbe9b272421574682332bb634900497ed2/CHANGES.txt",
                "sha": "bc620cd5cb35b960aa8cb56cc24a0c5a94259f0f",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/3407cdcbe9b272421574682332bb634900497ed2/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java?ref=3407cdcbe9b272421574682332bb634900497ed2",
                "deletions": 11,
                "filename": "src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java",
                "patch": "@@ -128,7 +128,8 @@ public void tearDown() throws IOException {\n   private void testSyncReplicas(ReplicaRecoveryInfo replica1, \n       ReplicaRecoveryInfo replica2,\n       InterDatanodeProtocol dn1,\n-      InterDatanodeProtocol dn2) throws IOException {\n+      InterDatanodeProtocol dn2,\n+      long expectLen) throws IOException {\n     \n     DatanodeInfo[] locs = new DatanodeInfo[]{\n         mock(DatanodeInfo.class), mock(DatanodeInfo.class)};\n@@ -141,6 +142,13 @@ private void testSyncReplicas(ReplicaRecoveryInfo replica1,\n         new DatanodeID(\"aa\", \"bb\", 11, 22), dn2, replica2);\n     syncList.add(record1);\n     syncList.add(record2);\n+    \n+    when(dn1.updateReplicaUnderRecovery((Block)anyObject(), anyLong(), \n+        anyLong())).thenReturn(new Block(block.getBlockId(), \n+            expectLen, block.getGenerationStamp()));\n+    when(dn2.updateReplicaUnderRecovery((Block)anyObject(), anyLong(), \n+        anyLong())).thenReturn(new Block(block.getBlockId(), \n+            expectLen, block.getGenerationStamp()));\n     dn.syncBlock(rBlock, syncList);\n   }\n   \n@@ -162,7 +170,7 @@ public void testFinalizedReplicas () throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);    \n \n@@ -173,7 +181,7 @@ public void testFinalizedReplicas () throws IOException {\n         REPLICA_LEN2, GEN_STAMP-2, ReplicaState.FINALIZED);\n \n     try {\n-      testSyncReplicas(replica1, replica2, dn1, dn2);\n+      testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n       Assert.fail(\"Two finalized replicas should not have different lengthes!\");\n     } catch (IOException e) {\n       Assert.assertTrue(e.getMessage().startsWith(\n@@ -201,7 +209,7 @@ public void testFinalizedRbwReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     \n@@ -214,7 +222,7 @@ public void testFinalizedRbwReplicas() throws IOException {\n     dn1 = mock(InterDatanodeProtocol.class);\n     dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -240,7 +248,7 @@ public void testFinalizedRwrReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -254,7 +262,7 @@ public void testFinalizedRwrReplicas() throws IOException {\n     dn1 = mock(InterDatanodeProtocol.class);\n     dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);\n@@ -278,8 +286,8 @@ public void testRBWReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n     long minLen = Math.min(REPLICA_LEN1, REPLICA_LEN2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, minLen);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);    \n   }\n@@ -302,7 +310,7 @@ public void testRBW_RWRReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, REPLICA_LEN1);\n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, REPLICA_LEN1);\n     verify(dn2, never()).updateReplicaUnderRecovery(\n         block, RECOVERY_ID, REPLICA_LEN1);    \n@@ -326,9 +334,9 @@ public void testRWRReplicas() throws IOException {\n     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);\n     InterDatanodeProtocol dn2 = mock(InterDatanodeProtocol.class);\n \n-    testSyncReplicas(replica1, replica2, dn1, dn2);\n-    \n     long minLen = Math.min(REPLICA_LEN1, REPLICA_LEN2);\n+    testSyncReplicas(replica1, replica2, dn1, dn2, minLen);\n+    \n     verify(dn1).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);\n     verify(dn2).updateReplicaUnderRecovery(block, RECOVERY_ID, minLen);    \n   }  ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/3407cdcbe9b272421574682332bb634900497ed2/src/test/unit/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java",
                "sha": "9061781d9c867f6df31677e72067f83b2330bcf5",
                "status": "modified"
            }
        ],
        "message": "HDFS-1502. TestBlockRecovery triggers NPE in assert. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1042517 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/15ce625e5f269cfe06803c54d21d5dff46ab5425",
        "patched_files": [
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockRecovery.java"
        ]
    },
    "hadoop-common_36cabb6": {
        "bug_id": "hadoop-common_36cabb6",
        "commit": "https://github.com/apache/hadoop-common/commit/36cabb654e315d62182b47c6bed6ce804a535e3e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -509,6 +509,10 @@ Release 0.23.0 - Unreleased\n     HADOOP-7360. Preserve relative paths that do not contain globs in FsShell.\n     (Daryn Sharp and Kihwal Lee via szetszwo)\n \n+    HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the\n+    destination directory does not exist.  (John George and Daryn Sharp\n+    via szetszwo)\n+\n   OPTIMIZATIONS\n   \n     HADOOP-7333. Performance improvement in PureJavaCrc32. (Eric Caspole",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "0b0e1beb76248ba3b27554efd8cb2bd9b239c27d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "patch": "@@ -55,6 +55,7 @@\n   protected int exitCode = 0;\n   protected int numErrors = 0;\n   protected boolean recursive = false;\n+  private int depth = 0;\n   protected ArrayList<Exception> exceptions = new ArrayList<Exception>();\n \n   private static final Log LOG = LogFactory.getLog(Command.class);\n@@ -86,6 +87,10 @@ protected boolean isRecursive() {\n     return recursive;\n   }\n \n+  protected int getDepth() {\n+    return depth;\n+  }\n+  \n   /** \n    * Execute the command on the input path\n    * \n@@ -269,6 +274,7 @@ protected void processArgument(PathData item) throws IOException {\n   protected void processPathArgument(PathData item) throws IOException {\n     // null indicates that the call is not via recursion, ie. there is\n     // no parent directory that was expanded\n+    depth = 0;\n     processPaths(null, item);\n   }\n   \n@@ -326,7 +332,12 @@ protected void processPath(PathData item) throws IOException {\n    *  @throws IOException if anything goes wrong...\n    */\n   protected void recursePath(PathData item) throws IOException {\n-    processPaths(item, item.getDirectoryContents());\n+    try {\n+      depth++;\n+      processPaths(item, item.getDirectoryContents());\n+    } finally {\n+      depth--;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "sha": "b24d47e02b1027b5decd17b36d9ab8a71b589875",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 26,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.util.LinkedList;\n \n+import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathIsDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIsNotDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathNotFoundException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n+import org.apache.hadoop.io.IOUtils;\n \n /**\n  * Provides: argument processing to ensure the destination is valid\n@@ -106,51 +111,136 @@ protected void processArguments(LinkedList<PathData> args)\n   }\n \n   @Override\n-  protected void processPaths(PathData parent, PathData ... items)\n+  protected void processPathArgument(PathData src)\n   throws IOException {\n+    if (src.stat.isDirectory() && src.fs.equals(dst.fs)) {\n+      PathData target = getTargetPath(src);\n+      String srcPath = src.fs.makeQualified(src.path).toString();\n+      String dstPath = dst.fs.makeQualified(target.path).toString();\n+      if (dstPath.equals(srcPath)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"are identical\");\n+        e.setTargetPath(dstPath.toString());\n+        throw e;\n+      }\n+      if (dstPath.startsWith(srcPath+Path.SEPARATOR)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"is a subdirectory of itself\");\n+        e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+    }\n+    super.processPathArgument(src);\n+  }\n+\n+  @Override\n+  protected void processPath(PathData src) throws IOException {\n+    processPath(src, getTargetPath(src));\n+  }\n+  \n+  /**\n+   * Called with a source and target destination pair\n+   * @param src for the operation\n+   * @param target for the operation\n+   * @throws IOException if anything goes wrong\n+   */\n+  protected void processPath(PathData src, PathData dst) throws IOException {\n+    if (src.stat.isSymlink()) {\n+      // TODO: remove when FileContext is supported, this needs to either\n+      // copy the symlink or deref the symlink\n+      throw new PathOperationException(src.toString());        \n+    } else if (src.stat.isFile()) {\n+      copyFileToTarget(src, dst);\n+    } else if (src.stat.isDirectory() && !isRecursive()) {\n+      throw new PathIsDirectoryException(src.toString());\n+    }\n+  }\n+\n+  @Override\n+  protected void recursePath(PathData src) throws IOException {\n     PathData savedDst = dst;\n     try {\n       // modify dst as we descend to append the basename of the\n       // current directory being processed\n-      if (parent != null) dst = dst.getPathDataForChild(parent);\n-      super.processPaths(parent, items);\n+      dst = getTargetPath(src);\n+      if (dst.exists) {\n+        if (!dst.stat.isDirectory()) {\n+          throw new PathIsNotDirectoryException(dst.toString());\n+        }\n+      } else {\n+        if (!dst.fs.mkdirs(dst.path)) {\n+          // too bad we have no clue what failed\n+          PathIOException e = new PathIOException(dst.toString());\n+          e.setOperation(\"mkdir\");\n+          throw e;\n+        }    \n+        dst.refreshStatus(); // need to update stat to know it exists now\n+      }      \n+      super.recursePath(src);\n     } finally {\n       dst = savedDst;\n     }\n   }\n   \n-  @Override\n-  protected void processPath(PathData src) throws IOException {\n+  protected PathData getTargetPath(PathData src) throws IOException {\n     PathData target;\n-    // if the destination is a directory, make target a child path,\n-    // else use the destination as-is\n-    if (dst.exists && dst.stat.isDirectory()) {\n+    // on the first loop, the dst may be directory or a file, so only create\n+    // a child path if dst is a dir; after recursion, it's always a dir\n+    if ((getDepth() > 0) || (dst.exists && dst.stat.isDirectory())) {\n       target = dst.getPathDataForChild(src);\n     } else {\n       target = dst;\n     }\n-    if (target.exists && !overwrite) {\n+    return target;\n+  }\n+  \n+  /**\n+   * Copies the source file to the target.\n+   * @param src item to copy\n+   * @param target where to copy the item\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyFileToTarget(PathData src, PathData target) throws IOException {\n+    copyStreamToTarget(src.fs.open(src.path), target);\n+  }\n+  \n+  /**\n+   * Copies the stream contents to a temporary file.  If the copy is\n+   * successful, the temporary file will be renamed to the real path,\n+   * else the temporary file will be deleted.\n+   * @param in the input stream for the copy\n+   * @param target where to store the contents of the stream\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyStreamToTarget(InputStream in, PathData target)\n+  throws IOException {\n+    if (target.exists && (target.stat.isDirectory() || !overwrite)) {\n       throw new PathExistsException(target.toString());\n     }\n-\n-    try { \n-      // invoke processPath with both a source and resolved target\n-      processPath(src, target);\n-    } catch (PathIOException e) {\n-      // add the target unless it already has one\n-      if (e.getTargetPath() == null) {\n+    PathData tempFile = null;\n+    try {\n+      tempFile = target.createTempFile(target+\"._COPYING_\");\n+      FSDataOutputStream out = target.fs.create(tempFile.path, true);\n+      IOUtils.copyBytes(in, out, getConf(), true);\n+      // the rename method with an option to delete the target is deprecated\n+      if (target.exists && !target.fs.delete(target.path, false)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(target.toString());\n+        e.setOperation(\"delete\");\n+        throw e;\n+      }\n+      if (!tempFile.fs.rename(tempFile.path, target.path)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(tempFile.toString());\n+        e.setOperation(\"rename\");\n         e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+      tempFile = null;\n+    } finally {\n+      if (tempFile != null) {\n+        tempFile.fs.delete(tempFile.path, false);\n       }\n-      throw e;\n     }\n   }\n-\n-  /**\n-   * Called with a source and target destination pair\n-   * @param src for the operation\n-   * @param target for the operation\n-   * @throws IOException if anything goes wrong\n-   */\n-  protected abstract void processPath(PathData src, PathData target)\n-  throws IOException;\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "sha": "6b3b40389f9f72528cdc248ee23af4af559df101",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 73,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "patch": "@@ -26,13 +26,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.fs.ChecksumFileSystem;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileUtil;\n-import org.apache.hadoop.fs.LocalFileSystem;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n-import org.apache.hadoop.io.IOUtils;\n \n /** Various commands for copy files */\n @InterfaceAudience.Private\n@@ -95,18 +89,10 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"f\");\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n+      // should have a -r option\n+      setRecursive(true);\n       getRemoteDestination(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      if (!FileUtil.copy(src.fs, src.path, target.fs, target.path, false, overwrite, getConf())) {\n-        // we have no idea what the error is...  FileUtils masks it and in\n-        // some cases won't even report an error\n-        throw new PathIOException(src.toString());\n-      }\n-    }\n   }\n   \n   /** \n@@ -126,7 +112,6 @@ protected void processPath(PathData src, PathData target)\n      * It must be at least three characters long, required by\n      * {@link java.io.File#createTempFile(String, String, File)}.\n      */\n-    private static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n     private boolean copyCrc;\n     private boolean verifyChecksum;\n \n@@ -144,7 +129,7 @@ protected void processOptions(LinkedList<String> args)\n     }\n \n     @Override\n-    protected void processPath(PathData src, PathData target)\n+    protected void copyFileToTarget(PathData src, PathData target)\n     throws IOException {\n       src.fs.setVerifyChecksum(verifyChecksum);\n \n@@ -153,41 +138,10 @@ protected void processPath(PathData src, PathData target)\n         copyCrc = false;\n       }      \n \n-      if (src.stat.isFile()) {\n-        // copy the file and maybe its crc\n-        copyFileToLocal(src, target);\n-        if (copyCrc) {\n-          copyFileToLocal(src.getChecksumFile(), target.getChecksumFile());\n-        }\n-      } else if (src.stat.isDirectory()) {\n-        // create the remote directory structure locally\n-        if (!target.toFile().mkdirs()) {\n-          throw new PathIOException(target.toString());\n-        }\n-      } else {\n-        throw new PathOperationException(src.toString());\n-      }\n-    }\n-\n-    private void copyFileToLocal(PathData src, PathData target)\n-    throws IOException {\n-      File targetFile = target.toFile();\n-      File tmpFile = FileUtil.createLocalTempFile(\n-          targetFile, COPYTOLOCAL_PREFIX, true);\n-      // too bad we can't tell exactly why it failed...\n-      if (!FileUtil.copy(src.fs, src.path, tmpFile, false, getConf())) {\n-        PathIOException e = new PathIOException(src.toString());\n-        e.setOperation(\"copy\");\n-        e.setTargetPath(tmpFile.toString());\n-        throw e;\n-      }\n-\n-      // too bad we can't tell exactly why it failed...\n-      if (!tmpFile.renameTo(targetFile)) {\n-        PathIOException e = new PathIOException(tmpFile.toString());\n-        e.setOperation(\"rename\");\n-        e.setTargetPath(targetFile.toString());\n-        throw e;\n+      super.copyFileToTarget(src, target);\n+      if (copyCrc) {\n+        // should we delete real file if crc copy fails?\n+        super.copyFileToTarget(src.getChecksumFile(), target.getChecksumFile());\n       }\n     }\n   }\n@@ -208,6 +162,8 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n       getRemoteDestination(args);\n+      // should have a -r option\n+      setRecursive(true);\n     }\n \n     // commands operating on local paths have no need for glob expansion\n@@ -223,30 +179,11 @@ protected void processArguments(LinkedList<PathData> args)\n     throws IOException {\n       // NOTE: this logic should be better, mimics previous implementation\n       if (args.size() == 1 && args.get(0).toString().equals(\"-\")) {\n-        if (dst.exists && !overwrite) {\n-          throw new PathExistsException(dst.toString());\n-        }\n-        copyFromStdin();\n+        copyStreamToTarget(System.in, getTargetPath(args.get(0)));\n         return;\n       }\n       super.processArguments(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      target.fs.copyFromLocalFile(false, overwrite, src.path, target.path);\n-    }\n-\n-    /** Copies from stdin to the destination file. */\n-    protected void copyFromStdin() throws IOException {\n-      FSDataOutputStream out = dst.fs.create(dst.path); \n-      try {\n-        IOUtils.copyBytes(System.in, out, getConf(), false);\n-      } finally {\n-        out.close();\n-      }\n-    }\n   }\n \n   public static class CopyFromLocal extends Put {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "sha": "066e5fdb899df7f517fb508451caf3a86f7caf68",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "patch": "@@ -182,6 +182,19 @@ public PathData getChecksumFile() throws IOException {\n     return new PathData(srcFs.getRawFileSystem(), srcPath.toString());\n   }\n \n+  /**\n+   * Returns a temporary file for this PathData with the given extension.\n+   * The file will be deleted on exit.\n+   * @param extension for the temporary file\n+   * @return PathData\n+   * @throws IOException shouldn't happen\n+   */\n+  public PathData createTempFile(String extension) throws IOException {\n+    PathData tmpFile = new PathData(fs, uri+\"._COPYING_\");\n+    fs.deleteOnExit(tmpFile.path);\n+    return tmpFile;\n+  }\n+\n   /**\n    * Returns a list of PathData objects of the items contained in the given\n    * directory.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "sha": "a3c88f1f2af2736442e7db320919298356c681b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the destination directory does not exist.  Contributed by John George and Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195760 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f3215940dc0fbbf0c75e744ede43605a42580d5a",
        "patched_files": [
            "CHANGES.txt",
            "PathData.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestPathData.java"
        ]
    },
    "hadoop-common_37ef56a": {
        "bug_id": "hadoop-common_37ef56a",
        "commit": "https://github.com/apache/hadoop-common/commit/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -171,6 +171,9 @@ Trunk (unreleased changes)\n \n     HDFS-1680. Fix TestBalancer. (szetszwo)\n \n+    HDFS-1705. Balancer command throws NullPointerException. (suresh via\n+    szetszwo)\n+\n Release 0.22.0 - Unreleased\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/CHANGES.txt",
                "sha": "3175574eecc7f888cf5475734b3a341b0385686f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java?ref=37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
                "patch": "@@ -1461,17 +1461,12 @@ public String toString() {\n   }\n \n   static class Cli extends Configured implements Tool {\n-    @Override\n-    public void setConf(Configuration conf) {\n-      super.setConf(conf);\n-      WIN_WIDTH = conf.getLong(\"dfs.balancer.movedWinWidth\", WIN_WIDTH);\n-    }\n-\n     /** Parse arguments and then run Balancer */\n     @Override\n     public int run(String[] args) {\n       final long startTime = Util.now();\n       final Configuration conf = getConf();\n+      WIN_WIDTH = conf.getLong(\"dfs.balancer.movedWinWidth\", WIN_WIDTH);\n \n       try {\n         checkReplicationPolicyCompatibility(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/37ef56a44cf74dd3c0991a8bc0ed6690ffc7387a/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
                "sha": "175107cce93c69f8736868eede0eef4b27554c63",
                "status": "modified"
            }
        ],
        "message": "HDFS-1705. Balancer command throws NullPointerException. Contributed by suresh\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/branches/HDFS-1052@1076479 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/c10ba8f02c34de23b520303d2907d96d5d2a8a52",
        "patched_files": [
            "CHANGES.txt",
            "Balancer.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBalancer.java"
        ]
    },
    "hadoop-common_400ef68": {
        "bug_id": "hadoop-common_400ef68",
        "commit": "https://github.com/apache/hadoop-common/commit/400ef68cbb5eae73ca0d80aecd0889256048330e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=400ef68cbb5eae73ca0d80aecd0889256048330e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -234,6 +234,10 @@ Trunk (Unreleased)\n     HADOOP-8815. RandomDatum needs to override hashCode().\n     (Brandon Li via suresh)\n \n+    HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the\n+    required context item is not configured\n+    (Brahma Reddy Battula via harsh)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "5b4066b80f22163dc57d54001f70fd088847f806",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java?ref=400ef68cbb5eae73ca0d80aecd0889256048330e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "patch": "@@ -265,6 +265,9 @@ public AllocatorPerContext(String contextCfgItemName) {\n     private synchronized void confChanged(Configuration conf) \n         throws IOException {\n       String newLocalDirs = conf.get(contextCfgItemName);\n+      if (null == newLocalDirs) {\n+        throw new IOException(contextCfgItemName + \" not configured\");\n+      }\n       if (!newLocalDirs.equals(savedLocalDirs)) {\n         localDirs = StringUtils.getTrimmedStrings(newLocalDirs);\n         localFS = FileSystem.getLocal(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "sha": "16a1c99b5c0ca566a6bce500f9ecdc44063a5bff",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java?ref=400ef68cbb5eae73ca0d80aecd0889256048330e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "patch": "@@ -293,6 +293,23 @@ public void testLocalPathForWriteDirCreation() throws IOException {\n     }\n   }\n \n+  /*\n+   * Test when mapred.local.dir not configured and called\n+   * getLocalPathForWrite\n+   */\n+  @Test\n+  public void testShouldNotthrowNPE() throws Exception {\n+    Configuration conf1 = new Configuration();\n+    try {\n+      dirAllocator.getLocalPathForWrite(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + CONTEXT + \" is not set\");\n+    } catch (IOException e) {\n+      assertEquals(CONTEXT + \" not configured\", e.getMessage());\n+    } catch (NullPointerException e) {\n+      fail(\"Lack of configuration should not have thrown an NPE.\");\n+    }\n+  }\n+\n   /** Test no side effect files are left over. After creating a temp\n    * temp file, remove both the temp file and its parent. Verify that\n    * no files or directories are left over as can happen when File objects",
                "raw_url": "https://github.com/apache/hadoop-common/raw/400ef68cbb5eae73ca0d80aecd0889256048330e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "sha": "7a4618e650b26f9419ce26022863a3a8c45027ae",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the required context item is not configured. Contributed by Brahma Reddy Battula. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389799 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d90e5090ba0df64546f2aaa7ddd9907fc337280b",
        "patched_files": [
            "LocalDirAllocator.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLocalDirAllocator.java"
        ]
    },
    "hadoop-common_408e44a": {
        "bug_id": "hadoop-common_408e44a",
        "commit": "https://github.com/apache/hadoop-common/commit/408e44ac2074c503caf21aa73113dc48584619bb",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/408e44ac2074c503caf21aa73113dc48584619bb/CHANGES.txt",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=408e44ac2074c503caf21aa73113dc48584619bb",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -341,6 +341,12 @@ Trunk (unreleased changes)\n     MAPREDUCE-1358. Avoid false positives in OutputLogFilter. (Todd Lipcon via\n     cdouglas)\n \n+    MAPREDUCE-1490. Fix a NullPointerException that could occur during \n+    instantiation and initialization of the DistributedRaidFileSystem. \n+    (Rodrigo Schmidt via dhruba)\n+\n+    cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/408e44ac2074c503caf21aa73113dc48584619bb/CHANGES.txt",
                "sha": "c1c8ef9bb508a8e5b749842085dbc5d0e0dbec6b",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java?ref=408e44ac2074c503caf21aa73113dc48584619bb",
                "deletions": 1,
                "filename": "src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java",
                "patch": "@@ -65,9 +65,17 @@\n   /* Initialize a Raid FileSystem\n    */\n   public void initialize(URI name, Configuration conf) throws IOException {\n-    super.initialize(name, conf);\n     this.conf = conf;\n \n+    Class<?> clazz = conf.getClass(\"fs.raid.underlyingfs.impl\",\n+        DistributedFileSystem.class);\n+    if (clazz == null) {\n+      throw new IOException(\"No FileSystem for fs.raid.underlyingfs.impl.\");\n+    }\n+    \n+    this.fs = (FileSystem)ReflectionUtils.newInstance(clazz, null); \n+    super.initialize(name, conf);\n+    \n     String alt = conf.get(\"hdfs.raid.locations\");\n     \n     // If no alternates are specified, then behave absolutely same as ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/java/org/apache/hadoop/hdfs/DistributedRaidFileSystem.java",
                "sha": "72cda6b9596cae00f5081babb7652897dabbcfaa",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java?ref=408e44ac2074c503caf21aa73113dc48584619bb",
                "deletions": 4,
                "filename": "src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.PrintWriter;\n+import java.net.URI;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.GregorianCalendar;\n@@ -177,10 +178,14 @@ public void testRaidDfs() throws Exception {\n \n       // filter all filesystem calls from client\n       Configuration clientConf = new Configuration(conf);\n-      clientConf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.dfs.DistributedRaidFileSystem\");\n-      DistributedRaidFileSystem raidfs = new DistributedRaidFileSystem(dfs);\n-      raidfs.initialize(dfs.getUri(), clientConf);\n-\n+      clientConf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedRaidFileSystem\");\n+      clientConf.set(\"fs.raid.underlyingfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+      URI dfsUri = dfs.getUri();\n+      FileSystem.closeAll();\n+      FileSystem raidfs = FileSystem.get(dfsUri, clientConf);\n+      \n+      assertTrue(\"raidfs not an instance of DistributedRaidFileSystem\",raidfs instanceof DistributedRaidFileSystem);\n+      \n       // corrupt first block of file\n       LOG.info(\"Corrupt first block of file\");\n       corruptBlock(file1, locations.get(0).getBlock());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/408e44ac2074c503caf21aa73113dc48584619bb/src/contrib/raid/src/test/org/apache/hadoop/hdfs/TestRaidDfs.java",
                "sha": "dab43e2c776a0206a126f685db1ca572eb96d726",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1490. Fix a NPE that could occur during instantiation and\ninitialization of the DistributedRaidFileSystem. \n(Rodrigo Schmidt via dhruba)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@909993 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/2e646c225c5dd696c345f1c1fd9190f0b35b165d",
        "patched_files": [
            "DistributedRaidFileSystem.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRaidDfs.java"
        ]
    },
    "hadoop-common_45035d7": {
        "bug_id": "hadoop-common_45035d7",
        "commit": "https://github.com/apache/hadoop-common/commit/45035d7823414fe09bd1dc79df1928ba865a65de",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/45035d7823414fe09bd1dc79df1928ba865a65de/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=45035d7823414fe09bd1dc79df1928ba865a65de",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -818,6 +818,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4053. Counters group names deprecation is wrong, iterating over\n     group names deprecated names don't show up  (Robert Evans via tgraves)\n \n+    MAPREDUCE-3506. Calling getPriority on JobInfo after parsing a history log\n+    with JobHistoryParser throws a NullPointerException (Jason Lowe via bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/45035d7823414fe09bd1dc79df1928ba865a65de/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "a070488c9e4eb770e8a0f5d054c795dcbcefca8e",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/45035d7823414fe09bd1dc79df1928ba865a65de/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java?ref=45035d7823414fe09bd1dc79df1928ba865a65de",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java",
                "patch": "@@ -441,6 +441,7 @@ public JobInfo() {\n       username = jobname = jobConfPath = jobQueueName = \"\";\n       tasksMap = new HashMap<TaskID, TaskInfo>();\n       jobACLs = new HashMap<JobACL, AccessControlList>();\n+      priority = JobPriority.NORMAL;\n     }\n     \n     /** Print all the job information */\n@@ -454,12 +455,20 @@ public void printAll() {\n       System.out.println(\"PRIORITY: \" + priority);\n       System.out.println(\"TOTAL_MAPS: \" + totalMaps);\n       System.out.println(\"TOTAL_REDUCES: \" + totalReduces);\n-      System.out.println(\"MAP_COUNTERS:\" + mapCounters.toString());\n-      System.out.println(\"REDUCE_COUNTERS:\" + reduceCounters.toString());\n-      System.out.println(\"TOTAL_COUNTERS: \" + totalCounters.toString());\n+      if (mapCounters != null) {\n+        System.out.println(\"MAP_COUNTERS:\" + mapCounters.toString());\n+      }\n+      if (reduceCounters != null) {\n+        System.out.println(\"REDUCE_COUNTERS:\" + reduceCounters.toString());\n+      }\n+      if (totalCounters != null) {\n+        System.out.println(\"TOTAL_COUNTERS: \" + totalCounters.toString());\n+      }\n       System.out.println(\"UBERIZED: \" + uberized);\n-      for (AMInfo amInfo : amInfos) {\n-        amInfo.printAll();\n+      if (amInfos != null) {\n+        for (AMInfo amInfo : amInfos) {\n+          amInfo.printAll();\n+        }\n       }\n       for (TaskInfo ti: tasksMap.values()) {\n         ti.printAll();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/45035d7823414fe09bd1dc79df1928ba865a65de/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java",
                "sha": "48c004b23b74db5767d699642e72eafa1016cb03",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/45035d7823414fe09bd1dc79df1928ba865a65de/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java?ref=45035d7823414fe09bd1dc79df1928ba865a65de",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "patch": "@@ -83,6 +83,13 @@\n     }\n   }\n \n+  @Test\n+  public void testJobInfo() throws Exception {\n+    JobInfo info = new JobInfo();\n+    Assert.assertEquals(\"NORMAL\", info.getPriority());\n+    info.printAll();\n+  }\n+\n   @Test\n   public void testHistoryParsing() throws Exception {\n     LOG.info(\"STARTING testHistoryParsing()\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/45035d7823414fe09bd1dc79df1928ba865a65de/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "sha": "b596a2123a7e19b99395d72781dcb7c789ca2f2c",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3506. Calling getPriority on JobInfo after parsing a history log with JobHistoryParser throws a NullPointerException (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1375602 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/27d051d32ee7ca624e48eba50ba72f00c3567a32",
        "patched_files": [
            "JobHistoryParser.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobHistoryParsing.java"
        ]
    },
    "hadoop-common_463f416": {
        "bug_id": "hadoop-common_463f416",
        "commit": "https://github.com/apache/hadoop-common/commit/463f416a07473540f866e1e26365a77e6aa18490",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=463f416a07473540f866e1e26365a77e6aa18490",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -744,6 +744,8 @@ Release 2.4.0 - UNRELEASED\n     HDFS-5756. hadoopRzOptionsSetByteBufferPool does not accept NULL argument,\n     contrary to docs. (cmccabe via wang)\n \n+    HDFS-5747. Fix NPEs in BlockManager. (Arpit Agarwal)\n+\n   BREAKDOWN OF HDFS-2832 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4985. Add storage type to the protocol and expose it in block report",
                "raw_url": "https://github.com/apache/hadoop-common/raw/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "af95486f1e8220955073a84f69feeca9fced908d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java?ref=463f416a07473540f866e1e26365a77e6aa18490",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "patch": "@@ -324,12 +324,14 @@ void addReplicaIfNotPresent(DatanodeStorageInfo storage,\n     Iterator<ReplicaUnderConstruction> it = replicas.iterator();\n     while (it.hasNext()) {\n       ReplicaUnderConstruction r = it.next();\n-      if(r.getExpectedStorageLocation() == storage) {\n+      DatanodeStorageInfo expectedLocation = r.getExpectedStorageLocation();\n+      if(expectedLocation == storage) {\n         // Record the gen stamp from the report\n         r.setGenerationStamp(block.getGenerationStamp());\n         return;\n-      } else if (r.getExpectedStorageLocation().getDatanodeDescriptor() ==\n-          storage.getDatanodeDescriptor()) {\n+      } else if (expectedLocation != null &&\n+                 expectedLocation.getDatanodeDescriptor() ==\n+                     storage.getDatanodeDescriptor()) {\n \n         // The Datanode reported that the block is on a different storage\n         // than the one chosen by BlockPlacementPolicy. This can occur as",
                "raw_url": "https://github.com/apache/hadoop-common/raw/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "sha": "1161077f49ddffb2f3294549d3f9322cf319f6fb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=463f416a07473540f866e1e26365a77e6aa18490",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -547,8 +547,8 @@ private void startCommonServices(Configuration conf) throws IOException {\n   }\n   \n   private void stopCommonServices() {\n-    if(namesystem != null) namesystem.close();\n     if(rpcServer != null) rpcServer.stop();\n+    if(namesystem != null) namesystem.close();\n     if (pauseMonitor != null) pauseMonitor.stop();\n     if (plugins != null) {\n       for (ServicePlugin p : plugins) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/463f416a07473540f866e1e26365a77e6aa18490/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "eb3755bdc4fda9bebbbf35ea7da30826fec2f3fa",
                "status": "modified"
            }
        ],
        "message": "HDFS-5747. Fix NPEs in BlockManager. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1557289 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/42f3f2c5a95f0cb71f96bfbe83ba5915740eb9fd",
        "patched_files": [
            "BlockInfoUnderConstruction.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockInfoUnderConstruction.java"
        ]
    },
    "hadoop-common_4b28c4e": {
        "bug_id": "hadoop-common_4b28c4e",
        "commit": "https://github.com/apache/hadoop-common/commit/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -214,6 +214,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-2518. The t flag is missing in distcp help message.  (Wei Yongjun\n     via szetszwo)\n \n+    MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters. (Robert Joseph Evans\n+    via cdouglas)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/CHANGES.txt",
                "sha": "e1eb16d5a672f5955bcf791150d2caaf76949320",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 6,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -42,8 +42,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RemoteException;\n-import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.util.Tool;\n@@ -149,7 +147,7 @@\n    * a JobProfile object to provide some info, and interacts with the\n    * remote service to provide certain functionality.\n    */\n-  class NetworkedJob implements RunningJob {\n+  static class NetworkedJob implements RunningJob {\n     Job job;\n     /**\n      * We store a JobProfile and a timestamp for when we last\n@@ -158,7 +156,7 @@\n      * has completely forgotten about the job.  (eg, 24 hours after the\n      * job completes.)\n      */\n-    public NetworkedJob(JobStatus status) throws IOException {\n+    public NetworkedJob(JobStatus status, Cluster cluster) throws IOException {\n       job = Job.getInstance(cluster, status, new JobConf(status.getJobFile()));\n     }\n \n@@ -380,7 +378,12 @@ public String toString() {\n      */\n     public Counters getCounters() throws IOException {\n       try { \n-        return Counters.downgrade(job.getCounters());\n+        Counters result = null;\n+        org.apache.hadoop.mapreduce.Counters temp = job.getCounters();\n+        if(temp != null) {\n+          result = Counters.downgrade(temp);\n+        }\n+        return result;\n       } catch (InterruptedException ie) {\n         throw new IOException(ie);\n       }\n@@ -557,7 +560,7 @@ public RunningJob getJob(JobID jobid) throws IOException {\n       if (job != null) {\n         JobStatus status = JobStatus.downgrade(job.getStatus());\n         if (status != null) {\n-          return new NetworkedJob(status);\n+          return new NetworkedJob(status, cluster);\n         } \n       }\n     } catch (InterruptedException ie) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "3b5f84bfe1fb6d4c0920e78cfbc20d3de32c28e1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/RunningJob.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/RunningJob.java",
                "patch": "@@ -193,7 +193,7 @@\n   /**\n    * Gets the counters for this job.\n    * \n-   * @return the counters for this job.\n+   * @return the counters for this job or null if the job has been retired.\n    * @throws IOException\n    */\n   public Counters getCounters() throws IOException;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapred/RunningJob.java",
                "sha": "b3af4f6c98d5ebe15971387e23173e2e4a0513a8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "patch": "@@ -230,7 +230,7 @@ public Counters getJobCounters(JobID jobid)\n   \n   /**\n    * Get task completion events for the jobid, starting from fromEventId. \n-   * Returns empty aray if no events are available. \n+   * Returns empty array if no events are available. \n    * @param jobid job id \n    * @param fromEventId event id to start from.\n    * @param maxEvents the max number of events we want to look at ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/java/org/apache/hadoop/mapreduce/protocol/ClientProtocol.java",
                "sha": "80e556bac2f3f4929cd313ea3e21c4a4839719c9",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java?ref=4b28c4e7d3999df0fb8b95487b217a5e801aa1e6",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapred;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.List;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.junit.Test;\n+import static org.mockito.Mockito.*;\n+\n+\n+public class TestNetworkedJob {\n+\n+  @SuppressWarnings(\"deprecation\")\n+  @Test\n+  public void testGetNullCounters() throws Exception {\n+    //mock creation\n+    Job mockJob = mock(Job.class);\n+    RunningJob underTest = new JobClient.NetworkedJob(mockJob); \n+\n+    when(mockJob.getCounters()).thenReturn(null);\n+    assertNull(underTest.getCounters());\n+    //verification\n+    verify(mockJob).getCounters();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4b28c4e7d3999df0fb8b95487b217a5e801aa1e6/src/test/mapred/org/apache/hadoop/mapred/TestNetworkedJob.java",
                "sha": "b6565e28956d7446628aae1da838b930a83313db",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2470. Fix NPE in RunningJobs::getCounters.\nContributed by Robert Joseph Evans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1125578 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/921c7920150466cb545dca153e05f0edeacd3754",
        "patched_files": [
            "JobClient.java",
            "ClientProtocol.java",
            "CHANGES.txt",
            "RunningJob.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java",
            "TestNetworkedJob.java"
        ]
    },
    "hadoop-common_4bcaa45": {
        "bug_id": "hadoop-common_4bcaa45",
        "commit": "https://github.com/apache/hadoop-common/commit/4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -326,6 +326,8 @@ Trunk (Unreleased)\n \n     HADOOP-10431. Change visibility of KeyStore.Options getter methods to public. (tucu)\n \n+    HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "36fe52b7b5d307bfa4020b5af7d58dc7c841997c",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 33,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "patch": "@@ -27,9 +27,7 @@\n import java.security.NoSuchAlgorithmException;\n import java.text.MessageFormat;\n import java.util.Date;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Map;\n \n import com.google.gson.stream.JsonReader;\n import com.google.gson.stream.JsonWriter;\n@@ -176,22 +174,26 @@ protected int addVersion() {\n     protected byte[] serialize() throws IOException {\n       ByteArrayOutputStream buffer = new ByteArrayOutputStream();\n       JsonWriter writer = new JsonWriter(new OutputStreamWriter(buffer));\n-      writer.beginObject();\n-      if (cipher != null) {\n-        writer.name(CIPHER_FIELD).value(cipher);\n-      }\n-      if (bitLength != 0) {\n-        writer.name(BIT_LENGTH_FIELD).value(bitLength);\n-      }\n-      if (created != null) {\n-        writer.name(CREATED_FIELD).value(created.getTime());\n-      }\n-      if (description != null) {\n-        writer.name(DESCRIPTION_FIELD).value(description);\n+      try {\n+        writer.beginObject();\n+        if (cipher != null) {\n+          writer.name(CIPHER_FIELD).value(cipher);\n+        }\n+        if (bitLength != 0) {\n+          writer.name(BIT_LENGTH_FIELD).value(bitLength);\n+        }\n+        if (created != null) {\n+          writer.name(CREATED_FIELD).value(created.getTime());\n+        }\n+        if (description != null) {\n+          writer.name(DESCRIPTION_FIELD).value(description);\n+        }\n+        writer.name(VERSIONS_FIELD).value(versions);\n+        writer.endObject();\n+        writer.flush();\n+      } finally {\n+        writer.close();\n       }\n-      writer.name(VERSIONS_FIELD).value(versions);\n-      writer.endObject();\n-      writer.flush();\n       return buffer.toByteArray();\n     }\n \n@@ -207,23 +209,27 @@ protected Metadata(byte[] bytes) throws IOException {\n       int versions = 0;\n       String description = null;\n       JsonReader reader = new JsonReader(new InputStreamReader\n-          (new ByteArrayInputStream(bytes)));\n-      reader.beginObject();\n-      while (reader.hasNext()) {\n-        String field = reader.nextName();\n-        if (CIPHER_FIELD.equals(field)) {\n-          cipher = reader.nextString();\n-        } else if (BIT_LENGTH_FIELD.equals(field)) {\n-          bitLength = reader.nextInt();\n-        } else if (CREATED_FIELD.equals(field)) {\n-          created = new Date(reader.nextLong());\n-        } else if (VERSIONS_FIELD.equals(field)) {\n-          versions = reader.nextInt();\n-        } else if (DESCRIPTION_FIELD.equals(field)) {\n-          description = reader.nextString();\n+        (new ByteArrayInputStream(bytes)));\n+      try {\n+        reader.beginObject();\n+        while (reader.hasNext()) {\n+          String field = reader.nextName();\n+          if (CIPHER_FIELD.equals(field)) {\n+            cipher = reader.nextString();\n+          } else if (BIT_LENGTH_FIELD.equals(field)) {\n+            bitLength = reader.nextInt();\n+          } else if (CREATED_FIELD.equals(field)) {\n+            created = new Date(reader.nextLong());\n+          } else if (VERSIONS_FIELD.equals(field)) {\n+            versions = reader.nextInt();\n+          } else if (DESCRIPTION_FIELD.equals(field)) {\n+            description = reader.nextString();\n+          }\n         }\n+        reader.endObject();\n+      } finally {\n+        reader.close();\n       }\n-      reader.endObject();\n       this.cipher = cipher;\n       this.bitLength = bitLength;\n       this.created = created;\n@@ -310,7 +316,6 @@ public abstract KeyVersion getKeyVersion(String versionName\n    */\n   public abstract List<String> getKeys() throws IOException;\n \n-\n   /**\n    * Get key metadata in bulk.\n    * @param names the names of the keys to get",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "sha": "0b031c0493b8f6cc2961f2ecd54b3d74ddc54ef8",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "changes": 168,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 80,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "patch": "@@ -23,9 +23,6 @@\n import java.security.InvalidParameterException;\n import java.security.NoSuchAlgorithmException;\n import java.util.List;\n-import java.util.Map;\n-\n-import javax.crypto.KeyGenerator;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n@@ -93,41 +90,54 @@ public int run(String[] args) throws Exception {\n    */\n   private int init(String[] args) throws IOException {\n     for (int i = 0; i < args.length; i++) { // parse command line\n+      boolean moreTokens = (i < args.length - 1);\n       if (args[i].equals(\"create\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new CreateCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new DeleteCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new RollCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n-      } else if (args[i].equals(\"list\")) {\n+      } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n-      } else if (args[i].equals(\"--size\")) {\n+      } else if (\"--size\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_BITLENGTH_NAME, args[++i]);\n-      } else if (args[i].equals(\"--cipher\")) {\n+      } else if (\"--cipher\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_CIPHER_NAME, args[++i]);\n-      } else if (args[i].equals(\"--provider\")) {\n+      } else if (\"--provider\".equals(args[i]) && moreTokens) {\n         userSuppliedProvider = true;\n         getConf().set(KeyProviderFactory.KEY_PROVIDER_PATH, args[++i]);\n-      } else if (args[i].equals(\"--metadata\")) {\n+      } else if (\"--metadata\".equals(args[i])) {\n         getConf().setBoolean(LIST_METADATA, true);\n-      } else if (args[i].equals(\"-i\") || (args[i].equals(\"--interactive\"))) {\n+      } else if (\"-i\".equals(args[i]) || (\"--interactive\".equals(args[i]))) {\n         interactive = true;\n-      } else if (args[i].equals(\"--help\")) {\n+      } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n         return -1;\n       } else {\n@@ -136,15 +146,20 @@ private int init(String[] args) throws IOException {\n         return -1;\n       }\n     }\n+\n+    if (command == null) {\n+      printKeyShellUsage();\n+      return -1;\n+    }\n+\n     return 0;\n   }\n \n   private void printKeyShellUsage() {\n     out.println(USAGE_PREFIX + COMMANDS);\n     if (command != null) {\n       out.println(command.getUsage());\n-    }\n-    else {\n+    } else {\n       out.println(\"=========================================================\" +\n       \t\t\"======\");\n       out.println(CreateCommand.USAGE + \":\\n\\n\" + CreateCommand.DESC);\n@@ -174,8 +189,7 @@ protected KeyProvider getKeyProvider() {\n         providers = KeyProviderFactory.getProviders(getConf());\n         if (userSuppliedProvider) {\n           provider = providers.get(0);\n-        }\n-        else {\n+        } else {\n           for (KeyProvider p : providers) {\n             if (!p.isTransient()) {\n               provider = p;\n@@ -190,7 +204,7 @@ protected KeyProvider getKeyProvider() {\n     }\n \n     protected void printProviderWritten() {\n-        out.println(provider.getClass().getName() + \" has been updated.\");\n+        out.println(provider + \" has been updated.\");\n     }\n \n     protected void warnIfTransientProvider() {\n@@ -206,12 +220,12 @@ protected void warnIfTransientProvider() {\n \n   private class ListCommand extends Command {\n     public static final String USAGE =\n-        \"list [--provider] [--metadata] [--help]\";\n+        \"list [--provider <provider>] [--metadata] [--help]\";\n     public static final String DESC =\n-        \"The list subcommand displays the keynames contained within \\n\" +\n-        \"a particular provider - as configured in core-site.xml or \" +\n-        \"indicated\\nthrough the --provider argument.\\n\" +\n-        \"If the --metadata option is used, the keys metadata will be printed\";\n+        \"The list subcommand displays the keynames contained within\\n\" +\n+        \"a particular provider as configured in core-site.xml or\\n\" +\n+        \"specified with the --provider argument. --metadata displays\\n\" +\n+        \"the metadata.\";\n \n     private boolean metadata = false;\n \n@@ -220,9 +234,9 @@ public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n         out.println(\"There are no non-transient KeyProviders configured.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\\n\"\n-            + \"to use. If you want to list a transient provider then you\\n\"\n-            + \"you MUST use the --provider argument.\");\n+          + \"Use the --provider option to specify a provider. If you\\n\"\n+          + \"want to list a transient provider then you must use the\\n\"\n+          + \"--provider argument.\");\n         rc = false;\n       }\n       metadata = getConf().getBoolean(LIST_METADATA, false);\n@@ -231,12 +245,12 @@ public boolean validate() {\n \n     public void execute() throws IOException {\n       try {\n-        List<String> keys = provider.getKeys();\n-        out.println(\"Listing keys for KeyProvider: \" + provider.toString());\n+        final List<String> keys = provider.getKeys();\n+        out.println(\"Listing keys for KeyProvider: \" + provider);\n         if (metadata) {\n-          Metadata[] meta =\n+          final Metadata[] meta =\n             provider.getKeysMetadata(keys.toArray(new String[keys.size()]));\n-          for(int i=0; i < meta.length; ++i) {\n+          for (int i = 0; i < meta.length; ++i) {\n             out.println(keys.get(i) + \" : \" + meta[i]);\n           }\n         } else {\n@@ -245,7 +259,7 @@ public void execute() throws IOException {\n           }\n         }\n       } catch (IOException e) {\n-        out.println(\"Cannot list keys for KeyProvider: \" + provider.toString()\n+        out.println(\"Cannot list keys for KeyProvider: \" + provider\n             + \": \" + e.getMessage());\n         throw e;\n       }\n@@ -258,11 +272,10 @@ public String getUsage() {\n   }\n \n   private class RollCommand extends Command {\n-    public static final String USAGE = \"roll <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"roll <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The roll subcommand creates a new version of the key specified\\n\" +\n-        \"through the <keyname> argument within the provider indicated using\\n\" +\n-        \"the --provider argument\";\n+      \"The roll subcommand creates a new version for the specified key\\n\" +\n+      \"within the provider indicated using the --provider argument\\n\";\n \n     String keyName = null;\n \n@@ -274,39 +287,37 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Key will not be rolled.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. The key\\n\" +\n+          \"has not been rolled. Use the --provider option to specify\\n\" +\n+          \"a provider.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>.\\n\" +\n+          \"See the usage description by using --help.\");\n         rc = false;\n       }\n       return rc;\n     }\n \n     public void execute() throws NoSuchAlgorithmException, IOException {\n       try {\n-        Metadata md = provider.getMetadata(keyName);\n         warnIfTransientProvider();\n         out.println(\"Rolling key version from KeyProvider: \"\n-            + provider.toString() + \" for key name: \" + keyName);\n+            + provider + \"\\n  for key name: \" + keyName);\n         try {\n           provider.rollNewVersion(keyName);\n           out.println(keyName + \" has been successfully rolled.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider.toString());\n+              + provider);\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider.toString());\n+            + provider);\n         throw e1;\n       }\n     }\n@@ -318,11 +329,11 @@ public String getUsage() {\n   }\n \n   private class DeleteCommand extends Command {\n-    public static final String USAGE = \"delete <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"delete <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The delete subcommand deletes all of the versions of the key\\n\" +\n-        \"specified as the <keyname> argument from within the provider\\n\" +\n-        \"indicated through the --provider argument\";\n+        \"The delete subcommand deletes all versions of the key\\n\" +\n+        \"specified by the <keyname> argument from within the\\n\" +\n+        \"provider specified --provider.\";\n \n     String keyName = null;\n     boolean cont = true;\n@@ -335,23 +346,21 @@ public DeleteCommand(String keyName) {\n     public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Nothing will be deleted.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. Nothing\\n\"\n+          + \"was deleted. Use the --provider option to specify a provider.\");\n         return false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"There is no keyName specified. Please specify a \" +\n+            \"<keyname>. See the usage description with --help.\");\n         return false;\n       }\n       if (interactive) {\n         try {\n           cont = ToolRunner\n               .confirmPrompt(\"You are about to DELETE all versions of \"\n-                  + \"the key: \" + keyName + \" from KeyProvider \"\n-                  + provider.toString() + \". Continue?:\");\n+                  + \" key: \" + keyName + \" from KeyProvider \"\n+                  + provider + \". Continue?:\");\n           if (!cont) {\n             out.println(\"Nothing has been be deleted.\");\n           }\n@@ -367,15 +376,15 @@ public boolean validate() {\n     public void execute() throws IOException {\n       warnIfTransientProvider();\n       out.println(\"Deleting key: \" + keyName + \" from KeyProvider: \"\n-          + provider.toString());\n+          + provider);\n       if (cont) {\n         try {\n           provider.deleteKey(keyName);\n           out.println(keyName + \" has been successfully deleted.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \"has NOT been deleted.\");\n+          out.println(keyName + \" has not been deleted.\");\n           throw e;\n         }\n       }\n@@ -388,16 +397,16 @@ public String getUsage() {\n   }\n \n   private class CreateCommand extends Command {\n-    public static final String USAGE = \"create <keyname> [--cipher] \" +\n-    \t\t\"[--size] [--provider] [--help]\";\n+    public static final String USAGE =\n+      \"create <keyname> [--cipher <cipher>] [--size <size>]\\n\" +\n+      \"                     [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The create subcommand creates a new key for the name specified\\n\" +\n-        \"as the <keyname> argument within the provider indicated through\\n\" +\n-        \"the --provider argument. You may also indicate the specific\\n\" +\n-        \"cipher through the --cipher argument. The default for cipher is\\n\" +\n-        \"currently \\\"AES/CTR/NoPadding\\\". The default keysize is \\\"256\\\".\\n\" +\n-        \"You may also indicate the requested key length through the --size\\n\" +\n-        \"argument.\";\n+      \"The create subcommand creates a new key for the name specified\\n\" +\n+      \"by the <keyname> argument within the provider specified by the\\n\" +\n+      \"--provider argument. You may specify a cipher with the --cipher\\n\" +\n+      \"argument. The default cipher is currently \\\"AES/CTR/NoPadding\\\".\\n\" +\n+      \"The default keysize is 256. You may specify the requested key\\n\" +\n+      \"length using the --size argument.\\n\";\n \n     String keyName = null;\n \n@@ -409,15 +418,14 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\nKey\" +\n-        \t\t\" will not be created.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\" +\n-            \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. No key\\n\" +\n+          \" was created. You can use the --provider option to specify\\n\" +\n+          \" a provider to use.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-        \t\t\"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>. See the usage description\" +\n+          \" with --help.\");\n         rc = false;\n       }\n       return rc;\n@@ -432,13 +440,13 @@ public void execute() throws IOException, NoSuchAlgorithmException {\n         provider.flush();\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "sha": "3d56640e11e4f674a7c09c65b47b7208373fd599",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "patch": "@@ -126,7 +126,6 @@ public KeyProvider createProvider(URI providerName, Configuration conf)\n     return o;\n   }\n \n-\n   public static String checkNotEmpty(String s, String name)\n       throws IllegalArgumentException {\n     checkNotNull(s, name);\n@@ -140,6 +139,13 @@ public static String checkNotEmpty(String s, String name)\n   private String kmsUrl;\n   private SSLFactory sslFactory;\n \n+  @Override\n+  public String toString() {\n+    final StringBuilder sb = new StringBuilder(\"KMSClientProvider[\");\n+    sb.append(kmsUrl).append(\"]\");\n+    return sb.toString();\n+  }\n+\n   public KMSClientProvider(URI uri, Configuration conf) throws IOException {\n     Path path = unnestUri(uri);\n     URL url = path.toUri().toURL();\n@@ -515,5 +521,4 @@ public void flush() throws IOException {\n   public static String buildVersionName(String name, int version) {\n     return KeyProvider.buildVersionName(name, version);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "sha": "ff30f86de377f4a763aec5ccd62d532101d99311",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java?ref=4bcaa45a2ea36fb440069c7a458cdc225cb862ca",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "patch": "@@ -121,7 +121,7 @@ public void testInvalidKeySize() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test\n@@ -134,7 +134,7 @@ public void testInvalidCipher() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/4bcaa45a2ea36fb440069c7a458cdc225cb862ca/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "sha": "ae6938eb68ce2fe2f963d70438e8c0959a4edd1d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594320 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d63d557df4d42fdbe313bf1c397a206740120670",
        "patched_files": [
            "KeyProvider.java",
            "CHANGES.txt",
            "KeyShell.java",
            "KMSClientProvider.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKeyShell.java",
            "TestKeyProvider.java"
        ]
    },
    "hadoop-common_52983ce": {
        "bug_id": "hadoop-common_52983ce",
        "commit": "https://github.com/apache/hadoop-common/commit/52983ce2880caf73d0e1ceff92196d72ac3c4abf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=52983ce2880caf73d0e1ceff92196d72ac3c4abf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -52,6 +52,8 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-4083. [Gridmix] NPE in cpu emulation. (amarrk)\n+\n     MAPREDUCE-4087. [Gridmix] GenerateDistCacheData job of Gridmix can\n                     become slow in some cases (ravigummadi).\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "b8921ae2d5d3ac7c334122c89d4551582c946bd7",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java?ref=52983ce2880caf73d0e1ceff92196d72ac3c4abf",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java",
                "patch": "@@ -235,7 +235,9 @@ private synchronized long getCurrentCPUUsage() {\n   \n   @Override\n   public float getProgress() {\n-    return Math.min(1f, ((float)getCurrentCPUUsage())/targetCpuUsage);\n+    return enabled \n+           ? Math.min(1f, ((float)getCurrentCPUUsage())/targetCpuUsage)\n+           : 1.0f;\n   }\n   \n   @Override\n@@ -297,6 +299,9 @@ public void emulate() throws IOException, InterruptedException {\n   public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n                          ResourceCalculatorPlugin monitor,\n                          Progressive progress) {\n+    this.monitor = monitor;\n+    this.progress = progress;\n+    \n     // get the target CPU usage\n     targetCpuUsage = metrics.getCumulativeCpuUsage();\n     if (targetCpuUsage <= 0 ) {\n@@ -306,8 +311,6 @@ public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n       enabled = true;\n     }\n     \n-    this.monitor = monitor;\n-    this.progress = progress;\n     emulationInterval =  conf.getFloat(CPU_EMULATION_PROGRESS_INTERVAL, \n                                        DEFAULT_EMULATION_FREQUENCY);\n     ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java",
                "sha": "c2b2a018ff365aee5c323f033640508b8be8b26d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java?ref=52983ce2880caf73d0e1ceff92196d72ac3c4abf",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java",
                "patch": "@@ -188,7 +188,9 @@ protected long getMaxHeapUsageInMB() {\n   \n   @Override\n   public float getProgress() {\n-    return Math.min(1f, ((float)getTotalHeapUsageInMB())/targetHeapUsageInMB);\n+    return enabled \n+           ? Math.min(1f, ((float)getTotalHeapUsageInMB())/targetHeapUsageInMB)\n+           : 1.0f;\n   }\n   \n   @Override\n@@ -237,6 +239,8 @@ public void emulate() throws IOException, InterruptedException {\n   public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n                          ResourceCalculatorPlugin monitor,\n                          Progressive progress) {\n+    this.progress = progress;\n+    \n     // get the target heap usage\n     targetHeapUsageInMB = metrics.getHeapUsage() / ONE_MB;\n     if (targetHeapUsageInMB <= 0 ) {\n@@ -248,7 +252,6 @@ public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n       enabled = true;\n     }\n     \n-    this.progress = progress;\n     emulationInterval = \n       conf.getFloat(HEAP_EMULATION_PROGRESS_INTERVAL, \n                     DEFAULT_EMULATION_PROGRESS_INTERVAL);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java",
                "sha": "47941ccfffb908da85dd655799ac1752addc161b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java?ref=52983ce2880caf73d0e1ceff92196d72ac3c4abf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java",
                "patch": "@@ -171,6 +171,11 @@ public void testTotalHeapUsageEmulatorPlugin() throws Exception {\n     assertEquals(\"Disabled heap usage emulation plugin works!\", \n                  heapUsagePre, heapUsagePost);\n     \n+    // test with get progress\n+    float progress = heapPlugin.getProgress();\n+    assertEquals(\"Invalid progress of disabled cumulative heap usage emulation \"\n+                 + \"plugin!\", 1.0f, progress, 0f);\n+    \n     // test with wrong/invalid configuration\n     Boolean failed = null;\n     invalidUsage = ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java",
                "sha": "486165d9efa2d2c808e31569243dd7d773323991",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java?ref=52983ce2880caf73d0e1ceff92196d72ac3c4abf",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;\n import org.apache.hadoop.mapreduce.task.MapContextImpl;\n import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;\n-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.ProcResourceValues;\n import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;\n import org.apache.hadoop.mapred.DummyResourceCalculatorPlugin;\n import org.apache.hadoop.mapred.gridmix.LoadJob.ResourceUsageMatcherRunner;\n@@ -484,6 +483,11 @@ public void testCumulativeCpuUsageEmulatorPlugin() throws Exception {\n     assertEquals(\"Disabled cumulative CPU usage emulation plugin works!\", \n                  cpuUsagePre, cpuUsagePost);\n     \n+    // test with get progress\n+    float progress = cpuPlugin.getProgress();\n+    assertEquals(\"Invalid progress of disabled cumulative CPU usage emulation \" \n+                 + \"plugin!\", 1.0f, progress, 0f);\n+    \n     // test with valid resource usage value\n     ResourceUsageMetrics metrics = createMetrics(targetCpuUsage);\n     ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/52983ce2880caf73d0e1ceff92196d72ac3c4abf/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java",
                "sha": "9874be3229e34651a3f5914335b8d82c9f46313f",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4083. [Gridmix] NPE in cpu emulation. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325145 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e8612157b44d1158649a8ddda421b4818f0c62cb",
        "patched_files": [
            "CumulativeCpuUsageEmulatorPlugin.java",
            "TotalHeapUsageEmulatorPlugin.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestGridmixMemoryEmulation.java",
            "TestResourceUsageEmulators.java"
        ]
    },
    "hadoop-common_53b14e7": {
        "bug_id": "hadoop-common_53b14e7",
        "commit": "https://github.com/apache/hadoop-common/commit/53b14e71ed9d2ddbc7e66315484e0af31f26758d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/53b14e71ed9d2ddbc7e66315484e0af31f26758d/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=53b14e71ed9d2ddbc7e66315484e0af31f26758d",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -984,6 +984,9 @@ Trunk (unreleased changes)\n     HADOOP-6229. Attempt to make a directory under an existing file on\n     LocalFileSystem should throw an Exception. (Boris Shkolnik via tomwhite)\n \n+    HADOOP-6243. Fix a NullPointerException in processing deprecated keys.\n+    (Sreekanth Ramakrishnan via yhemanth)\n+\n Release 0.20.1 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/53b14e71ed9d2ddbc7e66315484e0af31f26758d/CHANGES.txt",
                "sha": "75917b6261d9ced18865e1e68582512eeefaa204",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/53b14e71ed9d2ddbc7e66315484e0af31f26758d/src/java/core-default.xml",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/core-default.xml?ref=53b14e71ed9d2ddbc7e66315484e0af31f26758d",
                "deletions": 7,
                "filename": "src/java/core-default.xml",
                "patch": "@@ -485,11 +485,4 @@\n     IP address.\n   </description>\n </property>\n-\n-<property>\n-  <name>hadoop.conf.extra.classes</name>\n-  <value>org.apache.hadoop.mapred.JobConf</value>\n-  <final>true</final>\n-</property>\n-\n </configuration>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/53b14e71ed9d2ddbc7e66315484e0af31f26758d/src/java/core-default.xml",
                "sha": "9a2ae76ee0d27205fa34ff412dd4e6804a41f782",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/53b14e71ed9d2ddbc7e66315484e0af31f26758d/src/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/conf/Configuration.java?ref=53b14e71ed9d2ddbc7e66315484e0af31f26758d",
                "deletions": 48,
                "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -336,6 +336,12 @@ private String handleDeprecation(String name) {\n     }\n     addDefaultResource(\"core-default.xml\");\n     addDefaultResource(\"core-site.xml\");\n+    //Add code for managing deprecated key mapping\n+    //for example\n+    //addDeprecation(\"oldKey1\",new String[]{\"newkey1\",\"newkey2\"});\n+    //adds deprecation for oldKey1 to two new keys(newkey1, newkey2).\n+    //so get or set of oldKey1 will correctly populate/access values of \n+    //newkey1 and newkey2\n   }\n   \n   private Properties properties;\n@@ -1364,56 +1370,8 @@ private void loadResources(Properties properties,\n       loadResource(properties, resource, quiet);\n     }\n     // process for deprecation.\n-    processDeprecation();\n-  }\n-  \n-  /**\n-   * Flag to ensure that the classes mentioned in the value of the property\n-   * <code>hadoop.conf.extra.classes</code> are loaded only once for\n-   * all instances of <code>Configuration</code>\n-   */\n-  private static AtomicBoolean loadedDeprecation = new AtomicBoolean(false);\n-  \n-  private static final String extraConfKey = \"hadoop.conf.extra.classes\";\n-\n-  /**\n-   * adds all the deprecations to the deprecatedKeyMap and updates the values of\n-   * the appropriate keys\n-   */\n-  private void processDeprecation() {\n-    populateDeprecationMapping();\n     processDeprecatedKeys();\n   }\n-  \n-  /**\n-   * Loads all the classes in mapred and hdfs that extend Configuration and that\n-   * have deprecations to be added into deprecatedKeyMap\n-   */\n-  private synchronized void populateDeprecationMapping() {\n-    if (!loadedDeprecation.get()) {\n-      // load classes from mapred and hdfs which extend Configuration and have \n-      // deprecations added in their static blocks\n-      String classnames = substituteVars(properties.getProperty(extraConfKey));\n-      if (classnames == null) {\n-        return;\n-      }\n-      String[] classes = StringUtils.getStrings(classnames);\n-      for (String className : classes) {\n-        try {\n-          Class.forName(className);\n-        } catch (ClassNotFoundException e) {\n-          LOG.warn(className + \" is not in the classpath\");\n-        }\n-      }\n-      // make deprecatedKeyMap unmodifiable in order to prevent changes to \n-      // it in user's code.\n-      deprecatedKeyMap = Collections.unmodifiableMap(deprecatedKeyMap);\n-      // ensure that deprecation processing is done only once for all \n-      // instances of this object\n-      loadedDeprecation.set(true);\n-    }\n-  }\n-\n   /**\n    * Updates the keys that are replacing the deprecated keys and removes the \n    * deprecated keys from memory.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/53b14e71ed9d2ddbc7e66315484e0af31f26758d/src/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "d2572595842be861335c1e8b82fd1c34766e15f1",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop-common/blob/53b14e71ed9d2ddbc7e66315484e0af31f26758d/src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java?ref=53b14e71ed9d2ddbc7e66315484e0af31f26758d",
                "deletions": 26,
                "filename": "src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java",
                "patch": "@@ -81,30 +81,27 @@ void appendProperty(String name, String val, boolean isFinal)\n     out.write(\"</property>\\n\");\n   }\n   \n-  static class MyConf extends Configuration {\n-    static {\n-      // add deprecation mappings.\n-      Configuration.addDeprecation(\"old.key1\", new String[]{\"new.key1\"});\n-      Configuration.addDeprecation(\"old.key2\", new String[]{\"new.key2\"});\n-      Configuration.addDeprecation(\"old.key3\", new String[]{\"new.key3\"});\n-      Configuration.addDeprecation(\"old.key4\", new String[]{\"new.key4\"});\n-      Configuration.addDeprecation(\"old.key5\", new String[]{\"new.key5\"});\n-      Configuration.addDeprecation(\"old.key6\", new String[]{\"new.key6\"});\n-      Configuration.addDeprecation(\"old.key7\", new String[]{\"new.key7\"});\n-      Configuration.addDeprecation(\"old.key8\", new String[]{\"new.key8\"});\n-      Configuration.addDeprecation(\"old.key9\", new String[]{\"new.key9\"});\n-      Configuration.addDeprecation(\"old.key10\", new String[]{\"new.key10\"});\n-      Configuration.addDeprecation(\"old.key11\", new String[]{\"new.key11\"});\n-      Configuration.addDeprecation(\"old.key12\", new String[]{\"new.key12\"});\n-      Configuration.addDeprecation(\"old.key13\", new String[]{\"new.key13\"});\n-      Configuration.addDeprecation(\"old.key14\", new String[]{\"new.key14\"});\n-      Configuration.addDeprecation(\"old.key15\", new String[]{\"new.key15\"});\n-      Configuration.addDeprecation(\"old.key16\", new String[]{\"new.key16\"});\n-      Configuration.addDeprecation(\"A\", new String[]{\"B\"});\n-      Configuration.addDeprecation(\"C\", new String[]{\"D\"});\n-      Configuration.addDeprecation(\"E\", new String[]{\"F\"});\n-      Configuration.addDeprecation(\"G\", new String[]{\"H\",\"I\"});\n-    }\n+  private void addDeprecationToConfiguration() {\n+    Configuration.addDeprecation(\"old.key1\", new String[]{\"new.key1\"});\n+    Configuration.addDeprecation(\"old.key2\", new String[]{\"new.key2\"});\n+    Configuration.addDeprecation(\"old.key3\", new String[]{\"new.key3\"});\n+    Configuration.addDeprecation(\"old.key4\", new String[]{\"new.key4\"});\n+    Configuration.addDeprecation(\"old.key5\", new String[]{\"new.key5\"});\n+    Configuration.addDeprecation(\"old.key6\", new String[]{\"new.key6\"});\n+    Configuration.addDeprecation(\"old.key7\", new String[]{\"new.key7\"});\n+    Configuration.addDeprecation(\"old.key8\", new String[]{\"new.key8\"});\n+    Configuration.addDeprecation(\"old.key9\", new String[]{\"new.key9\"});\n+    Configuration.addDeprecation(\"old.key10\", new String[]{\"new.key10\"});\n+    Configuration.addDeprecation(\"old.key11\", new String[]{\"new.key11\"});\n+    Configuration.addDeprecation(\"old.key12\", new String[]{\"new.key12\"});\n+    Configuration.addDeprecation(\"old.key13\", new String[]{\"new.key13\"});\n+    Configuration.addDeprecation(\"old.key14\", new String[]{\"new.key14\"});\n+    Configuration.addDeprecation(\"old.key15\", new String[]{\"new.key15\"});\n+    Configuration.addDeprecation(\"old.key16\", new String[]{\"new.key16\"});\n+    Configuration.addDeprecation(\"A\", new String[]{\"B\"});\n+    Configuration.addDeprecation(\"C\", new String[]{\"D\"});\n+    Configuration.addDeprecation(\"E\", new String[]{\"F\"});\n+    Configuration.addDeprecation(\"G\", new String[]{\"H\",\"I\"});\n   }\n   \n   /**\n@@ -123,8 +120,6 @@ void appendProperty(String name, String val, boolean isFinal)\n   public void testDeprecation() throws IOException {\n     out=new BufferedWriter(new FileWriter(CONFIG));\n     startConfig();\n-    appendProperty(\"hadoop.conf.extra.classes\", MyConf.class.getName()\n-        + \",myconf1\");\n     // load keys with default values. Some of them are set to final to\n     // test the precedence order between deprecation and being final\n     appendProperty(\"new.key1\",\"default.value1\",true);\n@@ -145,6 +140,7 @@ public void testDeprecation() throws IOException {\n     appendProperty(\"new.key16\",\"default.value16\");\n     endConfig();\n     Path fileResource = new Path(CONFIG);\n+    addDeprecationToConfiguration();\n     conf.addResource(fileResource);\n     \n     out=new BufferedWriter(new FileWriter(CONFIG2));",
                "raw_url": "https://github.com/apache/hadoop-common/raw/53b14e71ed9d2ddbc7e66315484e0af31f26758d/src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java",
                "sha": "a5cb59de7ff7899e28dba89b2aa41503d5b74be7",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6243. Fix a NullPointerException in processing deprecated keys. Contributed by Sreekanth Ramakrishnan.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@812455 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/5f80fc094c36224daefa75b7d5de89e0f5d4cfcc",
        "patched_files": [
            "core-default.xml",
            "Configuration.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestConfigurationDeprecation.java",
            "TestConfiguration.java"
        ]
    },
    "hadoop-common_56288ad": {
        "bug_id": "hadoop-common_56288ad",
        "commit": "https://github.com/apache/hadoop-common/commit/56288adc4f5e6c922053c84c131c03f9ea29e79e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=56288adc4f5e6c922053c84c131c03f9ea29e79e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -139,6 +139,9 @@ Trunk (Unreleased)\n \n     MAPREDUCE-5717. Task pings are interpreted as task progress (jlowe)\n \n+    MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to \n+    ProportionalCapacityPreemptionPolicy (Sunil G via devaraj)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c2a4f14514d0bd184f92e05ba595fadc2966b1b3",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java?ref=56288adc4f5e6c922053c84c131c03f9ea29e79e",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "patch": "@@ -29,7 +29,9 @@\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.PreemptionContainer;\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\n import org.apache.hadoop.yarn.api.records.PreemptionMessage;\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\n import org.apache.hadoop.yarn.event.EventHandler;\n \n /**\n@@ -52,13 +54,18 @@ public void init(AppContext context) {\n   public void preempt(Context ctxt, PreemptionMessage preemptionRequests) {\n     // for both strict and negotiable preemption requests kill the\n     // container\n-    for (PreemptionContainer c :\n-        preemptionRequests.getStrictContract().getContainers()) {\n-      killContainer(ctxt, c);\n+    StrictPreemptionContract strictContract = preemptionRequests\n+        .getStrictContract();\n+    if (strictContract != null) {\n+      for (PreemptionContainer c : strictContract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n-    for (PreemptionContainer c :\n-         preemptionRequests.getContract().getContainers()) {\n-       killContainer(ctxt, c);\n+    PreemptionContract contract = preemptionRequests.getContract();\n+    if (contract != null) {\n+      for (PreemptionContainer c : contract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "sha": "09237aaa297f82b2a60ecb2f5ba9f0d620130094",
                "status": "modified"
            },
            {
                "additions": 144,
                "blob_url": "https://github.com/apache/hadoop-common/blob/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java?ref=56288adc4f5e6c922053c84c131c03f9ea29e79e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "patch": "@@ -0,0 +1,144 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+package org.apache.hadoop.mapreduce.v2.app;\r\n+\r\n+import static org.mockito.Matchers.any;\r\n+import static org.mockito.Mockito.mock;\r\n+import static org.mockito.Mockito.times;\r\n+import static org.mockito.Mockito.verify;\r\n+import static org.mockito.Mockito.when;\r\n+\r\n+import java.util.ArrayList;\r\n+import java.util.HashSet;\r\n+import java.util.List;\r\n+import java.util.Set;\r\n+\r\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\r\n+import org.apache.hadoop.mapreduce.v2.app.MRAppMaster.RunningAppContext;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.JobCounterUpdateEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\r\n+import org.apache.hadoop.yarn.api.records.Container;\r\n+import org.apache.hadoop.yarn.api.records.ContainerId;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContainer;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionMessage;\r\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\r\n+import org.apache.hadoop.yarn.event.EventHandler;\r\n+import org.apache.hadoop.yarn.factories.RecordFactory;\r\n+import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\r\n+import org.junit.Test;\r\n+\r\n+public class TestKillAMPreemptionPolicy {\r\n+  private final RecordFactory recordFactory = RecordFactoryProvider\r\n+      .getRecordFactory(null);\r\n+\r\n+  @SuppressWarnings(\"unchecked\")\r\n+  @Test\r\n+  public void testKillAMPreemptPolicy() {\r\n+\r\n+    ApplicationId appId = ApplicationId.newInstance(123456789, 1);\r\n+    ContainerId container = ContainerId.newInstance(\r\n+        ApplicationAttemptId.newInstance(appId, 1), 1);\r\n+    AMPreemptionPolicy.Context mPctxt = mock(AMPreemptionPolicy.Context.class);\r\n+    when(mPctxt.getTaskAttempt(any(ContainerId.class))).thenReturn(\r\n+        MRBuilderUtils.newTaskAttemptId(MRBuilderUtils.newTaskId(\r\n+            MRBuilderUtils.newJobId(appId, 1), 1, TaskType.MAP), 0));\r\n+    List<Container> p = new ArrayList<Container>();\r\n+    p.add(Container.newInstance(container, null, null, null, null, null));\r\n+    when(mPctxt.getContainers(any(TaskType.class))).thenReturn(p);\r\n+\r\n+    KillAMPreemptionPolicy policy = new KillAMPreemptionPolicy();\r\n+\r\n+    // strictContract is null & contract is null\r\n+    RunningAppContext mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    PreemptionMessage pM = getPreemptionMessage(false, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(false, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+  }\r\n+\r\n+  private RunningAppContext getRunningAppContext() {\r\n+    RunningAppContext mActxt = mock(RunningAppContext.class);\r\n+    EventHandler<?> eventHandler = mock(EventHandler.class);\r\n+    when(mActxt.getEventHandler()).thenReturn(eventHandler);\r\n+    return mActxt;\r\n+  }\r\n+\r\n+  private PreemptionMessage getPreemptionMessage(boolean strictContract,\r\n+      boolean contract, final ContainerId container) {\r\n+    PreemptionMessage preemptionMessage = recordFactory\r\n+        .newRecordInstance(PreemptionMessage.class);\r\n+    Set<PreemptionContainer> cntrs = new HashSet<PreemptionContainer>();\r\n+    PreemptionContainer preemptContainer = recordFactory\r\n+        .newRecordInstance(PreemptionContainer.class);\r\n+    preemptContainer.setId(container);\r\n+    cntrs.add(preemptContainer);\r\n+    if (strictContract) {\r\n+      StrictPreemptionContract set = recordFactory\r\n+          .newRecordInstance(StrictPreemptionContract.class);\r\n+      set.setContainers(cntrs);\r\n+      preemptionMessage.setStrictContract(set);\r\n+    }\r\n+    if (contract) {\r\n+      PreemptionContract preemptContract = recordFactory\r\n+          .newRecordInstance(PreemptionContract.class);\r\n+      preemptContract.setContainers(cntrs);\r\n+      preemptionMessage.setContract(preemptContract);\r\n+    }\r\n+    return preemptionMessage;\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop-common/raw/56288adc4f5e6c922053c84c131c03f9ea29e79e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "sha": "fa930ae1262be2ed4687cc09456a5d80496c7b0d",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to ProportionalCapacityPreemptionPolicy. Contributed by Sunil G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595754 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/8544b0c207a058fe3f22f20e303e80ba16fd1557",
        "patched_files": [
            "KillAMPreemptionPolicy.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKillAMPreemptionPolicy.java"
        ]
    },
    "hadoop-common_57fbdb2": {
        "bug_id": "hadoop-common_57fbdb2",
        "commit": "https://github.com/apache/hadoop-common/commit/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt?ref=57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "patch": "@@ -24,6 +24,9 @@ fs-encryption (Unreleased)\n     HADOOP-10653. Add a new constructor for CryptoInputStream that \n     receives current position of wrapped stream. (Yi Liu)\n \n+    HADOOP-10662. NullPointerException in CryptoInputStream while wrapped\n+    stream is not ByteBufferReadable. Add tests using normal stream. (Yi Liu)\n+\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "sha": "66b55b0f50888a76c2a5cfc0fb17d04f9d18983f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java?ref=57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java",
                "patch": "@@ -172,6 +172,8 @@ public int read(byte[] b, int off, int len) throws IOException {\n           } catch (UnsupportedOperationException e) {\n             usingByteBufferRead = Boolean.FALSE;\n           }\n+        } else {\n+          usingByteBufferRead = Boolean.FALSE;\n         }\n         if (!usingByteBufferRead) {\n           n = readFromUnderlyingStream(inBuffer);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java",
                "sha": "55c891a0acea65abdb14f882d82eedd7e3a5ded1",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop-common/blob/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java",
                "changes": 123,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java?ref=57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java",
                "patch": "@@ -0,0 +1,123 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.crypto;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Test crypto streams using normal stream which does not support the \n+ * additional interfaces that the Hadoop FileSystem streams implement \n+ * (Seekable, PositionedReadable, ByteBufferReadable, HasFileDescriptor, \n+ * CanSetDropBehind, CanSetReadahead, HasEnhancedByteBufferAccess, Syncable, \n+ * CanSetDropBehind)\n+ */\n+public class TestCryptoStreamsNormal extends CryptoStreamsTestBase {\n+  /**\n+   * Data storage.\n+   * {@link #getOutputStream(int, byte[], byte[])} will write to this buffer.\n+   * {@link #getInputStream(int, byte[], byte[])} will read from this buffer.\n+   */\n+  private byte[] buffer;\n+  private int bufferLen;\n+  \n+  @BeforeClass\n+  public static void init() throws Exception {\n+    Configuration conf = new Configuration();\n+    codec = CryptoCodec.getInstance(conf);\n+  }\n+  \n+  @AfterClass\n+  public static void shutdown() throws Exception {\n+  }\n+\n+  @Override\n+  protected OutputStream getOutputStream(int bufferSize, byte[] key, byte[] iv)\n+      throws IOException {\n+    OutputStream out = new ByteArrayOutputStream() {\n+      @Override\n+      public void flush() throws IOException {\n+        buffer = buf;\n+        bufferLen = count;\n+      }\n+      @Override\n+      public void close() throws IOException {\n+        buffer = buf;\n+        bufferLen = count;\n+      }\n+    };\n+    return new CryptoOutputStream(out, codec, bufferSize, key, iv);\n+  }\n+\n+  @Override\n+  protected InputStream getInputStream(int bufferSize, byte[] key, byte[] iv)\n+      throws IOException {\n+    ByteArrayInputStream in = new ByteArrayInputStream(buffer, 0, bufferLen);\n+    return new CryptoInputStream(in, codec, bufferSize, \n+        key, iv);\n+  }\n+  \n+  @Ignore(\"Wrapped stream doesn't support Syncable\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testSyncable() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support PositionedRead\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testPositionedRead() throws IOException {}\n+\n+  @Ignore(\"Wrapped stream doesn't support ReadFully\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testReadFully() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support Seek\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testSeek() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support ByteBufferRead\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testByteBufferRead() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support ByteBufferRead, Seek\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testCombinedOp() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support SeekToNewSource\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testSeekToNewSource() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support HasEnhancedByteBufferAccess\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testHasEnhancedByteBufferAccess() throws IOException {}\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/57fbdb23c0b363ae8de40c5ffcc0765a3f3c43e9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java",
                "sha": "e9c313fde3695b344305cec39a0d7d03ac886f9f",
                "status": "added"
            }
        ],
        "message": "HADOOP-10662. NullPointerException in CryptoInputStream while wrapped stream is not ByteBufferReadable. Add tests using normal stream. Contributed by Yi Liu\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1600553 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0e3ff9b0f4579c90f9f93084a2882c10cd61aab2",
        "patched_files": [
            "CHANGES-fs-encryption.txt",
            "CryptoInputStream.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCryptoStreamsNormal.java"
        ]
    },
    "hadoop-common_5a79a63": {
        "bug_id": "hadoop-common_5a79a63",
        "commit": "https://github.com/apache/hadoop-common/commit/5a79a633a96797443600d9437cf79cb4dcbf1a11",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -539,6 +539,9 @@ Release 2.4.0 - UNRELEASED\n     YARN-1670. Fixed a bug in log-aggregation that can cause the writer to write\n     more log-data than the log-length that it records. (Mit Desai via vinodk)\n \n+    YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM\n+    (Karthik Kambatla via jianhe )\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/CHANGES.txt",
                "sha": "cfb0052a201002073a5a8de6ed7ca0adea4dea93",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 24,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.service.AbstractService;\n import org.apache.hadoop.util.VersionUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n@@ -187,12 +188,51 @@ protected void serviceStop() throws Exception {\n     super.serviceStop();\n   }\n \n+  /**\n+   * Helper method to handle received ContainerStatus. If this corresponds to\n+   * the completion of a master-container of a managed AM,\n+   * we call the handler for RMAppAttemptContainerFinishedEvent.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  @VisibleForTesting\n+  void handleContainerStatus(ContainerStatus containerStatus) {\n+    ApplicationAttemptId appAttemptId =\n+        containerStatus.getContainerId().getApplicationAttemptId();\n+    RMApp rmApp =\n+        rmContext.getRMApps().get(appAttemptId.getApplicationId());\n+    if (rmApp == null) {\n+      LOG.error(\"Received finished container : \"\n+          + containerStatus.getContainerId()\n+          + \"for unknown application \" + appAttemptId.getApplicationId()\n+          + \" Skipping.\");\n+      return;\n+    }\n+\n+    if (rmApp.getApplicationSubmissionContext().getUnmanagedAM()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Ignoring container completion status for unmanaged AM\"\n+            + rmApp.getApplicationId());\n+      }\n+      return;\n+    }\n+\n+    RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n+    Container masterContainer = rmAppAttempt.getMasterContainer();\n+    if (masterContainer.getId().equals(containerStatus.getContainerId())\n+        && containerStatus.getState() == ContainerState.COMPLETE) {\n+      // sending master container finished event.\n+      RMAppAttemptContainerFinishedEvent evt =\n+          new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+              containerStatus);\n+      rmContext.getDispatcher().getEventHandler().handle(evt);\n+    }\n+  }\n+\n   @SuppressWarnings(\"unchecked\")\n   @Override\n   public RegisterNodeManagerResponse registerNodeManager(\n       RegisterNodeManagerRequest request) throws YarnException,\n       IOException {\n-\n     NodeId nodeId = request.getNodeId();\n     String host = nodeId.getHost();\n     int cmPort = nodeId.getPort();\n@@ -204,29 +244,7 @@ public RegisterNodeManagerResponse registerNodeManager(\n       LOG.info(\"received container statuses on node manager register :\"\n           + request.getContainerStatuses());\n       for (ContainerStatus containerStatus : request.getContainerStatuses()) {\n-        ApplicationAttemptId appAttemptId =\n-            containerStatus.getContainerId().getApplicationAttemptId();\n-        RMApp rmApp =\n-            rmContext.getRMApps().get(appAttemptId.getApplicationId());\n-        if (rmApp != null) {\n-          RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt != null) {\n-            if (rmAppAttempt.getMasterContainer().getId()\n-                .equals(containerStatus.getContainerId())\n-                && containerStatus.getState() == ContainerState.COMPLETE) {\n-              // sending master container finished event.\n-              RMAppAttemptContainerFinishedEvent evt =\n-                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                      containerStatus);\n-              rmContext.getDispatcher().getEventHandler().handle(evt);\n-            }\n-          }\n-        } else {\n-          LOG.error(\"Received finished container :\"\n-              + containerStatus.getContainerId()\n-              + \" for non existing application :\"\n-              + appAttemptId.getApplicationId());\n-        }\n+        handleContainerStatus(containerStatus);\n       }\n     }\n     RegisterNodeManagerResponse response = recordFactory",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "1d4032048e468a9652093acc18cf0c0b82007a70",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -35,9 +35,11 @@\n \n import javax.crypto.SecretKey;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -629,7 +631,9 @@ public Container getMasterContainer() {\n     }\n   }\n \n-  private void setMasterContainer(Container container) {\n+  @InterfaceAudience.Private\n+  @VisibleForTesting\n+  public void setMasterContainer(Container container) {\n     masterContainer = container;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "3e90ec8ec1d5aaec6619f357ceaadecbb7bca194",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=5a79a633a96797443600d9437cf79cb4dcbf1a11",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -26,8 +26,6 @@\n import java.util.HashMap;\n import java.util.List;\n \n-import org.junit.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.metrics2.MetricsSystem;\n@@ -45,21 +43,29 @@\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.event.Event;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n import org.apache.hadoop.yarn.util.YarnVersionInfo;\n+\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.Test;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.verify;\n \n public class TestResourceTrackerService {\n \n@@ -468,26 +474,64 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   @Test\n-  public void testNodeRegistrationWithContainers() throws Exception {\n-    rm = new MockRM();\n-    rm.init(new YarnConfiguration());\n+  public void testHandleContainerStatusInvalidCompletions() throws Exception {\n+    rm = new MockRM(new YarnConfiguration());\n     rm.start();\n-    RMApp app = rm.submitApp(1024);\n \n-    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n-    nm.nodeHeartbeat(true);\n+    EventHandler handler =\n+        spy(rm.getRMContext().getDispatcher().getEventHandler());\n \n-    // Register node with some container statuses\n+    // Case 1: Unmanaged AM\n+    RMApp app = rm.submitApp(1024, true);\n+\n+    // Case 1.1: AppAttemptId is null\n     ContainerStatus status = ContainerStatus.newInstance(\n         ContainerId.newInstance(ApplicationAttemptId.newInstance(\n             app.getApplicationId(), 2), 1),\n         ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event) any());\n+\n+    // Case 1.2: Master container is null\n+    RMAppAttemptImpl currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event)any());\n \n-    // The following shouldn't throw NPE\n-    nm.registerNode(Collections.singletonList(status));\n-    assertEquals(\"Incorrect number of nodes\", 1,\n-        rm.getRMContext().getRMNodes().size());\n+    // Case 2: Managed AM\n+    app = rm.submitApp(1024);\n+\n+    // Case 2.1: AppAttemptId is null\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n+\n+    // Case 2.2: Master container is null\n+    currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5a79a633a96797443600d9437cf79cb4dcbf1a11/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "2f16b85699d3705be3743e4d54b069b42fab2e3d",
                "status": "modified"
            }
        ],
        "message": "YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM. Contributed by Karthik Kambatla\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1580077 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f3bd13bf8b31785c787632de80b90c81d30a6ff1",
        "patched_files": [
            "RMAppAttemptImpl.java",
            "CHANGES.txt",
            "ResourceTrackerService.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop-common_5eb5803": {
        "bug_id": "hadoop-common_5eb5803",
        "commit": "https://github.com/apache/hadoop-common/commit/5eb5803c3158502e02096a9aa42fa8520eb413f6",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java?ref=5eb5803c3158502e02096a9aa42fa8520eb413f6",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "patch": "@@ -202,8 +202,12 @@ public static String uriToString(URI[] uris){\n   }\n   \n   /**\n-   * \n    * @param str\n+   *          The string array to be parsed into an URI array.\n+   * @return <tt>null</tt> if str is <tt>null</tt>, else the URI array\n+   *         equivalent to str.\n+   * @throws IllegalArgumentException\n+   *           If any string in str violates RFC&nbsp;2396.\n    */\n   public static URI[] stringToURI(String[] str){\n     if (str == null) \n@@ -213,9 +217,8 @@ public static String uriToString(URI[] uris){\n       try{\n         uris[i] = new URI(str[i]);\n       }catch(URISyntaxException ur){\n-        System.out.println(\"Exception in specified URI's \" + StringUtils.stringifyException(ur));\n-        //making sure its asssigned to null in case of an error\n-        uris[i] = null;\n+        throw new IllegalArgumentException(\n+            \"Failed to create uri for \" + str[i], ur);\n       }\n     }\n     return uris;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "sha": "67a8f82d9382cfd19d0c803144248c213359a0be",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java?ref=5eb5803c3158502e02096a9aa42fa8520eb413f6",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "patch": "@@ -269,6 +269,17 @@ public void testCamelize() {\n     assertEquals(\"Yy\", StringUtils.camelize(\"yY\"));\n     assertEquals(\"Zz\", StringUtils.camelize(\"zZ\"));\n   }\n+  \n+  @Test\n+  public void testStringToURI() {\n+    String[] str = new String[] { \"file://\" };\n+    try {\n+      StringUtils.stringToURI(str);\n+      fail(\"Ignoring URISyntaxException while creating URI from string file://\");\n+    } catch (IllegalArgumentException iae) {\n+      assertEquals(\"Failed to create uri for file://\", iae.getMessage());\n+    }\n+  }\n \n   // Benchmark for StringUtils split\n   public static void main(String []args) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "sha": "fc90984608fb1a2630184419a840177223e9cb12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=5eb5803c3158502e02096a9aa42fa8520eb413f6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -236,6 +236,9 @@ Branch-2 ( Unreleased changes )\n     HADOOP-8499. Lower min.user.id to 500 for the tests.\n     (Colin Patrick McCabe via eli)\n \n+    MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager\n+    #determineTimestamps (Bhallamudi via bobby)\n+\n Release 2.0.0-alpha - 05-23-2012\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/5eb5803c3158502e02096a9aa42fa8520eb413f6/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "db352254f14824b21d8b6f84f401ed03722e859e",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager#determineTimestamps (Bhallamudi via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362052 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9f9af6ed5506f0ab0cd0a67b989f8b7f5032b058",
        "patched_files": [
            "CHANGES.txt",
            "StringUtils.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestStringUtils.java"
        ]
    },
    "hadoop-common_6275147": {
        "bug_id": "hadoop-common_6275147",
        "commit": "https://github.com/apache/hadoop-common/commit/62751475462b1157660bef55fa579317d8dc6965",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/62751475462b1157660bef55fa579317d8dc6965/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=62751475462b1157660bef55fa579317d8dc6965",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -237,6 +237,9 @@ Release 2.4.0 - UNRELEASED\n     MAPREDUCE-5656. bzip2 codec can drop records when reading data in splits\n     (jlowe)\n \n+    MAPREDUCE-5623. TestJobCleanup fails because of RejectedExecutionException\n+    and NPE. (jlowe)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/62751475462b1157660bef55fa579317d8dc6965/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "08e1c2cc21d8c609f4fd3c04943dbc45aa44a2cb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/62751475462b1157660bef55fa579317d8dc6965/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java?ref=62751475462b1157660bef55fa579317d8dc6965",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java",
                "patch": "@@ -195,8 +195,7 @@ private void testFailedJob(String fileName,\n     RunningJob job = jobClient.submitJob(jc);\n     JobID id = job.getID();\n     job.waitForCompletion();\n-    Counters counters = job.getCounters();\n-    assertTrue(\"No. of failed maps should be 1\",counters.getCounter(JobCounter.NUM_FAILED_MAPS) == 1);\n+    assertEquals(\"Job did not fail\", JobStatus.FAILED, job.getJobState());\n \n     if (fileName != null) {\n       Path testFile = new Path(outDir, fileName);\n@@ -242,9 +241,7 @@ private void testKilledJob(String fileName,\n     job.killJob(); // kill the job\n \n     job.waitForCompletion(); // wait for the job to complete\n-    \n-    counters = job.getCounters();\n-    assertTrue(\"No. of killed maps should be 1\", counters.getCounter(JobCounter.NUM_KILLED_MAPS) == 1);\n+    assertEquals(\"Job was not killed\", JobStatus.KILLED, job.getJobState());\n \n     if (fileName != null) {\n       Path testFile = new Path(outDir, fileName);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/62751475462b1157660bef55fa579317d8dc6965/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java",
                "sha": "bf762d93d92fbdb9c9f93aac7e3ebd9c2575ef01",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5623. TestJobCleanup fails because of RejectedExecutionException and NPE. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551285 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b9f33eaa4c702ee544b467be9f2288eb0ea85216",
        "patched_files": [
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobCleanup.java"
        ]
    },
    "hadoop-common_661f094": {
        "bug_id": "hadoop-common_661f094",
        "commit": "https://github.com/apache/hadoop-common/commit/661f09463aa06f2e3b808fbc360d35f114b95a44",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/661f09463aa06f2e3b808fbc360d35f114b95a44/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=661f09463aa06f2e3b808fbc360d35f114b95a44",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -1638,3 +1638,7 @@ Release 0.21.0 - Unreleased\n     (Dick King and Amareshwari Sriramadasu via tomwhite)\n \n     MAPREDUCE-118. Fix Job.getJobID(). (Amareshwari Sriramadasu via sharad)\n+\n+    MAPREDUCE-913. TaskRunner crashes with NPE resulting in held up slots,\n+    UNINITIALIZED tasks and hung TaskTracker. (Amareshwari Sriramadasu and\n+    Sreekanth Ramakrishnan via vinodkv)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/661f09463aa06f2e3b808fbc360d35f114b95a44/CHANGES.txt",
                "sha": "3f651ddeb64410b64f79c35f384806689996f768",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/hadoop-common/blob/661f09463aa06f2e3b808fbc360d35f114b95a44/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/TaskTracker.java?ref=661f09463aa06f2e3b808fbc360d35f114b95a44",
                "deletions": 43,
                "filename": "src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "patch": "@@ -2093,7 +2093,16 @@ private void addToTaskQueue(LaunchTaskAction action) {\n       reduceLauncher.addToTaskQueue(action);\n     }\n   }\n-  \n+\n+  // This method is called from unit tests\n+  int getFreeSlots(boolean isMap) {\n+    if (isMap) {\n+      return mapLauncher.numFreeSlots.get();\n+    } else {\n+      return reduceLauncher.numFreeSlots.get();\n+    }\n+  }\n+\n   class TaskLauncher extends Thread {\n     private IntWritable numFreeSlots;\n     private final int maxSlots;\n@@ -2657,8 +2666,11 @@ public boolean wasKilled() {\n      */\n     void reportTaskFinished(boolean commitPending) {\n       if (!commitPending) {\n-        taskFinished();\n-        releaseSlot();\n+        try {\n+          taskFinished(); \n+        } finally {\n+          releaseSlot();\n+        }\n       }\n       notifyTTAboutTaskCompletion();\n     }\n@@ -2728,7 +2740,15 @@ public void taskFinished() {\n             setTaskFailState(true);\n             // call the script here for the failed tasks.\n             if (debugCommand != null) {\n-              runDebugScript();\n+              try {\n+                runDebugScript();\n+              } catch (Exception e) {\n+                String msg =\n+                    \"Debug-script could not be run successfully : \"\n+                        + StringUtils.stringifyException(e);\n+                LOG.warn(msg);\n+                reportDiagnosticInfo(msg);\n+              }\n             }\n           }\n           taskStatus.setProgress(0.0f);\n@@ -2749,14 +2769,17 @@ public void taskFinished() {\n       if (needCleanup) {\n         removeTaskFromJob(task.getJobID(), this);\n       }\n-      try {\n-        cleanup(needCleanup);\n-      } catch (IOException ie) {\n-      }\n \n+      cleanup(needCleanup);\n     }\n-    \n-    private void runDebugScript() {\n+\n+    /**\n+     * Run the debug-script now. Because debug-script can be user code, we use\n+     * {@link TaskController} to execute the debug script.\n+     * \n+     * @throws IOException\n+     */\n+    private void runDebugScript() throws IOException {\n       String taskStdout =\"\";\n       String taskStderr =\"\";\n       String taskSyslog =\"\";\n@@ -2774,23 +2797,14 @@ private void runDebugScript() {\n         taskSyslog = FileUtil\n             .makeShellPath(TaskLog.getRealTaskLogFileLocation(task.getTaskID(),\n                 task.isTaskCleanupTask(), TaskLog.LogName.SYSLOG));\n-      } catch(IOException e){\n-        LOG.warn(\"Exception finding task's stdout/err/syslog files\");\n-      }\n-      File workDir = null;\n-      try {\n-        workDir =\n-            new File(lDirAlloc.getLocalPathToRead(\n-                TaskTracker.getLocalTaskDir(task.getUser(), task\n-                    .getJobID().toString(), task.getTaskID()\n-                    .toString(), task.isTaskCleanupTask())\n-                    + Path.SEPARATOR + MRConstants.WORKDIR,\n-                localJobConf).toString());\n-      } catch (IOException e) {\n-        LOG.warn(\"Working Directory of the task \" + task.getTaskID() +\n-                        \" doesnt exist. Caught exception \" +\n-                  StringUtils.stringifyException(e));\n-      }\n+      } catch(Exception e){\n+        LOG.warn(\"Exception finding task's stdout/err/syslog files\", e);\n+      }\n+      File workDir = new File(lDirAlloc.getLocalPathToRead(\n+          TaskTracker.getLocalTaskDir(task.getUser(), task.getJobID()\n+              .toString(), task.getTaskID().toString(), task\n+              .isTaskCleanupTask())\n+              + Path.SEPARATOR + MRConstants.WORKDIR, localJobConf).toString());\n       // Build the command  \n       File stdout = TaskLog.getTaskLogFile(task.getTaskID(), task\n           .isTaskCleanupTask(), TaskLog.LogName.DEBUGOUT);\n@@ -2820,21 +2834,10 @@ private void runDebugScript() {\n       context.stdout = stdout;\n       context.workDir = workDir;\n       context.task = task;\n-      try {\n-        getTaskController().runDebugScript(context);\n-        // add all lines of debug out to diagnostics\n-        try {\n-          int num = localJobConf.getInt(MRJobConfig.TASK_DEBUGOUT_LINES,\n-              -1);\n-          addDiagnostics(FileUtil.makeShellPath(stdout),num,\n-              \"DEBUG OUT\");\n-        } catch(IOException ioe) {\n-          LOG.warn(\"Exception in add diagnostics!\");\n-        }\n-      } catch (IOException ie) {\n-        LOG.warn(\"runDebugScript failed with: \" + StringUtils.\n-                                              stringifyException(ie));\n-      }\n+      getTaskController().runDebugScript(context);\n+      // add the lines of debug out to diagnostics\n+      int num = localJobConf.getInt(MRJobConfig.TASK_DEBUGOUT_LINES, -1);\n+      addDiagnostics(FileUtil.makeShellPath(stdout), num, \"DEBUG OUT\");\n     }\n \n     /**\n@@ -2998,7 +3001,7 @@ private synchronized void mapOutputLost(String failure\n      * otherwise the current working directory of the task \n      * i.e. &lt;taskid&gt;/work is cleaned up.\n      */\n-    void cleanup(boolean needCleanup) throws IOException {\n+    void cleanup(boolean needCleanup) {\n       TaskAttemptID taskId = task.getTaskID();\n       LOG.debug(\"Cleaning up \" + taskId);\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/661f09463aa06f2e3b808fbc360d35f114b95a44/src/java/org/apache/hadoop/mapred/TaskTracker.java",
                "sha": "c3d2bdc4afbf88cdc416ffddabd365cb6adf2dbd",
                "status": "modified"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/hadoop-common/blob/661f09463aa06f2e3b808fbc360d35f114b95a44/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java",
                "changes": 115,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java?ref=661f09463aa06f2e3b808fbc360d35f114b95a44",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java",
                "patch": "@@ -0,0 +1,115 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapred;\n+\n+import java.io.File;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.ClusterMetrics;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+/**\n+ * Regression test for MAPREDUCE-913\n+ */\n+public class TestTaskTrackerSlotManagement {\n+\n+  private static final Path TEST_DIR = new Path(System.getProperty(\n+      \"test.build.data\", \"/tmp\"), \"tt_slots\");\n+  private static final String CACHE_FILE_PATH = new Path(TEST_DIR, \"test.txt\")\n+      .toString();\n+\n+  /**\n+   * Test-setup. Create the cache-file.\n+   * \n+   * @throws Exception\n+   */\n+  @Before\n+  public void setUp() throws Exception {\n+    new File(TEST_DIR.toString()).mkdirs();\n+    File myFile = new File(CACHE_FILE_PATH);\n+    myFile.createNewFile();\n+  }\n+\n+  /**\n+   * Test-cleanup. Remove the cache-file.\n+   * \n+   * @throws Exception\n+   */\n+  @After\n+  public void tearDown() throws Exception {\n+    File myFile = new File(CACHE_FILE_PATH);\n+    myFile.delete();\n+    new File(TEST_DIR.toString()).delete();\n+  }\n+\n+  /**\n+   * Test case to test addition of free slot when the job fails localization due\n+   * to cache file being modified after the job has started running.\n+   * \n+   * @throws Exception\n+   */\n+  @Test\n+  public void testFreeingOfTaskSlots() throws Exception {\n+    // Start a cluster with no task tracker.\n+    MiniMRCluster mrCluster = new MiniMRCluster(0, \"file:///\", 1);\n+    Configuration conf = mrCluster.createJobConf();\n+    Cluster cluster = new Cluster(conf);\n+    // set the debug script so that TT tries to launch the debug\n+    // script for failed tasks.\n+    conf.set(JobContext.MAP_DEBUG_SCRIPT, \"/bin/echo\");\n+    conf.set(JobContext.REDUCE_DEBUG_SCRIPT, \"/bin/echo\");\n+    Job j = MapReduceTestUtil.createJob(conf, new Path(TEST_DIR, \"in\"),\n+        new Path(TEST_DIR, \"out\"), 0, 0);\n+    // Add the local filed created to the cache files of the job\n+    j.addCacheFile(new URI(CACHE_FILE_PATH));\n+    j.setMaxMapAttempts(1);\n+    j.setMaxReduceAttempts(1);\n+    // Submit the job and return immediately.\n+    // Job submit now takes care setting the last\n+    // modified time of the cache file.\n+    j.submit();\n+    // Look up the file and modify the modification time.\n+    File myFile = new File(CACHE_FILE_PATH);\n+    myFile.setLastModified(0L);\n+    // Start up the task tracker after the time has been changed.\n+    mrCluster.startTaskTracker(null, null, 0, 1);\n+    // Now wait for the job to fail.\n+    j.waitForCompletion(false);\n+    Assert.assertFalse(\"Job successfully completed.\", j.isSuccessful());\n+\n+    ClusterMetrics metrics = cluster.getClusterStatus();\n+    // validate number of slots in JobTracker\n+    Assert.assertEquals(0, metrics.getOccupiedMapSlots());\n+    Assert.assertEquals(0, metrics.getOccupiedReduceSlots());\n+\n+    // validate number of slots in TaskTracker\n+    TaskTracker tt = mrCluster.getTaskTrackerRunner(0).getTaskTracker();\n+    Assert.assertEquals(metrics.getMapSlotCapacity(), tt.getFreeSlots(true));\n+    Assert.assertEquals(metrics.getReduceSlotCapacity(), tt.getFreeSlots(false));\n+\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/661f09463aa06f2e3b808fbc360d35f114b95a44/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerSlotManagement.java",
                "sha": "3bfbc681844c8af1caea38518ddd38a6981f6c84",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-913. TaskRunner crashes with NPE resulting in held up slots, UNINITIALIZED tasks and hung TaskTracker. Contributed by Amareshwari Sriramadasu and Sreekanth Ramakrishnan.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@950485 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ac557915ad5905a424acc9f166bd154fbf3acb5e",
        "patched_files": [
            "TaskTracker.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTaskTrackerSlotManagement.java"
        ]
    },
    "hadoop-common_67055a5": {
        "bug_id": "hadoop-common_67055a5",
        "commit": "https://github.com/apache/hadoop-common/commit/67055a5c86a325cca505a0a53e3f0d04d80378ab",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1695,6 +1695,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-2788. Normalize resource requests in FifoScheduler\n     appropriately. (Ahmed Radwan via acmurthy) \n \n+    MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "22917ec8c83d7882471017e315f2646fb1a47dea",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "changes": 160,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 37,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "patch": "@@ -509,18 +509,6 @@ void addMap(ContainerRequestEvent event) {\n         request = new ContainerRequest(event, PRIORITY_FAST_FAIL_MAP);\n       } else {\n         for (String host : event.getHosts()) {\n-          //host comes from data splitLocations which are hostnames. Containers\n-          // use IP addresses.\n-          //TODO Temporary fix for locality. Use resolvers from h-common. \n-          // Cache to make this more efficient ?\n-          InetAddress addr = null;\n-          try {\n-            addr = InetAddress.getByName(host);\n-          } catch (UnknownHostException e) {\n-            LOG.warn(\"Unable to resolve host to IP for host [: \" + host + \"]\");\n-          }\n-          if (addr != null) //Fallback to host if resolve fails.\n-            host = addr.getHostAddress();\n           LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n           if (list == null) {\n             list = new LinkedList<TaskAttemptId>();\n@@ -557,26 +545,101 @@ private void assign(List<Container> allocatedContainers) {\n       while (it.hasNext()) {\n         Container allocated = it.next();\n         LOG.info(\"Assigning container \" + allocated);\n-        ContainerRequest assigned = assign(allocated);\n-          \n-        if (assigned != null) {\n-          // Update resource requests\n-          decContainerReq(assigned);\n+        \n+        // check if allocated container meets memory requirements \n+        // and whether we have any scheduled tasks that need \n+        // a container to be assigned\n+        boolean isAssignable = true;\n+        Priority priority = allocated.getPriority();\n+        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+            || PRIORITY_MAP.equals(priority)) {\n+          if (allocated.getResource().getMemory() < mapResourceReqt\n+              || maps.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a map as either \"\n+                + \" container memory less than required \" + mapResourceReqt\n+                + \" or no pending map tasks - maps.isEmpty=\" \n+                + maps.isEmpty()); \n+            isAssignable = false; \n+          }\n+        } \n+        else if (PRIORITY_REDUCE.equals(priority)) {\n+          if (allocated.getResource().getMemory() < reduceResourceReqt\n+              || reduces.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a reduce as either \"\n+                + \" container memory less than required \" + reduceResourceReqt\n+                + \" or no pending reduce tasks - reduces.isEmpty=\" \n+                + reduces.isEmpty()); \n+            isAssignable = false;\n+          }\n+        }          \n+        \n+        boolean blackListed = false;         \n+        ContainerRequest assigned = null;\n+        \n+        if (isAssignable) {\n+          // do not assign if allocated container is on a  \n+          // blacklisted host\n+          blackListed = isNodeBlacklisted(allocated.getNodeId().getHost());\n+          if (blackListed) {\n+            // we need to request for a new container \n+            // and release the current one\n+            LOG.info(\"Got allocated container on a blacklisted \"\n+                + \" host. Releasing container \" + allocated);\n+\n+            // find the request matching this allocated container \n+            // and replace it with a new one \n+            ContainerRequest toBeReplacedReq = \n+                getContainerReqToReplace(allocated);\n+            if (toBeReplacedReq != null) {\n+              LOG.info(\"Placing a new container request for task attempt \" \n+                  + toBeReplacedReq.attemptID);\n+              ContainerRequest newReq = \n+                  getFilteredContainerRequest(toBeReplacedReq);\n+              decContainerReq(toBeReplacedReq);\n+              if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n+                  TaskType.MAP) {\n+                maps.put(newReq.attemptID, newReq);\n+              }\n+              else {\n+                reduces.put(newReq.attemptID, newReq);\n+              }\n+              addContainerReq(newReq);\n+            }\n+            else {\n+              LOG.info(\"Could not map allocated container to a valid request.\"\n+                  + \" Releasing allocated container \" + allocated);\n+            }\n+          }\n+          else {\n+            assigned = assign(allocated);\n+            if (assigned != null) {\n+              // Update resource requests\n+              decContainerReq(assigned);\n \n-          // send the container-assigned event to task attempt\n-          eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n-              assigned.attemptID, allocated));\n+              // send the container-assigned event to task attempt\n+              eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n+                  assigned.attemptID, allocated));\n \n-          assignedRequests.add(allocated.getId(), assigned.attemptID);\n-          \n-          LOG.info(\"Assigned container (\" + allocated + \") \" +\n-              \" to task \" + assigned.attemptID +\n-              \" on node \" + allocated.getNodeId().toString());\n-        } else {\n-          //not assigned to any request, release the container\n-          LOG.info(\"Releasing unassigned and invalid container \" + allocated\n-              + \". RM has gone crazy, someone go look!\"\n-              + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+              assignedRequests.add(allocated.getId(), assigned.attemptID);\n+\n+              LOG.info(\"Assigned container (\" + allocated + \") \" +\n+                  \" to task \" + assigned.attemptID +\n+                  \" on node \" + allocated.getNodeId().toString());\n+            }\n+            else {\n+              //not assigned to any request, release the container\n+              LOG.info(\"Releasing unassigned and invalid container \" \n+                  + allocated + \". RM has gone crazy, someone go look!\"\n+                  + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+            }\n+          }\n+        }\n+        \n+        // release container if it was blacklisted \n+        // or if we could not assign it \n+        if (blackListed || assigned == null) {\n           containersReleased++;\n           release(allocated.getId());\n         }\n@@ -604,12 +667,37 @@ private ContainerRequest assign(Container allocated) {\n       return assigned;\n     }\n     \n+    private ContainerRequest getContainerReqToReplace(Container allocated) {\n+      Priority priority = allocated.getPriority();\n+      ContainerRequest toBeReplaced = null;\n+      if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+          || PRIORITY_MAP.equals(priority)) {\n+        // allocated container was for a map\n+        String host = allocated.getNodeId().getHost();\n+        LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n+        if (list != null && list.size() > 0) {\n+          TaskAttemptId tId = list.removeLast();\n+          if (maps.containsKey(tId)) {\n+            toBeReplaced = maps.remove(tId);\n+          }\n+        }\n+        else {\n+          TaskAttemptId tId = maps.keySet().iterator().next();\n+          toBeReplaced = maps.remove(tId);          \n+        }        \n+      }\n+      else if (PRIORITY_REDUCE.equals(priority)) {\n+        TaskAttemptId tId = reduces.keySet().iterator().next();\n+        toBeReplaced = reduces.remove(tId);    \n+      }\n+      return toBeReplaced;\n+    }\n+    \n     \n     private ContainerRequest assignToFailedMap(Container allocated) {\n       //try to assign to earlierFailedMaps if present\n       ContainerRequest assigned = null;\n-      while (assigned == null && earlierFailedMaps.size() > 0 && \n-          allocated.getResource().getMemory() >= mapResourceReqt) {\n+      while (assigned == null && earlierFailedMaps.size() > 0) {\n         TaskAttemptId tId = earlierFailedMaps.removeFirst();\n         if (maps.containsKey(tId)) {\n           assigned = maps.remove(tId);\n@@ -627,8 +715,7 @@ private ContainerRequest assignToFailedMap(Container allocated) {\n     private ContainerRequest assignToReduce(Container allocated) {\n       ContainerRequest assigned = null;\n       //try to assign to reduces if present\n-      if (assigned == null && reduces.size() > 0\n-          && allocated.getResource().getMemory() >= reduceResourceReqt) {\n+      if (assigned == null && reduces.size() > 0) {\n         TaskAttemptId tId = reduces.keySet().iterator().next();\n         assigned = reduces.remove(tId);\n         LOG.info(\"Assigned to reduce\");\n@@ -640,9 +727,8 @@ private ContainerRequest assignToMap(Container allocated) {\n     //try to assign to maps if present \n       //first by host, then by rack, followed by *\n       ContainerRequest assigned = null;\n-      while (assigned == null && maps.size() > 0\n-          && allocated.getResource().getMemory() >= mapResourceReqt) {\n-        String host = getHost(allocated.getNodeId().toString());\n+      while (assigned == null && maps.size() > 0) {\n+        String host = allocated.getNodeId().getHost();\n         LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n         while (list != null && list.size() > 0) {\n           LOG.info(\"Host matched to the request list \" + host);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "sha": "fc45b8f11b285a1bb0fc475c184f61b8e1c65175",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 11,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.hadoop.mapreduce.v2.app.rm;\n \n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -63,7 +65,7 @@\n   //Key->ResourceName (e.g., hostname, rackname, *)\n   //Value->Map\n   //Key->Resource Capability\n-  //Value->ResourceReqeust\n+  //Value->ResourceRequest\n   private final Map<Priority, Map<String, Map<Resource, ResourceRequest>>>\n   remoteRequestsTable =\n       new TreeMap<Priority, Map<String, Map<Resource, ResourceRequest>>>();\n@@ -87,14 +89,22 @@ public RMContainerRequestor(ClientService clientService, AppContext context) {\n     final String[] racks;\n     //final boolean earlierAttemptFailed;\n     final Priority priority;\n+    \n     public ContainerRequest(ContainerRequestEvent event, Priority priority) {\n-      this.attemptID = event.getAttemptID();\n-      this.capability = event.getCapability();\n-      this.hosts = event.getHosts();\n-      this.racks = event.getRacks();\n-      //this.earlierAttemptFailed = event.getEarlierAttemptFailed();\n+      this(event.getAttemptID(), event.getCapability(), event.getHosts(),\n+          event.getRacks(), priority);\n+    }\n+    \n+    public ContainerRequest(TaskAttemptId attemptID,\n+        Resource capability, String[] hosts, String[] racks, \n+        Priority priority) {\n+      this.attemptID = attemptID;\n+      this.capability = capability;\n+      this.hosts = hosts;\n+      this.racks = racks;\n       this.priority = priority;\n     }\n+    \n   }\n \n   @Override\n@@ -149,14 +159,37 @@ protected void containerFailedOnHost(String hostName) {\n       //remove all the requests corresponding to this hostname\n       for (Map<String, Map<Resource, ResourceRequest>> remoteRequests \n           : remoteRequestsTable.values()){\n-        //remove from host\n-        Map<Resource, ResourceRequest> reqMap = remoteRequests.remove(hostName);\n+        //remove from host if no pending allocations\n+        boolean foundAll = true;\n+        Map<Resource, ResourceRequest> reqMap = remoteRequests.get(hostName);\n         if (reqMap != null) {\n           for (ResourceRequest req : reqMap.values()) {\n-            ask.remove(req);\n+            if (!ask.remove(req)) {\n+              foundAll = false;\n+            }\n+            else {\n+              // if ask already sent to RM, we can try and overwrite it if possible.\n+              // send a new ask to RM with numContainers\n+              // specified for the blacklisted host to be 0.\n+              ResourceRequest zeroedRequest = BuilderUtils.newResourceRequest(req);\n+              zeroedRequest.setNumContainers(0);\n+              // to be sent to RM on next heartbeat\n+              ask.add(zeroedRequest);\n+            }\n           }\n+          // if all requests were still in ask queue\n+          // we can remove this request\n+          if (foundAll) {\n+            remoteRequests.remove(hostName);\n+          }     \n         }\n-        //TODO: remove from rack\n+        // TODO handling of rack blacklisting\n+        // Removing from rack should be dependent on no. of failures within the rack \n+        // Blacklisting a rack on the basis of a single node's blacklisting \n+        // may be overly aggressive. \n+        // Node failures could be co-related with other failures on the same rack \n+        // but we probably need a better approach at trying to decide how and when \n+        // to blacklist a rack\n       }\n     } else {\n       nodeFailures.put(hostName, failures);\n@@ -171,7 +204,9 @@ protected void addContainerReq(ContainerRequest req) {\n     // Create resource requests\n     for (String host : req.hosts) {\n       // Data-local\n-      addResourceRequest(req.priority, host, req.capability);\n+      if (!isNodeBlacklisted(host)) {\n+        addResourceRequest(req.priority, host, req.capability);\n+      }      \n     }\n \n     // Nothing Rack-local for now\n@@ -234,6 +269,14 @@ private void decResourceRequest(Priority priority, String resourceName,\n     Map<String, Map<Resource, ResourceRequest>> remoteRequests =\n       this.remoteRequestsTable.get(priority);\n     Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);\n+    if (reqMap == null) {\n+      // as we modify the resource requests by filtering out blacklisted hosts \n+      // when they are added, this value may be null when being \n+      // decremented\n+      LOG.debug(\"Not decrementing resource as \" + resourceName\n+          + \" is not present in request table\");\n+      return;\n+    }\n     ResourceRequest remoteRequest = reqMap.get(capability);\n \n     LOG.info(\"BEFORE decResourceRequest:\" + \" applicationId=\" + applicationId.getId()\n@@ -267,4 +310,23 @@ protected void release(ContainerId containerId) {\n     release.add(containerId);\n   }\n   \n+  protected boolean isNodeBlacklisted(String hostname) {\n+    if (!nodeBlacklistingEnabled) {\n+      return false;\n+    }\n+    return blacklistedNodes.contains(hostname);\n+  }\n+  \n+  protected ContainerRequest getFilteredContainerRequest(ContainerRequest orig) {\n+    ArrayList<String> newHosts = new ArrayList<String>();\n+    for (String host : orig.hosts) {\n+      if (!isNodeBlacklisted(host)) {\n+        newHosts.add(host);      \n+      }\n+    }\n+    String[] hosts = newHosts.toArray(new String[newHosts.size()]);\n+    ContainerRequest newReq = new ContainerRequest(orig.attemptID, orig.capability,\n+        hosts, orig.racks, orig.priority); \n+    return newReq;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "sha": "cfedde2229aadb4355bc2ea93f25d3d13d83e858",
                "status": "modified"
            },
            {
                "additions": 121,
                "blob_url": "https://github.com/apache/hadoop-common/blob/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "changes": 121,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java?ref=67055a5c86a325cca505a0a53e3f0d04d80378ab",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobReport;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n@@ -44,6 +45,7 @@\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptContainerAssignedEvent;\n import org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl;\n+import org.apache.hadoop.mapreduce.v2.app.rm.ContainerFailedEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator;\n import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\n@@ -478,6 +480,105 @@ public void testReportedAppProgressWithOnlyMaps() throws Exception {\n     Assert.assertEquals(100.0f, app.getProgress(), 0.0);\n   }\n \n+  @Test\n+  public void testBlackListedNodes() throws Exception {\n+    \n+    LOG.info(\"Running testBlackListedNodes\");\n+\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\n+    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\n+    \n+    MyResourceManager rm = new MyResourceManager(conf);\n+    rm.start();\n+    DrainDispatcher dispatcher = (DrainDispatcher) rm.getRMContext()\n+        .getDispatcher();\n+\n+    // Submit the application\n+    RMApp app = rm.submitApp(1024);\n+    dispatcher.await();\n+\n+    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\n+    amNodeManager.nodeHeartbeat(true);\n+    dispatcher.await();\n+\n+    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt()\n+        .getAppAttemptId();\n+    rm.sendAMLaunched(appAttemptId);\n+    dispatcher.await();\n+    \n+    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getReport()).thenReturn(\n+        MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING,\n+            0, 0, 0, 0, 0, 0, \"jobfile\"));\n+    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf,\n+        appAttemptId, mockJob);\n+\n+    // add resources to scheduler\n+    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\n+    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\n+    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\n+    dispatcher.await();\n+\n+    // create the container request\n+    ContainerRequestEvent event1 = createReq(jobId, 1, 1024,\n+        new String[] { \"h1\" });\n+    allocator.sendRequest(event1);\n+\n+    // send 1 more request with different resource req\n+    ContainerRequestEvent event2 = createReq(jobId, 2, 1024,\n+        new String[] { \"h2\" });\n+    allocator.sendRequest(event2);\n+\n+    // send another request with different resource and priority\n+    ContainerRequestEvent event3 = createReq(jobId, 3, 1024,\n+        new String[] { \"h3\" });\n+    allocator.sendRequest(event3);\n+\n+    // this tells the scheduler about the requests\n+    // as nodes are not added, no allocations\n+    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\n+\n+    // Send events to blacklist nodes h1 and h2\n+    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);            \n+    allocator.sendFailure(f1);\n+    ContainerFailedEvent f2 = createFailEvent(jobId, 1, \"h2\", false);            \n+    allocator.sendFailure(f2);\n+\n+    // update resources in scheduler\n+    nodeManager1.nodeHeartbeat(true); // Node heartbeat\n+    nodeManager2.nodeHeartbeat(true); // Node heartbeat\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    // mark h1/h2 as bad nodes\n+    nodeManager1.nodeHeartbeat(false);\n+    nodeManager2.nodeHeartbeat(false);\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    nodeManager3.nodeHeartbeat(true); // Node heartbeat\n+    assigned = allocator.schedule();    \n+    dispatcher.await();\n+        \n+    Assert.assertTrue(\"No of assignments must be 3\", assigned.size() == 3);\n+    \n+    // validate that all containers are assigned to h3\n+    for (TaskAttemptContainerAssignedEvent assig : assigned) {\n+      Assert.assertTrue(\"Assigned container host not correct\", \"h3\".equals(assig\n+          .getContainer().getNodeId().getHost()));\n+    }\n+  }\n+  \n   private static class MyFifoScheduler extends FifoScheduler {\n \n     public MyFifoScheduler(RMContext rmContext) {\n@@ -534,6 +635,19 @@ private ContainerRequestEvent createReq(JobId jobId, int taskAttemptId,\n         new String[] { NetworkTopology.DEFAULT_RACK });\n   }\n \n+  private ContainerFailedEvent createFailEvent(JobId jobId, int taskAttemptId,\n+      String host, boolean reduce) {\n+    TaskId taskId;\n+    if (reduce) {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\n+    } else {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\n+    }\n+    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId,\n+        taskAttemptId);\n+    return new ContainerFailedEvent(attemptId, host);    \n+  }\n+  \n   private void checkAssignments(ContainerRequestEvent[] requests,\n       List<TaskAttemptContainerAssignedEvent> assignments,\n       boolean checkHostMatch) {\n@@ -653,6 +767,10 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n       }\n     }\n \n+    public void sendFailure(ContainerFailedEvent f) {\n+      super.handle(f);\n+    }\n+    \n     // API to be used by tests\n     public List<TaskAttemptContainerAssignedEvent> schedule() {\n       // run the scheduler\n@@ -672,6 +790,7 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n     protected void startAllocatorThread() {\n       // override to NOT start thread\n     }\n+        \n   }\n \n   public static void main(String[] args) throws Exception {\n@@ -681,5 +800,7 @@ public static void main(String[] args) throws Exception {\n     t.testMapReduceScheduling();\n     t.testReportedAppProgress();\n     t.testReportedAppProgressWithOnlyMaps();\n+    t.testBlackListedNodes();\n   }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/67055a5c86a325cca505a0a53e3f0d04d80378ab/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "sha": "dfbae8092c0b8f0e576ea16ce911cf9eb47e4e40",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2693. Fix NPE in job-blacklisting. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186529 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/07d7bb056fa9369ff5c1abac5a5b41109587c659",
        "patched_files": [
            "RMContainerRequestor.java",
            "RMContainerAllocator.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMContainerAllocator.java"
        ]
    },
    "hadoop-common_6802695": {
        "bug_id": "hadoop-common_6802695",
        "commit": "https://github.com/apache/hadoop-common/commit/6802695868b11e9a403384eb37ab9aa5661e90d1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6802695868b11e9a403384eb37ab9aa5661e90d1/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=6802695868b11e9a403384eb37ab9aa5661e90d1",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -455,6 +455,8 @@ Release 0.22.0 - Unreleased\n \n     HADOOP-7046. Fix Findbugs warning in Configuration. (Po Cheung via shv)\n \n+    HADOOP-7118. Fix NPE in Configuration.writeXml (todd)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6802695868b11e9a403384eb37ab9aa5661e90d1/CHANGES.txt",
                "sha": "18504560f655dd8c1cf667f83c94190fcf02c6b3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6802695868b11e9a403384eb37ab9aa5661e90d1/src/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/conf/Configuration.java?ref=6802695868b11e9a403384eb37ab9aa5661e90d1",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -1620,6 +1620,7 @@ private synchronized Document asXmlDocument() throws IOException {\n     Element conf = doc.createElement(\"configuration\");\n     doc.appendChild(conf);\n     conf.appendChild(doc.createTextNode(\"\\n\"));\n+    getProps(); // ensure properties is set\n     for (Enumeration e = properties.keys(); e.hasMoreElements();) {\n       String name = (String)e.nextElement();\n       Object object = properties.get(name);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6802695868b11e9a403384eb37ab9aa5661e90d1/src/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "c05acf36208020f5eca5fee3f4573c6e2292ace9",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6802695868b11e9a403384eb37ab9aa5661e90d1/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/conf/TestConfiguration.java?ref=6802695868b11e9a403384eb37ab9aa5661e90d1",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.conf;\n \n import java.io.BufferedWriter;\n+import java.io.ByteArrayOutputStream;\n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n@@ -255,6 +256,16 @@ public void testToString() throws IOException {\n     assertEquals(expectedOutput, conf.toString());\n   }\n   \n+  public void testWriteXml() throws IOException {\n+    Configuration conf = new Configuration();\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(); \n+    conf.writeXml(baos);\n+    String result = baos.toString();\n+    assertTrue(\"Result has proper header\", result.startsWith(\n+        \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" standalone=\\\"no\\\"?><configuration>\"));\n+    assertTrue(\"Result has proper footer\", result.endsWith(\"</configuration>\"));\n+  }\n+  \n   public void testIncludes() throws Exception {\n     tearDown();\n     System.out.println(\"XXX testIncludes\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6802695868b11e9a403384eb37ab9aa5661e90d1/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "sha": "fc9deef34ff7c4cc1cf51427257e6731420180b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7118. Fix NPE in Configuration.writeXml. Contributed by Todd Lipcon\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1063613 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d0f318899ac44fc592900b184f6d59acae4ef74c",
        "patched_files": [
            "Configuration.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestConfiguration.java"
        ]
    },
    "hadoop-common_6c1e091": {
        "bug_id": "hadoop-common_6c1e091",
        "commit": "https://github.com/apache/hadoop-common/commit/6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -280,6 +280,8 @@ Release 2.4.0 - UNRELEASED\n     MAPREDUCE-5724. JobHistoryServer does not start if HDFS is not running. \n     (tucu)\n \n+    MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "88b0ff8ec1ae84287873f6f5ba86abadc8e0a6ed",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java?ref=6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.hadoop.mapreduce.v2.util.MRApps;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueACL;\n@@ -445,11 +446,18 @@ public static JobStatus fromYarn(ApplicationReport application,\n     jobStatus.setStartTime(application.getStartTime());\n     jobStatus.setFinishTime(application.getFinishTime());\n     jobStatus.setFailureInfo(application.getDiagnostics());\n-    jobStatus.setNeededMem(application.getApplicationResourceUsageReport().getNeededResources().getMemory());\n-    jobStatus.setNumReservedSlots(application.getApplicationResourceUsageReport().getNumReservedContainers());\n-    jobStatus.setNumUsedSlots(application.getApplicationResourceUsageReport().getNumUsedContainers());\n-    jobStatus.setReservedMem(application.getApplicationResourceUsageReport().getReservedResources().getMemory());\n-    jobStatus.setUsedMem(application.getApplicationResourceUsageReport().getUsedResources().getMemory());\n+    ApplicationResourceUsageReport resourceUsageReport =\n+        application.getApplicationResourceUsageReport();\n+    if (resourceUsageReport != null) {\n+      jobStatus.setNeededMem(\n+          resourceUsageReport.getNeededResources().getMemory());\n+      jobStatus.setNumReservedSlots(\n+          resourceUsageReport.getNumReservedContainers());\n+      jobStatus.setNumUsedSlots(resourceUsageReport.getNumUsedContainers());\n+      jobStatus.setReservedMem(\n+          resourceUsageReport.getReservedResources().getMemory());\n+      jobStatus.setUsedMem(resourceUsageReport.getUsedResources().getMemory());\n+    }\n     return jobStatus;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "sha": "6b4aa4ed1e40a57ce9558d146e02b40b60bc35ea",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java?ref=6c1e091d998ed6980226bc2ddbe095d44e56a5fb",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "patch": "@@ -23,8 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import junit.framework.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobStatus.State;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n@@ -40,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.util.Records;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n@@ -112,6 +111,14 @@ public void testFromYarnApplicationReport() {\n     when(mockReport.getUser()).thenReturn(\"dummy-user\");\n     when(mockReport.getQueue()).thenReturn(\"dummy-queue\");\n     String jobFile = \"dummy-path/job.xml\";\n+\n+    try {\n+      JobStatus status = TypeConverter.fromYarn(mockReport, jobFile);\n+    } catch (NullPointerException npe) {\n+      Assert.fail(\"Type converstion from YARN fails for jobs without \" +\n+          \"ApplicationUsageReport\");\n+    }\n+\n     ApplicationResourceUsageReport appUsageRpt = Records\n         .newRecord(ApplicationResourceUsageReport.class);\n     Resource r = Records.newRecord(Resource.class);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6c1e091d998ed6980226bc2ddbe095d44e56a5fb/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "sha": "cc42b9c220f4b7704379054d7d90025117aed5d5",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1559811 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/69419b49d6740ffa46ddace7b4ee6f09205e77ae",
        "patched_files": [
            "TypeConverter.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTypeConverter.java"
        ]
    },
    "hadoop-common_6eaca1a": {
        "bug_id": "hadoop-common_6eaca1a",
        "commit": "https://github.com/apache/hadoop-common/commit/6eaca1a1251509d36950913d16d2833dec287cf0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6eaca1a1251509d36950913d16d2833dec287cf0/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=6eaca1a1251509d36950913d16d2833dec287cf0",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -280,6 +280,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4051. Remove the empty hadoop-mapreduce-project/assembly/all.xml\n     file (Ravi Prakash via bobby)\n \n+    MAPREDUCE-4117. mapred job -status throws NullPointerException (Devaraj K\n+    via bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6eaca1a1251509d36950913d16d2833dec287cf0/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "719669dd848c568c6a16b048950dd24ac595fe44",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6eaca1a1251509d36950913d16d2833dec287cf0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java?ref=6eaca1a1251509d36950913d16d2833dec287cf0",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "patch": "@@ -509,6 +509,11 @@ String getTaskFailureEventString() throws IOException,\n         lastEvent = event;\n       }\n     }\n+    if (lastEvent == null) {\n+      return \"There are no failed tasks for the job. \"\n+          + \"Job is failed due to some other reason and reason \"\n+          + \"can be found in the logs.\";\n+    }\n     String[] taskAttemptID = lastEvent.getTaskAttemptId().toString().split(\"_\", 2);\n     String taskID = taskAttemptID[1].substring(0, taskAttemptID[1].length()-2);\n     return (\" task \" + taskID + \" failed \" +",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6eaca1a1251509d36950913d16d2833dec287cf0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "sha": "51bac982285f65639d4d0ec49bd271c569287d40",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6eaca1a1251509d36950913d16d2833dec287cf0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java?ref=6eaca1a1251509d36950913d16d2833dec287cf0",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "patch": "@@ -0,0 +1,53 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapreduce;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.JobStatus.State;\n+import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestJob {\n+  @Test\n+  public void testJobToString() throws IOException, InterruptedException {\n+    Cluster cluster = mock(Cluster.class);\n+    ClientProtocol client = mock(ClientProtocol.class);\n+    when(cluster.getClient()).thenReturn(client);\n+    JobID jobid = new JobID(\"1014873536921\", 6);\n+    JobStatus status = new JobStatus(jobid, 0.0f, 0.0f, 0.0f, 0.0f,\n+        State.FAILED, JobPriority.NORMAL, \"root\", \"TestJobToString\",\n+        \"job file\", \"tracking url\");\n+    when(client.getJobStatus(jobid)).thenReturn(status);\n+    when(client.getTaskReports(jobid, TaskType.MAP)).thenReturn(\n+        new TaskReport[0]);\n+    when(client.getTaskReports(jobid, TaskType.REDUCE)).thenReturn(\n+        new TaskReport[0]);\n+    when(client.getTaskCompletionEvents(jobid, 0, 10)).thenReturn(\n+        new TaskCompletionEvent[0]);\n+    Job job = Job.getInstance(cluster, status, new JobConf());\n+    Assert.assertNotNull(job.toString());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6eaca1a1251509d36950913d16d2833dec287cf0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "sha": "110acba20808c16bc06af3ede5b25b69c708a84e",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-4117. mapred job -status throws NullPointerException (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311479 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/704a3bd17fc17fc0cf25ea63cbc48b5454aef0dd",
        "patched_files": [
            "Job.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJob.java"
        ]
    },
    "hadoop-common_6f01132": {
        "bug_id": "hadoop-common_6f01132",
        "commit": "https://github.com/apache/hadoop-common/commit/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "patch": "@@ -51,3 +51,6 @@ fs-encryption (Unreleased)\n   BUG FIXES\n \n     HADOOP-10871. incorrect prototype in OpensslSecureRandom.c (cmccabe)\n+\n+    HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not \n+    loaded. (umamahesh)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "sha": "498307e6bc7804db4ecdbc177b6561be579f7195",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "patch": "@@ -45,14 +45,21 @@\n   \n   /**\n    * Get crypto codec for specified algorithm/mode/padding.\n-   * @param conf the configuration\n-   * @param CipherSuite algorithm/mode/padding\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @param CipherSuite\n+   *          algorithm/mode/padding\n+   * @return CryptoCodec the codec object. Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf, \n       CipherSuite cipherSuite) {\n     List<Class<? extends CryptoCodec>> klasses = getCodecClasses(\n         conf, cipherSuite);\n+    if (klasses == null) {\n+      return null;\n+    }\n     CryptoCodec codec = null;\n     for (Class<? extends CryptoCodec> klass : klasses) {\n       try {\n@@ -80,10 +87,13 @@ public static CryptoCodec getInstance(Configuration conf,\n   }\n   \n   /**\n-   * Get crypto codec for algorithm/mode/padding in config value \n+   * Get crypto codec for algorithm/mode/padding in config value\n    * hadoop.security.crypto.cipher.suite\n-   * @param conf the configuration\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @return CryptoCodec the codec object Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf) {\n     String name = conf.get(HADOOP_SECURITY_CRYPTO_CIPHER_SUITE_KEY, \n@@ -97,6 +107,10 @@ public static CryptoCodec getInstance(Configuration conf) {\n     String configName = HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX + \n         cipherSuite.getConfigSuffix();\n     String codecString = conf.get(configName);\n+    if (codecString == null) {\n+      LOG.warn(\"No crypto codec classes with cipher suite configured.\");\n+      return null;\n+    }\n     for (String c : Splitter.on(',').trimResults().omitEmptyStrings().\n         split(codecString)) {\n       try {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "sha": "9de7f95200f5f44e440619269fbc2ae1135f599d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.io.OutputStream;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.LocalFileSystem;\n@@ -50,6 +51,11 @@ public static void init() throws Exception {\n     conf = new Configuration(false);\n     conf.set(\"fs.file.impl\", LocalFileSystem.class.getName());\n     fileSys = FileSystem.getLocal(conf);\n+    conf.set(\n+        CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX\n+            + CipherSuite.AES_CTR_NOPADDING.getConfigSuffix(),\n+        OpensslAesCtrCryptoCodec.class.getName() + \",\"\n+            + JceAesCtrCryptoCodec.class.getName());\n     codec = CryptoCodec.getInstance(conf);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "sha": "765a364faa6d5cffff9bd1a811caf51bc51b0cc5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -598,7 +598,9 @@ public DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode,\n         DFSUtil.getRandom().nextInt()  + \"_\" + Thread.currentThread().getId();\n     this.codec = CryptoCodec.getInstance(conf);\n     this.cipherSuites = Lists.newArrayListWithCapacity(1);\n-    cipherSuites.add(codec.getCipherSuite());\n+    if (codec != null) {\n+      cipherSuites.add(codec.getCipherSuite());\n+    }\n     provider = DFSUtil.createKeyProviderCryptoExtension(conf);\n     if (provider == null) {\n       LOG.info(\"No KeyProvider found.\");\n@@ -1333,9 +1335,12 @@ public HdfsDataInputStream createWrappedInputStream(DFSInputStream dfsis)\n     if (feInfo != null) {\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n+      CryptoCodec codec = CryptoCodec\n+          .getInstance(conf, feInfo.getCipherSuite());\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       final CryptoInputStream cryptoIn =\n-          new CryptoInputStream(dfsis, CryptoCodec.getInstance(conf, \n-              feInfo.getCipherSuite()), decrypted.getMaterial(),\n+          new CryptoInputStream(dfsis, codec, decrypted.getMaterial(),\n               feInfo.getIV());\n       return new HdfsDataInputStream(cryptoIn);\n     } else {\n@@ -1361,6 +1366,8 @@ public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos,\n       FileSystem.Statistics statistics, long startPos) throws IOException {\n     final FileEncryptionInfo feInfo = dfsos.getFileEncryptionInfo();\n     if (feInfo != null) {\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n       final CryptoOutputStream cryptoOut =",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "d3532607c84b80f9588043f78c1202a625bc1db9",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -38,7 +38,6 @@\n import java.util.EnumSet;\n import java.util.List;\n import java.util.Random;\n-import java.util.concurrent.CancellationException;\n \n import org.apache.commons.lang.ArrayUtils;\n import org.apache.commons.logging.impl.Log4JLogger;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f01132b7a6a4309562e54ffc8d5b489fc3b3fdb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "b71cc32fb80e93c51153e29045cbaa020c1bf461",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not loaded. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1615523 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e2140fb7ac61ed23b1e4974d8e4a4c0b89074520",
        "patched_files": [
            "CHANGES-fs-encryption.txt",
            "DistributedFileSystem.java",
            "CryptoCodec.java",
            "DFSClient.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCryptoCodec.java",
            "TestDistributedFileSystem.java",
            "TestCryptoStreamsForLocalFS.java"
        ]
    },
    "hadoop-common_6f3ce72": {
        "bug_id": "hadoop-common_6f3ce72",
        "commit": "https://github.com/apache/hadoop-common/commit/6f3ce726504bb5c57a5bd115536cbbbd63780dff",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f3ce726504bb5c57a5bd115536cbbbd63780dff/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=6f3ce726504bb5c57a5bd115536cbbbd63780dff",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -952,6 +952,9 @@ Release 2.1.0-beta - 2013-08-22\n     HDFS-5016. Deadlock in pipeline recovery causes Datanode to be marked dead.\n     (suresh)\n \n+    HDFS-5228. The RemoteIterator returned by DistributedFileSystem.listFiles\n+    may throw NullPointerException.  (szetszwo and cnauroth via szetszwo)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f3ce726504bb5c57a5bd115536cbbbd63780dff/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "096ead7f9924807cc4ddcc38c6af99de416481b4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f3ce726504bb5c57a5bd115536cbbbd63780dff/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java?ref=6f3ce726504bb5c57a5bd115536cbbbd63780dff",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
                "patch": "@@ -713,6 +713,7 @@ public Void next(final FileSystem fs, final Path p)\n   protected RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path p,\n       final PathFilter filter)\n   throws IOException {\n+    final Path absF = fixRelativePart(p);\n     return new RemoteIterator<LocatedFileStatus>() {\n       private DirectoryListing thisListing;\n       private int i;\n@@ -722,7 +723,7 @@ public Void next(final FileSystem fs, final Path p)\n       { // initializer\n         // Fully resolve symlinks in path first to avoid additional resolution\n         // round-trips as we fetch more batches of listings\n-        src = getPathName(resolvePath(p));\n+        src = getPathName(resolvePath(absF));\n         // fetch the first batch of entries in the directory\n         thisListing = dfs.listPaths(src, HdfsFileStatus.EMPTY_NAME, true);\n         statistics.incrementReadOps(1);\n@@ -736,7 +737,7 @@ public boolean hasNext() throws IOException {\n         while (curStat == null && hasNextNoFilter()) {\n           LocatedFileStatus next = \n               ((HdfsLocatedFileStatus)thisListing.getPartialListing()[i++])\n-              .makeQualifiedLocated(getUri(), p);\n+              .makeQualifiedLocated(getUri(), absF);\n           if (filter.accept(next.getPath())) {\n             curStat = next;\n           }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f3ce726504bb5c57a5bd115536cbbbd63780dff/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
                "sha": "6fb572aa472d76615b79c9b2feb29ad4f1fa19d6",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop-common/blob/6f3ce726504bb5c57a5bd115536cbbbd63780dff/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=6f3ce726504bb5c57a5bd115536cbbbd63780dff",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -31,8 +31,10 @@\n import java.io.IOException;\n import java.net.URI;\n import java.security.PrivilegedExceptionAction;\n+import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.EnumSet;\n+import java.util.List;\n import java.util.Random;\n \n import org.apache.commons.lang.ArrayUtils;\n@@ -47,9 +49,11 @@\n import org.apache.hadoop.fs.FileChecksum;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n import org.apache.hadoop.fs.MD5MD5CRC32FileChecksum;\n import org.apache.hadoop.fs.Options.ChecksumOpt;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n import org.apache.hadoop.fs.VolumeId;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.hdfs.web.WebHdfsFileSystem;\n@@ -226,7 +230,7 @@ public void testDFSClient() throws Exception {\n       final long millis = Time.now();\n \n       {\n-        DistributedFileSystem dfs = (DistributedFileSystem)cluster.getFileSystem();\n+        final DistributedFileSystem dfs = cluster.getFileSystem();\n         dfs.dfs.getLeaseRenewer().setGraceSleepPeriod(grace);\n         assertFalse(dfs.dfs.getLeaseRenewer().isRunning());\n   \n@@ -326,7 +330,7 @@ public void testDFSClient() throws Exception {\n       }\n \n       {\n-        DistributedFileSystem dfs = (DistributedFileSystem)cluster.getFileSystem();\n+        final DistributedFileSystem dfs = cluster.getFileSystem();\n         assertFalse(dfs.dfs.getLeaseRenewer().isRunning());\n \n         //open and check the file\n@@ -835,4 +839,25 @@ public void testFileCloseStatus() throws IOException {\n     }\n   }\n   \n+  @Test(timeout=60000)\n+  public void testListFiles() throws IOException {\n+    Configuration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();\n+    \n+    try {\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+  \n+      final Path relative = new Path(\"relative\");\n+      fs.create(new Path(relative, \"foo\")).close();\n+  \n+      final List<LocatedFileStatus> retVal = new ArrayList<LocatedFileStatus>();\n+      final RemoteIterator<LocatedFileStatus> iter = fs.listFiles(relative, true);\n+      while (iter.hasNext()) {\n+        retVal.add(iter.next());\n+      }\n+      System.out.println(\"retVal = \" + retVal);\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/6f3ce726504bb5c57a5bd115536cbbbd63780dff/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "a57ad74561201e0b79f669e7295b573e23234d89",
                "status": "modified"
            }
        ],
        "message": "HDFS-5228. The RemoteIterator returned by DistributedFileSystem.listFiles may throw NullPointerException.  Contributed by szetszwo and cnauroth\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525828 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/adf166a50b9cd8910a9df55f2270e14b13523a6d",
        "patched_files": [
            "DistributedFileSystem.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop-common_740bc98": {
        "bug_id": "hadoop-common_740bc98",
        "commit": "https://github.com/apache/hadoop-common/commit/740bc9868518a3e9d98f1b4c5da37772c481b71f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=740bc9868518a3e9d98f1b4c5da37772c481b71f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -222,6 +222,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1957. Consider the max capacity of the queue when computing the ideal\n     capacity for preemption. (Carlo Curino via cdouglas)\n \n+    YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and\n+    attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/CHANGES.txt",
                "sha": "d82cd482a1fb2a3384b3b6a8bfc544f343ee50ae",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java?ref=740bc9868518a3e9d98f1b4c5da37772c481b71f",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "patch": "@@ -360,7 +360,8 @@ private FiCaSchedulerNode getNode(NodeId nodeId) {\n     return nodes.get(nodeId);\n   }\n \n-  private synchronized void addApplication(ApplicationId applicationId,\n+  @VisibleForTesting\n+  public synchronized void addApplication(ApplicationId applicationId,\n       String queue, String user) {\n     SchedulerApplication application =\n         new SchedulerApplication(DEFAULT_QUEUE, user);\n@@ -372,7 +373,8 @@ private synchronized void addApplication(ApplicationId applicationId,\n         .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n   }\n \n-  private synchronized void\n+  @VisibleForTesting\n+  public synchronized void\n       addApplicationAttempt(ApplicationAttemptId appAttemptId,\n           boolean transferStateFromPreviousAttempt) {\n     SchedulerApplication application =\n@@ -458,6 +460,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n         .entrySet()) {\n       FiCaSchedulerApp application =\n           (FiCaSchedulerApp) e.getValue().getCurrentAppAttempt();\n+      if (application == null) {\n+        continue;\n+      }\n       LOG.debug(\"pre-assignContainers\");\n       application.showRequests();\n       synchronized (application) {\n@@ -497,6 +502,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n     for (SchedulerApplication application : applications.values()) {\n       FiCaSchedulerApp attempt =\n           (FiCaSchedulerApp) application.getCurrentAppAttempt();\n+      if (attempt == null) {\n+        continue;\n+      }\n       attempt.setHeadroom(Resources.subtract(clusterResource, usedResource));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "sha": "21fcdecf4f9ef9fa2ee555996ba746252012e656",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop-common/blob/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java?ref=740bc9868518a3e9d98f1b4c5da37772c481b71f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n+import org.apache.hadoop.yarn.util.resource.Resources;\n import org.apache.log4j.Level;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n@@ -66,7 +67,7 @@\n   \n   private final int GB = 1024;\n   private static YarnConfiguration conf;\n-  \n+\n   @BeforeClass\n   public static void setup() {\n     conf = new YarnConfiguration();\n@@ -213,6 +214,32 @@ public void test() throws Exception {\n     rm.stop();\n   }\n \n+  @Test\n+  public void testNodeUpdateBeforeAppAttemptInit() throws Exception {\n+    FifoScheduler scheduler = new FifoScheduler();\n+    MockRM rm = new MockRM(conf);\n+    scheduler.reinitialize(conf, rm.getRMContext());\n+\n+    RMNode node = MockNodes.newNodeInfo(1,\n+            Resources.createResource(1024, 4), 1, \"127.0.0.1\");\n+    scheduler.handle(new NodeAddedSchedulerEvent(node));\n+\n+    ApplicationId appId = ApplicationId.newInstance(0, 1);\n+    scheduler.addApplication(appId, \"queue1\", \"user1\");\n+\n+    NodeUpdateSchedulerEvent updateEvent = new NodeUpdateSchedulerEvent(node);\n+    try {\n+      scheduler.handle(updateEvent);\n+    } catch (NullPointerException e) {\n+        Assert.fail();\n+    }\n+\n+    ApplicationAttemptId attId = ApplicationAttemptId.newInstance(appId, 1);\n+    scheduler.addApplicationAttempt(attId, false);\n+\n+    rm.stop();\n+  }\n+\n   private void testMinimumAllocation(YarnConfiguration conf, int testAlloc)\n       throws Exception {\n     MockRM rm = new MockRM(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/740bc9868518a3e9d98f1b4c5da37772c481b71f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "sha": "fcd5041e4257d4cc1ce64ae90278aba09576f14d",
                "status": "modified"
            }
        ],
        "message": "YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594476 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ebaafb216fa02a4f3640357ce1c7cfddac5fcbae",
        "patched_files": [
            "FifoScheduler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFifoScheduler.java"
        ]
    },
    "hadoop-common_74abf55": {
        "bug_id": "hadoop-common_74abf55",
        "commit": "https://github.com/apache/hadoop-common/commit/74abf55e3279c110d5a8d3e0a860d471720b8de2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/74abf55e3279c110d5a8d3e0a860d471720b8de2/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=74abf55e3279c110d5a8d3e0a860d471720b8de2",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -299,6 +299,9 @@ Release 0.23.1 - Unreleased\n     MAPREDUCE-3398. Fixed log aggregation to work correctly in secure mode.\n     (Siddharth Seth via vinodkv)\n \n+    MAPREDUCE-3530. Fixed an NPE occuring during scheduling in the\n+    ResourceManager. (Arun C Murthy via vinodkv)\n+\n Release 0.23.0 - 2011-11-01 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/74abf55e3279c110d5a8d3e0a860d471720b8de2/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "6748d60a7f8edd48b2701df15ac4b4f5dbc0638f",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/74abf55e3279c110d5a8d3e0a860d471720b8de2/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=74abf55e3279c110d5a8d3e0a860d471720b8de2",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -262,6 +262,16 @@ public RMNodeState getState() {\n \n   }\n \n+  @Private\n+  public List<ContainerId> getContainersToCleanUp() {\n+    this.readLock.lock();\n+    try {\n+      return new ArrayList<ContainerId>(containersToClean);\n+    } finally {\n+      this.readLock.unlock();\n+    }\n+  }\n+  \n   @Override\n   public List<ContainerId> pullContainersToCleanUp() {\n \n@@ -342,7 +352,6 @@ public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n     @Override\n     public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n-\n       rmNode.containersToClean.add(((\n           RMNodeCleanContainerEvent) event).getContainerId());\n     }\n@@ -396,8 +405,17 @@ public RMNodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n       List<ContainerStatus> completedContainers = \n           new ArrayList<ContainerStatus>();\n       for (ContainerStatus remoteContainer : statusEvent.getContainers()) {\n-        // Process running containers\n         ContainerId containerId = remoteContainer.getContainerId();\n+        \n+        // Don't bother with containers already scheduled for cleanup,\n+        // the scheduler doens't need to know any more about this container\n+        if (rmNode.containersToClean.contains(containerId)) {\n+          LOG.info(\"Container \" + containerId + \" already scheduled for \" +\n+          \t\t\"cleanup, no further processing\");\n+          continue;\n+        }\n+        \n+        // Process running containers\n         if (remoteContainer.getState() == ContainerState.RUNNING) {\n           if (!rmNode.justLaunchedContainers.containsKey(containerId)) {\n             // Just launched container. RM knows about it the first time.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/74abf55e3279c110d5a8d3e0a860d471720b8de2/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "2cadd89071243d8e5afd12beb4683f8c4f84bc94",
                "status": "modified"
            },
            {
                "additions": 148,
                "blob_url": "https://github.com/apache/hadoop-common/blob/74abf55e3279c110d5a8d3e0a860d471720b8de2/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "changes": 148,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java?ref=74abf55e3279c110d5a8d3e0a860d471720b8de2",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "patch": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.yarn.server.resourcemanager;\n+\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerStatus;\n+import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n+import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.MemStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.InlineDispatcher;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeCleanContainerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeAddedSchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType;\n+import org.apache.hadoop.yarn.util.BuilderUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+public class TestRMNodeTransitions {\n+\n+  RMNodeImpl node;\n+  \n+  private RMContext rmContext;\n+  private YarnScheduler scheduler;\n+\n+  private SchedulerEventType eventType;\n+  private List<ContainerStatus> completedContainers;\n+  \n+  private final class TestSchedulerEventDispatcher implements\n+  EventHandler<SchedulerEvent> {\n+    @Override\n+    public void handle(SchedulerEvent event) {\n+      scheduler.handle(event);\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    InlineDispatcher rmDispatcher = new InlineDispatcher();\n+    \n+    rmContext = \n+        new RMContextImpl(new MemStore(), rmDispatcher, null, null, null);\n+    scheduler = mock(YarnScheduler.class);\n+    doAnswer(\n+        new Answer<Void>() {\n+\n+          @Override\n+          public Void answer(InvocationOnMock invocation) throws Throwable {\n+            final SchedulerEvent event = (SchedulerEvent)(invocation.getArguments()[0]);\n+            eventType = event.getType();\n+            if (eventType == SchedulerEventType.NODE_UPDATE) {\n+              completedContainers = \n+                  ((NodeUpdateSchedulerEvent)event).getCompletedContainers();\n+            } else {\n+              completedContainers = null;\n+            }\n+            return null;\n+          }\n+        }\n+        ).when(scheduler).handle(any(SchedulerEvent.class));\n+    \n+    rmDispatcher.register(SchedulerEventType.class, \n+        new TestSchedulerEventDispatcher());\n+    \n+    \n+    node = new RMNodeImpl(null, rmContext, null, 0, 0, null, null);\n+\n+  }\n+  \n+  @After\n+  public void tearDown() throws Exception {\n+  }\n+  \n+  private RMNodeStatusEvent getMockRMNodeStatusEvent() {\n+    HeartbeatResponse response = mock(HeartbeatResponse.class);\n+\n+    NodeHealthStatus healthStatus = mock(NodeHealthStatus.class);\n+    Boolean yes = new Boolean(true);\n+    doReturn(yes).when(healthStatus).getIsNodeHealthy();\n+    \n+    RMNodeStatusEvent event = mock(RMNodeStatusEvent.class);\n+    doReturn(healthStatus).when(event).getNodeHealthStatus();\n+    doReturn(response).when(event).getLatestResponse();\n+    doReturn(RMNodeEventType.STATUS_UPDATE).when(event).getType();\n+    return event;\n+  }\n+  \n+  @Test\n+  public void testExpiredContainer() {\n+    // Start the node\n+    node.handle(new RMNodeEvent(null, RMNodeEventType.STARTED));\n+    verify(scheduler).handle(any(NodeAddedSchedulerEvent.class));\n+    \n+    // Expire a container\n+\t\tContainerId completedContainerId = BuilderUtils.newContainerId(\n+\t\t\t\tBuilderUtils.newApplicationAttemptId(\n+\t\t\t\t\t\tBuilderUtils.newApplicationId(0, 0), 0), 0);\n+    node.handle(new RMNodeCleanContainerEvent(null, completedContainerId));\n+    Assert.assertEquals(1, node.getContainersToCleanUp().size());\n+    \n+    // Now verify that scheduler isn't notified of an expired container\n+    // by checking number of 'completedContainers' it got in the previous event\n+    RMNodeStatusEvent statusEvent = getMockRMNodeStatusEvent();\n+    ContainerStatus containerStatus = mock(ContainerStatus.class);\n+    doReturn(completedContainerId).when(containerStatus).getContainerId();\n+    doReturn(Collections.singletonList(containerStatus)).\n+        when(statusEvent).getContainers();\n+    node.handle(statusEvent);\n+    Assert.assertEquals(0, completedContainers.size());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/74abf55e3279c110d5a8d3e0a860d471720b8de2/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "sha": "6a717a4daf9c134bd9ff3fde38987e95c5c295d5",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-3530. Fixed an NPE occuring during scheduling in the ResourceManager. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214476 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b5cefed18b20f9e4978a740176eb72c919e7eb30",
        "patched_files": [
            "RMNodeImpl.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMNodeTransitions.java"
        ]
    },
    "hadoop-common_74d19b5": {
        "bug_id": "hadoop-common_74d19b5",
        "commit": "https://github.com/apache/hadoop-common/commit/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java?ref=74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "patch": "@@ -98,6 +98,7 @@\n   private int imagePort;\n   private String infoBindAddress;\n \n+  private FSNamesystem namesystem;\n   private Collection<URI> checkpointDirs;\n   private Collection<URI> checkpointEditsDirs;\n   private long checkpointPeriod;    // in seconds\n@@ -481,8 +482,9 @@ private void startCheckpoint() throws IOException {\n    */\n   private void doMerge(CheckpointSignature sig, boolean loadImage)\n   throws IOException {\n-    FSNamesystem namesystem = \n-            new FSNamesystem(checkpointImage, conf);\n+    if (loadImage) {\n+      namesystem = new FSNamesystem(checkpointImage, conf);\n+    }\n     assert namesystem.dir.fsImage == checkpointImage;\n     checkpointImage.doMerge(sig, loadImage);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java",
                "sha": "08df62a988f388de0d26b89bc0da42f8e25f6398",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java?ref=74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "patch": "@@ -684,6 +684,7 @@ public void testCheckpoint() throws IOException {\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();\n     cluster.waitActive();\n     fileSys = cluster.getFileSystem();\n+    Path tmpDir = new Path(\"/tmp_tmp\");\n     try {\n       // check that file1 still exists\n       checkFile(fileSys, file1, replication);\n@@ -698,6 +699,11 @@ public void testCheckpoint() throws IOException {\n       //\n       SecondaryNameNode secondary = startSecondaryNameNode(conf);\n       secondary.doCheckpoint();\n+      \n+      fileSys.delete(tmpDir, true);\n+      fileSys.mkdirs(tmpDir);\n+      secondary.doCheckpoint();\n+      \n       secondary.shutdown();\n     } finally {\n       fileSys.close();\n@@ -713,6 +719,7 @@ public void testCheckpoint() throws IOException {\n     fileSys = cluster.getFileSystem();\n \n     assertTrue(!fileSys.exists(file1));\n+    assertTrue(fileSys.exists(tmpDir));\n \n     try {\n       // verify that file2 exists",
                "raw_url": "https://github.com/apache/hadoop-common/raw/74d19b5ff2f3ee67174fb2bd1e5c61cf2f29ac6f/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java",
                "sha": "2187a9ef59fe05e92eb18865e45a7fde1785451a",
                "status": "modified"
            }
        ],
        "message": "Fix NullPointerException in Secondary NameNode. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1102465 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/01ffd46e3fec9037c17e887f932fb1765c1ab219",
        "patched_files": [
            "SecondaryNameNode.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCheckpoint.java"
        ]
    },
    "hadoop-common_770463b": {
        "bug_id": "hadoop-common_770463b",
        "commit": "https://github.com/apache/hadoop-common/commit/770463b3df0bacea4f598f7c73a3d976a7a3b937",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -45,3 +45,5 @@ IMPROVEMENTS:\n \n     HDFS-5390. Send one incremental block report per storage directory.\n     (Arpit Agarwal)\n+\n+    HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "cd139d4845e203e0125d72371d510352118fa6bc",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -180,10 +181,11 @@ public String toString() {\n     }\n   }\n   \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+                       String storageUuid, StorageType storageType) {\n     checkBlock(block);\n     for (BPServiceActor actor : bpServices) {\n-      actor.reportBadBlocks(block);\n+      actor.reportBadBlocks(block, storageUuid, storageType);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "5d584616df335258b426878141001405130adb92",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hdfs.DFSUtil;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -237,12 +238,18 @@ void scheduleBlockReport(long delay) {\n     resetBlockReportTime = true; // reset future BRs for randomness\n   }\n \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+      String storageUuid, StorageType storageType) {\n     if (bpRegistration == null) {\n       return;\n     }\n     DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n-    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n+    String[] uuids = { storageUuid };\n+    StorageType[] types = { storageType };\n+    // TODO: Corrupt flag is set to false for compatibility. We can probably\n+    // set it to true here.\n+    LocatedBlock[] blocks = {\n+        new LocatedBlock(block, dnArr, uuids, types, -1, false) };\n     \n     try {\n       bpNamenode.reportBadBlocks(blocks);  ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "sha": "172fb0fc30ec6cd3d5fdbe7dc2459e8c7d25cf48",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -559,7 +559,9 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n     BPOfferService bpos = getBPOSForBlock(block);\n-    bpos.reportBadBlocks(block);\n+    FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    bpos.reportBadBlocks(\n+        block, volume.getStorageID(), volume.getStorageType());\n   }\n \n   /**\n@@ -1265,8 +1267,10 @@ private void transferBlock(ExtendedBlock block, DatanodeInfo xferTargets[])\n     // Check if NN recorded length matches on-disk length \n     long onDiskLength = data.getLength(block);\n     if (block.getNumBytes() > onDiskLength) {\n+      FsVolumeSpi volume = getFSDataset().getVolume(block);\n       // Shorter on-disk len indicates corruption so report NN the corrupt block\n-      bpos.reportBadBlocks(block);\n+      bpos.reportBadBlocks(\n+          block, volume.getStorageID(), volume.getStorageType());\n       LOG.warn(\"Can't replicate block \" + block\n           + \" because on-disk length \" + onDiskLength \n           + \" is shorter than NameNode recorded length \" + block.getNumBytes());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "318d2f3705a05cad964d1b9b68ed52b7f4f3237a",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -198,7 +198,9 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n   //                 two maps. This might require some refactoring\n   //                 rewrite of FsDatasetImpl.\n   final ReplicaMap volumeMap;\n-  final Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap;\n+\n+  // Map from StorageID to ReplicaMap.\n+  final Map<String, ReplicaMap> perVolumeReplicaMap;\n \n \n   // Used for synchronizing access to usage stats\n@@ -249,7 +251,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n       LOG.info(\"Added volume - \" + dir + \", StorageType: \" + storageType);\n     }\n     volumeMap = new ReplicaMap(this);\n-    perVolumeReplicaMap = new HashMap<FsVolumeImpl, ReplicaMap>();\n+    perVolumeReplicaMap = new HashMap<String, ReplicaMap>();\n \n     @SuppressWarnings(\"unchecked\")\n     final VolumeChoosingPolicy<FsVolumeImpl> blockChooserImpl =\n@@ -628,7 +630,7 @@ private synchronized ReplicaBeingWritten append(String bpid,\n     \n     // Replace finalized replica by a RBW replica in replicas map\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(bpid, newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -759,7 +761,7 @@ public synchronized ReplicaInPipeline createRbw(ExtendedBlock b)\n     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     return newReplicaInfo;\n   }\n   \n@@ -878,7 +880,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(\n     rbw.setBytesAcked(visible);\n     // overwrite the RBW in the volume map\n     volumeMap.add(b.getBlockPoolId(), rbw);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), rbw);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), rbw);\n     return rbw;\n   }\n \n@@ -898,7 +900,7 @@ public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n     ReplicaInPipeline newReplicaInfo = new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -967,7 +969,8 @@ private synchronized FinalizedReplica finalizeReplica(String bpid,\n       newReplicaInfo = new FinalizedReplica(replicaInfo, v, dest.getParentFile());\n     }\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(newReplicaInfo.getVolume()).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(newReplicaInfo.getVolume().getStorageID())\n+        .add(bpid, newReplicaInfo);\n     return newReplicaInfo;\n   }\n \n@@ -981,7 +984,7 @@ public synchronized void unfinalizeBlock(ExtendedBlock b) throws IOException {\n     if (replicaInfo != null && replicaInfo.getState() == ReplicaState.TEMPORARY) {\n       // remove from volumeMap\n       volumeMap.remove(b.getBlockPoolId(), b.getLocalBlock());\n-      perVolumeReplicaMap.get((FsVolumeImpl) replicaInfo.getVolume())\n+      perVolumeReplicaMap.get(replicaInfo.getVolume().getStorageID())\n           .remove(b.getBlockPoolId(), b.getLocalBlock());\n       \n       // delete the on-disk temp file\n@@ -1064,7 +1067,7 @@ public BlockListAsLongs getBlockReport(String bpid) {\n         new HashMap<String, BlockListAsLongs>();\n \n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList = getBlockReportWithReplicaMap(bpid, rMap);\n       blockReportMap.put(v.getStorageID(), blockList);\n     }\n@@ -1212,7 +1215,7 @@ public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n@@ -1274,7 +1277,8 @@ public void checkDataDir() throws DiskErrorException {\n               LOG.warn(\"Removing replica \" + bpid + \":\" + b.getBlockId()\n                   + \" on failed volume \" + fv.getCurrentDir().getAbsolutePath());\n               ib.remove();\n-              perVolumeReplicaMap.get(fv).remove(bpid, b.getBlockId());\n+              perVolumeReplicaMap.get(fv.getStorageID())\n+                  .remove(bpid, b.getBlockId());\n               removedBlocks++;\n             }\n           }\n@@ -1391,8 +1395,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n           // Block is in memory and not on the disk\n           // Remove the block from volumeMap\n           volumeMap.remove(bpid, blockId);\n-          perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume())\n-              .remove(bpid, blockId);\n+          perVolumeReplicaMap.get(vol.getStorageID()).remove(bpid, blockId);\n           final DataBlockScanner blockScanner = datanode.getBlockScanner();\n           if (blockScanner != null) {\n             blockScanner.deleteBlock(bpid, new Block(blockId));\n@@ -1416,8 +1419,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n         ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n             diskFile.length(), diskGS, vol, diskFile.getParentFile());\n         volumeMap.add(bpid, diskBlockInfo);\n-        perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume()).\n-            remove(bpid, diskBlockInfo);\n+        perVolumeReplicaMap.get(vol.getStorageID())\n+            .remove(bpid, diskBlockInfo);\n         final DataBlockScanner blockScanner = datanode.getBlockScanner();\n         if (blockScanner != null) {\n           blockScanner.addBlock(new ExtendedBlock(bpid, diskBlockInfo));\n@@ -1695,7 +1698,7 @@ public synchronized void addBlockPool(String bpid, Configuration conf)\n \n     // TODO: Avoid the double scan.\n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       rMap.initBlockPool(bpid);\n       volumes.getVolumeMap(bpid, v, rMap);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "9077c40a8367a9f4bdddc10e439a1e95920ef481",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java?ref=770463b3df0bacea4f598f7c73a3d976a7a3b937",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "patch": "@@ -90,13 +90,13 @@ long getRemaining() throws IOException {\n     return remaining;\n   }\n     \n-  void initializeReplicaMaps(Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap,\n+  void initializeReplicaMaps(Map<String, ReplicaMap> perVolumeReplicaMap,\n                              ReplicaMap globalReplicaMap,\n                              Object mutex) throws IOException {\n     for (FsVolumeImpl v : volumes) {\n       ReplicaMap rMap = new ReplicaMap(mutex);\n       v.getVolumeMap(rMap);\n-      perVolumeReplicaMap.put(v, rMap);\n+      perVolumeReplicaMap.put(v.getStorageID(), rMap);\n       globalReplicaMap.addAll(rMap);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/770463b3df0bacea4f598f7c73a3d976a7a3b937/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "sha": "671996718be2eea6db9b076cf8a5aed4de36705e",
                "status": "modified"
            }
        ],
        "message": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/5d8a7875a3329c95d738f4b97001182d8235d79a",
        "patched_files": [
            "BPOfferService.java",
            "CHANGES_HDFS-2832.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBPOfferService.java"
        ]
    },
    "hadoop-common_77cf4ed": {
        "bug_id": "hadoop-common_77cf4ed",
        "commit": "https://github.com/apache/hadoop-common/commit/77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -131,3 +131,5 @@ HDFS-2804. Should not mark blocks under-replicated when exiting safemode (todd)\n HDFS-2807. Service level authorizartion for HAServiceProtocol. (jitendra)\n \n HDFS-2809. Add test to verify that delegation tokens are honored after failover. (jitendra and atm)\n+\n+HDFS-2838. NPE in FSNamesystem when in safe mode. (Gregory Chanan via eli)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "c8a760336a9c9c95f075c4e2d894747b31d6207e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3623,11 +3623,10 @@ private void doConsistencyCheck() {\n       assert assertsOn = true; // set to true if asserts are on\n       if (!assertsOn) return;\n       \n-      \n-      int activeBlocks = blockManager.getActiveBlockCount();\n       if (blockTotal == -1 && blockSafe == -1) {\n         return; // manual safe mode\n       }\n+      int activeBlocks = blockManager.getActiveBlockCount();\n       if ((blockTotal != activeBlocks) &&\n           !(blockSafe >= 0 && blockSafe <= blockTotal)) {\n         throw new AssertionError(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "b3b3dbdaf31bcb28f1c91d0cbdd4949a93c8f0fa",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java?ref=77cf4edee832532c1ab043a45b8ee94c7ba20bb1",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import junit.framework.Assert;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.protocol.FSConstants;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -37,6 +38,7 @@\n   private static final String CLUSTER_1 = \"cluster1\";\n   private static final String CLUSTER_2 = \"cluster2\";\n   private static final String CLUSTER_3 = \"cluster3\";\n+  private static final String CLUSTER_4 = \"cluster4\";\n   protected String testDataPath;\n   protected File testDataDir;\n   @Before\n@@ -104,5 +106,21 @@ public void testDualClusters() throws Throwable {\n     }\n   }\n \n-\n+  @Test(timeout=100000)\n+  public void testIsClusterUpAfterShutdown() throws Throwable {\n+    Configuration conf = new HdfsConfiguration();\n+    File testDataCluster4 = new File(testDataPath, CLUSTER_4);\n+    String c4Path = testDataCluster4.getAbsolutePath();\n+    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, c4Path);\n+    MiniDFSCluster cluster4 = new MiniDFSCluster.Builder(conf).build();\n+    try {\n+      DistributedFileSystem dfs = (DistributedFileSystem) cluster4.getFileSystem();\n+      dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_ENTER);\n+      cluster4.shutdown();\n+    } finally {\n+      while(cluster4.isClusterUp()){\n+        Thread.sleep(1000);\n+      }  \n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77cf4edee832532c1ab043a45b8ee94c7ba20bb1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "sha": "0eec0d187746184179ed9f814d59dbd059c390e0",
                "status": "modified"
            }
        ],
        "message": "HDFS-2838. NPE in FSNamesystem when in safe mode. Contributed by Gregory Chanan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1236450 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/83d2cbf7007e415f3c3a76153242858e89b33ed2",
        "patched_files": [
            "MiniDFSCluster.java",
            "CHANGES.HDFS-1623.txt",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestMiniDFSCluster.java"
        ]
    },
    "hadoop-common_77fb965": {
        "bug_id": "hadoop-common_77fb965",
        "commit": "https://github.com/apache/hadoop-common/commit/77fb965df780c375f6279f11dc9a902c1ff1380e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -144,6 +144,9 @@ Release 0.23.1 - Unreleased\n     MAPREDUCE-3369. Migrate MR1 tests to run on MR2 using the new interfaces\n     introduced in MAPREDUCE-3169. (Ahmed Radwan via tomwhite)\n \n+    MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. \n+    (Jonathan Eagles via mahadev)\n+\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "f53ffec718a02c41aca8181ac32600b7c013dd30",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -432,7 +432,6 @@ public String getFailureInfo() throws IOException {\n \n   }\n \n-  Cluster cluster;\n   /**\n    * Ugi of the client. We store this ugi when the client is created and \n    * then make sure that the same ugi is used to run the various protocols.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "fa3d799fe70d60bb3247edc940ff2f9d5f1d953e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "patch": "@@ -55,7 +55,7 @@\n @InterfaceStability.Stable\n public class CLI extends Configured implements Tool {\n   private static final Log LOG = LogFactory.getLog(CLI.class);\n-  private Cluster cluster;\n+  protected Cluster cluster;\n \n   public CLI() {\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "sha": "f7ac9c40a6a2074a6602bc8d12548d8c403682c0",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop-common/blob/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=77fb965df780c375f6279f11dc9a902c1ff1380e",
                "deletions": 8,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "patch": "@@ -19,21 +19,41 @@\n package org.apache.hadoop.mapred;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import java.io.IOException;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobPriority;\n+import org.apache.hadoop.mapreduce.JobStatus;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.TaskReport;\n import org.junit.Test;\n \n public class JobClientUnitTest {\n   \n+  public class TestJobClient extends JobClient {\n+\n+    TestJobClient(JobConf jobConf) throws IOException {\n+      super(jobConf);\n+    }\n+\n+    void setCluster(Cluster cluster) {\n+      this.cluster = cluster;\n+    }\n+  }\n+\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testMapTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -47,9 +67,9 @@ public void testMapTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testReduceTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -63,9 +83,9 @@ public void testReduceTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testSetupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -79,9 +99,9 @@ public void testSetupTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCleanupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -91,4 +111,49 @@ public void testCleanupTaskReportsWithNullJob() throws Exception {\n     \n     verify(mockCluster).getJob(id);\n   }\n+\n+  @Test\n+  public void testShowJob() throws Exception {\n+    TestJobClient client = new TestJobClient(new JobConf());\n+    JobID jobID = new JobID(\"test\", 0);\n+\n+    JobStatus mockJobStatus = mock(JobStatus.class);\n+    when(mockJobStatus.getJobID()).thenReturn(jobID);\n+    when(mockJobStatus.getState()).thenReturn(JobStatus.State.RUNNING);\n+    when(mockJobStatus.getStartTime()).thenReturn(0L);\n+    when(mockJobStatus.getUsername()).thenReturn(\"mockuser\");\n+    when(mockJobStatus.getQueue()).thenReturn(\"mockqueue\");\n+    when(mockJobStatus.getPriority()).thenReturn(JobPriority.NORMAL);\n+    when(mockJobStatus.getNumUsedSlots()).thenReturn(1);\n+    when(mockJobStatus.getNumReservedSlots()).thenReturn(1);\n+    when(mockJobStatus.getUsedMem()).thenReturn(1024);\n+    when(mockJobStatus.getReservedMem()).thenReturn(512);\n+    when(mockJobStatus.getNeededMem()).thenReturn(2048);\n+    when(mockJobStatus.getSchedulingInfo()).thenReturn(\"NA\");\n+\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getTaskReports(isA(TaskType.class))).thenReturn(new TaskReport[0]);\n+\n+    Cluster mockCluster = mock(Cluster.class);\n+    when(mockCluster.getJob(jobID)).thenReturn(mockJob);\n+\n+    client.setCluster(mockCluster);\n+    \n+    \n+    client.displayJobList(new JobStatus[] {mockJobStatus});\n+    verify(mockJobStatus, atLeastOnce()).getJobID();\n+    verify(mockJob, atLeastOnce()).getTaskReports(isA(TaskType.class));\n+    verify(mockCluster, atLeastOnce()).getJob(jobID);\n+    verify(mockJobStatus).getState();\n+    verify(mockJobStatus).getStartTime();\n+    verify(mockJobStatus).getUsername();\n+    verify(mockJobStatus).getQueue();\n+    verify(mockJobStatus).getPriority();\n+    verify(mockJobStatus).getNumUsedSlots();\n+    verify(mockJobStatus).getNumReservedSlots();\n+    verify(mockJobStatus).getUsedMem();\n+    verify(mockJobStatus).getReservedMem();\n+    verify(mockJobStatus).getNeededMem();\n+    verify(mockJobStatus).getSchedulingInfo();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/77fb965df780c375f6279f11dc9a902c1ff1380e/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "sha": "3f54e09a33d707a0e388239e7e2d8d9a0d274c07",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. (Jonathan Eagles via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213464 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ad3b9f14154660b24fa7f04d8f045c5e8fa4cc84",
        "patched_files": [
            "JobClient.java",
            "CLI.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCLI.java",
            "TestJobClient.java",
            "JobClientUnitTest.java"
        ]
    },
    "hadoop-common_796909b": {
        "bug_id": "hadoop-common_796909b",
        "commit": "https://github.com/apache/hadoop-common/commit/796909bd8462a11ee78c75625d7056fafefdb290",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=796909bd8462a11ee78c75625d7056fafefdb290",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -203,6 +203,10 @@ Release 0.23.2 - UNRELEASED\n     MAPREDUCE-3816. capacity scheduler web ui bar graphs for used capacity wrong\n     (tgraves via bobby)\n \n+    MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a \n+    task attempt without an assigned container. (Robert Joseph Evans via\n+    sseth)\n+\n Release 0.23.1 - 2012-02-17\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d293b52b3f7f2466e11cf730a8d4aa9ef58ad197",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java?ref=796909bd8462a11ee78c75625d7056fafefdb290",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "patch": "@@ -142,7 +142,7 @@ private static ApplicationId toApplicationId(\n   }\n \n   public static String toString(ContainerId cId) {\n-    return cId.toString();\n+    return cId == null ? null : cId.toString();\n   }\n \n   public static NodeId toNodeId(String nodeIdStr) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "sha": "21fe2d9874cf0223ea91b4fb1af96c13f306464c",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java?ref=796909bd8462a11ee78c75625d7056fafefdb290",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.net.URISyntaxException;\n \n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.URL;\n import org.junit.Test;\n \n@@ -35,4 +36,17 @@ public void testConvertUrlWithNoPort() throws URISyntaxException {\n     assertEquals(expectedPath, actualPath);\n   }\n \n+  @Test\n+  public void testContainerId() throws URISyntaxException {\n+    ContainerId id = BuilderUtils.newContainerId(0, 0, 0, 0);\n+    String cid = ConverterUtils.toString(id);\n+    assertEquals(\"container_0_0000_00_000000\", cid);\n+    ContainerId gen = ConverterUtils.toContainerId(cid);\n+    assertEquals(gen, id);\n+  }\n+\n+  @Test\n+  public void testContainerIdNull() throws URISyntaxException {\n+    assertNull(ConverterUtils.toString((ContainerId)null));\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/796909bd8462a11ee78c75625d7056fafefdb290/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "sha": "7924124bf40e7a3d6332a3ee5485a6a595bacfcd",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a task attempt without an assigned container. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1294901 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/9ea38c42160269cb24d5823268f60f5f817ad9a7",
        "patched_files": [
            "ConverterUtils.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestConverterUtils.java"
        ]
    },
    "hadoop-common_7a2d8a8": {
        "bug_id": "hadoop-common_7a2d8a8",
        "commit": "https://github.com/apache/hadoop-common/commit/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -385,6 +385,9 @@ Release 2.1.2 - UNRELEASED\n     HDFS-5255. Distcp job fails with hsftp when https is enabled in insecure\n     cluster. (Arpit Agarwal)\n \n+    HDFS-5279. Guard against NullPointerException in NameNode JSP pages before\n+    initialization of FSNamesystem. (cnauroth)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "29b9be665d62eeb6bcc3f105e961c776291ef5cc",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java?ref=7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java",
                "patch": "@@ -30,6 +30,7 @@\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.Date;\n import java.util.Iterator;\n import java.util.List;\n@@ -210,6 +211,9 @@ static String getCorruptFilesWarning(FSNamesystem fsn) {\n \n   static void generateSnapshotReport(JspWriter out, FSNamesystem fsn)\n       throws IOException {\n+    if (fsn == null) {\n+      return;\n+    }\n     out.println(\"<div id=\\\"snapshotstats\\\"><div class=\\\"dfstable\\\">\"\n         + \"<table class=\\\"storage\\\" title=\\\"Snapshot Summary\\\">\\n\"\n         + \"<thead><tr><td><b>Snapshottable directories</b></td>\"\n@@ -652,7 +656,8 @@ static void redirectToRandomDataNode(ServletContext context,\n         .getAttribute(JspHelper.CURRENT_CONF);\n     // We can't redirect if there isn't a DN to redirect to.\n     // Lets instead show a proper error message.\n-    if (nn.getNamesystem().getNumLiveDataNodes() < 1) {\n+    FSNamesystem fsn = nn.getNamesystem();\n+    if (fsn == null || fsn.getNumLiveDataNodes() < 1) {\n       throw new IOException(\"Can't browse the DFS since there are no \" +\n           \"live nodes available to redirect to.\");\n     }\n@@ -688,6 +693,20 @@ static void redirectToRandomDataNode(ServletContext context,\n     resp.sendRedirect(redirectLocation);\n   }\n \n+  /**\n+   * Returns a descriptive label for the running NameNode.  If the NameNode has\n+   * initialized to the point of running its RPC server, then this label consists\n+   * of the host and port of the RPC server.  Otherwise, the label is a message\n+   * stating that the NameNode is still initializing.\n+   * \n+   * @param nn NameNode to describe\n+   * @return String NameNode label\n+   */\n+  static String getNameNodeLabel(NameNode nn) {\n+    return nn.getRpcServer() != null ? nn.getNameNodeAddressHostPortString() :\n+      \"initializing\";\n+  }\n+\n   static class NodeListJsp {\n     private int rowNum = 0;\n \n@@ -843,6 +862,9 @@ void generateNodesList(ServletContext context, JspWriter out,\n         HttpServletRequest request) throws IOException {\n       final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n       final FSNamesystem ns = nn.getNamesystem();\n+      if (ns == null) {\n+        return;\n+      }\n       final DatanodeManager dm = ns.getBlockManager().getDatanodeManager();\n \n       final List<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();\n@@ -1022,14 +1044,16 @@ private static String getLocalParentDir(INode inode) {\n     final BlockManager blockManager;\n     \n     XMLBlockInfo(FSNamesystem fsn, Long blockId) {\n-      this.blockManager = fsn.getBlockManager();\n+      this.blockManager = fsn != null ? fsn.getBlockManager() : null;\n \n       if (blockId == null) {\n         this.block = null;\n         this.inode = null;\n       } else {\n         this.block = new Block(blockId);\n-        this.inode = ((INode)blockManager.getBlockCollection(block)).asFile();\n+        this.inode = blockManager != null ?\n+          ((INode)blockManager.getBlockCollection(block)).asFile() :\n+          null;\n       }\n     }\n \n@@ -1103,8 +1127,10 @@ public void toXML(XMLOutputter doc) throws IOException {\n         } \n \n         doc.startTag(\"replicas\");\n-        for(final Iterator<DatanodeDescriptor> it = blockManager.datanodeIterator(block);\n-            it.hasNext(); ) {\n+        for (final Iterator<DatanodeDescriptor> it = blockManager != null ?\n+            blockManager.datanodeIterator(block) :\n+            Collections.<DatanodeDescriptor>emptyList().iterator();\n+            it.hasNext();) {\n           doc.startTag(\"replica\");\n \n           DatanodeDescriptor dd = it.next();\n@@ -1140,7 +1166,7 @@ public void toXML(XMLOutputter doc) throws IOException {\n     \n     XMLCorruptBlockInfo(FSNamesystem fsn, Configuration conf,\n                                int numCorruptBlocks, Long startingBlockId) {\n-      this.blockManager = fsn.getBlockManager();\n+      this.blockManager = fsn != null ? fsn.getBlockManager() : null;\n       this.conf = conf;\n       this.numCorruptBlocks = numCorruptBlocks;\n       this.startingBlockId = startingBlockId;\n@@ -1163,16 +1189,19 @@ public void toXML(XMLOutputter doc) throws IOException {\n       doc.endTag();\n       \n       doc.startTag(\"num_missing_blocks\");\n-      doc.pcdata(\"\"+blockManager.getMissingBlocksCount());\n+      doc.pcdata(\"\" + (blockManager != null ?\n+        blockManager.getMissingBlocksCount() : 0));\n       doc.endTag();\n       \n       doc.startTag(\"num_corrupt_replica_blocks\");\n-      doc.pcdata(\"\"+blockManager.getCorruptReplicaBlocksCount());\n+      doc.pcdata(\"\" + (blockManager != null ?\n+        blockManager.getCorruptReplicaBlocksCount() : 0));\n       doc.endTag();\n      \n       doc.startTag(\"corrupt_replica_block_ids\");\n-      final long[] corruptBlockIds = blockManager.getCorruptReplicaBlockIds(\n-          numCorruptBlocks, startingBlockId);\n+      final long[] corruptBlockIds = blockManager != null ?\n+        blockManager.getCorruptReplicaBlockIds(numCorruptBlocks,\n+        startingBlockId) : null;\n       if (corruptBlockIds != null) {\n         for (Long blockId: corruptBlockIds) {\n           doc.startTag(\"block_id\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java",
                "sha": "1f3d328b8f0262b7256a7712419fb5f929608e22",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp?ref=7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp",
                "patch": "@@ -25,6 +25,7 @@\n \timport=\"org.apache.hadoop.fs.Path\"\n \timport=\"org.apache.hadoop.ha.HAServiceProtocol.HAServiceState\"\n \timport=\"java.util.Collection\"\n+\timport=\"java.util.Collections\"\n \timport=\"java.util.Arrays\" %>\n <%!//for java.io.Serializable\n   private static final long serialVersionUID = 1L;%>\n@@ -34,9 +35,10 @@\n   HAServiceState nnHAState = nn.getServiceState();\n   boolean isActive = (nnHAState == HAServiceState.ACTIVE);\n   String namenodeRole = nn.getRole().toString();\n-  String namenodeLabel = nn.getNameNodeAddressHostPortString();\n-  Collection<FSNamesystem.CorruptFileBlockInfo> corruptFileBlocks = \n-\tfsn.listCorruptFileBlocks(\"/\", null);\n+  String namenodeLabel = NamenodeJspHelper.getNameNodeLabel(nn);\n+  Collection<FSNamesystem.CorruptFileBlockInfo> corruptFileBlocks = fsn != null ?\n+    fsn.listCorruptFileBlocks(\"/\", null) :\n+    Collections.<FSNamesystem.CorruptFileBlockInfo>emptyList();\n   int corruptFileCount = corruptFileBlocks.size();\n %>\n \n@@ -48,7 +50,7 @@\n <h1><%=namenodeRole%> '<%=namenodeLabel%>'</h1>\n <%=NamenodeJspHelper.getVersionTable(fsn)%>\n <br>\n-<% if (isActive) { %> \n+<% if (isActive && fsn != null) { %> \n   <b><a href=\"/nn_browsedfscontent.jsp\">Browse the filesystem</a></b>\n   <br>\n <% } %> ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp",
                "sha": "7c9050ddb1c16072c46b6e8299f44f7a3aaa77bf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp?ref=7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
                "deletions": 11,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp",
                "patch": "@@ -34,29 +34,20 @@\n   boolean isActive = (nnHAState == HAServiceState.ACTIVE);\n   String namenodeRole = nn.getRole().toString();\n   String namenodeState = nnHAState.toString();\n-  String namenodeLabel = nn.getRpcServer() != null ?\n-    nn.getNameNodeAddressHostPortString() : null;\n+  String namenodeLabel = NamenodeJspHelper.getNameNodeLabel(nn);\n %>\n \n <!DOCTYPE html>\n <html>\n <head>\n <link rel=\"stylesheet\" type=\"text/css\" href=\"/static/hadoop.css\">\n-<% if (namenodeLabel != null) { %>\n <title>Hadoop <%=namenodeRole%>&nbsp;<%=namenodeLabel%></title>\n-<% } else { %>\n-<title>Hadoop <%=namenodeRole%></title>\n-<% } %>\n </head>    \n <body>\n-<% if (namenodeLabel != null) { %>\n <h1><%=namenodeRole%> '<%=namenodeLabel%>' (<%=namenodeState%>)</h1>\n-<% } else { %>\n-<h1><%=namenodeRole%> (<%=namenodeState%>)</h1>\n-<% } %>\n <%= NamenodeJspHelper.getVersionTable(fsn) %>\n <br />\n-<% if (isActive) { %> \n+<% if (isActive && fsn != null) { %> \n   <b><a href=\"/nn_browsedfscontent.jsp\">Browse the filesystem</a></b><br>\n <% } %> \n <b><a href=\"/logs/\"><%=namenodeRole%> Logs</a></b>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp",
                "sha": "10872a7af0957aed0e92337e1648b15b503dce3e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp?ref=7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp",
                "patch": "@@ -33,7 +33,7 @@ String namenodeRole = nn.getRole().toString();\n FSNamesystem fsn = nn.getNamesystem();\n HAServiceState nnHAState = nn.getServiceState();\n boolean isActive = (nnHAState == HAServiceState.ACTIVE);\n-String namenodeLabel = nn.getNameNodeAddressHostPortString();\n+String namenodeLabel = NamenodeJspHelper.getNameNodeLabel(nn);\n %>\n \n <!DOCTYPE html>\n@@ -46,7 +46,7 @@ String namenodeLabel = nn.getNameNodeAddressHostPortString();\n <h1><%=namenodeRole%> '<%=namenodeLabel%>'</h1>\n <%= NamenodeJspHelper.getVersionTable(fsn) %>\n <br />\n-<% if (isActive) { %> \n+<% if (isActive && fsn != null) { %> \n   <b><a href=\"/nn_browsedfscontent.jsp\">Browse the filesystem</a></b><br>\n <% } %> \n <b><a href=\"/logs/\"><%=namenodeRole%> Logs</a></b><br>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp",
                "sha": "3bb349860384cc7e65ad5c2032488da6d56ac700",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java?ref=7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java",
                "patch": "@@ -25,12 +25,15 @@\n import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.util.List;\n import java.util.regex.Pattern;\n \n+import javax.servlet.ServletContext;\n import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n import javax.servlet.jsp.JspWriter;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -45,6 +48,7 @@\n import org.junit.Before;\n import org.junit.Test;\n import org.mockito.ArgumentCaptor;\n+import org.znerd.xmlenc.XMLOutputter;\n \n public class TestNameNodeJspHelper {\n \n@@ -117,6 +121,75 @@ public void testGetRollingUpgradeText() {\n     Assert.assertEquals(\"\", NamenodeJspHelper.getRollingUpgradeText(null));\n   }\n \n+  /**\n+   * Tests for non-null, non-empty NameNode label.\n+   */\n+  @Test\n+  public void testGetNameNodeLabel() {\n+    String nameNodeLabel = NamenodeJspHelper.getNameNodeLabel(\n+      cluster.getNameNode());\n+    Assert.assertNotNull(nameNodeLabel);\n+    Assert.assertFalse(nameNodeLabel.isEmpty());\n+  }\n+\n+  /**\n+   * Tests for non-null, non-empty NameNode label when called before\n+   * initialization of the NameNode RPC server.\n+   */\n+  @Test\n+  public void testGetNameNodeLabelNullRpcServer() {\n+    NameNode nn = mock(NameNode.class);\n+    when(nn.getRpcServer()).thenReturn(null);\n+    String nameNodeLabel = NamenodeJspHelper.getNameNodeLabel(\n+      cluster.getNameNode());\n+    Assert.assertNotNull(nameNodeLabel);\n+    Assert.assertFalse(nameNodeLabel.isEmpty());\n+  }\n+\n+  /**\n+   * Tests that passing a null FSNamesystem to generateSnapshotReport does not\n+   * throw NullPointerException.\n+   */\n+  @Test\n+  public void testGenerateSnapshotReportNullNamesystem() throws Exception {\n+    NamenodeJspHelper.generateSnapshotReport(mock(JspWriter.class), null);\n+  }\n+\n+  /**\n+   * Tests that redirectToRandomDataNode does not throw NullPointerException if\n+   * it finds a null FSNamesystem.\n+   */\n+  @Test(expected=IOException.class)\n+  public void testRedirectToRandomDataNodeNullNamesystem() throws Exception {\n+    NameNode nn = mock(NameNode.class);\n+    when(nn.getNamesystem()).thenReturn(null);\n+    ServletContext context = mock(ServletContext.class);\n+    when(context.getAttribute(\"name.node\")).thenReturn(nn);\n+    NamenodeJspHelper.redirectToRandomDataNode(context,\n+      mock(HttpServletRequest.class), mock(HttpServletResponse.class));\n+  }\n+\n+  /**\n+   * Tests that XMLBlockInfo does not throw NullPointerException if it finds a\n+   * null FSNamesystem.\n+   */\n+  @Test\n+  public void testXMLBlockInfoNullNamesystem() throws IOException {\n+    XMLOutputter doc = new XMLOutputter(mock(JspWriter.class), \"UTF-8\");\n+    new NamenodeJspHelper.XMLBlockInfo(null, 1L).toXML(doc);\n+  }\n+\n+  /**\n+   * Tests that XMLCorruptBlockInfo does not throw NullPointerException if it\n+   * finds a null FSNamesystem.\n+   */\n+  @Test\n+  public void testXMLCorruptBlockInfoNullNamesystem() throws IOException {\n+    XMLOutputter doc = new XMLOutputter(mock(JspWriter.class), \"UTF-8\");\n+    new NamenodeJspHelper.XMLCorruptBlockInfo(null, mock(Configuration.class),\n+      10, 1L).toXML(doc);\n+  }\n+\n   /**\n    * Checks if the list contains any string that partially matches the regex.\n    * ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7a2d8a8ecde2ba8b9725ea86cb0fd6be2090f318/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java",
                "sha": "3207f0ccb01b1464913802e40b2fe376a2561f31",
                "status": "modified"
            }
        ],
        "message": "HDFS-5279. Guard against NullPointerException in NameNode JSP pages before initialization of FSNamesystem. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1528308 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/92447b9240af615664bbdf36b973080e312e85c7",
        "patched_files": [
            "corrupt_files.jsp",
            "dfshealth.jsp",
            "dfsnodelist.jsp",
            "CHANGES.txt",
            "NamenodeJspHelper.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNameNodeJspHelper.java"
        ]
    },
    "hadoop-common_7c4b3e9": {
        "bug_id": "hadoop-common_7c4b3e9",
        "commit": "https://github.com/apache/hadoop-common/commit/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt?ref=7c4b3e9a0405bb014c4fe8c79ef82249687dbf28",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "patch": "@@ -207,3 +207,6 @@ Branch-2802 Snapshot (Unreleased)\n \n   HDFS-4616. Update the FilesDeleted metric while deleting file/dir in the\n   current tree.  (Jing Zhao via szetszwo)\n+\n+  HDFS-4627. Fix FSImageFormat#Loader NPE and synchronization issues.\n+  (Jing Zhao via suresh)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "sha": "a02b36088c79dc1b671d5932b7a52c8509c280b0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java?ref=7c4b3e9a0405bb014c4fe8c79ef82249687dbf28",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
                "patch": "@@ -561,7 +561,7 @@ INodeWithAdditionalFields loadINode(final byte[] localName, boolean isSnapshotIN\n       \n       // read blocks\n       BlockInfo[] blocks = null;\n-      if (numBlocks > 0) {\n+      if (numBlocks >= 0) {\n         blocks = new BlockInfo[numBlocks];\n         for (int j = 0; j < numBlocks; j++) {\n           blocks[j] = new BlockInfo(replication);\n@@ -660,7 +660,7 @@ private void loadFilesUnderConstruction(DataInputStream in,\n               ((INodeFileWithSnapshot)oldnode).getDiffs());\n         }\n \n-        fsDir.unprotectedReplaceINodeFile(path, oldnode, cons);\n+        fsDir.replaceINodeFile(path, oldnode, cons);\n         namesystem.leaseManager.addLease(cons.getClientName(), path); \n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
                "sha": "61e0ae4e61d3eed9abb131cf9b9b9a662f1328bf",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java?ref=7c4b3e9a0405bb014c4fe8c79ef82249687dbf28",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hdfs.server.namenode;\n \n+import static org.junit.Assert.assertEquals;\n+\n import java.io.File;\n import java.io.IOException;\n import java.util.ArrayList;\n@@ -26,13 +28,16 @@\n \n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants.SafeModeAction;\n import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n import org.apache.hadoop.hdfs.server.namenode.NNStorage.NameNodeFile;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper;\n@@ -305,4 +310,68 @@ public void testSaveLoadImageWithAppending() throws Exception {\n     // compare two dumped tree\n     SnapshotTestHelper.compareDumpedTreeInFile(fsnBefore, fsnAfter);\n   }\n+  \n+  /**\n+   * Test the fsimage loading while there is file under construction.\n+   */\n+  @Test (timeout=60000)\n+  public void testLoadImageWithAppending() throws Exception {\n+    Path sub1 = new Path(dir, \"sub1\");\n+    Path sub1file1 = new Path(sub1, \"sub1file1\");\n+    Path sub1file2 = new Path(sub1, \"sub1file2\");\n+    DFSTestUtil.createFile(hdfs, sub1file1, BLOCKSIZE, REPLICATION, seed);\n+    DFSTestUtil.createFile(hdfs, sub1file2, BLOCKSIZE, REPLICATION, seed);\n+    \n+    // 1. create snapshot s0\n+    hdfs.allowSnapshot(dir.toString());\n+    hdfs.createSnapshot(dir, \"s0\");\n+    \n+    // 2. create snapshot s1 before appending sub1file1 finishes\n+    HdfsDataOutputStream out = appendFileWithoutClosing(sub1file1, BLOCKSIZE);\n+    out.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));      \n+    \n+    // save namespace and restart cluster\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+    hdfs.saveNamespace();\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+    \n+    cluster.shutdown();\n+    cluster = new MiniDFSCluster.Builder(conf).format(false)\n+        .numDataNodes(REPLICATION).build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+  }\n+  \n+  /**\n+   * Test fsimage loading when 1) there is an empty file loaded from fsimage,\n+   * and 2) there is later an append operation to be applied from edit log.\n+   */\n+  @Test\n+  public void testLoadImageWithEmptyFile() throws Exception {\n+    // create an empty file\n+    Path file = new Path(dir, \"file\");\n+    FSDataOutputStream out = hdfs.create(file);\n+    out.close();\n+    \n+    // save namespace\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+    hdfs.saveNamespace();\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+    \n+    // append to the empty file\n+    out = hdfs.append(file);\n+    out.write(1);\n+    out.close();\n+    \n+    // restart cluster\n+    cluster.shutdown();\n+    cluster = new MiniDFSCluster.Builder(conf).format(false)\n+        .numDataNodes(REPLICATION).build();\n+    cluster.waitActive();\n+    hdfs = cluster.getFileSystem();\n+    \n+    FileStatus status = hdfs.getFileStatus(file);\n+    assertEquals(1, status.getLen());\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7c4b3e9a0405bb014c4fe8c79ef82249687dbf28/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "sha": "5d22836b6c5fb7903178a31bccf0f442287c0a8d",
                "status": "modified"
            }
        ],
        "message": "HDFS-4627. Fix FSImageFormat#Loader NPE and synchronization issues. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1460389 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/861f8938e181fad4a69e92999aa76a05d8fe4826",
        "patched_files": [
            "CHANGES.HDFS-2802.txt",
            "FSImageFormat.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSImageWithSnapshot.java"
        ]
    },
    "hadoop-common_7d2b717": {
        "bug_id": "hadoop-common_7d2b717",
        "commit": "https://github.com/apache/hadoop-common/commit/7d2b7171ff0dab08e6e918425afd2b985bb25206",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=7d2b7171ff0dab08e6e918425afd2b985bb25206",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -401,6 +401,9 @@ Release 2.1.1-beta - UNRELEASED\n     HDFS-5132. Deadlock in NameNode between SafeModeMonitor#run and \n     DatanodeManager#handleHeartbeat. (kihwal)\n \n+    HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization().\n+    (Plamen Jeliazkov via shv)\n+\n Release 2.1.0-beta - 2013-08-22\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "84100e18c4dd693927880cc330ce7cad9758642e",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=7d2b7171ff0dab08e6e918425afd2b985bb25206",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -174,7 +174,6 @@\n import org.apache.hadoop.hdfs.server.namenode.INode.BlocksMapUpdateInfo;\n import org.apache.hadoop.hdfs.server.namenode.JournalSet.JournalAndStream;\n import org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNode.OperationCategory;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;\n@@ -3772,24 +3771,32 @@ void commitBlockSynchronization(ExtendedBlock lastblock,\n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n-        DatanodeDescriptor[] descriptors = null;\n+        List<DatanodeDescriptor> targetList =\n+            new ArrayList<DatanodeDescriptor>(newtargets.length);\n         if (newtargets.length > 0) {\n-          descriptors = new DatanodeDescriptor[newtargets.length];\n-          for(int i = 0; i < newtargets.length; i++) {\n-            descriptors[i] = blockManager.getDatanodeManager().getDatanode(\n-                newtargets[i]);\n+          for (DatanodeID newtarget : newtargets) {\n+            // try to get targetNode\n+            DatanodeDescriptor targetNode =\n+                blockManager.getDatanodeManager().getDatanode(newtarget);\n+            if (targetNode != null)\n+              targetList.add(targetNode);\n+            else if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"DatanodeDescriptor (=\" + newtarget + \") not found\");\n+            }\n           }\n         }\n-        if ((closeFile) && (descriptors != null)) {\n+        if ((closeFile) && !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i = 0; i < descriptors.length; i++) {\n-            descriptors[i].addBlock(storedBlock);\n+          for (DatanodeDescriptor targetNode : targetList) {\n+            targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n-        pendingFile.setLastBlock(storedBlock, descriptors);\n+        DatanodeDescriptor[] targetArray =\n+            new DatanodeDescriptor[targetList.size()];\n+        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "a397ce94fd1e4727903d2b56e5d84a487b8c0502",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java?ref=7d2b7171ff0dab08e6e918425afd2b985bb25206",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "patch": "@@ -169,4 +169,23 @@ public void testCommitBlockSynchronizationWithClose() throws IOException {\n     namesystemSpy.commitBlockSynchronization(\n         lastBlock, genStamp, length, true, false, newTargets, null);\n   }\n+\n+  @Test\n+  public void testCommitBlockSynchronizationWithCloseAndNonExistantTarget()\n+      throws IOException {\n+    INodeFileUnderConstruction file = mock(INodeFileUnderConstruction.class);\n+    Block block = new Block(blockId, length, genStamp);\n+    FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);\n+    DatanodeID[] newTargets = new DatanodeID[]{\n+        new DatanodeID(\"0.0.0.0\", \"nonexistantHost\", \"1\", 0, 0, 0)};\n+\n+    ExtendedBlock lastBlock = new ExtendedBlock();\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true,\n+        false, newTargets, null);\n+\n+    // Repeat the call to make sure it returns true\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true, false, newTargets, null);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/7d2b7171ff0dab08e6e918425afd2b985bb25206/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "sha": "f40b799d1a824c27dfc948f51d85052a8fb04392",
                "status": "modified"
            }
        ],
        "message": "HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization(). Contributed by Plamen Jeliazkov.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1518851 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/53d8868abe58a9469736ee7d7011fddd1f41bb62",
        "patched_files": [
            "CHANGES.txt",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java",
            "TestCommitBlockSynchronization.java"
        ]
    },
    "hadoop-common_8157dac": {
        "bug_id": "hadoop-common_8157dac",
        "commit": "https://github.com/apache/hadoop-common/commit/8157dac63eab6756110ed1d4e65552a9ac98bb4d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8157dac63eab6756110ed1d4e65552a9ac98bb4d/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=8157dac63eab6756110ed1d4e65552a9ac98bb4d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -55,6 +55,9 @@ Release 2.2.0 - UNRELEASED\n \n   BUG FIXES\n \n+    YARN-1128. FifoPolicy.computeShares throws NPE on empty list of Schedulables\n+    (Karthik Kambatla via Sandy Ryza)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8157dac63eab6756110ed1d4e65552a9ac98bb4d/hadoop-yarn-project/CHANGES.txt",
                "sha": "4b41f79b6975cd0632254abb65bf1c05f2ca0e3c",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8157dac63eab6756110ed1d4e65552a9ac98bb4d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java?ref=8157dac63eab6756110ed1d4e65552a9ac98bb4d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.Serializable;\n import java.util.Collection;\n import java.util.Comparator;\n+import java.util.Iterator;\n \n import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.classification.InterfaceStability.Unstable;\n@@ -73,6 +74,10 @@ public int compare(Schedulable s1, Schedulable s2) {\n   @Override\n   public void computeShares(Collection<? extends Schedulable> schedulables,\n       Resource totalResources) {\n+    if (schedulables.isEmpty()) {\n+      return;\n+    }\n+\n     Schedulable earliest = null;\n     for (Schedulable schedulable : schedulables) {\n       if (earliest == null ||",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8157dac63eab6756110ed1d4e65552a9ac98bb4d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java",
                "sha": "3451cfea4c50b68dab286eaa030dfd95eea9ef57",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8157dac63eab6756110ed1d4e65552a9ac98bb4d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java?ref=8157dac63eab6756110ed1d4e65552a9ac98bb4d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java",
                "patch": "@@ -0,0 +1,57 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies;\n+\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy;\n+import org.apache.hadoop.yarn.util.resource.Resources;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+\n+public class TestEmptyQueues {\n+  private Collection<? extends Schedulable> schedulables;\n+\n+  @Before\n+  public void setup() {\n+    schedulables = new ArrayList<Schedulable>();\n+  }\n+\n+  private void testComputeShares(SchedulingPolicy policy) {\n+    policy.computeShares(schedulables, Resources.none());\n+  }\n+\n+  @Test (timeout = 1000)\n+  public void testFifoPolicy() {\n+    testComputeShares(SchedulingPolicy.getInstance(FifoPolicy.class));\n+  }\n+\n+  @Test (timeout = 1000)\n+  public void testFairSharePolicy() {\n+    testComputeShares(SchedulingPolicy.getInstance(FairSharePolicy.class));\n+  }\n+\n+  @Test (timeout = 1000)\n+  public void testDRFPolicy() {\n+    testComputeShares(\n+        SchedulingPolicy.getInstance(DominantResourceFairnessPolicy.class));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8157dac63eab6756110ed1d4e65552a9ac98bb4d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java",
                "sha": "4636c5bbd8def9014b3db16d0333a7fc9af98942",
                "status": "added"
            }
        ],
        "message": "YARN-1128. FifoPolicy.computeShares throws NPE on empty list of Schedulables (Karthik Kambatla via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525313 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e157fc441b19058241930cda797f5728fadcaffc",
        "patched_files": [
            "FifoPolicy.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestEmptyQueues.java"
        ]
    },
    "hadoop-common_853e1e1": {
        "bug_id": "hadoop-common_853e1e1",
        "commit": "https://github.com/apache/hadoop-common/commit/853e1e1c60f3b949ad9877e55e59b38a91da6e48",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=853e1e1c60f3b949ad9877e55e59b38a91da6e48",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -201,6 +201,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2117. Fixed the issue that secret file reader is potentially not\n     closed in TimelineAuthenticationFilterInitializer. (Chen He via zjshen)\n \n+    YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator.\n+    (Zhijie Shen via vinodkv)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/CHANGES.txt",
                "sha": "f32978745e7f6532310c937ac3d8be265aeee4b5",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java?ref=853e1e1c60f3b949ad9877e55e59b38a91da6e48",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "patch": "@@ -35,13 +35,15 @@\n import org.apache.hadoop.security.authentication.client.KerberosAuthenticator;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.yarn.api.records.timeline.TimelineDelegationTokenResponse;\n+import org.apache.hadoop.yarn.security.client.TimelineAuthenticationConsts;\n import org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier;\n import org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation;\n-import org.apache.hadoop.yarn.security.client.TimelineAuthenticationConsts;\n import org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n \n+import com.google.common.annotations.VisibleForTesting;\n+\n /**\n  * A <code>KerberosAuthenticator</code> subclass that fallback to\n  * {@link TimelineAuthenticationConsts}.\n@@ -77,9 +79,15 @@ public static void injectDelegationToken(Map<String, String> params,\n     }\n   }\n \n-  private boolean hasDelegationToken(URL url) {\n-    return url.getQuery().contains(\n-        TimelineAuthenticationConsts.DELEGATION_PARAM + \"=\");\n+  @Private\n+  @VisibleForTesting\n+  boolean hasDelegationToken(URL url) {\n+    if (url.getQuery() == null) {\n+      return false;\n+    } else {\n+      return url.getQuery().contains(\n+          TimelineAuthenticationConsts.DELEGATION_PARAM + \"=\");\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "sha": "25333c7551b014e424045bebaf839ba3a7baf5dd",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop-common/blob/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java?ref=853e1e1c60f3b949ad9877e55e59b38a91da6e48",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "patch": "@@ -0,0 +1,40 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.client.api.impl;\n+\n+import java.net.URL;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestTimelineAuthenticator {\n+\n+  @Test\n+  public void testHasDelegationTokens() throws Exception {\n+    TimelineAuthenticator authenticator = new TimelineAuthenticator();\n+    Assert.assertFalse(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource\")));\n+    Assert.assertFalse(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?other=xxxx\")));\n+    Assert.assertTrue(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?delegation=yyyy\")));\n+    Assert.assertTrue(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?other=xxxx&delegation=yyyy\")));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/853e1e1c60f3b949ad9877e55e59b38a91da6e48/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "sha": "19aaa88533f24514a224645387244cc421a3db77",
                "status": "added"
            }
        ],
        "message": "YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator. Contributed by Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601000 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/81b9fd46eb8ac6e22d9523ff056b509d805e9017",
        "patched_files": [
            "CHANGES.txt",
            "TimelineAuthenticator.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestTimelineAuthenticator.java"
        ]
    },
    "hadoop-common_8747d46": {
        "bug_id": "hadoop-common_8747d46",
        "commit": "https://github.com/apache/hadoop-common/commit/8747d46d184a5764624131f0f9412602b3a24d9d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8747d46d184a5764624131f0f9412602b3a24d9d/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=8747d46d184a5764624131f0f9412602b3a24d9d",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -456,6 +456,8 @@ Release 0.21.0 - Unreleased\n     HDFS-725. Support the build error fix for HADOOP-6327.  (Sanjay Radia via\n     szetszwo)\n \n+    HDFS-625. Fix NullPointerException thrown from ListPathServlet. (suresh)\n+\n Release 0.20.2 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8747d46d184a5764624131f0f9412602b3a24d9d/CHANGES.txt",
                "sha": "4be7428e5d20198f5f8cf8ade1ce255410278661",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8747d46d184a5764624131f0f9412602b3a24d9d/src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java?ref=8747d46d184a5764624131f0f9412602b3a24d9d",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java",
                "patch": "@@ -152,7 +152,12 @@ public void doGet(HttpServletRequest request, HttpServletResponse response)\n       while (!pathstack.empty()) {\n         String p = pathstack.pop();\n         try {\n-          for (FileStatus i : nnproxy.getListing(p)) {\n+          FileStatus[] listing = nnproxy.getListing(p);\n+          if (listing == null) {\n+            LOG.warn(\"ListPathsServlet - Path \" + p + \" does not exist\");\n+            continue;\n+          }\n+          for (FileStatus i : listing) {\n             if (exclude.matcher(i.getPath().getName()).matches()\n                 || !filter.matcher(i.getPath().getName()).matches()) {\n               continue;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8747d46d184a5764624131f0f9412602b3a24d9d/src/java/org/apache/hadoop/hdfs/server/namenode/ListPathsServlet.java",
                "sha": "dd20aa1437a92f8ec9d3b07ff9b717446a088f42",
                "status": "modified"
            },
            {
                "additions": 136,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8747d46d184a5764624131f0f9412602b3a24d9d/src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java",
                "changes": 136,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java?ref=8747d46d184a5764624131f0f9412602b3a24d9d",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java",
                "patch": "@@ -0,0 +1,136 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.server.namenode.ListPathsServlet;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Test for {@link ListPathsServlet} that serves the URL\n+ * http://<namenodeaddress:httpport?/listPaths\n+ * \n+ * This test does not use the servlet directly. Instead it is based on\n+ * {@link HftpFileSystem}, which uses this servlet to implement\n+ * {@link HftpFileSystem#listStatus(Path)} method.\n+ */\n+public class TestListPathServlet {\n+  private static final Configuration CONF = new HdfsConfiguration();\n+  private static MiniDFSCluster cluster;\n+  private static FileSystem fs;\n+  private static URI hftpURI;\n+  private static HftpFileSystem hftpFs;\n+  private Random r = new Random();\n+  private List<String> filelist = new ArrayList<String>();\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    // start a cluster with single datanode\n+    cluster = new MiniDFSCluster(CONF, 1, true, null);\n+    cluster.waitActive();\n+    fs = cluster.getFileSystem();\n+\n+    final String str = \"hftp://\"\n+        + CONF.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY);\n+    hftpURI = new URI(str);\n+    hftpFs = (HftpFileSystem) FileSystem.newInstance(hftpURI, CONF);\n+  }\n+\n+  @AfterClass\n+  public static void teardown() {\n+    cluster.shutdown();\n+  }\n+\n+  /** create a file with a length of <code>fileLen</code> */\n+  private void createFile(String fileName, long fileLen) throws IOException {\n+    filelist.add(hftpURI + fileName);\n+    final Path filePath = new Path(fileName);\n+    DFSTestUtil.createFile(fs, filePath, fileLen, (short) 1, r.nextLong());\n+  }\n+\n+  private void mkdirs(String dirName) throws IOException {\n+    filelist.add(hftpURI + dirName);\n+    fs.mkdirs(new Path(dirName));\n+  }\n+\n+  @Test\n+  public void testListStatus() throws Exception {\n+    // Empty root directory\n+    checkStatus(\"/\");\n+\n+    // Root directory with files and directories\n+    createFile(\"/a\", 1);\n+    createFile(\"/b\", 1);\n+    mkdirs(\"/dir\");\n+    checkStatus(\"/\");\n+\n+    // A directory with files and directories\n+    createFile(\"/dir/a\", 1);\n+    createFile(\"/dir/b\", 1);\n+    mkdirs(\"/dir/dir1\");\n+    checkStatus(\"/dir\");\n+\n+    // Non existent path\n+    checkStatus(\"/nonexistent\");\n+    checkStatus(\"/nonexistent/a\");\n+  }\n+\n+  private void checkStatus(String listdir) throws IOException {\n+    final Path listpath = hftpFs.makeQualified(new Path(listdir));\n+    listdir = listpath.toString();\n+    final FileStatus[] statuslist = hftpFs.listStatus(listpath);\n+    for (String directory : filelist) {\n+      System.out.println(\"dir:\" + directory);\n+    }\n+    for (String file : filelist) {\n+      System.out.println(\"file:\" + file);\n+    }\n+    for (FileStatus status : statuslist) {\n+      System.out.println(\"status:\" + status.getPath().toString() + \" type \"\n+          + (status.isDir() ? \"directory\" : \"file\"));\n+    }\n+    for (String file : filelist) {\n+      boolean found = false;\n+      // Consider only file under the list path\n+      if (!file.startsWith(listpath.toString()) ||\n+          file.equals(listpath.toString())) {\n+        continue;\n+      }\n+      for (FileStatus status : statuslist) {\n+        if (status.getPath().toString().equals(file)) {\n+          found = true;\n+          break;\n+        }\n+      }\n+      Assert.assertTrue(\"Directory/file not returned in list status \" + file,\n+          found);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8747d46d184a5764624131f0f9412602b3a24d9d/src/test/hdfs/org/apache/hadoop/hdfs/TestListPathServlet.java",
                "sha": "3e1148fcc79fff827bea00fbbeeeb71cc73debd2",
                "status": "added"
            }
        ],
        "message": "HDFS-625. Fix NullPointerException thrown from ListPathServlet. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@829990 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/2fb7cc7cf9b48ac15ce7053bb23de9d50f8d80ed",
        "patched_files": [
            "ListPathsServlet.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestListPathServlet.java"
        ]
    },
    "hadoop-common_8809daa": {
        "bug_id": "hadoop-common_8809daa",
        "commit": "https://github.com/apache/hadoop-common/commit/8809daa4e71022ea3a420405a04165e1b8318994",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=8809daa4e71022ea3a420405a04165e1b8318994",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1384,6 +1384,9 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3023. Fixed clients to display queue state correctly. (Ravi\n     Prakash via acmurthy) \n \n+    MAPREDUCE-2970. Fixed NPEs in corner cases with different configurations\n+    for mapreduce.framework.name. (Venu Gopala Rao via vinodkv)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "36a5f24c037c8d3caf8b79c48d22df7ff361a02d",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java?ref=8809daa4e71022ea3a420405a04165e1b8318994",
                "deletions": 13,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "patch": "@@ -41,8 +41,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n-import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.Token;\n \n /**\n  * Provides a way to access information about the map/reduce cluster.\n@@ -68,30 +68,41 @@\n   }\n   \n   public Cluster(Configuration conf) throws IOException {\n-    this.conf = conf;\n-    this.ugi = UserGroupInformation.getCurrentUser();\n-    for (ClientProtocolProvider provider : ServiceLoader.load(ClientProtocolProvider.class)) {\n-      ClientProtocol clientProtocol = provider.create(conf);\n-      if (clientProtocol != null) {\n-        clientProtocolProvider = provider;\n-        client = clientProtocol;\n-        break;\n-      }\n-    }\n+    this(null, conf);\n   }\n \n   public Cluster(InetSocketAddress jobTrackAddr, Configuration conf) \n       throws IOException {\n     this.conf = conf;\n     this.ugi = UserGroupInformation.getCurrentUser();\n-    for (ClientProtocolProvider provider : ServiceLoader.load(ClientProtocolProvider.class)) {\n-      ClientProtocol clientProtocol = provider.create(jobTrackAddr, conf);\n+    initialize(jobTrackAddr, conf);\n+  }\n+  \n+  private void initialize(InetSocketAddress jobTrackAddr, Configuration conf)\n+      throws IOException {\n+\n+    for (ClientProtocolProvider provider : ServiceLoader\n+        .load(ClientProtocolProvider.class)) {\n+      ClientProtocol clientProtocol = null;\n+      if (jobTrackAddr == null) {\n+        clientProtocol = provider.create(conf);\n+      } else {\n+        clientProtocol = provider.create(jobTrackAddr, conf);\n+      }\n+\n       if (clientProtocol != null) {\n         clientProtocolProvider = provider;\n         client = clientProtocol;\n         break;\n       }\n     }\n+\n+    if (null == clientProtocolProvider || null == client) {\n+      throw new IOException(\n+          \"Cannot initialize Cluster. Please check your configuration for \"\n+              + MRConfig.FRAMEWORK_NAME\n+              + \" and the correspond server addresses.\");\n+    }\n   }\n \n   ClientProtocol getClient() {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "sha": "33d5f81b4fc3b5120fc28035823f38a43da848d3",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java?ref=8809daa4e71022ea3a420405a04165e1b8318994",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "patch": "@@ -0,0 +1,59 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.hadoop.mapreduce;\r\n+\r\n+import java.io.IOException;\r\n+\r\n+import junit.framework.TestCase;\r\n+\r\n+import org.apache.hadoop.conf.Configuration;\r\n+import org.apache.hadoop.mapred.YARNRunner;\r\n+import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\r\n+import org.junit.Test;\r\n+\r\n+public class TestYarnClientProtocolProvider extends TestCase {\r\n+\r\n+  @Test\r\n+  public void testClusterWithYarnClientProtocolProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration(false);\r\n+    Cluster cluster = null;\r\n+\r\n+    try {\r\n+      cluster = new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with out any framework name\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf = new Configuration();\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n+      cluster = new Cluster(conf);\r\n+      ClientProtocol client = cluster.getClient();\r\n+      assertTrue(client instanceof YARNRunner);\r\n+    } catch (IOException e) {\r\n+\r\n+    } finally {\r\n+      if (cluster != null) {\r\n+        cluster.close();\r\n+      }\r\n+    }\r\n+  }\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "sha": "2bc9030bf85ea491055dbced5260c7f3b728459f",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java?ref=8809daa4e71022ea3a420405a04165e1b8318994",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "patch": "@@ -43,20 +43,24 @@ public ClientProtocol create(Configuration conf) throws IOException {\n     String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");\n     if (!\"local\".equals(tracker)) {\n       return createRPCProxy(JobTracker.getAddress(conf), conf);\n+    } else {\n+      throw new IOException(\"Invalid \\\"\" + JTConfig.JT_IPC_ADDRESS\n+          + \"\\\" configuration value for JobTracker: \\\"\"\n+          + tracker + \"\\\"\");\n     }\n-    return null;\n   }\n \n   @Override\n-  public ClientProtocol create(InetSocketAddress addr, Configuration conf) throws IOException {\n+  public ClientProtocol create(InetSocketAddress addr, Configuration conf)\n+      throws IOException {\n     return createRPCProxy(addr, conf);\n   }\n-  \n+\n   private ClientProtocol createRPCProxy(InetSocketAddress addr,\n       Configuration conf) throws IOException {\n     return (ClientProtocol) RPC.getProxy(ClientProtocol.class,\n-      ClientProtocol.versionID, addr, UserGroupInformation.getCurrentUser(),\n-      conf, NetUtils.getSocketFactory(conf, ClientProtocol.class));\n+        ClientProtocol.versionID, addr, UserGroupInformation.getCurrentUser(),\n+        conf, NetUtils.getSocketFactory(conf, ClientProtocol.class));\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "sha": "d12132c68d24c5b0cc793c05818cb7aa016f95b5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java?ref=8809daa4e71022ea3a420405a04165e1b8318994",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "patch": "@@ -37,11 +37,16 @@ public ClientProtocol create(Configuration conf) throws IOException {\n     if (framework != null && !framework.equals(\"local\")) {\n       return null;\n     }\n-    if (\"local\".equals(conf.get(JTConfig.JT_IPC_ADDRESS, \"local\"))) {\n+    String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");\n+    if (\"local\".equals(tracker)) {\n       conf.setInt(\"mapreduce.job.maps\", 1);\n       return new LocalJobRunner(conf);\n+    } else {\n+\n+      throw new IOException(\"Invalid \\\"\" + JTConfig.JT_IPC_ADDRESS\n+          + \"\\\" configuration value for LocalJobRunner : \\\"\"\n+          + tracker + \"\\\"\");\n     }\n-    return null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "sha": "d09b222ee9b2ed9a6972d139e0da0fa66792c420",
                "status": "modified"
            },
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "changes": 99,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java?ref=8809daa4e71022ea3a420405a04165e1b8318994",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "patch": "@@ -0,0 +1,99 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.hadoop.mapreduce;\r\n+\r\n+import java.io.IOException;\r\n+\r\n+import junit.framework.TestCase;\r\n+\r\n+import org.apache.hadoop.conf.Configuration;\r\n+import org.apache.hadoop.mapred.LocalJobRunner;\r\n+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\r\n+import org.junit.Test;\r\n+\r\n+public class TestClientProtocolProviderImpls extends TestCase {\r\n+\r\n+  @Test\r\n+  public void testClusterWithLocalClientProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration();\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with incorrect framework name\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n+\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster with Local Framework name should use local JT address\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n+      Cluster cluster = new Cluster(conf);\r\n+      assertTrue(cluster.getClient() instanceof LocalJobRunner);\r\n+      cluster.close();\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+  }\r\n+\r\n+  @Test\r\n+  public void testClusterWithJTClientProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration();\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with incorrect framework name\");\r\n+\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster with classic Framework name shouldnot use local JT address\");\r\n+\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf = new Configuration();\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n+      Cluster cluster = new Cluster(conf);\r\n+      cluster.close();\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8809daa4e71022ea3a420405a04165e1b8318994/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "sha": "a9044e24308133a4d0902e600a3a34fbaa9e93b5",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2970. Fixed NPEs in corner cases with different configurations for mapreduce.framework.name. Contributed by Venu Gopala Rao.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0e208ab036ce5cc492b5e3bab84aee26e2f3d6f8",
        "patched_files": [
            "YarnClientProtocolProvider.java",
            "Cluster.java",
            "CHANGES.txt",
            "JobTrackerClientProtocolProvider.java",
            "LocalClientProtocolProvider.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestYarnClientProtocolProvider.java",
            "TestCluster.java",
            "TestClientProtocolProviderImpls.java"
        ]
    },
    "hadoop-common_88a0e02": {
        "bug_id": "hadoop-common_88a0e02",
        "commit": "https://github.com/apache/hadoop-common/commit/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -287,6 +287,9 @@ Trunk (unreleased changes)\n \n   IMPROVEMENTS\n \n+    HDFS-1934. Fix NullPointerException when certain File APIs return null\n+    (Bharath Mundlapudi via mattf)\n+\n     HDFS-1510. Added test-patch.properties required by test-patch.sh (nigel)\n \n     HDFS-1628. Display full path in AccessControlException.  (John George",
                "raw_url": "https://github.com/apache/hadoop-common/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/CHANGES.txt",
                "sha": "b5791e72f8dba408e2e601a225900db85354175d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -38,11 +39,13 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.server.common.GenerationStamp;\n import org.apache.hadoop.hdfs.server.datanode.FSDataset.FSVolume;\n import org.apache.hadoop.util.Daemon;\n+import org.apache.hadoop.util.StringUtils;\n \n /**\n  * Periodically scans the data directories for block and block metadata files.\n@@ -480,9 +483,15 @@ public ScanInfoPerBlockPool call() throws Exception {\n     /** Compile list {@link ScanInfo} for the blocks in the directory <dir> */\n     private LinkedList<ScanInfo> compileReport(FSVolume vol, File dir,\n         LinkedList<ScanInfo> report) {\n-      File[] files = dir.listFiles();\n+      File[] files;\n+      try {\n+        files = FileUtil.listFiles(dir);\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Exception occured while compiling report: \", ioe);\n+        // Ignore this directory and proceed.\n+        return report;\n+      }\n       Arrays.sort(files);\n-\n       /*\n        * Assumption: In the sorted list of files block file appears immediately\n        * before block metadata file. This is true for the current naming",
                "raw_url": "https://github.com/apache/hadoop-common/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "sha": "9b38a5f3f3d1b5d4d483712fc4092e6ee638c338",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java?ref=88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da",
                "deletions": 7,
                "filename": "src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "patch": "@@ -99,7 +99,7 @@ public FSDir(File dir)\n                                 dir.toString());\n         }\n       } else {\n-        File[] files = dir.listFiles();\n+        File[] files = FileUtil.listFiles(dir); \n         int numChildren = 0;\n         for (int idx = 0; idx < files.length; idx++) {\n           if (files[idx].isDirectory()) {\n@@ -187,7 +187,7 @@ void getVolumeMap(String bpid, ReplicasMap volumeMap, FSVolume volume)\n      * original file name; otherwise the tmp file is deleted.\n      */\n     private void recoverTempUnlinkedBlock() throws IOException {\n-      File files[] = dir.listFiles();\n+      File files[] = FileUtil.listFiles(dir);\n       for (File file : files) {\n         if (!FSDataset.isUnlinkTmpFile(file)) {\n           continue;\n@@ -420,9 +420,9 @@ void getVolumeMap(ReplicasMap volumeMap) throws IOException {\n      * @param isFinalized true if the directory has finalized replicas;\n      *                    false if the directory has rbw replicas\n      */\n-    private void addToReplicasMap(ReplicasMap volumeMap, \n-        File dir, boolean isFinalized) {\n-      File blockFiles[] = dir.listFiles();\n+    private void addToReplicasMap(ReplicasMap volumeMap, File dir,\n+        boolean isFinalized) throws IOException {\n+      File blockFiles[] = FileUtil.listFiles(dir);\n       for (File blockFile : blockFiles) {\n         if (!Block.isBlockFilename(blockFile))\n           continue;\n@@ -756,15 +756,15 @@ private void deleteBPDirectories(String bpid, boolean force)\n           throw new IOException(\"Failed to delete \" + finalizedDir);\n         }\n         FileUtil.fullyDelete(tmpDir);\n-        for (File f : bpCurrentDir.listFiles()) {\n+        for (File f : FileUtil.listFiles(bpCurrentDir)) {\n           if (!f.delete()) {\n             throw new IOException(\"Failed to delete \" + f);\n           }\n         }\n         if (!bpCurrentDir.delete()) {\n           throw new IOException(\"Failed to delete \" + bpCurrentDir);\n         }\n-        for (File f : bpDir.listFiles()) {\n+        for (File f : FileUtil.listFiles(bpDir)) {\n           if (!f.delete()) {\n             throw new IOException(\"Failed to delete \" + f);\n           }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/88a0e02a2e1c4ed5edf2fde03d27f63a44c8f5da/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
                "sha": "41b47973883fd482cb6b8e4c5bb5ffbc8ba9cbe2",
                "status": "modified"
            }
        ],
        "message": "HDFS-1934. Fix NullPointerException when certain File APIs return null. Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1130262 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/1d79d91876e75ec025028b216971b65b6ae1c581",
        "patched_files": [
            "DirectoryScanner.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDirectoryScanner.java"
        ]
    },
    "hadoop-common_8955b15": {
        "bug_id": "hadoop-common_8955b15",
        "commit": "https://github.com/apache/hadoop-common/commit/8955b152c13c1d256f745379ca54ec41e96c1dfb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt?ref=8955b152c13c1d256f745379ca54ec41e96c1dfb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "patch": "@@ -59,3 +59,6 @@ fs-encryption (Unreleased)\n   OPTIMIZATIONS\n \n   BUG FIXES\n+\n+    HDFS-6733. Creating encryption zone results in NPE when\n+    KeyProvider is null. (clamb)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "sha": "c447d49b52ceab9ba1ad9515a622bb17810bf5ff",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=8955b152c13c1d256f745379ca54ec41e96c1dfb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -8448,6 +8448,11 @@ void createEncryptionZone(final String src, String keyNameArg)\n     String keyName = keyNameArg;\n     boolean success = false;\n     try {\n+      if (provider == null) {\n+        throw new IOException(\n+            \"Can't create an encryption zone for \" + src +\n+            \" since no key provider is available.\");\n+      }\n       if (keyName == null || keyName.isEmpty()) {\n         keyName = UUID.randomUUID().toString();\n         createNewKey(keyName, src);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "9b7cb1995839e75c07298a7dc62e449b1192b541",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java?ref=8955b152c13c1d256f745379ca54ec41e96c1dfb",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "patch": "@@ -68,6 +68,7 @@\n   private MiniDFSCluster cluster;\n   private HdfsAdmin dfsAdmin;\n   private DistributedFileSystem fs;\n+  private File testRootDir;\n \n   protected FileSystemTestWrapper fsWrapper;\n   protected FileContextTestWrapper fcWrapper;\n@@ -78,14 +79,14 @@ public void setup() throws IOException {\n     fsHelper = new FileSystemTestHelper();\n     // Set up java key store\n     String testRoot = fsHelper.getTestRootDir();\n-    File testRootDir = new File(testRoot).getAbsoluteFile();\n+    testRootDir = new File(testRoot).getAbsoluteFile();\n     conf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n         JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n     );\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n     Logger.getLogger(EncryptionZoneManager.class).setLevel(Level.TRACE);\n     fs = cluster.getFileSystem();\n-    fsWrapper = new FileSystemTestWrapper(cluster.getFileSystem());\n+    fsWrapper = new FileSystemTestWrapper(fs);\n     fcWrapper = new FileContextTestWrapper(\n         FileContext.getFileContext(cluster.getURI(), conf));\n     dfsAdmin = new HdfsAdmin(cluster.getURI(), conf);\n@@ -429,4 +430,25 @@ public void testCipherSuiteNegotiation() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 120000)\n+  public void testCreateEZWithNoProvider() throws Exception {\n+\n+    final Configuration clusterConf = cluster.getConfiguration(0);\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"\");\n+    cluster.restartNameNode(true);\n+    /* Test failure of create EZ on a directory that doesn't exist. */\n+    final Path zone1 = new Path(\"/zone1\");\n+    /* Normal creation of an EZ */\n+    fsWrapper.mkdir(zone1, FsPermission.getDirDefault(), true);\n+    try {\n+      dfsAdmin.createEncryptionZone(zone1, null);\n+      fail(\"expected exception\");\n+    } catch (IOException e) {\n+      assertExceptionContains(\"since no key provider is available\", e);\n+    }\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n+        JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n+    );\n+    cluster.restartNameNode(true);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8955b152c13c1d256f745379ca54ec41e96c1dfb/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "sha": "421396bdf3873e51886b7e34eaad6a0037162d89",
                "status": "modified"
            }
        ],
        "message": "HDFS-6733. Creating encryption zone results in NPE when KeyProvider is null. (clamb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612843 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0d4a9e51c83323efe987a0a279df9bb3521a53e2",
        "patched_files": [
            "CHANGES-fs-encryption.txt",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java",
            "TestEncryptionZones.java"
        ]
    },
    "hadoop-common_8cc0089": {
        "bug_id": "hadoop-common_8cc0089",
        "commit": "https://github.com/apache/hadoop-common/commit/8cc0089994e2d8674396eebef72bda0544eae718",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=8cc0089994e2d8674396eebef72bda0544eae718",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -3114,6 +3114,9 @@ Release 0.23.9 - UNRELEASED\n \n   BUG FIXES\n \n+    HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue.\n+    (Plamen Jeliazkov and Ravi Prakash via shv)\n+\n Release 0.23.8 - 2013-06-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "9bc76f3d5078b72e219dfc0f9dcc468276a052a6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=8cc0089994e2d8674396eebef72bda0544eae718",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -458,7 +458,8 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName = ((BlockInfo)block).getBlockCollection().getName();\n+      BlockCollection bc = ((BlockInfo) block).getBlockCollection();\n+      String fileName = (bc == null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: == live:, d: == decommissioned c: == corrupt e: == excess",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "966c59656742edeb7431b031687a716c42c8f8b7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java?ref=8cc0089994e2d8674396eebef72bda0544eae718",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "patch": "@@ -24,11 +24,8 @@\n import java.io.FileInputStream;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.util.Random;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeys;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSConfigKeys;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8cc0089994e2d8674396eebef72bda0544eae718/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "sha": "c0775a62504d43d5526e674c082c2017edb8f8b0",
                "status": "modified"
            }
        ],
        "message": "HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue. Contributed by Plamen Jeliazkov and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490433 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/318276e2fff29e0af2f8d72c183d3db57fc225d7",
        "patched_files": [
            "BlockManager.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java",
            "TestMetaSave.java"
        ]
    },
    "hadoop-common_8e7a6f8": {
        "bug_id": "hadoop-common_8e7a6f8",
        "commit": "https://github.com/apache/hadoop-common/commit/8e7a6f89ec501030e4dcc53f5230310083e8312d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=8e7a6f89ec501030e4dcc53f5230310083e8312d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -335,6 +335,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4128. AM Recovery expects all attempts of a completed task to\n     also be completed. (Bikas Saha via bobby)\n \n+    MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node\n+    updates. (Jason Lowe via sseth)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d0eaa60a7342be1d2e8012c739814d903a2851de",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=8e7a6f89ec501030e4dcc53f5230310083e8312d",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -1118,13 +1118,12 @@ private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNo\n   boolean canAssign(SchedulerApp application, Priority priority, \n       SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n-    // Reserved... \n-    if (reservedContainer != null) {\n-      return true;\n-    }\n-    \n     // Clearly we need containers for this application...\n     if (type == NodeType.OFF_SWITCH) {\n+      if (reservedContainer != null) {\n+        return true;\n+      }\n+\n       // 'Delay' off-switch\n       ResourceRequest offSwitchRequest = \n           application.getResourceRequest(priority, RMNode.ANY);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "2256799f9b55e4212bc184560789dcf12bcfb178",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java?ref=8e7a6f89ec501030e4dcc53f5230310083e8312d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "patch": "@@ -926,6 +926,103 @@ public void testReservation() throws Exception {\n     assertEquals(4*GB, a.getMetrics().getAllocatedMB());\n   }\n   \n+  @Test\n+  public void testStolenReservedContainer() throws Exception {\n+    // Manipulate queue 'a'\n+    LeafQueue a = stubLeafQueue((LeafQueue)queues.get(A));\n+    //unset maxCapacity\n+    a.setMaxCapacity(1.0f);\n+\n+    // Users\n+    final String user_0 = \"user_0\";\n+    final String user_1 = \"user_1\";\n+\n+    // Submit applications\n+    final ApplicationAttemptId appAttemptId_0 =\n+        TestUtils.getMockApplicationAttemptId(0, 0);\n+    SchedulerApp app_0 =\n+        new SchedulerApp(appAttemptId_0, user_0, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_0, user_0, A);\n+\n+    final ApplicationAttemptId appAttemptId_1 =\n+        TestUtils.getMockApplicationAttemptId(1, 0);\n+    SchedulerApp app_1 =\n+        new SchedulerApp(appAttemptId_1, user_1, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_1, user_1, A);\n+\n+    // Setup some nodes\n+    String host_0 = \"host_0\";\n+    SchedulerNode node_0 = TestUtils.getMockNode(host_0, DEFAULT_RACK, 0, 4*GB);\n+    String host_1 = \"host_1\";\n+    SchedulerNode node_1 = TestUtils.getMockNode(host_1, DEFAULT_RACK, 0, 4*GB);\n+\n+    final int numNodes = 3;\n+    Resource clusterResource = Resources.createResource(numNodes * (4*GB));\n+    when(csContext.getNumClusterNodes()).thenReturn(numNodes);\n+\n+    // Setup resource-requests\n+    Priority priority = TestUtils.createMockPriority(1);\n+    app_0.updateResourceRequests(Collections.singletonList(\n+            TestUtils.createResourceRequest(RMNodeImpl.ANY, 2*GB, 1, priority,\n+                recordFactory)));\n+\n+    // Setup app_1 to request a 4GB container on host_0 and\n+    // another 4GB container anywhere.\n+    ArrayList<ResourceRequest> appRequests_1 =\n+        new ArrayList<ResourceRequest>(4);\n+    appRequests_1.add(TestUtils.createResourceRequest(host_0, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(DEFAULT_RACK, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(RMNodeImpl.ANY, 4*GB, 2,\n+        priority, recordFactory));\n+    app_1.updateResourceRequests(appRequests_1);\n+\n+    // Start testing...\n+\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(2*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+    assertEquals(0*GB, a.getMetrics().getAvailableMB());\n+\n+    // Now, reservation should kick in for app_1\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(6*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(2*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+\n+    // node_1 heartbeats in and gets the DEFAULT_RACK request for app_1\n+    a.assignContainers(clusterResource, node_1);\n+    assertEquals(10*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_1.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(6*GB, a.getMetrics().getAllocatedMB());\n+\n+    // Now free 1 container from app_0 and try to assign to node_0\n+    a.completedContainer(clusterResource, app_0, node_0,\n+        app_0.getLiveContainers().iterator().next(), null, RMContainerEventType.KILL);\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(8*GB, a.getUsedResources().getMemory());\n+    assertEquals(0*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(8*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(8*GB, a.getMetrics().getAllocatedMB());\n+  }\n+\n   @Test\n   public void testReservationExchange() throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8e7a6f89ec501030e4dcc53f5230310083e8312d/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "sha": "8be9b20193e1b19eb3e99808b94dc5b15b8325b6",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node updates. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/459ddddc435dfd1ae01a64321a691d65fa6ced6f",
        "patched_files": [
            "LeafQueue.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop-common_8ea3e22": {
        "bug_id": "hadoop-common_8ea3e22",
        "commit": "https://github.com/apache/hadoop-common/commit/8ea3e22086f1fac73ba23285348a2f019df0ec92",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -715,6 +715,9 @@ Release 0.23.1 - Unreleased\n \n     MAPREDUCE-3813. Added a cache for resolved racks. (vinodkv via acmurthy)   \n \n+    MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps\n+    but no reduces. (Robert Joseph Evans via vinodkv)\n+\n Release 0.23.0 - 2011-11-01 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "050af54fcebc18df5a1efc456ce94a44380010c8",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 7,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "patch": "@@ -85,18 +85,21 @@ private static Path getOutputPath(TaskAttemptContext context) {\n    */\n   @Private\n   Path getJobAttemptPath(JobContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getJobAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getJobAttemptPath(context, out);\n   }\n \n   @Private\n   Path getTaskAttemptPath(TaskAttemptContext context) throws IOException {\n-    return getTaskAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : getTaskAttemptPath(context, out);\n   }\n \n   private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOException {\n     Path workPath = FileOutputFormat.getWorkOutputPath(context.getJobConf());\n-    if(workPath == null) {\n+    if(workPath == null && out != null) {\n       return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n       .getTaskAttemptPath(context, out);\n     }\n@@ -110,14 +113,17 @@ private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOE\n    * @return the path where the output of a committed task is stored until\n    * the entire job is committed.\n    */\n+  @Private\n   Path getCommittedTaskPath(TaskAttemptContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getCommittedTaskPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getCommittedTaskPath(context, out);\n   }\n \n   public Path getWorkPath(TaskAttemptContext context, Path outputPath) \n   throws IOException {\n-    return getTaskAttemptPath(context, outputPath);\n+    return outputPath == null ? null : getTaskAttemptPath(context, outputPath);\n   }\n   \n   @Override\n@@ -156,6 +162,7 @@ public void abortJob(JobContext context, int runState)\n     getWrapped(context).abortJob(context, state);\n   }\n   \n+  @Override\n   public void setupTask(TaskAttemptContext context) throws IOException {\n     getWrapped(context).setupTask(context);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "sha": "a6190d2060d4c3ee0fc1934f86235ac51366e7ca",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 27,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "patch": "@@ -495,36 +495,40 @@ public boolean isRecoverySupported() {\n   @Override\n   public void recoverTask(TaskAttemptContext context)\n       throws IOException {\n-    context.progress();\n-    TaskAttemptID attemptId = context.getTaskAttemptID();\n-    int previousAttempt = getAppAttemptId(context) - 1;\n-    if (previousAttempt < 0) {\n-      throw new IOException (\"Cannot recover task output for first attempt...\");\n-    }\n-    \n-    Path committedTaskPath = getCommittedTaskPath(context);\n-    Path previousCommittedTaskPath = getCommittedTaskPath(\n-        previousAttempt, context);\n-    FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n-    \n-    LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n-        + \" into \" + committedTaskPath);\n-    if (fs.exists(previousCommittedTaskPath)) {\n-      if(fs.exists(committedTaskPath)) {\n-        if(!fs.delete(committedTaskPath, true)) {\n-          throw new IOException(\"Could not delete \"+committedTaskPath);\n-        }\n+    if(hasOutputPath()) {\n+      context.progress();\n+      TaskAttemptID attemptId = context.getTaskAttemptID();\n+      int previousAttempt = getAppAttemptId(context) - 1;\n+      if (previousAttempt < 0) {\n+        throw new IOException (\"Cannot recover task output for first attempt...\");\n       }\n-      //Rename can fail if the parent directory does not yet exist.\n-      Path committedParent = committedTaskPath.getParent();\n-      fs.mkdirs(committedParent);\n-      if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n-        throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n-            \" to \" + committedTaskPath);\n+\n+      Path committedTaskPath = getCommittedTaskPath(context);\n+      Path previousCommittedTaskPath = getCommittedTaskPath(\n+          previousAttempt, context);\n+      FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n+\n+      LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n+          + \" into \" + committedTaskPath);\n+      if (fs.exists(previousCommittedTaskPath)) {\n+        if(fs.exists(committedTaskPath)) {\n+          if(!fs.delete(committedTaskPath, true)) {\n+            throw new IOException(\"Could not delete \"+committedTaskPath);\n+          }\n+        }\n+        //Rename can fail if the parent directory does not yet exist.\n+        Path committedParent = committedTaskPath.getParent();\n+        fs.mkdirs(committedParent);\n+        if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n+          throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n+              \" to \" + committedTaskPath);\n+        }\n+        LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n+      } else {\n+        LOG.warn(attemptId+\" had no output to recover.\");\n       }\n-      LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n     } else {\n-      LOG.warn(attemptId+\" had no output to recover.\");\n+      LOG.warn(\"Output Path is null in recoverTask()\");\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "sha": "7bad09f303977f66b49e7aad180c0e1d4268703e",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java?ref=8ea3e22086f1fac73ba23285348a2f019df0ec92",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "patch": "@@ -104,7 +104,9 @@ public void testRecovery() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     Path jobTempDir1 = committer.getCommittedTaskPath(tContext);\n     File jtd1 = new File(jobTempDir1.toUri().getPath());\n     assertTrue(jtd1.exists());\n@@ -188,7 +190,9 @@ public void testCommitter() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n@@ -214,14 +218,38 @@ public void testMapFileOutputCommitter() throws Exception {\n     writeMapFileOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n     validateMapFileOutputContent(FileSystem.get(conf), outDir);\n     FileUtil.fullyDelete(new File(outDir.toString()));\n   }\n   \n+  public void testMapOnlyNoOutput() throws Exception {\n+    JobConf conf = new JobConf();\n+    //This is not set on purpose. FileOutputFormat.setOutputPath(conf, outDir);\n+    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\n+    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n+    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n+    FileOutputCommitter committer = new FileOutputCommitter();    \n+    \n+    // setup\n+    committer.setupJob(jContext);\n+    committer.setupTask(tContext);\n+    \n+    if(committer.needsTaskCommit(tContext)) {\n+      // do commit\n+      committer.commitTask(tContext);\n+    }\n+    committer.commitJob(jContext);\n+\n+    // validate output\n+    FileUtil.fullyDelete(new File(outDir.toString()));\n+  }\n+  \n   public void testAbort() throws IOException, InterruptedException {\n     JobConf conf = new JobConf();\n     FileOutputFormat.setOutputPath(conf, outDir);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/8ea3e22086f1fac73ba23285348a2f019df0ec92/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "sha": "0859571d1f2bde79064403fd1d5023d368f1826d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps but no reduces. Contributed by Robert Joseph Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241217 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/70c5e8dc338e0dbf00d5fe2f9aa99d0d9ac5b8f9",
        "patched_files": [
            "FileOutputCommitter.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileOutputCommitter.java"
        ]
    },
    "hadoop-common_90dbbfc": {
        "bug_id": "hadoop-common_90dbbfc",
        "commit": "https://github.com/apache/hadoop-common/commit/90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -481,6 +481,8 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4237. TestNodeStatusUpdater can fail if localhost has a domain\n     associated with it (bobby)\n \n+    MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "ff4664675d44bb97f2aff87288a3d2c7127d3307",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "patch": "@@ -21,22 +21,28 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.junit.AfterClass;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import static org.mockito.Mockito.*;\n \n public class TestRMNMInfo {\n   private static final Log LOG = LogFactory.getLog(TestRMNMInfo.class);\n@@ -116,14 +122,47 @@ public void testRMNMInfo() throws Exception {\n               n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n       Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n       Assert.assertNotNull(n.get(\"HealthReport\"));\n-      Assert.assertNotNull(n.get(\"NumContainersMB\"));\n+      Assert.assertNotNull(n.get(\"NumContainers\"));\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected number of used containers\",\n-              0, n.get(\"NumContainersMB\").getValueAsInt());\n+              0, n.get(\"NumContainers\").getValueAsInt());\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected amount of used memory\",\n               0, n.get(\"UsedMemoryMB\").getValueAsInt());\n       Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\n     }\n   }\n+  \n+  @Test\n+  public void testRMNMInfoMissmatch() throws Exception {\n+    RMContext rmc = mock(RMContext.class);\n+    ResourceScheduler rms = mock(ResourceScheduler.class);\n+    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\n+    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\n+    map.put(node.getNodeID(), node);\n+    when(rmc.getRMNodes()).thenReturn(map);\n+    \n+    RMNMInfo rmInfo = new RMNMInfo(rmc,rms);\n+    String liveNMs = rmInfo.getLiveNodeManagers();\n+    ObjectMapper mapper = new ObjectMapper();\n+    JsonNode jn = mapper.readTree(liveNMs);\n+    Assert.assertEquals(\"Unexpected number of live nodes:\",\n+                                               1, jn.size());\n+    Iterator<JsonNode> it = jn.iterator();\n+    while (it.hasNext()) {\n+      JsonNode n = it.next();\n+      Assert.assertNotNull(n.get(\"HostName\"));\n+      Assert.assertNotNull(n.get(\"Rack\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\",\n+              n.get(\"State\").getValueAsText().contains(\"RUNNING\"));\n+      Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be Healthy\",\n+              n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n+      Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n+      Assert.assertNotNull(n.get(\"HealthReport\"));\n+      Assert.assertNull(n.get(\"NumContainers\"));\n+      Assert.assertNull(n.get(\"UsedMemoryMB\"));\n+      Assert.assertNull(n.get(\"AvailableMemoryMB\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "sha": "4ee485644d91ad5b1fcffde9846301c36d237353",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "patch": "@@ -93,10 +93,12 @@ public String getLiveNodeManagers() {\n                         ni.getNodeHealthStatus().getLastHealthReportTime());\n         info.put(\"HealthReport\",\n                         ni.getNodeHealthStatus().getHealthReport());\n-        info.put(\"NumContainersMB\", report.getNumContainers());\n-        info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n-        info.put(\"AvailableMemoryMB\",\n-                                report.getAvailableResource().getMemory());\n+        if(report != null) {\n+          info.put(\"NumContainers\", report.getNumContainers());\n+          info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n+          info.put(\"AvailableMemoryMB\",\n+              report.getAvailableResource().getMemory());\n+        }\n \n         nodesInfo.add(info);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "sha": "0db42e40ec0e08d72413048956baf0393062157d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1337363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6135cf06692f8691f49a57704827b3bb729ae8bd",
        "patched_files": [
            "CHANGES.txt",
            "RMNMInfo.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMNMInfo.java"
        ]
    },
    "hadoop-common_961cb63": {
        "bug_id": "hadoop-common_961cb63",
        "commit": "https://github.com/apache/hadoop-common/commit/961cb63458fb22933081df5d316dd3e9d10c1bde",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/961cb63458fb22933081df5d316dd3e9d10c1bde/hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hdfs/CHANGES.txt?ref=961cb63458fb22933081df5d316dd3e9d10c1bde",
                "deletions": 0,
                "filename": "hdfs/CHANGES.txt",
                "patch": "@@ -951,6 +951,9 @@ Trunk (unreleased changes)\n     HDFS-2196. Make ant build system work with hadoop-common JAR generated\n     by Maven. (Alejandro Abdelnur via tomwhite)\n \n+    HDFS-2245. Fix a NullPointerException in BlockManager.chooseTarget(..).\n+    (szetszwo)\n+\n   BREAKDOWN OF HDFS-1073 SUBTASKS\n \n     HDFS-1521. Persist transaction ID on disk between NN restarts.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/961cb63458fb22933081df5d316dd3e9d10c1bde/hdfs/CHANGES.txt",
                "sha": "ff0ba15b050cb1f618d6bcf9d6b0348278b347a8",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/961cb63458fb22933081df5d316dd3e9d10c1bde/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=961cb63458fb22933081df5d316dd3e9d10c1bde",
                "deletions": 6,
                "filename": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1221,12 +1221,13 @@ private boolean computeReplicationWorkForBlock(Block block, int priority) {\n     final DatanodeDescriptor targets[] = blockplacement.chooseTarget(\n         src, numOfReplicas, client, excludedNodes, blocksize);\n     if (targets.length < minReplication) {\n-      throw new IOException(\"File \" + src + \" could only be replicated to \" +\n-                            targets.length + \" nodes, instead of \" +\n-                            minReplication + \". There are \"\n-                            + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n-                            + \" datanode(s) running but \"+excludedNodes.size() +\n-                            \" node(s) are excluded in this operation.\");\n+      throw new IOException(\"File \" + src + \" could only be replicated to \"\n+          + targets.length + \" nodes instead of minReplication (=\"\n+          + minReplication + \").  There are \"\n+          + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n+          + \" datanode(s) running and \"\n+          + (excludedNodes == null? \"no\": excludedNodes.size())\n+          + \" node(s) are excluded in this operation.\");\n     }\n     return targets;\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/961cb63458fb22933081df5d316dd3e9d10c1bde/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "f60530b4cc40b36c154b06a7d740b48216925864",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop-common/blob/961cb63458fb22933081df5d316dd3e9d10c1bde/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java?ref=961cb63458fb22933081df5d316dd3e9d10c1bde",
                "deletions": 0,
                "filename": "hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java",
                "patch": "@@ -404,6 +404,36 @@ public void testFileCreationError2() throws IOException {\n     }\n   }\n \n+  /** test addBlock(..) when replication<min and excludeNodes==null. */\n+  public void testFileCreationError3() throws IOException {\n+    System.out.println(\"testFileCreationError3 start\");\n+    Configuration conf = new HdfsConfiguration();\n+    // create cluster\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+    DistributedFileSystem dfs = null;\n+    try {\n+      cluster.waitActive();\n+      dfs = (DistributedFileSystem)cluster.getFileSystem();\n+      DFSClient client = dfs.dfs;\n+\n+      // create a new file.\n+      final Path f = new Path(\"/foo.txt\");\n+      createFile(dfs, f, 3);\n+      try {\n+        cluster.getNameNode().addBlock(f.toString(), \n+            client.clientName, null, null);\n+        fail();\n+      } catch(IOException ioe) {\n+        FileSystem.LOG.info(\"GOOD!\", ioe);\n+      }\n+\n+      System.out.println(\"testFileCreationError3 successful\");\n+    } finally {\n+      IOUtils.closeStream(dfs);\n+      cluster.shutdown();\n+    }\n+  }\n+\n   /**\n    * Test that file leases are persisted across namenode restarts.\n    * This test is currently not triggered because more HDFS work is ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/961cb63458fb22933081df5d316dd3e9d10c1bde/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java",
                "sha": "d2dfd7fc65e9aa9599969a6370322e602f9de383",
                "status": "modified"
            }
        ],
        "message": "HDFS-2245. Fix a NullPointerException in BlockManager.chooseTarget(..).\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156490 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/a9dba6f9a9449902e6773549b52537027d9c323e",
        "patched_files": [
            "BlockManager.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileCreation.java"
        ]
    },
    "hadoop-common_98d731e": {
        "bug_id": "hadoop-common_98d731e",
        "commit": "https://github.com/apache/hadoop-common/commit/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -49,3 +49,5 @@ IMPROVEMENTS:\n     HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)\n \n     HDFS-5417. Fix storage IDs in PBHelper and UpgradeUtilities.  (szetszwo)\n+\n+    HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "d878b66b3032b4dcaa4bfc1dbe688bbd82fcf1d9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1833,7 +1833,10 @@ private void reportDiff(DatanodeDescriptor dn, DatanodeStorage storage,\n       ReplicaState iState = itBR.getCurrentReplicaState();\n       BlockInfo storedBlock = processReportedBlock(dn, storage.getStorageID(),\n           iblk, iState, toAdd, toInvalidate, toCorrupt, toUC);\n-      toRemove.remove(storedBlock);\n+\n+      if (storedBlock != null) {\n+        toRemove.remove(storedBlock);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "6d5c604ba7dbc631855003ffb675575f3d842d5d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "patch": "@@ -187,7 +187,7 @@ public LinkedElement getNext() {\n         + hours + \" hours for block pool \" + bpid);\n \n     // get the list of blocks and arrange them in random order\n-    List<Block> arr = dataset.getFinalizedBlocks(blockPoolId);\n+    List<FinalizedReplica> arr = dataset.getFinalizedBlocks(blockPoolId);\n     Collections.shuffle(arr);\n     \n     long scanTime = -1;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "sha": "13a83bce5fdc7a701bba62f192f37f588df07ead",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "patch": "@@ -230,10 +230,6 @@ private static String getSuffix(File f, String prefix) {\n       throw new RuntimeException(prefix + \" is not a prefix of \" + fullPath);\n     }\n \n-    ScanInfo(long blockId) {\n-      this(blockId, null, null, null);\n-    }\n-\n     ScanInfo(long blockId, File blockFile, File metaFile, FsVolumeSpi vol) {\n       this.blockId = blockId;\n       String condensedVolPath = vol == null ? null :\n@@ -439,8 +435,8 @@ void scan() {\n         diffs.put(bpid, diffRecord);\n         \n         statsRecord.totalBlocks = blockpoolReport.length;\n-        List<Block> bl = dataset.getFinalizedBlocks(bpid);\n-        Block[] memReport = bl.toArray(new Block[bl.size()]);\n+        List<FinalizedReplica> bl = dataset.getFinalizedBlocks(bpid);\n+        FinalizedReplica[] memReport = bl.toArray(new FinalizedReplica[bl.size()]);\n         Arrays.sort(memReport); // Sort based on blockId\n   \n         int d = 0; // index for blockpoolReport\n@@ -458,7 +454,8 @@ void scan() {\n           }\n           if (info.getBlockId() > memBlock.getBlockId()) {\n             // Block is missing on the disk\n-            addDifference(diffRecord, statsRecord, memBlock.getBlockId());\n+            addDifference(diffRecord, statsRecord,\n+                          memBlock.getBlockId(), info.getVolume());\n             m++;\n             continue;\n           }\n@@ -478,7 +475,9 @@ void scan() {\n           m++;\n         }\n         while (m < memReport.length) {\n-          addDifference(diffRecord, statsRecord, memReport[m++].getBlockId());\n+          FinalizedReplica current = memReport[m++];\n+          addDifference(diffRecord, statsRecord,\n+                        current.getBlockId(), current.getVolume());\n         }\n         while (d < blockpoolReport.length) {\n           statsRecord.missingMemoryBlocks++;\n@@ -502,10 +501,11 @@ private void addDifference(LinkedList<ScanInfo> diffRecord,\n \n   /** Block is not found on the disk */\n   private void addDifference(LinkedList<ScanInfo> diffRecord,\n-                             Stats statsRecord, long blockId) {\n+                             Stats statsRecord, long blockId,\n+                             FsVolumeSpi vol) {\n     statsRecord.missingBlockFile++;\n     statsRecord.missingMetaFile++;\n-    diffRecord.add(new ScanInfo(blockId));\n+    diffRecord.add(new ScanInfo(blockId, null, null, vol));\n   }\n \n   /** Is the given volume still valid in the dataset? */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "sha": "17ec35d6fb670e13c585e456bbdfafc917a8d991",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "patch": "@@ -61,6 +61,10 @@ public FinalizedReplica(FinalizedReplica from) {\n     this.unlinked = from.isUnlinked();\n   }\n \n+  public FinalizedReplica(ReplicaInfo replicaInfo) {\n+    super(replicaInfo);\n+  }\n+\n   @Override  // ReplicaInfo\n   public ReplicaState getState() {\n     return ReplicaState.FINALIZED;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "sha": "1a852c346689ca36638e027e966dcc8ae1b080c2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hdfs.protocol.HdfsBlocksMetadata;\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataStorage;\n+import org.apache.hadoop.hdfs.server.datanode.FinalizedReplica;\n import org.apache.hadoop.hdfs.server.datanode.Replica;\n import org.apache.hadoop.hdfs.server.datanode.ReplicaInPipelineInterface;\n import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory;\n@@ -98,7 +99,7 @@ public RollingLogs createRollingLogs(String bpid, String prefix\n   public Map<String, Object> getVolumeInfoMap();\n \n   /** @return a list of finalized blocks for the given block pool. */\n-  public List<Block> getFinalizedBlocks(String bpid);\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid);\n \n   /**\n    * Check whether the in-memory block record matches the block on the disk,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "sha": "90edd5104ffbf33263006a7c4a17a07433bb74f4",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -1079,11 +1079,12 @@ public BlockListAsLongs getBlockReport(String bpid) {\n    * Get the list of finalized blocks from in-memory blockmap for a block pool.\n    */\n   @Override\n-  public synchronized List<Block> getFinalizedBlocks(String bpid) {\n-    ArrayList<Block> finalized = new ArrayList<Block>(volumeMap.size(bpid));\n+  public synchronized List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n+    ArrayList<FinalizedReplica> finalized =\n+        new ArrayList<FinalizedReplica>(volumeMap.size(bpid));\n     for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n       if(b.getState() == ReplicaState.FINALIZED) {\n-        finalized.add(new Block(b));\n+        finalized.add(new FinalizedReplica(b));\n       }\n     }\n     return finalized;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "8677131d4abbe7ffcae4aa85738d8991ce486dcc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -1006,7 +1006,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n   }\n \n   @Override\n-  public List<Block> getFinalizedBlocks(String bpid) {\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n     throw new UnsupportedOperationException();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "6f3bed9fda04b728c1665a972ff34b09696110fd",
                "status": "modified"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "changes": 163,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 48,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "patch": "@@ -17,14 +17,17 @@\n  */\n package org.apache.hadoop.hdfs.server.datanode;\n \n+import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n import java.io.File;\n import java.io.FilenameFilter;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Map;\n import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeoutException;\n@@ -89,7 +92,7 @@\n   private MiniDFSCluster cluster;\n   private DistributedFileSystem fs;\n \n-  Random rand = new Random(RAND_LIMIT);\n+  private static Random rand = new Random(RAND_LIMIT);\n \n   private static Configuration conf;\n \n@@ -113,6 +116,57 @@ public void shutDownCluster() throws IOException {\n     cluster.shutdown();\n   }\n \n+  private static StorageBlockReport[] getBlockReports(DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n+  // Get block reports but modify the GS of one of the blocks.\n+  private static StorageBlockReport[] getBlockReportsCorruptSingleBlockGS(\n+      DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    boolean corruptedBlock = false;\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      if (!corruptedBlock) {\n+        blockList[4] = rand.nextInt();      // Bad GS.\n+        corruptedBlock = true;\n+      }\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n   /**\n    * Test write a file, verifies and closes it. Then the length of the blocks\n    * are messed up and BlockReport is forced.\n@@ -153,10 +207,8 @@ public void blockReport_01() throws IOException {\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     List<LocatedBlock> blocksAfterReport =\n       DFSTestUtil.getAllBlocks(fs.open(filePath));\n@@ -211,7 +263,6 @@ public void blockReport_02() throws IOException {\n     for (Integer aRemovedIndex : removedIndex) {\n       blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());\n     }\n-    ArrayList<Block> blocks = locatedToBlocks(lBlocks, removedIndex);\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Number of blocks allocated \" + lBlocks.size());\n@@ -225,8 +276,11 @@ public void blockReport_02() throws IOException {\n       for (File f : findAllFiles(dataDir,\n         new MyFileFilter(b.getBlockName(), true))) {\n         DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);\n-        if (!f.delete())\n+        if (!f.delete()) {\n           LOG.warn(\"Couldn't delete \" + b.getBlockName());\n+        } else {\n+          LOG.debug(\"Deleted file \" + f.toString());\n+        }\n       }\n     }\n \n@@ -235,10 +289,8 @@ public void blockReport_02() throws IOException {\n     // all blocks belong to the same file, hence same BP\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn0.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn0, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     BlockManagerTestUtil.getComputedDatanodeWork(cluster.getNamesystem()\n         .getBlockManager());\n@@ -253,9 +305,8 @@ public void blockReport_02() throws IOException {\n \n \n   /**\n-   * Test writes a file and closes it. Then test finds a block\n-   * and changes its GS to be < of original one.\n-   * New empty block is added to the list of blocks.\n+   * Test writes a file and closes it.\n+   * Block reported is generated with a bad GS for a single block.\n    * Block report is forced and the check for # of corrupted blocks is performed.\n    *\n    * @throws IOException in case of an error\n@@ -264,41 +315,65 @@ public void blockReport_02() throws IOException {\n   public void blockReport_03() throws IOException {\n     final String METHOD_NAME = GenericTestUtils.getMethodName();\n     Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n-\n-    ArrayList<Block> blocks =\n-      prepareForRide(filePath, METHOD_NAME, FILE_SIZE);\n-\n-    // The block with modified GS won't be found. Has to be deleted\n-    blocks.get(0).setGenerationStamp(rand.nextLong());\n-    // This new block is unknown to NN and will be mark for deletion.\n-    blocks.add(new Block());\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n     \n     // all blocks belong to the same file, hence same BP\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] reports = getBlockReportsCorruptSingleBlockGS(dn, poolId);\n     DatanodeCommand dnCmd =\n-      cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+      cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Got the command: \" + dnCmd);\n     }\n     printStats();\n \n-    assertEquals(\"Wrong number of CorruptedReplica+PendingDeletion \" +\n-      \"blocks is found\", 2,\n-        cluster.getNamesystem().getCorruptReplicaBlocks() +\n-        cluster.getNamesystem().getPendingDeletionBlocks());\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(1L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(0L));\n   }\n \n   /**\n-   * This test isn't a representative case for BlockReport\n-   * The empty method is going to be left here to keep the naming\n-   * of the test plan in synch with the actual implementation\n+   * Test writes a file and closes it.\n+   * Block reported is generated with an extra block.\n+   * Block report is forced and the check for # of pendingdeletion\n+   * blocks is performed.\n+   *\n+   * @throws IOException in case of an error\n    */\n-  public void blockReport_04() {\n+  @Test\n+  public void blockReport_04() throws IOException {\n+    final String METHOD_NAME = GenericTestUtils.getMethodName();\n+    Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n+\n+\n+    DataNode dn = cluster.getDataNodes().get(DN_N0);\n+    // all blocks belong to the same file, hence same BP\n+    String poolId = cluster.getNamesystem().getBlockPoolId();\n+\n+    // Create a bogus new block which will not be present on the namenode.\n+    ExtendedBlock b = new ExtendedBlock(\n+        poolId, rand.nextLong(), 1024L, rand.nextLong());\n+    dn.getFSDataset().createRbw(b);\n+\n+    DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    DatanodeCommand dnCmd =\n+        cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Got the command: \" + dnCmd);\n+    }\n+    printStats();\n+\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(0L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(1L));\n   }\n \n   // Client requests new block from NN. The test corrupts this very block\n@@ -331,10 +406,8 @@ public void blockReport_06() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     printStats();\n     assertEquals(\"Wrong number of PendingReplication Blocks\",\n       0, cluster.getNamesystem().getUnderReplicatedBlocks());\n@@ -382,9 +455,7 @@ public void blockReport_07() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] report = getBlockReports(dn, poolId);\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n     assertEquals(\"Wrong number of Corrupted blocks\",\n@@ -407,7 +478,7 @@ public void blockReport_07() throws Exception {\n     }\n     \n     report[0] = new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n+        report[0].getStorage(),\n         new BlockListAsLongs(blocks, null).getBlockListAsLongs());\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n@@ -458,9 +529,7 @@ public void blockReport_08() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",\n@@ -506,9 +575,7 @@ public void blockReport_09() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "sha": "21d0339888a6729d0c75a8febe0d7c9413df252c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java?ref=98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "patch": "@@ -447,7 +447,7 @@ void testScanInfoObject(long blockId, File blockFile, File metaFile)\n   \n   void testScanInfoObject(long blockId) throws Exception {\n     DirectoryScanner.ScanInfo scanInfo =\n-        new DirectoryScanner.ScanInfo(blockId);\n+        new DirectoryScanner.ScanInfo(blockId, null, null, null);\n     assertEquals(blockId, scanInfo.getBlockId());\n     assertNull(scanInfo.getBlockFile());\n     assertNull(scanInfo.getMetaFile());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/98d731efce5c7cbda7f0a6a97adf5f3242d2e8b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "sha": "f5b535d394363b27012b10a4b62bf1d7de7481d1",
                "status": "modified"
            }
        ],
        "message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6ab218735f33260a7d68ab449461748acc3dbf4d",
        "patched_files": [
            "SimulatedFSDataset.java",
            "FsDatasetSpi.java",
            "BlockPoolSliceScanner.java",
            "CHANGES_HDFS-2832.txt",
            "BlockManager.java",
            "FinalizedReplica.java",
            "DirectoryScanner.java",
            "FsDatasetImpl.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java",
            "TestSimulatedFSDataset.java",
            "TestDirectoryScanner.java",
            "TestBlockReport.java"
        ]
    },
    "hadoop-common_9b88fa7": {
        "bug_id": "hadoop-common_9b88fa7",
        "commit": "https://github.com/apache/hadoop-common/commit/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -180,3 +180,5 @@ HDFS-2733. Document HA configuration and CLI. (atm)\n HDFS-2794. Active NN may purge edit log files before standby NN has a chance to read them (todd)\n \n HDFS-2901. Improvements for SBN web UI - not show under-replicated/missing blocks. (Brandon Li via jitendra)\n+\n+HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. (Bikas Saha via jitendra)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "36c162482b0eb0bf27c12881b58dd11c50b67e73",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "patch": "@@ -135,8 +135,7 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n    */\n   List<RemoteEditLog> getRemoteEditLogs(long firstTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(\n-        FileUtil.listFiles(currentDir));\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<RemoteEditLog> ret = Lists.newArrayListWithCapacity(\n         allLogFiles.size());\n \n@@ -155,6 +154,20 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n     return ret;\n   }\n \n+  /**\n+   * returns matching edit logs via the log directory. Simple helper function\n+   * that lists the files in the logDir and calls matchEditLogs(File[])\n+   * \n+   * @param logDir\n+   *          directory to match edit logs in\n+   * @return matched edit logs\n+   * @throws IOException\n+   *           IOException thrown for invalid logDir\n+   */\n+  static List<EditLogFile> matchEditLogs(File logDir) throws IOException {\n+    return matchEditLogs(FileUtil.listFiles(logDir));\n+  }\n+  \n   static List<EditLogFile> matchEditLogs(File[] filesInStorage) {\n     List<EditLogFile> ret = Lists.newArrayList();\n     for (File f : filesInStorage) {\n@@ -278,7 +291,7 @@ public long getNumberOfTransactions(long fromTxId, boolean inProgressOk)\n   synchronized public void recoverUnfinalizedSegments() throws IOException {\n     File currentDir = sd.getCurrentDir();\n     LOG.info(\"Recovering unfinalized segments in \" + currentDir);\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n \n     for (EditLogFile elf : allLogFiles) {\n       if (elf.getFile().equals(currentInProgress)) {\n@@ -318,7 +331,7 @@ synchronized public void recoverUnfinalizedSegments() throws IOException {\n \n   private List<EditLogFile> getLogFiles(long fromTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<EditLogFile> logFiles = Lists.newArrayList();\n     \n     for (EditLogFile elf : allLogFiles) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "sha": "1eca2797b44c09be1830b59be2c639d7f061435b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "patch": "@@ -440,7 +440,7 @@ public static EditLogFile findLatestEditsLog(StorageDirectory sd)\n   throws IOException {\n     File currentDir = sd.getCurrentDir();\n     List<EditLogFile> foundEditLogs \n-      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir.listFiles()));\n+      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir));\n     return Collections.max(foundEditLogs, EditLogFile.COMPARE_BY_START_TXID);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "sha": "665e088cb800bd95af4a5a4771d5f5b280e188b6",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java?ref=9b88fa7a60778cb08f5c0b41f49de5bb166e3c78",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "patch": "@@ -315,6 +315,15 @@ public void testGetRemoteEditLog() throws IOException {\n         \"\", getLogsAsString(fjm, 9999));\n   }\n \n+  /**\n+   * tests that passing an invalid dir to matchEditLogs throws IOException \n+   */\n+  @Test(expected = IOException.class)\n+  public void testMatchEditLogInvalidDirThrowsIOException() throws IOException {\n+    File badDir = new File(\"does not exist\");\n+    FileJournalManager.matchEditLogs(badDir);\n+  }\n+  \n   /**\n    * Make sure that we starting reading the correct op when we request a stream\n    * with a txid in the middle of an edit log file.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9b88fa7a60778cb08f5c0b41f49de5bb166e3c78/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "sha": "def293657768e434d587492e65dea58631153e45",
                "status": "modified"
            }
        ],
        "message": "HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. Contributed by Bikas Saha.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1241757 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d8b3427ae26e8105a0fc87b56a65908d7122c284",
        "patched_files": [
            "CHANGES.HDFS-1623.txt",
            "FileJournalManager.java",
            "FSImageTestUtil.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileJournalManager.java"
        ]
    },
    "hadoop-common_9c08b79": {
        "bug_id": "hadoop-common_9c08b79",
        "commit": "https://github.com/apache/hadoop-common/commit/9c08b79db0aa747720b5794ad67cba9187d29371",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -960,3 +960,7 @@ Release 0.21.0 - Unreleased\n \n     MAPREDUCE-1161. Remove ineffective synchronization in NotificationTestCase.\n     (Owen O'Malley via cdouglas)\n+\n+    MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent\n+    queue. (V.V.Chaitanya Krishna via yhemanth)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/CHANGES.txt",
                "sha": "385924c8cfabac25911a0372a8c58acbc643c1df",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -1001,8 +1001,12 @@ public Path getSystemDir() {\n   \n   public JobStatus[] getJobsFromQueue(String queueName) throws IOException {\n     try {\n+      QueueInfo queue = cluster.getQueue(queueName);\n+      if (queue == null) {\n+        return null;\n+      }\n       org.apache.hadoop.mapreduce.JobStatus[] stats = \n-        cluster.getQueue(queueName).getJobStatuses();\n+        queue.getJobStatuses();\n       JobStatus[] ret = new JobStatus[stats.length];\n       for (int i = 0 ; i < stats.length; i++ ) {\n         ret[i] = JobStatus.downgrade(stats[i]);\n@@ -1022,7 +1026,11 @@ public Path getSystemDir() {\n    */\n   public JobQueueInfo getQueueInfo(String queueName) throws IOException {\n     try {\n-      return new JobQueueInfo(cluster.getQueue(queueName));\n+      QueueInfo queueInfo = cluster.getQueue(queueName);\n+      if (queueInfo != null) {\n+        return new JobQueueInfo(queueInfo);\n+      }\n+      return null;\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "3ec640566492e9af0e17a0d348b52d43e1617fe4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobQueueClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "patch": "@@ -175,6 +175,11 @@ private void displayQueueList() throws IOException {\n   private void displayQueueInfo(String queue, boolean showJobs)\n       throws IOException {\n     JobQueueInfo jobQueueInfo = jc.getQueueInfo(queue);\n+    \n+    if (jobQueueInfo == null) {\n+      System.out.println(\"Queue \\\"\" + queue + \"\\\" does not exist.\");\n+      return;\n+    }\n     printJobQueueInfo(jobQueueInfo, new PrintWriter(System.out));\n     if (showJobs && (jobQueueInfo.getChildren() == null ||\n         jobQueueInfo.getChildren().size() == 0)) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobQueueClient.java",
                "sha": "129b175966e1fda1d229e1117f2ed5c64b8411ab",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "patch": "@@ -3966,7 +3966,9 @@ public JobQueueInfo getQueueInfo(String queue) throws IOException {\n   @Override\n   public QueueInfo getQueue(String queue) throws IOException {\n     JobQueueInfo jqueue = queueManager.getJobQueueInfo(queue);\n-    jqueue.setJobStatuses(getJobsFromQueue(jqueue.getQueueName()));\n+    if (jqueue != null) {\n+      jqueue.setJobStatuses(getJobsFromQueue(jqueue.getQueueName()));\n+    }\n     return jqueue;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "sha": "6686c97fb3dede682ea457b0a2b577795a5746d2",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9c08b79db0aa747720b5794ad67cba9187d29371/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java?ref=9c08b79db0aa747720b5794ad67cba9187d29371",
                "deletions": 4,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "patch": "@@ -17,20 +17,33 @@\n  */\n package org.apache.hadoop.mapred;\n \n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.CONFIG;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.checkForConfigFile;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.createDocument;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.createSimpleDocumentWithAcls;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.miniMRCluster;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.setUpCluster;\n+import static org.apache.hadoop.mapred.QueueManagerTestUtils.writeToFile;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.File;\n+import java.io.IOException;\n import java.io.StringWriter;\n import java.util.ArrayList;\n import java.util.List;\n \n-import org.junit.After;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.QueueInfo;\n import org.junit.Test;\n+import org.w3c.dom.Document;\n \n public class TestJobQueueClient {\n   @Test\n   public void testQueueOrdering() throws Exception {\n-    System.out.println(\"in test queue ordering\");\n     // create some sample queues in a hierarchy..\n     JobQueueInfo[] roots = new JobQueueInfo[2];\n     roots[0] = new JobQueueInfo(\"q1\", \"q1 scheduling info\");\n@@ -53,7 +66,6 @@ public void testQueueOrdering() throws Exception {\n   \n   @Test\n   public void testQueueInfoPrinting() throws Exception {\n-    System.out.println(\"in test queue info printing\");\n     // create a test queue with children.\n     // create some sample queues in a hierarchy..\n     JobQueueInfo root = new JobQueueInfo(\"q1\", \"q1 scheduling info\");\n@@ -76,4 +88,24 @@ public void testQueueInfoPrinting() throws Exception {\n     \n     assertEquals(sb.toString(), writer.toString());\n   }\n-}\n\\ No newline at end of file\n+  \n+  @Test\n+  public void testGetQueue() throws Exception {\n+    checkForConfigFile();\n+    Document doc = createDocument();\n+    createSimpleDocumentWithAcls(doc, \"true\");\n+    writeToFile(doc, CONFIG);\n+    Configuration conf = new Configuration();\n+    conf.addResource(CONFIG);\n+    setUpCluster(conf);\n+    JobClient jc = new JobClient(miniMRCluster.createJobConf());\n+    // test for existing queue\n+    QueueInfo queueInfo = jc.getQueueInfo(\"q1\");\n+    assertEquals(\"q1\",queueInfo.getQueueName());\n+    // try getting a non-existing queue\n+    queueInfo = jc.getQueueInfo(\"queue\");\n+    assertNull(queueInfo);\n+\n+    new File(CONFIG).delete();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9c08b79db0aa747720b5794ad67cba9187d29371/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java",
                "sha": "94735fe317e52351eb3e6335c25534af225b676b",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent queue. Contributed by V.V.Chaitanya Krishna.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@888257 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ddc67bf61e875e82b47ffb998172f5062791586d",
        "patched_files": [
            "JobClient.java",
            "JobQueueClient.java",
            "JobTracker.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobClient.java",
            "TestJobQueueClient.java"
        ]
    },
    "hadoop-common_9d9e32e": {
        "bug_id": "hadoop-common_9d9e32e",
        "commit": "https://github.com/apache/hadoop-common/commit/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -305,6 +305,9 @@ Branch-2 ( Unreleased changes )\n \n     HDFS-3243. TestParallelRead timing out on jenkins. (Henry Robinson via todd)\n \n+    HDFS-3490. DatanodeWebHdfsMethods throws NullPointerException if\n+    NamenodeRpcAddressParam is not set.  (szetszwo)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "0277f829a0478b82dbc2d4ce236e62a881cc7f94",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java?ref=9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "patch": "@@ -98,6 +98,10 @@ private void init(final UserGroupInformation ugi,\n       LOG.trace(\"HTTP \" + op.getValue().getType() + \": \" + op + \", \" + path\n           + \", ugi=\" + ugi + Param.toSortedString(\", \", parameters));\n     }\n+    if (nnRpcAddr == null) {\n+      throw new IllegalArgumentException(NamenodeRpcAddressParam.NAME\n+          + \" is not specified.\");\n+    }\n \n     //clear content type\n     response.setContentType(null);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "sha": "eb4afe75f85be5949890a6fdee9882413be3dbc0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java?ref=9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "patch": "@@ -123,7 +123,7 @@ private void init(final UserGroupInformation ugi,\n       final DelegationParam delegation,\n       final UserParam username, final DoAsParam doAsUser,\n       final UriFsPathParam path, final HttpOpParam<?> op,\n-      final Param<?, ?>... parameters) throws IOException {\n+      final Param<?, ?>... parameters) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"HTTP \" + op.getValue().getType() + \": \" + op + \", \" + path\n           + \", ugi=\" + ugi + \", \" + username + \", \" + doAsUser\n@@ -532,7 +532,7 @@ public Response getRoot(\n           final RenewerParam renewer,\n       @QueryParam(BufferSizeParam.NAME) @DefaultValue(BufferSizeParam.DEFAULT)\n           final BufferSizeParam bufferSize\n-      ) throws IOException, URISyntaxException, InterruptedException {\n+      ) throws IOException, InterruptedException {\n     return get(ugi, delegation, username, doAsUser, ROOT, op,\n         offset, length, renewer, bufferSize);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "sha": "de8f256705b2f3d1f3277af13ad27a997b6b5a64",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java?ref=9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java",
                "patch": "@@ -44,6 +44,10 @@ public String getDomain() {\n \n     @Override\n     InetSocketAddress parse(final String str) {\n+      if (str == null) {\n+        throw new IllegalArgumentException(\"The input string is null: expect \"\n+            + getDomain());\n+      }\n       final int i = str.indexOf(':');\n       if (i < 0) {\n         throw new IllegalArgumentException(\"Failed to parse \\\"\" + str",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java",
                "sha": "9879ba3032c35549c6b84d951727095f04c1d4bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java?ref=9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java",
                "patch": "@@ -59,7 +59,7 @@ public String toString() {\n \n     @Override\n     public String getDomain() {\n-      return \"<\" + NULL + \" | short in radix \" + radix + \">\";\n+      return \"<\" + NULL + \" | long in radix \" + radix + \">\";\n     }\n \n     @Override\n@@ -72,7 +72,7 @@ Long parse(final String str) {\n       }\n     }\n \n-    /** Convert a Short to a String. */ \n+    /** Convert a Long to a String. */ \n     String toString(final Long n) {\n       return n == null? NULL: Long.toString(n, radix);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java",
                "sha": "6f102e1c9f3f7e1d73f74ba30448936b297bd7b4",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop-common/blob/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java?ref=9d9e32ef403ca5ea093fce24b6aac926ebca6c2b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.hdfs.web.resources.DoAsParam;\n import org.apache.hadoop.hdfs.web.resources.GetOpParam;\n import org.apache.hadoop.hdfs.web.resources.HttpOpParam;\n+import org.apache.hadoop.hdfs.web.resources.NamenodeRpcAddressParam;\n import org.apache.hadoop.hdfs.web.resources.PutOpParam;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -351,5 +352,31 @@ public void testResponseCode() throws IOException {\n     {//test append.\n       AppendTestUtil.testAppend(fs, new Path(dir, \"append\"));\n     }\n+\n+    {//test NamenodeRpcAddressParam not set.\n+      final HttpOpParam.Op op = PutOpParam.Op.CREATE;\n+      final URL url = webhdfs.toUrl(op, dir);\n+      HttpURLConnection conn = (HttpURLConnection) url.openConnection();\n+      conn.setRequestMethod(op.getType().toString());\n+      conn.setDoOutput(false);\n+      conn.setInstanceFollowRedirects(false);\n+      conn.connect();\n+      final String redirect = conn.getHeaderField(\"Location\");\n+      conn.disconnect();\n+\n+      //remove NamenodeRpcAddressParam\n+      WebHdfsFileSystem.LOG.info(\"redirect = \" + redirect);\n+      final int i = redirect.indexOf(NamenodeRpcAddressParam.NAME);\n+      final int j = redirect.indexOf(\"&\", i);\n+      String modified = redirect.substring(0, i - 1) + redirect.substring(j);\n+      WebHdfsFileSystem.LOG.info(\"modified = \" + modified);\n+\n+      //connect to datanode\n+      conn = (HttpURLConnection)new URL(modified).openConnection();\n+      conn.setRequestMethod(op.getType().toString());\n+      conn.setDoOutput(op.getDoOutput());\n+      conn.connect();\n+      assertEquals(HttpServletResponse.SC_BAD_REQUEST, conn.getResponseCode());\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/9d9e32ef403ca5ea093fce24b6aac926ebca6c2b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "sha": "61734180cfd66918c98f69d5b2561cc2b4863eeb",
                "status": "modified"
            }
        ],
        "message": "HDFS-3490. DatanodeWebHdfsMethods throws NullPointerException if NamenodeRpcAddressParam is not set. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1348287 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6f3e57b85ca707de66965c379146f9abf1b2a673",
        "patched_files": [
            "DatanodeWebHdfsMethods.java",
            "InetSocketAddressParam.java",
            "NamenodeWebHdfsMethods.java",
            "CHANGES.txt",
            "LongParam.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestLongParam.java",
            "TestWebHdfsFileSystemContract.java"
        ]
    },
    "hadoop-common_a11e94b": {
        "bug_id": "hadoop-common_a11e94b",
        "commit": "https://github.com/apache/hadoop-common/commit/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -244,12 +244,14 @@ Trunk (Unreleased)\n     HDFS-5636. Enforce a max TTL per cache pool. (awang via cmccabe)\n \n   OPTIMIZATIONS\n+\n     HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only. (cmccabe)\n \n     HDFS-5665. Remove the unnecessary writeLock while initializing CacheManager\n     in FsNameSystem Ctor. (Uma Maheswara Rao G via Andrew Wang)\n \n   BUG FIXES\n+\n     HADOOP-9635 Fix potential Stack Overflow in DomainSocket.c (V. Karthik Kumar\n                 via cmccabe)\n \n@@ -456,6 +458,10 @@ Trunk (Unreleased)\n     HDFS-5701. Fix the CacheAdmin -addPool -maxTtl option name.\n     (Stephen Chu via wang)\n \n+    HDFS-5708. The CacheManager throws a NPE in the DataNode logs when\n+    processing cache reports that refer to a block not known to the\n+    BlockManager. (cmccabe via wang)\n+\n   BREAKDOWN OF HDFS-2832 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4985. Add storage type to the protocol and expose it in block report",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "b1bcdf81d36625433c59e105d302386686221198",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java?ref=a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
                "patch": "@@ -460,14 +460,21 @@ private void rescanFile(CacheDirective directive, INodeFile file) {\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal += cachedByBlock;\n \n-        if (mark != ocblock.getMark()) {\n-          // Mark hasn't been set in this scan, so update replication and mark.\n+        if ((mark != ocblock.getMark()) ||\n+            (ocblock.getReplication() < directive.getReplication())) {\n+          //\n+          // Overwrite the block's replication and mark in two cases:\n+          //\n+          // 1. If the mark on the CachedBlock is different from the mark for\n+          // this scan, that means the block hasn't been updated during this\n+          // scan, and we should overwrite whatever is there, since it is no\n+          // longer valid.\n+          //\n+          // 2. If the replication in the CachedBlock is less than what the\n+          // directive asks for, we want to increase the block's replication\n+          // field to what the directive asks for.\n+          //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n-        } else {\n-          // Mark already set in this scan.  Set replication to highest value in\n-          // any CacheDirective that covers this file.\n-          ocblock.setReplicationAndMark((short)Math.max(\n-              directive.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n@@ -483,6 +490,36 @@ private void rescanFile(CacheDirective directive, INodeFile file) {\n     }\n   }\n \n+  private String findReasonForNotCaching(CachedBlock cblock, \n+          BlockInfo blockInfo) {\n+    if (blockInfo == null) {\n+      // Somehow, a cache report with the block arrived, but the block\n+      // reports from the DataNode haven't (yet?) described such a block.\n+      // Alternately, the NameNode might have invalidated the block, but the\n+      // DataNode hasn't caught up.  In any case, we want to tell the DN\n+      // to uncache this.\n+      return \"not tracked by the BlockManager\";\n+    } else if (!blockInfo.isComplete()) {\n+      // When a cached block changes state from complete to some other state\n+      // on the DataNode (perhaps because of append), it will begin the\n+      // uncaching process.  However, the uncaching process is not\n+      // instantaneous, especially if clients have pinned the block.  So\n+      // there may be a period of time when incomplete blocks remain cached\n+      // on the DataNodes.\n+      return \"not complete\";\n+    }  else if (cblock.getReplication() == 0) {\n+      // Since 0 is not a valid value for a cache directive's replication\n+      // field, seeing a replication of 0 on a CacheBlock means that it\n+      // has never been reached by any sweep.\n+      return \"not needed by any directives\";\n+    } else if (cblock.getMark() != mark) { \n+      // Although the block was needed in the past, we didn't reach it during\n+      // the current sweep.  Therefore, it doesn't need to be cached any more.\n+      return \"no longer needed by any directives\";\n+    }\n+    return null;\n+  }\n+\n   /**\n    * Scan through the cached block map.\n    * Any blocks which are under-replicated should be assigned new Datanodes.\n@@ -508,11 +545,17 @@ private void rescanCachedBlockMap() {\n           iter.remove();\n         }\n       }\n-      // If the block's mark doesn't match with the mark of this scan, that\n-      // means that this block couldn't be reached during this scan.  That means\n-      // it doesn't need to be cached any more.\n-      int neededCached = (cblock.getMark() != mark) ?\n-          0 : cblock.getReplication();\n+      BlockInfo blockInfo = blockManager.\n+            getStoredBlock(new Block(cblock.getBlockId()));\n+      String reason = findReasonForNotCaching(cblock, blockInfo);\n+      int neededCached = 0;\n+      if (reason != null) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"not caching \" + cblock + \" because it is \" + reason);\n+        }\n+      } else {\n+        neededCached = cblock.getReplication();\n+      }\n       int numCached = cached.size();\n       if (numCached >= neededCached) {\n         // If we have enough replicas, drop all pending cached.\n@@ -612,8 +655,10 @@ private void addNewPendingCached(int neededCached,\n     BlockInfo blockInfo = blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo == null) {\n-      LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n-          \"was deleted from all DataNodes.\");\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n+            \"is no record of it on the NameNode.\");\n+      }\n       return;\n     }\n     if (!blockInfo.isComplete()) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
                "sha": "e86f345a4995b903a613b1af5ab91e8935f9bcba",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java?ref=a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc",
                "deletions": 25,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                "patch": "@@ -62,7 +62,6 @@\n import org.apache.hadoop.hdfs.protocol.CachePoolInfo;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n-import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n import org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n@@ -940,39 +939,28 @@ private void processCacheReportImpl(final DatanodeDescriptor datanode,\n       final List<Long> blockIds) {\n     CachedBlocksList cached = datanode.getCached();\n     cached.clear();\n+    CachedBlocksList cachedList = datanode.getCached();\n+    CachedBlocksList pendingCachedList = datanode.getPendingCached();\n     for (Iterator<Long> iter = blockIds.iterator(); iter.hasNext(); ) {\n-      Block block = new Block(iter.next());\n-      BlockInfo blockInfo = blockManager.getStoredBlock(block);\n-      if (!blockInfo.isComplete()) {\n-        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n-            \"it is in not complete yet.  It is in state \" + \n-            blockInfo.getBlockUCState());\n-        continue;\n-      }\n-      Collection<DatanodeDescriptor> corruptReplicas =\n-          blockManager.getCorruptReplicas(blockInfo);\n-      if ((corruptReplicas != null) && corruptReplicas.contains(datanode)) {\n-        // The NameNode will eventually remove or update the corrupt block.\n-        // Until then, we pretend that it isn't cached.\n-        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n-            \" because it is corrupt.\");\n-        continue;\n-      }\n+      long blockId = iter.next();\n       CachedBlock cachedBlock =\n-          new CachedBlock(block.getBlockId(), (short)0, false);\n+          new CachedBlock(blockId, (short)0, false);\n       CachedBlock prevCachedBlock = cachedBlocks.get(cachedBlock);\n-      // Use the existing CachedBlock if it's present; otherwise,\n-      // insert a new one.\n+      // Add the block ID from the cache report to the cachedBlocks map\n+      // if it's not already there.\n       if (prevCachedBlock != null) {\n         cachedBlock = prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n       }\n-      if (!cachedBlock.isPresent(datanode.getCached())) {\n-        datanode.getCached().add(cachedBlock);\n+      // Add the block to the datanode's implicit cached block list\n+      // if it's not already there.  Similarly, remove it from the pending\n+      // cached block list if it exists there.\n+      if (!cachedBlock.isPresent(cachedList)) {\n+        cachedList.add(cachedBlock);\n       }\n-      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n-        datanode.getPendingCached().remove(cachedBlock);\n+      if (cachedBlock.isPresent(pendingCachedList)) {\n+        pendingCachedList.remove(cachedBlock);\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                "sha": "f24b386df16942d0f23b2dbe5e22d29caea28c42",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java?ref=a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "patch": "@@ -69,6 +69,7 @@\n import org.apache.hadoop.hdfs.protocol.CachePoolStats;\n import org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.CachedBlocksList.Type;\n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.apache.hadoop.io.nativeio.NativeIO;\n import org.apache.hadoop.io.nativeio.NativeIO.POSIX.CacheManipulator;\n@@ -796,7 +797,15 @@ public Boolean get() {\n       }\n     }, 500, 60000);\n \n+    // Send a cache report referring to a bogus block.  It is important that\n+    // the NameNode be robust against this.\n     NamenodeProtocols nnRpc = namenode.getRpcServer();\n+    DataNode dn0 = cluster.getDataNodes().get(0);\n+    String bpid = cluster.getNamesystem().getBlockPoolId();\n+    LinkedList<Long> bogusBlockIds = new LinkedList<Long> ();\n+    bogusBlockIds.add(999999L);\n+    nnRpc.cacheReport(dn0.getDNRegistrationForBP(bpid), bpid, bogusBlockIds);\n+\n     Path rootDir = helper.getDefaultWorkingDirectory(dfs);\n     // Create the pool\n     final String pool = \"friendlyPool\";",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a11e94b86ea7c58cd9e5664f2e2ea5449259b0bc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "sha": "6ab808ea167465887f592f8f916321d5d1c3480d",
                "status": "modified"
            }
        ],
        "message": "HDFS-5708. The CacheManager throws a NPE in the DataNode logs when processing cache reports that refer to a block not known to the BlockManager. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554594 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/539dad7dce223da73ec53dbf5a5481af45a2d84a",
        "patched_files": [
            "CacheManager.java",
            "CacheReplicationMonitor.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCacheDirectives.java"
        ]
    },
    "hadoop-common_a341cdc": {
        "bug_id": "hadoop-common_a341cdc",
        "commit": "https://github.com/apache/hadoop-common/commit/a341cdc655cd07d00dd5ca73006950ef9326fbee",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -509,8 +509,6 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n-    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n-\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4c01ec63631384b08b4b0ab66dbb7a2226b41f61",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -540,7 +540,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if (UserGroupInformation.isSecurityEnabled()) {\n+    if(UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -558,7 +558,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.verifyToken(id, token.getPassword());\n+        nn.getNamesystem().verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "c0da8779fd354d13c394ddf75046f8315df5b240",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5464,11 +5464,21 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+  \n   @Override\n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n-\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "571bde80b7469124187525f0d6e184c117aea607",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,7 +51,6 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n-import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -79,7 +78,6 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n-import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1285,18 +1283,7 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t);\n   }\n-\n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-\n+  \n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "d69328565e623774d335e906df61d5be62cab16e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "44b0437d131646de1483dcf1da4787c6e4c18c38",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=a341cdc655cd07d00dd5ca73006950ef9326fbee",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -70,7 +69,6 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n-    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -81,8 +79,6 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n-    when(context.getAttribute(\n-        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a341cdc655cd07d00dd5ca73006950ef9326fbee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "c7fafdb13bc8b2bdf26bf5ed53b3353552975e3c",
                "status": "modified"
            }
        ],
        "message": "Revert HDFS-3654. TestJspHelper#testGetUgi fails with NPE.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362759 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/500909d63dc43d41584bc8b24a82ae7e1d51c3b1",
        "patched_files": [
            "CHANGES.txt",
            "NameNode.java",
            "NameNodeHttpServer.java",
            "JspHelper.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java",
            "TestJspHelper.java"
        ]
    },
    "hadoop-common_a5ab28a": {
        "bug_id": "hadoop-common_a5ab28a",
        "commit": "https://github.com/apache/hadoop-common/commit/a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -100,6 +100,9 @@ Trunk (unreleased changes)\n     HADOOP-7131. Exceptions thrown by Text methods should include the causing\n     exception. (Uma Maheswara Rao G via todd)\n \n+    HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased().\n+    (Kan Zhang via jitendra)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/CHANGES.txt",
                "sha": "ead2d3f88a9ba1f5b25e5526666fa9d823086f02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/UserGroupInformation.java?ref=a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -811,8 +811,8 @@ private boolean hasSufficientTimeElapsed(long now) {\n    * Did the login happen via keytab\n    * @return true or false\n    */\n-  public synchronized static boolean isLoginKeytabBased() {\n-    return loginUser.isKeytab;\n+  public synchronized static boolean isLoginKeytabBased() throws IOException {\n+    return getLoginUser().isKeytab;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "085ce61719eefec5cd06738b846867a1335f08cd",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased(). Contributed by Kan Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1079068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d232e914e91769f80269af43f07509e918b9d06a",
        "patched_files": [
            "UserGroupInformation.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop-common_b5e5a31": {
        "bug_id": "hadoop-common_b5e5a31",
        "commit": "https://github.com/apache/hadoop-common/commit/b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -273,6 +273,8 @@ Trunk (Unreleased)\n     HDFS-4785. Concat operation does not remove concatenated files from\n     InodeMap. (suresh)\n \n+    HDFS-4784. NPE in FSDirectory.resolvePath(). (Brandon Li via suresh)\n+\n   BREAKDOWN OF HADOOP-8562 and HDFS-3602 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4145. Merge hdfs cmd line scripts from branch-1-win. (David Lao,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4329d145cb8f54eb93bcf59ea28f76aa48002736",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1090,7 +1090,7 @@ int unprotectedDelete(String src, BlocksMapUpdateInfo collectedBlocks,\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedDelete: \"\n           +src+\" is removed\");\n     }\n-    remvoedAllFromInodesFromMap(targetNode);\n+    removeAllFromInodesFromMap(targetNode);\n     return filesRemoved;\n   }\n   \n@@ -1783,14 +1783,14 @@ private final void removeFromInodeMap(INode inode) {\n   }\n   \n   /** Remove all the inodes under given inode from the map */\n-  private void remvoedAllFromInodesFromMap(INode inode) {\n+  private void removeAllFromInodesFromMap(INode inode) {\n     removeFromInodeMap(inode);\n     if (!inode.isDirectory()) {\n       return;\n     }\n     INodeDirectory dir = (INodeDirectory) inode;\n     for (INode child : dir.getChildrenList()) {\n-      remvoedAllFromInodesFromMap(child);\n+      removeAllFromInodesFromMap(child);\n     }\n     dir.clearChildren();\n   }\n@@ -2258,14 +2258,18 @@ static String resolvePath(String src, byte[][] pathComponents, FSDirectory fsd)\n     try {\n       id = Long.valueOf(inodeId);\n     } catch (NumberFormatException e) {\n-      throw new FileNotFoundException(\n-          \"File for given inode path does not exist: \" + src);\n+      throw new FileNotFoundException(\"Invalid inode path: \" + src);\n     }\n     if (id == INodeId.ROOT_INODE_ID && pathComponents.length == 4) {\n       return Path.SEPARATOR;\n     }\n+    INode inode = fsd.getInode(id);\n+    if (inode == null) {\n+      throw new FileNotFoundException(\n+          \"File for given inode path does not exist: \" + src);\n+    }\n     StringBuilder path = id == INodeId.ROOT_INODE_ID ? new StringBuilder()\n-        : new StringBuilder(fsd.getInode(id).getFullPathName());\n+        : new StringBuilder(inode.getFullPathName());\n     for (int i = 4; i < pathComponents.length; i++) {\n       path.append(Path.SEPARATOR).append(DFSUtil.bytes2String(pathComponents[i]));\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "a3150a8b20eff2ab7bb2beb9b0699e8f923348fd",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java?ref=b5e5a3112d92016c2bf9f97c3eedd214d9950e45",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "patch": "@@ -909,6 +909,17 @@ public void testInodePath() throws FileNotFoundException {\n     components = INode.getPathComponents(testPath);\n     resolvedPath = FSDirectory.resolvePath(testPath, components, fsd);\n     assertEquals(testPath, resolvedPath);\n+    \n+    // Test path with nonexistent(deleted or wrong id) inode\n+    Mockito.doReturn(null).when(fsd).getInode(Mockito.anyLong());\n+    testPath = \"/.reserved/.inodes/1234\";\n+    components = INode.getPathComponents(testPath);\n+    try {\n+      String realPath = FSDirectory.resolvePath(testPath, components, fsd);\n+      fail(\"Path should not be resolved:\" + realPath);\n+    } catch (IOException e) {\n+      assertTrue(e instanceof FileNotFoundException);\n+    }\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b5e5a3112d92016c2bf9f97c3eedd214d9950e45/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "sha": "3b6491f6e5b4a08d5f5f0f430cb73caff9b9b0c7",
                "status": "modified"
            }
        ],
        "message": "HDFS-4784. NPE in FSDirectory.resolvePath(). Contributed by Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1478276 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/7dc8fc8442e3cf8a57c931bc732adc2e0577c705",
        "patched_files": [
            "FSDirectory.java",
            "CHANGES.txt",
            "INodeFile.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestINodeFile.java",
            "TestFSDirectory.java"
        ]
    },
    "hadoop-common_b671aa6": {
        "bug_id": "hadoop-common_b671aa6",
        "commit": "https://github.com/apache/hadoop-common/commit/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=b671aa67256e9e829da24e6f6c0be52ae3e2b2aa",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -426,6 +426,9 @@ Release 2.0.5-beta - UNRELEASED\n     MAPREDUCE-5244. Two functions changed their visibility in JobStatus. \n     (zjshen via tucu)\n \n+    MAPREDUCE-4927. Historyserver 500 error due to NPE when accessing specific\n+    counters page for failed job. (Ashwin Shankar via jlowe)\n+\n Release 2.0.4-alpha - 2013-04-25\n \n   INCOMPATIBLE CHANGES\n@@ -991,6 +994,9 @@ Release 0.23.8 - UNRELEASED\n     MAPREDUCE-5147. Maven build should create \n     hadoop-mapreduce-client-app-VERSION.jar directly (Robert Parker via tgraves)\n \n+    MAPREDUCE-4927. Historyserver 500 error due to NPE when accessing specific\n+    counters page for failed job. (Ashwin Shankar via jlowe)\n+\n Release 0.23.7 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "337144af92f814a0e9ddc57a8c225977b2b0f817",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java?ref=b671aa67256e9e829da24e6f6c0be52ae3e2b2aa",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java",
                "patch": "@@ -143,8 +143,9 @@ private void populateMembers(AppContext ctx) {\n     Map<TaskId, Task> tasks = job.getTasks();\n     for(Map.Entry<TaskId, Task> entry : tasks.entrySet()) {\n       long value = 0;\n-      CounterGroup group = entry.getValue().getCounters()\n-        .getGroup($(COUNTER_GROUP));\n+      Counters counters = entry.getValue().getCounters();\n+      CounterGroup group = (counters != null) ? counters\n+        .getGroup($(COUNTER_GROUP)) : null;\n       if(group != null)  {\n         Counter c = group.findCounter($(COUNTER_NAME));\n         if(c != null) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java",
                "sha": "974b3ff8f839584844a32fce7ce79671b4e32a58",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java?ref=b671aa67256e9e829da24e6f6c0be52ae3e2b2aa",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java",
                "patch": "@@ -182,6 +182,11 @@ public ClusterInfo getClusterInfo() {\n   \n   @Test public void testSingleCounterView() {\n     AppContext appContext = new TestAppContext();\n+    Job job = appContext.getAllJobs().values().iterator().next();\n+    // add a failed task to the job without any counters\n+    Task failedTask = MockJobs.newTask(job.getID(), 2, 1, true);\n+    Map<TaskId,Task> tasks = job.getTasks();\n+    tasks.put(failedTask.getID(), failedTask);\n     Map<String, String> params = getJobParams(appContext);\n     params.put(AMParams.COUNTER_GROUP, \n         \"org.apache.hadoop.mapreduce.FileSystemCounter\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b671aa67256e9e829da24e6f6c0be52ae3e2b2aa/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java",
                "sha": "7622a9ab955a8db7bada383d4061d917a1e574fa",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4927. Historyserver 500 error due to NPE when accessing specific counters page for failed job. Contributed by Ashwin Shankar\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1483974 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/94fe04ac411b9f3a9858b6a7d403ffe0b9ae33d9",
        "patched_files": [
            "SingleCounterBlock.java",
            "AMWebApp.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestAMWebApp.java"
        ]
    },
    "hadoop-common_b72e15a": {
        "bug_id": "hadoop-common_b72e15a",
        "commit": "https://github.com/apache/hadoop-common/commit/b72e15aa512a6bd1cd948ea76add1ad465478b8a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -526,6 +526,9 @@ Trunk (unreleased changes)\n     HDFS-1827. Fix timeout problem in TestBlockReplacement.  (Matt Foley\n     via szetszwo)\n \n+    HDFS-1908. Fix a NullPointerException in fi.DataTransferTestUtil.\n+    (szetszwo)\n+\n Release 0.22.0 - Unreleased\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/CHANGES.txt",
                "sha": "5b8bd0515e314c6b3a9931e339dce7f659b325e7",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 30,
                "filename": "src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.fi.FiTestUtil.CountdownConstraint;\n import org.apache.hadoop.fi.FiTestUtil.MarkerConstraint;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n \n@@ -55,7 +54,7 @@ public static DataTransferTest getDataTransferTest() {\n    * and some actions.\n    */\n   public static class DataTransferTest implements PipelineTest {\n-    private List<Pipeline> pipelines = new ArrayList<Pipeline>();\n+    private final List<Pipeline> pipelines = new ArrayList<Pipeline>();\n     private volatile boolean isSuccess = false;\n \n     /** Simulate action for the receiverOpWriteBlock pointcut */\n@@ -101,7 +100,8 @@ public void markSuccess() {\n     }\n \n     /** Initialize the pipeline. */\n-    public Pipeline initPipeline(LocatedBlock lb) {\n+    @Override\n+    public synchronized Pipeline initPipeline(LocatedBlock lb) {\n       final Pipeline pl = new Pipeline(lb);\n       if (pipelines.contains(pl)) {\n         throw new IllegalStateException(\"thepipeline != null\");\n@@ -110,20 +110,31 @@ public Pipeline initPipeline(LocatedBlock lb) {\n       return pl;\n     }\n \n-    /** Return the pipeline. */\n-    public Pipeline getPipeline(DatanodeID id) {\n-      if (pipelines == null) {\n-        throw new IllegalStateException(\"thepipeline == null\");\n-      }\n-      StringBuilder dnString = new StringBuilder();\n-      for (Pipeline pipeline : pipelines) {\n-        for (DatanodeInfo dni : pipeline.getDataNodes())\n-          dnString.append(dni.getStorageID());\n-        if (dnString.toString().contains(id.getStorageID()))\n-          return pipeline;\n+    /** Return the pipeline for the datanode. */\n+    @Override\n+    public synchronized Pipeline getPipelineForDatanode(DatanodeID id) {\n+      for (Pipeline p : pipelines) {\n+        if (p.contains(id)){\n+          return p;\n+        }\n       }\n+      FiTestUtil.LOG.info(\"FI: pipeline not found; id=\" + id\n+          + \", pipelines=\" + pipelines);\n       return null;\n     }\n+\n+    /**\n+     * Is the test not yet success\n+     * and the last pipeline contains the given datanode?\n+     */\n+    private synchronized boolean isNotSuccessAndLastPipelineContains(\n+        int index, DatanodeID id) {\n+      if (isSuccess()) {\n+        return false;\n+      }\n+      final int n = pipelines.size();\n+      return n == 0? false: pipelines.get(n-1).contains(index, id);\n+    }\n   }\n \n   /** Action for DataNode */\n@@ -171,8 +182,7 @@ public DatanodeMarkingAction(String currentTest, int index,\n     @Override\n     public void run(DatanodeID datanodeid) throws IOException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(datanodeid);\n-      if (p.contains(index, datanodeid)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, datanodeid)) {\n         marker.mark();\n       }\n     }\n@@ -193,8 +203,7 @@ public OomAction(String currentTest, int i) {\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (!test.isSuccess() && p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new OutOfMemoryError(s);\n@@ -215,8 +224,8 @@ public CountdownOomAction(String currentTest, int i, int count) {\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new OutOfMemoryError(s);\n@@ -234,8 +243,7 @@ public DoosAction(String currentTest, int i) {\n     @Override\n     public void run(DatanodeID id) throws DiskOutOfSpaceException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new DiskOutOfSpaceException(s);\n@@ -256,8 +264,7 @@ public IoeAction(String currentTest, int i, String error) {\n     @Override\n     public void run(DatanodeID id) throws IOException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new IOException(s);\n@@ -284,8 +291,8 @@ public CountdownDoosAction(String currentTest, int i, int count) {\n     @Override\n     public void run(DatanodeID id) throws DiskOutOfSpaceException {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id);\n         FiTestUtil.LOG.info(s);\n         throw new DiskOutOfSpaceException(s);\n@@ -339,8 +346,7 @@ public SleepAction(String currentTest, int i,\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (!test.isSuccess() && p.contains(index, id)) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)) {\n         FiTestUtil.LOG.info(toString(id));\n         if (maxDuration <= 0) {\n           for(; FiTestUtil.sleep(1000); ); //sleep forever until interrupt\n@@ -385,8 +391,8 @@ public CountdownSleepAction(String currentTest, int i,\n     @Override\n     public void run(DatanodeID id) {\n       final DataTransferTest test = getDataTransferTest();\n-      final Pipeline p = test.getPipeline(id);\n-      if (p.contains(index, id) && countdown.isSatisfied()) {\n+      if (test.isNotSuccessAndLastPipelineContains(index, id)\n+          && countdown.isSatisfied()) {\n         final String s = toString(id) + \", duration = [\"\n         + minDuration + \",\" + maxDuration + \")\";\n         FiTestUtil.LOG.info(s);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/DataTransferTestUtil.java",
                "sha": "4724595d4af6fceb707ea826818795342e38b82c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 2,
                "filename": "src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "patch": "@@ -44,9 +44,8 @@ public DerrAction(String currentTest, int index) {\n \n     /** {@inheritDoc} */\n     public void run(DatanodeID id) throws IOException {\n-      final Pipeline p = getPipelineTest().getPipeline(id);\n+      final Pipeline p = getPipelineTest().getPipelineForDatanode(id);\n       if (p == null) {\n-        FiTestUtil.LOG.info(\"FI: couldn't find a pipeline for \" + id);\n         return;\n       }\n       if (p.contains(index, id)) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/FiHFlushTestUtil.java",
                "sha": "0df95abde5a8cfc9f83e6e716493317fb34a2717",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/aop/org/apache/hadoop/fi/Pipeline.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 8,
                "filename": "src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "patch": "@@ -26,26 +26,24 @@\n \n public class Pipeline {\n   private final List<String> datanodes = new ArrayList<String>();\n-  private DatanodeInfo[] nodes;\n \n   Pipeline(LocatedBlock lb) {\n     for(DatanodeInfo d : lb.getLocations()) {\n       datanodes.add(d.getName());\n     }\n-    nodes = lb.getLocations();\n+  }\n+\n+  /** Does the pipeline contains d? */\n+  public boolean contains(DatanodeID d) {\n+    return datanodes.contains(d.getName());\n   }\n \n   /** Does the pipeline contains d at the n th position? */\n   public boolean contains(int n, DatanodeID d) {\n     return d.getName().equals(datanodes.get(n));\n   }\n \n-  /** Returns DatanodeInfo[] of the nodes of the constructed pipiline*/\n-  public DatanodeInfo[] getDataNodes () {\n-    return nodes;\n-  }\n-\n-  /** {@inheritDoc} */\n+  @Override\n   public String toString() {\n     return getClass().getSimpleName() + datanodes;\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/Pipeline.java",
                "sha": "877b100e4c6e6a1fd589f9d51060dce098b3b135",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/aop/org/apache/hadoop/fi/PipelineTest.java?ref=b72e15aa512a6bd1cd948ea76add1ad465478b8a",
                "deletions": 1,
                "filename": "src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "patch": "@@ -23,5 +23,5 @@\n /** A pipeline contains a list of datanodes. */\n public interface PipelineTest {\n   public Pipeline initPipeline(LocatedBlock lb);\n-  public Pipeline getPipeline(DatanodeID id);\n+  public Pipeline getPipelineForDatanode(DatanodeID id);\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/b72e15aa512a6bd1cd948ea76add1ad465478b8a/src/test/aop/org/apache/hadoop/fi/PipelineTest.java",
                "sha": "838d5b99d29f33546a9da9a93d4a302e46ffe428",
                "status": "modified"
            }
        ],
        "message": "HDFS-1908. Fix a NullPointerException in fi.DataTransferTestUtil.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1101675 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/06568fb9f4bbc9492b81dabc79087672e9b78697",
        "patched_files": [
            "DataTransferTestUtil.java",
            "Pipeline.java",
            "CHANGES.txt",
            "FiHFlushTestUtil.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "PipelineTest.java"
        ]
    },
    "hadoop-common_be31fd0": {
        "bug_id": "hadoop-common_be31fd0",
        "commit": "https://github.com/apache/hadoop-common/commit/be31fd05dae345e62772ecc235f14f3f59c37e04",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -471,6 +471,8 @@ Release 0.22.0 - Unreleased\n     HDFS-1560. dfs.data.dir permissions should default to 700. \n     (Todd Lipcon via eli)\n \n+    HDFS-1550. NPE when listing a file with no location. (hairong)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/CHANGES.txt",
                "sha": "71ebb27b84bef74df28bb21ef9fa2bd398da28cc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -204,6 +204,9 @@ public static String byteArray2String(byte[][] pathComponents) {\n     }\n     int nrBlocks = blocks.locatedBlockCount();\n     BlockLocation[] blkLocations = new BlockLocation[nrBlocks];\n+    if (nrBlocks == 0) {\n+      return blkLocations;\n+    }\n     int idx = 0;\n     for (LocatedBlock blk : blocks.getLocatedBlocks()) {\n       assert idx < nrBlocks : \"Incorrect index\";",
                "raw_url": "https://github.com/apache/hadoop-common/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "6975c53a25cd37604d11b0327e0db7a473aa136c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java?ref=be31fd05dae345e62772ecc235f14f3f59c37e04",
                "deletions": 0,
                "filename": "src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "patch": "@@ -19,6 +19,8 @@\n package org.apache.hadoop.hdfs;\n \n import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n@@ -65,5 +67,9 @@ public void testLocatedBlocks2Locations() {\n \n     assertTrue(\"expected 1 corrupt files but got \" + corruptCount, \n                corruptCount == 1);\n+    \n+    // test an empty location\n+    bs = DFSUtil.locatedBlocks2Locations(new LocatedBlocks());\n+    assertEquals(0, bs.length);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/be31fd05dae345e62772ecc235f14f3f59c37e04/src/test/hdfs/org/apache/hadoop/hdfs/TestDFSUtil.java",
                "sha": "03e6b39c735c4110059773a65fe0ee61de79ec4d",
                "status": "modified"
            }
        ],
        "message": "HDFS-1550. NPE when listing a file with no location. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1054807 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d1d9265d1758813b62c20157297e8cfd824628ee",
        "patched_files": [
            "DFSUtil.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop-common_bede777": {
        "bug_id": "hadoop-common_bede777",
        "commit": "https://github.com/apache/hadoop-common/commit/bede7770b03234b769a92362a2e53ded8398890f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bede7770b03234b769a92362a2e53ded8398890f/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=bede7770b03234b769a92362a2e53ded8398890f",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -310,6 +310,9 @@ Release 2.4.0 - UNRELEASED\n     HDFS-5843. DFSClient.getFileChecksum() throws IOException if checksum is \n     disabled. (Laurent Goujon via jing9)\n \n+    HDFS-5856. DataNode.checkDiskError might throw NPE.\n+    (Josh Elser via suresh)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bede7770b03234b769a92362a2e53ded8398890f/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "1fb5c1c190ce0d90159261c81bcb4b9c535923ac",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bede7770b03234b769a92362a2e53ded8398890f/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=bede7770b03234b769a92362a2e53ded8398890f",
                "deletions": 7,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -36,6 +36,7 @@\n import java.net.URI;\n import java.net.UnknownHostException;\n import java.nio.channels.ClosedByInterruptException;\n+import java.nio.channels.ClosedChannelException;\n import java.nio.channels.SocketChannel;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n@@ -51,7 +52,6 @@\n \n import javax.management.ObjectName;\n \n-\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n@@ -1324,12 +1324,7 @@ public void shutdown() {\n   protected void checkDiskError(Exception e ) throws IOException {\n     \n     LOG.warn(\"checkDiskError: exception: \", e);\n-    if (e instanceof SocketException || e instanceof SocketTimeoutException\n-    \t  || e instanceof ClosedByInterruptException \n-    \t  || e.getMessage().startsWith(\"An established connection was aborted\")\n-    \t  || e.getMessage().startsWith(\"Broken pipe\")\n-    \t  || e.getMessage().startsWith(\"Connection reset\")\n-    \t  || e.getMessage().contains(\"java.nio.channels.SocketChannel\")) {\n+    if (isNetworkRelatedException(e)) {\n       LOG.info(\"Not checking disk as checkDiskError was called on a network\" +\n       \t\t\" related exception\");\t\n       return;\n@@ -1342,6 +1337,28 @@ protected void checkDiskError(Exception e ) throws IOException {\n     }\n   }\n   \n+  /**\n+   * Check if the provided exception looks like it's from a network error\n+   * @param e the exception from a checkDiskError call\n+   * @return true if this exception is network related, false otherwise\n+   */\n+  protected boolean isNetworkRelatedException(Exception e) {\n+    if (e instanceof SocketException \n+        || e instanceof SocketTimeoutException\n+        || e instanceof ClosedChannelException \n+        || e instanceof ClosedByInterruptException) {\n+      return true;\n+    }\n+    \n+    String msg = e.getMessage();\n+    \n+    return null != msg \n+        && (msg.startsWith(\"An established connection was aborted\")\n+            || msg.startsWith(\"Broken pipe\")\n+            || msg.startsWith(\"Connection reset\")\n+            || msg.contains(\"java.nio.channels.SocketChannel\"));\n+  }\n+  \n   /**\n    *  Check if there is a disk failure and if so, handle the error\n    */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bede7770b03234b769a92362a2e53ded8398890f/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "6bf98b694659ebe8fa0d9406500f9c5c0735b4b1",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bede7770b03234b769a92362a2e53ded8398890f/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java?ref=bede7770b03234b769a92362a2e53ded8398890f",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "patch": "@@ -18,12 +18,16 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n import java.io.DataOutputStream;\n import java.io.File;\n import java.net.InetSocketAddress;\n import java.net.Socket;\n+import java.net.SocketException;\n+import java.net.SocketTimeoutException;\n+import java.nio.channels.ClosedChannelException;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -196,4 +200,16 @@ public void testLocalDirs() throws Exception {\n       }\n     }\n   }\n+  \n+  @Test\n+  public void testNetworkErrorsIgnored() {\n+    DataNode dn = cluster.getDataNodes().iterator().next();\n+    \n+    assertTrue(dn.isNetworkRelatedException(new SocketException()));\n+    assertTrue(dn.isNetworkRelatedException(new SocketTimeoutException()));\n+    assertTrue(dn.isNetworkRelatedException(new ClosedChannelException()));\n+    assertTrue(dn.isNetworkRelatedException(new Exception(\"Broken pipe foo bar\")));\n+    assertFalse(dn.isNetworkRelatedException(new Exception()));\n+    assertFalse(dn.isNetworkRelatedException(new Exception(\"random problem\")));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bede7770b03234b769a92362a2e53ded8398890f/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "sha": "e36005bafc6f486614b39c887d88315d9f797572",
                "status": "modified"
            }
        ],
        "message": "HDFS-5856. DataNode.checkDiskError might throw NPE. Contributed by Josh Elser.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563064 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/218809dac7daa2920fbc5358940d1945bca8e292",
        "patched_files": [
            "DataNode.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDiskError.java"
        ]
    },
    "hadoop-common_bee7247": {
        "bug_id": "hadoop-common_bee7247",
        "commit": "https://github.com/apache/hadoop-common/commit/bee72473f36c24d0103236153044bd7836bc19e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=bee72473f36c24d0103236153044bd7836bc19e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -439,6 +439,9 @@ Release 2.3.0 - UNRELEASED\n \n     HADOOP-10100. MiniKDC shouldn't use apacheds-all artifact. (rkanter via tucu)\n \n+    HADOOP-10107. Server.getNumOpenConnections may throw NPE. (Kihwal Lee via\n+    jing9)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2b9aeb88a5a759ab34e1eccab28d261d53a65826",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=bee72473f36c24d0103236153044bd7836bc19e3",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2109,6 +2109,7 @@ protected Server(String bindAddress, int port,\n     // Start the listener here and let it bind to the port\n     listener = new Listener();\n     this.port = listener.getAddress().getPort();    \n+    connectionManager = new ConnectionManager();\n     this.rpcMetrics = RpcMetrics.create(this);\n     this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port);\n     this.tcpNoDelay = conf.getBoolean(\n@@ -2117,7 +2118,6 @@ protected Server(String bindAddress, int port,\n \n     // Create the responder here\n     responder = new Responder();\n-    connectionManager = new ConnectionManager();\n     \n     if (secretManager != null) {\n       SaslRpcServer.init(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "7fb395cdb056029d85a7940a7e7fbedd941a5a4c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10107. Server.getNumOpenConnections may throw NPE. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/66f8ea400a5d97d9602405b5e9ff597d75e30b3f",
        "patched_files": [
            "Server.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop-common_c231fc2": {
        "bug_id": "hadoop-common_c231fc2",
        "commit": "https://github.com/apache/hadoop-common/commit/c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -434,6 +434,9 @@ Release 2.3.0 - UNRELEASED\n     HADOOP-10093. hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size\n     that is too small. (Shanyu Zhao via cnauroth)\n \n+    HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows().\n+    (Enis Soztutar via cnauroth)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "4fd41a876c51b5258033fc4e71d5f2656c402207",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java?ref=c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "patch": "@@ -431,6 +431,9 @@ private String validateFiles(String files, Configuration conf)\n     if (!Shell.WINDOWS) {\n       return args;\n     }\n+    if (args == null) {\n+      return null;\n+    }\n     List<String> newArgs = new ArrayList<String>(args.length);\n     for (int i=0; i < args.length; i++) {\n       String prop = null;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "sha": "678185553939200900c73b1ac6952a05c7c07816",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java?ref=c231fc2046a3c0b6f7dda83f71810809b3b7fac4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "patch": "@@ -282,4 +282,12 @@ private void assertDOptionParsing(String[] args,\n       Arrays.toString(remainingArgs) + Arrays.toString(expectedRemainingArgs),\n       expectedRemainingArgs, remainingArgs);\n   }\n+\n+  /** Test passing null as args. Some classes still call\n+   * Tool interface from java passing null.\n+   */\n+  public void testNullArgs() throws IOException {\n+    GenericOptionsParser parser = new GenericOptionsParser(conf, null);\n+    parser.getRemainingArgs();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c231fc2046a3c0b6f7dda83f71810809b3b7fac4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "sha": "48a419b3a52f7b72889e8820fb2561630d73531d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows(). Contributed by Enis Soztutar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b65d41d0b4fc6a9966eb5a7281bfa59f2c7a4307",
        "patched_files": [
            "CHANGES.txt",
            "GenericOptionsParser.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestGenericOptionsParser.java"
        ]
    },
    "hadoop-common_c2e41ce": {
        "bug_id": "hadoop-common_c2e41ce",
        "commit": "https://github.com/apache/hadoop-common/commit/c2e41ceb1541ce550119256a82661c704f54e4df",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -84,6 +84,8 @@ Trunk (unreleased changes)\n     MAPREDUCE-1505. Create RPC client on job submission, not in cstr of Job\n     instance. (Dick King via cdouglas)\n \n+    MAPREDUCE-1813. NPE in PipeMapred.MRErrorThread. (Ravi Gummadi via vinodkv)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/CHANGES.txt",
                "sha": "7895d3f98e1e5fae2bfe0a51ea4b3338b634bb3c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 2,
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java",
                "patch": "@@ -222,8 +222,6 @@ public void configure(JobConf job) {\n       clientErr_ = new DataInputStream(new BufferedInputStream(sim.getErrorStream()));\n       startTime_ = System.currentTimeMillis();\n \n-      errThread_ = new MRErrorThread();\n-      errThread_.start();\n     } catch (IOException e) {\n       logStackTrace(e);\n       LOG.error(\"configuration exception\", e);\n@@ -338,7 +336,9 @@ void startOutputThreads(OutputCollector output, Reporter reporter)\n     outReader_ = createOutputReader();\n     outThread_ = new MROutputThread(outReader_, output, reporter);\n     outThread_.start();\n+    errThread_ = new MRErrorThread();\n     errThread_.setReporter(reporter);\n+    errThread_.start();\n   }\n   \n   void waitOutputThreads() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java",
                "sha": "f66001b2b898a5f713fc57ba10494e7df79e513c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 2,
                "filename": "src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java",
                "patch": "@@ -25,8 +25,6 @@\n \n import java.io.IOException;\n \n-import org.apache.hadoop.util.ReflectionUtils;\n-\n public class PipeMapRunner<K1, V1, K2, V2> extends MapRunner<K1, V1, K2, V2> {\n   public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,\n                   Reporter reporter)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRunner.java",
                "sha": "da2790b861e50c93c7e06927dd68cb026b71a20c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2fb120dd1326b2b28c346f36d6097ee1dbe22d68/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java?ref=2fb120dd1326b2b28c346f36d6097ee1dbe22d68",
                "deletions": 122,
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java",
                "patch": "@@ -1,122 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.streaming;\n-\n-import org.junit.Test;\n-\n-import java.io.*;\n-\n-import org.apache.hadoop.fs.FileUtil;\n-\n-/**\n- * This class tests hadoopStreaming in MapReduce local mode by giving\n- * empty input to mapper and the mapper generates nonempty output. Since map()\n- * is not called at all, output thread was not getting created and the mapper\n- * was hanging forever. Now this issue is solved. Similarly reducer is also\n- * checked for task completion with empty input and nonempty output.\n- */\n-public class TestStreamingEmptyInpNonemptyOut\n-{\n-\n-  protected File INPUT_FILE = new File(\"emptyInputFile.txt\");\n-  protected File OUTPUT_DIR = new File(\"out\");\n-  protected File SCRIPT_FILE = new File(\"perlScript.pl\");\n-\n-  protected String map = \"perlScript.pl\";\n-  protected String reduce = \"org.apache.hadoop.mapred.lib.IdentityReducer\";\n-  protected String script = \"#!/usr/bin/perl\\nfor($count = 1500; $count >= 1; $count--) {print \\\"$count \\\";}\";\n-\n-  private StreamJob job;\n-\n-  public TestStreamingEmptyInpNonemptyOut() throws IOException\n-  {\n-    UtilTest utilTest = new UtilTest(getClass().getName());\n-    utilTest.checkUserDir();\n-    utilTest.redirectIfAntJunit();\n-  }\n-\n-  protected void createInputAndScript() throws IOException\n-  {\n-    DataOutputStream out = new DataOutputStream(\n-                           new FileOutputStream(INPUT_FILE.getAbsoluteFile()));\n-    out.close();\n-\n-    out = new DataOutputStream(\n-          new FileOutputStream(SCRIPT_FILE.getAbsoluteFile()));\n-    out.write(script.getBytes(\"UTF-8\"));\n-    out.close();\n-  }\n-\n-  protected String[] genArgs() {\n-    return new String[] {\n-      \"-input\", INPUT_FILE.getAbsolutePath(),\n-      \"-output\", OUTPUT_DIR.getAbsolutePath(),\n-      \"-mapper\", map,\n-      \"-reducer\", reduce,\n-      //\"-verbose\",\n-      //\"-jobconf\", \"stream.debug=set\"\n-      \"-jobconf\", \"mapreduce.task.files.preserve.failedtasks=true\",\n-      \"-jobconf\", \"stream.tmpdir=\"+System.getProperty(\"test.build.data\",\"/tmp\")\n-    };\n-  }\n-\n-  @Test\n-  public void testEmptyInputNonemptyOutput() throws IOException\n-  {\n-    try {\n-      try {\n-        OUTPUT_DIR.getAbsoluteFile().delete();\n-      } catch (Exception e) {\n-      }\n-\n-      createInputAndScript();\n-      boolean mayExit = false;\n-\n-      // During tests, the default Configuration will use a local mapred\n-      // So don't specify -config or -cluster.\n-      // First let us test if mapper doesn't hang for empty i/p and nonempty o/p\n-      job = new StreamJob(genArgs(), mayExit);      \n-      job.go();\n-      File outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\n-      outFile.delete();\n-\n-      // Now let us test if reducer doesn't hang for empty i/p and nonempty o/p\n-      map = \"org.apache.hadoop.mapred.lib.IdentityMapper\";\n-      reduce = \"perlScript.pl\";\n-      job = new StreamJob(genArgs(), mayExit);      \n-      job.go();\n-      outFile = new File(OUTPUT_DIR, \"part-00000\").getAbsoluteFile();\n-      outFile.delete();\n-    } finally {\n-      try {\n-        INPUT_FILE.delete();\n-        SCRIPT_FILE.delete();\n-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());\n-      } catch (IOException e) {\n-        e.printStackTrace();\n-      }\n-    }\n-  }\n-\n-  public static void main(String[]args) throws Exception\n-  {\n-    new TestStreaming().testCommandLine();\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2fb120dd1326b2b28c346f36d6097ee1dbe22d68/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingEmptyInpNonemptyOut.java",
                "sha": "0678a8759d0fe0bd6710f0d7fcea23ce6d7d0644",
                "status": "removed"
            },
            {
                "additions": 253,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java",
                "changes": 291,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 38,
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java",
                "patch": "@@ -22,88 +22,303 @@\n import java.io.IOException;\n import java.io.File;\n \n+import org.junit.After;\n+import org.junit.Before;\n import org.junit.Test;\n import static org.junit.Assert.*;\n \n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.Counters;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapred.TaskID;\n+import org.apache.hadoop.mapred.TaskLog;\n import org.apache.hadoop.mapred.TaskReport;\n import org.apache.hadoop.mapreduce.MRJobConfig;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n \n \n /**\n- * Tests for the ability of a streaming task to set the status\n- * by writing \"reporter:status:\" lines to stderr. Uses MiniMR\n- * since the local jobtracker doesn't track status.\n+ * Tests if mapper/reducer with empty/nonempty input works properly if\n+ * reporting is done using lines like \"reporter:status:\" and\n+ * \"reporter:counter:\" before map()/reduce() method is called.\n+ * Validates the task's log of STDERR if messages are written to stderr before\n+ * map()/reduce() is called.\n+ * Also validates job output.\n+ * Uses MiniMR since the local jobtracker doesn't track task status. \n  */\n public class TestStreamingStatus {\n-  private static String TEST_ROOT_DIR =\n-    new File(System.getProperty(\"test.build.data\",\"/tmp\"))\n+  protected static String TEST_ROOT_DIR =\n+    new File(System.getProperty(\"test.build.data\",\"/tmp\"),\n+    TestStreamingStatus.class.getSimpleName())\n     .toURI().toString().replace(' ', '+');\n   protected String INPUT_FILE = TEST_ROOT_DIR + \"/input.txt\";\n   protected String OUTPUT_DIR = TEST_ROOT_DIR + \"/out\";\n   protected String input = \"roses.are.red\\nviolets.are.blue\\nbunnies.are.pink\\n\";\n-  protected String map = StreamUtil.makeJavaCommand(StderrApp.class, new String[]{\"3\", \"0\", \"0\", \"true\"});\n+  protected String map = null;\n+  protected String reduce = null;\n \n-  protected String[] genArgs(int jobtrackerPort) {\n+  protected String scriptFile = TEST_ROOT_DIR + \"/perlScript.pl\";\n+  protected String scriptFileName = new Path(scriptFile).toUri().getPath();\n+\n+\n+  String expectedStderr = \"my error msg before consuming input\\n\" +\n+      \"my error msg after consuming input\\n\";\n+  String expectedOutput = null;// inited in setUp()\n+  String expectedStatus = \"before consuming input\";\n+\n+  // This script does the following\n+  // (a) setting task status before reading input\n+  // (b) writing to stderr before reading input and after reading input\n+  // (c) writing to stdout before reading input\n+  // (d) incrementing user counter before reading input and after reading input\n+  // Write lines to stdout before reading input{(c) above} is to validate\n+  // the hanging task issue when input to task is empty(because of not starting\n+  // output thread).\n+  protected String script =\n+    \"#!/usr/bin/perl\\n\" +\n+    \"print STDERR \\\"reporter:status:\" + expectedStatus + \"\\\\n\\\";\\n\" +\n+    \"print STDERR \\\"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\\\n\\\";\\n\" +\n+    \"print STDERR \\\"my error msg before consuming input\\\\n\\\";\\n\" +\n+    \"for($count = 1500; $count >= 1; $count--) {print STDOUT \\\"$count \\\";}\" +\n+    \"while(<STDIN>) {chomp;}\\n\" +\n+    \"print STDERR \\\"my error msg after consuming input\\\\n\\\";\\n\" +\n+    \"print STDERR \\\"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\\\n\\\";\\n\";\n+\n+  MiniMRCluster mr = null;\n+  FileSystem fs = null;\n+  JobConf conf = null;\n+\n+  /**\n+   * Start the cluster and create input file before running the actual test.\n+   *\n+   * @throws IOException\n+   */\n+  @Before\n+  public void setUp() throws IOException {\n+    conf = new JobConf();\n+    conf.setBoolean(JTConfig.JT_RETIREJOBS, false);\n+\n+    mr = new MiniMRCluster(1, \"file:///\", 3, null , null, conf);\n+\n+    Path inFile = new Path(INPUT_FILE);\n+    fs = inFile.getFileSystem(mr.createJobConf());\n+    clean(fs);\n+\n+    buildExpectedJobOutput();\n+  }\n+\n+  /**\n+   * Kill the cluster after the test is done.\n+   */\n+  @After\n+  public void tearDown() {\n+    if (fs != null) { clean(fs); }\n+    if (mr != null) { mr.shutdown(); }\n+  }\n+\n+  // Updates expectedOutput to have the expected job output as a string\n+  void buildExpectedJobOutput() {\n+    if (expectedOutput == null) {\n+      expectedOutput = \"\";\n+      for(int i = 1500; i >= 1; i--) {\n+        expectedOutput = expectedOutput.concat(Integer.toString(i) + \" \");\n+      }\n+      expectedOutput = expectedOutput.trim();\n+    }\n+  }\n+\n+  // Create empty/nonempty input file.\n+  // Create script file with the specified content.\n+  protected void createInputAndScript(boolean isEmptyInput,\n+      String script) throws IOException {\n+    makeInput(fs, isEmptyInput ? \"\" : input);\n+\n+    // create script file\n+    DataOutputStream file = fs.create(new Path(scriptFileName));\n+    file.writeBytes(script);\n+    file.close();\n+  }\n+\n+  protected String[] genArgs(int jobtrackerPort, String mapper, String reducer)\n+  {\n     return new String[] {\n       \"-input\", INPUT_FILE,\n       \"-output\", OUTPUT_DIR,\n-      \"-mapper\", map,\n+      \"-mapper\", mapper,\n+      \"-reducer\", reducer,\n       \"-jobconf\", MRJobConfig.NUM_MAPS + \"=1\",\n-      \"-jobconf\", MRJobConfig.NUM_REDUCES + \"=0\",      \n+      \"-jobconf\", MRJobConfig.NUM_REDUCES + \"=1\",\n       \"-jobconf\", MRJobConfig.PRESERVE_FAILED_TASK_FILES + \"=true\",\n-      \"-jobconf\", \"stream.tmpdir=\"+System.getProperty(\"test.build.data\",\"/tmp\"),\n+      \"-jobconf\", \"stream.tmpdir=\" + new Path(TEST_ROOT_DIR).toUri().getPath(),\n       \"-jobconf\", JTConfig.JT_IPC_ADDRESS + \"=localhost:\"+jobtrackerPort,\n       \"-jobconf\", \"fs.default.name=file:///\"\n     };\n   }\n-  \n-  public void makeInput(FileSystem fs) throws IOException {\n+\n+  // create input file with the given content\n+  public void makeInput(FileSystem fs, String input) throws IOException {\n     Path inFile = new Path(INPUT_FILE);\n     DataOutputStream file = fs.create(inFile);\n     file.writeBytes(input);\n     file.close();\n   }\n \n-  public void clean(FileSystem fs) {\n+  // Delete output directory\n+  protected void deleteOutDir(FileSystem fs) {\n     try {\n       Path outDir = new Path(OUTPUT_DIR);\n       fs.delete(outDir, true);\n     } catch (Exception e) {}\n+  }\n+\n+  // Delete input file, script file and output directory\n+  public void clean(FileSystem fs) {\n+    deleteOutDir(fs);\n     try {\n-      Path inFile = new Path(INPUT_FILE);    \n-      fs.delete(inFile, false);\n-    } catch (Exception e) {}\n+      Path file = new Path(INPUT_FILE);\n+      if (fs.exists(file)) {\n+        fs.delete(file, false);\n+      }\n+      file = new Path(scriptFile);\n+      if (fs.exists(file)) {\n+        fs.delete(file, false);\n+      }\n+    } catch (Exception e) {\n+      e.printStackTrace();\n+    }\n   }\n-  \n+\n+  /**\n+   * Check if mapper/reducer with empty/nonempty input works properly if\n+   * reporting is done using lines like \"reporter:status:\" and\n+   * \"reporter:counter:\" before map()/reduce() method is called.\n+   * Validate the task's log of STDERR if messages are written\n+   * to stderr before map()/reduce() is called.\n+   * Also validate job output.\n+   *\n+   * @throws IOException\n+   */\n   @Test\n-  public void testStreamingStatus() throws Exception {\n-    MiniMRCluster mr = null;\n-    FileSystem fs = null;\n-    JobConf conf = new JobConf();\n-    conf.setBoolean(JTConfig.JT_RETIREJOBS, false);\n-    try {\n-      mr = new MiniMRCluster(1, \"file:///\", 3, null , null, conf);\n+  public void testReporting() throws Exception {\n+    testStreamJob(false);// nonempty input\n+    testStreamJob(true);// empty input\n+  }\n+\n+  /**\n+   * Run a streaming job with the given script as mapper and validate.\n+   * Run another streaming job with the given script as reducer and validate.\n+   *\n+   * @param isEmptyInput Should the input to the script be empty ?\n+   * @param script The content of the script that will run as the streaming task\n+   */\n+  private void testStreamJob(boolean isEmptyInput)\n+      throws IOException {\n+\n+      createInputAndScript(isEmptyInput, script);\n \n-      Path inFile = new Path(INPUT_FILE);\n-      fs = inFile.getFileSystem(mr.createJobConf());\n+      // Check if streaming mapper works as expected\n+      map = scriptFileName;\n+      reduce = \"/bin/cat\";\n+      runStreamJob(TaskType.MAP, isEmptyInput);\n+      deleteOutDir(fs);\n+\n+      // Check if streaming reducer works as expected.\n+      map = \"/bin/cat\";\n+      reduce = scriptFileName;\n+      runStreamJob(TaskType.REDUCE, isEmptyInput);\n       clean(fs);\n-      makeInput(fs);\n-      \n-      StreamJob job = new StreamJob();\n-      int failed = job.run(genArgs(mr.getJobTrackerPort()));\n-      assertEquals(0, failed);\n-\n-      TaskReport[] reports = job.jc_.getMapTaskReports(job.jobId_);\n-      assertEquals(1, reports.length);\n-      assertEquals(\"starting echo > sort\", reports[0].getState());\n-    } finally {\n-      if (fs != null) { clean(fs); }\n-      if (mr != null) { mr.shutdown(); }\n+  }\n+\n+  // Run streaming job for the specified input file, mapper and reducer and\n+  // (1) Validate if the job succeeds.\n+  // (2) Validate if user counter is incremented properly for the cases of\n+  //   (a) nonempty input to map\n+  //   (b) empty input to map and\n+  //   (c) nonempty input to reduce\n+  // (3) Validate task status for the cases of (2)(a),(2)(b),(2)(c).\n+  //     Because empty input to reduce task => reporter is dummy and ignores\n+  //     all \"reporter:status\" and \"reporter:counter\" lines. \n+  // (4) Validate stderr of task of given task type.\n+  // (5) Validate job output\n+  void runStreamJob(TaskType type, boolean isEmptyInput) throws IOException {\n+    boolean mayExit = false;\n+    StreamJob job = new StreamJob(genArgs(\n+        mr.getJobTrackerPort(), map, reduce), mayExit);\n+    int returnValue = job.go();\n+    assertEquals(0, returnValue);\n+\n+    // If input to reducer is empty, dummy reporter(which ignores all\n+    // reporting lines) is set for MRErrorThread in waitOutputThreads(). So\n+    // expectedCounterValue is 0 for empty-input-to-reducer case.\n+    // Output of reducer is also empty for empty-input-to-reducer case.\n+    int expectedCounterValue = 0;\n+    if (type == TaskType.MAP || !isEmptyInput) {\n+      validateTaskStatus(job, type);\n+      // output is from \"print STDOUT\" statements in perl script\n+      validateJobOutput(job.getConf());\n+      expectedCounterValue = 2;\n+    }\n+    validateUserCounter(job, expectedCounterValue);\n+    validateTaskStderr(job, type);\n+\n+    deleteOutDir(fs);\n+  }\n+\n+  // validate task status of task of given type(validates 1st task of that type)\n+  void validateTaskStatus(StreamJob job, TaskType type) throws IOException {\n+    // Map Task has 2 phases: map, sort\n+    // Reduce Task has 3 phases: copy, sort, reduce\n+    String finalPhaseInTask;\n+    TaskReport[] reports;\n+    if (type == TaskType.MAP) {\n+      reports = job.jc_.getMapTaskReports(job.jobId_);\n+      finalPhaseInTask = \"sort\";\n+    } else {// reduce task\n+      reports = job.jc_.getReduceTaskReports(job.jobId_);\n+      finalPhaseInTask = \"reduce\";\n     }\n+    assertEquals(1, reports.length);\n+    assertEquals(expectedStatus + \" > \" + finalPhaseInTask,\n+        reports[0].getState());\n   }\n+\n+  // Validate the job output\n+  void validateJobOutput(Configuration conf)\n+      throws IOException {\n+\n+    String output = MapReduceTestUtil.readOutput(\n+        new Path(OUTPUT_DIR), conf).trim();\n+\n+    assertTrue(output.equals(expectedOutput));\n+  }\n+\n+  // Validate stderr task log of given task type(validates 1st\n+  // task of that type).\n+  void validateTaskStderr(StreamJob job, TaskType type)\n+      throws IOException {\n+    TaskAttemptID attemptId =\n+        new TaskAttemptID(new TaskID(job.jobId_, type, 0), 0);\n+\n+    String log = MapReduceTestUtil.readTaskLog(TaskLog.LogName.STDERR,\n+        attemptId, false);\n+\n+    // trim() is called on expectedStderr here because the method\n+    // MapReduceTestUtil.readTaskLog() returns trimmed String.\n+    assertTrue(log.equals(expectedStderr.trim()));\n+  }\n+\n+  // Validate if user counter is incremented properly\n+  void validateUserCounter(StreamJob job, int expectedCounterValue)\n+      throws IOException {\n+    Counters counters = job.running_.getCounters();\n+    assertEquals(expectedCounterValue, counters.findCounter(\n+        \"myOwnCounterGroup\", \"myOwnCounter\").getValue());\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingStatus.java",
                "sha": "0b353b5ae8bdd2a85c1d277adf217f527fbdd9e2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 2,
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java",
                "patch": "@@ -25,7 +25,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n-import org.apache.hadoop.mapred.TestMiniMRWithDFS;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.util.Shell;\n \n import org.junit.Test;\n@@ -131,7 +131,7 @@ private void runStreamJobAndValidateEnv() throws IOException {\n     assertEquals(\"StreamJob failed.\", 0, returnStatus);\n     \n     // validate environment variables set for the child(script) of java process\n-    String env = TestMiniMRWithDFS.readOutput(outputPath, mr.createJobConf());\n+    String env = MapReduceTestUtil.readOutput(outputPath, mr.createJobConf());\n     long logSize = USERLOG_LIMIT_KB * 1024;\n     assertTrue(\"environment set for child is wrong\", env.contains(\"INFO,TLA\")\n                && env.contains(\"-Dhadoop.tasklog.taskid=attempt_\")",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestStreamingTaskLog.java",
                "sha": "c8c92e6c64131840ab6e102c49eb78130d89a31e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 2,
                "filename": "src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java",
                "patch": "@@ -26,8 +26,8 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n-import org.apache.hadoop.mapred.TestMiniMRWithDFS;\n import org.apache.hadoop.mapreduce.MRJobConfig;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.util.StringUtils;\n \n@@ -122,7 +122,7 @@ private void runProgram(String memLimit) throws IOException {\n     boolean mayExit = false;\n     StreamJob job = new StreamJob(genArgs(memLimit), mayExit);\n     job.go();\n-    String output = TestMiniMRWithDFS.readOutput(outputPath,\n+    String output = MapReduceTestUtil.readOutput(outputPath,\n                                         mr.createJobConf());\n     assertEquals(\"output is wrong\", SET_MEMORY_LIMIT,\n                                     output.trim());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/contrib/streaming/src/test/org/apache/hadoop/streaming/TestUlimit.java",
                "sha": "854507946d865e16924d1c3f71979990b9b12fd2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n \n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n@@ -217,7 +218,7 @@ private String launchWordCount(JobConf conf,\n     conf.setNumMapTasks(numMaps);\n     conf.setNumReduceTasks(numReduces);\n     JobClient.runJob(conf);\n-    return TestMiniMRWithDFS.readOutput(outDir, conf);\n+    return MapReduceTestUtil.readOutput(outDir, conf);\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/NotificationTestCase.java",
                "sha": "83eed74993dc4259ef593b89be2a3e8f28a5ade8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.hadoop.fs.*;\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.mapred.lib.*;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionHelper;\n import org.apache.hadoop.mapreduce.lib.fieldsel.TestMRFieldSelection;\n \n@@ -86,7 +87,7 @@ public static void launch() throws Exception {\n     //\n     boolean success = true;\n     Path outPath = new Path(OUTPUT_DIR, \"part-00000\");\n-    String outdata = TestMiniMRWithDFS.readOutput(outPath,job);\n+    String outdata = MapReduceTestUtil.readOutput(outPath,job);\n \n     assertEquals(expectedOutput.toString(),outdata);\n     fs.delete(OUTPUT_DIR, true);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestFieldSelection.java",
                "sha": "239c239230e5a836902fb0b1e380ec2542e7c49a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.IntWritable;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n \n /**\n@@ -93,7 +94,7 @@ public static TestResult launchWordCount(JobConf conf,\n     // Check if the Job Tracker system dir is propogated to client\n     assertFalse(sysDir.contains(\"/tmp/subru/mapred/system\"));\n     assertTrue(sysDir.contains(\"custom\"));\n-    return new TestResult(job, TestMiniMRWithDFS.readOutput(outDir, conf));\n+    return new TestResult(job, MapReduceTestUtil.readOutput(outDir, conf));\n   }\n \n  static void runWordCount(MiniMRCluster mr, JobConf jobConf, String sysDir) ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestJobSysDirWithDFS.java",
                "sha": "b32d5263c95e455eb1206a209b677c9c22c71d81",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java",
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.hadoop.io.WritableUtils;\n import org.apache.hadoop.mapred.MRCaching.TestResult;\n import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.TaskCounter;\n import org.apache.hadoop.mapreduce.TestMapReduceLocal;\n import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n@@ -125,7 +126,7 @@ private void runCustomFormats(MiniMRCluster mr) throws IOException {\n     try {\n       JobClient.runJob(job);\n       String result = \n-        TestMiniMRWithDFS.readOutput(outDir, job);\n+        MapReduceTestUtil.readOutput(outDir, job);\n       assertEquals(\"output\", (\"aunt annie\\t1\\n\" +\n                               \"bumble boat\\t4\\n\" +\n                               \"crocodile pants\\t0\\n\" +",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRLocalFS.java",
                "sha": "2788a373402071023400e964d8b331a6dad0ba59",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 26,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.io.IntWritable;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapreduce.MRConfig;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.TaskCounter;\n import org.apache.hadoop.mapreduce.TaskType;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -97,32 +98,7 @@ public static TestResult launchWordCount(JobConf conf,\n     conf.setNumMapTasks(numMaps);\n     conf.setNumReduceTasks(numReduces);\n     RunningJob job = JobClient.runJob(conf);\n-    return new TestResult(job, readOutput(outDir, conf));\n-  }\n-\n-  public static String readOutput(Path outDir, \n-                                  JobConf conf) throws IOException {\n-    FileSystem fs = outDir.getFileSystem(conf);\n-    StringBuffer result = new StringBuffer();\n-    {\n-      \n-      Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,\n-                                   new Utils.OutputFileUtils\n-                                            .OutputFilesFilter()));\n-      for(int i=0; i < fileList.length; ++i) {\n-        LOG.info(\"File list[\" + i + \"]\" + \": \"+ fileList[i]);\n-        BufferedReader file = \n-          new BufferedReader(new InputStreamReader(fs.open(fileList[i])));\n-        String line = file.readLine();\n-        while (line != null) {\n-          result.append(line);\n-          result.append(\"\\n\");\n-          line = file.readLine();\n-        }\n-        file.close();\n-      }\n-    }\n-    return result.toString();\n+    return new TestResult(job, MapReduceTestUtil.readOutput(outDir, conf));\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/TestMiniMRWithDFS.java",
                "sha": "4b90a675b56828e1a3d2a58007973686b814afa6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java",
                "patch": "@@ -21,6 +21,8 @@\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.mapred.*;\n import org.apache.hadoop.mapred.lib.*;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n+\n import junit.framework.TestCase;\n import java.io.*;\n import java.util.*;\n@@ -107,7 +109,7 @@ public static void launch() throws Exception {\n     //\n     boolean success = true;\n     Path outPath = new Path(OUTPUT_DIR, \"part-00000\");\n-    String outdata = TestMiniMRWithDFS.readOutput(outPath,job);\n+    String outdata = MapReduceTestUtil.readOutput(outPath,job);\n     System.out.println(\"full out data:\");\n     System.out.println(outdata.toString());\n     outdata = outdata.substring(0, expectedOutput.toString().length());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/lib/aggregate/TestAggregates.java",
                "sha": "6da96ce22bd052d23e9cc397056db8282e14f4af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 2,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java",
                "patch": "@@ -38,9 +38,9 @@\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MiniMRCluster;\n import org.apache.hadoop.mapred.RunningJob;\n-import org.apache.hadoop.mapred.TestMiniMRWithDFS;\n import org.apache.hadoop.mapred.Utils;\n import org.apache.hadoop.mapred.Counters.Counter;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -199,7 +199,7 @@ static void runProgram(MiniMRCluster mr, MiniDFSCluster dfs,\n     for (Path p:FileUtil.stat2Paths(dfs.getFileSystem().listStatus(outputPath,\n     \t\t                        new Utils.OutputFileUtils\n     \t\t                                 .OutputFilesFilter()))) {\n-      results.add(TestMiniMRWithDFS.readOutput(p, job));\n+      results.add(MapReduceTestUtil.readOutput(p, job));\n     }\n     assertEquals(\"number of reduces is wrong\", \n                  expectedResults.length, results.size());",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapred/pipes/TestPipes.java",
                "sha": "2df117b107891ae50b99f1fd642ea89fcc298b2e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 1,
                "filename": "src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java",
                "patch": "@@ -396,7 +396,8 @@ public Counter getCounter(String group, String name) {\n       }\n     };\n   }\n-  \n+\n+  // Return output of MR job by reading from the given output directory\n   public static String readOutput(Path outDir, Configuration conf) \n       throws IOException {\n     FileSystem fs = outDir.getFileSystem(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/MapReduceTestUtil.java",
                "sha": "65462b9ca66ed1161086e0eeaee2c1b792dbbf53",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java?ref=c2e41ceb1541ce550119256a82661c704f54e4df",
                "deletions": 21,
                "filename": "src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.mapred.Utils;\n import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.MapReduceTestUtil;\n import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n@@ -112,7 +113,7 @@ public static void launch() throws Exception {\n     // original one.  Remember, we need to ignore zero-count items\n     // in the original key.\n     //\n-    String outdata = readOutput(OUTPUT_DIR, conf);\n+    String outdata = MapReduceTestUtil.readOutput(OUTPUT_DIR, conf);\n     System.out.println(\"full out data:\");\n     System.out.println(outdata.toString());\n     outdata = outdata.substring(0, expectedOutput.toString().length());\n@@ -121,26 +122,6 @@ public static void launch() throws Exception {\n     fs.delete(OUTPUT_DIR, true);\n     fs.delete(INPUT_DIR, true);\n   }\n-\n-  public static String readOutput(Path outDir, Configuration conf) \n-    throws IOException {\n-    FileSystem fs = outDir.getFileSystem(conf);\n-    StringBuffer result = new StringBuffer();\n-    Path[] fileList = FileUtil.stat2Paths(fs.listStatus(outDir,\n-                        new Utils.OutputFileUtils.OutputFilesFilter()));\n-    for(int i=0; i < fileList.length; ++i) {\n-      BufferedReader file = \n-        new BufferedReader(new InputStreamReader(fs.open(fileList[i])));\n-      String line = file.readLine();\n-      while (line != null) {\n-        result.append(line);\n-        result.append(\"\\n\");\n-        line = file.readLine();\n-      }\n-      file.close();\n-    }\n-    return result.toString();\n-  }\n   \n   /**\n    * Launches all the tasks in order.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2e41ceb1541ce550119256a82661c704f54e4df/src/test/mapred/org/apache/hadoop/mapreduce/lib/aggregate/TestMapReduceAggregates.java",
                "sha": "f24dffe2655c6129377ae953d3f50eafa03581bf",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1813. NPE in PipeMapred.MRErrorThread. Contributed by Ravi Gummadi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@953670 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/2fb120dd1326b2b28c346f36d6097ee1dbe22d68",
        "patched_files": [
            "PipeMapRed.java",
            "NotificationTestCase.java",
            "PipeMapRunner.java",
            "MapReduceTestUtil.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestMiniMRWithDFS.java",
            "TestUlimit.java",
            "TestMapReduceAggregates.java",
            "TestStreamingTaskLog.java",
            "TestPipes.java",
            "TestAggregates.java",
            "TestStreamingStatus.java",
            "TestStreamingEmptyInpNonemptyOut.java",
            "TestMiniMRLocalFS.java",
            "TestFieldSelection.java",
            "TestJobSysDirWithDFS.java"
        ]
    },
    "hadoop-common_c2f3e11": {
        "bug_id": "hadoop-common_c2f3e11",
        "commit": "https://github.com/apache/hadoop-common/commit/c2f3e11399ff6810b16491c01b96dbcb9279eee9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -241,6 +241,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-1388. Move the HDFS RAID package from HDFS to MAPREDUCE.\n     (Eli Collins via dhruba)\n \n+    MAPREDUCE-1313. Fix NPE in Sqoop when table with null fields uses escape\n+    during import. (Aaron Kimball via cdouglas)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/CHANGES.txt",
                "sha": "08688d0e4c2579b084284068677dad6da40f2333",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "deletions": 0,
                "filename": "src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "patch": "@@ -57,6 +57,10 @@ public static final String escapeAndEnclose(String str, String escape, String en\n     boolean escapingLegal = (null != escape && escape.length() > 0 && !escape.equals(\"\\000\"));\n     String withEscapes;\n \n+    if (null == str) {\n+      return null;\n+    }\n+\n     if (escapingLegal) {\n       // escaping is legal. Escape any instances of the escape char itself\n       withEscapes = str.replace(escape, escape + escape);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/java/org/apache/hadoop/sqoop/lib/FieldFormatter.java",
                "sha": "a1b6742133b49da13448a6a94d4b7cc87c2bf9cb",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "deletions": 2,
                "filename": "src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "patch": "@@ -58,6 +58,8 @@\n     args.add(HsqldbTestServer.getUrl());\n     args.add(\"--num-mappers\");\n     args.add(\"1\");\n+    args.add(\"--escaped-by\");\n+    args.add(\"\\\\\");\n \n     return args.toArray(new String[0]);\n   }\n@@ -86,9 +88,18 @@ public void setUp() {\n     // create two tables.\n     this.expectedStrings.add(\"A winner\");\n     this.expectedStrings.add(\"is you!\");\n+    this.expectedStrings.add(null);\n \n+    int i = 0;\n     for (String expectedStr: this.expectedStrings) {\n-      this.createTableForColType(\"VARCHAR(32) PRIMARY KEY\", \"'\" + expectedStr + \"'\");\n+      String wrappedStr = null;\n+      if (expectedStr != null) {\n+        wrappedStr = \"'\" + expectedStr + \"'\";\n+      }\n+\n+      String [] types = { \"INT NOT NULL PRIMARY KEY\", \"VARCHAR(32)\" };\n+      String [] vals = { Integer.toString(i++) , wrappedStr };\n+      this.createTableWithColTypes(types, vals);\n       this.tableNames.add(this.getTableName());\n       this.removeTableDir();\n       incrementTableNum();\n@@ -100,13 +111,15 @@ public void testMultiTableImport() throws IOException {\n     runImport(argv);\n \n     Path warehousePath = new Path(this.getWarehouseDir());\n+    int i = 0;\n     for (String tableName : this.tableNames) {\n       Path tablePath = new Path(warehousePath, tableName);\n       Path filePath = new Path(tablePath, \"part-m-00000\");\n \n       // dequeue the expected value for this table. This\n       // list has the same order as the tableNames list.\n-      String expectedVal = this.expectedStrings.get(0);\n+      String expectedVal = Integer.toString(i++) + \",\"\n+          + this.expectedStrings.get(0);\n       this.expectedStrings.remove(0);\n \n       BufferedReader reader = new BufferedReader(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/TestAllTables.java",
                "sha": "31c5167d4e0d5523ca5c903e6fc5f620d72868e9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java?ref=c2f3e11399ff6810b16491c01b96dbcb9279eee9",
                "deletions": 0,
                "filename": "src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "patch": "@@ -37,6 +37,10 @@ public void testAllEmpty() {\n   public void testNullArgs() {\n     String result = FieldFormatter.escapeAndEnclose(\"\", null, null, null, false);\n     assertEquals(\"\", result);\n+\n+    char [] encloseFor = { '\\\"' };\n+    assertNull(FieldFormatter.escapeAndEnclose(null, \"\\\\\", \"\\\"\", encloseFor,\n+        false));\n   }\n \n   public void testBasicStr() {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c2f3e11399ff6810b16491c01b96dbcb9279eee9/src/contrib/sqoop/src/test/org/apache/hadoop/sqoop/lib/TestFieldFormatter.java",
                "sha": "661a9ac98bb52a2dac6de286bb35a63c81ea1c78",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1313. Fix NPE in Sqoop when table with null fields uses escape\nduring import. Contributed by Aaron Kimball\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@902674 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/385b5e06b5121ae939a3e340e396bc1fa3472309",
        "patched_files": [
            "FieldFormatter.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFieldFormatter.java",
            "TestAllTables.java"
        ]
    },
    "hadoop-common_c6dcdf6": {
        "bug_id": "hadoop-common_c6dcdf6",
        "commit": "https://github.com/apache/hadoop-common/commit/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -488,6 +488,8 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n+    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "30d61c92e5b840b8c6a48e79d0ab2a3a58d1075e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -538,7 +538,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if(UserGroupInformation.isSecurityEnabled()) {\n+    if (UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -556,7 +556,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.getNamesystem().verifyToken(id, token.getPassword());\n+        nn.verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "39aa8db16d1c8139107767c3529deef7a1900cea",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 11,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5460,20 +5460,10 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-  \n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n+\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "88716a5ecdb890136bec739c7e5fdfc5501c42c3",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -78,6 +79,7 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n+import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1283,7 +1285,18 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t.getMessage());\n   }\n-  \n+\n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+\n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "2d5a90a8adb10e085161687ad7a661a74e29ea38",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "f00bb9c40a12d05de8d69885452b4d02a1604419",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=c6dcdf626c69a5213de4c03bcab64e2c2b5676c3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n+import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -69,6 +70,7 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n+    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -79,6 +81,8 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n+    when(context.getAttribute(\n+        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop-common/raw/c6dcdf626c69a5213de4c03bcab64e2c2b5676c3/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "8dad3b33e6fd65d7eefcf83507b93dd5257c86a8",
                "status": "modified"
            }
        ],
        "message": "HDFS-3654. TestJspHelper#testGetUgi fails with NPE. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1361463 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ebd86f6cecbc6a9200aaef3e47537c2ab8ff09aa",
        "patched_files": [
            "CHANGES.txt",
            "NameNode.java",
            "NameNodeHttpServer.java",
            "JspHelper.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java",
            "TestJspHelper.java"
        ]
    },
    "hadoop-common_ce96b98": {
        "bug_id": "hadoop-common_ce96b98",
        "commit": "https://github.com/apache/hadoop-common/commit/ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -74,3 +74,5 @@ Release 0.23.3 - Unreleased\n     YARN-63. RMNodeImpl is missing valid transitions from the UNHEALTHY state\n     (Jason Lowe via bobby)\n \n+    YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and\n+    thus causes all containers to be rejected. (vinodkv)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/CHANGES.txt",
                "sha": "8284215a98abec90778697f810fabb528d00e125",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "patch": "@@ -18,11 +18,14 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords;\n \n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n \n-\n public interface NodeHeartbeatRequest {\n-  public abstract NodeStatus getNodeStatus();\n-  \n-  public abstract void setNodeStatus(NodeStatus status);\n+\n+  NodeStatus getNodeStatus();\n+  void setNodeStatus(NodeStatus status);\n+\n+  MasterKey getLastKnownMasterKey();\n+  void setLastKnownMasterKey(MasterKey secretKey);\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "sha": "9e69680d87f6ed2198fd764ac2b9437dc80194fb",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "patch": "@@ -18,24 +18,25 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb;\n \n-\n import org.apache.hadoop.yarn.api.records.ProtoBase;\n+import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProtoOrBuilder;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl;\n import org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl;\n \n-\n-    \n-public class NodeHeartbeatRequestPBImpl extends ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n+public class NodeHeartbeatRequestPBImpl extends\n+    ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n   NodeHeartbeatRequestProto proto = NodeHeartbeatRequestProto.getDefaultInstance();\n   NodeHeartbeatRequestProto.Builder builder = null;\n   boolean viaProto = false;\n   \n   private NodeStatus nodeStatus = null;\n-  \n+  private MasterKey lastKnownMasterKey = null;\n   \n   public NodeHeartbeatRequestPBImpl() {\n     builder = NodeHeartbeatRequestProto.newBuilder();\n@@ -57,6 +58,10 @@ private void mergeLocalToBuilder() {\n     if (this.nodeStatus != null) {\n       builder.setNodeStatus(convertToProtoFormat(this.nodeStatus));\n     }\n+    if (this.lastKnownMasterKey != null) {\n+      builder\n+        .setLastKnownMasterKey(convertToProtoFormat(this.lastKnownMasterKey));\n+    }\n   }\n \n   private void mergeLocalToProto() {\n@@ -96,6 +101,27 @@ public void setNodeStatus(NodeStatus nodeStatus) {\n     this.nodeStatus = nodeStatus;\n   }\n \n+  @Override\n+  public MasterKey getLastKnownMasterKey() {\n+    NodeHeartbeatRequestProtoOrBuilder p = viaProto ? proto : builder;\n+    if (this.lastKnownMasterKey != null) {\n+      return this.lastKnownMasterKey;\n+    }\n+    if (!p.hasLastKnownMasterKey()) {\n+      return null;\n+    }\n+    this.lastKnownMasterKey = convertFromProtoFormat(p.getLastKnownMasterKey());\n+    return this.lastKnownMasterKey;\n+  }\n+\n+  @Override\n+  public void setLastKnownMasterKey(MasterKey masterKey) {\n+    maybeInitBuilder();\n+    if (masterKey == null) \n+      builder.clearLastKnownMasterKey();\n+    this.lastKnownMasterKey = masterKey;\n+  }\n+\n   private NodeStatusPBImpl convertFromProtoFormat(NodeStatusProto p) {\n     return new NodeStatusPBImpl(p);\n   }\n@@ -104,6 +130,11 @@ private NodeStatusProto convertToProtoFormat(NodeStatus t) {\n     return ((NodeStatusPBImpl)t).getProto();\n   }\n \n+  private MasterKeyPBImpl convertFromProtoFormat(MasterKeyProto p) {\n+    return new MasterKeyPBImpl(p);\n+  }\n \n-\n+  private MasterKeyProto convertToProtoFormat(MasterKey t) {\n+    return ((MasterKeyPBImpl)t).getProto();\n+  }\n }  ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "sha": "8fcf7f2c147a901117435dfd09262655f2bc75fc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "patch": "@@ -35,6 +35,7 @@ message RegisterNodeManagerResponseProto {\n \n message NodeHeartbeatRequestProto {\n   optional NodeStatusProto node_status = 1;\n+  optional MasterKeyProto last_known_master_key = 2;\n }\n \n message NodeHeartbeatResponseProto {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "sha": "e4d82c75d61e56c087f51bd90a52da149de00398",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.avro.AvroRuntimeException;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.YarnException;\n@@ -111,10 +112,7 @@ public synchronized void init(Configuration conf) {\n     this.totalResource = recordFactory.newRecordInstance(Resource.class);\n     this.totalResource.setMemory(memoryMb);\n     metrics.addResource(totalResource);\n-    this.tokenKeepAliveEnabled =\n-        conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n-            YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n-            && isSecurityEnabled();\n+    this.tokenKeepAliveEnabled = isTokenKeepAliveEnabled(conf);\n     this.tokenRemovalDelayMs =\n         conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n             YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n@@ -163,10 +161,17 @@ synchronized boolean hasToRebootNode() {\n     return this.hasToRebootNode;\n   }\n \n-  protected boolean isSecurityEnabled() {\n+  private boolean isSecurityEnabled() {\n     return UserGroupInformation.isSecurityEnabled();\n   }\n \n+  @Private\n+  protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n+    return conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n+        YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n+        && isSecurityEnabled();\n+  }\n+\n   protected ResourceTracker getRMClient() {\n     Configuration conf = getConfig();\n     YarnRPC rpc = YarnRPC.create(conf);\n@@ -321,7 +326,11 @@ public void run() {\n             \n             NodeHeartbeatRequest request = recordFactory\n                 .newRecordInstance(NodeHeartbeatRequest.class);\n-            request.setNodeStatus(nodeStatus);            \n+            request.setNodeStatus(nodeStatus);\n+            if (isSecurityEnabled()) {\n+              request.setLastKnownMasterKey(NodeStatusUpdaterImpl.this.context\n+                .getContainerTokenSecretManager().getCurrentKey());\n+            }\n             HeartbeatResponse response =\n               resourceTracker.nodeHeartbeat(request).getHeartbeatResponse();\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "sha": "819e22d2146ae18083ee7ee7609b7678351c8407",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "patch": "@@ -92,7 +92,6 @@ public synchronized void setMasterKey(MasterKey masterKeyRecord) {\n         containerId.getApplicationAttemptId().getApplicationId();\n \n     MasterKeyData masterKeyToUse = null;\n-\n     if (this.previousMasterKey != null\n         && keyId == this.previousMasterKey.getMasterKey().getKeyId()) {\n       // A container-launch has come in with a token generated off the last",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "sha": "bc70f26a07e93e1bbc53e26d7f8fbd938348a99f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "patch": "@@ -261,7 +261,7 @@ protected ResourceTracker getRMClient() {\n     }\n     \n     @Override\n-    protected boolean isSecurityEnabled() {\n+    protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n       return true;\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "sha": "41d171f97c034a2be50743f9d9887121ac3644af",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -159,7 +159,7 @@ public synchronized void init(Configuration conf) {\n     DelegationTokenRenewer tokenRenewer = createDelegationTokenRenewer();\n     addService(tokenRenewer);\n \n-    this.containerTokenSecretManager = new RMContainerTokenSecretManager(conf);\n+    this.containerTokenSecretManager = createContainerTokenSecretManager(conf);\n     \n     this.rmContext =\n         new RMContextImpl(this.store, this.rmDispatcher,\n@@ -231,6 +231,11 @@ public synchronized void init(Configuration conf) {\n     super.init(conf);\n   }\n \n+  protected RMContainerTokenSecretManager createContainerTokenSecretManager(\n+      Configuration conf) {\n+    return new RMContainerTokenSecretManager(conf);\n+  }\n+\n   protected EventHandler<SchedulerEvent> createSchedulerEventDispatcher() {\n     return new SchedulerEventDispatcher(this.scheduler);\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "e9e5340b80cbd23cc817c6a9708bf2dc3e82f614",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -169,14 +169,14 @@ public RegisterNodeManagerResponse registerNodeManager(\n       return response;\n     }\n \n-    MasterKey nextMasterKeyForNode = null;\n     if (isSecurityEnabled()) {\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getCurrentKey();\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getCurrentKey();\n       regResponse.setMasterKey(nextMasterKeyForNode);\n     }\n \n     RMNode rmNode = new RMNodeImpl(nodeId, rmContext, host, cmPort, httpPort,\n-        resolve(host), capability, nextMasterKeyForNode);\n+        resolve(host), capability);\n \n     RMNode oldNode = this.rmContext.getRMNodes().putIfAbsent(nodeId, rmNode);\n     if (oldNode == null) {\n@@ -266,17 +266,18 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     latestResponse.addAllApplicationsToCleanup(rmNode.getAppsToCleanup());\n     latestResponse.setNodeAction(NodeAction.NORMAL);\n \n-    MasterKey nextMasterKeyForNode = null;\n-\n     // Check if node's masterKey needs to be updated and if the currentKey has\n     // roller over, send it across\n     if (isSecurityEnabled()) {\n+\n       boolean shouldSendMasterKey = false;\n-      MasterKey nodeKnownMasterKey = rmNode.getCurrentMasterKey();\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getNextKey();\n+\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getNextKey();\n       if (nextMasterKeyForNode != null) {\n         // nextMasterKeyForNode can be null if there is no outstanding key that\n         // is in the activation period.\n+        MasterKey nodeKnownMasterKey = request.getLastKnownMasterKey();\n         if (nodeKnownMasterKey.getKeyId() != nextMasterKeyForNode.getKeyId()) {\n           shouldSendMasterKey = true;\n         }\n@@ -290,8 +291,7 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     this.rmContext.getDispatcher().getEventHandler().handle(\n         new RMNodeStatusEvent(nodeId, remoteNodeStatus.getNodeHealthStatus(),\n             remoteNodeStatus.getContainersStatuses(), \n-            remoteNodeStatus.getKeepAliveApplications(), latestResponse,\n-            nextMasterKeyForNode));\n+            remoteNodeStatus.getKeepAliveApplications(), latestResponse));\n \n     nodeHeartBeatResponse.setHeartbeatResponse(latestResponse);\n     return nodeHeartBeatResponse;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "ed4a021b0db631102cbc075c9d961020362aea4e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n /**\n  * Node managers information on available resources \n@@ -107,6 +106,4 @@\n   public List<ApplicationId> getAppsToCleanup();\n \n   public HeartbeatResponse getLastHeartBeatResponse();\n-  \n-  public MasterKey getCurrentMasterKey();\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "sha": "aafa3dbdefeaf585d509d3b0e3cc4dd2d56df23b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 19,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType;\n@@ -105,8 +104,6 @@\n   private HeartbeatResponse latestHeartBeatResponse = recordFactory\n       .newRecordInstance(HeartbeatResponse.class);\n   \n-  private MasterKey currentMasterKey;\n-\n   private static final StateMachineFactory<RMNodeImpl,\n                                            NodeState,\n                                            RMNodeEventType,\n@@ -167,8 +164,7 @@\n                              RMNodeEvent> stateMachine;\n \n   public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n-      int cmPort, int httpPort, Node node, Resource capability,\n-      MasterKey masterKey) {\n+      int cmPort, int httpPort, Node node, Resource capability) {\n     this.nodeId = nodeId;\n     this.context = context;\n     this.hostName = hostName;\n@@ -178,7 +174,6 @@ public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n     this.nodeAddress = hostName + \":\" + cmPort;\n     this.httpAddress = hostName + \":\" + httpPort;\n     this.node = node;\n-    this.currentMasterKey = masterKey;\n     this.nodeHealthStatus.setIsNodeHealthy(true);\n     this.nodeHealthStatus.setHealthReport(\"Healthy\");\n     this.nodeHealthStatus.setLastHealthReportTime(System.currentTimeMillis());\n@@ -312,17 +307,6 @@ public HeartbeatResponse getLastHeartBeatResponse() {\n       this.readLock.unlock();\n     }\n   }\n-  \n-  @Override\n-  public MasterKey getCurrentMasterKey() {\n-    this.readLock.lock();\n-    try {\n-      return this.currentMasterKey;\n-    } finally {\n-      this.readLock.unlock();\n-    }\n-  }\n-  \n \n   public void handle(RMNodeEvent event) {\n     LOG.debug(\"Processing \" + event.getNodeId() + \" of type \" + event.getType());\n@@ -500,7 +484,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n \n       NodeHealthStatus remoteNodeHealthStatus = \n           statusEvent.getNodeHealthStatus();\n@@ -582,7 +565,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n       NodeHealthStatus remoteNodeHealthStatus = statusEvent.getNodeHealthStatus();\n       rmNode.setNodeHealthStatus(remoteNodeHealthStatus);\n       if (remoteNodeHealthStatus.getIsNodeHealthy()) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "83833b9bdb3c4ac1e2ea1e9fac616bbfccbc09b0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "patch": "@@ -25,25 +25,22 @@\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n public class RMNodeStatusEvent extends RMNodeEvent {\n \n   private final NodeHealthStatus nodeHealthStatus;\n   private final List<ContainerStatus> containersCollection;\n   private final HeartbeatResponse latestResponse;\n   private final List<ApplicationId> keepAliveAppIds;\n-  private final MasterKey currentMasterKey;\n \n   public RMNodeStatusEvent(NodeId nodeId, NodeHealthStatus nodeHealthStatus,\n       List<ContainerStatus> collection, List<ApplicationId> keepAliveAppIds,\n-      HeartbeatResponse latestResponse, MasterKey currentMasterKey) {\n+      HeartbeatResponse latestResponse) {\n     super(nodeId, RMNodeEventType.STATUS_UPDATE);\n     this.nodeHealthStatus = nodeHealthStatus;\n     this.containersCollection = collection;\n     this.keepAliveAppIds = keepAliveAppIds;\n     this.latestResponse = latestResponse;\n-    this.currentMasterKey = currentMasterKey;\n   }\n \n   public NodeHealthStatus getNodeHealthStatus() {\n@@ -61,8 +58,4 @@ public HeartbeatResponse getLatestResponse() {\n   public List<ApplicationId> getKeepAliveAppIds() {\n     return this.keepAliveAppIds;\n   }\n-  \n-  public MasterKey getCurrentMasterKey() {\n-    return this.currentMasterKey;\n-  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "sha": "1285c2bed99d979c34b86cd59238b75bceb41aa8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "patch": "@@ -89,14 +89,17 @@ public void stop() {\n    * Creates a new master-key and sets it as the primary.\n    */\n   @Private\n-  protected void rollMasterKey() {\n+  public void rollMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Rolling master-key for container-tokens\");\n       if (this.currentMasterKey == null) { // Setting up for the first time.\n         this.currentMasterKey = createNewMasterKey();\n       } else {\n         this.nextMasterKey = createNewMasterKey();\n+        LOG.info(\"Going to activate master-key with key-id \"\n+            + this.nextMasterKey.getMasterKey().getKeyId() + \" in \"\n+            + this.activationDelay + \"ms\");\n         this.timer.schedule(new NextKeyActivator(), this.activationDelay);\n       }\n     } finally {\n@@ -122,7 +125,7 @@ public MasterKey getNextKey() {\n    * Activate the new master-key\n    */\n   @Private\n-  protected void activateNextMasterKey() {\n+  public void activateNextMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Activating next master key with id: \"",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "sha": "cc4ccd7e1ebd863edaae6f6f488c0341e520e4df",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -35,7 +35,9 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n import org.apache.hadoop.yarn.util.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n \n@@ -46,8 +48,9 @@\n   private final int memory;\n   private final ResourceTrackerService resourceTracker;\n   private final int httpPort = 2;\n+  private MasterKey currentMasterKey;\n \n-  MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n+  public MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n     this.memory = memory;\n     this.resourceTracker = resourceTracker;\n     String[] splits = nodeIdStr.split(\":\");\n@@ -72,21 +75,23 @@ public void containerStatus(Container container) throws Exception {\n     nodeHeartbeat(conts, true);\n   }\n \n-  public NodeId registerNode() throws Exception {\n+  public RegistrationResponse registerNode() throws Exception {\n     RegisterNodeManagerRequest req = Records.newRecord(\n         RegisterNodeManagerRequest.class);\n     req.setNodeId(nodeId);\n     req.setHttpPort(httpPort);\n     Resource resource = Records.newRecord(Resource.class);\n     resource.setMemory(memory);\n     req.setResource(resource);\n-    resourceTracker.registerNodeManager(req);\n-    return nodeId;\n+    RegistrationResponse registrationResponse =\n+        resourceTracker.registerNodeManager(req).getRegistrationResponse();\n+    this.currentMasterKey = registrationResponse.getMasterKey();\n+    return registrationResponse;\n   }\n \n-  public HeartbeatResponse nodeHeartbeat(boolean b) throws Exception {\n+  public HeartbeatResponse nodeHeartbeat(boolean isHealthy) throws Exception {\n     return nodeHeartbeat(new HashMap<ApplicationId, List<ContainerStatus>>(),\n-        b, ++responseId);\n+        isHealthy, ++responseId);\n   }\n \n   public HeartbeatResponse nodeHeartbeat(ApplicationAttemptId attemptId,\n@@ -123,7 +128,15 @@ public HeartbeatResponse nodeHeartbeat(Map<ApplicationId,\n     healthStatus.setLastHealthReportTime(1);\n     status.setNodeHealthStatus(healthStatus);\n     req.setNodeStatus(status);\n-    return resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    req.setLastKnownMasterKey(this.currentMasterKey);\n+    HeartbeatResponse heartbeatResponse =\n+        resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    MasterKey masterKeyFromRM = heartbeatResponse.getMasterKey();\n+    this.currentMasterKey =\n+        (masterKeyFromRM != null\n+            && masterKeyFromRM.getKeyId() != this.currentMasterKey.getKeyId()\n+            ? masterKeyFromRM : this.currentMasterKey);\n+    return heartbeatResponse;\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "ba999bfb2e094b837d3cb83cc890548543858186",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "patch": "@@ -25,12 +25,11 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.NodeState;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n \n import com.google.common.collect.Lists;\n@@ -188,11 +187,6 @@ public NodeState getState() {\n     public HeartbeatResponse getLastHeartBeatResponse() {\n       return null;\n     }\n-\n-    @Override\n-    public MasterKey getCurrentMasterKey() {\n-      return null;\n-    }\n   };\n \n   private static RMNode buildRMNode(int rack, final Resource perNode, NodeState state, String httpAddr) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "sha": "0c56a27ada03143e36ffac9c39f469e27807369e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "patch": "@@ -105,7 +105,7 @@ public Void answer(InvocationOnMock invocation) throws Throwable {\n         new TestSchedulerEventDispatcher());\n     \n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n-    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);\n+    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null);\n \n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "sha": "2b2decccb6bc4d6d2bb0e3f3b6217ef8d23786fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.w3c.dom.Element;\n import org.w3c.dom.NodeList;\n import org.xml.sax.InputSource;\n+\n import com.google.inject.Guice;\n import com.google.inject.Injector;\n import com.google.inject.servlet.GuiceServletContextListener;\n@@ -145,7 +146,7 @@ public void testNodesDefaultWithUnHealthyNode() throws JSONException,\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm3.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm3.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response =\n@@ -360,7 +361,7 @@ public void testNodesQueryHealthyAndState() throws JSONException, Exception {\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm1.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm1.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"cluster\")",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "sha": "084dcffe4cbb20c550e21ca1452c6a0a904decd9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "patch": "@@ -44,6 +44,12 @@\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n \n   <build>",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "sha": "600c647f9f775e49fb4a4e8e1078435449002a38",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.ipc.RPC;\n-import org.apache.hadoop.ipc.RemoteException;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.SecurityUtil;\n@@ -222,7 +221,7 @@ public void testMaliceUser() throws IOException, InterruptedException {\n     Resource modifiedResource = BuilderUtils.newResource(2048);\n     ContainerTokenIdentifier modifiedIdentifier = new ContainerTokenIdentifier(\n         dummyIdentifier.getContainerID(), dummyIdentifier.getNmHostAddress(),\n-        modifiedResource, Long.MAX_VALUE, 0);\n+        modifiedResource, Long.MAX_VALUE, dummyIdentifier.getMasterKeyId());\n     Token<ContainerTokenIdentifier> modifiedToken = new Token<ContainerTokenIdentifier>(\n         modifiedIdentifier.getBytes(), containerToken.getPassword().array(),\n         new Text(containerToken.getKind()), new Text(containerToken\n@@ -250,19 +249,14 @@ public Void run() {\n               + \"it will indicate RPC success\");\n         } catch (Exception e) {\n           Assert.assertEquals(\n-              java.lang.reflect.UndeclaredThrowableException.class\n-                  .getCanonicalName(), e.getClass().getCanonicalName());\n-          Assert.assertEquals(RemoteException.class.getCanonicalName(), e\n-            .getCause().getClass().getCanonicalName());\n-          Assert.assertEquals(\n-            \"org.apache.hadoop.security.token.SecretManager$InvalidToken\",\n-            ((RemoteException) e.getCause()).getClassName());\n+            java.lang.reflect.UndeclaredThrowableException.class\n+              .getCanonicalName(), e.getClass().getCanonicalName());\n           Assert.assertTrue(e\n             .getCause()\n             .getMessage()\n-            .matches(\n-              \"Given Container container_\\\\d*_\\\\d*_\\\\d\\\\d_\\\\d*\"\n-                  + \" seems to have an illegally generated token.\"));\n+            .contains(\n+              \"DIGEST-MD5: digest response format violation. \"\n+                  + \"Mismatched response.\"));\n         }\n         return null;\n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "sha": "1c7933ae275e20cc592feac9b26d687c3f2c8724",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java?ref=ce96b9873b9caaed94853d17f7df2be6d7efc7b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+* Licensed to the Apache Software Foundation (ASF) under one\n+* or more contributor license agreements.  See the NOTICE file\n+* distributed with this work for additional information\n+* regarding copyright ownership.  The ASF licenses this file\n+* to you under the Apache License, Version 2.0 (the\n+* \"License\"); you may not use this file except in compliance\n+* with the License.  You may obtain a copy of the License at\n+*\n+*     http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.hadoop.yarn.server;\n+\n+import java.io.IOException;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.event.Dispatcher;\n+import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNM;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n+import org.junit.Test;\n+\n+public class TestRMNMSecretKeys {\n+\n+  @Test\n+  public void testNMUpdation() throws Exception {\n+    YarnConfiguration conf = new YarnConfiguration();\n+    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,\n+      \"kerberos\");\n+    UserGroupInformation.setConfiguration(conf);\n+    // Default rolling and activation intervals are large enough, no need to\n+    // intervene\n+\n+    final DrainDispatcher dispatcher = new DrainDispatcher();\n+    ResourceManager rm = new ResourceManager(null) {\n+      @Override\n+      protected void doSecureLogin() throws IOException {\n+        // Do nothing.\n+      }\n+\n+      @Override\n+      protected Dispatcher createDispatcher() {\n+        return dispatcher;\n+      }\n+    };\n+    rm.init(conf);\n+    rm.start();\n+\n+    MockNM nm = new MockNM(\"host:1234\", 3072, rm.getResourceTrackerService());\n+    RegistrationResponse registrationResponse = nm.registerNode();\n+    MasterKey masterKey = registrationResponse.getMasterKey();\n+    Assert.assertNotNull(\"Registration should cause a key-update!\", masterKey);\n+    dispatcher.await();\n+\n+    HeartbeatResponse response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"First heartbeat after registration shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert\n+      .assertNull(\n+        \"Even second heartbeat after registration shouldn't get any key updates!\",\n+        response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force a roll-over\n+    RMContainerTokenSecretManager secretManager =\n+        rm.getRMContainerTokenSecretManager();\n+    secretManager.rollMasterKey();\n+\n+    // Heartbeats after roll-over and before activation should be fine.\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNotNull(\n+      \"Heartbeats after roll-over and before activation should not err out.\",\n+      response.getMasterKey());\n+    Assert.assertEquals(\n+      \"Roll-over should have incremented the key-id only by one!\",\n+      masterKey.getKeyId() + 1, response.getMasterKey().getKeyId());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Second heartbeat after roll-over shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force activation\n+    secretManager.activateNextMasterKey();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\"Activation shouldn't cause any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Even second heartbeat after activation shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    rm.stop();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ce96b9873b9caaed94853d17f7df2be6d7efc7b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "sha": "9b6024ce3c0621706b5ffce30d59fb58947ed517",
                "status": "added"
            }
        ],
        "message": "YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and thus causes all containers to be rejected. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379550 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/99c9057d2ca9b4ac2a0325f6ac618792dd170071",
        "patched_files": [
            "pom.xml",
            "NodeStatusUpdater.java",
            "CHANGES.txt",
            "yarn_server_common_service_protos.proto",
            "NMContainerTokenSecretManager.java",
            "RMContainerTokenSecretManager.java",
            "NodeHeartbeatRequestPBImpl.java",
            "RMNode.java",
            "NodeStatusUpdaterImpl.java",
            "RMNodeImpl.java",
            "RMNodeStatusEvent.java",
            "MockNM.java",
            "NodeHeartbeatRequest.java",
            "MockNodes.java",
            "ResourceManager.java",
            "ResourceTrackerService.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestContainerManagerSecurity.java",
            "TestRMNMSecretKeys.java",
            "TestNodeStatusUpdater.java",
            "TestResourceManager.java",
            "TestRMNodeTransitions.java",
            "TestRMWebServicesNodes.java",
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop-common_d055a72": {
        "bug_id": "hadoop-common_d055a72",
        "commit": "https://github.com/apache/hadoop-common/commit/d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7322. Adding a util method in FileUtil for directory listing,\n+    avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7023. Add listCorruptFileBlocks to Filesysem. (Patrick Kling\n     via hairong)\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/CHANGES.txt",
                "sha": "b6d57b4104b866a5cfb9af40fea15f9e617741c6",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -324,7 +324,7 @@ public static boolean copy(File src,\n       if (!dstFS.mkdirs(dst)) {\n         return false;\n       }\n-      File contents[] = src.listFiles();\n+      File contents[] = listFiles(src);\n       for (int i = 0; i < contents.length; i++) {\n         copy(contents[i], dstFS, new Path(dst, contents[i].getName()),\n              deleteSource, conf);\n@@ -486,8 +486,10 @@ public static long getDU(File dir) {\n     } else {\n       size = dir.length();\n       File[] allFiles = dir.listFiles();\n-      for (int i = 0; i < allFiles.length; i++) {\n-        size = size + getDU(allFiles[i]);\n+      if(allFiles != null) {\n+         for (int i = 0; i < allFiles.length; i++) {\n+            size = size + getDU(allFiles[i]);\n+         }\n       }\n       return size;\n     }\n@@ -707,4 +709,23 @@ public static void replaceFile(File src, File target) throws IOException {\n       }\n     }\n   }\n+  \n+  /**\n+   * A wrapper for {@link File#listFiles()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#listFiles() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of files or empty list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static File[] listFiles(File dir) throws IOException {\n+    File[] files = dir.listFiles();\n+    if(files == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return files;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "537959bd7317e49342f72650c5507729f3b5ea0a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "patch": "@@ -276,7 +276,7 @@ public boolean delete(Path p, boolean recursive) throws IOException {\n     if (f.isFile()) {\n       return f.delete();\n     } else if ((!recursive) && f.isDirectory() && \n-        (f.listFiles().length != 0)) {\n+        (FileUtil.listFiles(f).length != 0)) {\n       throw new IOException(\"Directory \" + f.toString() + \" is not empty\");\n     }\n     return FileUtil.fullyDelete(f);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "sha": "63579980cd8f3ca5d4d3b1b2b6132bf54426cc61",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=d055a7269a0a2231360fa48b9647fb8d75d4e1fa",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -117,6 +117,33 @@ private void createFile(File directory, String name, String contents)\n     }\n   }\n \n+  @Test\n+  public void testListFiles() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    File[] files = FileUtil.listFiles(partitioned);\n+    Assert.assertEquals(2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.listFiles(newDir);\n+    Assert.assertEquals(0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.listFiles(newDir);\n+      Assert.fail(\"IOException expected on listFiles() for non-existent dir \"\n+      \t\t+ newDir.toString());\n+    } catch(IOException ioe) {\n+    \t//Expected an IOException\n+    }\n+  }\n+  \n   @After\n   public void tearDown() throws IOException {\n     FileUtil.fullyDelete(del);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d055a7269a0a2231360fa48b9647fb8d75d4e1fa/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "1c193db3e74118e3d496ff68eec991522f008999",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7322. Adding a util method in FileUtil for directory listing, avoid NPEs on File.listFiles(). Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1127697 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/1b97bccd81c7f855c5126f1807d7c3bf059ec83c",
        "patched_files": [
            "RawLocalFileSystem.java",
            "FileUtil.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop-common_d1e971c": {
        "bug_id": "hadoop-common_d1e971c",
        "commit": "https://github.com/apache/hadoop-common/commit/d1e971c25f7c8e4e8e7aebbb5555a57829114236",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=d1e971c25f7c8e4e8e7aebbb5555a57829114236",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -149,3 +149,5 @@ HDFS-2845. SBN should not allow browsing of the file system via web UI. (Bikas S\n HDFS-2742. HA: observed dataloss in replication stress test. (todd via eli)\n \n HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks (todd)\n+\n+HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect (Bikas Saha via todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "7a4ef27f19513e6bb421cf8ad4d79f49e29b0397",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=d1e971c25f7c8e4e8e7aebbb5555a57829114236",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -61,6 +61,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n import com.google.common.base.Joiner;\n import com.google.common.collect.Lists;\n@@ -69,6 +71,8 @@\n \n @InterfaceAudience.Private\n public class DFSUtil {\n+  private static final Log LOG = LogFactory.getLog(DFSUtil.class.getName());\n+  \n   private DFSUtil() { /* Hidden constructor */ }\n   private static final ThreadLocal<Random> RANDOM = new ThreadLocal<Random>() {\n     @Override\n@@ -935,9 +939,10 @@ private static String getNameServiceId(Configuration conf, String addressKey) {\n         try {\n           s = NetUtils.createSocketAddr(addr);\n         } catch (Exception e) {\n+          LOG.warn(\"Exception in creating socket address\", e);\n           continue;\n         }\n-        if (matcher.match(s)) {\n+        if (!s.isUnresolved() && matcher.match(s)) {\n           nameserviceId = nsId;\n           namenodeId = nnId;\n           found++;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "c9ccf9f38c7cafd26872165b4890710acc96be21",
                "status": "modified"
            }
        ],
        "message": "HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect. Contributed by Bikas Saha.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239356 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/35f474465df629f5b7f6199a2c029139922588db",
        "patched_files": [
            "CHANGES.HDFS-1623.txt",
            "DFSUtil.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop-common_d1f7fd8": {
        "bug_id": "hadoop-common_d1f7fd8",
        "commit": "https://github.com/apache/hadoop-common/commit/d1f7fd801bf00ce62df6ef065cc906d2816352e2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=d1f7fd801bf00ce62df6ef065cc906d2816352e2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -443,6 +443,9 @@ Release 2.4.0 - UNRELEASED\n     apps-killed metrics correctly for killed applications. (Varun Vasudev via\n     vinodkv)\n \n+    YARN-1821. NPE on registerNodeManager if the request has containers for \n+    UnmanagedAMs. (kasha)\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/CHANGES.txt",
                "sha": "a222fed796ef219a5784f065c33fd29eaf1ec68e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=d1f7fd801bf00ce62df6ef065cc906d2816352e2",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -210,14 +210,16 @@ public RegisterNodeManagerResponse registerNodeManager(\n             rmContext.getRMApps().get(appAttemptId.getApplicationId());\n         if (rmApp != null) {\n           RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt.getMasterContainer().getId()\n-              .equals(containerStatus.getContainerId())\n-              && containerStatus.getState() == ContainerState.COMPLETE) {\n-            // sending master container finished event.\n-            RMAppAttemptContainerFinishedEvent evt =\n-                new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                    containerStatus);\n-            rmContext.getDispatcher().getEventHandler().handle(evt);\n+          if (rmAppAttempt != null) {\n+            if (rmAppAttempt.getMasterContainer().getId()\n+                .equals(containerStatus.getContainerId())\n+                && containerStatus.getState() == ContainerState.COMPLETE) {\n+              // sending master container finished event.\n+              RMAppAttemptContainerFinishedEvent evt =\n+                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+                      containerStatus);\n+              rmContext.getDispatcher().getEventHandler().handle(evt);\n+            }\n           }\n         } else {\n           LOG.error(\"Received finished container :\"",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "8a2c53958cba746d4fb5e7f0bf30e199235cb4bb",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=d1f7fd801bf00ce62df6ef065cc906d2816352e2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -21,6 +21,8 @@\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n \n@@ -29,7 +31,11 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n@@ -42,6 +48,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n@@ -50,6 +57,8 @@\n import org.junit.After;\n import org.junit.Test;\n \n+import static org.junit.Assert.assertEquals;\n+\n public class TestResourceTrackerService {\n \n   private final static File TEMP_DIR = new File(System.getProperty(\n@@ -457,6 +466,28 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @Test\n+  public void testNodeRegistrationWithContainers() throws Exception {\n+    MockRM rm = new MockRM();\n+    rm.init(new YarnConfiguration());\n+    rm.start();\n+    RMApp app = rm.submitApp(1024);\n+\n+    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n+    nm.nodeHeartbeat(true);\n+\n+    // Register node with some container statuses\n+    ContainerStatus status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+\n+    // The following shouldn't throw NPE\n+    nm.registerNode(Collections.singletonList(status));\n+    assertEquals(\"Incorrect number of nodes\", 1,\n+        rm.getRMContext().getRMNodes().size());\n+  }\n+\n   @Test\n   public void testReconnectNode() throws Exception {\n     final DrainDispatcher dispatcher = new DrainDispatcher();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1f7fd801bf00ce62df6ef065cc906d2816352e2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "eed9ecf9fa7e28f66bcb962fc41c2e842d9c7443",
                "status": "modified"
            }
        ],
        "message": "YARN-1821. NPE on registerNodeManager if the request has containers for UnmanagedAMs (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576525 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/0dd4a321c7399694a5f959eb95fe3337204ddf31",
        "patched_files": [
            "CHANGES.txt",
            "ResourceTrackerService.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop-common_da589c3": {
        "bug_id": "hadoop-common_da589c3",
        "commit": "https://github.com/apache/hadoop-common/commit/da589c34298a140cea761b17e7c8b3946ae5b97c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -966,3 +966,6 @@ Release 0.21.0 - Unreleased\n     MAPREDUCE-1075. Fix JobTracker to not throw an NPE for a non-existent\n     queue. (V.V.Chaitanya Krishna via yhemanth)\n \n+    MAPREDUCE-754. Fix NPE in expiry thread when a TT is lost. (Amar Kamat \n+    via sharad)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/CHANGES.txt",
                "sha": "30feefe5f363d9ac2ae63d31de5291b59cc5a267",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "deletions": 9,
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "patch": "@@ -804,17 +804,21 @@ boolean shouldAssignTasksToTracker(String hostName, long now) {\n     private void removeHostCapacity(String hostName) {\n       synchronized (taskTrackers) {\n         // remove the capacity of trackers on this host\n+        int numTrackersOnHost = 0;\n         for (TaskTrackerStatus status : getStatusesOnHost(hostName)) {\n           int mapSlots = status.getMaxMapSlots();\n           totalMapTaskCapacity -= mapSlots;\n           int reduceSlots = status.getMaxReduceSlots();\n           totalReduceTaskCapacity -= reduceSlots;\n+          ++numTrackersOnHost;\n           getInstrumentation().addBlackListedMapSlots(\n               mapSlots);\n           getInstrumentation().addBlackListedReduceSlots(\n               reduceSlots);\n         }\n-        incrBlackListedTrackers(uniqueHostsMap.remove(hostName));\n+        // remove the host\n+        uniqueHostsMap.remove(hostName);\n+        incrBlackListedTrackers(numTrackersOnHost);\n       }\n     }\n     \n@@ -2468,12 +2472,14 @@ boolean updateTaskTrackerStatus(String trackerName,\n         taskTrackers.remove(trackerName);\n         Integer numTaskTrackersInHost = \n           uniqueHostsMap.get(oldStatus.getHost());\n-        numTaskTrackersInHost --;\n-        if (numTaskTrackersInHost > 0)  {\n-          uniqueHostsMap.put(oldStatus.getHost(), numTaskTrackersInHost);\n-        }\n-        else {\n-          uniqueHostsMap.remove(oldStatus.getHost());\n+        if (numTaskTrackersInHost != null) {\n+          numTaskTrackersInHost --;\n+          if (numTaskTrackersInHost > 0)  {\n+            uniqueHostsMap.put(oldStatus.getHost(), numTaskTrackersInHost);\n+          }\n+          else {\n+            uniqueHostsMap.remove(oldStatus.getHost());\n+          }\n         }\n       }\n     }\n@@ -3841,8 +3847,8 @@ synchronized void decommissionNodes(Set<String> hosts)\n           Set<TaskTracker> trackers = hostnameToTaskTracker.remove(host);\n           if (trackers != null) {\n             for (TaskTracker tracker : trackers) {\n-              LOG.info(\"Decommission: Losing tracker \" + tracker + \n-                       \" on host \" + host);\n+              LOG.info(\"Decommission: Losing tracker \" \n+                       + tracker.getTrackerName() + \" on host \" + host);\n               removeTracker(tracker);\n             }\n             trackersDecommissioned += trackers.size();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "sha": "b07e2c0b1afb265140b3f69ccef41d31594f9ef5",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "deletions": 2,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java",
                "patch": "@@ -63,8 +63,10 @@\n     }\n     @Override\n     public ClusterStatus getClusterStatus(boolean detailed) {\n-      return new ClusterStatus(trackers.length,\n-          0, 0, 0, 0, totalSlots/2, totalSlots/2, JobTracker.State.RUNNING, 0);\n+      return new ClusterStatus(\n+          taskTrackers().size() - getBlacklistedTrackerCount(),\n+          getBlacklistedTrackerCount(), 0, 0, 0, totalSlots/2, totalSlots/2, \n+           JobTracker.State.RUNNING, 0);\n     }\n \n     public void setNumSlots(int totalSlots) {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/FakeObjectUtilities.java",
                "sha": "09a9fd12440f0832042150aa76f2d03471b84b04",
                "status": "modified"
            },
            {
                "additions": 137,
                "blob_url": "https://github.com/apache/hadoop-common/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java",
                "changes": 137,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobInProgress;\n import org.apache.hadoop.mapred.FakeObjectUtilities.FakeJobTracker;\n import org.apache.hadoop.mapred.UtilsForTests.FakeClock;\n+import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n \n /**\n@@ -47,6 +48,7 @@ protected void setUp() throws Exception {\n     conf.set(JTConfig.JT_IPC_ADDRESS, \"localhost:0\");\n     conf.set(JTConfig.JT_HTTP_ADDRESS, \"0.0.0.0:0\");\n     conf.setLong(JTConfig.JT_TRACKER_EXPIRY_INTERVAL, 1000);\n+    conf.set(JTConfig.JT_MAX_TRACKER_BLACKLISTS, \"1\");\n     jobTracker = new FakeJobTracker(conf, (clock = new FakeClock()), trackers);\n     jobTracker.startExpireTrackersThread();\n   }\n@@ -91,4 +93,139 @@ public void testLostTracker() throws IOException {\n     job.finishTask(tid[1]);\n     \n   }\n+  \n+  /**\n+   * Test whether the tracker gets blacklisted after its lost.\n+   */\n+  public void testLostTrackerBeforeBlacklisting() throws Exception {\n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[0]);\n+    TaskAttemptID[] tid = new TaskAttemptID[3];\n+    JobConf conf = new JobConf();\n+    conf.setNumMapTasks(1);\n+    conf.setNumReduceTasks(1);\n+    conf.set(JobContext.MAX_TASK_FAILURES_PER_TRACKER, \"1\");\n+    conf.set(JobContext.SETUP_CLEANUP_NEEDED, \"false\");\n+    FakeJobInProgress job = new FakeJobInProgress(conf, jobTracker);\n+    job.initTasks();\n+    job.setClusterSize(4);\n+    \n+    // Tracker 0 gets the map task\n+    tid[0] = job.findMapTask(trackers[0]);\n+\n+    job.finishTask(tid[0]);\n+\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.getClusterStatus(false).getTaskTrackers());\n+    \n+    // lose the tracker\n+    clock.advance(1100);\n+    jobTracker.checkExpiredTrackers();\n+    assertFalse(\"Tracker 0 not lost\", \n+        jobTracker.getClusterStatus(false).getActiveTrackerNames()\n+                  .contains(trackers[0]));\n+    \n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 0, jobTracker.getClusterStatus(false).getTaskTrackers());\n+    \n+    // Tracker 1 establishes contact with JT \n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[1]);\n+    \n+    // Tracker1 should get assigned the lost map task\n+    tid[1] =  job.findMapTask(trackers[1]);\n+\n+    assertNotNull(\"Map Task from Lost Tracker did not get reassigned\", tid[1]);\n+    \n+    assertEquals(\"Task ID of reassigned map task does not match\",\n+        tid[0].getTaskID().toString(), tid[1].getTaskID().toString());\n+    \n+    // finish the map task\n+    job.finishTask(tid[1]);\n+\n+    // finish the reduce task\n+    tid[2] =  job.findReduceTask(trackers[1]);\n+    job.finishTask(tid[2]);\n+    \n+    // check if job is successful\n+    assertEquals(\"Job not successful\", \n+                 JobStatus.SUCCEEDED, job.getStatus().getRunState());\n+    \n+    // check if the tracker is lost\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.getClusterStatus(false).getTaskTrackers());\n+    // validate blacklisted count .. since we lost one blacklisted tracker\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                0, jobTracker.getClusterStatus(false).getBlacklistedTrackers());\n+  }\n+\n+  /**\n+   * Test whether the tracker gets lost after its blacklisted.\n+   */\n+  public void testLostTrackerAfterBlacklisting() throws Exception {\n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[0]);\n+    clock.advance(600);\n+    TaskAttemptID[] tid = new TaskAttemptID[2];\n+    JobConf conf = new JobConf();\n+    conf.setNumMapTasks(1);\n+    conf.setNumReduceTasks(0);\n+    conf.set(JobContext.MAX_TASK_FAILURES_PER_TRACKER, \"1\");\n+    conf.set(JobContext.SETUP_CLEANUP_NEEDED, \"false\");\n+    FakeJobInProgress job = new FakeJobInProgress(conf, jobTracker);\n+    job.initTasks();\n+    job.setClusterSize(4);\n+    \n+    // check if the tracker count is correct\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.taskTrackers().size());\n+    \n+    // Tracker 0 gets the map task\n+    tid[0] = job.findMapTask(trackers[0]);\n+    // Fail the task\n+    job.failTask(tid[0]);\n+    \n+    // Tracker 1 establishes contact with JT\n+    FakeObjectUtilities.establishFirstContact(jobTracker, trackers[1]);\n+    // check if the tracker count is correct\n+    assertEquals(\"Active tracker count mismatch\", \n+                 2, jobTracker.taskTrackers().size());\n+    \n+    // Tracker 1 gets the map task\n+    tid[1] = job.findMapTask(trackers[1]);\n+    // Finish the task and also the job\n+    job.finishTask(tid[1]);\n+\n+    // check if job is successful\n+    assertEquals(\"Job not successful\", \n+                 JobStatus.SUCCEEDED, job.getStatus().getRunState());\n+    \n+    // check if the trackers 1 got blacklisted\n+    assertTrue(\"Tracker 0 not blacklisted\", \n+               jobTracker.getBlacklistedTrackers()[0].getTaskTrackerName()\n+                 .equals(trackers[0]));\n+    // check if the tracker count is correct\n+    assertEquals(\"Active tracker count mismatch\", \n+                 2, jobTracker.taskTrackers().size());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                1, jobTracker.getClusterStatus(false).getBlacklistedTrackers());\n+    \n+    // Advance clock. Tracker 0 should be lost\n+    clock.advance(500);\n+    jobTracker.checkExpiredTrackers();\n+    \n+    // check if the task tracker is lost\n+    assertFalse(\"Tracker 0 not lost\", \n+            jobTracker.getClusterStatus(false).getActiveTrackerNames()\n+                      .contains(trackers[0]));\n+    \n+    // check if the lost tracker has removed from the jobtracker\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jobTracker.taskTrackers().size());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                0, jobTracker.getClusterStatus(false).getBlacklistedTrackers());\n+    \n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestLostTracker.java",
                "sha": "ef38fee1aa24a51b742076df716832cff3cafc3c",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop-common/blob/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java?ref=da589c34298a140cea761b17e7c8b3946ae5b97c",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java",
                "patch": "@@ -32,7 +32,11 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.WritableComparable;\n import org.apache.hadoop.ipc.RPC;\n+import org.apache.hadoop.mapred.lib.IdentityReducer;\n+import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.UnixUserGroupInformation;\n@@ -377,4 +381,88 @@ public void testMRRefreshRecommissioning() throws IOException {\n     \n     stopCluster();\n   }\n+  \n+  // Mapper that fails once for the first time\n+  static class FailOnceMapper extends MapReduceBase implements\n+      Mapper<WritableComparable, Writable, WritableComparable, Writable> {\n+\n+    private boolean shouldFail = false;\n+    public void map(WritableComparable key, Writable value,\n+        OutputCollector<WritableComparable, Writable> out, Reporter reporter)\n+        throws IOException {\n+\n+      if (shouldFail) {\n+        throw new RuntimeException(\"failing map\");\n+      }\n+    }\n+    \n+    @Override\n+    public void configure(JobConf conf) {\n+      TaskAttemptID id = TaskAttemptID.forName(conf.get(\"mapred.task.id\"));\n+      shouldFail = id.getId() == 0 && id.getTaskID().getId() == 0; \n+    }\n+  }\n+  \n+  /**\n+   * Check refreshNodes for decommissioning blacklisted nodes. \n+   */\n+  public void testBlacklistedNodeDecommissioning() throws Exception {\n+    LOG.info(\"Testing blacklisted node decommissioning\");\n+\n+    Configuration conf = new Configuration();\n+    conf.set(JTConfig.JT_MAX_TRACKER_BLACKLISTS, \"1\");\n+\n+    startCluster(2, 1, 0, conf);\n+    \n+    assertEquals(\"Trackers not up\", 2,\n+           mr.getJobTrackerRunner().getJobTracker().getActiveTrackers().length);\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 2, jt.getClusterStatus(false).getTaskTrackers());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                0, jt.getClusterStatus(false).getBlacklistedTrackers());\n+\n+    // run a failing job to blacklist the tracker\n+    JobConf jConf = mr.createJobConf();\n+    jConf.set(JobContext.MAX_TASK_FAILURES_PER_TRACKER, \"1\");\n+    jConf.setJobName(\"test-job-fail-once\");\n+    jConf.setMapperClass(FailOnceMapper.class);\n+    jConf.setReducerClass(IdentityReducer.class);\n+    jConf.setNumMapTasks(1);\n+    jConf.setNumReduceTasks(0);\n+    \n+    RunningJob job = \n+      UtilsForTests.runJob(jConf, new Path(\"in\"), new Path(\"out\"));\n+    job.waitForCompletion();\n+    \n+    // check if the tracker is lost\n+    // validate the total tracker count\n+    assertEquals(\"Active tracker count mismatch\", \n+                 1, jt.getClusterStatus(false).getTaskTrackers());\n+    // validate blacklisted count\n+    assertEquals(\"Blacklisted tracker count mismatch\", \n+                1, jt.getClusterStatus(false).getBlacklistedTrackers());\n+    \n+    // find the tracker to decommission\n+    String hostToDecommission = \n+      JobInProgress.convertTrackerNameToHostName(\n+          jt.getBlacklistedTrackers()[0].getTaskTrackerName());\n+    LOG.info(\"Decommissioning host \" + hostToDecommission);\n+    \n+    Set<String> decom = new HashSet<String>(1);\n+    decom.add(hostToDecommission);\n+    jt.decommissionNodes(decom);\n+ \n+    // check the cluster status and tracker size\n+    assertEquals(\"Tracker is not lost upon host decommissioning\", \n+                 1, jt.getClusterStatus(false).getTaskTrackers());\n+    assertEquals(\"Blacklisted tracker count incorrect in cluster status after \"\n+                 + \"decommissioning\", \n+                 0, jt.getClusterStatus(false).getBlacklistedTrackers());\n+    assertEquals(\"Tracker is not lost upon host decommissioning\", \n+                 1, jt.taskTrackers().size());\n+    \n+    stopCluster();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/da589c34298a140cea761b17e7c8b3946ae5b97c/src/test/mapred/org/apache/hadoop/mapred/TestNodeRefresh.java",
                "sha": "f14c3a49e666072eb7989fae06878fb29046dd56",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-754. Fix NPE in expiry thread when a TT is lost. Contributed by Amar Kamat.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@888431 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/e38a2babaed31d7ca85523b36be78845daf2aef2",
        "patched_files": [
            "JobTracker.java",
            "FakeObjectUtilities.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNodeRefresh.java",
            "TestLostTracker.java"
        ]
    },
    "hadoop-common_dc0a849": {
        "bug_id": "hadoop-common_dc0a849",
        "commit": "https://github.com/apache/hadoop-common/commit/dc0a8497e9073732de2d70fea437eb96a0bbd735",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/dc0a8497e9073732de2d70fea437eb96a0bbd735/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=dc0a8497e9073732de2d70fea437eb96a0bbd735",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -383,6 +383,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-2179. Fix RaidBlockSender compilation failure. (Ramkumar Vadali\n     via schen)\n \n+    MAPREDUCE-2034. TestSubmitJob triggers NPE instead of permissions error.\n+    (Todd Lipcon via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/dc0a8497e9073732de2d70fea437eb96a0bbd735/CHANGES.txt",
                "sha": "0cb361dace0644dd8043be92c9d50f260b788f9e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/dc0a8497e9073732de2d70fea437eb96a0bbd735/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java?ref=dc0a8497e9073732de2d70fea437eb96a0bbd735",
                "deletions": 11,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java",
                "patch": "@@ -259,31 +259,30 @@ public RunningJob run() throws IOException {\n         getDFSClient(conf_other, user2);\n \n       // try accessing mapred.system.dir/jobid/*\n-      boolean failed = false;\n       try {\n-        Path path = new Path(new URI(jt.getSystemDir()).getPath());\n+        String path = new URI(jt.getSystemDir()).getPath();\n         LOG.info(\"Try listing the mapred-system-dir as the user (\" \n                  + user2.getUserName() + \")\");\n-        client.getListing(\n-            path.toString(), HdfsFileStatus.EMPTY_NAME, false);\n+        client.getListing(path, HdfsFileStatus.EMPTY_NAME, false);\n+        fail(\"JobTracker system dir is accessible to others\");\n       } catch (IOException ioe) {\n-        failed = true;\n+        assertTrue(ioe.toString(),\n+          ioe.toString().contains(\"Permission denied\"));\n       }\n-      assertTrue(\"JobTracker system dir is accessible to others\", failed);\n       // try accessing ~/.staging/jobid/*\n-      failed = false;\n       JobInProgress jip = jt.getJob(id);\n       Path jobSubmitDirpath = \n         new Path(jip.getJobConf().get(\"mapreduce.job.dir\"));\n       try {\n         LOG.info(\"Try accessing the job folder for job \" + id + \" as the user (\" \n                  + user2.getUserName() + \")\");\n-        client.getListing(\n-            jobSubmitDirpath.toString(), HdfsFileStatus.EMPTY_NAME, false);\n+        client.getListing(jobSubmitDirpath.toUri().getPath(),\n+          HdfsFileStatus.EMPTY_NAME, false);\n+        fail(\"User's staging folder is accessible to others\");\n       } catch (IOException ioe) {\n-        failed = true;\n+        assertTrue(ioe.toString(),\n+          ioe.toString().contains(\"Permission denied\"));\n       }\n-      assertTrue(\"User's staging folder is accessible to others\", failed);\n       UtilsForTests.signalTasks(dfs, fs, true, mapSignalFile.toString(), \n       reduceSignalFile.toString());\n       // wait for job to be done",
                "raw_url": "https://github.com/apache/hadoop-common/raw/dc0a8497e9073732de2d70fea437eb96a0bbd735/src/test/mapred/org/apache/hadoop/mapred/TestSubmitJob.java",
                "sha": "39a92cefd9134e1796f5ae671c30844ccb68906d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2034. TestSubmitJob triggers NPE instead of permissions error. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1033668 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/1085c204f5f73949b6a1f7cc8f085d91b3a2e666",
        "patched_files": [
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestSubmitJob.java"
        ]
    },
    "hadoop-common_dd6efbb": {
        "bug_id": "hadoop-common_dd6efbb",
        "commit": "https://github.com/apache/hadoop-common/commit/dd6efbb02303645db607e6ae2adbd9addd108a3d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/dd6efbb02303645db607e6ae2adbd9addd108a3d/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=dd6efbb02303645db607e6ae2adbd9addd108a3d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -331,6 +331,9 @@ Release 0.23.7 - UNRELEASED\n     YARN-362. Unexpected extra results when using webUI table search (Ravi\n     Prakash via jlowe)\n \n+    YARN-400. RM can return null application resource usage report leading to \n+    NPE in client (Jason Lowe via tgraves)\n+\n Release 0.23.6 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/dd6efbb02303645db607e6ae2adbd9addd108a3d/hadoop-yarn-project/CHANGES.txt",
                "sha": "ca9fff4bf2c0afac733c34b81395bdaea94c80f2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/dd6efbb02303645db607e6ae2adbd9addd108a3d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java?ref=dd6efbb02303645db607e6ae2adbd9addd108a3d",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "patch": "@@ -406,7 +406,8 @@ public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {\n       String host = UNAVAILABLE;\n       String origTrackingUrl = UNAVAILABLE;\n       int rpcPort = -1;\n-      ApplicationResourceUsageReport appUsageReport = null;\n+      ApplicationResourceUsageReport appUsageReport =\n+          DUMMY_APPLICATION_RESOURCE_USAGE_REPORT;\n       FinalApplicationStatus finishState = getFinalApplicationStatus();\n       String diags = UNAVAILABLE;\n       if (allowAccess) {\n@@ -418,18 +419,17 @@ public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {\n           host = this.currentAttempt.getHost();\n           rpcPort = this.currentAttempt.getRpcPort();\n           appUsageReport = currentAttempt.getApplicationResourceUsageReport();\n-        } else {\n-          currentApplicationAttemptId = \n-              BuilderUtils.newApplicationAttemptId(this.applicationId, \n-                  DUMMY_APPLICATION_ATTEMPT_NUMBER);\n         }\n+\n         diags = this.diagnostics.toString();\n-      } else {\n-        appUsageReport = DUMMY_APPLICATION_RESOURCE_USAGE_REPORT;\n+      }\n+\n+      if (currentApplicationAttemptId == null) {\n         currentApplicationAttemptId = \n             BuilderUtils.newApplicationAttemptId(this.applicationId, \n                 DUMMY_APPLICATION_ATTEMPT_NUMBER);\n       }\n+\n       return BuilderUtils.newApplicationReport(this.applicationId,\n           currentApplicationAttemptId, this.user, this.queue,\n           this.name, host, rpcPort, clientToken,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/dd6efbb02303645db607e6ae2adbd9addd108a3d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "sha": "fa8070929cc3a7a2fabad57143ffead013cc010d",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/dd6efbb02303645db607e6ae2adbd9addd108a3d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java?ref=dd6efbb02303645db607e6ae2adbd9addd108a3d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.MockApps;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl;\n@@ -616,4 +617,12 @@ public void testAppKilledKilled() throws IOException {\n     assertTimesAtFinish(application);\n     assertAppState(RMAppState.KILLED, application);\n   }\n+\n+  @Test\n+  public void testGetAppReport() {\n+    RMApp app = createNewTestApp(null);\n+    assertAppState(RMAppState.NEW, app);\n+    ApplicationReport report = app.createAndGetApplicationReport(true);\n+    Assert.assertNotNull(report.getApplicationResourceUsageReport());\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/dd6efbb02303645db607e6ae2adbd9addd108a3d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "sha": "6d407dd44599e17379acf015c85821be21d9bc1d",
                "status": "modified"
            }
        ],
        "message": "YARN-400. RM can return null application resource usage report leading to NPE in client (Jason Lowe via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448241 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/a93604a83f6922fb18c8af702958cdedffb62e50",
        "patched_files": [
            "RMAppImpl.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMAppTransitions.java"
        ]
    },
    "hadoop-common_e15d1ca": {
        "bug_id": "hadoop-common_e15d1ca",
        "commit": "https://github.com/apache/hadoop-common/commit/e15d1cad29929db69fc8dd987ce81074d4b8bfbd",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e15d1cad29929db69fc8dd987ce81074d4b8bfbd/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=e15d1cad29929db69fc8dd987ce81074d4b8bfbd",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -313,6 +313,9 @@ Branch-2 ( Unreleased changes )\n     HADOOP-8563. don't package hadoop-pipes examples/bin\n     (Colin Patrick McCabe via tgraves)\n \n+    HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no \n+    package (primitive types and arrays). (tucu)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HADOOP-8220. ZKFailoverController doesn't handle failure to become active",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e15d1cad29929db69fc8dd987ce81074d4b8bfbd/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "ece64bd2966b9bbcf1ae12f575865f439dc4aead",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e15d1cad29929db69fc8dd987ce81074d4b8bfbd/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java?ref=e15d1cad29929db69fc8dd987ce81074d4b8bfbd",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java",
                "patch": "@@ -58,8 +58,8 @@ public synchronized boolean accept(Class<?> c) {\n     if (packages == null) {\n       getPackages();\n     }\n-    return AvroReflectSerializable.class.isAssignableFrom(c) || \n-      packages.contains(c.getPackage().getName());\n+    return AvroReflectSerializable.class.isAssignableFrom(c) ||\n+      (c.getPackage() != null && packages.contains(c.getPackage().getName()));\n   }\n \n   private void getPackages() {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e15d1cad29929db69fc8dd987ce81074d4b8bfbd/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java",
                "sha": "cfbc60d10452b20976c4d0a91728342d851c5c0c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e15d1cad29929db69fc8dd987ce81074d4b8bfbd/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java?ref=e15d1cad29929db69fc8dd987ce81074d4b8bfbd",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java",
                "patch": "@@ -21,6 +21,7 @@\n import junit.framework.TestCase;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.serializer.SerializationFactory;\n import org.apache.hadoop.io.serializer.SerializationTestUtil;\n \n public class TestAvroSerialization extends TestCase {\n@@ -43,6 +44,12 @@ public void testReflectPkg() throws Exception {\n     assertEquals(before, after);\n   }\n \n+  public void testAcceptHandlingPrimitivesAndArrays() throws Exception {\n+    SerializationFactory factory = new SerializationFactory(conf);\n+    assertNull(factory.getSerializer(byte[].class));\n+    assertNull(factory.getSerializer(byte.class));\n+  }\n+\n   public void testReflectInnerClass() throws Exception {\n     InnerRecord before = new InnerRecord();\n     before.x = 10;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e15d1cad29929db69fc8dd987ce81074d4b8bfbd/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java",
                "sha": "181419c137ee185c20dc5106ee5421c68eb50264",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1358454 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/bb9a2bf0d7f6ad9133f730ee000928b9dd0ef684",
        "patched_files": [
            "AvroSerialization.java",
            "AvroReflectSerialization.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestAvroSerialization.java"
        ]
    },
    "hadoop-common_e21d6b0": {
        "bug_id": "hadoop-common_e21d6b0",
        "commit": "https://github.com/apache/hadoop-common/commit/e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -573,6 +573,9 @@ Release 0.23.0 - Unreleased\n     HADOOP-7598. Fix smart-apply-patch.sh to handle patching from a sub\n     directory correctly. (Robert Evans via acmurthy) \n \n+    HADOOP-7328. When a serializer class is missing, return null, not throw\n+    an NPE. (Harsh J Chouraria via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "23ac34d30664d2f504e3883127cb50b1959ae0b9",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -27,10 +27,10 @@\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.io.serializer.avro.AvroReflectSerialization;\n import org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization;\n import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.hadoop.util.StringUtils;\n \n /**\n  * <p>\n@@ -50,14 +50,15 @@\n    * <p>\n    * Serializations are found by reading the <code>io.serializations</code>\n    * property from <code>conf</code>, which is a comma-delimited list of\n-   * classnames. \n+   * classnames.\n    * </p>\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    for (String serializerName : conf.getStrings(\"io.serializations\", \n-      new String[]{WritableSerialization.class.getName(), \n-        AvroSpecificSerialization.class.getName(), \n+    for (String serializerName : conf.getStrings(\n+      CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n+      new String[]{WritableSerialization.class.getName(),\n+        AvroSpecificSerialization.class.getName(),\n         AvroReflectSerialization.class.getName()})) {\n       add(conf, serializerName);\n     }\n@@ -67,27 +68,35 @@ public SerializationFactory(Configuration conf) {\n   private void add(Configuration conf, String serializationName) {\n     try {\n       Class<? extends Serialization> serializionClass =\n-\t(Class<? extends Serialization>) conf.getClassByName(serializationName);\n+        (Class<? extends Serialization>) conf.getClassByName(serializationName);\n       serializations.add((Serialization)\n-\t  ReflectionUtils.newInstance(serializionClass, getConf()));\n+      ReflectionUtils.newInstance(serializionClass, getConf()));\n     } catch (ClassNotFoundException e) {\n       LOG.warn(\"Serialization class not found: \", e);\n     }\n   }\n \n   public <T> Serializer<T> getSerializer(Class<T> c) {\n-    return getSerialization(c).getSerializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getSerializer(c);\n+    }\n+    return null;\n   }\n \n   public <T> Deserializer<T> getDeserializer(Class<T> c) {\n-    return getSerialization(c).getDeserializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getDeserializer(c);\n+    }\n+    return null;\n   }\n \n   @SuppressWarnings(\"unchecked\")\n   public <T> Serialization<T> getSerialization(Class<T> c) {\n     for (Serialization serialization : serializations) {\n       if (serialization.accept(c)) {\n-\treturn (Serialization<T>) serialization;\n+        return (Serialization<T>) serialization;\n       }\n     }\n     return null;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "52a0a253bbaa6bb7e7d580f71292e72fd9c28678",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=e21d6b0c394452f3f73d66255e5734d3dbd4dd57",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io.serializer;\n+\n+import org.junit.Test;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertNotNull;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+\n+public class TestSerializationFactory {\n+\n+  @Test\n+  public void testSerializerAvailability() {\n+    Configuration conf = new Configuration();\n+    SerializationFactory factory = new SerializationFactory(conf);\n+    // Test that a valid serializer class is returned when its present\n+    assertNotNull(\"A valid class must be returned for default Writable Serde\",\n+        factory.getSerializer(Writable.class));\n+    assertNotNull(\"A valid class must be returned for default Writable serDe\",\n+        factory.getDeserializer(Writable.class));\n+    // Test that a null is returned when none can be found.\n+    assertNull(\"A null should be returned if there are no serializers found.\",\n+        factory.getSerializer(TestSerializationFactory.class));\n+    assertNull(\"A null should be returned if there are no deserializers found\",\n+        factory.getDeserializer(TestSerializationFactory.class));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e21d6b0c394452f3f73d66255e5734d3dbd4dd57/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "18c2637ec5adb644f7a56fb41aef5fe82967e9e2",
                "status": "added"
            }
        ],
        "message": "HADOOP-7328. When a serializer class is missing, return null, not throw an NPE. Contributed by Harsh J Chouraria.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/b69d3b0c121e6440690935a6f23e6914b0ad77b3",
        "patched_files": [
            "SerializationFactory.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop-common_e63a794": {
        "bug_id": "hadoop-common_e63a794",
        "commit": "https://github.com/apache/hadoop-common/commit/e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt?ref=e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "patch": "@@ -18,3 +18,5 @@ HDFS-3773. TestNNWithQJM fails after HDFS-3741. (atm)\n HDFS-3793. Implement genericized format() in QJM (todd)\n \n HDFS-3795. QJM: validate journal dir at startup (todd)\n+\n+HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "sha": "74e443d74d69e12b59b3ac49b5f769f3a7bd7c3b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java?ref=e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "patch": "@@ -273,6 +273,11 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n     }\n     \n     FileJournalManager.EditLogFile elf = fjm.getLogFile(startTxId);\n+    if (elf == null) {\n+      throw new IllegalStateException(\"No log file to finalize at \" +\n+          \"transaction ID \" + startTxId);\n+    }\n+\n     if (elf.isInProgress()) {\n       // TODO: this is slow to validate when in non-recovery cases\n       // we already know the length here!\n@@ -281,7 +286,7 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n       elf.validateLog();\n       \n       Preconditions.checkState(elf.getLastTxId() == endTxId,\n-          \"Trying to finalize log %s-%s, but current state of log\" +\n+          \"Trying to finalize log %s-%s, but current state of log \" +\n           \"is %s\", startTxId, endTxId, elf);\n       fjm.finalizeLogSegment(startTxId, endTxId);\n     } else {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "sha": "cf2c11d885db87a04fb4ddbc59f8db25b34bc1fb",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java?ref=e63a7949fdf084dd8a93573a9d9b96ae5d08b407",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "patch": "@@ -155,6 +155,60 @@ public void testJournalLocking() throws Exception {\n     journal2.newEpoch(FAKE_NSINFO, 2);\n   }\n   \n+  /**\n+   * Test finalizing a segment after some batch of edits were missed.\n+   * This should fail, since we validate the log before finalization.\n+   */\n+  @Test\n+  public void testFinalizeWhenEditsAreMissed() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    journal.startLogSegment(makeRI(1), 1);\n+    journal.journal(makeRI(2), 1, 3,\n+        QJMTestUtil.createTxnData(1, 3));\n+    \n+    // Try to finalize up to txn 6, even though we only wrote up to txn 3.\n+    try {\n+      journal.finalizeLogSegment(makeRI(3), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+    \n+    // Check that, even if we re-construct the journal by scanning the\n+    // disk, we don't allow finalizing incorrectly.\n+    journal.close();\n+    journal = new Journal(TEST_LOG_DIR, mockErrorReporter);\n+    \n+    try {\n+      journal.finalizeLogSegment(makeRI(4), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+  }\n+  \n+  /**\n+   * Ensure that finalizing a segment which doesn't exist throws the\n+   * appropriate exception.\n+   */\n+  @Test\n+  public void testFinalizeMissingSegment() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    try {\n+      journal.finalizeLogSegment(makeRI(1), 1000, 1001);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"No log file to finalize at transaction ID 1000\", ise);\n+    }\n+  }\n+  \n+  private static RequestInfo makeRI(int serial) {\n+    return new RequestInfo(JID, 1, serial);\n+  }\n+  \n   @Test\n   public void testNamespaceVerification() throws Exception {\n     journal.newEpoch(FAKE_NSINFO, 1);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e63a7949fdf084dd8a93573a9d9b96ae5d08b407/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "sha": "f9539d9bb61c8db495b076a4b32c7523cd71c5a4",
                "status": "modified"
            }
        ],
        "message": "HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/2c199eef8c87a9e972100b018a4b3b519ec6b8cd",
        "patched_files": [
            "CHANGES.HDFS-3077.txt",
            "Journal.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJournal.java"
        ]
    },
    "hadoop-common_e772b01": {
        "bug_id": "hadoop-common_e772b01",
        "commit": "https://github.com/apache/hadoop-common/commit/e772b01c839c438fefaf7cc53c5abc62cc22a379",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=e772b01c839c438fefaf7cc53c5abc62cc22a379",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -105,3 +105,5 @@ HDFS-2766. Test for case where standby partially reads log and then performs che\n HDFS-2738. FSEditLog.selectinputStreams is reading through in-progress streams even when non-in-progress are requested. (atm)\n \n HDFS-2789. TestHAAdmin.testFailover is failing (eli)\n+\n+HDFS-2747. Entering safe mode after starting SBN can NPE. (Uma Maheswara Rao G via todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "fc245ed3e690cc629d69542cf53170b90417b462",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=e772b01c839c438fefaf7cc53c5abc62cc22a379",
                "deletions": 15,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3774,21 +3774,28 @@ private long getCompleteBlocksTotal() {\n   void enterSafeMode(boolean resourcesLow) throws IOException {\n     writeLock();\n     try {\n-    // Ensure that any concurrent operations have been fully synced\n-    // before entering safe mode. This ensures that the FSImage\n-    // is entirely stable on disk as soon as we're in safe mode.\n-    getEditLog().logSyncAll();\n-    if (!isInSafeMode()) {\n-      safeMode = new SafeModeInfo(resourcesLow);\n-      return;\n-    }\n-    if (resourcesLow) {\n-      safeMode.setResourcesLow();\n-    }\n-    safeMode.setManual();\n-    getEditLog().logSyncAll();\n-    NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \" \n-                                + safeMode.getTurnOffTip());\n+      // Ensure that any concurrent operations have been fully synced\n+      // before entering safe mode. This ensures that the FSImage\n+      // is entirely stable on disk as soon as we're in safe mode.\n+      boolean isEditlogOpenForWrite = getEditLog().isOpenForWrite();\n+      // Before Editlog is in OpenForWrite mode, editLogStream will be null. So,\n+      // logSyncAll call can be called only when Edlitlog is in OpenForWrite mode\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      if (!isInSafeMode()) {\n+        safeMode = new SafeModeInfo(resourcesLow);\n+        return;\n+      }\n+      if (resourcesLow) {\n+        safeMode.setResourcesLow();\n+      }\n+      safeMode.setManual();\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \"\n+          + safeMode.getTurnOffTip());\n     } finally {\n       writeUnlock();\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "f1664f7e62d02065e7fb1231a7238e45d257ac00",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/hadoop-common/blob/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java?ref=e772b01c839c438fefaf7cc53c5abc62cc22a379",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;\n import org.junit.After;\n@@ -95,6 +96,68 @@ private void restartStandby() throws IOException {\n     nn1.getNamesystem().getEditLogTailer().interrupt();\n   }\n   \n+  /**\n+   * Test case for enter safemode in active namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInANNShouldNotThrowNPE() throws Exception {\n+    banner(\"Restarting active\");\n+    restartActive();\n+    FSNamesystem namesystem = nn0.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  /**\n+   * Test case for enter safemode in standby namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInSBNShouldNotThrowNPE() throws Exception {\n+    banner(\"Starting with NN0 active and NN1 standby, creating some blocks\");\n+    DFSTestUtil\n+        .createFile(fs, new Path(\"/test\"), 3 * BLOCK_SIZE, (short) 3, 1L);\n+    // Roll edit log so that, when the SBN restarts, it will load\n+    // the namespace during startup and enter safemode.\n+    nn0.getRpcServer().rollEditLog();\n+    banner(\"Creating some blocks that won't be in the edit log\");\n+    DFSTestUtil.createFile(fs, new Path(\"/test2\"), 5 * BLOCK_SIZE, (short) 3,\n+        1L);\n+    banner(\"Deleting the original blocks\");\n+    fs.delete(new Path(\"/test\"), true);\n+    banner(\"Restarting standby\");\n+    restartStandby();\n+    FSNamesystem namesystem = nn1.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  private void restartActive() throws IOException {\n+    cluster.shutdownNameNode(0);\n+    // Set the safemode extension to be lengthy, so that the tests\n+    // can check the safemode message after the safemode conditions\n+    // have been achieved, without being racy.\n+    cluster.getConfiguration(0).setInt(\n+        DFSConfigKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, 30000);\n+    cluster.restartNameNode(0);\n+    nn0 = cluster.getNameNode(0);\n+  }\n+  \n   /**\n    * Tests the case where, while a standby is down, more blocks are\n    * added to the namespace, but not rolled. So, when it starts up,",
                "raw_url": "https://github.com/apache/hadoop-common/raw/e772b01c839c438fefaf7cc53c5abc62cc22a379/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "sha": "af7985e21d37e0577092f1822490922f9fb88eec",
                "status": "modified"
            }
        ],
        "message": "HDFS-2747. Entering safe mode after starting SBN can NPE. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232176 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/24161c11d720d85da57cfb3b0f33f2413f934250",
        "patched_files": [
            "CHANGES.HDFS-1623.txt",
            "FSNamesystem.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHASafeMode.java"
        ]
    },
    "hadoop-common_ea51a26": {
        "bug_id": "hadoop-common_ea51a26",
        "commit": "https://github.com/apache/hadoop-common/commit/ea51a26a9435a23151f91c36cc24692ee9a58f7d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2128. FairScheduler: Incorrect calculation of amResource usage.\n     (Wei Yan via kasha)\n \n+    YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. (Wangda Tan\n+    via jianhe)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/CHANGES.txt",
                "sha": "401fb102bf47b883c2c57fbf85ee5e61fd4281bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -327,7 +327,7 @@ protected static void validateConfigs(Configuration conf) {\n    * RMActiveServices handles all the Active services in the RM.\n    */\n   @Private\n-  class RMActiveServices extends CompositeService {\n+  public class RMActiveServices extends CompositeService {\n \n     private DelegationTokenRenewer delegationTokenRenewer;\n     private EventHandler<SchedulerEvent> schedulerDispatcher;\n@@ -526,11 +526,9 @@ protected void createPolicyMonitors() {\n                   (PreemptableResourceScheduler) scheduler));\n           for (SchedulingEditPolicy policy : policies) {\n             LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n-            policy.init(conf, rmContext.getDispatcher().getEventHandler(),\n-                (PreemptableResourceScheduler) scheduler);\n             // periodically check whether we need to take action to guarantee\n             // constraints\n-            SchedulingMonitor mon = new SchedulingMonitor(policy);\n+            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n             addService(mon);\n           }\n         } else {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "77de2090a00479282f863f110299a592f5c0d77e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "patch": "@@ -21,6 +21,8 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.service.AbstractService;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler;\n \n import com.google.common.annotations.VisibleForTesting;\n \n@@ -34,18 +36,29 @@\n   private Thread checkerThread;\n   private volatile boolean stopped;\n   private long monitorInterval;\n+  private RMContext rmContext;\n \n-  public SchedulingMonitor(SchedulingEditPolicy scheduleEditPolicy) {\n+  public SchedulingMonitor(RMContext rmContext,\n+      SchedulingEditPolicy scheduleEditPolicy) {\n     super(\"SchedulingMonitor (\" + scheduleEditPolicy.getPolicyName() + \")\");\n     this.scheduleEditPolicy = scheduleEditPolicy;\n-    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n+    this.rmContext = rmContext;\n   }\n \n   public long getMonitorInterval() {\n     return monitorInterval;\n   }\n+  \n+  @VisibleForTesting\n+  public synchronized SchedulingEditPolicy getSchedulingEditPolicy() {\n+    return scheduleEditPolicy;\n+  }\n \n+  @SuppressWarnings(\"unchecked\")\n   public void serviceInit(Configuration conf) throws Exception {\n+    scheduleEditPolicy.init(conf, rmContext.getDispatcher().getEventHandler(),\n+        (PreemptableResourceScheduler) rmContext.getScheduler());\n+    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n     super.serviceInit(conf);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "sha": "1682f7d8612602446bc52b157945b5901a6a1a11",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -165,6 +165,11 @@ public void init(Configuration config,\n     observeOnly = config.getBoolean(OBSERVE_ONLY, false);\n     rc = scheduler.getResourceCalculator();\n   }\n+  \n+  @VisibleForTesting\n+  public ResourceCalculator getResourceCalculator() {\n+    return rc;\n+  }\n \n   @Override\n   public void editSchedule(){",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "sha": "6d1516158b5aad26990a58d562def6b0ca5df243",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "patch": "@@ -571,4 +571,8 @@ public void clearQueueMetrics(RMApp app) {\n       .getSchedulerApplications().get(app.getApplicationId()).getQueue()\n       .getMetrics().clearQueueMetrics();\n   }\n+  \n+  public RMActiveServices getRMActiveService() {\n+    return activeServices;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "sha": "67eac76e5f335d4df535ec2c888619ec677e9ae9",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java?ref=ea51a26a9435a23151f91c36cc24692ee9a58f7d",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -17,6 +17,25 @@\n  */\n package org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity;\n \n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.argThat;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n import java.util.ArrayList;\n import java.util.Comparator;\n import java.util.Deque;\n@@ -27,12 +46,16 @@\n import java.util.TreeSet;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.service.Service;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor;\n import org.apache.hadoop.yarn.server.resourcemanager.resource.Priority;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEvent;\n@@ -52,17 +75,6 @@\n import org.mockito.ArgumentCaptor;\n import org.mockito.ArgumentMatcher;\n \n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n-import static org.junit.Assert.*;\n-import static org.mockito.Mockito.*;\n-\n public class TestProportionalCapacityPreemptionPolicy {\n \n   static final long TS = 3141592653L;\n@@ -424,6 +436,36 @@ public void testContainerOrdering(){\n     assert containers.get(4).equals(rm5);\n \n   }\n+  \n+  @Test\n+  public void testPolicyInitializeAfterSchedulerInitialized() {\n+    Configuration conf = new Configuration();\n+    conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n+        ProportionalCapacityPreemptionPolicy.class.getCanonicalName());\n+    conf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);\n+    \n+    @SuppressWarnings(\"resource\")\n+    MockRM rm = new MockRM(conf);\n+    rm.init(conf);\n+    \n+    // ProportionalCapacityPreemptionPolicy should be initialized after\n+    // CapacityScheduler initialized. We will \n+    // 1) find SchedulingMonitor from RMActiveService's service list, \n+    // 2) check if ResourceCalculator in policy is null or not. \n+    // If it's not null, we can come to a conclusion that policy initialized\n+    // after scheduler got initialized\n+    for (Service service : rm.getRMActiveService().getServices()) {\n+      if (service instanceof SchedulingMonitor) {\n+        ProportionalCapacityPreemptionPolicy policy =\n+            (ProportionalCapacityPreemptionPolicy) ((SchedulingMonitor) service)\n+                .getSchedulingEditPolicy();\n+        assertNotNull(policy.getResourceCalculator());\n+        return;\n+      }\n+    }\n+    \n+    fail(\"Failed to find SchedulingMonitor service, please check what happened\");\n+  }\n \n   static class IsPreemptionRequestFor\n       extends ArgumentMatcher<ContainerPreemptEvent> {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ea51a26a9435a23151f91c36cc24692ee9a58f7d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "sha": "d0a80eb20bb320d8b1c1ec7009743049bc09eb3f",
                "status": "modified"
            }
        ],
        "message": "YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. Contributed by Wangda Tan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/dc1025cba292da086926dab06667621b051aa524",
        "patched_files": [
            "CHANGES.txt",
            "SchedulingMonitor.java",
            "ProportionalCapacityPreemptionPolicy.java",
            "ResourceManager.java",
            "MockRM.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestProportionalCapacityPreemptionPolicy.java",
            "TestResourceManager.java",
            "TestSchedulingMonitor.java"
        ]
    },
    "hadoop-common_eb3bdbb": {
        "bug_id": "hadoop-common_eb3bdbb",
        "commit": "https://github.com/apache/hadoop-common/commit/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -813,6 +813,8 @@ Release 2.3.0 - UNRELEASED\n     HDFS-5074. Allow starting up from an fsimage checkpoint in the middle of a\n     segment. (Todd Lipcon via atm)\n \n+    HDFS-4201. NPE in BPServiceActor#sendHeartBeat. (jxiang via cmccabe)\n+\n Release 2.2.0 - 2013-10-13\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "ee9b964724cead83d407f35b7c57096567db2885",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -274,12 +274,22 @@ DataNode getDataNode() {\n   synchronized void verifyAndSetNamespaceInfo(NamespaceInfo nsInfo) throws IOException {\n     if (this.bpNSInfo == null) {\n       this.bpNSInfo = nsInfo;\n-      \n+      boolean success = false;\n+\n       // Now that we know the namespace ID, etc, we can pass this to the DN.\n       // The DN can now initialize its local storage if we are the\n       // first BP to handshake, etc.\n-      dn.initBlockPool(this);\n-      return;\n+      try {\n+        dn.initBlockPool(this);\n+        success = true;\n+      } finally {\n+        if (!success) {\n+          // The datanode failed to initialize the BP. We need to reset\n+          // the namespace info so that other BPService actors still have\n+          // a chance to set it, and re-initialize the datanode.\n+          this.bpNSInfo = null;\n+        }\n+      }\n     } else {\n       checkNSEquality(bpNSInfo.getBlockPoolID(), nsInfo.getBlockPoolID(),\n           \"Blockpool ID\");",
                "raw_url": "https://github.com/apache/hadoop-common/raw/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "e646be9a650bd13d88caf1a6c51cbeda26d47173",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop-common/blob/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java?ref=eb3bdbb0040eceb2881bdecba8a9900e5759a4ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "patch": "@@ -25,7 +25,9 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -294,6 +296,47 @@ public void testPickActiveNameNode() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test datanode block pool initialization error handling.\n+   * Failure in initializing a block pool should not cause NPE.\n+   */\n+  @Test\n+  public void testBPInitErrorHandling() throws Exception {\n+    final DataNode mockDn = Mockito.mock(DataNode.class);\n+    Mockito.doReturn(true).when(mockDn).shouldRun();\n+    Configuration conf = new Configuration();\n+    File dnDataDir = new File(\n+      new File(TEST_BUILD_DATA, \"testBPInitErrorHandling\"), \"data\");\n+    conf.set(DFS_DATANODE_DATA_DIR_KEY, dnDataDir.toURI().toString());\n+    Mockito.doReturn(conf).when(mockDn).getConf();\n+    Mockito.doReturn(new DNConf(conf)).when(mockDn).getDnConf();\n+    Mockito.doReturn(DataNodeMetrics.create(conf, \"fake dn\")).\n+      when(mockDn).getMetrics();\n+    final AtomicInteger count = new AtomicInteger();\n+    Mockito.doAnswer(new Answer<Void>() {\n+      @Override\n+      public Void answer(InvocationOnMock invocation) throws Throwable {\n+        if (count.getAndIncrement() == 0) {\n+          throw new IOException(\"faked initBlockPool exception\");\n+        }\n+        // The initBlockPool is called again. Now mock init is done.\n+        Mockito.doReturn(mockFSDataset).when(mockDn).getFSDataset();\n+        return null;\n+      }\n+    }).when(mockDn).initBlockPool(Mockito.any(BPOfferService.class));\n+    BPOfferService bpos = setupBPOSForNNs(mockDn, mockNN1, mockNN2);\n+    bpos.start();\n+    try {\n+      waitForInitialization(bpos);\n+      List<BPServiceActor> actors = bpos.getBPServiceActors();\n+      assertEquals(1, actors.size());\n+      BPServiceActor actor = actors.get(0);\n+      waitForBlockReport(actor.getNameNodeProxy());\n+    } finally {\n+      bpos.stop();\n+    }\n+  }\n+\n   private void waitForOneToFail(final BPOfferService bpos)\n       throws Exception {\n     GenericTestUtils.waitFor(new Supplier<Boolean>() {\n@@ -311,6 +354,11 @@ public Boolean get() {\n    */\n   private BPOfferService setupBPOSForNNs(\n       DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n+    return setupBPOSForNNs(mockDn, nns);\n+  }\n+\n+  private BPOfferService setupBPOSForNNs(DataNode mockDn,\n+      DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n     // Set up some fake InetAddresses, then override the connectToNN\n     // function to return the corresponding proxies.\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/eb3bdbb0040eceb2881bdecba8a9900e5759a4ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "sha": "c5cb6c77f129e8e7a61507403342d8fab2b89505",
                "status": "modified"
            }
        ],
        "message": "HDFS-4201. NPE in BPServiceActor#sendHeartBeat (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550269 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/3bea061a58db747dd0fbdd920dc8f478e44bf9e5",
        "patched_files": [
            "BPOfferService.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBPOfferService.java"
        ]
    },
    "hadoop-common_ed0eae2": {
        "bug_id": "hadoop-common_ed0eae2",
        "commit": "https://github.com/apache/hadoop-common/commit/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -120,3 +120,6 @@ HDFS-5535 subtasks:\n \n     HDFS-6029. Secondary NN fails to checkpoint after -rollingUpgrade prepare.\n     (jing9)\n+\n+    HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. (Haohui Mai\n+    via jing9)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "002afcd44b67682655424e7e1ddb1daae4b83cab",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java?ref=ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "patch": "@@ -881,9 +881,12 @@ private void loadFSImage(File imageFile, FSNamesystem target,\n    */\n   private void loadFSImage(File curFile, MD5Hash expectedMd5,\n       FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n+    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n+    // information. Make sure the ID is properly set.\n+    target.setBlockPoolId(this.getBlockPoolID());\n+\n     FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n     loader.load(curFile);\n-    target.setBlockPoolId(this.getBlockPoolID());\n \n     // Check that the image digest we loaded matches up with what\n     // we expected",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "sha": "e79b6c9224665352c3ec945cab6ce37ad4b58b39",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java?ref=ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "patch": "@@ -432,6 +432,32 @@ public void testQuery() throws Exception {\n     }\n   }\n \n+  @Test (timeout = 300000)\n+  public void testQueryAfterRestart() throws IOException, InterruptedException {\n+    final Configuration conf = new Configuration();\n+    MiniDFSCluster cluster = null;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+      cluster.waitActive();\n+      DistributedFileSystem dfs = cluster.getFileSystem();\n+\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      // start rolling upgrade\n+      dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);\n+      queryForPreparation(dfs);\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      dfs.saveNamespace();\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+\n+      cluster.restartNameNodes();\n+      dfs.rollingUpgrade(RollingUpgradeAction.QUERY);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n   @Test(timeout = 300000)\n   public void testCheckpoint() throws IOException, InterruptedException {\n     final Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ed0eae2906b6e3b9994cb32747e7ee9cb1fa0712/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "sha": "321f8843d0f6f9091f76a43a50ea0d8ff9405171",
                "status": "modified"
            }
        ],
        "message": "HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1572801 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/bfecb2a3b99d46e8d9b57a23222524ed33e3d78e",
        "patched_files": [
            "CHANGES_HDFS-5535.txt",
            "FSImage.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSImage.java",
            "TestRollingUpgrade.java"
        ]
    },
    "hadoop-common_f00495a": {
        "bug_id": "hadoop-common_f00495a",
        "commit": "https://github.com/apache/hadoop-common/commit/f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -59,6 +59,9 @@ Release 2.1.0-alpha - Unreleased\n     YARN-79. Implement close on all clients to YARN so that RPC clients don't\n     throw exceptions on shut-down. (Vinod Kumar Vavilapalli)\n \n+    YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that\n+    NMs don't get NPEs on startup errors. (Devaraj K via vinodkv)\n+\n Release 0.23.4 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/CHANGES.txt",
                "sha": "8106a37f4a93cbc674e5f820b46b8ae0308f07c7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java?ref=f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "patch": "@@ -79,16 +79,18 @@ public void init(Configuration conf) {\n \n   @Override\n   public void stop() {\n-    sched.shutdown();\n-    boolean isShutdown = false;\n-    try {\n-      isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n-    } catch (InterruptedException e) {\n-      sched.shutdownNow();\n-      isShutdown = true;\n-    }\n-    if (!isShutdown) {\n-      sched.shutdownNow();\n+    if (sched != null) {\n+      sched.shutdown();\n+      boolean isShutdown = false;\n+      try {\n+        isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n+      } catch (InterruptedException e) {\n+        sched.shutdownNow();\n+        isShutdown = true;\n+      }\n+      if (!isShutdown) {\n+        sched.shutdownNow();\n+      }\n     }\n     super.stop();\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "sha": "7ec634b0ebe59ffd65ce3de334d67a3902be4e6e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java?ref=f00495a4baaa6c3243f9519c4e19c12f07f8ae26",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "patch": "@@ -183,6 +183,24 @@ public void testDelayedDelete() {\n     verify(mockSched).schedule(any(Runnable.class), eq(10800l),\n         eq(TimeUnit.SECONDS));\n   }\n+  \n+  @Test\n+  public void testStop() throws Exception {\n+    NonAggregatingLogHandler aggregatingLogHandler = \n+        new NonAggregatingLogHandler(null, null, null);\n+\n+    // It should not throw NullPointerException\n+    aggregatingLogHandler.stop();\n+\n+    NonAggregatingLogHandlerWithMockExecutor logHandler = \n+        new NonAggregatingLogHandlerWithMockExecutor(null, null, null);\n+    logHandler.init(new Configuration());\n+    logHandler.stop();\n+    verify(logHandler.mockSched).shutdown();\n+    verify(logHandler.mockSched)\n+        .awaitTermination(eq(10l), eq(TimeUnit.SECONDS));\n+    verify(logHandler.mockSched).shutdownNow();\n+  }\n \n   private class NonAggregatingLogHandlerWithMockExecutor extends\n       NonAggregatingLogHandler {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f00495a4baaa6c3243f9519c4e19c12f07f8ae26/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "sha": "36251e47e129c85380a287a665c8ca475ad71f27",
                "status": "modified"
            }
        ],
        "message": "YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that NMs don't get NPEs on startup errors. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1380954 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d123c4c0d828634756c39e67f13c0efc0984df10",
        "patched_files": [
            "NonAggregatingLogHandler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNonAggregatingLogHandler.java"
        ]
    },
    "hadoop-common_f14351c": {
        "bug_id": "hadoop-common_f14351c",
        "commit": "https://github.com/apache/hadoop-common/commit/f14351cbab17a4894a89f206f061b12be38e2be1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt?ref=f14351cbab17a4894a89f206f061b12be38e2be1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "patch": "@@ -329,3 +329,6 @@ Branch-2802 Snapshot (Unreleased)\n \n   HDFS-4758. Disallow nested snapshottable directories and unwrap\n   RemoteException.  (szetszwo)\n+\n+  HDFS-4781. Fix a NullPointerException when listing .snapshot under\n+  a non-existing directory.  (szetszwo)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "sha": "083d972fe55b5363fff6fb45ae176a6513601f38",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=f14351cbab17a4894a89f206f061b12be38e2be1",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1588,7 +1588,8 @@ private HdfsFileStatus getFileInfo4DotSnapshot(String src)\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node = this.getINode(dirPath);\n-    if (node.isDirectory()\n+    if (node != null\n+        && node.isDirectory()\n         && node.asDirectory() instanceof INodeDirectorySnapshottable) {\n       return new HdfsFileStatus(0, true, 0, 0, 0, 0, null, null, null, null,\n           HdfsFileStatus.EMPTY_NAME, -1L);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "4462099be9e0398f7bad7df668887fbf1b027c67",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=f14351cbab17a4894a89f206f061b12be38e2be1",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -313,12 +313,12 @@ public INodeSymlink asSymlink() {\n    * children.\n    * \n    * 1.3 The current inode is a {@link FileWithSnapshot}.\n-   * Call {@link INode#recordModification(Snapshot)} to capture the \n-   * current states. Mark the INode as deleted.\n+   * Call recordModification(..) to capture the current states.\n+   * Mark the INode as deleted.\n    * \n    * 1.4 The current inode is a {@link INodeDirectoryWithSnapshot}.\n-   * Call {@link INode#recordModification(Snapshot)} to capture the \n-   * current states. Destroy files/directories created after the latest snapshot \n+   * Call recordModification(..) to capture the current states. \n+   * Destroy files/directories created after the latest snapshot \n    * (i.e., the inodes stored in the created list of the latest snapshot).\n    * Recursively clean remaining children. \n    *",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "541f591113a0cacf819bd87a891188795779baf4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java?ref=f14351cbab17a4894a89f206f061b12be38e2be1",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java",
                "patch": "@@ -111,7 +111,7 @@ private int searchChildren(byte[] name) {\n    * Remove the specified child from this directory.\n    * \n    * @param child the child inode to be removed\n-   * @param latest See {@link INode#recordModification(Snapshot)}.\n+   * @param latest See {@link INode#recordModification(Snapshot, INodeMap)}.\n    */\n   public boolean removeChild(INode child, Snapshot latest,\n       final INodeMap inodeMap) throws QuotaExceededException {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java",
                "sha": "1dc1016498864ee8b6418c89da73fe0e2d51702e",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java?ref=f14351cbab17a4894a89f206f061b12be38e2be1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java",
                "patch": "@@ -22,6 +22,8 @@\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n+import java.io.FileNotFoundException;\n+\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n@@ -238,6 +240,18 @@ public void testSnapshotPathINodes() throws Exception {\n     final INode last = nodesInPath.getLastINode();\n     assertEquals(last.getFullPathName(), sub1.toString());\n     assertFalse(last instanceof INodeFileWithSnapshot);\n+    \n+    String[] invalidPathComponent = {\"invalidDir\", \"foo\", \".snapshot\", \"bar\"};\n+    Path invalidPath = new Path(invalidPathComponent[0]);\n+    for(int i = 1; i < invalidPathComponent.length; i++) {\n+      invalidPath = new Path(invalidPath, invalidPathComponent[i]);\n+      try {\n+        hdfs.getFileStatus(invalidPath);\n+        Assert.fail();\n+      } catch(FileNotFoundException fnfe) {\n+        System.out.println(\"The exception is expected: \" + fnfe);\n+      }\n+    }\n   }\n   \n   /** ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f14351cbab17a4894a89f206f061b12be38e2be1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java",
                "sha": "721a0fd3f659c7fdeeb656baee3374d9e4cccb08",
                "status": "modified"
            }
        ],
        "message": "HDFS-4781. Fix a NullPointerException when listing .snapshot under a non-existing directory.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1478135 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f9cef233c9739917756a38c7b851abbcbae9abba",
        "patched_files": [
            "INode.java",
            "INodeDirectory.java",
            "CHANGES.HDFS-2802.txt",
            "FSDirectory.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestINode.java",
            "TestFSDirectory.java",
            "TestSnapshotPathINodes.java"
        ]
    },
    "hadoop-common_f1f8352": {
        "bug_id": "hadoop-common_f1f8352",
        "commit": "https://github.com/apache/hadoop-common/commit/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -104,6 +104,10 @@ Trunk (Unreleased)\n     HADOOP-8814. Replace string equals \"\" by String#isEmpty().\n     (Brandon Li via suresh)\n \n+    HADOOP-8588. SerializationFactory shouldn't throw a\n+    NullPointerException if the serializations list is empty.\n+    (Sho Shimauchi via harsh)\n+\n   BUG FIXES\n \n     HADOOP-8177. MBeans shouldn't try to register when it fails to create MBeanName.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "18e2f8fdd74d313c1e436b2991d32543171db787",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -40,12 +40,12 @@\n @InterfaceAudience.LimitedPrivate({\"HDFS\", \"MapReduce\"})\n @InterfaceStability.Evolving\n public class SerializationFactory extends Configured {\n-  \n-  private static final Log LOG =\n+\n+  static final Log LOG =\n     LogFactory.getLog(SerializationFactory.class.getName());\n \n   private List<Serialization<?>> serializations = new ArrayList<Serialization<?>>();\n-  \n+\n   /**\n    * <p>\n    * Serializations are found by reading the <code>io.serializations</code>\n@@ -55,15 +55,21 @@\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    for (String serializerName : conf.getStrings(\n-      CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n-      new String[]{WritableSerialization.class.getName(),\n-        AvroSpecificSerialization.class.getName(),\n-        AvroReflectSerialization.class.getName()})) {\n-      add(conf, serializerName);\n+    if (conf.get(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY).equals(\"\")) {\n+      LOG.warn(\"Serialization for various data types may not be available. Please configure \"\n+          + CommonConfigurationKeys.IO_SERIALIZATIONS_KEY\n+          + \" properly to have serialization support (it is currently not set).\");\n+    } else {\n+      for (String serializerName : conf.getStrings(\n+          CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, new String[] {\n+              WritableSerialization.class.getName(),\n+              AvroSpecificSerialization.class.getName(),\n+              AvroReflectSerialization.class.getName() })) {\n+        add(conf, serializerName);\n+      }\n     }\n   }\n-  \n+\n   @SuppressWarnings(\"unchecked\")\n   private void add(Configuration conf, String serializationName) {\n     try {\n@@ -101,5 +107,5 @@ private void add(Configuration conf, String serializationName) {\n     }\n     return null;\n   }\n-  \n+\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "d6c6588e7944c6413967fb9185030efa8ee954bf",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -17,27 +17,62 @@\n  */\n package org.apache.hadoop.io.serializer;\n \n+import org.junit.BeforeClass;\n import org.junit.Test;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertNotNull;\n \n+import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.io.Writable;\n+import org.apache.log4j.Level;\n \n public class TestSerializationFactory {\n \n+  static {\n+    ((Log4JLogger) SerializationFactory.LOG).getLogger().setLevel(Level.ALL);\n+  }\n+\n+  static Configuration conf;\n+  static SerializationFactory factory;\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    conf = new Configuration();\n+    factory = new SerializationFactory(conf);\n+  }\n+\n+  @Test\n+  public void testSerializationKeyIsEmpty() {\n+    Configuration conf = new Configuration();\n+    conf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, \"\");\n+    SerializationFactory factory = new SerializationFactory(conf);\n+  }\n+\n   @Test\n-  public void testSerializerAvailability() {\n+  public void testSerializationKeyIsInvalid() {\n     Configuration conf = new Configuration();\n+    conf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, \"INVALID_KEY_XXX\");\n     SerializationFactory factory = new SerializationFactory(conf);\n+  }\n+\n+  @Test\n+  public void testGetSerializer() {\n     // Test that a valid serializer class is returned when its present\n-    assertNotNull(\"A valid class must be returned for default Writable Serde\",\n+    assertNotNull(\"A valid class must be returned for default Writable SerDe\",\n         factory.getSerializer(Writable.class));\n-    assertNotNull(\"A valid class must be returned for default Writable serDe\",\n-        factory.getDeserializer(Writable.class));\n     // Test that a null is returned when none can be found.\n     assertNull(\"A null should be returned if there are no serializers found.\",\n         factory.getSerializer(TestSerializationFactory.class));\n+  }\n+\n+  @Test\n+  public void testGetDeserializer() {\n+    // Test that a valid serializer class is returned when its present\n+    assertNotNull(\"A valid class must be returned for default Writable SerDe\",\n+        factory.getDeserializer(Writable.class));\n+    // Test that a null is returned when none can be found.\n     assertNull(\"A null should be returned if there are no deserializers found\",\n         factory.getDeserializer(TestSerializationFactory.class));\n   }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f1f83527ab402f9c5ec27ae9bcc3ff2878712ff1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "c5805be5b47439ada980f5ecb1454161b6ae8b8f",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389002 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d3c448c6a2de061eab5aaf3bfa0ce754609a5db5",
        "patched_files": [
            "SerializationFactory.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop-common_f36bac8": {
        "bug_id": "hadoop-common_f36bac8",
        "commit": "https://github.com/apache/hadoop-common/commit/f36bac8b7873d4e45fda6dfebedff04456d667b2",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f36bac8b7873d4e45fda6dfebedff04456d667b2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=f36bac8b7873d4e45fda6dfebedff04456d667b2",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -87,7 +87,10 @@ Trunk (unreleased changes)\n     HDFS-3037. TestMulitipleNNDataBlockScanner#testBlockScannerAfterRestart is\n     racy. (atm)\n \n-    HDFS-2966 TestNameNodeMetrics tests can fail under load. (stevel)\n+    HDFS-2966. TestNameNodeMetrics tests can fail under load. (stevel)\n+\n+    HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on\n+    corrupted block. (Henry Robinson via atm)\n \n Release 0.23.3 - UNRELEASED \n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f36bac8b7873d4e45fda6dfebedff04456d667b2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3cc339a596e7986ab09a071236546d5c0b042b5b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f36bac8b7873d4e45fda6dfebedff04456d667b2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=f36bac8b7873d4e45fda6dfebedff04456d667b2",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -538,7 +538,9 @@ public synchronized int read(byte buf[], int off, int len) throws IOException {\n       int retries = 2;\n       while (retries > 0) {\n         try {\n-          if (pos > blockEnd) {\n+          // currentNode can be left as null if previous read had a checksum\n+          // error on the same block. See HDFS-3067\n+          if (pos > blockEnd || currentNode == null) {\n             currentNode = blockSeekTo(pos);\n           }\n           int realLen = (int) Math.min(len, (blockEnd - pos + 1L));",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f36bac8b7873d4e45fda6dfebedff04456d667b2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "71c8a500a83b1fa157204dd18a14c16863585a6d",
                "status": "modified"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f36bac8b7873d4e45fda6dfebedff04456d667b2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java?ref=f36bac8b7873d4e45fda6dfebedff04456d667b2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "patch": "@@ -48,6 +48,7 @@\n import org.apache.hadoop.fs.FileChecksum;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.UnresolvedLinkException;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n import org.apache.hadoop.hdfs.protocol.Block;\n@@ -64,6 +65,7 @@\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.ipc.RpcPayloadHeader.RpcKind;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.mockito.internal.stubbing.answers.ThrowsException;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n@@ -649,5 +651,52 @@ public void testClientDNProtocolTimeout() throws IOException {\n       server.stop();\n     }\n   }\n+\n+  /**\n+   * Test that checksum failures are recovered from by the next read on the same\n+   * DFSInputStream. Corruption information is not persisted from read call to\n+   * read call, so the client should expect consecutive calls to behave the same\n+   * way. See HDFS-3067.\n+   */\n+  public void testRetryOnChecksumFailure()\n+      throws UnresolvedLinkException, IOException {\n+    HdfsConfiguration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster =\n+      new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n+\n+    try {\n+      final short REPL_FACTOR = 1;\n+      final long FILE_LENGTH = 512L;\n+      cluster.waitActive();\n+      FileSystem fs = cluster.getFileSystem();\n+\n+      Path path = new Path(\"/corrupted\");\n+\n+      DFSTestUtil.createFile(fs, path, FILE_LENGTH, REPL_FACTOR, 12345L);\n+      DFSTestUtil.waitReplication(fs, path, REPL_FACTOR);\n+\n+      ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, path);\n+      int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);\n+      assertEquals(\"All replicas not corrupted\", REPL_FACTOR,\n+          blockFilesCorrupted);\n+\n+      InetSocketAddress nnAddr =\n+        new InetSocketAddress(\"localhost\", cluster.getNameNodePort());\n+      DFSClient client = new DFSClient(nnAddr, conf);\n+      DFSInputStream dis = client.open(path.toString());\n+      byte[] arr = new byte[(int)FILE_LENGTH];\n+      for (int i = 0; i < 2; ++i) {\n+        try {\n+          dis.read(arr, 0, (int)FILE_LENGTH);\n+          fail(\"Expected ChecksumException not thrown\");\n+        } catch (Exception ex) {\n+          GenericTestUtils.assertExceptionContains(\n+              \"Checksum error\", ex);\n+        }\n+      }\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f36bac8b7873d4e45fda6dfebedff04456d667b2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "sha": "1e39b9a40d1854b90379763f0ac23c94a4e83d7a",
                "status": "modified"
            }
        ],
        "message": "HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on corrupted block. Contributed by Henry Robinson.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301182 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/c5b3707ec25198fad698d4dd6698bebd20579be9",
        "patched_files": [
            "DFSInputStream.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSClientRetries.java"
        ]
    },
    "hadoop-common_f56d99c": {
        "bug_id": "hadoop-common_f56d99c",
        "commit": "https://github.com/apache/hadoop-common/commit/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -561,6 +561,8 @@ Release 2.1.0-beta - UNRELEASED\n \n     HDFS-4382. Fix typo MAX_NOT_CHANGED_INTERATIONS. (Ted Yu via suresh)\n \n+    HDFS-4840. ReplicationMonitor gets NPE during shutdown. (kihwal)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a85341ec5480ca225f9dfb62af0e0a0ffabd4601",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3094,10 +3094,15 @@ public void run() {\n           computeDatanodeWork();\n           processPendingReplications();\n           Thread.sleep(replicationRecheckInterval);\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n-          break;\n         } catch (Throwable t) {\n+          if (!namesystem.isRunning()) {\n+            LOG.info(\"Stopping ReplicationMonitor.\");\n+            if (!(t instanceof InterruptedException)) {\n+              LOG.info(\"ReplicationMonitor received an exception\"\n+                  + \" while shutting down.\", t);\n+            }\n+            break;\n+          }\n           LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n           terminate(1, t);\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "46809da5cb1dca4eaeaef0696b662f86e6bdd197",
                "status": "modified"
            }
        ],
        "message": "HDFS-4840. ReplicationMonitor gets NPE during shutdown. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489634 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/60f27393764d63af7b14b66e106ee9055385873f",
        "patched_files": [
            "BlockManager.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop-common_f5822bd": {
        "bug_id": "hadoop-common_f5822bd",
        "commit": "https://github.com/apache/hadoop-common/commit/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -629,3 +629,6 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-971. distcp does not always remove distcp.tmp.dir. (Aaron Kimball\n     via tomwhite)\n+\n+    MAPREDUCE-995. Fix a bug in JobHistory where tasks completing after the job\n+    is closed cause a NPE. (Jothi Padmanabhan via cdouglas)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/CHANGES.txt",
                "sha": "2464a02711e43999d7b52749b7bc0dfc84e75831",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "deletions": 27,
                "filename": "src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "patch": "@@ -21,7 +21,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Date;\n+import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n import java.util.List;\n@@ -55,7 +55,8 @@\n   final Log LOG = LogFactory.getLog(JobHistory.class);\n \n   private long jobHistoryBlockSize;\n-  private Map<JobID, MetaInfo> fileMap;\n+  private final Map<JobID, MetaInfo> fileMap =\n+    Collections.<JobID,MetaInfo>synchronizedMap(new HashMap<JobID,MetaInfo>());\n   private ThreadPoolExecutor executor = null;\n   static final FsPermission HISTORY_DIR_PERMISSION =\n     FsPermission.createImmutable((short) 0750); // rwxr-x---\n@@ -115,8 +116,6 @@ public void init(JobTracker jt, JobConf conf, String hostname,\n           3 * 1024 * 1024);\n     \n     jobTracker = jt;\n-    \n-    fileMap = new HashMap<JobID, MetaInfo> ();\n   }\n   \n   /** Initialize the done directory and start the history cleaner thread */\n@@ -305,40 +304,28 @@ public void setupEventWriter(JobID jobId, JobConf jobConf)\n   /** Close the event writer for this id */\n   public void closeWriter(JobID id) {\n     try {\n-      EventWriter writer = getWriter(id);\n-      writer.close();\n+      final MetaInfo mi = fileMap.get(id);\n+      if (mi != null) {\n+        mi.closeWriter();\n+      }\n     } catch (IOException e) {\n       LOG.info(\"Error closing writer for JobID: \" + id);\n     }\n   }\n \n-\n-  /**\n-   * Get the JsonEventWriter for the specified Job Id\n-   * @param jobId\n-   * @return\n-   * @throws IOException if a writer is not available\n-   */\n-  private EventWriter getWriter(final JobID jobId) throws IOException {\n-    EventWriter writer = null;\n-    MetaInfo mi = fileMap.get(jobId);\n-    if (mi == null || (writer = mi.getEventWriter()) == null) {\n-      throw new IOException(\"History File does not exist for JobID\");\n-    }\n-    return writer;\n-  }\n-\n   /**\n    * Method to log the specified event\n    * @param event The event to log\n    * @param id The Job ID of the event\n    */\n   public void logEvent(HistoryEvent event, JobID id) {\n     try {\n-      EventWriter writer = getWriter(id);\n-      writer.write(event);\n+      final MetaInfo mi = fileMap.get(id);\n+      if (mi != null) {\n+        mi.writeEvent(event);\n+      }\n     } catch (IOException e) {\n-      LOG.error(\"Error creating writer, \" + e.getMessage());\n+      LOG.error(\"Error Logging event, \" + e.getMessage());\n     }\n   }\n \n@@ -388,7 +375,7 @@ private void moveOldFiles() throws IOException {\n   \n   private void moveToDone(final JobID id) {\n     final List<Path> paths = new ArrayList<Path>();\n-    MetaInfo metaInfo = fileMap.get(id);\n+    final MetaInfo metaInfo = fileMap.get(id);\n     if (metaInfo == null) {\n       LOG.info(\"No file for job-history with \" + id + \" found in cache!\");\n       return;\n@@ -456,7 +443,19 @@ private String getUserName(JobConf jobConf) {\n \n     Path getHistoryFile() { return historyFile; }\n     Path getConfFile() { return confFile; }\n-    EventWriter getEventWriter() { return writer; }\n+\n+    synchronized void closeWriter() throws IOException {\n+      if (writer != null) {\n+        writer.close();\n+      }\n+      writer = null;\n+    }\n+\n+    synchronized void writeEvent(HistoryEvent event) throws IOException {\n+      if (writer != null) {\n+        writer.write(event);\n+      }\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/java/org/apache/hadoop/mapreduce/jobhistory/JobHistory.java",
                "sha": "77644667b2c2acaa1e2fc6bf9f25fd566d132a65",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java?ref=f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4",
                "deletions": 0,
                "filename": "src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "patch": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapred;\n+\n+import java.io.IOException;\n+\n+import junit.framework.TestCase;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapreduce.Counters;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.jobhistory.JobFinishedEvent;\n+import org.apache.hadoop.mapreduce.jobhistory.JobHistory;\n+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser;\n+import org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent;\n+import org.apache.hadoop.mapreduce.jobhistory.TaskFinishedEvent;\n+\n+/**\n+ * Unit test to test if the JobHistory writer/parser is able to handle\n+ * values with special characters\n+ * This test also tests if the job history module is able to gracefully\n+ * ignore events after the event writer is closed\n+ *\n+ */\n+public class TestJobHistoryParsing  extends TestCase {\n+\n+  public void testHistoryParsing() throws IOException {\n+    // open a test history file\n+    Path historyDir = new Path(System.getProperty(\"test.build.data\", \".\"),\n+                                \"history\");\n+    JobConf conf = new JobConf();\n+    conf.set(\"hadoop.job.history.location\", historyDir.toString());\n+    FileSystem fs = FileSystem.getLocal(new JobConf());\n+\n+    // Some weird strings\n+    String username = \"user\";\n+    String weirdJob = \"Value has \\n new line \\n and \" +\n+                    \"dot followed by new line .\\n in it +\" +\n+                    \"ends with escape\\\\\";\n+    String weirdPath = \"Value has characters: \" +\n+                    \"`1234567890-=qwertyuiop[]\\\\asdfghjkl;'zxcvbnm,./\" +\n+                    \"~!@#$%^&*()_+QWERTYUIOP{}|ASDFGHJKL:\\\"'ZXCVBNM<>?\" +\n+                    \"\\t\\b\\n\\f\\\"\\n in it\";\n+\n+    conf.setUser(username);\n+\n+    MiniMRCluster mr = null;\n+    mr = new MiniMRCluster(2, \"file:///\", 3, null, null, conf);\n+\n+    JobTracker jt = mr.getJobTrackerRunner().getJobTracker();\n+    JobHistory jh = jt.getJobHistory();\n+\n+    jh.init(jt, conf, \"localhost\", 1234);\n+    JobID jobId = JobID.forName(\"job_200809171136_0001\");\n+    jh.setupEventWriter(jobId, conf);\n+    JobSubmittedEvent jse =\n+      new JobSubmittedEvent(jobId, weirdJob, username, 12345, weirdPath);\n+    jh.logEvent(jse, jobId);\n+\n+    JobFinishedEvent jfe =\n+      new JobFinishedEvent(jobId, 12346, 1, 1, 0, 0, new Counters());\n+    jh.logEvent(jfe, jobId);\n+    jh.closeWriter(jobId);\n+\n+    // Try to write one more event now, should not fail\n+    TaskID tid = TaskID.forName(\"task_200809171136_0001_m_000002\");\n+    TaskFinishedEvent tfe =\n+      new TaskFinishedEvent(tid, 0, TaskType.MAP, \"\", null);\n+    boolean caughtException = false;\n+\n+    try {\n+      jh.logEvent(tfe, jobId);\n+    } catch (Exception e) {\n+      caughtException = true;\n+    }\n+\n+    assertFalse(\"Writing an event after closing event writer is not handled\",\n+        caughtException);\n+\n+    String historyFileName = jobId.toString() + \"_\" + username;\n+    Path historyFilePath = new Path (historyDir.toString(),\n+      historyFileName);\n+\n+    System.out.println(\"History File is \" + historyFilePath.toString());\n+\n+    JobHistoryParser parser =\n+      new JobHistoryParser(fs, historyFilePath);\n+\n+    JobHistoryParser.JobInfo jobInfo = parser.parse();\n+\n+    assertTrue (jobInfo.getUsername().equals(username));\n+    assertTrue(jobInfo.getJobname().equals(weirdJob));\n+    assertTrue(jobInfo.getJobConfPath().equals(weirdPath));\n+\n+    if (mr != null) {\n+      mr.shutdown();\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f5822bd5c9c1b70e443d98ae82ed00e5c6f6e0c4/src/test/mapred/org/apache/hadoop/mapred/TestJobHistoryParsing.java",
                "sha": "5dc445ec0633fe0f9529f9376fb55ef007a9afcc",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-995. Fix a bug in JobHistory where tasks completing after the job\nis closed cause a NPE. Contributed by Jothi Padmanabhan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@816454 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/c82125320349962f05edc66d819c00db2681db97",
        "patched_files": [
            "JobHistory.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestJobHistoryParsing.java",
            "TestJobHistory.java"
        ]
    },
    "hadoop-common_fb7ece9": {
        "bug_id": "hadoop-common_fb7ece9",
        "commit": "https://github.com/apache/hadoop-common/commit/fb7ece969d62f859de9fdc18941b53b816760161",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=fb7ece969d62f859de9fdc18941b53b816760161",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -126,6 +126,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it\n     to fail occassionally. (Zhijie Shen via vinodkv)\n \n+    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling\n+    Disconnected event from ZK. (Karthik Kambatla via jianhe)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed12a7bc8acf7df38decfbfa771b2a7dea07b881",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=fb7ece969d62f859de9fdc18941b53b816760161",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -280,10 +280,9 @@ public String run() throws KeeperException, InterruptedException {\n     }\n   }\n \n-  private void logRootNodeAcls(String prefix) throws KeeperException,\n-      InterruptedException {\n+  private void logRootNodeAcls(String prefix) throws Exception {\n     Stat getStat = new Stat();\n-    List<ACL> getAcls = zkClient.getACL(zkRootNodePath, getStat);\n+    List<ACL> getAcls = getACLWithRetries(zkRootNodePath, getStat);\n \n     StringBuilder builder = new StringBuilder();\n     builder.append(prefix);\n@@ -363,7 +362,7 @@ protected synchronized void storeVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n     byte[] data =\n         ((RMStateVersionPBImpl) CURRENT_VERSION_INFO).getProto().toByteArray();\n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       setDataWithRetries(versionNodePath, data, -1);\n     } else {\n       createWithRetries(versionNodePath, data, zkAcl, CreateMode.PERSISTENT);\n@@ -374,7 +373,7 @@ protected synchronized void storeVersion() throws Exception {\n   protected synchronized RMStateVersion loadVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n \n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       byte[] data = getDataWithRetries(versionNodePath, true);\n       RMStateVersion version =\n           new RMStateVersionPBImpl(RMStateVersionProto.parseFrom(data));\n@@ -442,7 +441,8 @@ private void loadRMSequentialNumberState(RMState rmState) throws Exception {\n   }\n \n   private void loadRMDelegationTokenState(RMState rmState) throws Exception {\n-    List<String> childNodes = zkClient.getChildren(delegationTokensRootPath, true);\n+    List<String> childNodes =\n+        getChildrenWithRetries(delegationTokensRootPath, true);\n     for (String childNodeName : childNodes) {\n       String childNodePath =\n           getNodePath(delegationTokensRootPath, childNodeName);\n@@ -567,7 +567,7 @@ public synchronized void updateApplicationStateInternal(ApplicationId appId,\n     }\n     byte[] appStateData = appStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, appStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n@@ -610,7 +610,7 @@ public synchronized void updateApplicationAttemptStateInternal(\n     }\n     byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, attemptStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,\n@@ -661,7 +661,7 @@ protected synchronized void removeRMDelegationTokenState(\n       LOG.debug(\"Removing RMDelegationToken_\"\n           + rmDTIdentifier.getSequenceNumber());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       opList.add(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -677,7 +677,7 @@ protected void updateRMDelegationTokenAndSequenceNumberInternal(\n     String nodeRemovePath =\n         getNodePath(delegationTokensRootPath, DELEGATION_TOKEN_PREFIX\n             + rmDTIdentifier.getSequenceNumber());\n-    if (zkClient.exists(nodeRemovePath, true) == null) {\n+    if (existsWithRetries(nodeRemovePath, true) == null) {\n       // in case znode doesn't exist\n       addStoreOrUpdateOps(\n           opList, rmDTIdentifier, renewDate, latestSequenceNumber, false);\n@@ -760,7 +760,7 @@ protected synchronized void removeRMDTMasterKeyState(\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Removing RMDelegationKey_\" + delegationKey.getKeyId());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       doMultiWithRetries(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -891,6 +891,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private List<ACL> getACLWithRetries(\n+      final String path, final Stat stat) throws Exception {\n+    return new ZKAction<List<ACL>>() {\n+      @Override\n+      public List<ACL> run() throws KeeperException, InterruptedException {\n+        return zkClient.getACL(path, stat);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   private List<String> getChildrenWithRetries(\n       final String path, final boolean watch) throws Exception {\n     return new ZKAction<List<String>>() {\n@@ -901,6 +911,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private Stat existsWithRetries(\n+      final String path, final boolean watch) throws Exception {\n+    return new ZKAction<Stat>() {\n+      @Override\n+      Stat run() throws KeeperException, InterruptedException {\n+        return zkClient.exists(path, watch);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   /**\n    * Helper class that periodically attempts creating a znode to ensure that\n    * this RM continues to be the Active.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "9b15bb21e7e62a1dab06a6799dd5a39621c06a4f",
                "status": "modified"
            }
        ],
        "message": "YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling Disconnected event from ZK. Contributed by Karthik Kambatla.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587776 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/31f22748b38fda8442888c8991ba541d7590dbaa",
        "patched_files": [
            "ZKRMStateStore.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop-common_fc2dbd0": {
        "bug_id": "hadoop-common_fc2dbd0",
        "commit": "https://github.com/apache/hadoop-common/commit/fc2dbd08729737553300a66aae55a5220ae8d246",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fc2dbd08729737553300a66aae55a5220ae8d246/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=fc2dbd08729737553300a66aae55a5220ae8d246",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -265,6 +265,9 @@ Release 2.0.3-alpha - Unreleased\n \n     HDFS-3964. Make NN log of fs.defaultFS debug rather than info. (eli)\n \n+    HDFS-3992. Method org.apache.hadoop.hdfs.TestHftpFileSystem.tearDown()\n+    sometimes throws NPEs. (Ivan A. Veselovsky via atm)\n+\n Release 2.0.2-alpha - 2012-09-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fc2dbd08729737553300a66aae55a5220ae8d246/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "931dea087408d9b23b477708e12b16fa1de234fc",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fc2dbd08729737553300a66aae55a5220ae8d246/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java?ref=fc2dbd08729737553300a66aae55a5220ae8d246",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java",
                "patch": "@@ -102,9 +102,15 @@ public static void setUp() throws IOException {\n   \n   @AfterClass\n   public static void tearDown() throws IOException {\n-    hdfs.close();\n-    hftpFs.close();\n-    cluster.shutdown();\n+    if (hdfs != null) {\n+      hdfs.close();\n+    }\n+    if (hftpFs != null) {\n+      hftpFs.close();\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fc2dbd08729737553300a66aae55a5220ae8d246/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java",
                "sha": "6cb0ad1ce83701d03dc77d8f30137f686637148a",
                "status": "modified"
            }
        ],
        "message": "HDFS-3992. Method org.apache.hadoop.hdfs.TestHftpFileSystem.tearDown() sometimes throws NPEs. Contributed by Ivan A. Veselovsky.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1391763 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6df04098c8137d6ec82611f2f322ead7c638afc2",
        "patched_files": [
            "HftpFileSystem.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHftpFileSystem.java"
        ]
    },
    "hadoop-common_fca1ffb": {
        "bug_id": "hadoop-common_fca1ffb",
        "commit": "https://github.com/apache/hadoop-common/commit/fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -893,3 +893,7 @@ Release 0.21.0 - Unreleased\n     cdouglas)\n \n     MAPREDUCE-915. The debug scripts are run as the job user. (ddas)\n+\n+    MAPREDUCE-1007. Fix NPE in CapacityTaskScheduler.getJobs(). \n+    (V.V.Chaitanya Krishna via sharad)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/CHANGES.txt",
                "sha": "4f47403239cb191f7bc0b68b787dece264504bdc",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "deletions": 2,
                "filename": "src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java",
                "patch": "@@ -989,13 +989,17 @@ private synchronized void updateContextObjects(int mapClusterCapacity,\n   @Override\n   public synchronized Collection<JobInProgress> getJobs(String queueName) {\n     Collection<JobInProgress> jobCollection = new ArrayList<JobInProgress>();\n+    JobQueue jobQueue = jobQueuesManager.getJobQueue(queueName);\n+    if (jobQueue == null) {\n+      return jobCollection;\n+    }\n     Collection<JobInProgress> runningJobs =\n-      jobQueuesManager.getJobQueue(queueName).getRunningJobs();\n+      jobQueue.getRunningJobs();\n     if (runningJobs != null) {\n       jobCollection.addAll(runningJobs);\n     }\n     Collection<JobInProgress> waitingJobs = \n-      jobQueuesManager.getJobQueue(queueName).getWaitingJobs();\n+      jobQueue.getWaitingJobs();\n     Collection<JobInProgress> tempCollection = new ArrayList<JobInProgress>();\n     if(waitingJobs != null) {\n       tempCollection.addAll(waitingJobs);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacityTaskScheduler.java",
                "sha": "04c6f2dd2d8a7ca507c05a09fe26e2d2a6b0c1ee",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "deletions": 50,
                "filename": "src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.mapreduce.server.jobtracker.TaskTracker;\n import static org.apache.hadoop.mapred.CapacityTestUtils.*;\n \n+import java.io.File;\n import java.io.IOException;\n import java.util.*;\n \n@@ -36,6 +37,11 @@\n   static final Log LOG =\n     LogFactory.getLog(org.apache.hadoop.mapred.TestCapacityScheduler.class);\n \n+  String queueConfigPath =\n+    System.getProperty(\"test.build.extraconf\", \"build/test/extraconf\");\n+  File queueConfigFile =\n+    new File(queueConfigPath, QueueManager.QUEUE_CONF_FILE_NAME);\n+\n   private static int jobCounter;\n \n   private ControlledInitializationPoller controlledInitializationPoller;\n@@ -341,65 +347,55 @@ public void testJobFinished() throws Exception {\n     taskTrackerManager.finishTask(\"attempt_test_0002_m_000003_0\", j2);\n   }\n \n-  // basic tests, should be able to submit to queues\n-  public void testSubmitToQueues() throws Exception {\n-    // set up some queues\n-    String[] qs = {\"default\", \"q2\"};\n-    taskTrackerManager.addQueues(qs);\n-    ArrayList<FakeQueueInfo> queues = new ArrayList<FakeQueueInfo>();\n-    queues.add(new FakeQueueInfo(\"default\", 50.0f, true, 25));\n-    queues.add(new FakeQueueInfo(\"q2\", 50.0f, true, 25));\n-\n-\n-    taskTrackerManager.setFakeQueues(queues);\n+  /**\n+   * tests the submission of jobs to container and job queues\n+   * @throws Exception\n+   */\n+  public void testJobSubmission() throws Exception {\n+    JobQueueInfo[] queues = TestQueueManagerRefresh.getSimpleQueueHierarchy();\n+\n+    queues[0].getProperties().setProperty(\n+        CapacitySchedulerConf.CAPACITY_PROPERTY, String.valueOf(100));\n+    queues[1].getProperties().setProperty(\n+        CapacitySchedulerConf.CAPACITY_PROPERTY, String.valueOf(50));\n+    queues[2].getProperties().setProperty(\n+        CapacitySchedulerConf.CAPACITY_PROPERTY, String.valueOf(50));\n+\n+    // write the configuration file\n+    QueueManagerTestUtils.writeQueueConfigurationFile(\n+        queueConfigFile.getAbsolutePath(), new JobQueueInfo[] { queues[0] });\n+    setUp(1, 4, 4);\n+    // use the queues from the config file.\n+    taskTrackerManager.setQueueManager(new QueueManager());\n     scheduler.start();\n \n-    // submit a job with no queue specified. It should be accepted\n-    // and given to the default queue. \n-    JobInProgress j = taskTrackerManager.submitJobAndInit(JobStatus.PREP, \n-                                                    10, 10, null, \"u1\");\n-    // when we ask for tasks, we should get them for the job submitted\n-    Map<String, String> expectedTaskStrings = new HashMap<String, String>();\n-    expectedTaskStrings.put(CapacityTestUtils.MAP, \n-                            \"attempt_test_0001_m_000001_0 on tt1\");\n-    expectedTaskStrings.put(CapacityTestUtils.REDUCE, \n-                            \"attempt_test_0001_r_000001_0 on tt1\");\n-    checkMultipleTaskAssignment(taskTrackerManager, scheduler, \n-                                      \"tt1\", expectedTaskStrings);\n-\n-    // submit another job, to a different queue\n-    j = taskTrackerManager.submitJobAndInit(JobStatus.PREP, 10, 10, \"q2\", \"u1\");\n-    // now when we get tasks, it should be from the second job\n-    expectedTaskStrings.clear();\n-    expectedTaskStrings.put(CapacityTestUtils.MAP,\n-                              \"attempt_test_0002_m_000001_0 on tt2\");\n-    expectedTaskStrings.put(CapacityTestUtils.REDUCE,\n-                              \"attempt_test_0002_r_000001_0 on tt2\");\n-    checkMultipleTaskAssignment(taskTrackerManager, scheduler, \n-                                  \"tt2\", expectedTaskStrings);\n-  }\n+    // submit a job to the container queue\n+    try {\n+      taskTrackerManager.submitJobAndInit(JobStatus.PREP, 20, 0,\n+          queues[0].getQueueName(), \"user\");\n+      fail(\"Jobs are being able to be submitted to the container queue\");\n+    } catch (Exception e) {\n+      assertTrue(scheduler.getJobs(queues[0].getQueueName()).isEmpty());\n+    }\n \n-  public void testGetJobs() throws Exception {\n-    // need only one queue\n-    String[] qs = {\"default\"};\n-    taskTrackerManager.addQueues(qs);\n-    ArrayList<FakeQueueInfo> queues = new ArrayList<FakeQueueInfo>();\n-    queues.add(new FakeQueueInfo(\"default\", 100.0f, true, 100));\n+    FakeJobInProgress job = taskTrackerManager.submitJobAndInit(JobStatus.PREP,\n+        1, 0, queues[1].getQueueName(), \"user\");\n+    assertEquals(1, scheduler.getJobs(queues[1].getQueueName()).size());\n+    assertTrue(scheduler.getJobs(queues[1].getQueueName()).contains(job));\n \n+    // check if the job is submitted\n+    checkAssignment(taskTrackerManager, scheduler, \"tt1\", \n+    \"attempt_test_0002_m_000001_0 on tt1\");\n \n-    taskTrackerManager.setFakeQueues(queues);\n-    scheduler.start();\n+    // test for getJobs\n     HashMap<String, ArrayList<FakeJobInProgress>> subJobsList =\n-      taskTrackerManager.submitJobs(1, 4, \"default\");\n+      taskTrackerManager.submitJobs(1, 4, queues[2].getQueueName());\n \n     JobQueuesManager mgr = scheduler.jobQueuesManager;\n-\n-    while (mgr.getJobQueue(\"default\").getWaitingJobs().size() < 4) {\n-      Thread.sleep(1);\n-    }\n     //Raise status change events for jobs submitted.\n-    raiseStatusChangeEvents(mgr);\n-    Collection<JobInProgress> jobs = scheduler.getJobs(\"default\");\n+    raiseStatusChangeEvents(mgr, queues[2].getQueueName());\n+    Collection<JobInProgress> jobs =\n+      scheduler.getJobs(queues[2].getQueueName());\n \n     assertTrue(\n       \"Number of jobs returned by scheduler is wrong\"",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/contrib/capacity-scheduler/src/test/org/apache/hadoop/mapred/TestCapacityScheduler.java",
                "sha": "c93616490d52d9a735dc58ab626bc32c228420f8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/mapred/JobTracker.java?ref=fca1ffb03358d9234d5e5fa59d91d6f4a816af41",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/mapred/JobTracker.java",
                "patch": "@@ -3962,7 +3962,10 @@ public QueueInfo getQueue(String queue) throws IOException {\n \n   public org.apache.hadoop.mapreduce.JobStatus[] getJobsFromQueue(String queue) \n       throws IOException {\n-    Collection<JobInProgress> jips = taskScheduler.getJobs(queue);\n+    Collection<JobInProgress> jips = null;\n+    if (queueManager.getLeafQueueNames().contains(queue)) {\n+      jips = taskScheduler.getJobs(queue);\n+    }\n     return getJobStatus(jips,false);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fca1ffb03358d9234d5e5fa59d91d6f4a816af41/src/java/org/apache/hadoop/mapred/JobTracker.java",
                "sha": "fe271dea3d208e5e0c054ac6db07092ea25767fb",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1007. Fix NPE in CapacityTaskScheduler.getJobs(). Contributed by V.V.Chaitanya Krishna.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@882470 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/978e6a360cdafc522eb5233895040e2d3f3a4b21",
        "patched_files": [
            "JobTracker.java",
            "CapacityTaskScheduler.java",
            "CHANGES.txt"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop-common_fd45dc1": {
        "bug_id": "hadoop-common_fd45dc1",
        "commit": "https://github.com/apache/hadoop-common/commit/fd45dc18fb4d4349e812b936ad269fb68b3ee447",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=fd45dc18fb4d4349e812b936ad269fb68b3ee447",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -125,6 +125,10 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-2764. Fix renewal of dfs delegation tokens. (Owen via jitendra)\n \n+    HDFS-2439. Fix NullPointerException in webhdfs when opening a non-existing\n+    file or creating a file without specifying the replication parameter.\n+    (szetszwo)\n+\n Release 0.23.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "24232461fb30fc4f20a266142876386fee6acd81",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java?ref=fd45dc18fb4d4349e812b936ad269fb68b3ee447",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.hadoop.hdfs;\n \n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.net.HttpURLConnection;\n@@ -107,6 +108,8 @@ private InputStream getInputStream() throws IOException {\n           HftpFileSystem.LOG.debug(\"filelength = \" + filelength);\n         }\n         in = connection.getInputStream();\n+      } catch (FileNotFoundException fnfe) {\n+        throw fnfe;\n       } catch (IOException ioe) {\n         HftpFileSystem.throwIOExceptionFromConnection(connection, ioe);\n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
                "sha": "e2e8bf37c423849219d62495486bf482094b2b81",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java?ref=fd45dc18fb4d4349e812b936ad269fb68b3ee447",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "patch": "@@ -131,7 +131,7 @@ public Response run() throws IOException, URISyntaxException {\n           fullpath, permission.getFsPermission(), \n           overwrite.getValue() ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n               : EnumSet.of(CreateFlag.CREATE),\n-          replication.getValue(), blockSize.getValue(conf), null, b), null);\n+          replication.getValue(conf), blockSize.getValue(conf), null, b), null);\n       try {\n         IOUtils.copyBytes(in, out, b);\n       } finally {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "sha": "aaa1fd0173b8ebbc0dfba29caa147022f42b0d79",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java?ref=fd45dc18fb4d4349e812b936ad269fb68b3ee447",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "patch": "@@ -46,6 +46,7 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.ContentSummary;\n import org.apache.hadoop.fs.Options;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n@@ -119,6 +120,9 @@ private static DatanodeInfo chooseDatanode(final NameNode namenode,\n         || op == PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np = namenode.getRpcServer();\n       final HdfsFileStatus status = np.getFileInfo(path);\n+      if (status == null) {\n+        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n+      }\n       final long len = status.getLen();\n       if (op == GetOpParam.Op.OPEN && (openOffset < 0L || openOffset >= len)) {\n         throw new IOException(\"Offset=\" + openOffset + \" out of the range [0, \"\n@@ -238,6 +242,7 @@ public Response run() throws IOException, URISyntaxException {\n         try {\n \n     final String fullpath = path.getAbsolutePath();\n+    final Configuration conf = (Configuration)context.getAttribute(JspHelper.CURRENT_CONF);\n     final NameNode namenode = (NameNode)context.getAttribute(\"name.node\");\n     final NamenodeProtocols np = namenode.getRpcServer();\n \n@@ -259,7 +264,6 @@ public Response run() throws IOException, URISyntaxException {\n     {\n       final EnumSet<Options.Rename> s = renameOptions.getValue();\n       if (s.isEmpty()) {\n-        @SuppressWarnings(\"deprecation\")\n         final boolean b = np.rename(fullpath, dstPath.getValue());\n         final String js = JsonUtil.toJsonString(\"boolean\", b);\n         return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n@@ -271,7 +275,7 @@ public Response run() throws IOException, URISyntaxException {\n     }\n     case SETREPLICATION:\n     {\n-      final boolean b = np.setReplication(fullpath, replication.getValue());\n+      final boolean b = np.setReplication(fullpath, replication.getValue(conf));\n       final String js = JsonUtil.toJsonString(\"boolean\", b);\n       return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "sha": "b5152b4558a3d0c6154884e788ce96cfdca58d9b",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java?ref=fd45dc18fb4d4349e812b936ad269fb68b3ee447",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java",
                "patch": "@@ -17,6 +17,11 @@\n  */\n package org.apache.hadoop.hdfs.web.resources;\n \n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_REPLICATION_DEFAULT;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_REPLICATION_KEY;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n /** Replication parameter. */\n public class ReplicationParam extends ShortParam {\n   /** Parameter name. */\n@@ -46,4 +51,10 @@ public ReplicationParam(final String str) {\n   public String getName() {\n     return NAME;\n   }\n+\n+  /** @return the value or, if it is null, return the default from conf. */\n+  public short getValue(final Configuration conf) {\n+    return getValue() != null? getValue()\n+        : (short)conf.getInt(DFS_REPLICATION_KEY, DFS_REPLICATION_DEFAULT);\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java",
                "sha": "1eee7ee34d43c6c9f98ef03afb2f377d50c35352",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java?ref=fd45dc18fb4d4349e812b936ad269fb68b3ee447",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hdfs.web;\n \n import java.io.BufferedReader;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStreamReader;\n import java.net.HttpURLConnection;\n@@ -28,6 +29,7 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileSystemContractBaseTest;\n import org.apache.hadoop.fs.Path;\n@@ -158,4 +160,16 @@ public void testCaseInsensitive() throws IOException {\n     //check if the command successes.\n     assertTrue(fs.getFileStatus(p).isDirectory());\n   }\n+\n+  public void testOpenNonExistFile() throws IOException {\n+    final Path p = new Path(\"/test/testOpenNonExistFile\");\n+    //open it as a file, should get FileNotFoundException \n+    try {\n+      final FSDataInputStream in = fs.open(p);\n+      in.read();\n+      fail();\n+    } catch(FileNotFoundException fnfe) {\n+      WebHdfsFileSystem.LOG.info(\"This is expected.\", fnfe);\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fd45dc18fb4d4349e812b936ad269fb68b3ee447/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "sha": "7cc938709b175b3049e0812d086fdf9618b955cd",
                "status": "modified"
            }
        ],
        "message": "HDFS-2439. Fix NullPointerException in webhdfs when opening a non-existing file or creating a file without specifying the replication parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183554 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/7d80cb7db26fc02cf00b416e07c5b722d71e850d",
        "patched_files": [
            "DatanodeWebHdfsMethods.java",
            "NamenodeWebHdfsMethods.java",
            "CHANGES.txt",
            "ReplicationParam.java",
            "ByteRangeInputStream.java"
        ],
        "repo": "hadoop-common",
        "unit_tests": [
            "TestByteRangeInputStream.java",
            "TestWebHdfsFileSystemContract.java"
        ]
    }
}