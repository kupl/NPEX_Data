[
    {
        "repo": "beam",
        "commit": "https://github.com/apache/beam/commit/4078c22fde9501bc28a5119b6f59522261776106",
        "bug_id": "beam_4078c22",
        "message": "This closes #2610: Merge master into gearpump-runner branch\n\n  Update gearpump-runner against master changes.\n  add temp dataset location for non-query BigQuerySource\n  added module option, use more common zero test, show module name in log\n  Modify types for input PCollections of Flatten transform to that of the output PCollection\n  [BEAM-1871] Remove another depedendency by moving TestCredential\n  [BEAM-2017] Fix NPE in DataflowRunner when there are no metrics\n  [BEAM-2013] Upgrade to Jackson 2.8.8\n  [BEAM-2014] Upgrade to Google Auth 0.6.1\n  [BEAM-2015] Remove shared profile in runners/pom.xml and fix Dataflow ValidatesRunner PostCommit\n  Cache result of BigQuerySourceBase.split\n  Ensure all Read outputs are consumed in Dataflow\n  [BEAM-1441] Remove deprecated ChannelFactory\n  [BEAM-1994] Remove Flink examples package\n  Pin default commons-compress version to beam-parent pom\n  [BEAM-1914]\u00a0XmlIO now complies with PTransform style guide\n  Separate streaming writes into two pluggable components - CreateTables, and StreamingWriteTables. Also address many code review comments. Also merge with master.\n  Fix tests to properly fake out BigQueryService, and add tests for dynamic-table functionality.\n  Refactor batch loads, and add support for windowed writes.\n  Refactor batch load job path, and add support for data-dependent tables.\n  Refactor streaming write branch into separate reusable components.\n  Add PrepareWrite transform.\n  Use tableRefFunction throughout BigQueryIO. Constant table writes use ConstantTableSpecFunction.\n  Explodes windows before GBKIKWI\n  Creates ProcessFnRunner and wires it through ParDoEvaluator\n  Extracts interface from PushbackSideInputDoFnRunner\n  Minor cleanups in ParDoEvaluator\n  ProcessFn remembers more info about its application context\n  Separates side input test and side output test\n  Changed snappy version to 1.1.4-M3\n  Upgrade worker to not depend on deprecated now deleted code\n  Delete AppEngineEnvironment\n  Delete IntervalBoundedExponentialBackoff\n  Delete AttemptBoundedExponentialBackoff\n  Remove deprecated/unused code from Pipeline\n  Remove deprecated method in IOChannelUtils\n  Delete deprecated AttemptAndTimeBoundedExponentialBackoff\n  [BEAM-1871] Create new GCP core module package and move several GCP related classes from beam-sdks-java-core over.\n  [BEAM-1964] Upgrade Pylint\n  Remove options_id concept from templated runs.\n  Revert \"Revert \"Throw specialized exception in value providers\"\"\n  Revert \"Revert \"Revert \"Revert \"Add ValueProvider class for FileBasedSource I/O Transforms\"\"\"\"\n  Removes unused validation parameter\n  Converts TFRecordIO.Write to AutoValue\n  Gets rid of TFRecordIO.Write.Bound\n  Converts TFRecordIO.Read to AutoValue\n  Gets rid of TFRecordIO.Read.Bound\n  runners-core-construction-java fix artifact name\n  Rename SideOutputValue to OutputValue\n  [BEAM-1990] Comment: Don't use Window.Assign\n  [BEAM-1272] Align the naming of \"generateInitialSplits\" and \"splitIntoBundles\" to better reflect their intention\n  Revert \"Removes final minor usages of OldDoFn outside OldDoFn itself\"\n  Fix Hadoop pom.xml\n  Making metrics usage in datastore_wordcount consistent\n  Remove overloading of __call__ in DirectRunner\n  Clean up DirectRunner Clock and TransformResult\n  Translate PTransforms to and from Runner API Protos\n  [BEAM-1993] Remove special unbounded Flink source/sink\n  Remove flink-annotations dependency\n  Fix Javadoc warnings on Flink Runner\n  Enable flink dependency enforcement and make dependencies explicit\n  [BEAM-59] Register standard FileSystems wherever we register IOChannelFactories\n  [BEAM-1991] Sum.SumDoubleFn => Sum.ofDoubles\n  clean up description for sdk_location\n  Set the Project of a Table Reference at Runtime\n  Only compile HIFIO ITs when compiling with java 8.\n  Update assertions of source_test_utils from camelcase to underscore-separated.\n  Add no-else return to pylintrc\n  Remove getSideInputWindow\n  Remove reference to the isStreaming flag\n  Javadoc fixups after style guide changes\n  Update Dataflow Worker Version\n  [BEAM-1922] Close datasource in JdbcIO when possible\n  Fix javadoc warnings\n  Add javadoc to getCheckpointMark in UnboundedSource\n  Removes final minor usages of OldDoFn outside OldDoFn itself\n  [BEAM-1915] Removes use of OldDoFn from Apex\n  Update Signature of PTransformOverrideFactory\n  [BEAM-1964] Fix lint issues and pylint upgrade\n  Rename DoFn.Context#sideOutput to output\n  [BEAM-1964] Fix lint issues for linter upgrade -3\n  [BEAM-1964] Fix lint issues for linter upgrade -2\n  Avoi repackaging bigtable classes in dataflow runner.\n  ApexRunner: register standard IOs when deserializing pipeline options\n  Add PCollections Utilities\n  Free PTransform Names if they are being Replaced\n  [BEAM-1347] Update protos related to State API for prototyping purposes.\n  Update java8 examples pom files to include maven-shade-plugin.\n  fix the simplest typo\n  [BEAM-1964] Fix lint issues for linter upgrade\n  Merge PR#2423: Add Kubernetes scripts for clusters for Performance and Integration tests of Cassandra and ES for Hadoop Input Format IO\n  Remove Triggers.java from SDK entirely\n  [BEAM-1708] Improve error message when GCP not installed\n  Improve gcloud logging message\n  [BEAM-1101, BEAM-1068] Remove service account name credential pipeline options\n  Update user_score.py\n  Pin versions in tox script\n  Improve Empty Create Default Coder Error Message\n  Represent a Pipeline via a list of Top-level Transforms\n  Test all Known Coders to ensure they Serialize via URN\n  [BEAM-1950] Add missing 'static' keyword to MicrobatchSource#initReaderCache\n  ...",
        "parent": "https://github.com/apache/beam/commit/ebbb6139057deda05691fc357799506e5f9f3bf2",
        "patched_files": [
            "FlinkStatefulDoFnFunction.java",
            "settings.xml",
            "TriggerExample.java",
            "UnboundedReadFromBoundedSource.java",
            "OutputAndTimeBoundedSplittableProcessElementInvoker.java",
            "InMemoryStateInternals.java",
            "WriteWindowedFilesDoFn.java",
            "KafkaIOExamples.java",
            "FlinkPipelineTranslator.java",
            "ViewEvaluatorFactory.java",
            "UnboundedReadEvaluatorFactory.java",
            "BoundedReadEvaluatorFactory.java",
            "ReduceFnRunner.java",
            "StreamingWordExtract.java",
            "TranslationContext.java",
            "PTransformReplacements.java",
            "TransformExecutorServices.java",
            "ApexPipelineTranslator.java",
            "GroupByKeyTranslator.java",
            "data-load.sh",
            "elasticsearch-service-for-local-dev.yaml",
            "CombinePerKeyExamples.java",
            "UnboundedFlinkSink.java",
            "ApexRunner.java",
            "data-load-setup.sh",
            "FlinkMergingNonShuffleReduceFunction.java",
            "PrimitiveCreate.java",
            "package-info.java",
            "GroupAlsoByWindowsProperties.java",
            "ParDoEvaluatorFactory.java",
            "UnconsumedReads.java",
            "SplittableProcessElementsEvaluatorFactory.java",
            "WordCount.java",
            "test_wordcount.sh",
            "TransformEvaluatorFactory.java",
            "job_seed.groovy",
            "MaxPerKeyExamples.java",
            "LeaderBoard.java",
            "start-up.sh",
            "WindowingInternals.java",
            "FlinkPartialReduceFunction.java",
            "job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
            "DirectMetrics.java",
            "es_test_data.py",
            "ForwardingPTransform.java",
            "ParDoBoundTranslator.java",
            "Injector.java",
            "FlinkStreamingTransformTranslators.java",
            "FlinkMergingReduceFunction.java",
            "ApexProcessFnOperator.java",
            "job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
            "teardown.sh",
            "BigQueryTornadoes.java",
            "cassandra-svc-temp.yaml",
            "FlinkAssignContext.java",
            "NoOpStepContext.java",
            "StatefulParDoEvaluatorFactory.java",
            "WindowingInternalsAdapters.java",
            "StateMerging.java",
            "SplittableProcessElementInvoker.java",
            "TfIdf.java",
            "postgres.yml",
            "SimplePushbackSideInputDoFnRunner.java",
            "WindowAssignTranslator.java",
            "AutoComplete.java",
            "CopyOnAccessInMemoryStateInternals.java",
            "job_beam_PerformanceTests_JDBC.groovy",
            "FlattenPCollectionTranslator.java",
            "DirectRunner.java",
            "TransformExecutorService.java",
            "ViewOverrideFactory.java",
            "PULL_REQUEST_TEMPLATE.md",
            "UnsupportedOverrideFactory.java",
            "FlinkRunnerResult.java",
            "ReduceFnTester.java",
            "CoderTypeInformation.java",
            "job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
            "UnboundedTextSource.java",
            "AfterWatermarkStateMachine.java",
            "postgres-service-for-local-dev.yml",
            "DeduplicatedFlattenFactory.java",
            "FlinkSideInputReader.java",
            "KafkaWindowedWordCountExample.java",
            "README.md",
            "NonEmptyPanes.java",
            "WindowedWordCountIT.java",
            "GameStats.java",
            "ParDoTranslator.java",
            "ValuesSource.java",
            "WindowingStrategies.java",
            "ExecutionContext.java",
            "DirectGraphVisitor.java",
            "PipelineExecutor.java",
            "job_beam_PerformanceTests_Spark.groovy",
            "es-services-deployments.yaml",
            "SingleInputOutputOverrideFactory.java",
            "TransformEvaluatorRegistry.java",
            "job_beam_PerformanceTests_Dataflow.groovy",
            "StateTag.java",
            "job_beam_PreCommit_Website_Test.groovy",
            "ModelEnforcement.java",
            "FlinkNoOpStepContext.java",
            "PTransforms.java",
            "FlinkBatchTranslationContext.java",
            "Triggers.java",
            "FlattenEvaluatorFactory.java",
            "DirectGroupByKey.java",
            "SideInputHandler.java",
            "cassandra-svc-statefulset.yaml",
            "CollectionSource.java",
            "FlinkPipelineOptions.java",
            "PipelineTranslationOptimizer.java",
            "WriteWithShardingFactory.java",
            "TFIDF.java",
            "UserScore.java",
            "FlinkAssignWindows.java",
            "GroupAlsoByWindowViaWindowSetDoFn.java",
            "elasticsearch-svc-rc.yaml",
            "FlinkStreamingViewOverrides.java",
            "CoderTypeSerializer.java",
            "FlinkStreamingPipelineTranslator.java",
            "SimpleDoFnRunner.java",
            "Coders.java",
            ".travis.yml",
            "SideInputInitializer.java",
            "job_beam_PreCommit_Java_MavenInstall.groovy",
            "FlinkAggregatorFactory.java",
            "SystemReduceFn.java",
            "ExecutorServiceParallelExecutor.java",
            "ApexStateInternals.java",
            "WriteToBigQuery.java",
            "GroupByKeyOnlyEvaluatorFactory.java",
            "ParDoEvaluator.java",
            "ApexParDoOperator.java",
            "StateInternalsProxy.java",
            "ParDoSingleViaMultiOverrideFactory.java",
            "cassandra-svc-rc.yaml",
            "OldDoFn.java",
            "WriteOneFilePerWindow.java",
            "show-health.sh",
            "TranslationMode.java",
            ".gitignore",
            "job_beam_Release_NightlySnapshot.groovy",
            "StatefulDoFnRunner.java",
            "AfterPaneStateMachine.java",
            "WindowEvaluatorFactory.java",
            "SerializablePipelineOptions.java",
            "TestStreamEvaluatorFactory.java",
            "DistinctExample.java",
            "FlinkMergingPartialReduceFunction.java",
            "TopWikipediaSessions.java",
            "EmptyFlattenAsCreateFactory.java",
            "SideInputContainer.java",
            "DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
            "TrafficRoutes.java",
            "UnboundedFlinkSource.java",
            "GroupAlsoByWindowViaWindowSetNewDoFn.java",
            "PTransformMatchers.java",
            "MinimalWordCountJava8.java",
            "StateTags.java",
            "job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
            "FlinkDetachedRunnerResult.java",
            "FlinkCoder.java",
            "WindowedWordCount.java",
            "job_beam_PostCommit_Java_MavenInstall.groovy",
            "DirectOptions.java",
            "ProcessFnRunner.java",
            "FlinkStreamingTranslationContext.java",
            "FlinkPipelineExecutionEnvironment.java",
            "WatermarkManager.java",
            "BaseExecutionContext.java",
            "JoinExamples.java",
            "ReduceFnContextFactory.java",
            "NoOpOldDoFn.java",
            "job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
            "FlinkMultiOutputPruningFunction.java",
            "ParDoMultiOverrideFactory.java",
            "FlinkRunner.java",
            "OutputWindowedValue.java",
            "EncodedValueComparator.java",
            "job_beam_PreCommit_Website_Stage.groovy",
            "EvaluationContext.java",
            "TrafficMaxLaneFlow.java",
            "common_job_properties.groovy",
            "job_beam_PostCommit_Python_Verify.groovy",
            "FlinkRunnerRegistrar.java",
            "FlinkBatchTransformTranslators.java",
            "DebuggingWordCount.java",
            "WriteWindowedToBigQuery.java",
            "SdkComponents.java",
            "DirectGroupByKeyOverrideFactory.java",
            "ReplacementOutputs.java",
            "DoFnLifecycleManagerRemovingTransformEvaluator.java",
            "ApexYarnLauncher.java",
            "GroupAlsoByWindowViaOutputBufferDoFn.java",
            "ApexGroupByKeyOperator.java",
            "FlinkReduceFunction.java",
            "DoFnAdapters.java",
            "pom.xml",
            "HourlyTeamScore.java",
            "SplittableParDo.java",
            "show_health.sh",
            "DefaultParallelismFactory.java",
            "FlinkBatchPipelineTranslator.java",
            "cassandra-service-for-local-dev.yaml",
            "FilterExamples.java",
            "PushbackSideInputDoFnRunner.java",
            "FlinkDoFnFunction.java",
            "SimpleOldDoFnRunner.java",
            "PCollections.java",
            "KeyedPValueTrackingVisitor.java",
            "AfterDelayFromFirstElementStateMachine.java",
            "DoFnRunners.java",
            "GroupAlsoByWindowEvaluatorFactory.java"
        ],
        "file": [
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.github/PULL_REQUEST_TEMPLATE.md",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.github/PULL_REQUEST_TEMPLATE.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".github/PULL_REQUEST_TEMPLATE.md",
                "deletions": 1,
                "sha": "9bbc9f73770d5209adda11fc47ce59f2c43b0518",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.github/PULL_REQUEST_TEMPLATE.md",
                "patch": "@@ -8,6 +8,6 @@ quickly and easily:\n  - [ ] Replace `<Jira issue #>` in the title with the actual Jira issue\n        number, if there is one.\n  - [ ] If this contribution is large, please file an Apache\n-       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).\n+       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.pdf).\n \n ---",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.gitignore",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.gitignore?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".gitignore",
                "deletions": 0,
                "sha": "69946a9224185b25fb499a164fedde9912ac474e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.gitignore",
                "patch": "@@ -19,6 +19,9 @@ build/\n dist/\n distribute-*\n env/\n+sdks/python/**/*.c\n+sdks/python/**/*.so\n+sdks/python/**/*.egg\n \n # Ignore IntelliJ files.\n .idea/",
                "changes": 3
            },
            {
                "status": "renamed",
                "additions": 40,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/common_job_properties.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/common_job_properties.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/common_job_properties.groovy",
                "previous_filename": ".jenkins/common_job_properties.groovy",
                "deletions": 2,
                "sha": "ee102812d413f1dec76ddf7ff0c911f163c70194",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/common_job_properties.groovy",
                "patch": "@@ -69,6 +69,7 @@ class common_job_properties {\n         branch('${sha1}')\n         extensions {\n           cleanAfterCheckout()\n+          pruneBranches()\n         }\n       }\n     }\n@@ -205,7 +206,8 @@ class common_job_properties {\n   static void setPostCommit(context,\n                             String buildSchedule = '0 */6 * * *',\n                             boolean triggerEveryPush = true,\n-                            String notifyAddress = 'commits@beam.apache.org') {\n+                            String notifyAddress = 'commits@beam.apache.org',\n+                            boolean emailIndividuals = true) {\n     // Set build triggers\n     context.triggers {\n       // By default runs every 6 hours.\n@@ -217,7 +219,43 @@ class common_job_properties {\n \n     context.publishers {\n       // Notify an email address for each failed build (defaults to commits@).\n-      mailer(notifyAddress, false, true)\n+      mailer(notifyAddress, false, emailIndividuals)\n+    }\n+  }\n+\n+  // Configures the argument list for performance tests, adding the standard\n+  // performance test job arguments.\n+  private static def genPerformanceArgs(def argMap) {\n+    def standard_args = [\n+      project: 'apache-beam-testing',\n+      dpb_log_level: 'INFO',\n+      maven_binary: '/home/jenkins/tools/maven/latest/bin/mvn',\n+      bigquery_table: 'beam_performance.pkb_results',\n+      // Publishes results with official tag, for use in dashboards.\n+      official: 'true'\n+    ]\n+    // Note: in case of key collision, keys present in ArgMap win.\n+    def joined_args = standard_args.plus(argMap)\n+    def argList = []\n+    joined_args.each({\n+        // FYI: Replacement only works with double quotes.\n+        key, value -> argList.add(\"--$key=$value\")\n+    })\n+    return argList.join(' ')\n+  }\n+\n+  // Adds the standard performance test job steps.\n+  static def buildPerformanceTest(def context, def argMap) {\n+    def pkbArgs = genPerformanceArgs(argMap)\n+    context.steps {\n+        // Clean up environment.\n+        shell('rm -rf PerfKitBenchmarker')\n+        // Clone appropriate perfkit branch\n+        shell('git clone https://github.com/GoogleCloudPlatform/PerfKitBenchmarker.git')\n+        // Install job requirements.\n+        shell('pip install --user -r PerfKitBenchmarker/requirements.txt')\n+        // Launch performance test.\n+        shell(\"python PerfKitBenchmarker/pkb.py $pkbArgs\")\n     }\n   }\n }",
                "changes": 42
            },
            {
                "status": "added",
                "additions": 43,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy",
                "deletions": 0,
                "sha": "51c73f34d59e0b0adca7855f5b2f6d6318d54b46",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy",
                "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the Beam performance tests on PerfKit Benchmarker.\n+job('beam_PerformanceTests_Dataflow'){\n+    // Set default Beam job properties.\n+    common_job_properties.setTopLevelMainJobProperties(delegate)\n+\n+    // Run job in postcommit every 6 hours, don't trigger every push, and\n+    // don't email individual committers.\n+    common_job_properties.setPostCommit(\n+        delegate,\n+        '0 */6 * * *',\n+        false,\n+        'commits@beam.apache.org',\n+        false)\n+\n+    def argMap = [\n+      benchmarks: 'dpb_wordcount_benchmark',\n+      dpb_dataflow_staging_location: 'gs://temp-storage-for-perf-tests/staging',\n+      dpb_wordcount_input: 'dataflow-samples/shakespeare/kinglear.txt',\n+      config_override: 'dpb_wordcount_benchmark.dpb_service.service_type=dataflow'\n+    ]\n+\n+    common_job_properties.buildPerformanceTest(delegate, argMap)\n+}",
                "changes": 43
            },
            {
                "status": "added",
                "additions": 60,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy",
                "deletions": 0,
                "sha": "8e581c2a4a7cf70d11888d8418064f89e43e6988",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy",
                "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the Beam performance tests on PerfKit Benchmarker.\n+job('beam_PerformanceTests_JDBC'){\n+    // Set default Beam job properties.\n+    common_job_properties.setTopLevelMainJobProperties(delegate)\n+\n+    // Run job in postcommit every 6 hours, don't trigger every push, and\n+    // don't email individual committers.\n+    common_job_properties.setPostCommit(\n+        delegate,\n+        '0 */6 * * *',\n+        false,\n+        'commits@beam.apache.org',\n+        false)\n+\n+    def pipelineArgs = [\n+        tempRoot: 'gs://temp-storage-for-end-to-end-tests',\n+        project: 'apache-beam-testing',\n+        postgresServerName: '10.36.0.11',\n+        postgresUsername: 'postgres',\n+        postgresDatabaseName: 'postgres',\n+        postgresPassword: 'uuinkks',\n+        postgresSsl: 'false'\n+    ]\n+    def pipelineArgList = []\n+    pipelineArgs.each({\n+        key, value -> pipelineArgList.add(\"--$key=$value\")\n+    })\n+    def pipelineArgsJoined = pipelineArgList.join(',')\n+\n+    def argMap = [\n+      benchmarks: 'beam_integration_benchmark',\n+      beam_it_module: 'sdks/java/io/jdbc',\n+      beam_it_args: pipelineArgsJoined,\n+      beam_it_class: 'org.apache.beam.sdk.io.jdbc.JdbcIOIT',\n+      // Profile is located in $BEAM_ROOT/sdks/java/io/pom.xml.\n+      beam_it_profile: 'io-it'\n+    ]\n+\n+    common_job_properties.buildPerformanceTest(delegate, argMap)\n+}",
                "changes": 60
            },
            {
                "status": "added",
                "additions": 44,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy",
                "deletions": 0,
                "sha": "ba719bfa5ba62eff84ae798bbcaadb486f61c66c",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy",
                "patch": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the Beam performance tests on PerfKit Benchmarker.\n+job('beam_PerformanceTests_Spark'){\n+    // Set default Beam job properties.\n+    common_job_properties.setTopLevelMainJobProperties(delegate)\n+\n+    // Run job in postcommit every 6 hours, don't trigger every push, and\n+    // don't email individual committers.\n+    common_job_properties.setPostCommit(\n+        delegate,\n+        '0 */6 * * *',\n+        false,\n+        'commits@beam.apache.org',\n+        false)\n+\n+    def argMap = [\n+      benchmarks: 'dpb_wordcount_benchmark',\n+      // There are currently problems uploading to Dataproc, so we use a file\n+      // already present on the machines as input.\n+      dpb_wordcount_input: '/etc/hosts',\n+      config_override: 'dpb_wordcount_benchmark.dpb_service.service_type=dataproc'\n+    ]\n+\n+    common_job_properties.buildPerformanceTest(delegate, argMap)\n+}",
                "changes": 44
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "deletions": 0,
                "sha": "a288a8448211345f4569e959b8712e199f2eb841",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy"
            },
            {
                "status": "renamed",
                "additions": 8,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Apex.groovy",
                "deletions": 7,
                "sha": "c16a1e2f9d00d86ad344096770b434dd76c068f0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
                "patch": "@@ -18,9 +18,10 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Apex runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Apex') {\n-  description('Runs the RunnableOnService suite on the Apex runner.')\n+// This job runs the suite of ValidatesRunner tests against the Apex runner.\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Apex') {\n+  description('Runs the ValidatesRunner suite on the Apex runner.')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Apex')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate)\n@@ -34,14 +35,14 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Apex') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Apex Runner RunnableOnService Tests',\n-    'Run Apex RunnableOnService')\n+    'Apache Apex Runner ValidatesRunner Tests',\n+    'Run Apex ValidatesRunner')\n \n   // Maven goals for this job.\n   goals('''clean verify --projects runners/apex \\\n       --also-make \\\n       --batch-mode \\\n       --errors \\\n-      --activate-profiles runnable-on-service-tests \\\n-      --activate-profiles local-runnable-on-service-tests''')\n+      --activate-profiles validates-runner-tests \\\n+      --activate-profiles local-validates-runner-tests''')\n }",
                "changes": 15
            },
            {
                "status": "renamed",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Dataflow.groovy",
                "deletions": 7,
                "sha": "33235ff833c2c9267638c6d545761fc99e4785eb",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
                "patch": "@@ -18,12 +18,12 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Dataflow\n+// This job runs the suite of ValidatesRunner tests against the Dataflow\n // runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Dataflow') {\n-  description('Runs the RunnableOnService suite on the Dataflow runner.')\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Dataflow') {\n+  description('Runs the ValidatesRunner suite on the Dataflow runner.')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Dataflow')\n \n-  previousNames('beam_PostCommit_RunnableOnService_GoogleCloudDataflow')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate, 'master', 120)\n@@ -37,9 +37,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Dataflow') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Google Cloud Dataflow Runner RunnableOnService Tests',\n-    'Run Dataflow RunnableOnService')\n+    'Google Cloud Dataflow Runner ValidatesRunner Tests',\n+    'Run Dataflow ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/google-cloud-dataflow-java -DforkCount=0 -DrunnableOnServicePipelineOptions=\\'[ \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\", \"--project=apache-beam-testing\", \"--tempRoot=gs://temp-storage-for-runnable-on-service-tests/\" ]\\'')\n+  goals('-B -e clean verify -am -pl runners/google-cloud-dataflow-java -DforkCount=0 -DvalidatesRunnerPipelineOptions=\\'[ \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\", \"--project=apache-beam-testing\", \"--tempRoot=gs://temp-storage-for-validates-runner-tests/\" ]\\'')\n }",
                "changes": 14
            },
            {
                "status": "renamed",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Flink.groovy",
                "deletions": 8,
                "sha": "5b228bc9cb64cf2d34c86f13933f122b02c8134c",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
                "patch": "@@ -18,11 +18,10 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Flink runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Flink') {\n-  description('Runs the RunnableOnService suite on the Flink runner.')\n-\n-  previousNames('beam_PostCommit_RunnableOnService_FlinkLocal')\n+// This job runs the suite of ValidatesRunner tests against the Flink runner.\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Flink') {\n+  description('Runs the ValidatesRunner suite on the Flink runner.')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Flink')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate)\n@@ -36,9 +35,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Flink') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Flink Runner RunnableOnService Tests',\n-    'Run Flink RunnableOnService')\n+    'Apache Flink Runner ValidatesRunner Tests',\n+    'Run Flink ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/flink/runner -Plocal-runnable-on-service-tests -Prunnable-on-service-tests')\n+  goals('-B -e clean verify -am -pl runners/flink -Plocal-validates-runner-tests -Pvalidates-runner-tests')\n }",
                "changes": 15
            },
            {
                "status": "renamed",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Gearpump.groovy",
                "deletions": 7,
                "sha": "e1cbafe6e4b4af167c3b1e2372247e056bbe88ac",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
                "patch": "@@ -18,12 +18,12 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Gearpump\n+// This job runs the suite of ValidatesRunner tests against the Gearpump\n // runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Gearpump') {\n-  description('Runs the RunnableOnService suite on the Gearpump runner.')\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Gearpump') {\n+  description('Runs the ValidatesRunner suite on the Gearpump runner.')\n \n-  previousNames('beam_PostCommit_RunnableOnService_GearpumpLocal')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Gearpump')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(\n@@ -41,9 +41,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Gearpump') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Gearpump Runner RunnableOnService Tests',\n-    'Run Gearpump RunnableOnService')\n+    'Apache Gearpump Runner ValidatesRunner Tests',\n+    'Run Gearpump ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/gearpump  -Plocal-runnable-on-service-tests -Prunnable-on-service-tests')\n+  goals('-B -e clean verify -am -pl runners/gearpump -DforkCount=0 -DvalidatesRunnerPipelineOptions=\\'[ \"--runner=TestGearpumpRunner\"]\\'')\n }",
                "changes": 14
            },
            {
                "status": "renamed",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Spark.groovy",
                "deletions": 7,
                "sha": "9fbc219a910e5976f4bb83d3e5284cc9e43c5cf4",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
                "patch": "@@ -18,11 +18,11 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Spark runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Spark') {\n-  description('Runs the RunnableOnService suite on the Spark runner.')\n+// This job runs the suite of ValidatesRunner tests against the Spark runner.\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Spark') {\n+  description('Runs the ValidatesRunner suite on the Spark runner.')\n \n-  previousNames('beam_PostCommit_RunnableOnService_SparkLocal')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Spark')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate)\n@@ -36,9 +36,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Spark') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Spark Runner RunnableOnService Tests',\n-    'Run Spark RunnableOnService')\n+    'Apache Spark Runner ValidatesRunner Tests',\n+    'Run Spark ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/spark -Prunnable-on-service-tests -Plocal-runnable-on-service-tests -Dspark.ui.enabled=false')\n+  goals('-B -e clean verify -am -pl runners/spark -Pvalidates-runner-tests -Plocal-validates-runner-tests -Dspark.ui.enabled=false')\n }",
                "changes": 14
            },
            {
                "status": "renamed",
                "additions": 12,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "deletions": 0,
                "sha": "28cf77e6963d941d645d08daf15497ae95a9e16d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "patch": "@@ -36,6 +36,18 @@ job('beam_PostCommit_Python_Verify') {\n     'Python SDK PostCommit Tests',\n     'Run Python PostCommit')\n \n+  // Allow the test to only run on particular nodes\n+  // TODO(BEAM-1817): Remove once the tests can run on all nodes\n+  parameters {\n+      nodeParam('TEST_HOST') {\n+          description('select test host as either beam1, 2 or 3')\n+          defaultNodes(['beam3'])\n+          allowedNodes(['beam1', 'beam2', 'beam3'])\n+          trigger('multiSelectionDisallowed')\n+          eligibility('IgnoreOfflineNodeEligibility')\n+      }\n+  }\n+\n   // Execute shell command to test Python SDK.\n   steps {\n     shell('bash sdks/python/run_postcommit.sh')",
                "changes": 12
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "previous_filename": ".jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "deletions": 0,
                "sha": "371855159984d7843857dda6541af2a6a8d8f8ff",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "previous_filename": ".jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "deletions": 0,
                "sha": "7c64f1119bb522dcd41fdd8e3bb90d6ee153b419",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy",
                "previous_filename": ".jenkins/job_beam_PreCommit_Website_Test.groovy",
                "deletions": 0,
                "sha": "421b58a804e8b0be8e0654bba0a5d278641a9816",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy",
                "previous_filename": ".jenkins/job_beam_Release_NightlySnapshot.groovy",
                "deletions": 0,
                "sha": "f2c3ff0740d53c18116b9a7e7511256af531c337",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy"
            },
            {
                "status": "renamed",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_seed.groovy",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_seed.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/jenkins/job_seed.groovy",
                "previous_filename": ".jenkins/job_seed.groovy",
                "deletions": 1,
                "sha": "2d1b07c7cd2c1cc54c4ca6bdf982865cbcac29f2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_seed.groovy",
                "patch": "@@ -44,7 +44,7 @@ job('beam_SeedJob') {\n   steps {\n     dsl {\n       // A list or a glob of other groovy files to process.\n-      external('.jenkins/job_*.groovy')\n+      external('.test-infra/jenkins/job_*.groovy')\n \n       // If a job is removed from the script, disable it (rather than deleting).\n       removeAction('DISABLE')",
                "changes": 2
            },
            {
                "status": "renamed",
                "additions": 9,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/postgres-pod-no-vol.yml",
                "deletions": 13,
                "sha": "dd0da93e10cb8e4be787fff811856cc0a82b2cd1",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml",
                "patch": "@@ -13,20 +13,16 @@\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n \n+# Cassandra external service which is exposed as a load balancer.\n apiVersion: v1\n-kind: Pod\n+kind: Service\n metadata:\n-  name: postgres-no-pv\n   labels:\n-    name: postgres-no-pv\n+    app: cassandra\n+  name: cassandra-external\n spec:\n-  containers:\n-    - name: postgres\n-      image: postgres\n-      env:\n-        - name: POSTGRES_PASSWORD\n-          value: uuinkks\n-        - name: PGDATA\n-          value: /var/lib/postgresql/data/pgdata\n-      ports:\n-        - containerPort: 5432\n+  ports:\n+    - port: 9042\n+  selector:\n+    app: cassandra\n+  type: LoadBalancer",
                "changes": 22
            },
            {
                "status": "added",
                "additions": 114,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml",
                "deletions": 0,
                "sha": "f2ff571c89ebbf69cdf9f9672f18dde0831ba4cd",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml",
                "patch": "@@ -0,0 +1,114 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# Kubernetes service for cassandra\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    app: cassandra\n+  name: cassandra\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 9042\n+  selector:\n+    app: cassandra\n+  type: NodePort\n+---\n+# Kubernetes statefulset to set up cassandra multinode cluster\n+apiVersion: \"apps/v1beta1\"\n+kind: StatefulSet\n+metadata:\n+  name: cassandra\n+spec:\n+  serviceName: cassandra\n+  replicas: 3\n+  template:\n+    metadata:\n+      labels:\n+        app: cassandra\n+    spec:\n+      containers:\n+      - name: cassandra\n+# Tag v1.2 of cassandra image loads 3.10 version of Cassandra\n+        image: quay.io/vorstella/cassandra-k8s:v1.2\n+        imagePullPolicy: Always\n+        ports:\n+        - containerPort: 7000\n+          name: intra-node\n+        - containerPort: 7001\n+          name: tls-intra-node\n+        - containerPort: 7199\n+          name: jmx\n+        - containerPort: 9042\n+          name: cql\n+        securityContext:\n+          capabilities:\n+            add:\n+              - IPC_LOCK\n+        lifecycle:\n+          preStop:\n+            exec:\n+              command: [\"/bin/sh\", \"-c\", \"PID=$(pidof java) && kill $PID && while ps -p $PID > /dev/null; do sleep 1; done\"]\n+        env:\n+          - name: MAX_HEAP_SIZE\n+            value: 512M\n+          - name: HEAP_NEWSIZE\n+            value: 100M\n+          - name: CASSANDRA_SEEDS\n+            value: \"cassandra-0.cassandra.default.svc.cluster.local\"\n+          - name: CASSANDRA_CLUSTER_NAME\n+            value: \"K8Demo\"\n+          - name: CASSANDRA_DC\n+            value: \"DC1-K8Demo\"\n+          - name: CASSANDRA_RACK\n+            value: \"Rack1-K8Demo\"\n+          - name: CASSANDRA_AUTO_BOOTSTRAP\n+            value: \"false\"\n+          - name: POD_IP\n+            valueFrom:\n+              fieldRef:\n+                fieldPath: status.podIP\n+          - name: POD_NAMESPACE\n+            valueFrom:\n+              fieldRef:\n+                fieldPath: metadata.namespace\n+        readinessProbe:\n+          exec:\n+            command:\n+            - /bin/bash\n+            - -c\n+            - /ready-probe.sh\n+          initialDelaySeconds: 15\n+          timeoutSeconds: 5\n+        # These volume mounts are persistent. They are like inline claims,\n+        # but not exactly because the names need to match exactly one of\n+        # the stateful pod volumes.\n+        volumeMounts:\n+        - name: cassandra-data\n+          mountPath: /cassandra_data\n+  # These are converted to volume claims by the controller\n+  # and mounted at the paths mentioned above.\n+  volumeClaimTemplates:\n+  - metadata:\n+      name: cassandra-data\n+      annotations:\n+        volume.alpha.kubernetes.io/storage-class: anything\n+    spec:\n+      accessModes: [ \"ReadWriteOnce\" ]\n+      resources:\n+        requests:\n+          storage: 30Gi",
                "changes": 114
            },
            {
                "status": "added",
                "additions": 74,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml",
                "deletions": 0,
                "sha": "79139b70914efa8b7493e473bcd8733bb589a224",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml",
                "patch": "@@ -0,0 +1,74 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Temporary cassandra single node cluster set up \n+# to connect to production cluster through cqlsh remotely. \n+# Headless service that allows us to get the IP addresses of our Cassandra nodes\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    name: cassandra-temp\n+  name: cassandra-temp\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 7000\n+      name: intra-node-communication\n+    - port: 7001\n+      name: tls-intra-node-communication\n+    - port: 9042\n+      name: cql\n+  selector:\n+    name: cassandra-temp\n+---\n+# Replication Controller for Cassandra which tracks the Cassandra pods.\n+apiVersion: v1\n+kind: ReplicationController\n+metadata:\n+  labels:\n+    name: cassandra-temp\n+  name: cassandra-temp\n+spec:\n+  replicas: 1\n+  selector:\n+    name: cassandra-temp\n+  template:\n+    metadata:\n+      labels:\n+        name: cassandra-temp\n+    spec:\n+      containers:\n+        - image: cassandra\n+          name: cassandra-temp\n+          env:\n+            - name: PEER_DISCOVERY_SERVICE\n+              value: cassandra-temp\n+            - name: CASSANDRA_CLUSTER_NAME\n+              value: Cassandra\n+            - name: CASSANDRA_DC\n+              value: DC1\n+            - name: CASSANDRA_RACK\n+              value: Kubernetes Cluster\n+          ports:\n+            - containerPort: 9042\n+              name: cql\n+          volumeMounts:\n+            - mountPath: /var/lib/cassandra/data\n+              name: data\n+      volumes:\n+        - name: data\n+          emptyDir: {}",
                "changes": 74
            },
            {
                "status": "added",
                "additions": 122,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh",
                "deletions": 0,
                "sha": "38e856fdd9f9ab1e42b24ac3a1e75247a12f2d28",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh",
                "patch": "@@ -0,0 +1,122 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Hashcode for 50m records is 85b9cec947fc5d849f0a778801696d2b\n+\n+# Script to load data using YCSB on Cassandra multi node cluster.\n+ \n+#!/bin/bash\n+\n+set -e\n+\n+# Record count set to 50000000, change this value to load as per requirement.\n+recordcount=50000000\n+\n+# Function to delete the temporary cassandra service in an erroneous and successful situation\n+function delete_service {\n+  cd ../LargeITCluster\n+  kubectl delete -f cassandra-svc-temp.yaml\n+}\n+\n+# Delete cassandra single node set up before exit \n+trap delete_service EXIT\n+\n+# Check and delete cassandra service if already exists\n+if [ \"$(kubectl get svc -o=name | grep cassandra-temp)\" ]; then\n+  echo \"Service cassandra-temp already exists\"\n+  echo \"Deleting service cassandra-temp \"\n+  delete_service\n+fi\n+  \n+# Temporarily set up cassandra single node cluster for invoking cqlsh on actual cluster remotely\n+kubectl create -f cassandra-svc-temp.yaml\n+\n+num_of_replicas=$(kubectl get statefulset cassandra --output=jsonpath={.spec.replicas})\n+\n+echo \"Script to load data on $num_of_replicas replicas\"\n+echo \"Waiting for Cassandra pods to be in ready state\"\n+\n+# Wait until all the pods configured as per number of replicas, come in running state\n+i=0\n+while [ $i -lt $num_of_replicas ]\n+do\n+   container_state=\"$(kubectl get pods -l app=cassandra -o jsonpath=\"{.items[$i].status.containerStatuses[0].ready}\")\"\n+   while ! $container_state; do\n+      sleep 10s\n+      container_state=\"$(kubectl get pods -l app=cassandra -o jsonpath=\"{.items[$i].status.containerStatuses[0].ready}\")\"\n+      echo \".\"\n+   done\n+   ready_pod=\"$(kubectl get pods -l app=cassandra -o jsonpath=\"{.items[$i].metadata.name}\")\"\n+   echo \"$ready_pod is ready\"\n+   i=$((i+1))\n+done\n+\n+echo \"Waiting for temporary pod to be in ready state\"\n+temp_container_state=\"$(kubectl get pods -l name=cassandra-temp -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+while ! $temp_container_state; do\n+  sleep 10s\n+  temp_container_state=\"$(kubectl get pods -l name=cassandra-temp -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+  echo \".\"\n+done\n+\n+temp_running_seed=\"$(kubectl get pods -l name=cassandra-temp -o jsonpath=\"{.items[0].metadata.name}\")\"\n+\n+# After starting the service, it takes couple of minutes to generate the external IP for the\n+# service. Hence, wait for sometime and identify external IP of the pod\n+external_ip=\"$(kubectl get svc cassandra-external -o jsonpath=\\\n+'{.status.loadBalancer.ingress[0].ip}')\"\n+\n+echo \"Waiting for the Cassandra service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc cassandra-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+echo \"External IP - $external_ip\"\n+\n+echo \"Loading data\"\n+# Create keyspace\n+keyspace_creation_command=\"drop keyspace if exists ycsb;create keyspace ycsb WITH REPLICATION = {\\\n+'class' : 'SimpleStrategy', 'replication_factor': 3 };\"\n+kubectl exec -ti $temp_running_seed -- cqlsh $external_ip -e \"$keyspace_creation_command\"\n+echo \"Keyspace creation............\"\n+echo \"-----------------------------\"\n+echo \"$keyspace_creation_command\"\n+echo\n+\n+# Create table\n+table_creation_command=\"use ycsb;drop table if exists usertable;create table usertable (\\\n+y_id varchar primary key,field0 varchar,field1 varchar,field2 varchar,field3 varchar,\\\n+field4 varchar,field5 varchar,field6 varchar,field7 varchar,field8 varchar,field9 varchar);\"\n+kubectl exec -ti $temp_running_seed -- cqlsh $external_ip -e \"$table_creation_command\"\n+echo \"Table creation ..............\"\n+echo \"-----------------------------\"\n+echo \"$table_creation_command\"\n+\n+# Create index\n+index_creation_command=\"CREATE INDEX IF NOT EXISTS field0_index ON ycsb.usertable (field0);\"\n+kubectl exec -ti $temp_running_seed -- cqlsh $external_ip -e \"$index_creation_command\"\n+\n+cd ../ycsb-0.12.0\n+\n+echo \"Starting to load data on ${external_ip}\"\n+echo \"-----------------------------\"\n+\n+# dataintegrity flag is set to true to load deterministic data\n+./bin/ycsb load cassandra-cql -p hosts=${external_ip} -p dataintegrity=true -p recordcount=\\\n+${recordcount} -p insertorder=ordered -p fieldlength=20 -threads 200 -P workloads/workloadd \\\n+-s > workloada_load_res.txt",
                "changes": 122
            },
            {
                "status": "added",
                "additions": 47,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh",
                "deletions": 0,
                "sha": "a538a9d5a40700ef09201fd26182c007fa371fc9",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh",
                "patch": "@@ -0,0 +1,47 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Output Cassandra cluster and pod status.\n+\n+#!/bin/bash\n+\n+find_cassandra_pods=\"kubectl get pods -l app=cassandra\"\n+\n+first_running_seed=\"$($find_cassandra_pods -o jsonpath=\"{.items[0].metadata.name}\")\"\n+\n+# Use nodetool status command to determine the status of pods and display\n+cluster_status=$(kubectl exec $first_running_seed \\\n+    -- /usr/local/apache-cassandra/bin/nodetool status -r)\n+echo\n+echo \"  Cassandra Node      Kubernetes Pod\"\n+echo \"  --------------      --------------\"\n+while read -r line; do\n+    node_name=$(echo $line | awk '{print $1}')\n+    status=$(echo \"$cluster_status\" | grep $node_name | awk '{print $1}')\n+\n+    long_status=$(echo \"$status\" | \\\n+        sed 's/U/  Up/g' | \\\n+\tsed 's/D/Down/g' | \\\n+\tsed 's/N/|Normal /g' | \\\n+\tsed 's/L/|Leaving/g' | \\\n+\tsed 's/J/|Joining/g' | \\\n+\tsed 's/M/|Moving /g')\n+\n+    : ${long_status:=\"            \"}\n+    echo \"$long_status           $line\"\n+done <<< \"$($find_cassandra_pods)\"\n+\n+echo",
                "changes": 47
            },
            {
                "status": "added",
                "additions": 22,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh",
                "deletions": 0,
                "sha": "7341209ccaf8a723021d8140dd6f313e8dd69863",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh",
                "patch": "@@ -0,0 +1,22 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Create Cassandra services and statefulset.\n+kubectl create -f cassandra-service-for-local-dev.yaml\n+kubectl create -f cassandra-svc-statefulset.yaml",
                "changes": 22
            },
            {
                "status": "added",
                "additions": 25,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh",
                "deletions": 0,
                "sha": "367b6049fcbe60247278f8c44305bd13e122c46b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh",
                "patch": "@@ -0,0 +1,25 @@\n+#\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+#!/bin/bash\n+\n+set -e\n+\n+# Delete Cassandra services and statefulset.\n+kubectl delete -f cassandra-svc-statefulset.yaml\n+kubectl delete -f cassandra-service-for-local-dev.yaml\n+# Delete the persistent storage media for the PersistentVolumes\n+kubectl delete pvc -l app=cassandra",
                "changes": 25
            },
            {
                "status": "added",
                "additions": 30,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml",
                "deletions": 0,
                "sha": "f2f506934e88c7ca28bda69c44f22b95a1e405c3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml",
                "patch": "@@ -0,0 +1,30 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Kubernetes service exposing a public LoadBalancer cassandra service.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    name: cassandra-external\n+  name: cassandra-external\n+spec:\n+  ports:\n+    - port: 9042\n+      name: cql\n+  selector:\n+    name: cassandra\n+  type: LoadBalancer",
                "changes": 30
            },
            {
                "status": "added",
                "additions": 74,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml",
                "deletions": 0,
                "sha": "181689a7e2435317675ba153b991cd4cc74549ab",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml",
                "patch": "@@ -0,0 +1,74 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Headless service that allows us to get the IP addresses of our Cassandra nodes\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    name: cassandra-peers\n+  name: cassandra-peers\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 7000\n+      name: intra-node-communication\n+    - port: 7001\n+      name: tls-intra-node-communication\n+  selector:\n+    name: cassandra\n+  type: NodePort\n+---\n+# Replication Controller for Cassandra which tracks the Cassandra pods.\n+apiVersion: v1\n+kind: ReplicationController\n+metadata:\n+  labels:\n+    name: cassandra\n+  name: cassandra\n+spec:\n+  replicas: 1\n+  selector:\n+    name: cassandra\n+  template:\n+    metadata:\n+      labels:\n+        name: cassandra\n+    spec:\n+      containers:\n+        - image: cassandra\n+          name: cassandra\n+          env:\n+            - name: PEER_DISCOVERY_SERVICE\n+              value: cassandra-peers\n+            - name: CASSANDRA_CLUSTER_NAME\n+              value: Cassandra\n+            - name: CASSANDRA_DC\n+              value: DC1\n+            - name: CASSANDRA_RACK\n+              value: Kubernetes Cluster\n+# Number of tokens currently configured to 1. If this is not configured, default value is 256. You can change it as per requirement.\t\t\t  \n+            - name: CASSANDRA_NUM_TOKENS\n+              value: '1'\n+          ports:\n+            - containerPort: 9042\n+              name: cql\n+          volumeMounts:\n+            - mountPath: /var/lib/cassandra/data\n+              name: data\n+      volumes:\n+        - name: data\n+          emptyDir: {}\n\\ No newline at end of file",
                "changes": 74
            },
            {
                "status": "added",
                "additions": 86,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh",
                "deletions": 0,
                "sha": "203c8a859e551a9b24a75e68b7483ac51414825b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh",
                "patch": "@@ -0,0 +1,86 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Hashcode for 1000 records is 1a30ad400afe4ebf5fde75f5d2d95408, \n+# For test with query to select one record from 1000 docs, \n+# hashcode is 7bead6d6385c5f4dd0524720cd320b49\n+\n+# Script to load data using YCSB on Cassandra one node cluster.\n+\n+#!/bin/bash\n+set -e\n+\n+# Record count set to 1000, change this value to load as per requirement.\n+recordcount=1000\n+\n+# Identify the pod\n+cassandra_pods=\"kubectl get pods -l name=cassandra\"\n+running_seed=\"$(kubectl get pods -o json -l name=cassandra -o jsonpath=\\\n+'{.items[0].metadata.name}')\"\n+echo \"Detected Pod $running_seed\"\n+\n+echo \"Waiting for Cassandra pod to be in ready state\"\n+container_state=\"$(kubectl get pods -l name=cassandra -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+while ! $container_state; do\n+  sleep 10s\n+  container_state=\"$(kubectl get pods -l name=cassandra -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+  echo \".\"\n+done\n+\n+# After starting the service, it takes couple of minutes to generate the external IP for the\n+# service. Hence, wait for sometime.\n+# Identify external IP of the pod\n+external_ip=\"$(kubectl get svc cassandra-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+echo \"Waiting for the Cassandra service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc cassandra-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+echo \"External IP - $external_ip\"\n+\n+# Create keyspace\n+keyspace_creation_command=\"drop keyspace if exists ycsb;create keyspace ycsb WITH REPLICATION = {\\\n+'class' : 'SimpleStrategy', 'replication_factor': 3 };\"\n+kubectl exec -ti $running_seed -- cqlsh -e \"$keyspace_creation_command\"\n+echo \"Keyspace creation............\"\n+echo \"-----------------------------\"\n+echo \"$keyspace_creation_command\"\n+echo\n+\n+# Create table\n+table_creation_command=\"use ycsb;drop table if exists usertable;create table usertable (\\\n+y_id varchar primary key,field0 varchar,field1 varchar,field2 varchar,field3 varchar,\\\n+field4 varchar,field5 varchar,field6 varchar,field7 varchar,field8 varchar,field9 varchar);\"\n+kubectl exec -ti $running_seed -- cqlsh -e \"$table_creation_command\"\n+echo \"Table creation ..............\"\n+echo \"-----------------------------\"\n+echo \"$table_creation_command\"\n+\n+# Create index\n+index_creation_command=\"CREATE INDEX IF NOT EXISTS field0_index ON ycsb.usertable (field0);\"\n+kubectl exec -ti $running_seed -- cqlsh -e \"$index_creation_command\"\n+\n+cd ../ycsb-0.12.0\n+\n+echo \"Starting to load data on ${external_ip}\"\n+echo \"-----------------------------\"\n+# Record count set to 1000, change this value to load as per requirement.\n+# dataintegrity flag is set to true to load deterministic data\n+./bin/ycsb load cassandra-cql -p hosts=${external_ip} -p dataintegrity=true -p recordcount=\\\n+${recordcount} -p insertorder=ordered -p fieldlength=20 -P workloads/workloadd \\\n+-s > workloada_load_res.txt",
                "changes": 86
            },
            {
                "status": "added",
                "additions": 47,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh",
                "deletions": 0,
                "sha": "a3ea94136f83dfd5fd2c415a9f15f20f80304577",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh",
                "patch": "@@ -0,0 +1,47 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Output Cassandra cluster and pod status.\n+\n+#!/bin/bash\n+\n+find_cassandra_pods=\"kubectl get pods -l name=cassandra\"\n+\n+first_running_seed=\"$($find_cassandra_pods -o jsonpath=\"{.items[0].metadata.name}\")\"\n+\n+# Use nodetool status command to determine the status of pods and display\n+cluster_status=$(kubectl exec $first_running_seed \\\n+    -- nodetool status -r)\n+echo\n+echo \"  Cassandra Node      Kubernetes Pod\"\n+echo \"  --------------      --------------\"\n+while read -r line; do\n+    node_name=$(echo $line | awk '{print $1}')\n+    status=$(echo \"$cluster_status\" | grep $node_name | awk '{print $1}')\n+\n+    long_status=$(echo \"$status\" | \\\n+        sed 's/U/  Up/g' | \\\n+\tsed 's/D/Down/g' | \\\n+\tsed 's/N/|Normal /g' | \\\n+\tsed 's/L/|Leaving/g' | \\\n+\tsed 's/J/|Joining/g' | \\\n+\tsed 's/M/|Moving /g')\n+\n+    : ${long_status:=\"            \"}\n+    echo \"$long_status           $line\"\n+done <<< \"$($find_cassandra_pods)\"\n+\n+echo",
                "changes": 47
            },
            {
                "status": "added",
                "additions": 23,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh",
                "deletions": 0,
                "sha": "9377a9c190a0b6bf18c18c54d4188d0c3d4ce606",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh",
                "patch": "@@ -0,0 +1,23 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Create Cassandra services and Replication controller.\n+kubectl create -f cassandra-service-for-local-dev.yaml\n+kubectl create -f cassandra-svc-rc.yaml\n+",
                "changes": 23
            },
            {
                "status": "renamed",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/teardown.sh",
                "deletions": 3,
                "sha": "f4ad0be9045665bdee329167f438cba61e0fb1be",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh",
                "patch": "@@ -1,4 +1,3 @@\n-#!/bin/bash\n #\n #    Licensed to the Apache Software Foundation (ASF) under one or more\n #    contributor license agreements.  See the NOTICE file distributed with\n@@ -15,6 +14,9 @@\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n #\n+#!/bin/bash\n+set -e\n \n-kubectl delete service postgres-no-pv\n-kubectl delete pod postgres-no-pv\n+# Delete Cassandra services and Replication controller.\n+kubectl delete -f cassandra-svc-rc.yaml\n+kubectl delete -f cassandra-service-for-local-dev.yaml",
                "changes": 8
            },
            {
                "status": "added",
                "additions": 29,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/data-load-setup.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/data-load-setup.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/cassandra/data-load-setup.sh",
                "deletions": 0,
                "sha": "4e12f89479bd70e0c21eb961d0c18a68b461e830",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/data-load-setup.sh",
                "patch": "@@ -0,0 +1,29 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Load YCSB tool\n+echo \"Downloading YCSB tool\"\n+echo \"------------------------------\"\n+curl -O --location https://github.com/brianfrankcooper/YCSB/releases/download/0.12.0/ycsb-0.12.0.tar.gz\n+tar xfz ycsb-0.12.0.tar.gz\n+wget https://www.slf4j.org/dist/slf4j-1.7.22.tar.gz\n+tar xfz slf4j-1.7.22.tar.gz\n+cp slf4j-1.7.22/slf4j-simple-*.jar ycsb-0.12.0/lib/\n+cp slf4j-1.7.22/slf4j-api-*.jar ycsb-0.12.0/lib/\n+echo \"YCSB tool loaded\"",
                "changes": 29
            },
            {
                "status": "added",
                "additions": 33,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml",
                "deletions": 0,
                "sha": "d28d70ad98ae945b3d8b1e943d20230c9c468919",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml",
                "patch": "@@ -0,0 +1,33 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# To create Elasticsearch frontend cluster Kubernetes service.\n+# It sets up a load balancer on TCP port 9200 that distributes network traffic to the ES client nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch-external\n+  labels:\n+    component: elasticsearch\n+    role: client\n+spec:\n+  type: LoadBalancer\n+  selector:\n+    component: elasticsearch\n+    role: client\n+  ports:\n+  - name: http\n+    port: 9200\n+    protocol: TCP",
                "changes": 33
            },
            {
                "status": "added",
                "additions": 258,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml",
                "deletions": 0,
                "sha": "8f29fb667736d5dc856b94570edf0e3a2ac7f053",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml",
                "patch": "@@ -0,0 +1,258 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# Service file containing services for ES discovery, elasticsearch and master node deployment.\n+\n+# Kubernetes headless service for Elasticsearch discovery of nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch-discovery\n+  labels:\n+    component: elasticsearch\n+    role: master\n+spec:\n+  selector:\n+    component: elasticsearch\n+    role: master\n+  ports:\n+  - name: transport\n+    port: 9300\n+    protocol: TCP\n+---\n+# The Kubernetes deployment script for Elasticsearch master nodes.\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es-master\n+  labels:\n+    component: elasticsearch\n+    role: master\n+spec:\n+  replicas: 3\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+        role: master\n+      annotations:\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es-master\n+        securityContext:\n+          privileged: false\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.\n+              - IPC_LOCK\n+# SYS_RESOURCE is docker capability key to control and override the resource limits.\n+# This could be needed to increase base limits.(e.g. File descriptor limit for elasticsearch)\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        - name: NODE_NAME\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.name\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: \"NUMBER_OF_MASTERS\"\n+          value: \"2\"\n+        - name: NODE_MASTER\n+          value: \"true\"\n+        - name: NODE_INGEST\n+          value: \"false\"\n+        - name: NODE_DATA\n+          value: \"false\"\n+        - name: HTTP_ENABLE\n+          value: \"false\"\n+        - name: \"ES_JAVA_OPTS\"\n+          value: \"-Xms2g -Xmx2g\"\n+        ports:\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - name: storage\n+          mountPath: /data\n+      volumes:\n+          - emptyDir:\n+              medium: \"\"\n+            name: \"storage\"\n+---\n+# Kubernetes deployment script for Elasticsearch client nodes (aka load balancing proxies).\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es-client\n+  labels:\n+    component: elasticsearch\n+    role: client\n+spec:\n+  # The no. of replicas can be incremented based on the client usage using HTTP API.\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+        role: client\n+      annotations:\n+      # Elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices.\n+      # The default operating system limits on mmap counts is likely to be too low, which may result\n+      # in out of memory exceptions. Therefore, the need to increase virtual memory\n+      # vm.max_map_count for large amount of data in the pod initialization annotation.\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es-client\n+        securityContext:\n+          privileged: false\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.\n+              - IPC_LOCK\n+# SYS_RESOURCE is docker capability key to control and override the resource limits.\n+# This could be needed to increase base limits.(e.g. File descriptor limit for elasticsearch)\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        - name: NODE_NAME\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.name\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: NODE_MASTER\n+          value: \"false\"\n+        - name: NODE_DATA\n+          value: \"false\"\n+        - name: HTTP_ENABLE\n+          value: \"true\"\n+        - name: \"ES_JAVA_OPTS\"\n+          value: \"-Xms2g -Xmx2g\"\n+        ports:\n+        - containerPort: 9200\n+          name: http\n+          protocol: TCP\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - name: storage\n+          mountPath: /data\n+      volumes:\n+          - emptyDir:\n+              medium: \"\"\n+            name: \"storage\"\n+---\n+# Kubernetes deployment script for Elasticsearch data nodes which store and index data.\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es-data\n+  labels:\n+    component: elasticsearch\n+    role: data\n+spec:\n+  replicas: 2\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+        role: data\n+      annotations:\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=1048575\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es-data\n+        securityContext:\n+          privileged: false\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.\n+              - IPC_LOCK\n+# SYS_RESOURCE is docker capability key to control and override the resource limits.\n+# This could be needed to increase base limits.(e.g. File descriptor limit for elasticsearch)\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        - name: NODE_NAME\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.name\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: NODE_MASTER\n+          value: \"false\"\n+        - name: NODE_INGEST\n+          value: \"false\"\n+        - name: HTTP_ENABLE\n+          value: \"false\"\n+        - name: \"ES_JAVA_OPTS\"\n+          value: \"-Xms2g -Xmx2g\"\n+        ports:\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - name: storage\n+          mountPath: /data\n+      volumes:\n+          - emptyDir:\n+              medium: \"\"\n+            name: \"storage\"",
                "changes": 258
            },
            {
                "status": "added",
                "additions": 22,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh",
                "deletions": 0,
                "sha": "93022c7244000c7532d39ebc173fd73fbad4dfa8",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh",
                "patch": "@@ -0,0 +1,22 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+\n+#!/bin/sh\n+set -e\n+\n+# Create Elasticsearch services and deployments.\n+kubectl create -f elasticsearch-service-for-local-dev.yaml\n+kubectl create -f es-services-deployments.yaml",
                "changes": 22
            },
            {
                "status": "added",
                "additions": 21,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh",
                "deletions": 0,
                "sha": "bdc9ab9341a6fbac62ab6c96da2e9cd79f37e574",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh",
                "patch": "@@ -0,0 +1,21 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Delete elasticsearch services and deployments.\n+kubectl delete -f es-services-deployments.yaml\n+kubectl delete -f elasticsearch-service-for-local-dev.yaml",
                "changes": 21
            },
            {
                "status": "added",
                "additions": 34,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml",
                "deletions": 0,
                "sha": "0a16cdb03cbecf7ce62daa8fa66abd3ba565058b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml",
                "patch": "@@ -0,0 +1,34 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# To create Elasticsearch frontend cluster Kubernetes service. \n+# It sets up a load balancer on TCP port 9200 that distributes network traffic to the ES nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch-external\n+  labels:\n+    component: elasticsearch\n+spec:\n+  type: LoadBalancer\n+  selector:\n+    component: elasticsearch\n+  ports:\n+  - name: http\n+    port: 9200\n+    protocol: TCP\n+  - name: transport\n+    port: 9300\n+    protocol: TCP",
                "changes": 34
            },
            {
                "status": "added",
                "additions": 96,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml",
                "deletions": 0,
                "sha": "a4e1ea367ee914b37487015597050bd3199a83f0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml",
                "patch": "@@ -0,0 +1,96 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# To create Elasticsearch frontend cluster Kubernetes service. \n+# It sets up a load balancer on TCP port 9200 that distributes network traffic to the ES nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch\n+  labels:\n+    component: elasticsearch\n+spec:\n+  selector:\n+    component: elasticsearch\n+  type: NodePort\n+  ports:\n+  - name: http\n+    port: 9200\n+    protocol: TCP\n+  - name: transport\n+    port: 9300\n+    protocol: TCP\n+---\n+# The Kubernetes deployment script for Elasticsearch replication nodes. It will create 1 node cluster.\n+# To scale the cluster as desired, you can create replicas of node use 'kubectl scale --replicas=3 rc es' command\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es\n+  labels:\n+    component: elasticsearch\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+      annotations:\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es\n+        securityContext:\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.   \n+              - IPC_LOCK\n+# SYS_RESOURCE capability is set to control and override various resource limits.\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: \"DISCOVERY_SERVICE\"\n+          value: \"elasticsearch\"\n+        - name: NODE_MASTER\n+          value: \"true\"\n+        - name: NODE_DATA\n+          value: \"true\"\n+        - name: HTTP_ENABLE\n+          value: \"true\"\n+        ports:\n+        - containerPort: 9200\n+          name: http\n+          protocol: TCP\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - mountPath: /data\n+          name: storage\n+      volumes:\n+      - name: storage\n+        emptyDir: {}",
                "changes": 96
            },
            {
                "status": "added",
                "additions": 23,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh",
                "deletions": 0,
                "sha": "2d6522ea9c54337aaa9e02372c6004cddd37547b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh",
                "patch": "@@ -0,0 +1,23 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+\n+#!/bin/sh\n+set -e\n+\n+# Create Elasticsearch services and deployments.\n+kubectl create -f elasticsearch-service-for-local-dev.yaml\n+kubectl create -f elasticsearch-svc-rc.yaml\n+",
                "changes": 23
            },
            {
                "status": "renamed",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/setup.sh",
                "deletions": 5,
                "sha": "61c079fed894d5c83389857d26ff3183903d3d23",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh",
                "patch": "@@ -1,5 +1,3 @@\n-#!/bin/bash\n-#\n #    Licensed to the Apache Software Foundation (ASF) under one or more\n #    contributor license agreements.  See the NOTICE file distributed with\n #    this work for additional information regarding copyright ownership.\n@@ -14,7 +12,10 @@\n #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n-#\n \n-kubectl create -f postgres-pod-no-vol.yml\n-kubectl create -f postgres-service-public.yml\n+#!/bin/bash\n+set -e\n+\n+# Delete elasticsearch services and deployments.\n+kubectl delete -f elasticsearch-svc-rc.yaml\n+kubectl delete -f elasticsearch-service-for-local-dev.yaml",
                "changes": 11
            },
            {
                "status": "added",
                "additions": 26,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load-setup.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/data-load-setup.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/data-load-setup.sh",
                "deletions": 0,
                "sha": "00991bc6d3379d43c67414256b60515141fd4dc7",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load-setup.sh",
                "patch": "@@ -0,0 +1,26 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Install python\n+sudo apt-get update\n+sudo apt-get install python-pip\n+sudo pip install --upgrade pip\n+sudo apt-get install python-dev\n+sudo pip install tornado numpy\n+echo",
                "changes": 26
            },
            {
                "status": "added",
                "additions": 33,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/data-load.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/data-load.sh",
                "deletions": 0,
                "sha": "a7dd84ad1ea854075178243e2cb25f9c15c8b74a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load.sh",
                "patch": "@@ -0,0 +1,33 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Identify external IP\n+external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+echo \"Waiting for the Elasticsearch service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+echo \"External IP - $external_ip\"\n+echo\n+\n+# Run the script\n+/usr/bin/python es_test_data.py --count=1000 --format=Txn_ID:int,Item_Code:int,Item_ID:int,User_Name:str,last_updated:ts,Price:int,Title:str,Description:str,Age:int,Item_Name:str,Item_Price:int,Availability:bool,Batch_Num:int,Last_Ordered:tstxt,City:text --es_url=http://$external_ip:9200 &",
                "changes": 33
            },
            {
                "status": "added",
                "additions": 299,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/es_test_data.py",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/es_test_data.py?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/es_test_data.py",
                "deletions": 0,
                "sha": "cf10d39af03ca09870b108c212729736bb4db054",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/es_test_data.py",
                "patch": "@@ -0,0 +1,299 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Script to populate data on Elasticsearch\n+# Hashcode for 1000 records is 42e254c8689050ed0a617ff5e80ea392, \n+# For test with query to select one record from 1000 docs, \n+# hashcode is d7a7e4e42c2ca7b83ef7c1ad1ebce000\n+# Hashcode for 50m records (~20 gigs) is 42e254c8689050ed0a617ff5e80ea392 \n+#!/usr/bin/python\n+\n+import json\n+import time\n+import logging\n+import random\n+import string\n+import uuid\n+import datetime\n+\n+import tornado.gen\n+import tornado.httpclient\n+import tornado.ioloop\n+import tornado.options\n+\n+async_http_client = tornado.httpclient.AsyncHTTPClient()\n+id_counter = 0\n+upload_data_count = 0\n+_dict_data = None\n+\n+\n+\n+def delete_index(idx_name):\n+    try:\n+        url = \"%s/%s?refresh=true\" % (tornado.options.options.es_url, idx_name)\n+        request = tornado.httpclient.HTTPRequest(url, method=\"DELETE\", request_timeout=240, \n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        response = tornado.httpclient.HTTPClient().fetch(request)\n+        logging.info('Deleting index  \"%s\" done   %s' % (idx_name, response.body))\n+    except tornado.httpclient.HTTPError:\n+        pass\n+\n+\n+def create_index(idx_name):\n+    schema = {\n+        \"settings\": {\n+            \"number_of_shards\":   tornado.options.options.num_of_shards,\n+            \"number_of_replicas\": tornado.options.options.num_of_replicas\n+        },\n+        \"refresh\": True\n+    }\n+\n+    body = json.dumps(schema)\n+    url = \"%s/%s\" % (tornado.options.options.es_url, idx_name)\n+    try:\n+        logging.info('Trying to create index %s' % (url))\n+        request = tornado.httpclient.HTTPRequest(url, method=\"PUT\", body=body, request_timeout=240,\n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        response = tornado.httpclient.HTTPClient().fetch(request)\n+        logging.info('Creating index \"%s\" done   %s' % (idx_name, response.body))\n+    except tornado.httpclient.HTTPError:\n+        logging.info('Looks like the index exists already')\n+        pass\n+\n+\n+@tornado.gen.coroutine\n+def upload_batch(upload_data_txt):\n+    try:\n+        request = tornado.httpclient.HTTPRequest(tornado.options.options.es_url + \"/_bulk\",\n+                                                 method=\"POST\", body=upload_data_txt,\n+                                                 request_timeout=\n+                                                 tornado.options.options.http_upload_timeout,\n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        response = yield async_http_client.fetch(request)\n+    except Exception as ex:\n+        logging.error(\"upload failed, error: %s\" % ex)\n+        return\n+\n+    result = json.loads(response.body.decode('utf-8'))\n+    res_txt = \"OK\" if not result['errors'] else \"FAILED\"\n+    took = int(result['took'])\n+    logging.info(\"Upload: %s - upload took: %5dms, total docs uploaded: %7d\" % (res_txt, took, \n+                                                                                upload_data_count))\n+\n+\n+def get_data_for_format(format,count):\n+    split_f = format.split(\":\")\n+    if not split_f:\n+        return None, None\n+\n+    field_name = split_f[0]\n+    field_type = split_f[1]\n+\n+    return_val = ''\n+\n+    if field_type == \"bool\":\n+        if count%2 == 0:\n+           return_val = True\n+        else:\n+           return_val = False\n+\n+    elif field_type == \"str\":\n+        return_val = field_name + str(count)\n+\n+    elif field_type == \"int\":\n+        return_val = count\n+    \n+    elif field_type == \"ipv4\":\n+        return_val = \"{0}.{1}.{2}.{3}\".format(1,2,3,count%255)\n+\n+    elif field_type in [\"ts\", \"tstxt\"]:\n+        return_val = int(count * 1000) if field_type == \"ts\" else\\\n+        \t\t\t datetime.datetime.fromtimestamp(count)\\\n+        \t\t\t .strftime(\"%Y-%m-%dT%H:%M:%S.000-0000\")\n+\n+    elif field_type == \"words\":\n+        return_val = field_name + str(count)\n+\n+    elif field_type == \"dict\":\n+        mydict = dict(a=field_name + str(count), b=field_name + str(count), c=field_name + str(count),\n+                      d=field_name + str(count), e=field_name + str(count), f=field_name + str(count),\n+                      g=field_name + str(count), h=field_name + str(count), i=field_name + str(count), \n+                      j=field_name + str(count))\n+        return_val = \", \".join(\"=\".join(_) for _ in mydict.items())\n+\n+    elif field_type == \"text\":\n+        return_val = field_name + str(count)\n+\n+    return field_name, return_val\n+\n+\n+def generate_count(min, max):\n+    if min == max:\n+        return max\n+    elif min > max:\n+        return random.randrange(max, min);\n+    else:\n+        return random.randrange(min, max);\n+\n+\n+def generate_random_doc(format,count):\n+    global id_counter\n+\n+    res = {}\n+\n+    for f in format:\n+        f_key, f_val = get_data_for_format(f,count)\n+        if f_key:\n+            res[f_key] = f_val\n+\n+    if not tornado.options.options.id_type:\n+        return res\n+\n+    if tornado.options.options.id_type == 'int':\n+        res['_id'] = id_counter\n+        id_counter += 1\n+    elif tornado.options.options.id_type == 'uuid4':\n+        res['_id'] = str(uuid.uuid4())\n+\n+    return res\n+\n+\n+def set_index_refresh(val):\n+\n+    params = {\"index\": {\"refresh_interval\": val}}\n+    body = json.dumps(params)\n+    url = \"%s/%s/_settings\" % (tornado.options.options.es_url, tornado.options.options.index_name)\n+    try:\n+        request = tornado.httpclient.HTTPRequest(url, method=\"PUT\", body=body, request_timeout=240,\n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        http_client = tornado.httpclient.HTTPClient()\n+        http_client.fetch(request)\n+        logging.info('Set index refresh to %s' % val)\n+    except Exception as ex:\n+        logging.exception(ex)\n+\n+\n+@tornado.gen.coroutine\n+def generate_test_data():\n+\n+    global upload_data_count\n+\n+    if tornado.options.options.force_init_index:\n+        delete_index(tornado.options.options.index_name)\n+\n+    create_index(tornado.options.options.index_name)\n+\n+    # todo: query what refresh is set to, then restore later\n+    if tornado.options.options.set_refresh:\n+        set_index_refresh(\"-1\")\n+\n+    if tornado.options.options.out_file:\n+        out_file = open(tornado.options.options.out_file, \"w\")\n+    else:\n+        out_file = None\n+\n+    if tornado.options.options.dict_file:\n+        global _dict_data\n+        with open(tornado.options.options.dict_file, 'r') as f:\n+            _dict_data = f.readlines()\n+        logging.info(\"Loaded %d words from the %s\" % (len(_dict_data), \n+                                                      tornado.options.options.dict_file))\n+\n+    format = tornado.options.options.format.split(',')\n+    if not format:\n+        logging.error('invalid format')\n+        exit(1)\n+\n+    ts_start = int(time.time())\n+    upload_data_txt = \"\"\n+    total_uploaded = 0\n+\n+    logging.info(\"Generating %d docs, upload batch size is %d\" % (tornado.options.options.count,\n+                                                                  tornado.options\n+                                                                  .options.batch_size))\n+    for num in range(0, tornado.options.options.count):\n+\n+        item = generate_random_doc(format,num)\n+\n+        if out_file:\n+            out_file.write(\"%s\\n\" % json.dumps(item))\n+\n+        cmd = {'index': {'_index': tornado.options.options.index_name,\n+                         '_type': tornado.options.options.index_type}}\n+        if '_id' in item:\n+            cmd['index']['_id'] = item['_id']\n+\n+        upload_data_txt += json.dumps(cmd) + \"\\n\"\n+        upload_data_txt += json.dumps(item) + \"\\n\"\n+        upload_data_count += 1\n+\n+        if upload_data_count % tornado.options.options.batch_size == 0:\n+            yield upload_batch(upload_data_txt)\n+            upload_data_txt = \"\"\n+\n+    # upload remaining items in `upload_data_txt`\n+    if upload_data_txt:\n+        yield upload_batch(upload_data_txt)\n+\n+    if tornado.options.options.set_refresh:\n+        set_index_refresh(\"1s\")\n+\n+    if out_file:\n+        out_file.close()\n+\n+    took_secs = int(time.time() - ts_start)\n+\n+    logging.info(\"Done - total docs uploaded: %d, took %d seconds\" % \n+    \t\t\t (tornado.options.options.count, took_secs))\n+\n+\n+if __name__ == '__main__':\n+    tornado.options.define(\"es_url\", type=str, default='http://localhost:9200/', \n+                           help=\"URL of your Elasticsearch node\")\n+    tornado.options.define(\"index_name\", type=str, default='test_data', \n+                           help=\"Name of the index to store your messages\")\n+    tornado.options.define(\"index_type\", type=str, default='test_type', help=\"Type\")\n+    tornado.options.define(\"batch_size\", type=int, default=1000, \n+                           help=\"Elasticsearch bulk index batch size\")\n+    tornado.options.define(\"num_of_shards\", type=int, default=2, \n+                           help=\"Number of shards for ES index\")\n+    tornado.options.define(\"http_upload_timeout\", type=int, default=3, \n+                           help=\"Timeout in seconds when uploading data\")\n+    tornado.options.define(\"count\", type=int, default=100000, help=\"Number of docs to generate\")\n+    tornado.options.define(\"format\", type=str, default='name:str,age:int,last_updated:ts', \n+                           help=\"message format\")\n+    tornado.options.define(\"num_of_replicas\", type=int, default=0, \n+                           help=\"Number of replicas for ES index\")\n+    tornado.options.define(\"force_init_index\", type=bool, default=False, \n+                           help=\"Force deleting and re-initializing the Elasticsearch index\")\n+    tornado.options.define(\"set_refresh\", type=bool, default=False, \n+                           help=\"Set refresh rate to -1 before starting the upload\")\n+    tornado.options.define(\"out_file\", type=str, default=False, \n+                           help=\"If set, write test data to out_file as well.\")\n+    tornado.options.define(\"id_type\", type=str, default=None, \n+                           help=\"Type of 'id' to use for the docs, \\\n+                           valid settings are int and uuid4, None is default\")\n+    tornado.options.define(\"dict_file\", type=str, default=None, \n+                           help=\"Name of dictionary file to use\")\n+    tornado.options.define(\"username\", type=str, default=None, help=\"Username for elasticsearch\")\n+    tornado.options.define(\"password\", type=str, default=None, help=\"Password for elasticsearch\")\n+    tornado.options.parse_command_line()\n+\n+    tornado.ioloop.IOLoop.instance().run_sync(generate_test_data)",
                "changes": 299
            },
            {
                "status": "added",
                "additions": 33,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/show-health.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/show-health.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/elasticsearch/show-health.sh",
                "deletions": 0,
                "sha": "abc3c89d9307e6c5957ce5db0fb19a48c3e0b270",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/show-health.sh",
                "patch": "@@ -0,0 +1,33 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/sh\n+set -e\n+\n+external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+\n+echo \"Waiting for the Elasticsearch service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+\n+echo \"Elasticsearch cluster health info\"\n+echo \"---------------------------------\"\n+curl $external_ip:9200/_cluster/health\n+echo # empty line since curl doesn't output CRLF",
                "changes": 33
            },
            {
                "status": "renamed",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/postgres-service-public.yml",
                "deletions": 5,
                "sha": "5d2c6648590d39e93fb7d52ea96b29aa073df65d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml",
                "patch": "@@ -13,16 +13,16 @@\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n \n+\n apiVersion: v1\n kind: Service\n metadata:\n-  name: postgres-no-pv\n+  name: postgres-for-dev\n   labels:\n-    name: postgres-no-pv\n+    name: postgres\n spec:\n   ports:\n     - port: 5432\n-      nodePort: 31234\n   selector:\n-    name: postgres-no-pv\n-  type: NodePort\n+    name: postgres\n+  type: LoadBalancer",
                "changes": 10
            },
            {
                "status": "added",
                "additions": 56,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres.yml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/postgres/postgres.yml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/kubernetes/postgres/postgres.yml",
                "deletions": 0,
                "sha": "62449689b32be95b31d6ec2e8d5e0e616dcb8a1e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres.yml",
                "patch": "@@ -0,0 +1,56 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: postgres\n+  labels:\n+    name: postgres\n+spec:\n+  ports:\n+    - port: 5432\n+      nodePort: 31234\n+  selector:\n+    name: postgres\n+  type: NodePort\n+\n+---\n+\n+apiVersion: v1\n+kind: ReplicationController\n+metadata:\n+  name: postgres\n+spec:\n+  replicas: 1\n+  selector:\n+    name: postgres\n+  template:\n+    metadata:\n+      name: postgres\n+      labels:\n+        name: postgres\n+    spec:\n+      containers:\n+        - name: postgres\n+          image: postgres\n+          env:\n+            - name: POSTGRES_PASSWORD\n+              value: uuinkks\n+            - name: PGDATA\n+              value: /var/lib/postgresql/data/pgdata\n+          ports:\n+            - containerPort: 5432",
                "changes": 56
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/README.md",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/travis/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/travis/README.md",
                "previous_filename": ".travis/README.md",
                "deletions": 0,
                "sha": "526995aa69ccbab8d71d741747153293a309e0a0",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/README.md"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/settings.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/travis/settings.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/travis/settings.xml",
                "previous_filename": ".travis/settings.xml",
                "deletions": 0,
                "sha": "e086aec3a732d309e5de1a0783a7cd8c86eebe28",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/settings.xml"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/test_wordcount.sh",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/travis/test_wordcount.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".test-infra/travis/test_wordcount.sh",
                "previous_filename": ".travis/test_wordcount.sh",
                "deletions": 0,
                "sha": "e059a3552bfbc142750bd1a76e58536cdb69477a",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/test_wordcount.sh"
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.travis.yml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.travis.yml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": ".travis.yml",
                "deletions": 4,
                "sha": "a1b28f90862c3def718a71e95c6a31cdf308b384",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.travis.yml",
                "patch": "@@ -34,7 +34,7 @@ addons:\n     - python2.7\n env:\n   global:\n-   - MAVEN_OVERRIDE=\"--settings=.travis/settings.xml\"\n+   - MAVEN_OVERRIDE=\"--settings=.test-infra/travis/settings.xml\"\n    - MAVEN_CONTAINER_OVERRIDE=\"-DbeamSurefireArgline='-Xmx512m'\"\n \n matrix:\n@@ -68,7 +68,6 @@ before_install:\n   - if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then jdk_switcher use \"$CUSTOM_JDK\"; fi\n   - export BEAM_SUREFIRE_ARGLINE=\"-Xmx512m\"\n   # Python SDK environment settings.\n-  - export TOX_ENV=py27\n   - if [ \"$TRAVIS_OS_NAME\" == \"osx\" ]; then export TOX_HOME=$HOME/Library/Python/2.7/bin; fi\n   - if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then export TOX_HOME=$HOME/.local/bin; fi\n \n@@ -81,8 +80,8 @@ install:\n   - rm -rf \"$HOME/.m2/repository/org/apache/gearpump\"\n \n script:\n-  - if [ \"$TEST_PYTHON\" ]; then travis_retry $TOX_HOME/tox -e $TOX_ENV -c sdks/python/tox.ini; fi\n-  - if [ ! \"$TEST_PYTHON\" ]; then travis_retry mvn --batch-mode --update-snapshots --no-snapshot-updates --threads 1C $MAVEN_OVERRIDE install && travis_retry bash -ex .travis/test_wordcount.sh; fi\n+  - if [ \"$TEST_PYTHON\" ]; then travis_retry $TOX_HOME/tox -c sdks/python/tox.ini; fi\n+  - if [ ! \"$TEST_PYTHON\" ]; then travis_retry mvn --batch-mode --update-snapshots --no-snapshot-updates --threads 1C $MAVEN_OVERRIDE install && travis_retry bash -ex .test-infra/travis/test_wordcount.sh; fi\n \n cache:\n   directories:",
                "changes": 7
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/README.md",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "README.md",
                "deletions": 0,
                "sha": "23768ce64d3b47e8967903a5527d43233e05b19d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/README.md",
                "patch": "@@ -61,6 +61,7 @@ Have ideas for new SDKs or DSLs? See the [JIRA](https://issues.apache.org/jira/b\n Beam supports executing programs on multiple distributed processing backends through PipelineRunners. Currently, the following PipelineRunners are available:\n \n - The `DirectRunner` runs the pipeline on your local machine.\n+- The `ApexRunner` runs the pipeline on an Apache Hadoop YARN cluster (or in embedded mode).\n - The `DataflowRunner` submits the pipeline to the [Google Cloud Dataflow](http://cloud.google.com/dataflow/).\n - The `FlinkRunner` runs the pipeline on an Apache Flink cluster. The code has been donated from [dataArtisans/flink-dataflow](https://github.com/dataArtisans/flink-dataflow) and is now part of Beam.\n - The `SparkRunner` runs the pipeline on an Apache Spark cluster. The code has been donated from [cloudera/spark-dataflow](https://github.com/cloudera/spark-dataflow) and is now part of Beam.\n@@ -96,6 +97,8 @@ To get involved in Apache Beam:\n * [Subscribe](mailto:dev-subscribe@beam.apache.org) or [mail](mailto:dev@beam.apache.org) the [dev@beam.apache.org](http://mail-archives.apache.org/mod_mbox/beam-dev/) list.\n * Report issues on [JIRA](https://issues.apache.org/jira/browse/BEAM).\n \n+We also have a [contributor's guide](https://beam.apache.org/contribute/contribution-guide/).\n+\n ## More Information\n \n * [Apache Beam](http://beam.apache.org)",
                "changes": 3
            },
            {
                "status": "modified",
                "additions": 34,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/README.md",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/README.md",
                "deletions": 27,
                "sha": "d891fb87842d35c9a1d63ed84b3b50fdc123ec4e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/README.md",
                "patch": "@@ -20,25 +20,23 @@\n # Example Pipelines\n \n The examples included in this module serve to demonstrate the basic\n-functionality of Google Cloud Dataflow, and act as starting points for\n+functionality of Apache Beam, and act as starting points for\n the development of more complex pipelines.\n \n ## Word Count\n \n A good starting point for new users is our set of\n-[word count](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples) examples, which computes word frequencies.  This series of four successively more detailed pipelines is described in detail in the accompanying [walkthrough](https://cloud.google.com/dataflow/examples/wordcount-example).\n+[word count](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples) examples, which computes word frequencies.  This series of four successively more detailed pipelines is described in detail in the accompanying [walkthrough](https://beam.apache.org/get-started/wordcount-example/).\n \n-1. [`MinimalWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java) is the simplest word count pipeline and introduces basic concepts like [Pipelines](https://cloud.google.com/dataflow/model/pipelines),\n-[PCollections](https://cloud.google.com/dataflow/model/pcollection),\n-[ParDo](https://cloud.google.com/dataflow/model/par-do),\n-and [reading and writing data](https://cloud.google.com/dataflow/model/reading-and-writing-data) from external storage.\n+1. [`MinimalWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java) is the simplest word count pipeline and introduces basic concepts like [Pipelines](https://beam.apache.org/documentation/programming-guide/#pipeline),\n+[PCollections](https://beam.apache.org/documentation/programming-guide/#pcollection),\n+[ParDo](https://beam.apache.org/documentation/programming-guide/#transforms-pardo),\n+and [reading and writing data](https://beam.apache.org/documentation/programming-guide/#io) from external storage.\n \n-1. [`WordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java) introduces Dataflow best practices like [PipelineOptions](https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#Creating) and custom [PTransforms](https://cloud.google.com/dataflow/model/composite-transforms).\n+1. [`WordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java) introduces best practices like [PipelineOptions](https://beam.apache.org/documentation/programming-guide/#pipeline) and custom [PTransforms](https://beam.apache.org/documentation/programming-guide/#transforms-composite).\n \n 1. [`DebuggingWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java)\n-shows how to view live aggregators in the [Dataflow Monitoring Interface](https://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf), get the most out of\n-[Cloud Logging](https://cloud.google.com/dataflow/pipelines/logging) integration, and start writing\n-[good tests](https://cloud.google.com/dataflow/pipelines/testing-your-pipeline).\n+demonstrates some best practices for instrumenting your pipeline code.\n \n 1. [`WindowedWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java) shows how to run the same pipeline over either unbounded PCollections in streaming mode or bounded PCollections in batch mode.\n \n@@ -50,46 +48,55 @@ Change directory into `examples/java` and run the examples:\n     -Dexec.mainClass=<MAIN CLASS> \\\n     -Dexec.args=\"<EXAMPLE-SPECIFIC ARGUMENTS>\"\n \n-For example, you can execute the `WordCount` pipeline on your local machine as follows:\n+Alternatively, you may choose to bundle all dependencies into a single JAR and\n+execute it outside of the Maven environment.\n+\n+### Direct Runner\n+\n+You can execute the `WordCount` pipeline on your local machine as follows:\n \n     mvn compile exec:java \\\n     -Dexec.mainClass=org.apache.beam.examples.WordCount \\\n     -Dexec.args=\"--inputFile=<LOCAL INPUT FILE> --output=<LOCAL OUTPUT FILE>\"\n \n-Once you have followed the general Cloud Dataflow\n-[Getting Started](https://cloud.google.com/dataflow/getting-started) instructions, you can execute\n-the same pipeline on fully managed resources in Google Cloud Platform:\n+To create the bundled JAR of the examples and execute it locally:\n+\n+    mvn package\n+\n+    java -cp examples/java/target/beam-examples-java-bundled-<VERSION>.jar \\\n+    org.apache.beam.examples.WordCount \\\n+    --inputFile=<INPUT FILE PATTERN> --output=<OUTPUT FILE>\n+\n+### Google Cloud Dataflow Runner\n+\n+After you have followed the general Cloud Dataflow\n+[prerequisites and setup](https://beam.apache.org/documentation/runners/dataflow/), you can execute\n+the pipeline on fully managed resources in Google Cloud Platform:\n \n     mvn compile exec:java \\\n     -Dexec.mainClass=org.apache.beam.examples.WordCount \\\n     -Dexec.args=\"--project=<YOUR CLOUD PLATFORM PROJECT ID> \\\n     --tempLocation=<YOUR CLOUD STORAGE LOCATION> \\\n-    --runner=BlockingDataflowRunner\"\n+    --runner=DataflowRunner\"\n \n Make sure to use your project id, not the project number or the descriptive name.\n-The Cloud Storage location should be entered in the form of\n+The Google Cloud Storage location should be entered in the form of\n `gs://bucket/path/to/staging/directory`.\n \n-Alternatively, you may choose to bundle all dependencies into a single JAR and\n-execute it outside of the Maven environment. For example, you can execute the\n-following commands to create the\n-bundled JAR of the examples and execute it both locally and in Cloud\n-Platform:\n+To create the bundled JAR of the examples and execute it in Google Cloud Platform:\n \n     mvn package\n \n-    java -cp examples/java/target/beam-examples-java-bundled-<VERSION>.jar \\\n-    org.apache.beam.examples.WordCount \\\n-    --inputFile=<INPUT FILE PATTERN> --output=<OUTPUT FILE>\n-\n     java -cp examples/java/target/beam-examples-java-bundled-<VERSION>.jar \\\n     org.apache.beam.examples.WordCount \\\n     --project=<YOUR CLOUD PLATFORM PROJECT ID> \\\n     --tempLocation=<YOUR CLOUD STORAGE LOCATION> \\\n-    --runner=BlockingDataflowRunner\n+    --runner=DataflowRunner\n+\n+## Other Examples\n \n Other examples can be run similarly by replacing the `WordCount` class path with the example classpath, e.g.\n-`org.apache.beam.examples.cookbook.BigQueryTornadoes`,\n+`org.apache.beam.examples.cookbook.CombinePerKeyExamples`,\n and adjusting runtime options under the `Dexec.args` parameter, as specified in\n the example itself.\n ",
                "changes": 61
            },
            {
                "status": "modified",
                "additions": 31,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/pom.xml",
                "deletions": 1,
                "sha": "96d6917fe17999c9f40bceb6c02d356884f19832",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/pom.xml",
                "patch": "@@ -280,6 +280,31 @@\n                   </systemPropertyVariables>\n                 </configuration>\n               </execution>\n+              <execution>\n+                <id>dataflow-runner-integration-tests-streaming</id>\n+                <goals>\n+                  <goal>integration-test</goal>\n+                  <goal>verify</goal>\n+                </goals>\n+                <configuration>\n+                  <includes>\n+                    <include>WordCountIT.java</include>\n+                    <include>WindowedWordCountIT.java</include>\n+                  </includes>\n+                  <parallel>all</parallel>\n+                  <threadCount>4</threadCount>\n+                  <systemPropertyVariables>\n+                    <beamTestPipelineOptions>\n+                      [\n+                      \"--project=apache-beam-testing\",\n+                      \"--tempRoot=gs://temp-storage-for-end-to-end-tests\",\n+                      \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\",\n+                      \"--streaming=true\"\n+                      ]\n+                    </beamTestPipelineOptions>\n+                  </systemPropertyVariables>\n+                </configuration>\n+              </execution>\n               <execution>\n                 <id>apex-runner-integration-tests</id>\n                 <goals>\n@@ -446,6 +471,11 @@\n       <artifactId>beam-sdks-java-core</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-java-extensions-gcp-core</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n@@ -525,7 +555,7 @@\n \n     <!--\n       For testing the example itself, use the direct runner. This is separate from\n-      the use of RunnableOnService tests for testing a particular runner.\n+      the use of ValidatesRunner tests for testing a particular runner.\n     -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>",
                "changes": 32
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
                "deletions": 1,
                "sha": "031f317ea774440e3f96a133cf6dff2e58286415",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
                "patch": "@@ -151,7 +151,7 @@ public static void main(String[] args) {\n      * <p>Below we verify that the set of filtered words matches our expected counts. Note\n      * that PAssert does not provide any output and that successful completion of the\n      * Pipeline implies that the expectations were met. Learn more at\n-     * https://cloud.google.com/dataflow/pipelines/testing-your-pipeline on how to test\n+     * https://beam.apache.org/documentation/pipelines/test-your-pipeline/ on how to test\n      * your Pipeline and see {@link DebuggingWordCountTest} for an example unit test.\n      */\n     List<KV<String, Long>> expectedResults = Arrays.asList(",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "deletions": 28,
                "sha": "d88de543cd87a654bd3897a6379a3e1bf0648209",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "patch": "@@ -21,7 +21,7 @@\n import java.util.concurrent.ThreadLocalRandom;\n import org.apache.beam.examples.common.ExampleBigQueryTableOptions;\n import org.apache.beam.examples.common.ExampleOptions;\n-import org.apache.beam.examples.common.WriteWindowedFilesDoFn;\n+import org.apache.beam.examples.common.WriteOneFilePerWindow;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.io.TextIO;\n@@ -31,11 +31,9 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.MapElements;\n import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -203,33 +201,13 @@ public static void main(String[] args) throws IOException {\n     PCollection<KV<String, Long>> wordCounts = windowedWords.apply(new WordCount.CountWords());\n \n     /**\n-     * Concept #5: Customize the output format using windowing information\n-     *\n-     * <p>At this point, the data is organized by window. We're writing text files and and have no\n-     * late data, so for simplicity we can use the window as the key and {@link GroupByKey} to get\n-     * one output file per window. (if we had late data this key would not be unique)\n-     *\n-     * <p>To access the window in a {@link DoFn}, add a {@link BoundedWindow} parameter. This will\n-     * be automatically detected and populated with the window for the current element.\n-     */\n-    PCollection<KV<IntervalWindow, KV<String, Long>>> keyedByWindow =\n-        wordCounts.apply(\n-            ParDo.of(\n-                new DoFn<KV<String, Long>, KV<IntervalWindow, KV<String, Long>>>() {\n-                  @ProcessElement\n-                  public void processElement(ProcessContext context, IntervalWindow window) {\n-                    context.output(KV.of(window, context.element()));\n-                  }\n-                }));\n-\n-    /**\n-     * Concept #6: Format the results and write to a sharded file partitioned by window, using a\n+     * Concept #5: Format the results and write to a sharded file partitioned by window, using a\n      * simple ParDo operation. Because there may be failures followed by retries, the\n      * writes must be idempotent, but the details of writing to files is elided here.\n      */\n-    keyedByWindow\n-        .apply(GroupByKey.<IntervalWindow, KV<String, Long>>create())\n-        .apply(ParDo.of(new WriteWindowedFilesDoFn(output)));\n+    wordCounts\n+        .apply(MapElements.via(new WordCount.FormatAsTextFn()))\n+        .apply(new WriteOneFilePerWindow(output));\n \n     PipelineResult result = pipeline.run();\n     try {",
                "changes": 34
            },
            {
                "status": "added",
                "additions": 91,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
                "deletions": 0,
                "sha": "2ed8a741de69be422417fd93ccfedc8bc081e622",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
                "patch": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples.common;\n+\n+import org.apache.beam.sdk.io.FileBasedSink.FilenamePolicy;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.joda.time.format.ISODateTimeFormat;\n+\n+/**\n+ * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n+ * and upper bounds of their key (an {@link IntervalWindow}).\n+ *\n+ * <p>This is test utility code, not for end-users, so examples can be focused on their primary\n+ * lessons.\n+ */\n+public class WriteOneFilePerWindow extends PTransform<PCollection<String>, PDone> {\n+\n+  private static DateTimeFormatter formatter = ISODateTimeFormat.hourMinute();\n+  private String filenamePrefix;\n+\n+  public WriteOneFilePerWindow(String filenamePrefix) {\n+    this.filenamePrefix = filenamePrefix;\n+  }\n+\n+  @Override\n+  public PDone expand(PCollection<String> input) {\n+    return input.apply(\n+        TextIO.Write.to(new PerWindowFiles(filenamePrefix)).withWindowedWrites().withNumShards(3));\n+  }\n+\n+  /**\n+   * A {@link FilenamePolicy} produces a base file name for a write based on metadata about the data\n+   * being written. This always includes the shard number and the total number of shards. For\n+   * windowed writes, it also includes the window and pane index (a sequence number assigned to each\n+   * trigger firing).\n+   */\n+  public static class PerWindowFiles extends FilenamePolicy {\n+\n+    private final String output;\n+\n+    public PerWindowFiles(String output) {\n+      this.output = output;\n+    }\n+\n+    @Override\n+    public ValueProvider<String> getBaseOutputFilenameProvider() {\n+      return StaticValueProvider.of(output);\n+    }\n+\n+    public String   filenamePrefixForWindow(IntervalWindow window) {\n+      return String.format(\n+          \"%s-%s-%s\", output, formatter.print(window.start()), formatter.print(window.end()));\n+    }\n+\n+    @Override\n+    public String windowedFilename(WindowedContext context) {\n+      IntervalWindow window = (IntervalWindow) context.getWindow();\n+      return String.format(\n+          \"%s-%s-of-%s\",\n+          filenamePrefixForWindow(window), context.getShardNumber(), context.getNumShards());\n+    }\n+\n+    @Override\n+    public String unwindowedFilename(Context context) {\n+      throw new UnsupportedOperationException(\"Unsupported.\");\n+    }\n+  }\n+}",
                "changes": 91
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "deletions": 77,
                "sha": "cd6baad44292018ac6c4d8a839c14197ffeb752e",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "patch": "@@ -1,77 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.examples.common;\n-\n-import com.google.common.annotations.VisibleForTesting;\n-import java.io.OutputStream;\n-import java.nio.channels.Channels;\n-import java.nio.charset.StandardCharsets;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n-import org.apache.beam.sdk.util.IOChannelFactory;\n-import org.apache.beam.sdk.util.IOChannelUtils;\n-import org.apache.beam.sdk.values.KV;\n-import org.joda.time.format.DateTimeFormatter;\n-import org.joda.time.format.ISODateTimeFormat;\n-\n-/**\n- * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n- * and upper bounds of their key (an {@link IntervalWindow}).\n- *\n- * <p>This is test utility code, not for end-users, so examples can be focused\n- * on their primary lessons.\n- */\n-public class WriteWindowedFilesDoFn\n-    extends DoFn<KV<IntervalWindow, Iterable<KV<String, Long>>>, Void> {\n-\n-  static final byte[] NEWLINE = \"\\n\".getBytes(StandardCharsets.UTF_8);\n-  static final Coder<String> STRING_CODER = StringUtf8Coder.of();\n-\n-  private static DateTimeFormatter formatter = ISODateTimeFormat.hourMinute();\n-\n-  private final String output;\n-\n-  public WriteWindowedFilesDoFn(String output) {\n-    this.output = output;\n-  }\n-\n-  @VisibleForTesting\n-  public static String fileForWindow(String output, IntervalWindow window) {\n-    return String.format(\n-        \"%s-%s-%s\", output, formatter.print(window.start()), formatter.print(window.end()));\n-  }\n-\n-  @ProcessElement\n-  public void processElement(ProcessContext context) throws Exception {\n-    // Build a file name from the window\n-    IntervalWindow window = context.element().getKey();\n-    String outputShard = fileForWindow(output, window);\n-\n-    // Open the file and write all the values\n-    IOChannelFactory factory = IOChannelUtils.getFactory(outputShard);\n-    OutputStream out = Channels.newOutputStream(factory.create(outputShard, \"text/plain\"));\n-    for (KV<String, Long> wordCount : context.element().getValue()) {\n-      STRING_CODER.encode(\n-          wordCount.getKey() + \": \" + wordCount.getValue(), out, Coder.Context.OUTER);\n-      out.write(NEWLINE);\n-    }\n-    out.close();\n-  }\n-}",
                "changes": 77
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "deletions": 1,
                "sha": "e6621cead488497e6e24a1abf536fea9eef4fdfc",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "patch": "@@ -491,7 +491,7 @@ public static void main(String[] args) throws IOException {\n \n       toWrite\n         .apply(ParDo.of(new FormatForBigquery()))\n-        .apply(BigQueryIO.Write\n+        .apply(BigQueryIO.writeTableRows()\n                .to(tableRef)\n                .withSchema(FormatForBigquery.getSchema())\n                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
                "deletions": 1,
                "sha": "20cee01ad1e60e5042a8131c54e09379eb568563",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
                "patch": "@@ -136,7 +136,7 @@ public static void main(String[] args) throws IOException {\n         .apply(ParDo.of(new ExtractWords()))\n         .apply(ParDo.of(new Uppercase()))\n         .apply(ParDo.of(new StringToRowConverter()))\n-        .apply(BigQueryIO.Write.to(tableSpec)\n+        .apply(BigQueryIO.writeTableRows().to(tableSpec)\n             .withSchema(StringToRowConverter.getSchema()));\n \n     PipelineResult result = pipeline.run();",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "deletions": 2,
                "sha": "f7904d3ea27a5c079c295f84e2dd02c52450fab4",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "patch": "@@ -323,7 +323,6 @@ public void processElement(ProcessContext c) {\n       // presented to each invocation of the DoFn.\n       PCollection<KV<String, Double>> wordToDf = wordToDocCount\n           .apply(\"ComputeDocFrequencies\", ParDo\n-              .withSideInputs(totalDocuments)\n               .of(new DoFn<KV<String, Long>, KV<String, Double>>() {\n                 @ProcessElement\n                 public void processElement(ProcessContext c) {\n@@ -335,7 +334,7 @@ public void processElement(ProcessContext c) {\n \n                   c.output(KV.of(word, documentFrequency));\n                 }\n-              }));\n+              }).withSideInputs(totalDocuments));\n \n       // Join the term frequency and document frequency\n       // collections, each keyed on the word.",
                "changes": 3
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "deletions": 1,
                "sha": "e57da93b0262cefd68971e7d2457165ce0acea08",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "patch": "@@ -348,7 +348,7 @@ public static void main(String[] args) throws IOException {\n             Duration.standardMinutes(options.getWindowDuration())).\n             every(Duration.standardMinutes(options.getWindowSlideEvery()))))\n         .apply(new MaxLaneFlow())\n-        .apply(BigQueryIO.Write.to(tableRef)\n+        .apply(BigQueryIO.writeTableRows().to(tableRef)\n             .withSchema(FormatMaxesFn.getSchema()));\n \n     // Run the pipeline.",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "deletions": 1,
                "sha": "b1f938bca07aa4f01f5ff934e6f6d2a6536ac1ea",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "patch": "@@ -359,7 +359,7 @@ public static void main(String[] args) throws IOException {\n             Duration.standardMinutes(options.getWindowDuration())).\n             every(Duration.standardMinutes(options.getWindowSlideEvery()))))\n         .apply(new TrackSpeed())\n-        .apply(BigQueryIO.Write.to(tableRef)\n+        .apply(BigQueryIO.writeTableRows().to(tableRef)\n             .withSchema(FormatStatsFn.getSchema()));\n \n     // Run the pipeline.",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "deletions": 6,
                "sha": "07a3eddde88b48544eb36ade835d34f74a6efac0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "patch": "@@ -156,13 +156,13 @@ public static void main(String[] args) {\n     fields.add(new TableFieldSchema().setName(\"tornado_count\").setType(\"INTEGER\"));\n     TableSchema schema = new TableSchema().setFields(fields);\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(new CountTornadoes())\n-     .apply(BigQueryIO.Write\n-        .to(options.getOutput())\n-        .withSchema(schema)\n-        .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n-        .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));\n+     .apply(BigQueryIO.writeTableRows()\n+         .to(options.getOutput())\n+         .withSchema(schema)\n+         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n+         .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));\n \n     p.run().waitUntilFinish();\n   }",
                "changes": 12
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "deletions": 2,
                "sha": "8d13b90a72b05ddb33956b36de4e32c5a54ab435",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "patch": "@@ -200,9 +200,9 @@ public static void main(String[] args)\n     fields.add(new TableFieldSchema().setName(\"all_plays\").setType(\"STRING\"));\n     TableSchema schema = new TableSchema().setFields(fields);\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(new PlaysForWord())\n-     .apply(BigQueryIO.Write\n+     .apply(BigQueryIO.writeTableRows()\n         .to(options.getOutput())\n         .withSchema(schema)\n         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "deletions": 4,
                "sha": "fed9db79d1b274cfe9c91c34e616e8b25a035aca",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "patch": "@@ -175,7 +175,6 @@ public BelowGlobalMean(Integer monthFilter) {\n       // We'll only output readings with temperatures below this mean.\n       PCollection<TableRow> filteredRows = monthFilteredRows\n           .apply(\"ParseAndFilter\", ParDo\n-              .withSideInputs(globalMeanTemp)\n               .of(new DoFn<TableRow, TableRow>() {\n                 @ProcessElement\n                 public void processElement(ProcessContext c) {\n@@ -185,7 +184,7 @@ public void processElement(ProcessContext c) {\n                     c.output(c.element());\n                   }\n                 }\n-              }));\n+              }).withSideInputs(globalMeanTemp));\n \n       return filteredRows;\n     }\n@@ -238,10 +237,10 @@ public static void main(String[] args)\n \n     TableSchema schema = buildWeatherSchemaProjection();\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(ParDo.of(new ProjectionFn()))\n      .apply(new BelowGlobalMean(options.getMonthFilter()))\n-     .apply(BigQueryIO.Write\n+     .apply(BigQueryIO.writeTableRows()\n         .to(options.getOutput())\n         .withSchema(schema)\n         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "changes": 7
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
                "deletions": 2,
                "sha": "05a3ad34eb4ac16615ef57135d6834376c3085af",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
                "patch": "@@ -166,8 +166,8 @@ public static void main(String[] args) throws Exception {\n     Pipeline p = Pipeline.create(options);\n     // the following two 'applys' create multiple inputs to our pipeline, one for each\n     // of our two input sources.\n-    PCollection<TableRow> eventsTable = p.apply(BigQueryIO.Read.from(GDELT_EVENTS_TABLE));\n-    PCollection<TableRow> countryCodes = p.apply(BigQueryIO.Read.from(COUNTRY_CODES));\n+    PCollection<TableRow> eventsTable = p.apply(BigQueryIO.read().from(GDELT_EVENTS_TABLE));\n+    PCollection<TableRow> countryCodes = p.apply(BigQueryIO.read().from(COUNTRY_CODES));\n     PCollection<String> formattedResults = joinEvents(eventsTable, countryCodes);\n     formattedResults.apply(TextIO.Write.to(options.getOutput()));\n     p.run().waitUntilFinish();",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "deletions": 2,
                "sha": "295b3f4a0e9c9dc4d14eb50dfd63f5bd31955fd6",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "patch": "@@ -149,9 +149,9 @@ public static void main(String[] args)\n     fields.add(new TableFieldSchema().setName(\"max_mean_temp\").setType(\"FLOAT\"));\n     TableSchema schema = new TableSchema().setFields(fields);\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(new MaxMeanTemp())\n-     .apply(BigQueryIO.Write\n+     .apply(BigQueryIO.writeTableRows()\n         .to(options.getOutput())\n         .withSchema(schema)\n         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
                "deletions": 1,
                "sha": "b167cd7a937b96acc909ad0ca4c58ae8882d0a3f",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
                "patch": "@@ -21,7 +21,7 @@\n \n This directory holds simple \"cookbook\" examples, which show how to define\n commonly-used data analysis patterns that you would likely incorporate into a\n-larger Dataflow pipeline. They include:\n+larger Apache Beam pipeline. They include:\n \n  <ul>\n   <li><a href=\"https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java\">BigQueryTornadoes</a>",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "deletions": 1,
                "sha": "0b5d9adc637fda50265d759599bbc00a4f22fe04",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "patch": "@@ -452,7 +452,9 @@ public static void main(String[] args) throws Exception {\n         .apply(new CalculateTotalFlow(options.getWindowDuration()));\n \n     for (int i = 0; i < resultList.size(); i++){\n-      resultList.get(i).apply(BigQueryIO.Write.to(tableRef).withSchema(getSchema()));\n+      resultList.get(i).apply(BigQueryIO.writeTableRows()\n+          .to(tableRef)\n+          .withSchema(getSchema()));\n     }\n \n     PipelineResult result = pipeline.run();",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 26,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "deletions": 15,
                "sha": "857f1d3eb5cacfef3ac539e1eb19293f9f9e63aa",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "patch": "@@ -23,13 +23,14 @@\n import com.google.common.base.MoreObjects;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n+import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Date;\n import java.util.List;\n import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.concurrent.ThreadLocalRandom;\n-import org.apache.beam.examples.common.WriteWindowedFilesDoFn;\n+import org.apache.beam.examples.common.WriteOneFilePerWindow.PerWindowFiles;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.StreamingOptions;\n@@ -42,6 +43,7 @@\n import org.apache.beam.sdk.util.ExplicitShardedFile;\n import org.apache.beam.sdk.util.FluentBackoff;\n import org.apache.beam.sdk.util.IOChannelUtils;\n+import org.apache.beam.sdk.util.NumberedShardedFile;\n import org.apache.beam.sdk.util.ShardedFile;\n import org.hamcrest.Description;\n import org.hamcrest.TypeSafeMatcher;\n@@ -64,7 +66,7 @@\n   @Rule public TestName testName = new TestName();\n \n   private static final String DEFAULT_INPUT =\n-      \"gs://apache-beam-samples/shakespeare/winterstale-personae\";\n+      \"gs://apache-beam-samples/shakespeare/sonnets.txt\";\n   static final int MAX_READ_RETRIES = 4;\n   static final Duration DEFAULT_SLEEP_DURATION = Duration.standardSeconds(10L);\n   static final FluentBackoff BACK_OFF_FACTORY =\n@@ -130,14 +132,18 @@ private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) t\n \n     String outputPrefix = options.getOutput();\n \n-    List<String> expectedOutputFiles = Lists.newArrayListWithCapacity(6);\n+    PerWindowFiles filenamePolicy = new PerWindowFiles(outputPrefix);\n+\n+    List<ShardedFile> expectedOutputFiles = Lists.newArrayListWithCapacity(6);\n+\n     for (int startMinute : ImmutableList.of(0, 10, 20, 30, 40, 50)) {\n-      Instant windowStart =\n+      final Instant windowStart =\n           new Instant(options.getMinTimestampMillis()).plus(Duration.standardMinutes(startMinute));\n       expectedOutputFiles.add(\n-          WriteWindowedFilesDoFn.fileForWindow(\n-              outputPrefix,\n-              new IntervalWindow(windowStart, windowStart.plus(Duration.standardMinutes(10)))));\n+          new NumberedShardedFile(\n+              filenamePolicy.filenamePrefixForWindow(\n+                  new IntervalWindow(\n+                      windowStart, windowStart.plus(Duration.standardMinutes(10)))) + \"*\"));\n     }\n \n     ShardedFile inputFile = new ExplicitShardedFile(Collections.singleton(options.getInputFile()));\n@@ -157,7 +163,7 @@ private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) t\n     }\n \n     options.setOnSuccessMatcher(\n-        new WordCountsMatcher(expectedWordCounts, new ExplicitShardedFile(expectedOutputFiles)));\n+        new WordCountsMatcher(expectedWordCounts, expectedOutputFiles));\n \n     WindowedWordCount.main(TestPipeline.convertToArgs(options));\n   }\n@@ -172,24 +178,28 @@ private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) t\n     private static final Logger LOG = LoggerFactory.getLogger(FileChecksumMatcher.class);\n \n     private final SortedMap<String, Long> expectedWordCounts;\n-    private final ShardedFile outputFile;\n+    private final List<ShardedFile> outputFiles;\n     private SortedMap<String, Long> actualCounts;\n \n-    public WordCountsMatcher(SortedMap<String, Long> expectedWordCounts, ShardedFile outputFile) {\n+    public WordCountsMatcher(\n+        SortedMap<String, Long> expectedWordCounts, List<ShardedFile> outputFiles) {\n       this.expectedWordCounts = expectedWordCounts;\n-      this.outputFile = outputFile;\n+      this.outputFiles = outputFiles;\n     }\n \n     @Override\n     public boolean matchesSafely(PipelineResult pipelineResult) {\n       try {\n         // Load output data\n-        List<String> lines =\n-            outputFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff());\n+        List<String> outputLines = new ArrayList<>();\n+        for (ShardedFile outputFile : outputFiles) {\n+          outputLines.addAll(\n+              outputFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff()));\n+        }\n \n         // Since the windowing is nondeterministic we only check the sums\n         actualCounts = new TreeMap<>();\n-        for (String line : lines) {\n+        for (String line : outputLines) {\n           String[] splits = line.split(\": \");\n           String word = splits[0];\n           long count = Long.parseLong(splits[1]);\n@@ -205,7 +215,8 @@ public boolean matchesSafely(PipelineResult pipelineResult) {\n         return actualCounts.equals(expectedWordCounts);\n       } catch (Exception e) {\n         throw new RuntimeException(\n-            String.format(\"Failed to read from sharded output: %s\", outputFile));\n+            String.format(\"Failed to read from sharded output: %s due to exception\",\n+                outputFiles), e);\n       }\n     }\n ",
                "changes": 41
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java",
                "deletions": 2,
                "sha": "54ce1e31beb186158f49bb44b3a24a1626413efa",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java",
                "patch": "@@ -24,8 +24,8 @@\n import org.apache.beam.examples.WordCount.FormatAsTextFn;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFnTester;\n@@ -73,7 +73,7 @@ public void testExtractWordsFn() throws Exception {\n \n   /** Example test that tests a PTransform by using an in-memory input and inspecting the output. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testCountWords() throws Exception {\n     PCollection<String> input = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));\n ",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java",
                "deletions": 2,
                "sha": "d263643807a0a8c203751520ca27f214d9da2ba3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java",
                "patch": "@@ -21,8 +21,8 @@\n import java.util.Arrays;\n import org.apache.beam.sdk.coders.StringDelegateCoder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Distinct;\n import org.apache.beam.sdk.transforms.Keys;\n@@ -45,7 +45,7 @@\n \n   /** Test that the example runs. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testTfIdf() throws Exception {\n \n     pipeline.getCoderRegistry().registerCoder(URI.class, StringDelegateCoder.of(URI.class));",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java",
                "deletions": 2,
                "sha": "5415281ac4555381cd738dfad094ee49227911d2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java",
                "patch": "@@ -20,8 +20,8 @@\n import com.google.api.services.bigquery.model.TableRow;\n import java.util.Arrays;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.values.PCollection;\n import org.junit.Rule;\n@@ -38,7 +38,7 @@\n   public TestPipeline p = TestPipeline.create();\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testComputeTopUsers() {\n \n     PCollection<String> output =",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java",
                "deletions": 3,
                "sha": "c9dab801ae7134545161103cd4e9184c0fc91317",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java",
                "patch": "@@ -21,8 +21,8 @@\n import java.util.List;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Distinct;\n import org.apache.beam.sdk.values.PCollection;\n@@ -40,7 +40,7 @@\n   public TestPipeline p = TestPipeline.create();\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testDistinct() {\n     List<String> strings = Arrays.asList(\n         \"k1\",\n@@ -64,7 +64,7 @@ public void testDistinct() {\n   }\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testDistinctEmpty() {\n     List<String> strings = Arrays.asList();\n ",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java",
                "deletions": 2,
                "sha": "b2fcd736e4591401b9df510ad1e051467b05329b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java",
                "patch": "@@ -23,8 +23,8 @@\n import org.apache.beam.examples.cookbook.JoinExamples.ExtractCountryInfoFn;\n import org.apache.beam.examples.cookbook.JoinExamples.ExtractEventDataFn;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFnTester;\n import org.apache.beam.sdk.values.KV;\n@@ -103,7 +103,7 @@ public void testExtractCountryInfoFn() throws Exception {\n \n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testJoin() throws java.lang.Exception {\n     PCollection<TableRow> input1 = p.apply(\"CreateEvent\", Create.of(EVENT_ARRAY));\n     PCollection<TableRow> input2 = p.apply(\"CreateCC\", Create.of(CC_ARRAY));",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java",
                "deletions": 2,
                "sha": "706cfb91b435132b46e092a8f27801c059d58961",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java",
                "patch": "@@ -27,8 +27,8 @@\n import org.apache.beam.examples.cookbook.TriggerExample.ExtractFlowInfo;\n import org.apache.beam.examples.cookbook.TriggerExample.TotalFlow;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFnTester;\n@@ -111,7 +111,7 @@ public void testExtractTotalFlow() throws Exception {\n   }\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testTotalFlow () {\n     PCollection<KV<String, Integer>> flow = pipeline\n         .apply(Create.timestamped(TIME_STAMPED_INPUT))",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 40,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/pom.xml",
                "deletions": 1,
                "sha": "cd69acbd3cbc412132f567a8b7fd473cc3fcbd37",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/pom.xml",
                "patch": "@@ -153,6 +153,40 @@\n         </configuration>\n       </plugin>\n \n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-shade-plugin</artifactId>\n+        <executions>\n+          <execution>\n+            <phase>package</phase>\n+            <goals>\n+              <goal>shade</goal>\n+            </goals>\n+            <configuration>\n+              <finalName>${project.artifactId}-bundled-${project.version}</finalName>\n+              <artifactSet>\n+                <includes>\n+                  <include>*:*</include>\n+                </includes>\n+              </artifactSet>\n+              <filters>\n+                <filter>\n+                  <artifact>*:*</artifact>\n+                  <excludes>\n+                    <exclude>META-INF/*.SF</exclude>\n+                    <exclude>META-INF/*.DSA</exclude>\n+                    <exclude>META-INF/*.RSA</exclude>\n+                  </excludes>\n+                </filter>\n+              </filters>\n+              <transformers>\n+                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n+              </transformers>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+\n       <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-jar-plugin</artifactId>\n@@ -172,6 +206,11 @@\n       <artifactId>beam-sdks-java-core</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-java-extensions-gcp-core</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n@@ -257,7 +296,7 @@\n \n     <!--\n       For testing the example itself, use the direct runner. This is separate from\n-      the use of RunnableOnService tests for testing a particular runner.\n+      the use of ValidatesRunner tests for testing a particular runner.\n     -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>",
                "changes": 41
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
                "deletions": 4,
                "sha": "f424a7b7296439294a306c83e90019bfdb43e574",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
                "patch": "@@ -56,13 +56,14 @@ public static void main(String[] args) {\n     Pipeline p = Pipeline.create(options);\n \n     p.apply(TextIO.Read.from(\"gs://apache-beam-samples/shakespeare/*\"))\n-     .apply(FlatMapElements.via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\")))\n-         .withOutputType(TypeDescriptors.strings()))\n+     .apply(FlatMapElements\n+         .into(TypeDescriptors.strings())\n+         .via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\"))))\n      .apply(Filter.by((String word) -> !word.isEmpty()))\n      .apply(Count.<String>perElement())\n      .apply(MapElements\n-         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue())\n-         .withOutputType(TypeDescriptors.strings()))\n+         .into(TypeDescriptors.strings())\n+         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue()))\n \n      // CHANGE 3/3: The Google Cloud Storage path is required for outputting the results to.\n      .apply(TextIO.Write.to(\"gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX\"));",
                "changes": 9
            },
            {
                "status": "modified",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "deletions": 9,
                "sha": "9c79fad4a77abdeef7ca13825e726f58829e1178",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "patch": "@@ -25,7 +25,7 @@\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.PubsubIO;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n import org.apache.beam.sdk.options.Default;\n import org.apache.beam.sdk.options.Description;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n@@ -125,7 +125,6 @@\n       PCollection<KV<String, Integer>> filtered = sumScores\n           .apply(\"ProcessAndFilter\", ParDo\n               // use the derived mean total score as a side input\n-              .withSideInputs(globalMeanScore)\n               .of(new DoFn<KV<String, Integer>, KV<String, Integer>>() {\n                 private final Aggregator<Long, Long> numSpammerUsers =\n                   createAggregator(\"SpammerUsers\", Sum.ofLongs());\n@@ -140,7 +139,7 @@ public void processElement(ProcessContext c) {\n                     c.output(c.element());\n                   }\n                 }\n-              }));\n+              }).withSideInputs(globalMeanScore));\n       return filtered;\n     }\n   }\n@@ -261,9 +260,9 @@ public static void main(String[] args) throws Exception {\n     // Extract username/score pairs from the event stream\n     PCollection<KV<String, Integer>> userEvents =\n         rawEvents.apply(\"ExtractUserScore\",\n-          MapElements.via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore()))\n-            .withOutputType(\n-                TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())));\n+          MapElements\n+              .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+              .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));\n \n     // Calculate the total score per user over fixed windows, and\n     // cumulative updates for late data.\n@@ -288,7 +287,6 @@ public static void main(String[] args) throws Exception {\n           FixedWindows.of(Duration.standardMinutes(options.getFixedWindowDuration()))))\n       // Filter out the detected spammer users, using the side input derived above.\n       .apply(\"FilterOutSpammers\", ParDo\n-              .withSideInputs(spammersView)\n               .of(new DoFn<GameActionInfo, GameActionInfo>() {\n                 @ProcessElement\n                 public void processElement(ProcessContext c) {\n@@ -297,8 +295,8 @@ public void processElement(ProcessContext c) {\n                     c.output(c.element());\n                   }\n                 }\n-              }))\n-      // Extract and sum teamname/score pairs from the event data.\n+              }).withSideInputs(spammersView))\n+        // Extract and sum teamname/score pairs from the event data.\n       .apply(\"ExtractTeamScore\", new ExtractAndSumScore(\"team\"))\n       // [END DocInclude_FilterAndCalc]\n       // Write the result to BigQuery",
                "changes": 16
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "deletions": 1,
                "sha": "96f42919e7028ac9c149361e6d9f5a5ff409bd87",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "patch": "@@ -28,7 +28,7 @@\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.PubsubIO;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n import org.apache.beam.sdk.options.Default;\n import org.apache.beam.sdk.options.Description;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
                "deletions": 3,
                "sha": "fdce05cd240565bf3c5d27cabfe5488098ecee19",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
                "patch": "@@ -20,10 +20,10 @@\n # 'Gaming' examples\n \n \n-This directory holds a series of example Dataflow pipelines in a simple 'mobile\n+This directory holds a series of example Apache Beam pipelines in a simple 'mobile\n gaming' domain. They all require Java 8.  Each pipeline successively introduces\n new concepts, and gives some examples of using Java 8 syntax in constructing\n-Dataflow pipelines. Other than usage of Java 8 lambda expressions, the concepts\n+Beam pipelines. Other than usage of Java 8 lambda expressions, the concepts\n that are used apply equally well in Java 7.\n \n In the gaming scenario, many users play, as members of different teams, over\n@@ -58,7 +58,7 @@ the day's cutoff point.\n \n The next pipeline in the series is `HourlyTeamScore`. This pipeline also\n processes data collected from gaming events in batch. It builds on `UserScore`,\n-but uses [fixed windows](https://cloud.google.com/dataflow/model/windowing), by\n+but uses [fixed windows](https://beam.apache.org/documentation/programming-guide/#windowing), by\n default an hour in duration. It calculates the sum of scores per team, for each\n window, optionally allowing specification of two timestamps before and after\n which data is filtered out. This allows a model where late data collected after",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "deletions": 3,
                "sha": "b4b023fdcd70442d883b107bc76e3140606bd6cc",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "patch": "@@ -165,9 +165,8 @@ public void processElement(ProcessContext c) {\n \n       return gameInfo\n         .apply(MapElements\n-            .via((GameActionInfo gInfo) -> KV.of(gInfo.getKey(field), gInfo.getScore()))\n-            .withOutputType(\n-                TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())))\n+            .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+            .via((GameActionInfo gInfo) -> KV.of(gInfo.getKey(field), gInfo.getScore())))\n         .apply(Sum.<String>integersPerKey());\n     }\n   }",
                "changes": 5
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
                "deletions": 2,
                "sha": "b9a3ff23f77b1a03d30839fb8cef41cd9f472370",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
                "patch": "@@ -260,8 +260,7 @@ private static String generateEvent(Long currTime, int delayInMillis) {\n       user = team.getRandomUser();\n     }\n     String event = user + \",\" + teamName + \",\" + random.nextInt(MAX_SCORE);\n-    // Randomly introduce occasional parse errors. You can see a custom counter tracking the number\n-    // of such errors in the Dataflow Monitoring UI, as the example pipeline runs.\n+    // Randomly introduce occasional parse errors.\n     if (random.nextInt(parseErrorRate) == 0) {\n       System.out.println(\"Introducing a parse error.\");\n       event = \"THIS LINE REPRESENTS CORRUPT DATA AND WILL CAUSE A PARSE ERROR\";",
                "changes": 3
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "deletions": 7,
                "sha": "5eecddb4e82d6d8598cb675e2ecbb1c25763492a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "patch": "@@ -119,14 +119,13 @@ protected TableSchema getSchema() {\n \n   @Override\n   public PDone expand(PCollection<InputT> teamAndScore) {\n-    return teamAndScore\n+    teamAndScore\n       .apply(\"ConvertToRow\", ParDo.of(new BuildRowFn()))\n-      .apply(BigQueryIO.Write\n-                .to(getTable(teamAndScore.getPipeline(),\n-                    tableName))\n-                .withSchema(getSchema())\n-                .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n-                .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n+      .apply(BigQueryIO.writeTableRows().to(getTable(teamAndScore.getPipeline(), tableName))\n+          .withSchema(getSchema())\n+          .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n+          .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n+    return PDone.in(teamAndScore.getPipeline());\n   }\n \n   /** Utility to construct an output table reference. */",
                "changes": 13
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "deletions": 4,
                "sha": "e602258c74fa3b6f2ac487868d2992f70055583b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import com.google.api.services.bigquery.model.TableRow;\n import java.util.Map;\n+\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;\n@@ -58,14 +59,14 @@ public void processElement(ProcessContext c, BoundedWindow window) {\n \n   @Override\n   public PDone expand(PCollection<T> teamAndScore) {\n-    return teamAndScore\n+    teamAndScore\n       .apply(\"ConvertToRow\", ParDo.of(new BuildRowFn()))\n-      .apply(BigQueryIO.Write\n-                .to(getTable(teamAndScore.getPipeline(),\n-                    tableName))\n+      .apply(BigQueryIO.writeTableRows()\n+                .to(getTable(teamAndScore.getPipeline(), tableName))\n                 .withSchema(getSchema())\n                 .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n                 .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n+    return PDone.in(teamAndScore.getPipeline());\n   }\n \n }",
                "changes": 9
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java",
                "deletions": 4,
                "sha": "6c66d8f1b71ff01d29c43371d88944d784b7dee5",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java",
                "patch": "@@ -63,13 +63,14 @@ public void testMinimalWordCountJava8() throws Exception {\n     p.getOptions().as(GcsOptions.class).setGcsUtil(buildMockGcsUtil());\n \n     p.apply(TextIO.Read.from(\"gs://apache-beam-samples/shakespeare/*\"))\n-     .apply(FlatMapElements.via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\")))\n-         .withOutputType(TypeDescriptors.strings()))\n+     .apply(FlatMapElements\n+         .into(TypeDescriptors.strings())\n+         .via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\"))))\n      .apply(Filter.by((String word) -> !word.isEmpty()))\n      .apply(Count.<String>perElement())\n      .apply(MapElements\n-         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue())\n-         .withOutputType(TypeDescriptors.strings()))\n+         .into(TypeDescriptors.strings())\n+         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue()))\n      .apply(TextIO.Write.to(\"gs://your-output-bucket/and-output-prefix\"));\n   }\n ",
                "changes": 9
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java",
                "deletions": 3,
                "sha": "44481c55df4d50d8b67baef995775b6cb3372f99",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java",
                "patch": "@@ -23,8 +23,8 @@\n import org.apache.beam.examples.complete.game.GameStats.CalculateSpammyUsers;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -38,7 +38,7 @@\n  * Tests of GameStats.\n  * Because the pipeline was designed for easy readability and explanations, it lacks good\n  * modularity for testing. See our testing documentation for better ideas:\n- * https://cloud.google.com/dataflow/pipelines/testing-your-pipeline.\n+ * https://beam.apache.org/documentation/pipelines/test-your-pipeline/\n  */\n @RunWith(JUnit4.class)\n public class GameStatsTest implements Serializable {\n@@ -63,7 +63,7 @@\n \n   /** Test the calculation of 'spammy users'. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testCalculateSpammyUsers() throws Exception {\n     PCollection<KV<String, Integer>> input = p.apply(Create.of(USER_SCORES));\n     PCollection<KV<String, Integer>> output = input.apply(new CalculateSpammyUsers());",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java",
                "deletions": 6,
                "sha": "409fc923ed20901ad37d3a005793e1d3668d3b29",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Filter;\n import org.apache.beam.sdk.transforms.MapElements;\n@@ -45,7 +45,7 @@\n  * Tests of HourlyTeamScore.\n  * Because the pipeline was designed for easy readability and explanations, it lacks good\n  * modularity for testing. See our testing documentation for better ideas:\n- * https://cloud.google.com/dataflow/pipelines/testing-your-pipeline.\n+ * https://beam.apache.org/documentation/pipelines/test-your-pipeline/\n  */\n @RunWith(JUnit4.class)\n public class HourlyTeamScoreTest implements Serializable {\n@@ -86,7 +86,7 @@\n \n   /** Test the filtering. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testUserScoresFilter() throws Exception {\n \n     final Instant startMinTimestamp = new Instant(1447965680000L);\n@@ -101,9 +101,8 @@ public void testUserScoresFilter() throws Exception {\n               -> gInfo.getTimestamp() > startMinTimestamp.getMillis()))\n       // run a map to access the fields in the result.\n       .apply(MapElements\n-          .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore()))\n-          .withOutputType(\n-              TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())));\n+          .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+          .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));\n \n       PAssert.that(output).containsInAnyOrder(FILTERED_EVENTS);\n ",
                "changes": 11
            },
            {
                "status": "modified",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java",
                "deletions": 7,
                "sha": "2eb63aa47202b3b0285ad56268633ee6f3976646",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.beam.examples.complete.game.UserScore.ParseEventFn;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFnTester;\n import org.apache.beam.sdk.transforms.MapElements;\n@@ -99,7 +99,7 @@ public void testParseEventFn() throws Exception {\n \n   /** Tests ExtractAndSumScore(\"user\"). */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testUserScoreSums() throws Exception {\n \n     PCollection<String> input = p.apply(Create.of(GAME_EVENTS).withCoder(StringUtf8Coder.of()));\n@@ -117,7 +117,7 @@ public void testUserScoreSums() throws Exception {\n \n   /** Tests ExtractAndSumScore(\"team\"). */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testTeamScoreSums() throws Exception {\n \n     PCollection<String> input = p.apply(Create.of(GAME_EVENTS).withCoder(StringUtf8Coder.of()));\n@@ -135,17 +135,17 @@ public void testTeamScoreSums() throws Exception {\n \n   /** Test that bad input data is dropped appropriately. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testUserScoresBadInput() throws Exception {\n \n     PCollection<String> input = p.apply(Create.of(GAME_EVENTS2).withCoder(StringUtf8Coder.of()));\n \n     PCollection<KV<String, Integer>> extract = input\n       .apply(ParDo.of(new ParseEventFn()))\n       .apply(\n-          MapElements.via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore()))\n-          .withOutputType(\n-              TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())));\n+          MapElements\n+              .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+              .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));\n \n     PAssert.that(extract).empty();\n ",
                "changes": 14
            },
            {
                "status": "modified",
                "additions": 140,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "pom.xml",
                "deletions": 21,
                "sha": "2945e86dc21e1385ba23f121ec34e2e66e9e2579",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/pom.xml",
                "patch": "@@ -102,6 +102,7 @@\n \n     <!-- If updating dependencies, please update any relevant javadoc offlineLinks -->\n     <apache.commons.lang.version>3.5</apache.commons.lang.version>\n+    <apache.commons.compress.version>1.9</apache.commons.compress.version>\n     <apex.kryo.version>2.24.0</apex.kryo.version>\n     <avro.version>1.8.1</avro.version>\n     <bigquery.version>v2-rev295-1.22.0</bigquery.version>\n@@ -114,28 +115,30 @@\n     <datastore.proto.version>1.3.0</datastore.proto.version>\n     <google-auto-service.version>1.0-rc2</google-auto-service.version>\n     <google-auto-value.version>1.3</google-auto-value.version>\n-    <google-auth.version>0.6.0</google-auth.version>\n+    <google-auth.version>0.6.1</google-auth.version>\n     <google-clients.version>1.22.0</google-clients.version>\n     <google-cloud-bigdataoss.version>1.4.5</google-cloud-bigdataoss.version>\n     <google-cloud-dataflow-java-proto-library-all.version>0.5.160304</google-cloud-dataflow-java-proto-library-all.version>\n     <guava.version>20.0</guava.version>\n-    <grpc.version>1.0.1</grpc.version>\n+    <grpc.version>1.2.0</grpc.version>\n+    <grpc-google-common-protos.version>0.1.0</grpc-google-common-protos.version>\n     <hamcrest.version>1.3</hamcrest.version>\n-    <jackson.version>2.7.2</jackson.version>\n+    <jackson.version>2.8.8</jackson.version>\n     <findbugs.version>3.0.1</findbugs.version>\n     <joda.version>2.4</joda.version>\n     <junit.version>4.12</junit.version>\n     <mockito.version>1.9.5</mockito.version>\n-    <netty.version>4.1.3.Final</netty.version>\n-    <os-maven-plugin.version>1.4.0.Final</os-maven-plugin.version>\n-    <protobuf.version>3.1.0</protobuf.version>\n+    <netty.version>4.1.8.Final</netty.version>\n+    <os-maven-plugin.version>1.5.0.Final</os-maven-plugin.version>\n+    <protobuf.version>3.2.0</protobuf.version>\n     <pubsub.version>v1-rev10-1.22.0</pubsub.version>\n     <slf4j.version>1.7.14</slf4j.version>\n     <spark.version>1.6.2</spark.version>\n     <stax2.version>3.1.4</stax2.version>\n     <storage.version>v1-rev71-1.22.0</storage.version>\n     <woodstox.version>4.4.1</woodstox.version>\n     <spring.version>4.3.5.RELEASE</spring.version>\n+    <groovy-maven-plugin.version>2.0</groovy-maven-plugin.version>\n     \n     <compiler.error.flag>-Werror</compiler.error.flag>\n     <compiler.default.pkginfo.flag>-Xpkginfo:always</compiler.default.pkginfo.flag>\n@@ -169,7 +172,7 @@\n               <artifactId>maven-javadoc-plugin</artifactId>\n               <executions>\n                 <execution>\n-                  <id>javadoc</id>\n+                  <id>attach-javadocs</id>\n                   <phase>package</phase>\n                   <goals>\n                     <goal>jar</goal>\n@@ -353,6 +356,12 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-extensions-gcp-core</artifactId>\n+        <version>${project.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>org.apache.beam</groupId>\n         <artifactId>beam-sdks-java-extensions-sorter</artifactId>\n@@ -365,6 +374,19 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-io-common</artifactId>\n+        <version>${project.version}</version>\n+      </dependency>\n+\n+      <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-io-common</artifactId>\n+        <classifier>tests</classifier>\n+        <version>${project.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>org.apache.beam</groupId>\n         <artifactId>beam-sdks-java-io-elasticsearch</artifactId>\n@@ -431,6 +453,12 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+\t  <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-io-hadoop-input-format</artifactId>\n+\t    <version>${project.version}</version>\n+      </dependency>\n+\t\n       <dependency>\n         <groupId>org.apache.beam</groupId>\n         <artifactId>beam-runners-core-construction-java</artifactId>\n@@ -491,6 +519,12 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>org.apache.commons</groupId>\n+        <artifactId>commons-compress</artifactId>\n+        <version>${apache.commons.compress.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>org.apache.commons</groupId>\n         <artifactId>commons-lang3</artifactId>\n@@ -525,12 +559,12 @@\n         <groupId>io.grpc</groupId>\n         <artifactId>grpc-protobuf-lite</artifactId>\n         <version>${grpc.version}</version>\n-      </dependency>\n-\n-      <dependency>\n-        <groupId>com.google.protobuf</groupId>\n-        <artifactId>protobuf-lite</artifactId>\n-        <version>3.0.1</version>\n+        <exclusions>\n+          <exclusion>\n+            <groupId>com.google.protobuf</groupId>\n+            <artifactId>protobuf-lite</artifactId>\n+          </exclusion>\n+        </exclusions>\n       </dependency>\n \n       <dependency>\n@@ -823,6 +857,12 @@\n         <version>${protobuf.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>com.google.api.grpc</groupId>\n+        <artifactId>grpc-google-common-protos</artifactId>\n+        <version>${grpc-google-common-protos.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>com.fasterxml.jackson.core</groupId>\n         <artifactId>jackson-core</artifactId>\n@@ -1168,13 +1208,7 @@\n           <artifactId>versions-maven-plugin</artifactId>\n           <version>2.3</version>\n         </plugin>\n-\t\n-        <plugin>\n-          <groupId>org.codehaus.mojo</groupId>\n-          <artifactId>build-helper-maven-plugin</artifactId>\n-          <version>1.12</version>\n-        </plugin>\n-\t\n+\n         <plugin>\n           <groupId>org.codehaus.mojo</groupId>\n           <artifactId>exec-maven-plugin</artifactId>\n@@ -1317,6 +1351,51 @@\n           <groupId>org.apache.maven.plugins</groupId>\n           <artifactId>maven-shade-plugin</artifactId>\n           <version>3.0.0</version>\n+          <executions>\n+            <execution>\n+              <id>bundle-and-repackage</id>\n+              <phase>package</phase>\n+              <goals>\n+                <goal>shade</goal>\n+              </goals>\n+              <configuration>\n+                <artifactSet>\n+                  <includes>\n+                    <include>com.google.guava:guava</include>\n+                  </includes>\n+                </artifactSet>\n+                <filters>\n+                  <filter>\n+                    <artifact>*:*</artifact>\n+                    <excludes>\n+                      <exclude>META-INF/*.SF</exclude>\n+                      <exclude>META-INF/*.DSA</exclude>\n+                      <exclude>META-INF/*.RSA</exclude>\n+                    </excludes>\n+                  </filter>\n+                </filters>\n+                <relocations>\n+                  <relocation>\n+                    <pattern>com.google.common</pattern>\n+                    <!--suppress MavenModelInspection -->\n+                    <shadedPattern>\n+                      org.apache.${renderedArtifactId}.repackaged.com.google.common\n+                    </shadedPattern>\n+                  </relocation>\n+                  <relocation>\n+                    <pattern>com.google.thirdparty</pattern>\n+                    <!--suppress MavenModelInspection -->\n+                    <shadedPattern>\n+                      org.apache.${renderedArtifactId}.repackaged.com.google.thirdparty\n+                    </shadedPattern>\n+                  </relocation>\n+                </relocations>\n+                <transformers>\n+                  <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n+                </transformers>\n+              </configuration>\n+            </execution>\n+          </executions>\n         </plugin>\n \n         <plugin>\n@@ -1409,6 +1488,32 @@\n             </filesets>\n           </configuration>\n         </plugin>\n+\n+        <plugin>\n+          <groupId>org.codehaus.mojo</groupId>\n+          <artifactId>build-helper-maven-plugin</artifactId>\n+          <version>3.0.0</version>\n+          <executions>\n+            <execution>\n+              <id>render-artifact-id</id>\n+              <goals>\n+                <goal>regex-properties</goal>\n+              </goals>\n+              <phase>prepare-package</phase>\n+              <configuration>\n+                <regexPropertySettings>\n+                  <regexPropertySetting>\n+                    <name>renderedArtifactId</name>\n+                    <regex>[^A-Za-z0-9]</regex>\n+                    <replacement>.</replacement>\n+                    <value>${project.artifactId}</value>\n+                    <failIfNoMatch>false</failIfNoMatch>\n+                  </regexPropertySetting>\n+                </regexPropertySettings>\n+              </configuration>\n+            </execution>\n+          </executions>\n+        </plugin>\n       </plugins>\n     </pluginManagement>\n \n@@ -1454,9 +1559,14 @@\n                   <version>[1.7,)</version>\n                 </requireJavaVersion>\n                 <requireMavenVersion>\n-                  <!-- Keep aligned with prerequisite section below. -->\n+                  <!-- Keep aligned with preqrequisite section below. -->\n                   <version>[3.2,)</version>\n                 </requireMavenVersion>\n+                <bannedDependencies>\n+                  <excludes>\n+                    <exclude>com.google.protobuf:protobuf-lite</exclude>\n+                  </excludes>\n+                </bannedDependencies>\n               </rules>\n             </configuration>\n           </execution>\n@@ -1469,6 +1579,14 @@\n           </dependency>\n         </dependencies>\n       </plugin>\n+      <plugin>\n+        <groupId>org.codehaus.mojo</groupId>\n+        <artifactId>build-helper-maven-plugin</artifactId>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-shade-plugin</artifactId>\n+      </plugin>\n     </plugins>\n   </build>\n \n@@ -1489,6 +1607,7 @@\n       </plugin>\n     </plugins>\n   </reporting>\n+\n   <prerequisites>\n     <!-- Keep aligned with requireMavenVersion section above. -->\n     <maven>3.2</maven>",
                "changes": 161
            },
            {
                "status": "modified",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/pom.xml",
                "deletions": 5,
                "sha": "f441e3d3217bdee7f2aafb3768ed64ddba232497",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/pom.xml",
                "patch": "@@ -42,7 +42,7 @@\n \n   <profiles>\n     <profile>\n-      <id>local-runnable-on-service-tests</id>\n+      <id>local-validates-runner-tests</id>\n       <activation><activeByDefault>false</activeByDefault></activation>\n       <properties>\n         <skipIntegrationTests>false</skipIntegrationTests>\n@@ -130,7 +130,7 @@\n       <scope>test</scope>\n     </dependency>\n \n-    <!-- Depend on test jar to scan for RunnableOnService tests -->\n+    <!-- Depend on test jar to scan for ValidatesRunner tests -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-core</artifactId>\n@@ -200,20 +200,21 @@\n         </configuration>\n         <executions>\n           <execution>\n-            <id>runnable-on-service-tests</id>\n+            <id>validates-runner-tests</id>\n             <phase>integration-test</phase>\n             <goals>\n               <goal>test</goal>\n             </goals>\n             <configuration>\n-              <groups>org.apache.beam.sdk.testing.RunnableOnService</groups>\n+              <groups>org.apache.beam.sdk.testing.ValidatesRunner</groups>\n               <excludedGroups>\n                 org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders,\n                 org.apache.beam.sdk.testing.UsesStatefulParDo,\n                 org.apache.beam.sdk.testing.UsesTimersInParDo,\n                 org.apache.beam.sdk.testing.UsesSplittableParDo,\n                 org.apache.beam.sdk.testing.UsesAttemptedMetrics,\n-                org.apache.beam.sdk.testing.UsesCommittedMetrics\n+                org.apache.beam.sdk.testing.UsesCommittedMetrics,\n+                org.apache.beam.sdk.testing.UsesTestStream\n               </excludedGroups>\n               <parallel>none</parallel>\n               <failIfNoTests>true</failIfNoTests>\n@@ -228,6 +229,7 @@\n                 </beamTestPipelineOptions>\n               </systemPropertyVariables>\n               <skipTests>${skipIntegrationTests}</skipTests>\n+              <threadCount>4</threadCount>\n             </configuration>\n           </execution>\n         </executions>",
                "changes": 12
            },
            {
                "status": "modified",
                "additions": 47,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "deletions": 34,
                "sha": "1c845c6c628b00a5378b5de8ceecf5b8f391c6f3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "patch": "@@ -22,7 +22,7 @@\n import com.datatorrent.api.DAG;\n import com.datatorrent.api.StreamingApplication;\n import com.google.common.base.Throwables;\n-import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableList;\n import java.io.File;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -31,7 +31,6 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n import java.util.Properties;\n import java.util.concurrent.atomic.AtomicReference;\n import org.apache.apex.api.EmbeddedAppLauncher;\n@@ -40,6 +39,7 @@\n import org.apache.apex.api.Launcher.LaunchMode;\n import org.apache.beam.runners.apex.translation.ApexPipelineTranslator;\n import org.apache.beam.runners.core.construction.PTransformMatchers;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.PrimitiveCreate;\n import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n import org.apache.beam.sdk.Pipeline;\n@@ -48,9 +48,9 @@\n import org.apache.beam.sdk.coders.ListCoder;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsValidator;\n-import org.apache.beam.sdk.runners.PTransformMatcher;\n-import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.runners.PTransformOverride;\n import org.apache.beam.sdk.runners.PipelineRunner;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.Combine.GloballyAsSingletonView;\n import org.apache.beam.sdk.transforms.Create;\n@@ -69,8 +69,6 @@\n  * A {@link PipelineRunner} that translates the\n  * pipeline to an Apex DAG and executes it on an Apex cluster.\n  *\n- * <p>Currently execution is always in embedded mode,\n- * launch on Hadoop cluster will be added in subsequent iteration.\n  */\n @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n public class ApexRunner extends PipelineRunner<ApexRunnerResult> {\n@@ -95,27 +93,30 @@ public static ApexRunner fromOptions(PipelineOptions options) {\n     return new ApexRunner(apexPipelineOptions);\n   }\n \n-  private Map<PTransformMatcher, PTransformOverrideFactory> getOverrides() {\n-    return ImmutableMap.<PTransformMatcher, PTransformOverrideFactory>builder()\n-        .put(PTransformMatchers.classEqualTo(Create.Values.class), new PrimitiveCreate.Factory())\n-        .put(\n-            PTransformMatchers.classEqualTo(View.AsSingleton.class),\n-            new StreamingViewAsSingleton.Factory())\n-        .put(\n-            PTransformMatchers.classEqualTo(View.AsIterable.class),\n-            new StreamingViewAsIterable.Factory())\n-        .put(\n-            PTransformMatchers.classEqualTo(Combine.GloballyAsSingletonView.class),\n-            new StreamingCombineGloballyAsSingletonView.Factory())\n+  private List<PTransformOverride> getOverrides() {\n+    return ImmutableList.<PTransformOverride>builder()\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(Create.Values.class),\n+                new PrimitiveCreate.Factory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(View.AsSingleton.class),\n+                new StreamingViewAsSingleton.Factory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(View.AsIterable.class),\n+                new StreamingViewAsIterable.Factory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(Combine.GloballyAsSingletonView.class),\n+                new StreamingCombineGloballyAsSingletonView.Factory()))\n         .build();\n   }\n \n   @Override\n   public ApexRunnerResult run(final Pipeline pipeline) {\n-    for (Map.Entry<PTransformMatcher, PTransformOverrideFactory> override :\n-        getOverrides().entrySet()) {\n-      pipeline.replace(override.getKey(), override.getValue());\n-    }\n+    pipeline.replaceAll(getOverrides());\n \n     final ApexPipelineTranslator translator = new ApexPipelineTranslator(options);\n     final AtomicReference<DAG> apexDAG = new AtomicReference<>();\n@@ -241,7 +242,7 @@ private StreamingCombineGloballyAsSingletonView(\n           .apply(Combine.globally(transform.getCombineFn())\n               .withoutDefaults().withFanout(transform.getFanout()));\n \n-      PCollectionView<OutputT> view = PCollectionViews.singletonView(combined.getPipeline(),\n+      PCollectionView<OutputT> view = PCollectionViews.singletonView(combined,\n           combined.getWindowingStrategy(), transform.getInsertDefault(),\n           transform.getInsertDefault() ? transform.getCombineFn().defaultValue() : null,\n               combined.getCoder());\n@@ -259,9 +260,15 @@ protected String getKindString() {\n             PCollection<InputT>, PCollectionView<OutputT>,\n             Combine.GloballyAsSingletonView<InputT, OutputT>> {\n       @Override\n-      public PTransform<PCollection<InputT>, PCollectionView<OutputT>> getReplacementTransform(\n-          GloballyAsSingletonView<InputT, OutputT> transform) {\n-        return new StreamingCombineGloballyAsSingletonView<>(transform);\n+      public PTransformReplacement<PCollection<InputT>, PCollectionView<OutputT>>\n+          getReplacementTransform(\n+              AppliedPTransform<\n+                      PCollection<InputT>, PCollectionView<OutputT>,\n+                      GloballyAsSingletonView<InputT, OutputT>>\n+                  transform) {\n+        return PTransformReplacement.of(\n+            PTransformReplacements.getSingletonMainInput(transform),\n+            new StreamingCombineGloballyAsSingletonView<>(transform.getTransform()));\n       }\n     }\n   }\n@@ -322,9 +329,11 @@ public T identity() {\n         extends SingleInputOutputOverrideFactory<\n             PCollection<T>, PCollectionView<T>, View.AsSingleton<T>> {\n       @Override\n-      public PTransform<PCollection<T>, PCollectionView<T>> getReplacementTransform(\n-          AsSingleton<T> transform) {\n-        return new StreamingViewAsSingleton<>(transform);\n+      public PTransformReplacement<PCollection<T>, PCollectionView<T>> getReplacementTransform(\n+          AppliedPTransform<PCollection<T>, PCollectionView<T>, AsSingleton<T>> transform) {\n+        return PTransformReplacement.of(\n+            PTransformReplacements.getSingletonMainInput(transform),\n+            new StreamingViewAsSingleton<>(transform.getTransform()));\n       }\n     }\n   }\n@@ -337,8 +346,8 @@ private StreamingViewAsIterable() {}\n \n     @Override\n     public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\n-      PCollectionView<Iterable<T>> view = PCollectionViews.iterableView(input.getPipeline(),\n-          input.getWindowingStrategy(), input.getCoder());\n+      PCollectionView<Iterable<T>> view =\n+          PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n \n       return input.apply(Combine.globally(new Concatenate<T>()).withoutDefaults())\n           .apply(CreateApexPCollectionView.<T, Iterable<T>> of(view));\n@@ -353,9 +362,13 @@ protected String getKindString() {\n         extends SingleInputOutputOverrideFactory<\n             PCollection<T>, PCollectionView<Iterable<T>>, View.AsIterable<T>> {\n       @Override\n-      public PTransform<PCollection<T>, PCollectionView<Iterable<T>>> getReplacementTransform(\n-          AsIterable<T> transform) {\n-        return new StreamingViewAsIterable<>();\n+      public PTransformReplacement<PCollection<T>, PCollectionView<Iterable<T>>>\n+          getReplacementTransform(\n+              AppliedPTransform<PCollection<T>, PCollectionView<Iterable<T>>, AsIterable<T>>\n+                  transform) {\n+        return PTransformReplacement.of(\n+            PTransformReplacements.getSingletonMainInput(transform),\n+            new StreamingViewAsIterable<T>());\n       }\n     }\n   }",
                "changes": 81
            },
            {
                "status": "modified",
                "additions": 55,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "deletions": 56,
                "sha": "b84144cd251ccdc3fb6a7688fd001c51168561a5",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "patch": "@@ -173,56 +173,57 @@ public void shutdown(ShutdownMode arg0) throws LauncherException {\n    * @throws IOException when dependency information cannot be read\n    */\n   public static List<File> getYarnDeployDependencies() throws IOException {\n-    InputStream dependencyTree = ApexRunner.class.getResourceAsStream(\"dependency-tree\");\n-    BufferedReader br = new BufferedReader(new InputStreamReader(dependencyTree));\n-    String line = null;\n-    List<String> excludes = new ArrayList<>();\n-    int excludeLevel = Integer.MAX_VALUE;\n-    while ((line = br.readLine()) != null) {\n-      for (int i = 0; i < line.length(); i++) {\n-        char c = line.charAt(i);\n-        if (Character.isLetter(c)) {\n-          if (i > excludeLevel) {\n-            excludes.add(line.substring(i));\n-          } else {\n-            if (line.substring(i).startsWith(\"org.apache.hadoop\")) {\n-              excludeLevel = i;\n-              excludes.add(line.substring(i));\n-            } else {\n-              excludeLevel = Integer.MAX_VALUE;\n+    try (InputStream dependencyTree = ApexRunner.class.getResourceAsStream(\"dependency-tree\")) {\n+      try (BufferedReader br = new BufferedReader(new InputStreamReader(dependencyTree))) {\n+        String line;\n+        List<String> excludes = new ArrayList<>();\n+        int excludeLevel = Integer.MAX_VALUE;\n+        while ((line = br.readLine()) != null) {\n+          for (int i = 0; i < line.length(); i++) {\n+            char c = line.charAt(i);\n+            if (Character.isLetter(c)) {\n+              if (i > excludeLevel) {\n+                excludes.add(line.substring(i));\n+              } else {\n+                if (line.substring(i).startsWith(\"org.apache.hadoop\")) {\n+                  excludeLevel = i;\n+                  excludes.add(line.substring(i));\n+                } else {\n+                  excludeLevel = Integer.MAX_VALUE;\n+                }\n+              }\n+              break;\n             }\n           }\n-          break;\n         }\n-      }\n-    }\n-    br.close();\n-\n-    Set<String> excludeJarFileNames = Sets.newHashSet();\n-    for (String exclude : excludes) {\n-      String[] mvnc = exclude.split(\":\");\n-      String fileName = mvnc[1] + \"-\";\n-      if (mvnc.length == 6) {\n-        fileName += mvnc[4] + \"-\" + mvnc[3]; // with classifier\n-      } else {\n-        fileName += mvnc[3];\n-      }\n-      fileName += \".jar\";\n-      excludeJarFileNames.add(fileName);\n-    }\n \n-    ClassLoader classLoader = ApexYarnLauncher.class.getClassLoader();\n-    URL[] urls = ((URLClassLoader) classLoader).getURLs();\n-    List<File> dependencyJars = new ArrayList<>();\n-    for (int i = 0; i < urls.length; i++) {\n-      File f = new File(urls[i].getFile());\n-      // dependencies can also be directories in the build reactor,\n-      // the Apex client will automatically create jar files for those.\n-      if (f.exists() && !excludeJarFileNames.contains(f.getName())) {\n-          dependencyJars.add(f);\n+        Set<String> excludeJarFileNames = Sets.newHashSet();\n+        for (String exclude : excludes) {\n+          String[] mvnc = exclude.split(\":\");\n+          String fileName = mvnc[1] + \"-\";\n+          if (mvnc.length == 6) {\n+            fileName += mvnc[4] + \"-\" + mvnc[3]; // with classifier\n+          } else {\n+            fileName += mvnc[3];\n+          }\n+          fileName += \".jar\";\n+          excludeJarFileNames.add(fileName);\n+        }\n+\n+        ClassLoader classLoader = ApexYarnLauncher.class.getClassLoader();\n+        URL[] urls = ((URLClassLoader) classLoader).getURLs();\n+        List<File> dependencyJars = new ArrayList<>();\n+        for (int i = 0; i < urls.length; i++) {\n+          File f = new File(urls[i].getFile());\n+          // dependencies can also be directories in the build reactor,\n+          // the Apex client will automatically create jar files for those.\n+          if (f.exists() && !excludeJarFileNames.contains(f.getName())) {\n+            dependencyJars.add(f);\n+          }\n+        }\n+        return dependencyJars;\n       }\n     }\n-    return dependencyJars;\n   }\n \n   /**\n@@ -238,17 +239,17 @@ public static void createJar(File dir, File jarFile) throws IOException {\n       throw new RuntimeException(\"Failed to remove \" + jarFile);\n     }\n     URI uri = URI.create(\"jar:\" + jarFile.toURI());\n-    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env);) {\n+    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env)) {\n \n       File manifestFile = new File(dir, JarFile.MANIFEST_NAME);\n       Files.createDirectory(zipfs.getPath(\"META-INF\"));\n-      final OutputStream out = Files.newOutputStream(zipfs.getPath(JarFile.MANIFEST_NAME));\n-      if (!manifestFile.exists()) {\n-        new Manifest().write(out);\n-      } else {\n-        FileUtils.copyFile(manifestFile, out);\n+      try (final OutputStream out = Files.newOutputStream(zipfs.getPath(JarFile.MANIFEST_NAME))) {\n+        if (!manifestFile.exists()) {\n+          new Manifest().write(out);\n+        } else {\n+          FileUtils.copyFile(manifestFile, out);\n+        }\n       }\n-      out.close();\n \n       final java.nio.file.Path root = dir.toPath();\n       Files.walkFileTree(root, new java.nio.file.SimpleFileVisitor<Path>() {\n@@ -274,9 +275,9 @@ public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs)\n         public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n           String name = relativePath + file.getFileName();\n           if (!JarFile.MANIFEST_NAME.equals(name)) {\n-            final OutputStream out = Files.newOutputStream(zipfs.getPath(name));\n-            FileUtils.copyFile(file.toFile(), out);\n-            out.close();\n+            try (final OutputStream out = Files.newOutputStream(zipfs.getPath(name))) {\n+              FileUtils.copyFile(file.toFile(), out);\n+            }\n           }\n           return super.visitFile(file, attrs);\n         }\n@@ -295,8 +296,6 @@ public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOEx\n \n   /**\n    * Transfer the properties to the configuration object.\n-   * @param conf\n-   * @param props\n    */\n   public static void addProperties(Configuration conf, Properties props) {\n     for (final String propertyName : props.stringPropertyNames()) {",
                "changes": 111
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "deletions": 3,
                "sha": "fdeefc70144783f08008ea296907f9172907923e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "patch": "@@ -24,8 +24,8 @@\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner.CreateApexPCollectionView;\n import org.apache.beam.runners.apex.translation.operators.ApexReadUnboundedInputOperator;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n import org.apache.beam.runners.core.construction.PrimitiveCreate;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n@@ -59,8 +59,7 @@\n \n   static {\n     // register TransformTranslators\n-    registerTransformTranslator(ParDo.Bound.class, new ParDoBoundTranslator());\n-    registerTransformTranslator(ParDo.BoundMulti.class, new ParDoBoundMultiTranslator<>());\n+    registerTransformTranslator(ParDo.MultiOutput.class, new ParDoTranslator<>());\n     registerTransformTranslator(Read.Unbounded.class, new ReadUnboundedTranslator());\n     registerTransformTranslator(Read.Bounded.class, new ReadBoundedTranslator());\n     registerTransformTranslator(GroupByKey.class, new GroupByKeyTranslator());",
                "changes": 5
            },
            {
                "status": "modified",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
                "deletions": 6,
                "sha": "440b8015e1f072fd001820cacc2672a386d155f2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
                "patch": "@@ -32,7 +32,8 @@\n import org.apache.beam.sdk.io.UnboundedSource;\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * {@link Flatten.PCollections} translation to Apex operator.\n@@ -63,15 +64,15 @@ public void translate(Flatten.PCollections<T> transform, TranslationContext cont\n     }\n   }\n \n-  private List<PCollection<T>> extractPCollections(List<TaggedPValue> inputs) {\n+  private List<PCollection<T>> extractPCollections(Map<TupleTag<?>, PValue> inputs) {\n     List<PCollection<T>> collections = Lists.newArrayList();\n-    for (TaggedPValue pv : inputs) {\n+    for (PValue pv : inputs.values()) {\n       checkArgument(\n-          pv.getValue() instanceof PCollection,\n+          pv instanceof PCollection,\n           \"Non-PCollection provided as input to flatten: %s of type %s\",\n-          pv.getValue(),\n+          pv,\n           pv.getClass().getSimpleName());\n-      collections.add((PCollection<T>) pv.getValue());\n+      collections.add((PCollection<T>) pv);\n     }\n     return collections;\n   }",
                "changes": 13
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
                "deletions": 2,
                "sha": "2e0bae70186a443505221da86f5a39efd51f7483",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
                "patch": "@@ -31,9 +31,9 @@\n \n   @Override\n   public void translate(GroupByKey<K, V> transform, TranslationContext context) {\n-    PCollection<KV<K, V>> input = (PCollection<KV<K, V>>) context.getInput();\n+    PCollection<KV<K, V>> input = context.getInput();\n     ApexGroupByKeyOperator<K, V> group = new ApexGroupByKeyOperator<>(context.getPipelineOptions(),\n-        input, context.<K>stateInternalsFactory()\n+        input, context.getStateBackend()\n         );\n     context.addOperator(group, group.output);\n     context.addStream(input, group.input);",
                "changes": 4
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "deletions": 95,
                "sha": "5195809bdbbf3e3ccf23f09273f629a251f06a40",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "patch": "@@ -1,95 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.beam.runners.apex.translation;\n-\n-import java.util.List;\n-import org.apache.beam.runners.apex.ApexRunner;\n-import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n-import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n-import org.apache.beam.sdk.util.WindowedValue.WindowedValueCoder;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.beam.sdk.values.TupleTagList;\n-\n-/** {@link ParDo.Bound} is translated to {link ApexParDoOperator} that wraps the {@link DoFn}. */\n-class ParDoBoundTranslator<InputT, OutputT>\n-    implements TransformTranslator<ParDo.Bound<InputT, OutputT>> {\n-  private static final long serialVersionUID = 1L;\n-\n-  @Override\n-  public void translate(ParDo.Bound<InputT, OutputT> transform, TranslationContext context) {\n-    DoFn<InputT, OutputT> doFn = transform.getFn();\n-    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n-\n-    if (signature.processElement().isSplittable()) {\n-      throw new UnsupportedOperationException(\n-          String.format(\n-              \"%s does not support splittable DoFn: %s\", ApexRunner.class.getSimpleName(), doFn));\n-    }\n-    if (signature.stateDeclarations().size() > 0) {\n-      throw new UnsupportedOperationException(\n-          String.format(\n-              \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-              DoFn.StateId.class.getSimpleName(),\n-              doFn.getClass().getName(),\n-              DoFn.class.getSimpleName(),\n-              ApexRunner.class.getSimpleName()));\n-    }\n-\n-    if (signature.timerDeclarations().size() > 0) {\n-      throw new UnsupportedOperationException(\n-          String.format(\n-              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n-              DoFn.TimerId.class.getSimpleName(),\n-              doFn.getClass().getName(),\n-              DoFn.class.getSimpleName(),\n-              ApexRunner.class.getSimpleName()));\n-    }\n-\n-    PCollection<OutputT> output = (PCollection<OutputT>) context.getOutput();\n-    PCollection<InputT> input = (PCollection<InputT>) context.getInput();\n-    List<PCollectionView<?>> sideInputs = transform.getSideInputs();\n-    Coder<InputT> inputCoder = input.getCoder();\n-    WindowedValueCoder<InputT> wvInputCoder =\n-        FullWindowedValueCoder.of(\n-            inputCoder, input.getWindowingStrategy().getWindowFn().windowCoder());\n-\n-    ApexParDoOperator<InputT, OutputT> operator =\n-        new ApexParDoOperator<>(\n-            context.getPipelineOptions(),\n-            doFn,\n-            new TupleTag<OutputT>(),\n-            TupleTagList.empty().getAll() /*sideOutputTags*/,\n-            output.getWindowingStrategy(),\n-            sideInputs,\n-            wvInputCoder,\n-            context.<Void>stateInternalsFactory());\n-    context.addOperator(operator, operator.output);\n-    context.addStream(context.getInput(), operator.input);\n-    if (!sideInputs.isEmpty()) {\n-      ParDoBoundMultiTranslator.addSideInputs(operator, sideInputs, context);\n-    }\n-  }\n-}",
                "changes": 95
            },
            {
                "status": "renamed",
                "additions": 21,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
                "previous_filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundMultiTranslator.java",
                "deletions": 21,
                "sha": "2e3d902e01067187c6215418ff77f4f295951ab3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Map.Entry;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n import org.apache.beam.sdk.coders.Coder;\n@@ -38,21 +39,21 @@\n import org.apache.beam.sdk.util.WindowedValue.WindowedValueCoder;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * {@link ParDo.BoundMulti} is translated to {@link ApexParDoOperator} that wraps the {@link DoFn}.\n+ * {@link ParDo.MultiOutput} is translated to {@link ApexParDoOperator} that wraps the {@link DoFn}.\n  */\n-class ParDoBoundMultiTranslator<InputT, OutputT>\n-    implements TransformTranslator<ParDo.BoundMulti<InputT, OutputT>> {\n+class ParDoTranslator<InputT, OutputT>\n+    implements TransformTranslator<ParDo.MultiOutput<InputT, OutputT>> {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LoggerFactory.getLogger(ParDoBoundMultiTranslator.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(ParDoTranslator.class);\n \n   @Override\n-  public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationContext context) {\n+  public void translate(ParDo.MultiOutput<InputT, OutputT> transform, TranslationContext context) {\n     DoFn<InputT, OutputT> doFn = transform.getFn();\n     DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n \n@@ -81,42 +82,41 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n               ApexRunner.class.getSimpleName()));\n     }\n \n-    List<TaggedPValue> outputs = context.getOutputs();\n+    Map<TupleTag<?>, PValue> outputs = context.getOutputs();\n     PCollection<InputT> input = (PCollection<InputT>) context.getInput();\n     List<PCollectionView<?>> sideInputs = transform.getSideInputs();\n     Coder<InputT> inputCoder = input.getCoder();\n     WindowedValueCoder<InputT> wvInputCoder =\n         FullWindowedValueCoder.of(\n             inputCoder, input.getWindowingStrategy().getWindowFn().windowCoder());\n \n-    ApexParDoOperator<InputT, OutputT> operator =\n-        new ApexParDoOperator<>(\n+    ApexParDoOperator<InputT, OutputT> operator = new ApexParDoOperator<>(\n             context.getPipelineOptions(),\n             doFn,\n             transform.getMainOutputTag(),\n-            transform.getSideOutputTags().getAll(),\n-            ((PCollection<InputT>) context.getInput()).getWindowingStrategy(),\n+            transform.getAdditionalOutputTags().getAll(),\n+            input.getWindowingStrategy(),\n             sideInputs,\n             wvInputCoder,\n-            context.<Void>stateInternalsFactory());\n+            context.getStateBackend());\n \n     Map<PCollection<?>, OutputPort<?>> ports = Maps.newHashMapWithExpectedSize(outputs.size());\n-    for (TaggedPValue output : outputs) {\n+    for (Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {\n       checkArgument(\n           output.getValue() instanceof PCollection,\n           \"%s %s outputs non-PCollection %s of type %s\",\n-          ParDo.BoundMulti.class.getSimpleName(),\n+          ParDo.MultiOutput.class.getSimpleName(),\n           context.getFullName(),\n           output.getValue(),\n           output.getValue().getClass().getSimpleName());\n       PCollection<?> pc = (PCollection<?>) output.getValue();\n-      if (output.getTag().equals(transform.getMainOutputTag())) {\n+      if (output.getKey().equals(transform.getMainOutputTag())) {\n         ports.put(pc, operator.output);\n       } else {\n         int portIndex = 0;\n-        for (TupleTag<?> tag : transform.getSideOutputTags().getAll()) {\n-          if (tag.equals(output.getTag())) {\n-            ports.put(pc, operator.sideOutputPorts[portIndex]);\n+        for (TupleTag<?> tag : transform.getAdditionalOutputTags().getAll()) {\n+          if (tag.equals(output.getKey())) {\n+            ports.put(pc, operator.additionalOutputPorts[portIndex]);\n             break;\n           }\n           portIndex++;\n@@ -126,15 +126,15 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n     context.addOperator(operator, ports);\n     context.addStream(context.getInput(), operator.input);\n     if (!sideInputs.isEmpty()) {\n-      addSideInputs(operator, sideInputs, context);\n+      addSideInputs(operator.sideInput1, sideInputs, context);\n     }\n   }\n \n   static void addSideInputs(\n-      ApexParDoOperator<?, ?> operator,\n+      Operator.InputPort<?> sideInputPort,\n       List<PCollectionView<?>> sideInputs,\n       TranslationContext context) {\n-    Operator.InputPort<?>[] sideInputPorts = {operator.sideInput1};\n+    Operator.InputPort<?>[] sideInputPorts = {sideInputPort};\n     if (sideInputs.size() > sideInputPorts.length) {\n       PCollection<?> unionCollection = unionSideInputs(sideInputs, context);\n       context.addStream(unionCollection, sideInputPorts[0]);",
                "changes": 42
            },
            {
                "status": "modified",
                "additions": 11,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "deletions": 11,
                "sha": "c78028ec57203c8fc20ddfc4da949c423578b587",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "patch": "@@ -31,9 +31,9 @@\n import java.util.Map;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.translation.utils.ApexStateInternals;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.CoderAdapterStreamCodec;\n-import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n@@ -42,7 +42,7 @@\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.apache.commons.lang3.tuple.ImmutablePair;\n import org.apache.commons.lang3.tuple.Pair;\n \n@@ -85,20 +85,20 @@ public String getFullName() {\n     return getCurrentTransform().getFullName();\n   }\n \n-  public List<TaggedPValue> getInputs() {\n+  public Map<TupleTag<?>, PValue> getInputs() {\n     return getCurrentTransform().getInputs();\n   }\n \n-  public PValue getInput() {\n-    return Iterables.getOnlyElement(getCurrentTransform().getInputs()).getValue();\n+  public <InputT extends PValue> InputT getInput() {\n+    return (InputT) Iterables.getOnlyElement(getCurrentTransform().getInputs().values());\n   }\n \n-  public List<TaggedPValue> getOutputs() {\n+  public Map<TupleTag<?>, PValue> getOutputs() {\n     return getCurrentTransform().getOutputs();\n   }\n \n-  public PValue getOutput() {\n-    return Iterables.getOnlyElement(getCurrentTransform().getOutputs()).getValue();\n+  public <OutputT extends PValue> OutputT getOutput() {\n+    return (OutputT) Iterables.getOnlyElement(getCurrentTransform().getOutputs().values());\n   }\n \n   private AppliedPTransform<?, ?, ?> getCurrentTransform() {\n@@ -192,10 +192,10 @@ public void populateDAG(DAG dag) {\n   }\n \n   /**\n-   * Return the {@link StateInternalsFactory} for the pipeline translation.\n+   * Return the state backend for the pipeline translation.\n    * @return\n    */\n-  public <K> StateInternalsFactory<K> stateInternalsFactory() {\n-    return new ApexStateInternals.ApexStateInternalsFactory();\n+  public ApexStateBackend getStateBackend() {\n+    return new ApexStateInternals.ApexStateBackend();\n   }\n }",
                "changes": 22
            },
            {
                "status": "modified",
                "additions": 16,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
                "deletions": 42,
                "sha": "f34f9eecd9341ca2fc4db87c88a65aed667ec908",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
                "patch": "@@ -18,61 +18,35 @@\n \n package org.apache.beam.runners.apex.translation;\n \n-import java.util.Collections;\n-import org.apache.beam.runners.apex.ApexPipelineOptions;\n-import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n-import org.apache.beam.runners.core.AssignWindowsDoFn;\n-import org.apache.beam.runners.core.DoFnAdapters;\n-import org.apache.beam.runners.core.OldDoFn;\n-import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.runners.apex.translation.operators.ApexProcessFnOperator;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.beam.sdk.values.TupleTagList;\n \n /**\n- * {@link Window.Bound} is translated to {link ApexParDoOperator} that wraps an {@link\n- * AssignWindowsDoFn}.\n+ * {@link Window} is translated to {@link ApexProcessFnOperator#assignWindows}.\n  */\n class WindowAssignTranslator<T> implements TransformTranslator<Window.Assign<T>> {\n   private static final long serialVersionUID = 1L;\n \n   @Override\n   public void translate(Window.Assign<T> transform, TranslationContext context) {\n-    PCollection<T> output = (PCollection<T>) context.getOutput();\n-    PCollection<T> input = (PCollection<T>) context.getInput();\n-    @SuppressWarnings(\"unchecked\")\n-    WindowingStrategy<T, BoundedWindow> windowingStrategy =\n-        (WindowingStrategy<T, BoundedWindow>) output.getWindowingStrategy();\n+    PCollection<T> output = context.getOutput();\n+    PCollection<T> input = context.getInput();\n \n-    OldDoFn<T, T> fn =\n-        (transform.getWindowFn() == null)\n-            ? DoFnAdapters.toOldDoFn(new IdentityFn<T>())\n-            : new AssignWindowsDoFn<>(transform.getWindowFn());\n+   if (transform.getWindowFn() == null) {\n+     // no work to do\n+     context.addAlias(output, input);\n+   } else {\n+      @SuppressWarnings(\"unchecked\")\n+      WindowFn<T, BoundedWindow> windowFn = (WindowFn<T, BoundedWindow>) transform.getWindowFn();\n+      ApexProcessFnOperator<T> operator = ApexProcessFnOperator.assignWindows(windowFn,\n+          context.getPipelineOptions());\n+      context.addOperator(operator, operator.outputPort);\n+      context.addStream(context.getInput(), operator.inputPort);\n+   }\n \n-    ApexParDoOperator<T, T> operator =\n-        new ApexParDoOperator<T, T>(\n-            context.getPipelineOptions().as(ApexPipelineOptions.class),\n-            fn,\n-            new TupleTag<T>(),\n-            TupleTagList.empty().getAll(),\n-            windowingStrategy,\n-            Collections.<PCollectionView<?>>emptyList(),\n-            WindowedValue.getFullCoder(\n-                input.getCoder(), windowingStrategy.getWindowFn().windowCoder()),\n-            context.<Void>stateInternalsFactory());\n-    context.addOperator(operator, operator.output);\n-    context.addStream(context.getInput(), operator.input);\n   }\n \n-  private static class IdentityFn<T> extends DoFn<T, T> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      c.output(c.element());\n-    }\n-  }\n }",
                "changes": 58
            },
            {
                "status": "modified",
                "additions": 81,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "deletions": 192,
                "sha": "7d17ac660e477f77782c6284e029ca8d79d2a46f",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "patch": "@@ -25,12 +25,12 @@\n import com.datatorrent.api.Operator;\n import com.datatorrent.api.StreamCodec;\n import com.datatorrent.api.annotation.OutputPortFieldAnnotation;\n+import com.datatorrent.netlet.util.Slice;\n import com.esotericsoftware.kryo.serializers.FieldSerializer.Bind;\n import com.esotericsoftware.kryo.serializers.JavaSerializer;\n import com.google.common.base.Throwables;\n import com.google.common.collect.HashMultimap;\n import com.google.common.collect.Multimap;\n-import java.nio.ByteBuffer;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n@@ -39,34 +39,32 @@\n import java.util.Map;\n import java.util.Set;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.SerializablePipelineOptions;\n-import org.apache.beam.runners.core.GroupAlsoByWindowViaWindowSetDoFn;\n-import org.apache.beam.runners.core.KeyedWorkItem;\n-import org.apache.beam.runners.core.KeyedWorkItems;\n-import org.apache.beam.runners.core.OldDoFn;\n+import org.apache.beam.runners.core.OutputWindowedValue;\n+import org.apache.beam.runners.core.ReduceFnRunner;\n import org.apache.beam.runners.core.StateInternals;\n import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.runners.core.TimerInternals;\n-import org.apache.beam.runners.core.WindowingInternals;\n+import org.apache.beam.runners.core.construction.Triggers;\n+import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n+import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.util.CoderUtils;\n+import org.apache.beam.sdk.util.NullSideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Instant;\n import org.slf4j.Logger;\n@@ -95,11 +93,8 @@\n   private final SerializablePipelineOptions serializedOptions;\n   @Bind(JavaSerializer.class)\n   private final StateInternalsFactory<K> stateInternalsFactory;\n-  private Map<ByteBuffer, StateInternals<K>> perKeyStateInternals = new HashMap<>();\n-  private Map<ByteBuffer, Set<TimerInternals.TimerData>> activeTimers = new HashMap<>();\n+  private Map<Slice, Set<TimerInternals.TimerData>> activeTimers = new HashMap<>();\n \n-  private transient ProcessContext context;\n-  private transient OldDoFn<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> fn;\n   private transient ApexTimerInternals timerInternals = new ApexTimerInternals();\n   private Instant inputWatermark = BoundedWindow.TIMESTAMP_MIN_VALUE;\n \n@@ -135,13 +130,13 @@ public void process(ApexStreamTuple<WindowedValue<KV<K, V>>> t) {\n \n   @SuppressWarnings(\"unchecked\")\n   public ApexGroupByKeyOperator(ApexPipelineOptions pipelineOptions, PCollection<KV<K, V>> input,\n-      StateInternalsFactory<K> stateInternalsFactory) {\n+      ApexStateBackend stateBackend) {\n     checkNotNull(pipelineOptions);\n     this.serializedOptions = new SerializablePipelineOptions(pipelineOptions);\n     this.windowingStrategy = (WindowingStrategy<V, BoundedWindow>) input.getWindowingStrategy();\n     this.keyCoder = ((KvCoder<K, V>) input.getCoder()).getKeyCoder();\n     this.valueCoder = ((KvCoder<K, V>) input.getCoder()).getValueCoder();\n-    this.stateInternalsFactory = stateInternalsFactory;\n+    this.stateInternalsFactory = stateBackend.newStateInternalsFactory(keyCoder);\n   }\n \n   @SuppressWarnings(\"unused\") // for Kryo\n@@ -161,34 +156,71 @@ public void endWindow() {\n   @Override\n   public void setup(OperatorContext context) {\n     this.traceTuples = ApexStreamTuple.Logging.isDebugEnabled(serializedOptions.get(), this);\n-    StateInternalsFactory<K> stateInternalsFactory = new GroupByKeyStateInternalsFactory();\n-    this.fn = GroupAlsoByWindowViaWindowSetDoFn.create(this.windowingStrategy,\n-        stateInternalsFactory, SystemReduceFn.<K, V, BoundedWindow>buffering(this.valueCoder));\n-    this.context = new ProcessContext(fn, this.timerInternals);\n   }\n \n   @Override\n   public void teardown() {\n   }\n \n+\n+  private ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> newReduceFnRunner(K key) {\n+    return new ReduceFnRunner<>(\n+        key,\n+        windowingStrategy,\n+        ExecutableTriggerStateMachine.create(\n+            TriggerStateMachines.stateMachineForTrigger(\n+                Triggers.toProto(windowingStrategy.getTrigger()))),\n+        stateInternalsFactory.stateInternalsForKey(key),\n+        timerInternals,\n+        new OutputWindowedValue<KV<K, Iterable<V>>>() {\n+          @Override\n+          public void outputWindowedValue(\n+              KV<K, Iterable<V>> output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            if (traceTuples) {\n+              LOG.debug(\"\\nemitting {} timestamp {}\\n\", output, timestamp);\n+            }\n+            ApexGroupByKeyOperator.this.output.emit(\n+                ApexStreamTuple.DataTuple.of(WindowedValue.of(output, timestamp, windows, pane)));\n+          }\n+\n+          @Override\n+          public <AdditionalOutputT> void outputWindowedValue(\n+              TupleTag<AdditionalOutputT> tag,\n+              AdditionalOutputT output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            throw new UnsupportedOperationException(\n+                \"GroupAlsoByWindow should not use side outputs\");\n+          }\n+        },\n+        NullSideInputReader.empty(),\n+        null,\n+        SystemReduceFn.<K, V, BoundedWindow>buffering(this.valueCoder),\n+        serializedOptions.get());\n+  }\n+\n   /**\n    * Returns the list of timers that are ready to fire. These are the timers\n    * that are registered to be triggered at a time before the current watermark.\n    * We keep these timers in a Set, so that they are deduplicated, as the same\n    * timer can be registered multiple times.\n    */\n-  private Multimap<ByteBuffer, TimerInternals.TimerData> getTimersReadyToProcess(\n+  private Multimap<Slice, TimerInternals.TimerData> getTimersReadyToProcess(\n       long currentWatermark) {\n \n     // we keep the timers to return in a different list and launch them later\n     // because we cannot prevent a trigger from registering another trigger,\n     // which would lead to concurrent modification exception.\n-    Multimap<ByteBuffer, TimerInternals.TimerData> toFire = HashMultimap.create();\n+    Multimap<Slice, TimerInternals.TimerData> toFire = HashMultimap.create();\n \n-    Iterator<Map.Entry<ByteBuffer, Set<TimerInternals.TimerData>>> it =\n+    Iterator<Map.Entry<Slice, Set<TimerInternals.TimerData>>> it =\n         activeTimers.entrySet().iterator();\n     while (it.hasNext()) {\n-      Map.Entry<ByteBuffer, Set<TimerInternals.TimerData>> keyWithTimers = it.next();\n+      Map.Entry<Slice, Set<TimerInternals.TimerData>> keyWithTimers = it.next();\n \n       Iterator<TimerInternals.TimerData> timerIt = keyWithTimers.getValue().iterator();\n       while (timerIt.hasNext()) {\n@@ -212,34 +244,21 @@ private void processElement(WindowedValue<KV<K, V>> windowedValue) throws Except\n         windowedValue.getTimestamp(),\n         windowedValue.getWindows(),\n         windowedValue.getPane());\n-\n-    KeyedWorkItem<K, V> kwi = KeyedWorkItems.elementsWorkItem(\n-            kv.getKey(),\n-            Collections.singletonList(updatedWindowedValue));\n-\n-    context.setElement(kwi, getStateInternalsForKey(kwi.key()));\n-    fn.processElement(context);\n+    timerInternals.setKey(kv.getKey());\n+    ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> reduceFnRunner =\n+        newReduceFnRunner(kv.getKey());\n+    reduceFnRunner.processElements(Collections.singletonList(updatedWindowedValue));\n+    reduceFnRunner.persist();\n   }\n \n   private StateInternals<K> getStateInternalsForKey(K key) {\n-    final ByteBuffer keyBytes;\n-    try {\n-      keyBytes = ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, key));\n-    } catch (CoderException e) {\n-      throw new RuntimeException(e);\n-    }\n-    StateInternals<K> stateInternals = perKeyStateInternals.get(keyBytes);\n-    if (stateInternals == null) {\n-      stateInternals = stateInternalsFactory.stateInternalsForKey(key);\n-      perKeyStateInternals.put(keyBytes, stateInternals);\n-    }\n-    return stateInternals;\n+    return stateInternalsFactory.stateInternalsForKey(key);\n   }\n \n   private void registerActiveTimer(K key, TimerInternals.TimerData timer) {\n-    final ByteBuffer keyBytes;\n+    final Slice keyBytes;\n     try {\n-      keyBytes = ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, key));\n+      keyBytes = new Slice(CoderUtils.encodeToByteArray(keyCoder, key));\n     } catch (CoderException e) {\n       throw new RuntimeException(e);\n     }\n@@ -252,9 +271,9 @@ private void registerActiveTimer(K key, TimerInternals.TimerData timer) {\n   }\n \n   private void unregisterActiveTimer(K key, TimerInternals.TimerData timer) {\n-    final ByteBuffer keyBytes;\n+    final Slice keyBytes;\n     try {\n-      keyBytes = ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, key));\n+      keyBytes = new Slice(CoderUtils.encodeToByteArray(keyCoder, key));\n     } catch (CoderException e) {\n       throw new RuntimeException(e);\n     }\n@@ -271,164 +290,34 @@ private void unregisterActiveTimer(K key, TimerInternals.TimerData timer) {\n \n   private void processWatermark(ApexStreamTuple.WatermarkTuple<?> mark) throws Exception {\n     this.inputWatermark = new Instant(mark.getTimestamp());\n-    Multimap<ByteBuffer, TimerInternals.TimerData> timers = getTimersReadyToProcess(\n+    Multimap<Slice, TimerInternals.TimerData> timers = getTimersReadyToProcess(\n         mark.getTimestamp());\n     if (!timers.isEmpty()) {\n-      for (ByteBuffer keyBytes : timers.keySet()) {\n-        K key = CoderUtils.decodeFromByteArray(keyCoder, keyBytes.array());\n-        KeyedWorkItem<K, V> kwi = KeyedWorkItems.<K, V>timersWorkItem(key, timers.get(keyBytes));\n-        context.setElement(kwi, getStateInternalsForKey(kwi.key()));\n-        fn.processElement(context);\n+      for (Slice keyBytes : timers.keySet()) {\n+        K key = CoderUtils.decodeFromByteArray(keyCoder, keyBytes.buffer);\n+        timerInternals.setKey(key);\n+        ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> reduceFnRunner = newReduceFnRunner(key);\n+        reduceFnRunner.onTimers(timers.get(keyBytes));\n+        reduceFnRunner.persist();\n       }\n     }\n   }\n \n-  private class ProcessContext extends GroupAlsoByWindowViaWindowSetDoFn<K, V, Iterable<V>, ?,\n-      KeyedWorkItem<K, V>>.ProcessContext {\n-\n-    private final ApexTimerInternals timerInternals;\n-    private StateInternals<K> stateInternals;\n-    private KeyedWorkItem<K, V> element;\n-\n-    public ProcessContext(OldDoFn<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> function,\n-                          ApexTimerInternals timerInternals) {\n-      function.super();\n-      this.timerInternals = checkNotNull(timerInternals);\n-    }\n-\n-    public void setElement(KeyedWorkItem<K, V> element, StateInternals<K> stateForKey) {\n-      this.element = element;\n-      this.stateInternals = stateForKey;\n-    }\n-\n-    @Override\n-    public KeyedWorkItem<K, V> element() {\n-      return this.element;\n-    }\n-\n-    @Override\n-    public Instant timestamp() {\n-      throw new UnsupportedOperationException(\n-          \"timestamp() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public PipelineOptions getPipelineOptions() {\n-      return serializedOptions.get();\n-    }\n-\n-    @Override\n-    public void output(KV<K, Iterable<V>> output) {\n-      throw new UnsupportedOperationException(\n-          \"output() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public void outputWithTimestamp(KV<K, Iterable<V>> output, Instant timestamp) {\n-      throw new UnsupportedOperationException(\n-          \"outputWithTimestamp() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public PaneInfo pane() {\n-      throw new UnsupportedOperationException(\n-          \"pane() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public BoundedWindow window() {\n-      throw new UnsupportedOperationException(\n-          \"window() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public WindowingInternals<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> windowingInternals() {\n-      return new WindowingInternals<KeyedWorkItem<K, V>, KV<K, Iterable<V>>>() {\n-\n-        @Override\n-        public StateInternals<K> stateInternals() {\n-          return stateInternals;\n-        }\n-\n-        @Override\n-        public void outputWindowedValue(\n-            KV<K, Iterable<V>> output,\n-            Instant timestamp,\n-            Collection<? extends BoundedWindow> windows,\n-            PaneInfo pane) {\n-          if (traceTuples) {\n-            LOG.debug(\"\\nemitting {} timestamp {}\\n\", output, timestamp);\n-          }\n-          ApexGroupByKeyOperator.this.output.emit(\n-              ApexStreamTuple.DataTuple.of(WindowedValue.of(output, timestamp, windows, pane)));\n-        }\n-\n-        @Override\n-        public <SideOutputT> void sideOutputWindowedValue(\n-            TupleTag<SideOutputT> tag,\n-            SideOutputT output,\n-            Instant timestamp,\n-            Collection<? extends BoundedWindow> windows,\n-            PaneInfo pane) {\n-          throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs\");\n-        }\n-\n-        @Override\n-        public TimerInternals timerInternals() {\n-          return timerInternals;\n-        }\n-\n-        @Override\n-        public Collection<? extends BoundedWindow> windows() {\n-          throw new UnsupportedOperationException(\"windows() is not available in Streaming mode.\");\n-        }\n-\n-        @Override\n-        public PaneInfo pane() {\n-          throw new UnsupportedOperationException(\"pane() is not available in Streaming mode.\");\n-        }\n-\n-        @Override\n-        public <T> T sideInput(PCollectionView<T> view, BoundedWindow mainInputWindow) {\n-          throw new RuntimeException(\"sideInput() is not available in Streaming mode.\");\n-        }\n-      };\n-    }\n-\n-    @Override\n-    public <T> T sideInput(PCollectionView<T> view) {\n-      throw new RuntimeException(\"sideInput() is not supported in Streaming mode.\");\n-    }\n-\n-    @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      // ignore the side output, this can happen when a user does not register\n-      // side outputs but then outputs using a freshly created TupleTag.\n-      throw new RuntimeException(\"sideOutput() is not available when grouping by window.\");\n-    }\n-\n-    @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      sideOutput(tag, output);\n-    }\n-\n-    @Override\n-    public <AggInputT, AggOutputT> Aggregator<AggInputT, AggOutputT> createAggregatorInternal(\n-        String name, Combine.CombineFn<AggInputT, ?, AggOutputT> combiner) {\n-      throw new UnsupportedOperationException();\n-    }\n-  }\n-\n   /**\n    * An implementation of Beam's {@link TimerInternals}.\n    *\n    */\n-  public class ApexTimerInternals implements TimerInternals {\n+  private class ApexTimerInternals implements TimerInternals {\n+    private K key;\n+\n+    public void setKey(K key) {\n+      this.key = key;\n+    }\n \n     @Deprecated\n     @Override\n     public void setTimer(TimerData timerData) {\n-      registerActiveTimer(context.element().key(), timerData);\n+      registerActiveTimer(key, timerData);\n     }\n \n     @Override\n@@ -439,7 +328,7 @@ public void deleteTimer(StateNamespace namespace, String timerId, TimeDomain tim\n     @Deprecated\n     @Override\n     public void deleteTimer(TimerData timerKey) {\n-      unregisterActiveTimer(context.element().key(), timerKey);\n+      unregisterActiveTimer(key, timerKey);\n     }\n \n     @Override",
                "changes": 273
            },
            {
                "status": "modified",
                "additions": 177,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "deletions": 61,
                "sha": "52d1d430fdd926c083227c3cb9631a68f6686490",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "patch": "@@ -25,41 +25,53 @@\n import com.datatorrent.common.util.BaseOperator;\n import com.esotericsoftware.kryo.serializers.FieldSerializer.Bind;\n import com.esotericsoftware.kryo.serializers.JavaSerializer;\n-import com.google.common.base.Throwables;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Maps;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.NoOpStepContext;\n import org.apache.beam.runners.apex.translation.utils.SerializablePipelineOptions;\n+import org.apache.beam.runners.apex.translation.utils.StateInternalsProxy;\n import org.apache.beam.runners.apex.translation.utils.ValueAndCoderKryoSerializable;\n import org.apache.beam.runners.core.AggregatorFactory;\n-import org.apache.beam.runners.core.DoFnAdapters;\n import org.apache.beam.runners.core.DoFnRunner;\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.ExecutionContext;\n-import org.apache.beam.runners.core.OldDoFn;\n+import org.apache.beam.runners.core.InMemoryTimerInternals;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.SideInputHandler;\n+import org.apache.beam.runners.core.SimplePushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.StateInternals;\n-import org.apache.beam.runners.core.StateInternalsFactory;\n+import org.apache.beam.runners.core.StateNamespace;\n+import org.apache.beam.runners.core.StatefulDoFnRunner;\n+import org.apache.beam.runners.core.TimerInternals;\n+import org.apache.beam.runners.core.TimerInternalsFactory;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.ListCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n+import org.apache.beam.sdk.transforms.reflect.DoFnInvokers;\n import org.apache.beam.sdk.util.NullSideInputReader;\n import org.apache.beam.sdk.util.SideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n+import org.joda.time.Instant;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -73,16 +85,22 @@\n   @Bind(JavaSerializer.class)\n   private final SerializablePipelineOptions pipelineOptions;\n   @Bind(JavaSerializer.class)\n-  private final OldDoFn<InputT, OutputT> doFn;\n+  private final DoFn<InputT, OutputT> doFn;\n   @Bind(JavaSerializer.class)\n   private final TupleTag<OutputT> mainOutputTag;\n   @Bind(JavaSerializer.class)\n-  private final List<TupleTag<?>> sideOutputTags;\n+  private final List<TupleTag<?>> additionalOutputTags;\n   @Bind(JavaSerializer.class)\n   private final WindowingStrategy<?, ?> windowingStrategy;\n   @Bind(JavaSerializer.class)\n   private final List<PCollectionView<?>> sideInputs;\n \n+  private StateInternalsProxy<?> currentKeyStateInternals;\n+  // TODO: if the operator gets restored to checkpointed state due to a failure,\n+  // the timer state is lost.\n+  private final transient CurrentKeyTimerInternals<Object> currentKeyTimerInternals =\n+      new CurrentKeyTimerInternals<>();\n+\n   private final StateInternals<Void> sideInputStateInternals;\n   private final ValueAndCoderKryoSerializable<List<WindowedValue<InputT>>> pushedBack;\n   private LongMin pushedBackWatermark = new LongMin();\n@@ -91,31 +109,32 @@\n \n   private transient PushbackSideInputDoFnRunner<InputT, OutputT> pushbackDoFnRunner;\n   private transient SideInputHandler sideInputHandler;\n-  private transient Map<TupleTag<?>, DefaultOutputPort<ApexStreamTuple<?>>> sideOutputPortMapping =\n-      Maps.newHashMapWithExpectedSize(5);\n+  private transient Map<TupleTag<?>, DefaultOutputPort<ApexStreamTuple<?>>>\n+      additionalOutputPortMapping = Maps.newHashMapWithExpectedSize(5);\n+  private transient DoFnInvoker<InputT, OutputT> doFnInvoker;\n \n-  @Deprecated\n   public ApexParDoOperator(\n       ApexPipelineOptions pipelineOptions,\n-      OldDoFn<InputT, OutputT> doFn,\n+      DoFn<InputT, OutputT> doFn,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       WindowingStrategy<?, ?> windowingStrategy,\n       List<PCollectionView<?>> sideInputs,\n       Coder<WindowedValue<InputT>> inputCoder,\n-      StateInternalsFactory<Void> stateInternalsFactory\n+      ApexStateBackend stateBackend\n       ) {\n     this.pipelineOptions = new SerializablePipelineOptions(pipelineOptions);\n     this.doFn = doFn;\n     this.mainOutputTag = mainOutputTag;\n-    this.sideOutputTags = sideOutputTags;\n+    this.additionalOutputTags = additionalOutputTags;\n     this.windowingStrategy = windowingStrategy;\n     this.sideInputs = sideInputs;\n-    this.sideInputStateInternals = stateInternalsFactory.stateInternalsForKey(null);\n+    this.sideInputStateInternals = new StateInternalsProxy<>(\n+        stateBackend.newStateInternalsFactory(VoidCoder.of()));\n \n-    if (sideOutputTags.size() > sideOutputPorts.length) {\n-      String msg = String.format(\"Too many side outputs (currently only supporting %s).\",\n-          sideOutputPorts.length);\n+    if (additionalOutputTags.size() > additionalOutputPorts.length) {\n+      String msg = String.format(\"Too many additional outputs (currently only supporting %s).\",\n+          additionalOutputPorts.length);\n       throw new UnsupportedOperationException(msg);\n     }\n \n@@ -125,33 +144,12 @@ public ApexParDoOperator(\n \n   }\n \n-  public ApexParDoOperator(\n-      ApexPipelineOptions pipelineOptions,\n-      DoFn<InputT, OutputT> doFn,\n-      TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n-      WindowingStrategy<?, ?> windowingStrategy,\n-      List<PCollectionView<?>> sideInputs,\n-      Coder<WindowedValue<InputT>> inputCoder,\n-      StateInternalsFactory<Void> stateInternalsFactory\n-      ) {\n-    this(\n-        pipelineOptions,\n-        DoFnAdapters.toOldDoFn(doFn),\n-        mainOutputTag,\n-        sideOutputTags,\n-        windowingStrategy,\n-        sideInputs,\n-        inputCoder,\n-        stateInternalsFactory);\n-  }\n-\n   @SuppressWarnings(\"unused\") // for Kryo\n   private ApexParDoOperator() {\n     this.pipelineOptions = null;\n     this.doFn = null;\n     this.mainOutputTag = null;\n-    this.sideOutputTags = null;\n+    this.additionalOutputTags = null;\n     this.windowingStrategy = null;\n     this.sideInputs = null;\n     this.pushedBack = null;\n@@ -221,29 +219,31 @@ public void process(ApexStreamTuple<WindowedValue<Iterable<?>>> t) {\n   public final transient DefaultOutputPort<ApexStreamTuple<?>> output = new DefaultOutputPort<>();\n \n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput1 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput1 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput2 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput2 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput3 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput3 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput4 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput4 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput5 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput5 =\n       new DefaultOutputPort<>();\n \n-  public final transient DefaultOutputPort<?>[] sideOutputPorts = {sideOutput1, sideOutput2,\n-      sideOutput3, sideOutput4, sideOutput5};\n+  public final transient DefaultOutputPort<?>[] additionalOutputPorts = {\n+    additionalOutput1, additionalOutput2, additionalOutput3, additionalOutput4, additionalOutput5\n+  };\n \n   @Override\n   public <T> void output(TupleTag<T> tag, WindowedValue<T> tuple) {\n-    DefaultOutputPort<ApexStreamTuple<?>> sideOutputPort = sideOutputPortMapping.get(tag);\n-    if (sideOutputPort != null) {\n-      sideOutputPort.emit(ApexStreamTuple.DataTuple.of(tuple));\n+    DefaultOutputPort<ApexStreamTuple<?>> additionalOutputPort =\n+        additionalOutputPortMapping.get(tag);\n+    if (additionalOutputPort != null) {\n+      additionalOutputPort.emit(ApexStreamTuple.DataTuple.of(tuple));\n     } else {\n       output.emit(ApexStreamTuple.DataTuple.of(tuple));\n     }\n@@ -255,6 +255,17 @@ public void process(ApexStreamTuple<WindowedValue<Iterable<?>>> t) {\n   private Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n     try {\n       pushbackDoFnRunner.startBundle();\n+      if (currentKeyStateInternals != null) {\n+        InputT value = elem.getValue();\n+        Object key;\n+        if (value instanceof KeyedWorkItem) {\n+          key = ((KeyedWorkItem) value).key();\n+        } else {\n+          key = ((KV) value).getKey();\n+        }\n+        ((StateInternalsProxy) currentKeyStateInternals).setKey(key);\n+        currentKeyTimerInternals.currentKey = key;\n+      }\n       Iterable<WindowedValue<InputT>> pushedBack = pushbackDoFnRunner\n           .processElementInReadyWindows(elem);\n       pushbackDoFnRunner.finishBundle();\n@@ -298,35 +309,74 @@ public void setup(OperatorContext context) {\n       sideInputReader = sideInputHandler;\n     }\n \n-    for (int i = 0; i < sideOutputTags.size(); i++) {\n+    for (int i = 0; i < additionalOutputTags.size(); i++) {\n       @SuppressWarnings(\"unchecked\")\n       DefaultOutputPort<ApexStreamTuple<?>> port = (DefaultOutputPort<ApexStreamTuple<?>>)\n-          sideOutputPorts[i];\n-      sideOutputPortMapping.put(sideOutputTags.get(i), port);\n+          additionalOutputPorts[i];\n+      additionalOutputPortMapping.put(additionalOutputTags.get(i), port);\n     }\n \n+    NoOpStepContext stepContext = new NoOpStepContext() {\n+\n+      @Override\n+      public StateInternals<?> stateInternals() {\n+        return currentKeyStateInternals;\n+      }\n+\n+      @Override\n+      public TimerInternals timerInternals() {\n+        return currentKeyTimerInternals;\n+      }\n+\n+    };\n     DoFnRunner<InputT, OutputT> doFnRunner = DoFnRunners.simpleRunner(\n         pipelineOptions.get(),\n         doFn,\n         sideInputReader,\n         this,\n         mainOutputTag,\n-        sideOutputTags,\n-        new NoOpStepContext(),\n+        additionalOutputTags,\n+        stepContext,\n         new NoOpAggregatorFactory(),\n         windowingStrategy\n         );\n \n-    pushbackDoFnRunner =\n-        PushbackSideInputDoFnRunner.create(doFnRunner, sideInputs, sideInputHandler);\n+    doFnInvoker = DoFnInvokers.invokerFor(doFn);\n+    doFnInvoker.invokeSetup();\n \n-    try {\n-      doFn.setup();\n-    } catch (Exception e) {\n-      Throwables.propagateIfPossible(e);\n-      throw new RuntimeException(e);\n+    if (this.currentKeyStateInternals != null) {\n+\n+      StatefulDoFnRunner.CleanupTimer cleanupTimer =\n+          new StatefulDoFnRunner.TimeInternalsCleanupTimer(\n+              stepContext.timerInternals(), windowingStrategy);\n+\n+      @SuppressWarnings({\"rawtypes\"})\n+      Coder windowCoder = windowingStrategy.getWindowFn().windowCoder();\n+\n+      @SuppressWarnings({\"unchecked\"})\n+      StatefulDoFnRunner.StateCleaner<?> stateCleaner =\n+          new StatefulDoFnRunner.StateInternalsStateCleaner<>(\n+              doFn, stepContext.stateInternals(), windowCoder);\n+\n+      doFnRunner = DoFnRunners.defaultStatefulDoFnRunner(\n+          doFn,\n+          doFnRunner,\n+          stepContext,\n+          new NoOpAggregatorFactory(),\n+          windowingStrategy,\n+          cleanupTimer,\n+          stateCleaner);\n     }\n \n+    pushbackDoFnRunner =\n+        SimplePushbackSideInputDoFnRunner.create(doFnRunner, sideInputs, sideInputHandler);\n+\n+  }\n+\n+  @Override\n+  public void teardown() {\n+    doFnInvoker.invokeTeardown();\n+    super.teardown();\n   }\n \n   @Override\n@@ -393,4 +443,70 @@ public void clear() {\n \n   }\n \n+  private class CurrentKeyTimerInternals<K> implements TimerInternals {\n+\n+    private TimerInternalsFactory<K> factory = new TimerInternalsFactory<K>() {\n+      @Override\n+      public TimerInternals timerInternalsForKey(K key) {\n+        InMemoryTimerInternals timerInternals = perKeyTimerInternals.get(key);\n+        if (timerInternals == null) {\n+          perKeyTimerInternals.put(key, timerInternals = new InMemoryTimerInternals());\n+        }\n+        return timerInternals;\n+      }\n+    };\n+\n+    // TODO: durable state store\n+    final Map<K, InMemoryTimerInternals> perKeyTimerInternals = new HashMap<>();\n+    private K currentKey;\n+\n+    @Override\n+    public void setTimer(StateNamespace namespace, String timerId, Instant target,\n+        TimeDomain timeDomain) {\n+      factory.timerInternalsForKey(currentKey).setTimer(\n+          namespace, timerId, target, timeDomain);\n+    }\n+\n+    @Override\n+    public void setTimer(TimerData timerData) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void deleteTimer(StateNamespace namespace, String timerId, TimeDomain timeDomain) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void deleteTimer(StateNamespace namespace, String timerId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void deleteTimer(TimerData timerKey) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Instant currentProcessingTime() {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Instant currentSynchronizedProcessingTime() {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Instant currentInputWatermarkTime() {\n+      return new Instant(currentInputWatermark);\n+    }\n+\n+    @Override\n+    public Instant currentOutputWatermarkTime() {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+  }\n+\n }",
                "changes": 238
            },
            {
                "status": "added",
                "additions": 184,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
                "deletions": 0,
                "sha": "835c9e0cdec8f55e5239794dcb79214ca581988b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
                "patch": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.apex.translation.operators;\n+\n+import com.datatorrent.api.DefaultInputPort;\n+import com.datatorrent.api.DefaultOutputPort;\n+import com.datatorrent.api.annotation.OutputPortFieldAnnotation;\n+import com.datatorrent.common.util.BaseOperator;\n+import com.esotericsoftware.kryo.serializers.FieldSerializer.Bind;\n+import com.esotericsoftware.kryo.serializers.JavaSerializer;\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Iterables;\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.Collections;\n+import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Apex operator for simple native map operations.\n+ */\n+public class ApexProcessFnOperator<InputT> extends BaseOperator {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ApexProcessFnOperator.class);\n+  private boolean traceTuples = false;\n+  @Bind(JavaSerializer.class)\n+  private final ApexOperatorFn<InputT> fn;\n+\n+  public ApexProcessFnOperator(ApexOperatorFn<InputT> fn, boolean traceTuples) {\n+    super();\n+    this.traceTuples = traceTuples;\n+    this.fn = fn;\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  private ApexProcessFnOperator() {\n+    // for Kryo\n+    fn = null;\n+  }\n+\n+  private final transient OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter =\n+      new OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>>() {\n+    @Override\n+    public void emit(ApexStreamTuple<? extends WindowedValue<?>> tuple) {\n+      if (traceTuples) {\n+        LOG.debug(\"\\nemitting {}\\n\", tuple);\n+      }\n+      outputPort.emit(tuple);\n+    }\n+  };\n+\n+  /**\n+   * Something that emits results.\n+   */\n+  public interface OutputEmitter<T> {\n+    void emit(T tuple);\n+  };\n+\n+  /**\n+   * The processing logic for this operator.\n+   */\n+  public interface ApexOperatorFn<InputT> extends Serializable {\n+    void process(ApexStreamTuple<WindowedValue<InputT>> input,\n+        OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter) throws Exception;\n+  }\n+\n+  /**\n+   * Convert {@link KV} into {@link KeyedWorkItem}s.\n+   */\n+  public static class ToKeyedWorkItems<K, V> implements ApexOperatorFn<KV<K, V>> {\n+    @Override\n+    public final void process(ApexStreamTuple<WindowedValue<KV<K, V>>> tuple,\n+        OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter) {\n+\n+      if (tuple instanceof ApexStreamTuple.WatermarkTuple) {\n+        outputEmitter.emit(tuple);\n+      } else {\n+        for (WindowedValue<KV<K, V>> in : tuple.getValue().explodeWindows()) {\n+          KeyedWorkItem<K, V> kwi = KeyedWorkItems.elementsWorkItem(in.getValue().getKey(),\n+              Collections.singletonList(in.withValue(in.getValue().getValue())));\n+          outputEmitter.emit(ApexStreamTuple.DataTuple.of(in.withValue(kwi)));\n+        }\n+      }\n+    }\n+  }\n+\n+  public static <T, W extends BoundedWindow> ApexProcessFnOperator<T> assignWindows(\n+      WindowFn<T, W> windowFn, ApexPipelineOptions options) {\n+    ApexOperatorFn<T> fn = new AssignWindows<T, W>(windowFn);\n+    return new ApexProcessFnOperator<T>(fn, options.isTupleTracingEnabled());\n+  }\n+\n+  /**\n+   * Function for implementing {@link org.apache.beam.sdk.transforms.windowing.Window.Assign}.\n+   */\n+  private static class AssignWindows<T, W extends BoundedWindow> implements ApexOperatorFn<T> {\n+    private final WindowFn<T, W> windowFn;\n+\n+    private AssignWindows(WindowFn<T, W> windowFn) {\n+      this.windowFn = windowFn;\n+    }\n+\n+    @Override\n+    public final void process(ApexStreamTuple<WindowedValue<T>> tuple,\n+        OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter) throws Exception {\n+      if (tuple instanceof ApexStreamTuple.WatermarkTuple) {\n+        outputEmitter.emit(tuple);\n+      } else {\n+        final WindowedValue<T> input = tuple.getValue();\n+        Collection<W> windows =\n+            (windowFn).assignWindows(\n+                (windowFn).new AssignContext() {\n+                    @Override\n+                    public T element() {\n+                      return input.getValue();\n+                    }\n+\n+                    @Override\n+                    public Instant timestamp() {\n+                      return input.getTimestamp();\n+                    }\n+\n+                    @Override\n+                    public BoundedWindow window() {\n+                      return Iterables.getOnlyElement(input.getWindows());\n+                    }\n+                  });\n+        for (W w: windows) {\n+          WindowedValue<T> wv = WindowedValue.of(input.getValue(), input.getTimestamp(),\n+              w, input.getPane());\n+          outputEmitter.emit(ApexStreamTuple.DataTuple.of(wv));\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Input port.\n+   */\n+  public final transient DefaultInputPort<ApexStreamTuple<WindowedValue<InputT>>> inputPort =\n+      new DefaultInputPort<ApexStreamTuple<WindowedValue<InputT>>>() {\n+    @Override\n+    public void process(ApexStreamTuple<WindowedValue<InputT>> tuple) {\n+      try {\n+        fn.process(tuple, outputEmitter);\n+      } catch (Exception e) {\n+        Throwables.throwIfUnchecked(e);\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  };\n+\n+  /**\n+   * Output port.\n+   */\n+  @OutputPortFieldAnnotation(optional = true)\n+  public final transient DefaultOutputPort<ApexStreamTuple<? extends WindowedValue<?>>>\n+    outputPort = new DefaultOutputPort<>();\n+\n+}",
                "changes": 184
            },
            {
                "status": "modified",
                "additions": 67,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
                "deletions": 34,
                "sha": "cfc57cd5753651f3db3d9c188877e6ed810c0981",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.apex.translation.utils;\n \n+import com.datatorrent.netlet.util.Slice;\n import com.esotericsoftware.kryo.DefaultSerializer;\n import com.esotericsoftware.kryo.io.Input;\n import com.esotericsoftware.kryo.serializers.JavaSerializer;\n@@ -27,24 +28,28 @@\n import java.io.Serializable;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n import org.apache.beam.runners.core.StateInternals;\n import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.StateTag;\n import org.apache.beam.runners.core.StateTag.StateBinder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.Coder.Context;\n+import org.apache.beam.sdk.coders.CoderException;\n import org.apache.beam.sdk.coders.InstantCoder;\n import org.apache.beam.sdk.coders.ListCoder;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.Combine.KeyedCombineFn;\n import org.apache.beam.sdk.transforms.CombineWithContext.KeyedCombineFnWithContext;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n+import org.apache.beam.sdk.util.CoderUtils;\n import org.apache.beam.sdk.util.CombineFnUtil;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.SetState;\n@@ -56,34 +61,25 @@\n import org.joda.time.Instant;\n \n /**\n- * Implementation of {@link StateInternals} that can be serialized and\n- * checkpointed with the operator. Suitable for small states, in the future this\n- * should be based on the incremental state saving components in the Apex\n- * library.\n+ * Implementation of {@link StateInternals} for transient use.\n+ *\n+ * <p>For fields that need to be serialized, use {@link ApexStateInternalsFactory}\n+ * or {@link StateInternalsProxy}\n  */\n-@DefaultSerializer(JavaSerializer.class)\n-public class ApexStateInternals<K> implements StateInternals<K>, Serializable {\n-  private static final long serialVersionUID = 1L;\n-  public static <K> ApexStateInternals<K> forKey(K key) {\n-    return new ApexStateInternals<>(key);\n-  }\n-\n+public class ApexStateInternals<K> implements StateInternals<K> {\n   private final K key;\n+  private final Table<String, String, byte[]> stateTable;\n \n-  protected ApexStateInternals(K key) {\n+  protected ApexStateInternals(K key, Table<String, String, byte[]> stateTable) {\n     this.key = key;\n+    this.stateTable = stateTable;\n   }\n \n   @Override\n   public K getKey() {\n     return key;\n   }\n \n-  /**\n-   * Serializable state for internals (namespace to state tag to coded value).\n-   */\n-  private final Table<String, String, byte[]> stateTable = HashBasedTable.create();\n-\n   @Override\n   public <T extends State> T state(StateNamespace namespace, StateTag<? super K, T> address) {\n     return state(namespace, address, StateContexts.nullContext());\n@@ -139,12 +135,12 @@ private ApexStateBinder(K key, StateNamespace namespace, StateTag<? super K, ?>\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             final CombineFn<InputT, AccumT, OutputT> combineFn) {\n-      return new ApexAccumulatorCombiningState<>(\n+      return new ApexCombiningState<>(\n           namespace,\n           address,\n           accumCoder,\n@@ -161,22 +157,22 @@ private ApexStateBinder(K key, StateNamespace namespace, StateTag<? super K, ?>\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n-      return new ApexAccumulatorCombiningState<>(\n+      return new ApexCombiningState<>(\n           namespace,\n           address,\n           accumCoder,\n           key, combineFn);\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValueWithContext(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n       return bindKeyedCombiningValue(address, accumCoder, CombineFnUtil.bindContext(combineFn, c));\n@@ -323,14 +319,14 @@ public Boolean read() {\n \n   }\n \n-  private final class ApexAccumulatorCombiningState<K, InputT, AccumT, OutputT>\n+  private final class ApexCombiningState<K, InputT, AccumT, OutputT>\n       extends AbstractState<AccumT>\n-      implements AccumulatorCombiningState<InputT, AccumT, OutputT> {\n+      implements CombiningState<InputT, AccumT, OutputT> {\n     private final K key;\n     private final KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn;\n \n-    private ApexAccumulatorCombiningState(StateNamespace namespace,\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+    private ApexCombiningState(StateNamespace namespace,\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n         Coder<AccumT> coder,\n         K key, KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n       super(namespace, address, coder);\n@@ -339,7 +335,7 @@ private ApexAccumulatorCombiningState(StateNamespace namespace,\n     }\n \n     @Override\n-    public ApexAccumulatorCombiningState<K, InputT, AccumT, OutputT> readLater() {\n+    public ApexCombiningState<K, InputT, AccumT, OutputT> readLater() {\n       return this;\n     }\n \n@@ -437,17 +433,54 @@ public Boolean read() {\n   }\n \n   /**\n-   * Factory for {@link ApexStateInternals}.\n+   * Implementation of {@link StateInternals} that can be serialized and\n+   * checkpointed with the operator. Suitable for small states, in the future this\n+   * should be based on the incremental state saving components in the Apex\n+   * library.\n    *\n    * @param <K> key type\n    */\n+  @DefaultSerializer(JavaSerializer.class)\n   public static class ApexStateInternalsFactory<K>\n       implements StateInternalsFactory<K>, Serializable {\n     private static final long serialVersionUID = 1L;\n+    /**\n+     * Serializable state for internals (namespace to state tag to coded value).\n+     */\n+    private Map<Slice, Table<String, String, byte[]>> perKeyState = new HashMap<>();\n+    private final Coder<K> keyCoder;\n+\n+    private ApexStateInternalsFactory(Coder<K> keyCoder) {\n+      this.keyCoder = keyCoder;\n+    }\n \n     @Override\n-    public StateInternals<K> stateInternalsForKey(K key) {\n-      return ApexStateInternals.forKey(key);\n+    public ApexStateInternals<K> stateInternalsForKey(K key) {\n+      final Slice keyBytes;\n+      try {\n+        keyBytes = (key != null) ? new Slice(CoderUtils.encodeToByteArray(keyCoder, key)) :\n+          new Slice(null);\n+      } catch (CoderException e) {\n+        throw new RuntimeException(e);\n+      }\n+      Table<String, String, byte[]> stateTable = perKeyState.get(keyBytes);\n+      if (stateTable == null) {\n+        stateTable = HashBasedTable.create();\n+        perKeyState.put(keyBytes, stateTable);\n+      }\n+      return new ApexStateInternals<>(key, stateTable);\n+    }\n+\n+  }\n+\n+  /**\n+   * Factory to create the state internals.\n+   */\n+  public static class ApexStateBackend implements Serializable {\n+    private static final long serialVersionUID = 1L;\n+\n+    public <K> ApexStateInternalsFactory<K> newStateInternalsFactory(Coder<K> keyCoder) {\n+      return new ApexStateInternalsFactory<K>(keyCoder);\n     }\n   }\n ",
                "changes": 101
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
                "deletions": 1,
                "sha": "cc64c7c6bb17c57b4f64400c9b4aab1db39e8e3d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
                "patch": "@@ -48,7 +48,7 @@ public void noteOutput(WindowedValue<?> output) {\n   }\n \n   @Override\n-  public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) {\n+  public void noteOutput(TupleTag<?> tag, WindowedValue<?> output) {\n   }\n \n   @Override",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 11,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
                "deletions": 2,
                "sha": "1a47ed574e2bffbf72274eb07d368f75979129d0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
                "patch": "@@ -18,20 +18,24 @@\n package org.apache.beam.runners.apex.translation.utils;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n-\n import java.io.Externalizable;\n import java.io.IOException;\n import java.io.ObjectInput;\n import java.io.ObjectOutput;\n-\n+import java.util.concurrent.atomic.AtomicBoolean;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.sdk.io.FileSystems;\n import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.util.IOChannelUtils;\n \n /**\n  * A wrapper to enable serialization of {@link PipelineOptions}.\n  */\n public class SerializablePipelineOptions implements Externalizable {\n \n+  /* Used to ensure we initialize file systems exactly once, because it's a slow operation. */\n+  private static final AtomicBoolean FILE_SYSTEMS_INTIIALIZED = new AtomicBoolean(false);\n+\n   private transient ApexPipelineOptions pipelineOptions;\n \n   public SerializablePipelineOptions(ApexPipelineOptions pipelineOptions) {\n@@ -55,6 +59,11 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n     String s = in.readUTF();\n     this.pipelineOptions = new ObjectMapper().readValue(s, PipelineOptions.class)\n         .as(ApexPipelineOptions.class);\n+\n+    if (FILE_SYSTEMS_INTIIALIZED.compareAndSet(false, true)) {\n+      IOChannelUtils.registerIOFactoriesAllowOverride(pipelineOptions);\n+      FileSystems.setDefaultConfigInWorkers(pipelineOptions);\n+    }\n   }\n \n }",
                "changes": 13
            },
            {
                "status": "added",
                "additions": 67,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
                "deletions": 0,
                "sha": "1f28364269215c03b6ce866ab416b8033e542ef9",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
                "patch": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.apex.translation.utils;\n+\n+import com.esotericsoftware.kryo.DefaultSerializer;\n+import com.esotericsoftware.kryo.serializers.JavaSerializer;\n+import java.io.Serializable;\n+import org.apache.beam.runners.core.StateInternals;\n+import org.apache.beam.runners.core.StateInternalsFactory;\n+import org.apache.beam.runners.core.StateNamespace;\n+import org.apache.beam.runners.core.StateTag;\n+import org.apache.beam.sdk.util.state.State;\n+import org.apache.beam.sdk.util.state.StateContext;\n+\n+/**\n+ * State internals for reusable processing context.\n+ * @param <K>\n+ */\n+@DefaultSerializer(JavaSerializer.class)\n+public class StateInternalsProxy<K> implements StateInternals<K>, Serializable {\n+\n+  private final StateInternalsFactory<K> factory;\n+  private transient K currentKey;\n+\n+  public StateInternalsProxy(ApexStateInternals.ApexStateInternalsFactory<K> factory) {\n+    this.factory = factory;\n+  }\n+\n+  public StateInternalsFactory<K> getFactory() {\n+    return this.factory;\n+  }\n+\n+  public void setKey(K key) {\n+    currentKey = key;\n+  }\n+\n+  @Override\n+  public K getKey() {\n+    return currentKey;\n+  }\n+\n+  @Override\n+  public <T extends State> T state(StateNamespace namespace, StateTag<? super K, T> address) {\n+    return factory.stateInternalsForKey(currentKey).state(namespace, address);\n+  }\n+\n+  @Override\n+  public <T extends State> T state(StateNamespace namespace, StateTag<? super K, T> address,\n+      StateContext<?> c) {\n+    return factory.stateInternalsForKey(currentKey).state(namespace, address, c);\n+  }\n+}",
                "changes": 67
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
                "deletions": 1,
                "sha": "62c92a00489260732f22d9e508872a27ee6e79a8",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
                "patch": "@@ -55,7 +55,7 @@ public ValuesSource(Iterable<T> values, Coder<T> coder) {\n   }\n \n   @Override\n-  public java.util.List<? extends UnboundedSource<T, CheckpointMark>> generateInitialSplits(\n+  public java.util.List<? extends UnboundedSource<T, CheckpointMark>> split(\n       int desiredNumSplits, PipelineOptions options) throws Exception {\n     return Collections.singletonList(this);\n   }",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
                "deletions": 1,
                "sha": "abe97f6de049a2615bcadbc5924380c7323e6ebf",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
                "patch": "@@ -39,7 +39,7 @@\n   private static final long serialVersionUID = 1L;\n \n   @Override\n-  public List<? extends UnboundedSource<String, CheckpointMark>> generateInitialSplits(\n+  public List<? extends UnboundedSource<String, CheckpointMark>> split(\n       int desiredNumSplits, PipelineOptions options) throws Exception {\n     return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this);\n   }",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java",
                "deletions": 1,
                "sha": "4b73114da4feee04697192a24af57a05f35afd8c",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java",
                "patch": "@@ -66,7 +66,7 @@ public void testGlobalWindowMinTimestamp() throws Exception {\n     input.setCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));\n \n     ApexGroupByKeyOperator<String, Integer> operator = new ApexGroupByKeyOperator<>(options,\n-        input, new ApexStateInternals.ApexStateInternalsFactory<String>()\n+        input, new ApexStateInternals.ApexStateBackend()\n         );\n \n     operator.setup(null);",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "deletions": 1,
                "sha": "64ca0ee4fd0769945d77f9f3ceb2517f76d760bc",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "patch": "@@ -110,7 +110,8 @@ public void testFlattenSingleCollection() {\n     PCollectionList.of(single).apply(Flatten.<String>pCollections())\n       .apply(ParDo.of(new EmbeddedCollector()));\n     translator.translate(p, dag);\n-    Assert.assertNotNull(dag.getOperatorMeta(\"ParDo(EmbeddedCollector)\"));\n+    Assert.assertNotNull(\n+        dag.getOperatorMeta(\"ParDo(EmbeddedCollector)/ParMultiDo(EmbeddedCollector)\"));\n   }\n \n }",
                "changes": 3
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "deletions": 1,
                "sha": "193de718ceff67b8dd630b7d550d3869a9f6fc88",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "patch": "@@ -131,7 +131,7 @@ public TestSource(List<KV<String, Instant>> data, Instant watermark) {\n     }\n \n     @Override\n-    public List<? extends UnboundedSource<String, CheckpointMark>> generateInitialSplits(\n+    public List<? extends UnboundedSource<String, CheckpointMark>> split(\n         int desiredNumSplits, PipelineOptions options) throws Exception {\n       return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this);\n     }",
                "changes": 2
            },
            {
                "status": "renamed",
                "additions": 19,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java",
                "previous_filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslatorTest.java",
                "deletions": 18,
                "sha": "1a5c8be4da5cf66f8a00809200134d23e999e511",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java",
                "patch": "@@ -68,11 +68,11 @@\n import org.slf4j.LoggerFactory;\n \n /**\n- * integration test for {@link ParDoBoundTranslator}.\n+ * integration test for {@link ParDoTranslator}.\n  */\n @RunWith(JUnit4.class)\n-public class ParDoBoundTranslatorTest {\n-  private static final Logger LOG = LoggerFactory.getLogger(ParDoBoundTranslatorTest.class);\n+public class ParDoTranslatorTest {\n+  private static final Logger LOG = LoggerFactory.getLogger(ParDoTranslatorTest.class);\n   private static final long SLEEP_MILLIS = 500;\n   private static final long TIMEOUT_MILLIS = 30000;\n \n@@ -98,7 +98,7 @@ public void test() throws Exception {\n     Assert.assertNotNull(om);\n     Assert.assertEquals(om.getOperator().getClass(), ApexReadUnboundedInputOperator.class);\n \n-    om = dag.getOperatorMeta(\"ParDo(Add)\");\n+    om = dag.getOperatorMeta(\"ParDo(Add)/ParMultiDo(Add)\");\n     Assert.assertNotNull(om);\n     Assert.assertEquals(om.getOperator().getClass(), ApexParDoOperator.class);\n \n@@ -216,7 +216,7 @@ public void testSerialization() throws Exception {\n             WindowingStrategy.globalDefault(),\n             Collections.<PCollectionView<?>>singletonList(singletonView),\n             coder,\n-            new ApexStateInternals.ApexStateInternalsFactory<Void>());\n+            new ApexStateInternals.ApexStateBackend());\n     operator.setup(null);\n     operator.beginWindow(0);\n     WindowedValue<Integer> wv1 = WindowedValue.valueInGlobalWindow(1);\n@@ -267,7 +267,7 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n \n     List<Integer> inputs = Arrays.asList(3, -42, 666);\n     final TupleTag<String> mainOutputTag = new TupleTag<>(\"main\");\n-    final TupleTag<Void> sideOutputTag = new TupleTag<>(\"sideOutput\");\n+    final TupleTag<Void> additionalOutputTag = new TupleTag<>(\"output\");\n \n     PCollectionView<Integer> sideInput1 = pipeline\n         .apply(\"CreateSideInput1\", Create.of(11))\n@@ -281,16 +281,17 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n \n     PCollectionTuple outputs = pipeline\n         .apply(Create.of(inputs))\n-        .apply(ParDo.withSideInputs(sideInput1)\n-            .withSideInputs(sideInputUnread)\n-            .withSideInputs(sideInput2)\n-            .withOutputTags(mainOutputTag, TupleTagList.of(sideOutputTag))\n+        .apply(ParDo\n             .of(new TestMultiOutputWithSideInputsFn(\n                 Arrays.asList(sideInput1, sideInput2),\n-                Arrays.<TupleTag<String>>asList())));\n+                Arrays.<TupleTag<String>>asList()))\n+            .withSideInputs(sideInput1)\n+            .withSideInputs(sideInputUnread)\n+            .withSideInputs(sideInput2)\n+            .withOutputTags(mainOutputTag, TupleTagList.of(additionalOutputTag)));\n \n     outputs.get(mainOutputTag).apply(ParDo.of(new EmbeddedCollector()));\n-    outputs.get(sideOutputTag).setCoder(VoidCoder.of());\n+    outputs.get(additionalOutputTag).setCoder(VoidCoder.of());\n     ApexRunnerResult result = (ApexRunnerResult) pipeline.run();\n \n     HashSet<String> expected = Sets.newHashSet(\"processing: 3: [11, 222]\",\n@@ -311,12 +312,12 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n     private static final long serialVersionUID = 1L;\n \n     final List<PCollectionView<Integer>> sideInputViews = new ArrayList<>();\n-    final List<TupleTag<String>> sideOutputTupleTags = new ArrayList<>();\n+    final List<TupleTag<String>> additionalOutputTupleTags = new ArrayList<>();\n \n     public TestMultiOutputWithSideInputsFn(List<PCollectionView<Integer>> sideInputViews,\n-        List<TupleTag<String>> sideOutputTupleTags) {\n+        List<TupleTag<String>> additionalOutputTupleTags) {\n       this.sideInputViews.addAll(sideInputViews);\n-      this.sideOutputTupleTags.addAll(sideOutputTupleTags);\n+      this.additionalOutputTupleTags.addAll(additionalOutputTupleTags);\n     }\n \n     @ProcessElement\n@@ -333,9 +334,9 @@ private void outputToAllWithSideInputs(ProcessContext c, String value) {\n         value += \": \" + sideInputValues;\n       }\n       c.output(value);\n-      for (TupleTag<String> sideOutputTupleTag : sideOutputTupleTags) {\n-        c.sideOutput(sideOutputTupleTag,\n-                     sideOutputTupleTag.getId() + \": \" + value);\n+      for (TupleTag<String> additionalOutputTupleTag : additionalOutputTupleTags) {\n+        c.output(additionalOutputTupleTag,\n+                     additionalOutputTupleTag.getId() + \": \" + value);\n       }\n     }\n ",
                "changes": 37
            },
            {
                "status": "modified",
                "additions": 26,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java",
                "deletions": 17,
                "sha": "7160e4544ab1e252e2e94126a4194da93f314256",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java",
                "patch": "@@ -24,6 +24,8 @@\n \n import com.datatorrent.lib.util.KryoCloneUtils;\n import java.util.Arrays;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateInternalsFactory;\n import org.apache.beam.runners.core.StateMerging;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.StateNamespaceForTest;\n@@ -35,9 +37,9 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.ValueState;\n import org.apache.beam.sdk.util.state.WatermarkHoldState;\n@@ -58,7 +60,7 @@\n \n   private static final StateTag<Object, ValueState<String>> STRING_VALUE_ADDR =\n       StateTags.value(\"stringValue\", StringUtf8Coder.of());\n-  private static final StateTag<Object, AccumulatorCombiningState<Integer, int[], Integer>>\n+  private static final StateTag<Object, CombiningState<Integer, int[], Integer>>\n       SUM_INTEGER_ADDR = StateTags.combiningValueFromInputInternal(\n           \"sumInteger\", VarIntCoder.of(), Sum.ofIntegers());\n   private static final StateTag<Object, BagState<String>> STRING_BAG_ADDR =\n@@ -76,7 +78,9 @@\n \n   @Before\n   public void initStateInternals() {\n-    underTest = new ApexStateInternals<>(null);\n+    underTest = new ApexStateInternals.ApexStateBackend()\n+        .newStateInternalsFactory(StringUtf8Coder.of())\n+        .stateInternalsForKey((String) null);\n   }\n \n   @Test\n@@ -148,7 +152,7 @@ public void testMergeBagIntoNewNamespace() throws Exception {\n \n   @Test\n   public void testCombiningValue() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     // State instances are cached, but depend on the namespace.\n     assertEquals(value, underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR));\n@@ -168,7 +172,7 @@ public void testCombiningValue() throws Exception {\n \n   @Test\n   public void testCombiningIsEmpty() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     assertThat(value.isEmpty().read(), Matchers.is(true));\n     ReadableState<Boolean> readFuture = value.isEmpty();\n@@ -181,9 +185,9 @@ public void testCombiningIsEmpty() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoSource() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n \n     value1.add(5);\n@@ -202,11 +206,11 @@ public void testMergeCombiningValueIntoSource() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoNewNamespace() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value3 =\n+    CombiningState<Integer, int[], Integer> value3 =\n         underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR);\n \n     value1.add(5);\n@@ -344,16 +348,21 @@ public void testMergeLatestWatermarkIntoSource() throws Exception {\n \n   @Test\n   public void testSerialization() throws Exception {\n-    ApexStateInternals<String> original = new ApexStateInternals<String>(null);\n-    ValueState<String> value = original.state(NAMESPACE_1, STRING_VALUE_ADDR);\n-    assertEquals(original.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n+    ApexStateInternalsFactory<String> sif = new ApexStateBackend().\n+        newStateInternalsFactory(StringUtf8Coder.of());\n+    ApexStateInternals<String> keyAndState = sif.stateInternalsForKey(\"dummy\");\n+\n+    ValueState<String> value = keyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR);\n+    assertEquals(keyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n     value.write(\"hello\");\n \n-    ApexStateInternals<String> cloned;\n-    assertNotNull(\"Serialization\", cloned = KryoCloneUtils.cloneObject(original));\n-    ValueState<String> clonedValue = cloned.state(NAMESPACE_1, STRING_VALUE_ADDR);\n+    ApexStateInternalsFactory<String> cloned;\n+    assertNotNull(\"Serialization\", cloned = KryoCloneUtils.cloneObject(sif));\n+    ApexStateInternals<String> clonedKeyAndState = cloned.stateInternalsForKey(\"dummy\");\n+\n+    ValueState<String> clonedValue = clonedKeyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR);\n     assertThat(clonedValue.read(), Matchers.equalTo(\"hello\"));\n-    assertEquals(cloned.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n+    assertEquals(clonedKeyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n   }\n \n }",
                "changes": 43
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
                "deletions": 1,
                "sha": "92812b4f4f5dfc62f4b68efbfdb6fbce57d4bb6a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
                "patch": "@@ -47,7 +47,7 @@ public CollectionSource(Collection<T> collection, Coder<T> coder) {\n   }\n \n   @Override\n-  public List<? extends UnboundedSource<T, CheckpointMark>> generateInitialSplits(\n+  public List<? extends UnboundedSource<T, CheckpointMark>> split(\n       int desiredNumSplits, PipelineOptions options) throws Exception {\n     return Collections.singletonList(this);\n   }",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 44,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/pom.xml",
                "deletions": 53,
                "sha": "dfab3e2ca7004e193227c9d0ba05f1abaa52ee4d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/pom.xml",
                "patch": "@@ -29,7 +29,7 @@\n   </parent>\n \n   <artifactId>beam-runners-core-construction-java</artifactId>\n-  <name>Apache Beam :: Runners :: Core Java Construction</name>\n+  <name>Apache Beam :: Runners :: Core Construction Java</name>\n   <description>Beam Runners Core provides utilities to aid runner authors interact with a Pipeline\n     prior to execution.\n   </description>\n@@ -50,67 +50,40 @@\n           </systemPropertyVariables>\n         </configuration>\n       </plugin>\n-\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-shade-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <id>bundle-and-repackage</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>shade</goal>\n-            </goals>\n-            <configuration>\n-              <shadeTestJar>true</shadeTestJar>\n-              <artifactSet>\n-                <includes>\n-                  <include>com.google.guava:guava</include>\n-                </includes>\n-              </artifactSet>\n-              <filters>\n-                <filter>\n-                  <artifact>*:*</artifact>\n-                  <excludes>\n-                    <exclude>META-INF/*.SF</exclude>\n-                    <exclude>META-INF/*.DSA</exclude>\n-                    <exclude>META-INF/*.RSA</exclude>\n-                  </excludes>\n-                </filter>\n-              </filters>\n-              <relocations>\n-                <!-- TODO: Once ready, change the following pattern to 'com'\n-                   only, exclude 'org.apache.beam.**', and remove\n-                   the second relocation. -->\n-                <relocation>\n-                  <pattern>com.google.common</pattern>\n-                  <shadedPattern>\n-                    org.apache.beam.runners.core.construction.repackaged.com.google.common\n-                  </shadedPattern>\n-                </relocation>\n-                <relocation>\n-                  <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>\n-                    org.apache.beam.runners.core.construction.repackaged.com.google.thirdparty\n-                  </shadedPattern>\n-                </relocation>\n-              </relocations>\n-              <transformers>\n-                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n-              </transformers>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n     </plugins>\n   </build>\n \n   <dependencies>\n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-common-runner-api</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-core</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>com.google.protobuf</groupId>\n+      <artifactId>protobuf-java</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-annotations</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-databind</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.google.code.findbugs</groupId>\n+      <artifactId>jsr305</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>joda-time</groupId>\n       <artifactId>joda-time</artifactId>\n@@ -121,6 +94,17 @@\n       <artifactId>guava</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-api</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.google.auto.value</groupId>\n+      <artifactId>auto-value</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+\n     <!-- test dependencies -->\n \n     <dependency>\n@@ -134,5 +118,12 @@\n       <artifactId>junit</artifactId>\n       <scope>test</scope>\n     </dependency>\n+\n+    <dependency>\n+      <groupId>org.mockito</groupId>\n+      <artifactId>mockito-all</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+\n   </dependencies>\n </project>",
                "changes": 97
            },
            {
                "status": "added",
                "additions": 174,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
                "deletions": 0,
                "sha": "043a01030d6ec016ffb4bca7b19e4558dbcca028",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
                "patch": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.BiMap;\n+import com.google.common.collect.ImmutableBiMap;\n+import com.google.protobuf.Any;\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.BytesValue;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.IterableCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.StandardCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.FunctionSpec;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.SdkFunctionSpec;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow.IntervalWindowCoder;\n+import org.apache.beam.sdk.util.CloudObject;\n+import org.apache.beam.sdk.util.Serializer;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n+\n+/** Converts to and from Beam Runner API representations of {@link Coder Coders}. */\n+public class Coders {\n+  private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n+\n+  // This URN says that the coder is just a UDF blob this SDK understands\n+  // TODO: standardize such things\n+  public static final String CUSTOM_CODER_URN = \"urn:beam:coders:javasdk:0.1\";\n+\n+  // The URNs for coders which are shared across languages\n+  @VisibleForTesting\n+  static final BiMap<Class<? extends StandardCoder>, String> KNOWN_CODER_URNS =\n+      ImmutableBiMap.<Class<? extends StandardCoder>, String>builder()\n+          .put(ByteArrayCoder.class, \"urn:beam:coders:bytes:0.1\")\n+          .put(KvCoder.class, \"urn:beam:coders:kv:0.1\")\n+          .put(VarLongCoder.class, \"urn:beam:coders:varint:0.1\")\n+          .put(IntervalWindowCoder.class, \"urn:beam:coders:interval_window:0.1\")\n+          .put(IterableCoder.class, \"urn:beam:coders:stream:0.1\")\n+          .put(GlobalWindow.Coder.class, \"urn:beam:coders:global_window:0.1\")\n+          .put(FullWindowedValueCoder.class, \"urn:beam:coders:windowed_value:0.1\")\n+          .build();\n+\n+  public static RunnerApi.Coder toProto(\n+      Coder<?> coder, @SuppressWarnings(\"unused\") SdkComponents components) throws IOException {\n+    if (KNOWN_CODER_URNS.containsKey(coder.getClass())) {\n+      return toKnownCoder(coder, components);\n+    }\n+    return toCustomCoder(coder);\n+  }\n+\n+  private static RunnerApi.Coder toKnownCoder(Coder<?> coder, SdkComponents components)\n+      throws IOException {\n+    checkArgument(\n+        coder instanceof StandardCoder,\n+        \"A Known %s must implement %s, but %s of class %s does not\",\n+        Coder.class.getSimpleName(),\n+        StandardCoder.class.getSimpleName(),\n+        coder,\n+        coder.getClass().getName());\n+    StandardCoder<?> stdCoder = (StandardCoder<?>) coder;\n+    List<String> componentIds = new ArrayList<>();\n+    for (Coder<?> componentCoder : stdCoder.getComponents()) {\n+      componentIds.add(components.registerCoder(componentCoder));\n+    }\n+    return RunnerApi.Coder.newBuilder()\n+        .addAllComponentCoderIds(componentIds)\n+        .setSpec(\n+            SdkFunctionSpec.newBuilder()\n+                .setSpec(FunctionSpec.newBuilder().setUrn(KNOWN_CODER_URNS.get(coder.getClass()))))\n+        .build();\n+  }\n+\n+  private static RunnerApi.Coder toCustomCoder(Coder<?> coder) throws IOException {\n+    RunnerApi.Coder.Builder coderBuilder = RunnerApi.Coder.newBuilder();\n+    return coderBuilder\n+        .setSpec(\n+            SdkFunctionSpec.newBuilder()\n+                .setSpec(\n+                    FunctionSpec.newBuilder()\n+                        .setUrn(CUSTOM_CODER_URN)\n+                        .setParameter(\n+                            Any.pack(\n+                                BytesValue.newBuilder()\n+                                    .setValue(\n+                                        ByteString.copyFrom(\n+                                            OBJECT_MAPPER.writeValueAsBytes(coder.asCloudObject())))\n+                                    .build()))))\n+        .build();\n+  }\n+\n+  public static Coder<?> fromProto(RunnerApi.Coder protoCoder, Components components)\n+      throws IOException {\n+    String coderSpecUrn = protoCoder.getSpec().getSpec().getUrn();\n+    if (coderSpecUrn.equals(CUSTOM_CODER_URN)) {\n+      return fromCustomCoder(protoCoder, components);\n+    }\n+    return fromKnownCoder(protoCoder, components);\n+  }\n+\n+  private static Coder<?> fromKnownCoder(RunnerApi.Coder coder, Components components)\n+      throws IOException {\n+    String coderUrn = coder.getSpec().getSpec().getUrn();\n+    List<Coder<?>> coderComponents = new LinkedList<>();\n+    for (String componentId : coder.getComponentCoderIdsList()) {\n+      Coder<?> innerCoder = fromProto(components.getCodersOrThrow(componentId), components);\n+      coderComponents.add(innerCoder);\n+    }\n+    switch (coderUrn) {\n+      case \"urn:beam:coders:bytes:0.1\":\n+        return ByteArrayCoder.of();\n+      case \"urn:beam:coders:kv:0.1\":\n+        return KvCoder.of(coderComponents);\n+      case \"urn:beam:coders:varint:0.1\":\n+        return VarLongCoder.of();\n+      case \"urn:beam:coders:interval_window:0.1\":\n+        return IntervalWindowCoder.of();\n+      case \"urn:beam:coders:stream:0.1\":\n+        return IterableCoder.of(coderComponents);\n+      case \"urn:beam:coders:global_window:0.1\":\n+        return GlobalWindow.Coder.INSTANCE;\n+      case \"urn:beam:coders:windowed_value:0.1\":\n+        return WindowedValue.FullWindowedValueCoder.of(coderComponents);\n+      default:\n+        throw new IllegalStateException(\n+            String.format(\n+                \"Unknown coder URN %s. Known URNs: %s\", coderUrn, KNOWN_CODER_URNS.values()));\n+    }\n+  }\n+\n+  private static Coder<?> fromCustomCoder(\n+      RunnerApi.Coder protoCoder, @SuppressWarnings(\"unused\") Components components)\n+      throws IOException {\n+    CloudObject coderCloudObject =\n+        OBJECT_MAPPER.readValue(\n+            protoCoder\n+                .getSpec()\n+                .getSpec()\n+                .getParameter()\n+                .unpack(BytesValue.class)\n+                .getValue()\n+                .toByteArray(),\n+            CloudObject.class);\n+    return Serializer.deserialize(coderCloudObject, Coder.class);\n+  }\n+}",
                "changes": 174
            },
            {
                "status": "added",
                "additions": 120,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
                "deletions": 0,
                "sha": "13e7593057d2c5908a2486805501fd5020b3ab32",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
                "patch": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.PCollections;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ * A {@link PTransformOverrideFactory} that will apply a flatten where no element appears in the\n+ * input {@link PCollectionList} more than once.\n+ */\n+public class DeduplicatedFlattenFactory<T>\n+    implements PTransformOverrideFactory<\n+        PCollectionList<T>, PCollection<T>, Flatten.PCollections<T>> {\n+\n+  public static <T> DeduplicatedFlattenFactory<T> create() {\n+    return new DeduplicatedFlattenFactory<>();\n+  }\n+\n+  private DeduplicatedFlattenFactory() {}\n+\n+  @Override\n+  public PTransformReplacement<PCollectionList<T>, PCollection<T>> getReplacementTransform(\n+      AppliedPTransform<PCollectionList<T>, PCollection<T>, PCollections<T>> transform) {\n+    return PTransformReplacement.of(\n+        getInput(transform.getInputs(), transform.getPipeline()),\n+        new FlattenWithoutDuplicateInputs<T>());\n+  }\n+\n+  /**\n+   * {@inheritDoc}.\n+   *\n+   * <p>The input {@link PCollectionList} that is constructed will have the same values in the same\n+   */\n+  private PCollectionList<T> getInput(Map<TupleTag<?>, PValue> inputs, Pipeline p) {\n+    PCollectionList<T> pCollections = PCollectionList.empty(p);\n+    for (PValue input : inputs.values()) {\n+      PCollection<T> pcollection = (PCollection<T>) input;\n+      pCollections = pCollections.and(pcollection);\n+    }\n+    return pCollections;\n+  }\n+\n+  @Override\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n+    return ReplacementOutputs.singleton(outputs, newOutput);\n+  }\n+\n+  @VisibleForTesting\n+  static class FlattenWithoutDuplicateInputs<T>\n+      extends PTransform<PCollectionList<T>, PCollection<T>> {\n+    @Override\n+    public PCollection<T> expand(PCollectionList<T> input) {\n+      Map<PCollection<T>, Integer> instances = new HashMap<>();\n+      for (PCollection<T> pCollection : input.getAll()) {\n+        int existing = instances.get(pCollection) == null ? 0 : instances.get(pCollection);\n+        instances.put(pCollection, existing + 1);\n+      }\n+      PCollectionList<T> output = PCollectionList.empty(input.getPipeline());\n+      for (Map.Entry<PCollection<T>, Integer> instanceEntry : instances.entrySet()) {\n+        if (instanceEntry.getValue().equals(1)) {\n+          output = output.and(instanceEntry.getKey());\n+        } else {\n+          String duplicationName = String.format(\"Multiply %s\", instanceEntry.getKey().getName());\n+          PCollection<T> duplicated =\n+              instanceEntry\n+                  .getKey()\n+                  .apply(duplicationName, ParDo.of(new DuplicateFn<T>(instanceEntry.getValue())));\n+          output = output.and(duplicated);\n+        }\n+      }\n+      return output.apply(Flatten.<T>pCollections());\n+    }\n+  }\n+\n+  private static class DuplicateFn<T> extends DoFn<T, T> {\n+    private final int numTimes;\n+\n+    private DuplicateFn(int numTimes) {\n+      this.numTimes = numTimes;\n+    }\n+\n+    @ProcessElement\n+    public void emitCopies(ProcessContext context) {\n+      for (int i = 0; i < numTimes; i++) {\n+        context.output(context.element());\n+      }\n+    }\n+  }\n+}",
                "changes": 120
            },
            {
                "status": "modified",
                "additions": 20,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
                "deletions": 14,
                "sha": "a6982d4801f2e86ec6cd646f8bf1c45040a96ffb",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
                "patch": "@@ -20,18 +20,18 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n \n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.PCollections;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} that provides an empty {@link Create} to replace a {@link\n@@ -50,22 +50,28 @@\n   private EmptyFlattenAsCreateFactory() {}\n \n   @Override\n-  public PTransform<PCollectionList<T>, PCollection<T>> getReplacementTransform(\n-      Flatten.PCollections<T> transform) {\n-    return (PTransform) Create.empty(VoidCoder.of());\n-  }\n-\n-  @Override\n-  public PCollectionList<T> getInput(\n-      List<TaggedPValue> inputs, Pipeline p) {\n+  public PTransformReplacement<PCollectionList<T>, PCollection<T>> getReplacementTransform(\n+      AppliedPTransform<PCollectionList<T>, PCollection<T>, PCollections<T>> transform) {\n     checkArgument(\n-        inputs.isEmpty(), \"Must have an empty input to use %s\", getClass().getSimpleName());\n-    return PCollectionList.empty(p);\n+        transform.getInputs().isEmpty(),\n+        \"Unexpected nonempty input %s for %s\",\n+        transform.getInputs(),\n+        getClass().getSimpleName());\n+    return PTransformReplacement.of(\n+        PCollectionList.<T>empty(transform.getPipeline()), new CreateEmptyFromList<T>());\n   }\n \n   @Override\n   public Map<PValue, ReplacementOutput> mapOutputs(\n-      List<TaggedPValue> outputs, PCollection<T> newOutput) {\n+      Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n     return ReplacementOutputs.singleton(outputs, newOutput);\n   }\n+\n+  private static class CreateEmptyFromList<T>\n+      extends PTransform<PCollectionList<T>, PCollection<T>> {\n+    @Override\n+    public PCollection<T> expand(PCollectionList<T> input) {\n+      return (PCollection) input.getPipeline().apply(Create.empty(VoidCoder.of()));\n+    }\n+  }\n }",
                "changes": 34
            },
            {
                "status": "renamed",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
                "previous_filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ForwardingPTransform.java",
                "deletions": 1,
                "sha": "3bee2817a8a1fb60b12505396a7aa82bac5e5443",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.direct;\n+package org.apache.beam.runners.core.construction;\n \n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;",
                "changes": 2
            },
            {
                "status": "added",
                "additions": 97,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
                "deletions": 0,
                "sha": "907e54dd21c153bc1be44ac07ea45505153ef556",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
                "patch": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import com.google.protobuf.InvalidProtocolBufferException;\n+import java.io.IOException;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollection.IsBounded;\n+\n+/**\n+ * Utility methods for translating {@link PCollection PCollections} to and from Runner API protos.\n+ */\n+public class PCollections {\n+  private PCollections() {}\n+\n+  public static RunnerApi.PCollection toProto(PCollection<?> pCollection, SdkComponents components)\n+      throws IOException {\n+    String coderId = components.registerCoder(pCollection.getCoder());\n+    String windowingStrategyId =\n+        components.registerWindowingStrategy(pCollection.getWindowingStrategy());\n+    // TODO: Display Data\n+\n+    return RunnerApi.PCollection.newBuilder()\n+        .setUniqueName(pCollection.getName())\n+        .setCoderId(coderId)\n+        .setIsBounded(toProto(pCollection.isBounded()))\n+        .setWindowingStrategyId(windowingStrategyId)\n+        .build();\n+  }\n+\n+  public static IsBounded isBounded(RunnerApi.PCollection pCollection) {\n+    return fromProto(pCollection.getIsBounded());\n+  }\n+\n+  public static Coder<?> getCoder(\n+      RunnerApi.PCollection pCollection, RunnerApi.Components components) throws IOException {\n+    return Coders.fromProto(components.getCodersOrThrow(pCollection.getCoderId()), components);\n+  }\n+\n+  public static WindowingStrategy<?, ?> getWindowingStrategy(\n+      RunnerApi.PCollection pCollection, RunnerApi.Components components)\n+      throws InvalidProtocolBufferException {\n+    return WindowingStrategies.fromProto(\n+        components.getWindowingStrategiesOrThrow(pCollection.getWindowingStrategyId()), components);\n+  }\n+\n+  private static RunnerApi.IsBounded toProto(IsBounded bounded) {\n+    switch (bounded) {\n+      case BOUNDED:\n+        return RunnerApi.IsBounded.BOUNDED;\n+      case UNBOUNDED:\n+        return RunnerApi.IsBounded.UNBOUNDED;\n+      default:\n+        throw new IllegalArgumentException(\n+            String.format(\"Unknown %s %s\", IsBounded.class.getSimpleName(), bounded));\n+    }\n+  }\n+\n+  private static IsBounded fromProto(RunnerApi.IsBounded isBounded) {\n+    switch (isBounded) {\n+      case BOUNDED:\n+        return IsBounded.BOUNDED;\n+      case UNBOUNDED:\n+        return IsBounded.UNBOUNDED;\n+      case UNRECOGNIZED:\n+      default:\n+        // Whether or not this enum cannot be recognized by the proto (due to the version of the\n+        // generated code we link to) or the switch hasn't been updated to handle it,\n+        // the situation is the same: we don't know what this IsBounded means\n+        throw new IllegalArgumentException(\n+            String.format(\n+                \"Cannot convert unknown %s to %s: %s\",\n+                RunnerApi.IsBounded.class.getCanonicalName(),\n+                IsBounded.class.getCanonicalName(),\n+                isBounded));\n+    }\n+  }\n+}",
                "changes": 97
            },
            {
                "status": "modified",
                "additions": 111,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
                "deletions": 24,
                "sha": "09946bcb1580e8554e910e81bc87c24de0d91a36",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
                "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.beam.runners.core.construction;\n \n+import com.google.common.base.MoreObjects;\n+import java.util.HashSet;\n+import java.util.Set;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.annotations.Experimental.Kind;\n import org.apache.beam.sdk.io.Write;\n@@ -26,10 +29,14 @@\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.ViewFn;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature.ProcessElementMethod;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n+import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n \n /**\n  * A {@link PTransformMatcher} that matches {@link PTransform PTransforms} based on the class of the\n@@ -45,8 +52,6 @@ private PTransformMatchers() {}\n   /**\n    * Returns a {@link PTransformMatcher} that matches a {@link PTransform} if the class of the\n    * {@link PTransform} is equal to the {@link Class} provided ot this matcher.\n-   * @param clazz\n-   * @return\n    */\n   public static PTransformMatcher classEqualTo(Class<? extends PTransform> clazz) {\n     return new EqualClassPTransformMatcher(clazz);\n@@ -63,68 +68,90 @@ private EqualClassPTransformMatcher(Class<? extends PTransform> clazz) {\n     public boolean matches(AppliedPTransform<?, ?, ?> application) {\n       return application.getTransform().getClass().equals(clazz);\n     }\n+\n+    @Override\n+    public String toString() {\n+      return MoreObjects.toStringHelper(EqualClassPTransformMatcher.class)\n+          .add(\"class\", clazz)\n+          .toString();\n+    }\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.Bound} containing a {@link DoFn} that\n-   * is splittable, as signified by {@link ProcessElementMethod#isSplittable()}.\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.SingleOutput} containing a {@link DoFn}\n+   * that is splittable, as signified by {@link ProcessElementMethod#isSplittable()}.\n    */\n   public static PTransformMatcher splittableParDoSingle() {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.Bound) {\n-          DoFn<?, ?> fn = ((ParDo.Bound<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.SingleOutput) {\n+          DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.processElement().isSplittable();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"SplittableParDoSingleMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.Bound} containing a {@link DoFn} that\n-   * uses state or timers, as specified by {@link DoFnSignature#usesState()} and\n-   * {@link DoFnSignature#usesTimers()}.\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.SingleOutput} containing a {@link DoFn}\n+   * that uses state or timers, as specified by {@link DoFnSignature#usesState()} and {@link\n+   * DoFnSignature#usesTimers()}.\n    */\n   public static PTransformMatcher stateOrTimerParDoSingle() {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.Bound) {\n-          DoFn<?, ?> fn = ((ParDo.Bound<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.SingleOutput) {\n+          DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.usesState() || signature.usesTimers();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"StateOrTimerParDoSingleMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.BoundMulti} containing a {@link DoFn}\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.MultiOutput} containing a {@link DoFn}\n    * that is splittable, as signified by {@link ProcessElementMethod#isSplittable()}.\n    */\n   public static PTransformMatcher splittableParDoMulti() {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.BoundMulti) {\n-          DoFn<?, ?> fn = ((ParDo.BoundMulti<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.MultiOutput) {\n+          DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.processElement().isSplittable();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"SplittableParDoMultiMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.BoundMulti} containing a {@link DoFn}\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.MultiOutput} containing a {@link DoFn}\n    * that uses state or timers, as specified by {@link DoFnSignature#usesState()} and\n    * {@link DoFnSignature#usesTimers()}.\n    */\n@@ -133,34 +160,61 @@ public static PTransformMatcher stateOrTimerParDoMulti() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.BoundMulti) {\n-          DoFn<?, ?> fn = ((ParDo.BoundMulti<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.MultiOutput) {\n+          DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.usesState() || signature.usesTimers();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"StateOrTimerParDoMultiMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} which matches a {@link ParDo.Bound} or {@link ParDo.BoundMulti}\n-   * where the {@link DoFn} is of the provided type.\n+   * A {@link PTransformMatcher} which matches a {@link ParDo.SingleOutput} or {@link\n+   * ParDo.MultiOutput} where the {@link DoFn} is of the provided type.\n    */\n   public static PTransformMatcher parDoWithFnType(final Class<? extends DoFn> fnType) {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         DoFn<?, ?> fn;\n-        if (application.getTransform() instanceof ParDo.Bound) {\n-          fn = ((ParDo.Bound) application.getTransform()).getFn();\n-        } else if (application.getTransform() instanceof ParDo.BoundMulti) {\n-          fn = ((ParDo.BoundMulti) application.getTransform()).getFn();\n+        if (application.getTransform() instanceof ParDo.SingleOutput) {\n+          fn = ((ParDo.SingleOutput) application.getTransform()).getFn();\n+        } else if (application.getTransform() instanceof ParDo.MultiOutput) {\n+          fn = ((ParDo.MultiOutput) application.getTransform()).getFn();\n         } else {\n           return false;\n         }\n         return fnType.equals(fn.getClass());\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"ParDoWithFnTypeMatcher\")\n+            .add(\"fnType\", fnType)\n+            .toString();\n+      }\n+    };\n+  }\n+\n+  public static PTransformMatcher createViewWithViewFn(final Class<? extends ViewFn> viewFnType) {\n+    return new PTransformMatcher() {\n+      @Override\n+      public boolean matches(AppliedPTransform<?, ?, ?> application) {\n+        if (!(application.getTransform() instanceof CreatePCollectionView)) {\n+          return false;\n+        }\n+        CreatePCollectionView<?, ?> createView =\n+            (CreatePCollectionView<?, ?>) application.getTransform();\n+        ViewFn<Iterable<WindowedValue<?>>, ?> viewFn = createView.getView().getViewFn();\n+        return viewFn.getClass().equals(viewFnType);\n+      }\n     };\n   }\n \n@@ -175,6 +229,38 @@ public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         return (application.getTransform() instanceof Flatten.PCollections)\n             && application.getInputs().isEmpty();\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"EmptyFlattenMatcher\").toString();\n+      }\n+    };\n+  }\n+\n+  /**\n+   * A {@link PTransformMatcher} which matches a {@link Flatten.PCollections} which\n+   * consumes a single input {@link PCollection} multiple times.\n+   */\n+  public static PTransformMatcher flattenWithDuplicateInputs() {\n+    return new PTransformMatcher() {\n+      @Override\n+      public boolean matches(AppliedPTransform<?, ?, ?> application) {\n+        if (application.getTransform() instanceof Flatten.PCollections) {\n+          Set<PValue> observed = new HashSet<>();\n+          for (PValue pvalue : application.getInputs().values()) {\n+            boolean firstInstance = observed.add(pvalue);\n+            if (!firstInstance) {\n+              return true;\n+            }\n+          }\n+        }\n+        return false;\n+      }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"FlattenWithDuplicateInputsMatcher\").toString();\n+      }\n     };\n   }\n \n@@ -183,7 +269,8 @@ public static PTransformMatcher writeWithRunnerDeterminedSharding() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         if (application.getTransform() instanceof Write) {\n-          return ((Write) application.getTransform()).getSharding() == null;\n+          Write write = (Write) application.getTransform();\n+          return write.getSharding() == null && write.getNumShards() == null;\n         }\n         return false;\n       }",
                "changes": 135
            },
            {
                "status": "added",
                "additions": 69,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
                "deletions": 0,
                "sha": "72a3425bada86e02e93210af0677277771e4ee2b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
                "patch": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ */\n+public class PTransformReplacements {\n+  /**\n+   * Gets the singleton input of an {@link AppliedPTransform}, ignoring any additional inputs\n+   * returned by {@link PTransform#getAdditionalInputs()}.\n+   */\n+  public static <T> PCollection<T> getSingletonMainInput(\n+      AppliedPTransform<? extends PCollection<? extends T>, ?, ?> application) {\n+    return getSingletonMainInput(\n+        application.getInputs(), application.getTransform().getAdditionalInputs().keySet());\n+  }\n+\n+  private static <T> PCollection<T> getSingletonMainInput(\n+      Map<TupleTag<?>, PValue> inputs, Set<TupleTag<?>> ignoredTags) {\n+    PCollection<T> mainInput = null;\n+    for (Map.Entry<TupleTag<?>, PValue> input : inputs.entrySet()) {\n+      if (!ignoredTags.contains(input.getKey())) {\n+        checkArgument(\n+            mainInput == null,\n+            \"Got multiple inputs that are not additional inputs for a \"\n+                + \"singleton main input: %s and %s\",\n+            mainInput,\n+            input.getValue());\n+        checkArgument(\n+            input.getValue() instanceof PCollection,\n+            \"Unexpected input type %s\",\n+            input.getValue().getClass());\n+        mainInput = (PCollection<T>) input.getValue();\n+      }\n+    }\n+    checkArgument(\n+        mainInput != null,\n+        \"No main input found in inputs: Inputs %s, Side Input tags %s\",\n+        inputs,\n+        ignoredTags);\n+    return mainInput;\n+  }\n+}",
                "changes": 69
            },
            {
                "status": "added",
                "additions": 107,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
                "deletions": 0,
                "sha": "7ec0863860b6471774cf461e7b97dc8e35e30d95",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
                "patch": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.common.collect.ImmutableMap;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.FunctionSpec;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ * Utilities for converting {@link PTransform PTransforms} to and from {@link RunnerApi Runner API\n+ * protocol buffers}.\n+ */\n+public class PTransforms {\n+  private static final Map<Class<? extends PTransform>, TransformPayloadTranslator>\n+      KNOWN_PAYLOAD_TRANSLATORS =\n+          ImmutableMap.<Class<? extends PTransform>, TransformPayloadTranslator>builder().build();\n+  // TODO: ParDoPayload, WindowIntoPayload, ReadPayload, CombinePayload\n+  // TODO: \"Flatten Payload\", etc?\n+  // TODO: Load via service loader.\n+  private PTransforms() {}\n+\n+  /**\n+   * Translates an {@link AppliedPTransform} into a runner API proto.\n+   *\n+   * <p>Does not register the {@code appliedPTransform} within the provided {@link SdkComponents}.\n+   */\n+  static RunnerApi.PTransform toProto(\n+      AppliedPTransform<?, ?, ?> appliedPTransform,\n+      List<AppliedPTransform<?, ?, ?>> subtransforms,\n+      SdkComponents components)\n+      throws IOException {\n+    RunnerApi.PTransform.Builder transformBuilder = RunnerApi.PTransform.newBuilder();\n+    for (Map.Entry<TupleTag<?>, PValue> taggedInput : appliedPTransform.getInputs().entrySet()) {\n+      checkArgument(\n+          taggedInput.getValue() instanceof PCollection,\n+          \"Unexpected input type %s\",\n+          taggedInput.getValue().getClass());\n+      transformBuilder.putInputs(\n+          toProto(taggedInput.getKey()),\n+          components.registerPCollection((PCollection<?>) taggedInput.getValue()));\n+    }\n+    for (Map.Entry<TupleTag<?>, PValue> taggedOutput : appliedPTransform.getOutputs().entrySet()) {\n+      checkArgument(\n+          taggedOutput.getValue() instanceof PCollection,\n+          \"Unexpected output type %s\",\n+          taggedOutput.getValue().getClass());\n+      transformBuilder.putOutputs(\n+          toProto(taggedOutput.getKey()),\n+          components.registerPCollection((PCollection<?>) taggedOutput.getValue()));\n+    }\n+    for (AppliedPTransform<?, ?, ?> subtransform : subtransforms) {\n+      transformBuilder.addSubtransforms(components.getExistingPTransformId(subtransform));\n+    }\n+\n+    transformBuilder.setUniqueName(appliedPTransform.getFullName());\n+    // TODO: Display Data\n+\n+    PTransform<?, ?> transform = appliedPTransform.getTransform();\n+    if (KNOWN_PAYLOAD_TRANSLATORS.containsKey(transform.getClass())) {\n+      FunctionSpec payload =\n+          KNOWN_PAYLOAD_TRANSLATORS\n+              .get(transform.getClass())\n+              .translate(appliedPTransform, components);\n+      transformBuilder.setSpec(payload);\n+    }\n+\n+    return transformBuilder.build();\n+  }\n+\n+  private static String toProto(TupleTag<?> tag) {\n+    return tag.getId();\n+  }\n+\n+  /**\n+   * A translator consumes a {@link PTransform} application and produces the appropriate\n+   * FunctionSpec for a distinguished or primitive transform within the Beam runner API.\n+   */\n+  public interface TransformPayloadTranslator<T extends PTransform<?, ?>> {\n+    FunctionSpec translate(AppliedPTransform<?, ?, T> transform, SdkComponents components);\n+  }\n+}",
                "changes": 107
            },
            {
                "status": "modified",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
                "deletions": 11,
                "sha": "5a2140b63ee4f430b928daa9f4b407441e69a820",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
                "patch": "@@ -18,10 +18,9 @@\n \n package org.apache.beam.runners.core.construction;\n \n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Create.Values;\n import org.apache.beam.sdk.transforms.PTransform;\n@@ -30,7 +29,7 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * An implementation of {@link Create} that returns a primitive {@link PCollection}.\n@@ -58,18 +57,15 @@ private PrimitiveCreate(Create.Values<T> transform) {\n   public static class Factory<T>\n       implements PTransformOverrideFactory<PBegin, PCollection<T>, Values<T>> {\n     @Override\n-    public PTransform<PBegin, PCollection<T>> getReplacementTransform(Values<T> transform) {\n-      return new PrimitiveCreate<>(transform);\n-    }\n-\n-    @Override\n-    public PBegin getInput(List<TaggedPValue> inputs, Pipeline p) {\n-      return p.begin();\n+    public PTransformReplacement<PBegin, PCollection<T>> getReplacementTransform(\n+        AppliedPTransform<PBegin, PCollection<T>, Values<T>> transform) {\n+      return PTransformReplacement.of(\n+          transform.getPipeline().begin(), new PrimitiveCreate<T>(transform.getTransform()));\n     }\n \n     @Override\n     public Map<PValue, ReplacementOutput> mapOutputs(\n-        List<TaggedPValue> outputs, PCollection<T> newOutput) {\n+        Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }",
                "changes": 18
            },
            {
                "status": "modified",
                "additions": 22,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
                "deletions": 41,
                "sha": "3d485aebc743465d634a3c5ac060f9676fc420f0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
                "patch": "@@ -21,10 +21,11 @@\n \n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n-import java.util.List;\n import java.util.Map;\n+import java.util.Map.Entry;\n import java.util.Set;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n import org.apache.beam.sdk.values.POutput;\n@@ -39,60 +40,40 @@\n   private ReplacementOutputs() {}\n \n   public static Map<PValue, ReplacementOutput> singleton(\n-      List<TaggedPValue> original, PValue replacement) {\n-    TaggedPValue taggedReplacement = Iterables.getOnlyElement(replacement.expand());\n-    return ImmutableMap.<PValue, ReplacementOutput>builder()\n-        .put(\n-            taggedReplacement.getValue(),\n-            ReplacementOutput.of(Iterables.getOnlyElement(original), taggedReplacement))\n-        .build();\n-  }\n-\n-  public static Map<PValue, ReplacementOutput> ordered(\n-      List<TaggedPValue> original, POutput replacement) {\n-    ImmutableMap.Builder<PValue, ReplacementOutput> result = ImmutableMap.builder();\n-    List<TaggedPValue> replacements = replacement.expand();\n-    checkArgument(\n-        original.size() == replacements.size(),\n-        \"Original and Replacements must be the same size. Original: %s Replacement: %s\",\n-        original.size(),\n-        replacements.size());\n-    int i = 0;\n-    for (TaggedPValue replacementPvalue : replacements) {\n-      result.put(\n-          replacementPvalue.getValue(), ReplacementOutput.of(original.get(i), replacementPvalue));\n-      i++;\n-    }\n-    return result.build();\n+      Map<TupleTag<?>, PValue> original, PValue replacement) {\n+    Entry<TupleTag<?>, PValue> originalElement = Iterables.getOnlyElement(original.entrySet());\n+    TupleTag<?> replacementTag = Iterables.getOnlyElement(replacement.expand().entrySet()).getKey();\n+    return Collections.singletonMap(\n+        replacement,\n+        ReplacementOutput.of(\n+            TaggedPValue.of(originalElement.getKey(), originalElement.getValue()),\n+            TaggedPValue.of(replacementTag, replacement)));\n   }\n \n   public static Map<PValue, ReplacementOutput> tagged(\n-      List<TaggedPValue> original, POutput replacement) {\n+      Map<TupleTag<?>, PValue> original, POutput replacement) {\n     Map<TupleTag<?>, TaggedPValue> originalTags = new HashMap<>();\n-    for (TaggedPValue value : original) {\n-      TaggedPValue former = originalTags.put(value.getTag(), value);\n-      checkArgument(\n-          former == null || former.equals(value),\n-          \"Found two tags in an expanded output which map to different values: output: %s \"\n-              + \"Values: %s and %s\",\n-          original,\n-          former,\n-          value);\n+    for (Map.Entry<TupleTag<?>, PValue> originalValue : original.entrySet()) {\n+      originalTags.put(\n+          originalValue.getKey(),\n+          TaggedPValue.of(originalValue.getKey(), originalValue.getValue()));\n     }\n     ImmutableMap.Builder<PValue, ReplacementOutput> resultBuilder = ImmutableMap.builder();\n     Set<TupleTag<?>> missingTags = new HashSet<>(originalTags.keySet());\n-    for (TaggedPValue replacementValue : replacement.expand()) {\n-      TaggedPValue mapped = originalTags.get(replacementValue.getTag());\n+    for (Map.Entry<TupleTag<?>, PValue> replacementValue : replacement.expand().entrySet()) {\n+      TaggedPValue mapped = originalTags.get(replacementValue.getKey());\n       checkArgument(\n           mapped != null,\n           \"Missing original output for Tag %s and Value %s Between original %s and replacement %s\",\n-          replacementValue.getTag(),\n+          replacementValue.getKey(),\n           replacementValue.getValue(),\n           original,\n           replacement.expand());\n       resultBuilder.put(\n-          replacementValue.getValue(), ReplacementOutput.of(mapped, replacementValue));\n-      missingTags.remove(replacementValue.getTag());\n+          replacementValue.getValue(),\n+          ReplacementOutput.of(\n+              mapped, TaggedPValue.of(replacementValue.getKey(), replacementValue.getValue())));\n+      missingTags.remove(replacementValue.getKey());\n     }\n     ImmutableMap<PValue, ReplacementOutput> result = resultBuilder.build();\n     checkArgument(",
                "changes": 63
            },
            {
                "status": "added",
                "additions": 195,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
                "deletions": 0,
                "sha": "35af3006d2fe519a2046687799128fc699585767",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
                "patch": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.common.base.Equivalence;\n+import com.google.common.collect.BiMap;\n+import com.google.common.collect.HashBiMap;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.NameUtils;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+\n+/** SDK objects that will be represented at some later point within a {@link Components} object. */\n+class SdkComponents {\n+  private final RunnerApi.Components.Builder componentsBuilder;\n+\n+  private final BiMap<AppliedPTransform<?, ?, ?>, String> transformIds;\n+  private final BiMap<PCollection<?>, String> pCollectionIds;\n+  private final BiMap<WindowingStrategy<?, ?>, String> windowingStrategyIds;\n+\n+  /** A map of Coder to IDs. Coders are stored here with identity equivalence. */\n+  private final BiMap<Equivalence.Wrapper<? extends Coder<?>>, String> coderIds;\n+  // TODO: Specify environments\n+\n+  /** Create a new {@link SdkComponents} with no components. */\n+  static SdkComponents create() {\n+    return new SdkComponents();\n+  }\n+\n+  private SdkComponents() {\n+    this.componentsBuilder = RunnerApi.Components.newBuilder();\n+    this.transformIds = HashBiMap.create();\n+    this.pCollectionIds = HashBiMap.create();\n+    this.windowingStrategyIds = HashBiMap.create();\n+    this.coderIds = HashBiMap.create();\n+  }\n+\n+  /**\n+   * Registers the provided {@link AppliedPTransform} into this {@link SdkComponents}, returning a\n+   * unique ID for the {@link AppliedPTransform}. Multiple registrations of the same\n+   * {@link AppliedPTransform} will return the same unique ID.\n+   *\n+   * <p>All of the children must already be registered within this {@link SdkComponents}.\n+   */\n+  String registerPTransform(\n+      AppliedPTransform<?, ?, ?> appliedPTransform, List<AppliedPTransform<?, ?, ?>> children)\n+      throws IOException {\n+    String name = getApplicationName(appliedPTransform);\n+    // If this transform is present in the components, nothing to do. return the existing name.\n+    // Otherwise the transform must be translated and added to the components.\n+    if (componentsBuilder.getTransformsOrDefault(name, null) != null) {\n+      return name;\n+    }\n+    checkNotNull(children, \"child nodes may not be null\");\n+    componentsBuilder.putTransforms(name, PTransforms.toProto(appliedPTransform, children, this));\n+    return name;\n+  }\n+\n+  /**\n+   * Gets the ID for the provided {@link AppliedPTransform}. The provided {@link AppliedPTransform}\n+   * will not be added to the components produced by this {@link SdkComponents} until it is\n+   * translated via {@link #registerPTransform(AppliedPTransform, List)}.\n+   */\n+  private String getApplicationName(AppliedPTransform<?, ?, ?> appliedPTransform) {\n+    String existing = transformIds.get(appliedPTransform);\n+    if (existing != null) {\n+      return existing;\n+    }\n+\n+    String name = appliedPTransform.getFullName();\n+    if (name.isEmpty()) {\n+      name = \"unnamed-ptransform\";\n+    }\n+    name = uniqify(name, transformIds.values());\n+    transformIds.put(appliedPTransform, name);\n+    return name;\n+  }\n+\n+  String getExistingPTransformId(AppliedPTransform<?, ?, ?> appliedPTransform) {\n+    checkArgument(\n+        transformIds.containsKey(appliedPTransform),\n+        \"%s %s has not been previously registered\",\n+        AppliedPTransform.class.getSimpleName(),\n+        appliedPTransform);\n+    return transformIds.get(appliedPTransform);\n+  }\n+\n+  /**\n+   * Registers the provided {@link PCollection} into this {@link SdkComponents}, returning a unique\n+   * ID for the {@link PCollection}. Multiple registrations of the same {@link PCollection} will\n+   * return the same unique ID.\n+   */\n+  String registerPCollection(PCollection<?> pCollection) throws IOException {\n+    String existing = pCollectionIds.get(pCollection);\n+    if (existing != null) {\n+      return existing;\n+    }\n+    String uniqueName = uniqify(pCollection.getName(), pCollectionIds.values());\n+    pCollectionIds.put(pCollection, uniqueName);\n+    componentsBuilder.putPcollections(uniqueName, PCollections.toProto(pCollection, this));\n+    return uniqueName;\n+  }\n+\n+  /**\n+   * Registers the provided {@link WindowingStrategy} into this {@link SdkComponents}, returning a\n+   * unique ID for the {@link WindowingStrategy}. Multiple registrations of the same {@link\n+   * WindowingStrategy} will return the same unique ID.\n+   */\n+  String registerWindowingStrategy(WindowingStrategy<?, ?> windowingStrategy) throws IOException {\n+    String existing = windowingStrategyIds.get(windowingStrategy);\n+    if (existing != null) {\n+      return existing;\n+    }\n+    String baseName =\n+        String.format(\n+            \"%s(%s)\",\n+            NameUtils.approximateSimpleName(windowingStrategy),\n+            NameUtils.approximateSimpleName(windowingStrategy.getWindowFn()));\n+    String name = uniqify(baseName, windowingStrategyIds.values());\n+    windowingStrategyIds.put(windowingStrategy, name);\n+    RunnerApi.WindowingStrategy windowingStrategyProto =\n+        WindowingStrategies.toProto(windowingStrategy, this);\n+    componentsBuilder.putWindowingStrategies(name, windowingStrategyProto);\n+    return name;\n+  }\n+\n+  /**\n+   * Registers the provided {@link Coder} into this {@link SdkComponents}, returning a unique ID for\n+   * the {@link Coder}. Multiple registrations of the same {@link Coder} will return the same\n+   * unique ID.\n+   *\n+   * <p>Coders are stored by identity to ensure that coders with implementations of {@link\n+   * #equals(Object)} and {@link #hashCode()} but incompatible binary formats are not considered the\n+   * same coder.\n+   */\n+  String registerCoder(Coder<?> coder) throws IOException {\n+    String existing = coderIds.get(Equivalence.identity().wrap(coder));\n+    if (existing != null) {\n+      return existing;\n+    }\n+    String baseName = NameUtils.approximateSimpleName(coder);\n+    String name = uniqify(baseName, coderIds.values());\n+    coderIds.put(Equivalence.identity().wrap(coder), name);\n+    RunnerApi.Coder coderProto = Coders.toProto(coder, this);\n+    componentsBuilder.putCoders(name, coderProto);\n+    return name;\n+  }\n+\n+  private String uniqify(String baseName, Set<String> existing) {\n+    String name = baseName;\n+    int increment = 1;\n+    while (existing.contains(name)) {\n+      name = baseName + Integer.toString(increment);\n+      increment++;\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Convert this {@link SdkComponents} into a {@link RunnerApi.Components}, including all of the\n+   * contained {@link Coder coders}, {@link WindowingStrategy windowing strategies}, {@link\n+   * PCollection PCollections}, and {@link PTransform PTransforms}.\n+   */\n+  @Experimental\n+  RunnerApi.Components toComponents() {\n+    return componentsBuilder.build();\n+  }\n+}",
                "changes": 195
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
                "deletions": 11,
                "sha": "7a59c1c1c80b124271c3901a140138ba443d9565",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
                "patch": "@@ -18,33 +18,25 @@\n \n package org.apache.beam.runners.core.construction;\n \n-import com.google.common.collect.Iterables;\n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} which consumes from a {@link PValue} and produces a\n- * {@link PValue}. {@link #getInput(List, Pipeline)} and {@link #mapOutputs(List, PValue)} are\n+ * {@link PValue}. {@link #mapOutputs(Map, PValue)} is\n  * implemented.\n  */\n public abstract class SingleInputOutputOverrideFactory<\n         InputT extends PValue,\n         OutputT extends PValue,\n         TransformT extends PTransform<InputT, OutputT>>\n     implements PTransformOverrideFactory<InputT, OutputT, TransformT> {\n-  @Override\n-  public final InputT getInput(List<TaggedPValue> inputs, Pipeline p) {\n-    return (InputT) Iterables.getOnlyElement(inputs).getValue();\n-  }\n-\n   @Override\n   public final Map<PValue, ReplacementOutput> mapOutputs(\n-      List<TaggedPValue> outputs, OutputT newOutput) {\n+      Map<TupleTag<?>, PValue> outputs, OutputT newOutput) {\n     return ReplacementOutputs.singleton(outputs, newOutput);\n   }\n }",
                "changes": 14
            },
            {
                "status": "renamed",
                "additions": 19,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/windowing/Triggers.java",
                "deletions": 3,
                "sha": "81f738da0dd5bc06b3b89f3d8d07dd2e8a2ad4a8",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.transforms.windowing;\n+package org.apache.beam.runners.core.construction;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.collect.Lists;\n@@ -25,8 +25,22 @@\n import java.util.List;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.transforms.windowing.AfterAll;\n+import org.apache.beam.sdk.transforms.windowing.AfterEach;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterSynchronizedProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n import org.apache.beam.sdk.transforms.windowing.AfterWatermark.AfterWatermarkEarlyAndLate;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark.FromEndOfWindow;\n+import org.apache.beam.sdk.transforms.windowing.DefaultTrigger;\n+import org.apache.beam.sdk.transforms.windowing.Never;\n import org.apache.beam.sdk.transforms.windowing.Never.NeverTrigger;\n+import org.apache.beam.sdk.transforms.windowing.OrFinallyTrigger;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.TimestampTransform;\n+import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.apache.beam.sdk.transforms.windowing.Trigger.OnceTrigger;\n import org.apache.beam.sdk.util.ReshuffleTrigger;\n import org.apache.beam.sdk.util.TimeDomain;\n@@ -84,7 +98,7 @@ private Method getEvaluationMethod(Class<?> clazz) {\n           .build();\n     }\n \n-    private RunnerApi.Trigger convertSpecific(AfterWatermark.FromEndOfWindow v) {\n+    private RunnerApi.Trigger convertSpecific(FromEndOfWindow v) {\n       return RunnerApi.Trigger.newBuilder()\n           .setAfterEndOfWindow(RunnerApi.Trigger.AfterEndOfWindow.newBuilder())\n           .build();\n@@ -149,7 +163,7 @@ private Method getEvaluationMethod(Class<?> clazz) {\n           .build();\n     }\n \n-    private RunnerApi.Trigger convertSpecific(AfterWatermark.AfterWatermarkEarlyAndLate v) {\n+    private RunnerApi.Trigger convertSpecific(AfterWatermarkEarlyAndLate v) {\n       RunnerApi.Trigger.AfterEndOfWindow.Builder builder =\n           RunnerApi.Trigger.AfterEndOfWindow.newBuilder();\n \n@@ -287,6 +301,8 @@ public static Trigger fromProto(RunnerApi.Trigger triggerProto) {\n         return trigger;\n       case AFTER_SYNCHRONIZED_PROCESSING_TIME:\n         return AfterSynchronizedProcessingTime.ofFirstElement();\n+      case ALWAYS:\n+        return new ReshuffleTrigger();\n       case ELEMENT_COUNT:\n         return AfterPane.elementCountAtLeast(triggerProto.getElementCount().getElementCount());\n       case NEVER:",
                "changes": 22
            },
            {
                "status": "renamed",
                "additions": 13,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
                "previous_filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSource.java",
                "deletions": 6,
                "sha": "f67af8a6ea070d83831eebfc4a517317d86140b3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.core;\n+package org.apache.beam.runners.core.construction;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkNotNull;\n@@ -61,7 +61,8 @@\n /**\n  * {@link PTransform} that converts a {@link BoundedSource} as an {@link UnboundedSource}.\n  *\n- * <p>{@link BoundedSource} is read directly without calling {@link BoundedSource#splitIntoBundles},\n+ * <p>{@link BoundedSource} is read directly without calling\n+ * {@link BoundedSource#split},\n  * and element timestamps are propagated. While any elements remain, the watermark is the beginning\n  * of time {@link BoundedWindow#TIMESTAMP_MIN_VALUE}, and after all elements have been produced\n  * the watermark goes to the end of time {@link BoundedWindow#TIMESTAMP_MAX_VALUE}.\n@@ -130,7 +131,7 @@ public void validate() {\n     }\n \n     @Override\n-    public List<BoundedToUnboundedSourceAdapter<T>> generateInitialSplits(\n+    public List<BoundedToUnboundedSourceAdapter<T>> split(\n         int desiredNumSplits, PipelineOptions options) throws Exception {\n       try {\n         long desiredBundleSize = boundedSource.getEstimatedSizeBytes(options) / desiredNumSplits;\n@@ -140,7 +141,7 @@ public void validate() {\n           return ImmutableList.of(this);\n         }\n         List<? extends BoundedSource<T>> splits =\n-            boundedSource.splitIntoBundles(desiredBundleSize, options);\n+            boundedSource.split(desiredBundleSize, options);\n         if (splits == null) {\n           LOG.warn(\"BoundedSource cannot split {}, skips the initial splits.\", boundedSource);\n           return ImmutableList.of(this);\n@@ -458,22 +459,28 @@ Instant getCurrentTimestamp() {\n       private PipelineOptions options;\n       private @Nullable BoundedReader<T> reader;\n       private boolean closed;\n+      private boolean readerDone;\n \n       public ResidualSource(BoundedSource<T> residualSource, PipelineOptions options) {\n         this.residualSource = checkNotNull(residualSource, \"residualSource\");\n         this.options = checkNotNull(options, \"options\");\n         this.reader = null;\n         this.closed = false;\n+        this.readerDone = false;\n       }\n \n       private boolean advance() throws IOException {\n         checkArgument(!closed, \"advance() call on closed %s\", getClass().getName());\n+        if (readerDone) {\n+          return false;\n+        }\n         if (reader == null) {\n           reader = residualSource.createReader(options);\n-          return reader.start();\n+          readerDone = !reader.start();\n         } else {\n-          return reader.advance();\n+          readerDone = !reader.advance();\n         }\n+        return !readerDone;\n       }\n \n       T getCurrent() throws NoSuchElementException {",
                "changes": 19
            },
            {
                "status": "added",
                "additions": 72,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
                "deletions": 0,
                "sha": "c191eeb8617d402c54ee6f663eaa804907f82683",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
                "patch": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.runners.TransformHierarchy.Node;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n+\n+/**\n+ * Utilities for ensuring that all {@link Read} {@link PTransform PTransforms} are consumed by some\n+ * {@link PTransform}.\n+ */\n+public class UnconsumedReads {\n+  public static void ensureAllReadsConsumed(Pipeline pipeline) {\n+    final Set<PCollection<?>> unconsumed = new HashSet<>();\n+    pipeline.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            unconsumed.removeAll(node.getInputs().values());\n+          }\n+\n+          @Override\n+          public void visitValue(PValue value, Node producer) {\n+            if (producer.getTransform() instanceof Read.Bounded\n+                || producer.getTransform() instanceof Read.Unbounded) {\n+              unconsumed.add((PCollection<?>) value);\n+            }\n+          }\n+        });\n+    int i = 0;\n+    for (PCollection<?> unconsumedPCollection : unconsumed) {\n+      consume(unconsumedPCollection, i);\n+      i++;\n+    }\n+  }\n+\n+  private static <T> void consume(PCollection<T> unconsumedPCollection, int uniq) {\n+    // Multiple applications should never break due to stable unique names.\n+    String uniqueName = \"DropInputs\" + (uniq == 0 ? \"\" : uniq);\n+    unconsumedPCollection.apply(uniqueName, ParDo.of(new NoOpDoFn<T>()));\n+  }\n+\n+  private static class NoOpDoFn<T> extends DoFn<T, T> {\n+    @ProcessElement\n+    public void doNothing(ProcessContext context) {}\n+  }\n+}",
                "changes": 72
            },
            {
                "status": "modified",
                "additions": 8,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
                "deletions": 12,
                "sha": "efafa33b3513e1fe1cddbfd456856653e621d26b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
                "patch": "@@ -18,20 +18,19 @@\n \n package org.apache.beam.runners.core.construction;\n \n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} that throws an exception when a call to\n- * {@link #getReplacementTransform(PTransform)} is made. This is for {@link PTransform PTransforms}\n- * which are not supported by a runner.\n+ * {@link #getReplacementTransform(AppliedPTransform)} is made. This is for\n+ * {@link PTransform PTransforms} which are not supported by a runner.\n  */\n public final class UnsupportedOverrideFactory<\n         InputT extends PInput,\n@@ -55,17 +54,14 @@ private UnsupportedOverrideFactory(String message) {\n   }\n \n   @Override\n-  public PTransform<InputT, OutputT> getReplacementTransform(TransformT transform) {\n+  public PTransformReplacement<InputT, OutputT> getReplacementTransform(\n+      AppliedPTransform<InputT, OutputT, TransformT> transform) {\n     throw new UnsupportedOperationException(message);\n   }\n \n   @Override\n-  public InputT getInput(List<TaggedPValue> inputs, Pipeline p) {\n-    throw new UnsupportedOperationException(message);\n-  }\n-\n-  @Override\n-  public Map<PValue, ReplacementOutput> mapOutputs(List<TaggedPValue> outputs, OutputT newOutput) {\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, OutputT newOutput) {\n     throw new UnsupportedOperationException(message);\n   }\n }",
                "changes": 20
            },
            {
                "status": "renamed",
                "additions": 46,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/WindowingStrategies.java",
                "deletions": 67,
                "sha": "3d7deef1d7b84768091b51cf5dd4ba35174d7726",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
                "patch": "@@ -15,30 +15,27 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core.construction;\n \n import static com.google.common.base.Preconditions.checkArgument;\n \n-import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.protobuf.Any;\n import com.google.protobuf.ByteString;\n import com.google.protobuf.BytesValue;\n import com.google.protobuf.InvalidProtocolBufferException;\n import java.io.IOException;\n import java.io.Serializable;\n-import java.util.UUID;\n-import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi.FunctionSpec;\n-import org.apache.beam.sdk.common.runner.v1.RunnerApi.UrnWithParameter;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.SdkFunctionSpec;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.SerializableUtils;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n import org.apache.beam.sdk.util.WindowingStrategy.CombineWindowFnOutputTimes;\n import org.joda.time.Duration;\n@@ -82,7 +79,7 @@ public static AccumulationMode fromProto(RunnerApi.AccumulationMode proto) {\n     }\n   }\n \n-  public static RunnerApi.ClosingBehavior toProto(Window.ClosingBehavior closingBehavior) {\n+  public static RunnerApi.ClosingBehavior toProto(ClosingBehavior closingBehavior) {\n     switch (closingBehavior) {\n       case FIRE_ALWAYS:\n         return RunnerApi.ClosingBehavior.EMIT_ALWAYS;\n@@ -126,62 +123,30 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n     }\n   }\n \n-  // This URN says that the coder is just a UDF blob the indicated SDK understands\n-  // TODO: standardize such things\n-  private static final String CUSTOM_CODER_URN = \"urn:beam:coders:javasdk:0.1\";\n-\n   // This URN says that the WindowFn is just a UDF blob the indicated SDK understands\n   // TODO: standardize such things\n-  private static final String CUSTOM_WINDOWFN_URN = \"urn:beam:windowfn:javasdk:0.1\";\n-\n-  private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n+  public static final String CUSTOM_WINDOWFN_URN = \"urn:beam:windowfn:javasdk:0.1\";\n \n   /**\n-   * Converts a {@link WindowFn} into a {@link RunnerApi.MessageWithComponents} where\n-   * {@link RunnerApi.MessageWithComponents#getFunctionSpec()} is a {@link RunnerApi.FunctionSpec}\n-   * for the input {@link WindowFn}.\n+   * Converts a {@link WindowFn} into a {@link RunnerApi.MessageWithComponents} where {@link\n+   * RunnerApi.MessageWithComponents#getFunctionSpec()} is a {@link RunnerApi.FunctionSpec} for the\n+   * input {@link WindowFn}.\n    */\n-  public static RunnerApi.MessageWithComponents toProto(WindowFn<?, ?> windowFn)\n+  public static SdkFunctionSpec toProto(\n+      WindowFn<?, ?> windowFn, @SuppressWarnings(\"unused\") SdkComponents components)\n       throws IOException {\n-    Coder<?> windowCoder = windowFn.windowCoder();\n-\n-    // TODO: re-use components\n-    String windowCoderId = UUID.randomUUID().toString();\n-\n-    RunnerApi.FunctionSpec windowFnSpec =\n-        RunnerApi.FunctionSpec.newBuilder()\n-            .setSpec(\n-                UrnWithParameter.newBuilder()\n-                    .setUrn(CUSTOM_WINDOWFN_URN)\n-                    .setParameter(\n-                        Any.pack(\n-                            BytesValue.newBuilder()\n-                                .setValue(\n-                                    ByteString.copyFrom(\n-                                        SerializableUtils.serializeToByteArray(windowFn)))\n-                                .build())))\n-            .build();\n-\n-    RunnerApi.Coder windowCoderProto =\n-        RunnerApi.Coder.newBuilder()\n-            .setSpec(\n-                FunctionSpec.newBuilder()\n-                    .setSpec(\n-                        UrnWithParameter.newBuilder()\n-                            .setUrn(CUSTOM_CODER_URN)\n-                            .setParameter(\n-                                Any.pack(\n-                                    BytesValue.newBuilder()\n-                                        .setValue(\n-                                            ByteString.copyFrom(\n-                                                OBJECT_MAPPER.writeValueAsBytes(\n-                                                    windowCoder.asCloudObject())))\n-                                        .build()))))\n-            .build();\n-\n-    return RunnerApi.MessageWithComponents.newBuilder()\n-        .setFunctionSpec(windowFnSpec)\n-        .setComponents(Components.newBuilder().putCoders(windowCoderId, windowCoderProto))\n+    return SdkFunctionSpec.newBuilder()\n+        // TODO: Set environment ID\n+        .setSpec(\n+            FunctionSpec.newBuilder()\n+                .setUrn(CUSTOM_WINDOWFN_URN)\n+                .setParameter(\n+                    Any.pack(\n+                        BytesValue.newBuilder()\n+                            .setValue(\n+                                ByteString.copyFrom(\n+                                    SerializableUtils.serializeToByteArray(windowFn)))\n+                            .build())))\n         .build();\n   }\n \n@@ -193,9 +158,22 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n    */\n   public static RunnerApi.MessageWithComponents toProto(WindowingStrategy<?, ?> windowingStrategy)\n       throws IOException {\n+    SdkComponents components = SdkComponents.create();\n+    RunnerApi.WindowingStrategy windowingStrategyProto = toProto(windowingStrategy, components);\n \n-    RunnerApi.MessageWithComponents windowFnWithComponents =\n-        toProto(windowingStrategy.getWindowFn());\n+    return RunnerApi.MessageWithComponents.newBuilder()\n+        .setWindowingStrategy(windowingStrategyProto)\n+        .setComponents(components.toComponents())\n+        .build();\n+  }\n+\n+  /**\n+   * Converts a {@link WindowingStrategy} into a {@link RunnerApi.WindowingStrategy}, registering\n+   * any components in the provided {@link SdkComponents}.\n+   */\n+  public static RunnerApi.WindowingStrategy toProto(\n+      WindowingStrategy<?, ?> windowingStrategy, SdkComponents components) throws IOException {\n+    SdkFunctionSpec windowFnSpec = toProto(windowingStrategy.getWindowFn(), components);\n \n     RunnerApi.WindowingStrategy.Builder windowingStrategyProto =\n         RunnerApi.WindowingStrategy.newBuilder()\n@@ -204,15 +182,15 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n             .setClosingBehavior(toProto(windowingStrategy.getClosingBehavior()))\n             .setAllowedLateness(windowingStrategy.getAllowedLateness().getMillis())\n             .setTrigger(Triggers.toProto(windowingStrategy.getTrigger()))\n-            .setWindowFn(windowFnWithComponents.getFunctionSpec());\n+            .setWindowFn(windowFnSpec)\n+            .setWindowCoderId(\n+                components.registerCoder(windowingStrategy.getWindowFn().windowCoder()));\n \n-    return RunnerApi.MessageWithComponents.newBuilder()\n-        .setWindowingStrategy(windowingStrategyProto)\n-        .setComponents(windowFnWithComponents.getComponents()).build();\n+    return windowingStrategyProto.build();\n   }\n \n   /**\n-   * Converts from a {@link RunnerApi.WindowingStrategy} accompanied by {@link RunnerApi.Components}\n+   * Converts from a {@link RunnerApi.WindowingStrategy} accompanied by {@link Components}\n    * to the SDK's {@link WindowingStrategy}.\n    */\n   public static WindowingStrategy<?, ?> fromProto(RunnerApi.MessageWithComponents proto)\n@@ -233,15 +211,16 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n    * the provided components to dereferences identifiers found in the proto.\n    */\n   public static WindowingStrategy<?, ?> fromProto(\n-      RunnerApi.WindowingStrategy proto, RunnerApi.Components components)\n+      RunnerApi.WindowingStrategy proto, Components components)\n       throws InvalidProtocolBufferException {\n \n-    FunctionSpec windowFnSpec = proto.getWindowFn();\n+    SdkFunctionSpec windowFnSpec = proto.getWindowFn();\n \n     checkArgument(\n         windowFnSpec.getSpec().getUrn().equals(CUSTOM_WINDOWFN_URN),\n         \"Only Java-serialized %s instances are supported, with URN %s. But found URN %s\",\n         WindowFn.class.getSimpleName(),\n+        CUSTOM_WINDOWFN_URN,\n         windowFnSpec.getSpec().getUrn());\n \n     Object deserializedWindowFn =",
                "changes": 113
            },
            {
                "status": "added",
                "additions": 163,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java",
                "deletions": 0,
                "sha": "b2b99558922aa247a460a17a96eded32b45a2dbe",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java",
                "patch": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderException;\n+import org.apache.beam.sdk.coders.CustomCoder;\n+import org.apache.beam.sdk.coders.IterableCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.SerializableCoder;\n+import org.apache.beam.sdk.coders.StandardCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow.IntervalWindowCoder;\n+import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n+import org.hamcrest.Matchers;\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+/** Tests for {@link Coders}. */\n+@RunWith(Enclosed.class)\n+public class CodersTest {\n+  private static final Set<StandardCoder<?>> KNOWN_CODERS =\n+      ImmutableSet.<StandardCoder<?>>builder()\n+          .add(ByteArrayCoder.of())\n+          .add(KvCoder.of(VarLongCoder.of(), VarLongCoder.of()))\n+          .add(VarLongCoder.of())\n+          .add(IntervalWindowCoder.of())\n+          .add(IterableCoder.of(ByteArrayCoder.of()))\n+          .add(GlobalWindow.Coder.INSTANCE)\n+          .add(\n+              FullWindowedValueCoder.of(\n+                  IterableCoder.of(VarLongCoder.of()), IntervalWindowCoder.of()))\n+          .build();\n+\n+  /**\n+   * Tests that all known coders are present in the parameters that will be used by\n+   * {@link ToFromProtoTest}.\n+   */\n+  @RunWith(JUnit4.class)\n+  public static class ValidateKnownCodersPresentTest {\n+    @Test\n+    public void validateKnownCoders() {\n+      // Validates that every known coder in the Coders class is represented in a \"Known Coder\"\n+      // tests, which demonstrates that they are serialized via components and specified URNs rather\n+      // than java serialized\n+      Set<Class<? extends StandardCoder>> knownCoderClasses = Coders.KNOWN_CODER_URNS.keySet();\n+      Set<Class<? extends StandardCoder>> knownCoderTests = new HashSet<>();\n+      for (StandardCoder<?> coder : KNOWN_CODERS) {\n+        knownCoderTests.add(coder.getClass());\n+      }\n+      Set<Class<? extends StandardCoder>> missingKnownCoders = new HashSet<>(knownCoderClasses);\n+      missingKnownCoders.removeAll(knownCoderTests);\n+      checkState(\n+          missingKnownCoders.isEmpty(),\n+          \"Missing validation of known coder %s in %s\",\n+          missingKnownCoders,\n+          CodersTest.class.getSimpleName());\n+    }\n+  }\n+\n+\n+  /**\n+   * Tests round-trip coder encodings for both known and unknown {@link Coder coders}.\n+   */\n+  @RunWith(Parameterized.class)\n+  public static class ToFromProtoTest {\n+    @Parameters(name = \"{index}: {0}\")\n+    public static Iterable<Coder<?>> data() {\n+      return ImmutableList.<Coder<?>>builder()\n+          .addAll(KNOWN_CODERS)\n+          .add(\n+              StringUtf8Coder.of(),\n+              SerializableCoder.of(Record.class),\n+              new RecordCoder(),\n+              KvCoder.of(new RecordCoder(), AvroCoder.of(Record.class)))\n+          .build();\n+    }\n+\n+    @Parameter(0)\n+    public Coder<?> coder;\n+\n+    @Test\n+    public void toAndFromProto() throws Exception {\n+      SdkComponents componentsBuilder = SdkComponents.create();\n+      RunnerApi.Coder coderProto = Coders.toProto(coder, componentsBuilder);\n+\n+      Components encodedComponents = componentsBuilder.toComponents();\n+      Coder<?> decodedCoder = Coders.fromProto(coderProto, encodedComponents);\n+      assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(coder));\n+\n+      if (KNOWN_CODERS.contains(coder)) {\n+        for (RunnerApi.Coder encodedCoder : encodedComponents.getCodersMap().values()) {\n+          assertThat(\n+              encodedCoder.getSpec().getSpec().getUrn(), not(equalTo(Coders.CUSTOM_CODER_URN)));\n+        }\n+      }\n+    }\n+\n+    static class Record implements Serializable {}\n+\n+    private static class RecordCoder extends CustomCoder<Record> {\n+      @Override\n+      public void encode(Record value, OutputStream outStream, Context context)\n+          throws CoderException, IOException {}\n+\n+      @Override\n+      public Record decode(InputStream inStream, Context context)\n+          throws CoderException, IOException {\n+        return new Record();\n+      }\n+\n+      @Override\n+      public boolean equals(Object other) {\n+        return other != null && getClass().equals(other.getClass());\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+        return getClass().hashCode();\n+      }\n+    }\n+  }\n+}",
                "changes": 163
            },
            {
                "status": "added",
                "additions": 104,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java",
                "deletions": 0,
                "sha": "4e08c213ff1f61378d27afe4dcd671df0aae9700",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java",
                "patch": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import org.apache.beam.runners.core.construction.DeduplicatedFlattenFactory.FlattenWithoutDuplicateInputs;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor.Defaults;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n+import org.apache.beam.sdk.runners.TransformHierarchy;\n+import org.apache.beam.sdk.testing.NeedsRunner;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TaggedPValue;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link DeduplicatedFlattenFactory}.\n+ */\n+@RunWith(JUnit4.class)\n+public class DeduplicatedFlattenFactoryTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create();\n+\n+  private PCollection<String> first = pipeline.apply(\"FirstCreate\", Create.of(\"one\"));\n+  private PCollection<String> second = pipeline.apply(\"SecondCreate\", Create.of(\"two\"));\n+  private DeduplicatedFlattenFactory<String> factory = DeduplicatedFlattenFactory.create();\n+\n+  @Test\n+  public void duplicatesInsertsMultipliers() {\n+    PTransform<PCollectionList<String>, PCollection<String>> replacement =\n+        new DeduplicatedFlattenFactory.FlattenWithoutDuplicateInputs<>();\n+    final PCollectionList<String> inputList =\n+        PCollectionList.of(first).and(second).and(first).and(first);\n+    inputList.apply(replacement);\n+    pipeline.traverseTopologically(\n+        new Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n+            if (node.getTransform() instanceof Flatten.PCollections) {\n+              assertThat(node.getInputs(), not(equalTo(inputList.expand())));\n+            }\n+          }\n+        });\n+  }\n+\n+  @Test\n+  @Category(NeedsRunner.class)\n+  public void testOverride() {\n+    final PCollectionList<String> inputList =\n+        PCollectionList.of(first).and(second).and(first).and(first);\n+    PTransform<PCollectionList<String>, PCollection<String>> replacement =\n+        new FlattenWithoutDuplicateInputs<>();\n+    PCollection<String> flattened = inputList.apply(replacement);\n+\n+    PAssert.that(flattened).containsInAnyOrder(\"one\", \"two\", \"one\", \"one\");\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void outputMapping() {\n+    final PCollectionList<String> inputList =\n+        PCollectionList.of(first).and(second).and(first).and(first);\n+    PCollection<String> original =\n+        inputList.apply(Flatten.<String>pCollections());\n+    PCollection<String> replacement = inputList.apply(new FlattenWithoutDuplicateInputs<String>());\n+\n+    assertThat(\n+        factory.mapOutputs(original.expand(), replacement),\n+        Matchers.<PValue, ReplacementOutput>hasEntry(\n+            replacement,\n+            ReplacementOutput.of(\n+                TaggedPValue.ofExpandedValue(original),\n+                TaggedPValue.ofExpandedValue(replacement))));\n+  }\n+}",
                "changes": 104
            },
            {
                "status": "added",
                "additions": 122,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java",
                "deletions": 0,
                "sha": "ae2d0a9d17659d761fd0222032fd19b103d1728a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java",
                "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.junit.Assert.assertThat;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.PTransformReplacement;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n+import org.apache.beam.sdk.testing.NeedsRunner;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.PCollections;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link EmptyFlattenAsCreateFactory}.\n+ */\n+@RunWith(JUnit4.class)\n+public class EmptyFlattenAsCreateFactoryTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  private EmptyFlattenAsCreateFactory<Long> factory = EmptyFlattenAsCreateFactory.instance();\n+\n+  @Test\n+  public void getInputEmptySucceeds() {\n+    PTransformReplacement<PCollectionList<Long>, PCollection<Long>> replacement =\n+        factory.getReplacementTransform(\n+            AppliedPTransform.<PCollectionList<Long>, PCollection<Long>, PCollections<Long>>of(\n+                \"nonEmptyInput\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Flatten.<Long>pCollections(),\n+                pipeline));\n+    assertThat(replacement.getInput().getAll(), emptyIterable());\n+  }\n+\n+  @Test\n+  public void getInputNonEmptyThrows() {\n+    PCollectionList<Long> nonEmpty =\n+        PCollectionList.of(pipeline.apply(CountingInput.unbounded()))\n+            .and(pipeline.apply(CountingInput.upTo(100L)));\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(nonEmpty.expand().toString());\n+    thrown.expectMessage(EmptyFlattenAsCreateFactory.class.getSimpleName());\n+    factory.getReplacementTransform(\n+        AppliedPTransform.<PCollectionList<Long>, PCollection<Long>, Flatten.PCollections<Long>>of(\n+            \"nonEmptyInput\",\n+            nonEmpty.expand(),\n+            Collections.<TupleTag<?>, PValue>emptyMap(),\n+            Flatten.<Long>pCollections(),\n+            pipeline));\n+  }\n+\n+  @Test\n+  public void mapOutputsSucceeds() {\n+    PCollection<Long> original = pipeline.apply(\"Original\", CountingInput.unbounded());\n+    PCollection<Long> replacement = pipeline.apply(\"Replacement\", CountingInput.unbounded());\n+    Map<PValue, ReplacementOutput> mapping = factory.mapOutputs(original.expand(), replacement);\n+\n+    assertThat(\n+        mapping,\n+        Matchers.<PValue, ReplacementOutput>hasEntry(\n+            replacement,\n+            ReplacementOutput.of(\n+                TaggedPValue.ofExpandedValue(original),\n+                TaggedPValue.ofExpandedValue(replacement))));\n+  }\n+\n+  @Test\n+  @Category(NeedsRunner.class)\n+  public void testOverride() {\n+    PCollectionList<Long> empty = PCollectionList.empty(pipeline);\n+    PCollection<Long> emptyFlattened =\n+        empty.apply(\n+            factory\n+                .getReplacementTransform(\n+                    AppliedPTransform\n+                        .<PCollectionList<Long>, PCollection<Long>, Flatten.PCollections<Long>>of(\n+                            \"nonEmptyInput\",\n+                            Collections.<TupleTag<?>, PValue>emptyMap(),\n+                            Collections.<TupleTag<?>, PValue>emptyMap(),\n+                            Flatten.<Long>pCollections(),\n+                            pipeline))\n+                .getTransform());\n+    PAssert.that(emptyFlattened).empty();\n+    pipeline.run();\n+  }\n+}",
                "changes": 122
            },
            {
                "status": "renamed",
                "additions": 15,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java",
                "previous_filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ForwardingPTransformTest.java",
                "deletions": 16,
                "sha": "7d3bfd8c6a8492a01d73bb5bd65ff063f0277755",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java",
                "patch": "@@ -15,14 +15,11 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.direct;\n+package org.apache.beam.runners.core.construction;\n \n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Matchers.any;\n-import static org.mockito.Mockito.doThrow;\n-import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.when;\n \n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.PTransform;\n@@ -35,6 +32,7 @@\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n import org.mockito.Mock;\n+import org.mockito.Mockito;\n import org.mockito.MockitoAnnotations;\n \n /**\n@@ -63,26 +61,26 @@ public void setup() {\n   @Test\n   public void applyDelegates() {\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<Integer> collection = mock(PCollection.class);\n+    PCollection<Integer> collection = Mockito.mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<String> output = mock(PCollection.class);\n-    when(delegate.expand(collection)).thenReturn(output);\n+    PCollection<String> output = Mockito.mock(PCollection.class);\n+    Mockito.when(delegate.expand(collection)).thenReturn(output);\n     PCollection<String> result = forwarding.expand(collection);\n     assertThat(result, equalTo(output));\n   }\n \n   @Test\n   public void getNameDelegates() {\n     String name = \"My_forwardingptransform-name;for!thisTest\";\n-    when(delegate.getName()).thenReturn(name);\n+    Mockito.when(delegate.getName()).thenReturn(name);\n     assertThat(forwarding.getName(), equalTo(name));\n   }\n \n   @Test\n   public void validateDelegates() {\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<Integer> input = mock(PCollection.class);\n-    doThrow(RuntimeException.class).when(delegate).validate(input);\n+    PCollection<Integer> input = Mockito.mock(PCollection.class);\n+    Mockito.doThrow(RuntimeException.class).when(delegate).validate(input);\n \n     thrown.expect(RuntimeException.class);\n     forwarding.validate(input);\n@@ -91,20 +89,21 @@ public void validateDelegates() {\n   @Test\n   public void getDefaultOutputCoderDelegates() throws Exception {\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<Integer> input = mock(PCollection.class);\n+    PCollection<Integer> input = Mockito.mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<String> output = mock(PCollection.class);\n+    PCollection<String> output = Mockito.mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n-    Coder<String> outputCoder = mock(Coder.class);\n+    Coder<String> outputCoder = Mockito.mock(Coder.class);\n \n-    when(delegate.getDefaultOutputCoder(input, output)).thenReturn(outputCoder);\n+    Mockito.when(delegate.getDefaultOutputCoder(input, output)).thenReturn(outputCoder);\n     assertThat(forwarding.getDefaultOutputCoder(input, output), equalTo(outputCoder));\n   }\n \n   @Test\n   public void populateDisplayDataDelegates() {\n-    doThrow(RuntimeException.class)\n-        .when(delegate).populateDisplayData(any(DisplayData.Builder.class));\n+    Mockito.doThrow(RuntimeException.class)\n+        .when(delegate)\n+        .populateDisplayData(any(DisplayData.Builder.class));\n \n     thrown.expect(RuntimeException.class);\n     DisplayData.from(forwarding);",
                "changes": 31
            },
            {
                "status": "added",
                "additions": 188,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java",
                "deletions": 0,
                "sha": "636d2459293b7610f3675db7ab09c370733adee2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java",
                "patch": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.auto.value.AutoValue;\n+import com.google.common.collect.ImmutableList;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.Collection;\n+import java.util.Collections;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CustomCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.NonMergingWindowFn;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n+import org.apache.beam.sdk.util.VarInt;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollection.IsBounded;\n+import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+/**\n+ * Tests for {@link PCollections}.\n+ */\n+@RunWith(Parameterized.class)\n+public class PCollectionsTest {\n+  // Each spec activates tests of all subsets of its fields\n+  @Parameters(name = \"{index}: {0}\")\n+  public static Iterable<PCollection<?>> data() {\n+    Pipeline pipeline = TestPipeline.create();\n+    PCollection<Integer> ints = pipeline.apply(\"ints\", Create.of(1, 2, 3));\n+    PCollection<Long> longs = pipeline.apply(\"unbounded longs\", CountingInput.unbounded());\n+    PCollection<Long> windowedLongs =\n+        longs.apply(\n+            \"into fixed windows\",\n+            Window.<Long>into(FixedWindows.of(Duration.standardMinutes(10L))));\n+    PCollection<KV<String, Iterable<String>>> groupedStrings =\n+        pipeline\n+            .apply(\n+                \"kvs\", Create.of(KV.of(\"foo\", \"spam\"), KV.of(\"bar\", \"ham\"), KV.of(\"baz\", \"eggs\")))\n+            .apply(\"group\", GroupByKey.<String, String>create());\n+    PCollection<Long> coderLongs =\n+        pipeline\n+            .apply(\"counts with alternative coder\", CountingInput.upTo(10L))\n+            .setCoder(BigEndianLongCoder.of());\n+    PCollection<Integer> allCustomInts =\n+        pipeline\n+            .apply(\n+                \"intsWithCustomCoder\",\n+                Create.of(1, 2).withCoder(new AutoValue_PCollectionsTest_CustomIntCoder()))\n+            .apply(\n+                \"into custom windows\",\n+                Window.<Integer>into(new CustomWindows())\n+                    .triggering(\n+                        AfterWatermark.pastEndOfWindow()\n+                            .withEarlyFirings(\n+                                AfterFirst.of(\n+                                    AfterPane.elementCountAtLeast(5),\n+                                    AfterProcessingTime.pastFirstElementInPane()\n+                                        .plusDelayOf(Duration.millis(227L)))))\n+                    .accumulatingFiredPanes()\n+                    .withAllowedLateness(Duration.standardMinutes(12L)));\n+    return ImmutableList.<PCollection<?>>of(ints, longs, windowedLongs, coderLongs, groupedStrings);\n+  }\n+\n+  @Parameter(0)\n+  public PCollection<?> testCollection;\n+\n+  @Test\n+  public void testEncodeDecodeCycle() throws Exception {\n+    SdkComponents sdkComponents = SdkComponents.create();\n+    RunnerApi.PCollection protoCollection = PCollections.toProto(testCollection, sdkComponents);\n+    RunnerApi.Components protoComponents = sdkComponents.toComponents();\n+    Coder<?> decodedCoder = PCollections.getCoder(protoCollection, protoComponents);\n+    WindowingStrategy<?, ?> decodedStrategy =\n+        PCollections.getWindowingStrategy(protoCollection, protoComponents);\n+    IsBounded decodedIsBounded = PCollections.isBounded(protoCollection);\n+\n+    assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(testCollection.getCoder()));\n+    assertThat(\n+        decodedStrategy,\n+        Matchers.<WindowingStrategy<?, ?>>equalTo(\n+            testCollection.getWindowingStrategy().fixDefaults()));\n+    assertThat(decodedIsBounded, equalTo(testCollection.isBounded()));\n+  }\n+\n+  @AutoValue\n+  abstract static class CustomIntCoder extends CustomCoder<Integer> {\n+    @Override\n+    public void encode(Integer value, OutputStream outStream, Context context) throws IOException {\n+      VarInt.encode(value, outStream);\n+    }\n+\n+    @Override\n+    public Integer decode(InputStream inStream, Context context) throws IOException {\n+      return VarInt.decodeInt(inStream);\n+    }\n+  }\n+\n+  private static class CustomWindows extends NonMergingWindowFn<Integer, BoundedWindow> {\n+    @Override\n+    public Collection<BoundedWindow> assignWindows(final AssignContext c) throws Exception {\n+      return Collections.<BoundedWindow>singleton(\n+          new BoundedWindow() {\n+            @Override\n+            public Instant maxTimestamp() {\n+              return new Instant(c.element().longValue());\n+            }\n+          });\n+    }\n+\n+    @Override\n+    public boolean isCompatible(WindowFn<?, ?> other) {\n+      return other != null && this.getClass().equals(other.getClass());\n+    }\n+\n+    @Override\n+    public Coder<BoundedWindow> windowCoder() {\n+      return new CustomCoder<BoundedWindow>() {\n+        @Override public void verifyDeterministic() {}\n+\n+        @Override\n+        public void encode(BoundedWindow value, OutputStream outStream, Context context)\n+            throws IOException {\n+          VarInt.encode(value.maxTimestamp().getMillis(), outStream);\n+        }\n+\n+        @Override\n+        public BoundedWindow decode(InputStream inStream, Context context) throws IOException {\n+          final Instant ts = new Instant(VarInt.decodeLong(inStream));\n+          return new BoundedWindow() {\n+            @Override\n+            public Instant maxTimestamp() {\n+              return ts;\n+            }\n+          };\n+        }\n+      };\n+    }\n+\n+    @Override\n+    public WindowMappingFn<BoundedWindow> getDefaultWindowMappingFn() {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+}",
                "changes": 188
            },
            {
                "status": "modified",
                "additions": 155,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java",
                "deletions": 43,
                "sha": "4084cdcaed15803fe9b1153d1a71ae7f2f1b16de",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java",
                "patch": "@@ -24,6 +24,7 @@\n import static org.junit.Assert.assertThat;\n \n import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n import java.io.Serializable;\n import java.util.Collections;\n import org.apache.beam.sdk.coders.VarIntCoder;\n@@ -37,16 +38,23 @@\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Materialization;\n+import org.apache.beam.sdk.transforms.Materializations;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.Sum;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.ViewFn;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.util.PCollectionViews;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timer;\n import org.apache.beam.sdk.util.TimerSpec;\n import org.apache.beam.sdk.util.TimerSpecs;\n+import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.state.StateSpec;\n import org.apache.beam.sdk.util.state.StateSpecs;\n@@ -55,8 +63,9 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PDone;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.apache.beam.sdk.values.TupleTagList;\n import org.hamcrest.Matchers;\n@@ -91,7 +100,7 @@\n \n   @Test\n   public void classEqualToMatchesSameClass() {\n-    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.Bound.class);\n+    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.SingleOutput.class);\n     AppliedPTransform<?, ?, ?> application =\n         getAppliedTransform(\n             ParDo.of(\n@@ -126,7 +135,7 @@ public void classEqualToDoesNotMatchSubclass() {\n \n   @Test\n   public void classEqualToDoesNotMatchUnrelatedClass() {\n-    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.Bound.class);\n+    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.SingleOutput.class);\n     AppliedPTransform<?, ?, ?> application =\n         getAppliedTransform(Window.<KV<String, Integer>>into(new GlobalWindows()));\n \n@@ -191,7 +200,7 @@ public void onTimer(OnTimerContext context) {\n       };\n \n   /**\n-   * Demonstrates that a {@link ParDo.Bound} does not match any ParDo matcher.\n+   * Demonstrates that a {@link ParDo.SingleOutput} does not match any ParDo matcher.\n    */\n   @Test\n   public void parDoSingle() {\n@@ -323,21 +332,68 @@ public void parDoWithFnTypeNotParDo() {\n     assertThat(matcher.matches(notParDo), is(false));\n   }\n \n+  @Test\n+  public void createViewWithViewFn() {\n+    PCollection<Integer> input = p.apply(Create.of(1));\n+    PCollectionView<Iterable<Integer>> view =\n+        PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n+    ViewFn<Iterable<WindowedValue<?>>, Iterable<Integer>> viewFn = view.getViewFn();\n+    CreatePCollectionView<?, ?> createView = CreatePCollectionView.of(view);\n+\n+    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(viewFn.getClass());\n+    assertThat(matcher.matches(getAppliedTransform(createView)), is(true));\n+  }\n+\n+  @Test\n+  public void createViewWithViewFnDifferentViewFn() {\n+    PCollection<Integer> input = p.apply(Create.of(1));\n+    PCollectionView<Iterable<Integer>> view =\n+        PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n+    ViewFn<Iterable<WindowedValue<?>>, Iterable<Integer>> viewFn =\n+        new ViewFn<Iterable<WindowedValue<?>>, Iterable<Integer>>() {\n+          @Override\n+          public Materialization<Iterable<WindowedValue<?>>> getMaterialization() {\n+            @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+            Materialization<Iterable<WindowedValue<?>>> materialization =\n+                (Materialization) Materializations.iterable();\n+            return materialization;\n+          }\n+\n+          @Override\n+          public Iterable<Integer> apply(Iterable<WindowedValue<?>> contents) {\n+            return Collections.emptyList();\n+          }\n+        };\n+    CreatePCollectionView<?, ?> createView = CreatePCollectionView.of(view);\n+\n+    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(viewFn.getClass());\n+    assertThat(matcher.matches(getAppliedTransform(createView)), is(false));\n+  }\n+\n+  @Test\n+  public void createViewWithViewFnNotCreatePCollectionView() {\n+    PCollection<Integer> input = p.apply(Create.of(1));\n+    PCollectionView<Iterable<Integer>> view =\n+        PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n+\n+    PTransformMatcher matcher =\n+        PTransformMatchers.createViewWithViewFn(view.getViewFn().getClass());\n+    assertThat(matcher.matches(getAppliedTransform(View.asIterable())), is(false));\n+  }\n+\n   @Test\n   public void emptyFlattenWithEmptyFlatten() {\n     AppliedPTransform application =\n         AppliedPTransform\n-            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>\n-                of(\n-                    \"EmptyFlatten\",\n-                    Collections.<TaggedPValue>emptyList(),\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Flatten.pCollections(),\n-                    p);\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"EmptyFlatten\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n \n     assertThat(PTransformMatchers.emptyFlatten().matches(application), is(true));\n   }\n@@ -346,21 +402,18 @@ public void emptyFlattenWithEmptyFlatten() {\n   public void emptyFlattenWithNonEmptyFlatten() {\n     AppliedPTransform application =\n         AppliedPTransform\n-            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>\n-                of(\n-                    \"Flatten\",\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Flatten.pCollections(),\n-                    p);\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"Flatten\",\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n \n     assertThat(PTransformMatchers.emptyFlatten().matches(application), is(false));\n   }\n@@ -369,22 +422,81 @@ public void emptyFlattenWithNonEmptyFlatten() {\n   public void emptyFlattenWithNonFlatten() {\n     AppliedPTransform application =\n         AppliedPTransform\n-            .<PCollection<Iterable<Object>>, PCollection<Object>, Flatten.Iterables<Object>>\n-                of(\n-                    \"EmptyFlatten\",\n-                    Collections.<TaggedPValue>emptyList(),\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Flatten.iterables() /* This isn't actually possible to construct,\n+            .<PCollection<Iterable<Object>>, PCollection<Object>, Flatten.Iterables<Object>>of(\n+                \"EmptyFlatten\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.iterables() /* This isn't actually possible to construct,\n                                  * but for the sake of example */,\n-                    p);\n+                p);\n \n     assertThat(PTransformMatchers.emptyFlatten().matches(application), is(false));\n   }\n \n+  @Test\n+  public void flattenWithDuplicateInputsWithoutDuplicates() {\n+    AppliedPTransform application =\n+        AppliedPTransform\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"Flatten\",\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n+\n+    assertThat(PTransformMatchers.flattenWithDuplicateInputs().matches(application), is(false));\n+  }\n+\n+  @Test\n+  public void flattenWithDuplicateInputsWithDuplicates() {\n+    PCollection<Object> duplicate =\n+        PCollection.createPrimitiveOutputInternal(\n+            p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED);\n+    AppliedPTransform application =\n+        AppliedPTransform\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"Flatten\",\n+                ImmutableMap.<TupleTag<?>, PValue>builder()\n+                    .put(new TupleTag<Object>(), duplicate)\n+                    .put(new TupleTag<Object>(), duplicate)\n+                    .build(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n+\n+    assertThat(PTransformMatchers.flattenWithDuplicateInputs().matches(application), is(true));\n+  }\n+\n+  @Test\n+  public void flattenWithDuplicateInputsNonFlatten() {\n+    AppliedPTransform application =\n+        AppliedPTransform\n+            .<PCollection<Iterable<Object>>, PCollection<Object>, Flatten.Iterables<Object>>of(\n+                \"EmptyFlatten\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.iterables() /* This isn't actually possible to construct,\n+                                 * but for the sake of example */,\n+                p);\n+\n+    assertThat(PTransformMatchers.flattenWithDuplicateInputs().matches(application), is(false));\n+  }\n+\n   @Test\n   public void writeWithRunnerDeterminedSharding() {\n     Write<Integer> write =\n@@ -417,8 +529,8 @@ public void writeWithRunnerDeterminedSharding() {\n   private AppliedPTransform<?, ?, ?> appliedWrite(Write<Integer> write) {\n     return AppliedPTransform.<PCollection<Integer>, PDone, Write<Integer>>of(\n         \"Write\",\n-        Collections.<TaggedPValue>emptyList(),\n-        Collections.<TaggedPValue>emptyList(),\n+        Collections.<TupleTag<?>, PValue>emptyMap(),\n+        Collections.<TupleTag<?>, PValue>emptyMap(),\n         write,\n         p);\n   }",
                "changes": 198
            },
            {
                "status": "added",
                "additions": 131,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java",
                "deletions": 0,
                "sha": "b0656177abe2b787a86746aecfa0cb8056397eab",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java",
                "patch": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.common.collect.ImmutableMap;\n+import java.util.Collections;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link PTransformReplacements}.\n+ */\n+@RunWith(JUnit4.class)\n+public class PTransformReplacementsTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+  private PCollection<Long> mainInput = pipeline.apply(CountingInput.unbounded());\n+  private PCollectionView<String> sideInput =\n+      pipeline.apply(Create.of(\"foo\")).apply(View.<String>asSingleton());\n+\n+  private PCollection<Long> output = mainInput.apply(ParDo.of(new TestDoFn()));\n+\n+  @Test\n+  public void getMainInputSingleOutputSingleInput() {\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), mainInput),\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()),\n+            pipeline);\n+    PCollection<Long> input = PTransformReplacements.getSingletonMainInput(application);\n+    assertThat(input, equalTo(mainInput));\n+  }\n+\n+  @Test\n+  public void getMainInputSingleOutputSideInputs() {\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            ImmutableMap.<TupleTag<?>, PValue>builder()\n+                .put(new TupleTag<Long>(), mainInput)\n+                .put(sideInput.getTagInternal(), sideInput.getPCollection())\n+                .build(),\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()).withSideInputs(sideInput),\n+            pipeline);\n+    PCollection<Long> input = PTransformReplacements.getSingletonMainInput(application);\n+    assertThat(input, equalTo(mainInput));\n+  }\n+\n+  @Test\n+  public void getMainInputExtraMainInputsThrows() {\n+    PCollection<Long> notInParDo = pipeline.apply(\"otherPCollection\", Create.of(1L, 2L, 3L));\n+    ImmutableMap<TupleTag<?>, PValue> inputs =\n+        ImmutableMap.<TupleTag<?>, PValue>builder()\n+            .putAll(mainInput.expand())\n+            // Not represnted as an input\n+            .put(new TupleTag<Long>(), notInParDo)\n+            .put(sideInput.getTagInternal(), sideInput.getPCollection())\n+            .build();\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            inputs,\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()).withSideInputs(sideInput),\n+            pipeline);\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(\"multiple inputs\");\n+    thrown.expectMessage(\"not additional inputs\");\n+    thrown.expectMessage(mainInput.toString());\n+    thrown.expectMessage(notInParDo.toString());\n+    PTransformReplacements.getSingletonMainInput(application);\n+  }\n+\n+  @Test\n+  public void getMainInputNoMainInputsThrows() {\n+    ImmutableMap<TupleTag<?>, PValue> inputs =\n+        ImmutableMap.<TupleTag<?>, PValue>builder()\n+            .put(sideInput.getTagInternal(), sideInput.getPCollection())\n+            .build();\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            inputs,\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()).withSideInputs(sideInput),\n+            pipeline);\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(\"No main input\");\n+    PTransformReplacements.getSingletonMainInput(application);\n+  }\n+\n+  private static class TestDoFn extends DoFn<Long, Long> {\n+    @ProcessElement public void process(ProcessContext context) {}\n+  }\n+}",
                "changes": 131
            },
            {
                "status": "added",
                "additions": 189,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java",
                "deletions": 0,
                "sha": "4e3cdb63bcb5a9b17b128307b4be74fbe0b0ca84",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java",
                "patch": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.auto.value.AutoValue;\n+import com.google.common.collect.ImmutableList;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.PTransform;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.io.CountingInput.UnboundedCountingInput;\n+import org.apache.beam.sdk.io.CountingSource;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionTuple;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.apache.beam.sdk.values.TupleTagList;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+/**\n+ * Tests for {@link PTransforms}.\n+ */\n+@RunWith(Parameterized.class)\n+public class PTransformsTest {\n+\n+  @Parameters(name = \"{index}: {0}\")\n+  public static Iterable<ToAndFromProtoSpec> data() {\n+    // This pipeline exists for construction, not to run any test.\n+    // TODO: Leaf node with understood payload - i.e. validate payloads\n+    ToAndFromProtoSpec readLeaf = ToAndFromProtoSpec.leaf(read(TestPipeline.create()));\n+    ToAndFromProtoSpec readMultipleInAndOut =\n+        ToAndFromProtoSpec.leaf(multiMultiParDo(TestPipeline.create()));\n+    TestPipeline compositeReadPipeline = TestPipeline.create();\n+    ToAndFromProtoSpec compositeRead =\n+        ToAndFromProtoSpec.composite(\n+            countingInput(compositeReadPipeline),\n+            ToAndFromProtoSpec.leaf(read(compositeReadPipeline)));\n+    return ImmutableList.<ToAndFromProtoSpec>builder()\n+        .add(readLeaf)\n+        .add(readMultipleInAndOut)\n+        .add(compositeRead)\n+        // TODO: Composite with multiple children\n+        // TODO: Composite with a composite child\n+        .build();\n+  }\n+\n+  @AutoValue\n+  abstract static class ToAndFromProtoSpec {\n+    public static ToAndFromProtoSpec leaf(AppliedPTransform<?, ?, ?> transform) {\n+      return new AutoValue_PTransformsTest_ToAndFromProtoSpec(\n+          transform, Collections.<ToAndFromProtoSpec>emptyList());\n+    }\n+\n+    public static ToAndFromProtoSpec composite(\n+        AppliedPTransform<?, ?, ?> topLevel, ToAndFromProtoSpec spec, ToAndFromProtoSpec... specs) {\n+      List<ToAndFromProtoSpec> childSpecs = new ArrayList<>();\n+      childSpecs.add(spec);\n+      childSpecs.addAll(Arrays.asList(specs));\n+      return new AutoValue_PTransformsTest_ToAndFromProtoSpec(topLevel, childSpecs);\n+    }\n+\n+    abstract AppliedPTransform<?, ?, ?> getTransform();\n+    abstract Collection<ToAndFromProtoSpec> getChildren();\n+  }\n+\n+  @Parameter(0)\n+  public ToAndFromProtoSpec spec;\n+\n+  @Test\n+  public void toAndFromProto() throws IOException {\n+    SdkComponents components = SdkComponents.create();\n+    RunnerApi.PTransform converted = convert(spec, components);\n+    Components protoComponents = components.toComponents();\n+\n+    // Sanity checks\n+    assertThat(converted.getInputsCount(), equalTo(spec.getTransform().getInputs().size()));\n+    assertThat(converted.getOutputsCount(), equalTo(spec.getTransform().getOutputs().size()));\n+    assertThat(converted.getSubtransformsCount(), equalTo(spec.getChildren().size()));\n+\n+    assertThat(converted.getUniqueName(), equalTo(spec.getTransform().getFullName()));\n+    for (PValue inputValue : spec.getTransform().getInputs().values()) {\n+      PCollection<?> inputPc = (PCollection<?>) inputValue;\n+      protoComponents.getPcollectionsOrThrow(components.registerPCollection(inputPc));\n+    }\n+    for (PValue outputValue : spec.getTransform().getOutputs().values()) {\n+      PCollection<?> outputPc = (PCollection<?>) outputValue;\n+      protoComponents.getPcollectionsOrThrow(components.registerPCollection(outputPc));\n+    }\n+  }\n+\n+  private RunnerApi.PTransform convert(ToAndFromProtoSpec spec, SdkComponents components)\n+      throws IOException {\n+    List<AppliedPTransform<?, ?, ?>> childTransforms = new ArrayList<>();\n+    for (ToAndFromProtoSpec child : spec.getChildren()) {\n+      childTransforms.add(child.getTransform());\n+      System.out.println(\"Converting child \" + child);\n+      convert(child, components);\n+      // Sanity call\n+      components.getExistingPTransformId(child.getTransform());\n+    }\n+    PTransform convert = PTransforms.toProto(spec.getTransform(), childTransforms, components);\n+    // Make sure the converted transform is registered. Convert it independently, but if this is a\n+    // child spec, the child must be in the components.\n+    components.registerPTransform(spec.getTransform(), childTransforms);\n+    return convert;\n+  }\n+\n+  private static class TestDoFn extends DoFn<Long, KV<Long, String>> {\n+    // Exists to stop the ParDo application from throwing\n+    @ProcessElement public void process(ProcessContext context) {}\n+  }\n+\n+  private static AppliedPTransform<?, ?, ?> countingInput(Pipeline pipeline) {\n+    UnboundedCountingInput input = CountingInput.unbounded();\n+    PCollection<Long> pcollection = pipeline.apply(input);\n+    return AppliedPTransform.<PBegin, PCollection<Long>, UnboundedCountingInput>of(\n+        \"Count\", pipeline.begin().expand(), pcollection.expand(), input, pipeline);\n+  }\n+\n+  private static AppliedPTransform<?, ?, ?> read(Pipeline pipeline) {\n+    Read.Unbounded<Long> transform = Read.from(CountingSource.unbounded());\n+    PCollection<Long> pcollection = pipeline.apply(transform);\n+    return AppliedPTransform.<PBegin, PCollection<Long>, Read.Unbounded<Long>>of(\n+        \"ReadTheCount\", pipeline.begin().expand(), pcollection.expand(), transform, pipeline);\n+  }\n+\n+  private static AppliedPTransform<?, ?, ?> multiMultiParDo(Pipeline pipeline) {\n+    PCollectionView<String> view =\n+        pipeline.apply(Create.of(\"foo\")).apply(View.<String>asSingleton());\n+    PCollection<Long> input = pipeline.apply(CountingInput.unbounded());\n+    ParDo.MultiOutput<Long, KV<Long, String>> parDo =\n+        ParDo.of(new TestDoFn())\n+            .withSideInputs(view)\n+            .withOutputTags(\n+                new TupleTag<KV<Long, String>>() {},\n+                TupleTagList.of(new TupleTag<KV<String, Long>>() {}));\n+    PCollectionTuple output = input.apply(parDo);\n+\n+    Map<TupleTag<?>, PValue> inputs = new HashMap<>();\n+    inputs.putAll(parDo.getAdditionalInputs());\n+    inputs.putAll(input.expand());\n+\n+    return AppliedPTransform\n+        .<PCollection<Long>, PCollectionTuple, ParDo.MultiOutput<Long, KV<Long, String>>>of(\n+            \"MultiParDoInAndOut\", inputs, output.expand(), parDo, pipeline);\n+  }\n+}",
                "changes": 189
            },
            {
                "status": "modified",
                "additions": 10,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java",
                "deletions": 99,
                "sha": "00c436d5cb3feead112c58fd95788320d6782ded",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java",
                "patch": "@@ -21,18 +21,15 @@\n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n \n-import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n-import java.util.List;\n import java.util.Map;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n-import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PCollectionTuple;\n-import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TaggedPValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -79,53 +76,22 @@ public void singletonSucceeds() {\n     assertThat(replacements, Matchers.<PValue>hasKey(replacementInts));\n \n     ReplacementOutput replacement = replacements.get(replacementInts);\n-    TaggedPValue taggedInts = Iterables.getOnlyElement(ints.expand());\n-    assertThat(replacement.getOriginal(), equalTo(taggedInts));\n+    Map.Entry<TupleTag<?>, PValue> taggedInts = Iterables.getOnlyElement(ints.expand().entrySet());\n+    assertThat(\n+        replacement.getOriginal().getTag(), Matchers.<TupleTag<?>>equalTo(taggedInts.getKey()));\n+    assertThat(replacement.getOriginal().getValue(), equalTo(taggedInts.getValue()));\n     assertThat(replacement.getReplacement().getValue(), Matchers.<PValue>equalTo(replacementInts));\n   }\n \n   @Test\n   public void singletonMultipleOriginalsThrows() {\n     thrown.expect(IllegalArgumentException.class);\n     ReplacementOutputs.singleton(\n-        ImmutableList.copyOf(Iterables.concat(ints.expand(), moreInts.expand())), replacementInts);\n-  }\n-\n-  @Test\n-  public void orderedSucceeds() {\n-    List<TaggedPValue> originals = PCollectionList.of(ints).and(moreInts).expand();\n-    Map<PValue, ReplacementOutput> replacements =\n-        ReplacementOutputs.ordered(\n-            originals, PCollectionList.of(replacementInts).and(moreReplacementInts));\n-    assertThat(\n-        replacements.keySet(),\n-        Matchers.<PValue>containsInAnyOrder(replacementInts, moreReplacementInts));\n-\n-    ReplacementOutput intsMapping = replacements.get(replacementInts);\n-    assertThat(intsMapping.getOriginal().getValue(), Matchers.<PValue>equalTo(ints));\n-    assertThat(intsMapping.getReplacement().getValue(), Matchers.<PValue>equalTo(replacementInts));\n-\n-    ReplacementOutput moreIntsMapping = replacements.get(moreReplacementInts);\n-    assertThat(moreIntsMapping.getOriginal().getValue(), Matchers.<PValue>equalTo(moreInts));\n-    assertThat(\n-        moreIntsMapping.getReplacement().getValue(), Matchers.<PValue>equalTo(moreReplacementInts));\n-  }\n-\n-  @Test\n-  public void orderedTooManyReplacements() {\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"same size\");\n-    ReplacementOutputs.ordered(\n-        PCollectionList.of(ints).expand(),\n-        PCollectionList.of(replacementInts).and(moreReplacementInts));\n-  }\n-\n-  @Test\n-  public void orderedTooFewReplacements() {\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"same size\");\n-    ReplacementOutputs.ordered(\n-        PCollectionList.of(ints).and(moreInts).expand(), PCollectionList.of(moreReplacementInts));\n+        ImmutableMap.<TupleTag<?>, PValue>builder()\n+            .putAll(ints.expand())\n+            .putAll(moreInts.expand())\n+            .build(),\n+        replacementInts);\n   }\n \n   private TupleTag<Integer> intsTag = new TupleTag<>();\n@@ -168,61 +134,6 @@ public void taggedSucceeds() {\n                 TaggedPValue.of(moreIntsTag, moreReplacementInts))));\n   }\n \n-  /**\n-   * When a call to {@link ReplacementOutputs#tagged(List, POutput)} is made where the first\n-   * argument contains multiple copies of the same {@link TaggedPValue}, the call succeeds using\n-   * that mapping.\n-   */\n-  @Test\n-  public void taggedMultipleInstances() {\n-    List<TaggedPValue> original =\n-        ImmutableList.of(\n-            TaggedPValue.of(intsTag, ints),\n-            TaggedPValue.of(strsTag, strs),\n-            TaggedPValue.of(intsTag, ints));\n-\n-    Map<PValue, ReplacementOutput> replacements =\n-        ReplacementOutputs.tagged(\n-            original, PCollectionTuple.of(strsTag, replacementStrs).and(intsTag, replacementInts));\n-    assertThat(\n-        replacements.keySet(),\n-        Matchers.<PValue>containsInAnyOrder(replacementStrs, replacementInts));\n-    ReplacementOutput intsReplacement = replacements.get(replacementInts);\n-    ReplacementOutput strsReplacement = replacements.get(replacementStrs);\n-\n-    assertThat(\n-        intsReplacement,\n-        equalTo(\n-            ReplacementOutput.of(\n-                TaggedPValue.of(intsTag, ints), TaggedPValue.of(intsTag, replacementInts))));\n-    assertThat(\n-        strsReplacement,\n-        equalTo(\n-            ReplacementOutput.of(\n-                TaggedPValue.of(strsTag, strs), TaggedPValue.of(strsTag, replacementStrs))));\n-  }\n-\n-  /**\n-   * When a call to {@link ReplacementOutputs#tagged(List, POutput)} is made where a single tag\n-   * has multiple {@link PValue PValues} mapped to it, the call fails.\n-   */\n-  @Test\n-  public void taggedMultipleConflictingInstancesThrows() {\n-    List<TaggedPValue> original =\n-        ImmutableList.of(\n-            TaggedPValue.of(intsTag, ints), TaggedPValue.of(intsTag, moreReplacementInts));\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"different values\");\n-    thrown.expectMessage(intsTag.toString());\n-    thrown.expectMessage(ints.toString());\n-    thrown.expectMessage(moreReplacementInts.toString());\n-    ReplacementOutputs.tagged(\n-        original,\n-        PCollectionTuple.of(strsTag, replacementStrs)\n-            .and(moreIntsTag, moreReplacementInts)\n-            .and(intsTag, replacementInts));\n-  }\n-\n   @Test\n   public void taggedMissingReplacementThrows() {\n     PCollectionTuple original =",
                "changes": 109
            },
            {
                "status": "added",
                "additions": 223,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java",
                "deletions": 0,
                "sha": "895aec48572d95f86bdff26c9aa3990925cbc8ad",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java",
                "patch": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.isEmptyOrNullString;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.IterableCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.SetCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Tests for {@link SdkComponents}. */\n+@RunWith(JUnit4.class)\n+public class SdkComponentsTest {\n+  @Rule\n+  public TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+  @Rule\n+  public ExpectedException thrown = ExpectedException.none();\n+\n+  private SdkComponents components = SdkComponents.create();\n+\n+  @Test\n+  public void registerCoder() throws IOException {\n+    Coder<?> coder =\n+        KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));\n+    String id = components.registerCoder(coder);\n+    assertThat(components.registerCoder(coder), equalTo(id));\n+    assertThat(id, not(isEmptyOrNullString()));\n+    VarLongCoder otherCoder = VarLongCoder.of();\n+    assertThat(components.registerCoder(otherCoder), not(equalTo(id)));\n+\n+    components.toComponents().getCodersOrThrow(id);\n+    components.toComponents().getCodersOrThrow(components.registerCoder(otherCoder));\n+  }\n+\n+  @Test\n+  public void registerCoderEqualsNotSame() throws IOException {\n+    Coder<?> coder =\n+        KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));\n+    Coder<?> otherCoder =\n+        KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));\n+    assertThat(coder, Matchers.<Coder<?>>equalTo(otherCoder));\n+    String id = components.registerCoder(coder);\n+    String otherId = components.registerCoder(otherCoder);\n+    assertThat(otherId, not(equalTo(id)));\n+\n+    components.toComponents().getCodersOrThrow(id);\n+    components.toComponents().getCodersOrThrow(otherId);\n+  }\n+\n+  @Test\n+  public void registerTransformNoChildren() throws IOException {\n+    Create.Values<Integer> create = Create.of(1, 2, 3);\n+    PCollection<Integer> pt = pipeline.apply(create);\n+    String userName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Integer>, Create.Values<Integer>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    String componentName =\n+        components.registerPTransform(\n+            transform, Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n+    assertThat(componentName, equalTo(userName));\n+    assertThat(components.getExistingPTransformId(transform), equalTo(componentName));\n+  }\n+\n+  @Test\n+  public void registerTransformAfterChildren() throws IOException {\n+    Create.Values<Long> create = Create.of(1L, 2L, 3L);\n+    CountingInput.UnboundedCountingInput createChild = CountingInput.unbounded();\n+\n+    PCollection<Long> pt = pipeline.apply(create);\n+    String userName = \"my_transform\";\n+    String childUserName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, Create.Values<Long>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    AppliedPTransform<?, ?, ?> childTransform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, CountingInput.UnboundedCountingInput>of(\n+            childUserName, pipeline.begin().expand(), pt.expand(), createChild, pipeline);\n+\n+    String childId = components.registerPTransform(childTransform,\n+        Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n+    String parentId = components.registerPTransform(transform,\n+        Collections.<AppliedPTransform<?, ?, ?>>singletonList(childTransform));\n+    Components components = this.components.toComponents();\n+    assertThat(components.getTransformsOrThrow(parentId).getSubtransforms(0), equalTo(childId));\n+    assertThat(components.getTransformsOrThrow(childId).getSubtransformsCount(), equalTo(0));\n+  }\n+\n+  @Test\n+  public void registerTransformEmptyFullName() throws IOException {\n+    Create.Values<Integer> create = Create.of(1, 2, 3);\n+    PCollection<Integer> pt = pipeline.apply(create);\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Integer>, Create.Values<Integer>>of(\n+            \"\", pipeline.begin().expand(), pt.expand(), create, pipeline);\n+\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(transform.toString());\n+    components.getExistingPTransformId(transform);\n+  }\n+\n+  @Test\n+  public void registerTransformNullComponents() throws IOException {\n+    Create.Values<Integer> create = Create.of(1, 2, 3);\n+    PCollection<Integer> pt = pipeline.apply(create);\n+    String userName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Integer>, Create.Values<Integer>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    thrown.expect(NullPointerException.class);\n+    thrown.expectMessage(\"child nodes may not be null\");\n+    components.registerPTransform(transform, null);\n+  }\n+\n+  /**\n+   * Tests that trying to register a transform which has unregistered children throws.\n+   */\n+  @Test\n+  public void registerTransformWithUnregisteredChildren() throws IOException {\n+    Create.Values<Long> create = Create.of(1L, 2L, 3L);\n+    CountingInput.UnboundedCountingInput createChild = CountingInput.unbounded();\n+\n+    PCollection<Long> pt = pipeline.apply(create);\n+    String userName = \"my_transform\";\n+    String childUserName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, Create.Values<Long>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    AppliedPTransform<?, ?, ?> childTransform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, CountingInput.UnboundedCountingInput>of(\n+            childUserName, pipeline.begin().expand(), pt.expand(), createChild, pipeline);\n+\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(childTransform.toString());\n+    components.registerPTransform(\n+        transform, Collections.<AppliedPTransform<?, ?, ?>>singletonList(childTransform));\n+  }\n+\n+  @Test\n+  public void registerPCollection() throws IOException {\n+    PCollection<Long> pCollection = pipeline.apply(CountingInput.unbounded()).setName(\"foo\");\n+    String id = components.registerPCollection(pCollection);\n+    assertThat(id, equalTo(\"foo\"));\n+    components.toComponents().getPcollectionsOrThrow(id);\n+  }\n+\n+  @Test\n+  public void registerPCollectionExistingNameCollision() throws IOException {\n+    PCollection<Long> pCollection =\n+        pipeline.apply(\"FirstCount\", CountingInput.unbounded()).setName(\"foo\");\n+    String firstId = components.registerPCollection(pCollection);\n+    PCollection<Long> duplicate =\n+        pipeline.apply(\"SecondCount\", CountingInput.unbounded()).setName(\"foo\");\n+    String secondId = components.registerPCollection(duplicate);\n+    assertThat(firstId, equalTo(\"foo\"));\n+    assertThat(secondId, containsString(\"foo\"));\n+    assertThat(secondId, not(equalTo(\"foo\")));\n+    components.toComponents().getPcollectionsOrThrow(firstId);\n+    components.toComponents().getPcollectionsOrThrow(secondId);\n+  }\n+\n+  @Test\n+  public void registerWindowingStrategy() throws IOException {\n+    WindowingStrategy<?, ?> strategy =\n+        WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n+    String name = components.registerWindowingStrategy(strategy);\n+    assertThat(name, not(isEmptyOrNullString()));\n+\n+    components.toComponents().getWindowingStrategiesOrThrow(name);\n+  }\n+\n+  @Test\n+  public void registerWindowingStrategyIdEqualStrategies() throws IOException {\n+    WindowingStrategy<?, ?> strategy =\n+        WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n+    String name = components.registerWindowingStrategy(strategy);\n+    String duplicateName =\n+        components.registerWindowingStrategy(\n+            WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES));\n+    assertThat(name, equalTo(duplicateName));\n+  }\n+}",
                "changes": 223
            },
            {
                "status": "modified",
                "additions": 13,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java",
                "deletions": 24,
                "sha": "acca5cd28b8490604aa3007d843596287808605b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java",
                "patch": "@@ -20,18 +20,18 @@\n \n import static org.junit.Assert.assertThat;\n \n-import com.google.common.collect.Iterables;\n import java.io.Serializable;\n import java.util.Map;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.MapElements;\n-import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.SimpleFunction;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TaggedPValue;\n import org.hamcrest.Matchers;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -55,9 +55,15 @@\n               PCollection<? extends Integer>, PCollection<Integer>,\n               MapElements<Integer, Integer>>() {\n             @Override\n-            public PTransform<PCollection<? extends Integer>, PCollection<Integer>>\n-                getReplacementTransform(MapElements<Integer, Integer> transform) {\n-              return transform;\n+            public PTransformReplacement<PCollection<? extends Integer>, PCollection<Integer>>\n+                getReplacementTransform(\n+                    AppliedPTransform<\n+                            PCollection<? extends Integer>, PCollection<Integer>,\n+                            MapElements<Integer, Integer>>\n+                        transform) {\n+              return PTransformReplacement.of(\n+                  PTransformReplacements.getSingletonMainInput(transform),\n+                  transform.getTransform());\n             }\n           };\n \n@@ -68,23 +74,6 @@ public Integer apply(Integer input) {\n       }\n     };\n \n-  @Test\n-  public void testGetInput() {\n-    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));\n-    assertThat(\n-        factory.getInput(input.expand(), pipeline),\n-        Matchers.<PCollection<? extends Integer>>equalTo(input));\n-  }\n-\n-  @Test\n-  public void testGetInputMultipleInputsFails() {\n-    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));\n-    PCollection<Integer> otherInput = pipeline.apply(\"OtherCreate\", Create.of(1, 2, 3));\n-\n-    thrown.expect(IllegalArgumentException.class);\n-    factory.getInput(PCollectionList.of(input).and(otherInput).expand(), pipeline);\n-  }\n-\n   @Test\n   public void testMapOutputs() {\n     PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));\n@@ -97,8 +86,8 @@ public void testMapOutputs() {\n         Matchers.<PValue, ReplacementOutput>hasEntry(\n             reappliedOutput,\n             ReplacementOutput.of(\n-                Iterables.getOnlyElement(output.expand()),\n-                Iterables.getOnlyElement(reappliedOutput.expand()))));\n+                TaggedPValue.ofExpandedValue(output),\n+                TaggedPValue.ofExpandedValue(reappliedOutput))));\n   }\n \n   @Test",
                "changes": 37
            },
            {
                "status": "renamed",
                "additions": 12,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/transforms/windowing/TriggersTest.java",
                "deletions": 1,
                "sha": "cf9d40c8c9cfb11bef42da57df47e8025d4ee2e3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java",
                "patch": "@@ -15,13 +15,24 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.transforms.windowing;\n+package org.apache.beam.runners.core.construction;\n \n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n \n import com.google.auto.value.AutoValue;\n import com.google.common.collect.ImmutableList;\n+import org.apache.beam.sdk.transforms.windowing.AfterAll;\n+import org.apache.beam.sdk.transforms.windowing.AfterEach;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterSynchronizedProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n+import org.apache.beam.sdk.transforms.windowing.DefaultTrigger;\n+import org.apache.beam.sdk.transforms.windowing.Never;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.junit.Test;",
                "changes": 13
            },
            {
                "status": "renamed",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java",
                "previous_filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSourceTest.java",
                "deletions": 4,
                "sha": "c905cf573a5a016edc32fbcae7258bc235f91c4c",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.core;\n+package org.apache.beam.runners.core.construction;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n@@ -33,9 +33,9 @@\n import java.util.List;\n import java.util.NoSuchElementException;\n import java.util.Random;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.Checkpoint;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.CheckpointCoder;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.Checkpoint;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.CheckpointCoder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.SerializableCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;",
                "changes": 8
            },
            {
                "status": "added",
                "additions": 105,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java",
                "deletions": 0,
                "sha": "1966a93ec4d9f73ac8764040b8d7142d76ae6896",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java",
                "patch": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.junit.Assert.assertThat;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor;\n+import org.apache.beam.sdk.io.CountingSource;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.io.Read.Bounded;\n+import org.apache.beam.sdk.io.Read.Unbounded;\n+import org.apache.beam.sdk.runners.TransformHierarchy.Node;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link UnconsumedReads}.\n+ */\n+@RunWith(JUnit4.class)\n+public class UnconsumedReadsTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+\n+  @Test\n+  public void matcherProducesUnconsumedValueBoundedRead() {\n+    Bounded<Long> transform = Read.from(CountingSource.upTo(20L));\n+    PCollection<Long> output = pipeline.apply(transform);\n+    UnconsumedReads.ensureAllReadsConsumed(pipeline);\n+    validateConsumed();\n+  }\n+\n+  @Test\n+  public void matcherProducesUnconsumedValueUnboundedRead() {\n+    Unbounded<Long> transform = Read.from(CountingSource.unbounded());\n+    PCollection<Long> output = pipeline.apply(transform);\n+    UnconsumedReads.ensureAllReadsConsumed(pipeline);\n+    validateConsumed();\n+  }\n+\n+  @Test\n+  public void doesNotConsumeAlreadyConsumedRead() {\n+    Unbounded<Long> transform = Read.from(CountingSource.unbounded());\n+    final PCollection<Long> output = pipeline.apply(transform);\n+    final Flatten.PCollections<Long> consumer = Flatten.<Long>pCollections();\n+    PCollectionList.of(output).apply(consumer);\n+    UnconsumedReads.ensureAllReadsConsumed(pipeline);\n+    pipeline.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            // The output should only be consumed by a single consumer\n+            if (node.getInputs().values().contains(output)) {\n+              assertThat(node.getTransform(), Matchers.<PTransform<?, ?>>is(consumer));\n+            }\n+          }\n+        });\n+  }\n+\n+  private void validateConsumed() {\n+    final Set<PValue> consumedOutputs = new HashSet<PValue>();\n+    final Set<PValue> allReadOutputs = new HashSet<PValue>();\n+    pipeline.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            consumedOutputs.addAll(node.getInputs().values());\n+          }\n+\n+          @Override\n+          public void visitValue(PValue value, Node producer) {\n+            if (producer.getTransform() instanceof Read.Bounded\n+                || producer.getTransform() instanceof Read.Unbounded) {\n+              allReadOutputs.add(value);\n+            }\n+          }\n+        });\n+    assertThat(consumedOutputs, Matchers.hasItems(allReadOutputs.toArray(new PValue[0])));\n+  }\n+}",
                "changes": 105
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java",
                "deletions": 12,
                "sha": "6d3b263c2618468c9a17c270a51b824b85391b41",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java",
                "patch": "@@ -19,11 +19,10 @@\n package org.apache.beam.runners.core.construction;\n \n import java.util.Collections;\n-import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.values.PDone;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.ExpectedException;\n@@ -46,20 +45,13 @@\n   public void getReplacementTransformThrows() {\n     thrown.expect(UnsupportedOperationException.class);\n     thrown.expectMessage(message);\n-    factory.getReplacementTransform(Create.empty(VoidCoder.of()));\n-  }\n-\n-  @Test\n-  public void getInputThrows() {\n-    thrown.expect(UnsupportedOperationException.class);\n-    thrown.expectMessage(message);\n-    factory.getInput(Collections.<TaggedPValue>emptyList(), pipeline);\n+    factory.getReplacementTransform(null);\n   }\n \n   @Test\n   public void mapOutputThrows() {\n     thrown.expect(UnsupportedOperationException.class);\n     thrown.expectMessage(message);\n-    factory.mapOutputs(Collections.<TaggedPValue>emptyList(), PDone.in(pipeline));\n+    factory.mapOutputs(Collections.<TupleTag<?>, PValue>emptyMap(), PDone.in(pipeline));\n   }\n }",
                "changes": 16
            },
            {
                "status": "renamed",
                "additions": 20,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/util/WindowingStrategiesTest.java",
                "deletions": 1,
                "sha": "62bba8eb40ae76d600e80aa7dd68774e7ad0ebb4",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java",
                "patch": "@@ -15,20 +15,23 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core.construction;\n \n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n \n import com.google.auto.value.AutoValue;\n import com.google.common.collect.ImmutableList;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n+import org.hamcrest.Matchers;\n import org.joda.time.Duration;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -88,4 +91,20 @@ public void testToProtoAndBack() throws Exception {\n         toProtoAndBackWindowingStrategy,\n         equalTo((WindowingStrategy) windowingStrategy.fixDefaults()));\n   }\n+\n+  @Test\n+  public void testToProtoAndBackWithComponents() throws Exception {\n+    WindowingStrategy<?, ?> windowingStrategy = toProtoAndBackSpec.getWindowingStrategy();\n+    SdkComponents components = SdkComponents.create();\n+    RunnerApi.WindowingStrategy proto =\n+        WindowingStrategies.toProto(windowingStrategy, components);\n+    RunnerApi.Components protoComponents = components.toComponents();\n+\n+    assertThat(\n+        WindowingStrategies.fromProto(proto, protoComponents).fixDefaults(),\n+        Matchers.<WindowingStrategy<?, ?>>equalTo(windowingStrategy.fixDefaults()));\n+\n+    protoComponents.getCodersOrThrow(\n+        components.registerCoder(windowingStrategy.getWindowFn().windowCoder()));\n+  }\n }",
                "changes": 21
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/pom.xml",
                "deletions": 53,
                "sha": "f066abf449ba1f20077fbd7c07feb362e3a9bb83",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/pom.xml",
                "patch": "@@ -57,54 +57,6 @@\n         <artifactId>maven-jar-plugin</artifactId>\n       </plugin>\n \n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-shade-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <id>bundle-and-repackage</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>shade</goal>\n-            </goals>\n-            <configuration>\n-              <shadeTestJar>true</shadeTestJar>\n-              <artifactSet>\n-                <includes>\n-                  <include>com.google.guava:guava</include>\n-                </includes>\n-              </artifactSet>\n-              <filters>\n-                <filter>\n-                  <artifact>*:*</artifact>\n-                  <excludes>\n-                    <exclude>META-INF/*.SF</exclude>\n-                    <exclude>META-INF/*.DSA</exclude>\n-                    <exclude>META-INF/*.RSA</exclude>\n-                  </excludes>\n-                </filter>\n-              </filters>\n-              <relocations>\n-                <!-- TODO: Once ready, change the following pattern to 'com'\n-                     only, exclude 'org.apache.beam.**', and remove\n-                     the second relocation. -->\n-                <relocation>\n-                  <pattern>com.google.common</pattern>\n-                  <shadedPattern>org.apache.beam.runners.core.repackaged.com.google.common</shadedPattern>\n-                </relocation>\n-                <relocation>\n-                  <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>org.apache.beam.runners.core.repackaged.com.google.thirdparty</shadedPattern>\n-                </relocation>\n-              </relocations>\n-              <transformers>\n-                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n-              </transformers>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n-\n       <!-- Coverage analysis for unit tests. -->\n       <plugin>\n         <groupId>org.jacoco</groupId>\n@@ -125,6 +77,11 @@\n       <artifactId>beam-sdks-common-runner-api</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-runners-core-construction-java</artifactId>\n+    </dependency>\n+\n     <!-- build dependencies -->\n \n     <dependency>\n@@ -153,11 +110,6 @@\n       <artifactId>joda-time</artifactId>\n     </dependency>\n \n-    <dependency>\n-      <groupId>org.slf4j</groupId>\n-      <artifactId>slf4j-api</artifactId>\n-    </dependency>\n-\n     <!-- test dependencies -->\n \n     <!-- Utilities such as WindowMatchers -->",
                "changes": 58
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
                "deletions": 7,
                "sha": "cc7b5747c243cd372cd0ff41b6cb337d41ce98b4",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.HashMap;\n import java.util.Map;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn.Context;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -106,19 +107,17 @@ protected final T getOrCreateStepContext(String stepName,\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@code DoFn.Context#output}\n-   * is called.\n+   * {@link Context#output(Object)} is called.\n    */\n   @Override\n   public void noteOutput(WindowedValue<?> output) {}\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@code DoFn.Context#sideOutput}\n-   * is called.\n+   * {@link Context#output(TupleTag, Object)} is called.\n    */\n   @Override\n-  public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) {}\n+  public void noteOutput(TupleTag<?> tag, WindowedValue<?> output) {}\n \n   /**\n    * Base class for implementations of {@link ExecutionContext.StepContext}.\n@@ -153,8 +152,8 @@ public void noteOutput(WindowedValue<?> output) {\n     }\n \n     @Override\n-    public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) {\n-      executionContext.noteSideOutput(tag, output);\n+    public void noteOutput(TupleTag<?> tag, WindowedValue<?> output) {\n+      executionContext.noteOutput(tag, output);\n     }\n \n     @Override",
                "changes": 13
            },
            {
                "status": "modified",
                "additions": 13,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
                "deletions": 8,
                "sha": "66ad7360fe40b23bcd41e358465a0e05e9e822e3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
                "patch": "@@ -162,13 +162,13 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      context.sideOutput(tag, output);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      context.output(tag, output);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.outputWithTimestamp(tag, output, timestamp);\n     }\n \n     @Override\n@@ -255,13 +255,13 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      context.sideOutput(tag, output);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      context.output(tag, output);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.outputWithTimestamp(tag, output, timestamp);\n     }\n \n     @Override\n@@ -285,6 +285,11 @@ public PaneInfo pane() {\n       return context.pane();\n     }\n \n+    @Override\n+    public void updateWatermark(Instant watermark) {\n+      throw new UnsupportedOperationException(\"Only splittable DoFn's can use updateWatermark()\");\n+    }\n+\n     @Override\n     public BoundedWindow window() {\n       return context.window();",
                "changes": 21
            },
            {
                "status": "modified",
                "additions": 36,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "deletions": 4,
                "sha": "8501e7270b9eafa5f20fc8deb2c0892bd3a9bb33",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "patch": "@@ -17,19 +17,23 @@\n  */\n package org.apache.beam.runners.core;\n \n+import java.util.Collection;\n import java.util.List;\n import org.apache.beam.runners.core.ExecutionContext.StepContext;\n+import org.apache.beam.runners.core.SplittableParDo.ProcessFn;\n import org.apache.beam.runners.core.StatefulDoFnRunner.CleanupTimer;\n import org.apache.beam.runners.core.StatefulDoFnRunner.StateCleaner;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n \n /**\n@@ -59,7 +63,7 @@\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       StepContext stepContext,\n       AggregatorFactory aggregatorFactory,\n       WindowingStrategy<?, ?> windowingStrategy) {\n@@ -69,7 +73,7 @@\n         sideInputReader,\n         outputManager,\n         mainOutputTag,\n-        sideOutputTags,\n+        additionalOutputTags,\n         stepContext,\n         aggregatorFactory,\n         windowingStrategy);\n@@ -86,7 +90,7 @@\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       StepContext stepContext,\n       AggregatorFactory aggregatorFactory,\n       WindowingStrategy<?, ?> windowingStrategy) {\n@@ -96,7 +100,7 @@\n         sideInputReader,\n         outputManager,\n         mainOutputTag,\n-        sideOutputTags,\n+        additionalOutputTags,\n         stepContext,\n         aggregatorFactory,\n         windowingStrategy);\n@@ -146,4 +150,32 @@\n         stateCleaner,\n         droppedDueToLateness);\n   }\n+\n+  public static <InputT, OutputT, RestrictionT>\n+  ProcessFnRunner<InputT, OutputT, RestrictionT>\n+  newProcessFnRunner(\n+      ProcessFn<InputT, OutputT, RestrictionT, ?> fn,\n+      PipelineOptions options,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader,\n+      OutputManager outputManager,\n+      TupleTag<OutputT> mainOutputTag,\n+      List<TupleTag<?>> additionalOutputTags,\n+      StepContext stepContext,\n+      AggregatorFactory aggregatorFactory,\n+      WindowingStrategy<?, ?> windowingStrategy) {\n+    return new ProcessFnRunner<>(\n+        simpleRunner(\n+            options,\n+            fn,\n+            sideInputReader,\n+            outputManager,\n+            mainOutputTag,\n+            additionalOutputTags,\n+            stepContext,\n+            aggregatorFactory,\n+            windowingStrategy),\n+        views,\n+        sideInputReader);\n+  }\n }",
                "changes": 40
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
                "deletions": 7,
                "sha": "ecd30c01dda6332ef8999d9c6313278d3687cabe",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.IOException;\n import java.util.Collection;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn.Context;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -41,17 +42,15 @@\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@link org.apache.beam.sdk.transforms.DoFn.Context#output}\n-   * is called.\n+   * {@link Context#output(TupleTag, Object)} is called.\n    */\n   void noteOutput(WindowedValue<?> output);\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@link org.apache.beam.sdk.transforms.DoFn.Context#sideOutput}\n-   * is called.\n+   * {@link Context#output(TupleTag, Object)} is called.\n    */\n-  void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output);\n+  void noteOutput(TupleTag<?> tag, WindowedValue<?> output);\n \n   /**\n    * Per-step, per-key context used for retrieving state.\n@@ -77,10 +76,10 @@\n \n     /**\n      * Hook for subclasses to implement that will be called whenever\n-     * {@link org.apache.beam.sdk.transforms.DoFn.Context#sideOutput}\n+     * {@link org.apache.beam.sdk.transforms.DoFn.Context#output}\n      * is called.\n      */\n-    void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output);\n+    void noteOutput(TupleTag<?> tag, WindowedValue<?> output);\n \n     /**\n      * Writes the given {@code PCollectionView} data to a globally accessible location.",
                "changes": 13
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
                "deletions": 1,
                "sha": "5508b2e206d4e04272a44cc2fb502264a2110574",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
                "patch": "@@ -19,10 +19,10 @@\n \n import java.util.ArrayList;\n import java.util.List;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.joda.time.Instant;",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "deletions": 1,
                "sha": "bf48df19f980345ade1591b5b2e16a5fe900c3d2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "patch": "@@ -17,12 +17,12 @@\n  */\n package org.apache.beam.runners.core;\n \n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
                "deletions": 4,
                "sha": "0cf6e2d49c3861b61687bca2825345415080d8e5",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
                "patch": "@@ -18,14 +18,14 @@\n package org.apache.beam.runners.core;\n \n import java.util.Collection;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -104,9 +104,9 @@ public void outputWindowedValue(\n       }\n \n       @Override\n-      public <SideOutputT> void sideOutputWindowedValue(\n-              TupleTag<SideOutputT> tag,\n-              SideOutputT output,\n+      public <AdditionalOutputT> void outputWindowedValue(\n+              TupleTag<AdditionalOutputT> tag,\n+              AdditionalOutputT output,\n               Instant timestamp,\n               Collection<? extends BoundedWindow> windows,\n               PaneInfo pane) {",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 35,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
                "deletions": 85,
                "sha": "55b7fc2967bcad59fbd89fd88524ba224ed56abb",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
                "patch": "@@ -17,8 +17,6 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static com.google.common.base.Preconditions.checkNotNull;\n-\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -38,10 +36,11 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.util.CombineFnUtil;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.ReadableState;\n+import org.apache.beam.sdk.util.state.ReadableStates;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n import org.apache.beam.sdk.util.state.StateContext;\n@@ -148,12 +147,12 @@ public InMemoryStateBinder(K key, StateContext<?> c) {\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             final CombineFn<InputT, AccumT, OutputT> combineFn) {\n-      return new InMemoryCombiningValue<K, InputT, AccumT, OutputT>(key, combineFn.<K>asKeyedFn());\n+      return new InMemoryCombiningState<K, InputT, AccumT, OutputT>(key, combineFn.<K>asKeyedFn());\n     }\n \n     @Override\n@@ -164,18 +163,18 @@ public InMemoryStateBinder(K key, StateContext<?> c) {\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n-      return new InMemoryCombiningValue<K, InputT, AccumT, OutputT>(key, combineFn);\n+      return new InMemoryCombiningState<K, InputT, AccumT, OutputT>(key, combineFn);\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValueWithContext(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n       return bindKeyedCombiningValue(address, accumCoder, CombineFnUtil.bindContext(combineFn, c));\n@@ -307,25 +306,25 @@ public String toString() {\n   }\n \n   /**\n-   * An {@link InMemoryState} implementation of {@link AccumulatorCombiningState}.\n+   * An {@link InMemoryState} implementation of {@link CombiningState}.\n    */\n-  public static final class InMemoryCombiningValue<K, InputT, AccumT, OutputT>\n-      implements AccumulatorCombiningState<InputT, AccumT, OutputT>,\n-          InMemoryState<InMemoryCombiningValue<K, InputT, AccumT, OutputT>> {\n+  public static final class InMemoryCombiningState<K, InputT, AccumT, OutputT>\n+      implements CombiningState<InputT, AccumT, OutputT>,\n+          InMemoryState<InMemoryCombiningState<K, InputT, AccumT, OutputT>> {\n     private final K key;\n     private boolean isCleared = true;\n     private final KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn;\n     private AccumT accum;\n \n-    public InMemoryCombiningValue(\n+    public InMemoryCombiningState(\n         K key, KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n       this.key = key;\n       this.combineFn = combineFn;\n       accum = combineFn.createAccumulator(key);\n     }\n \n     @Override\n-    public InMemoryCombiningValue<K, InputT, AccumT, OutputT> readLater() {\n+    public InMemoryCombiningState<K, InputT, AccumT, OutputT> readLater() {\n       return this;\n     }\n \n@@ -384,9 +383,9 @@ public boolean isCleared() {\n     }\n \n     @Override\n-    public InMemoryCombiningValue<K, InputT, AccumT, OutputT> copy() {\n-      InMemoryCombiningValue<K, InputT, AccumT, OutputT> that =\n-          new InMemoryCombiningValue<>(key, combineFn);\n+    public InMemoryCombiningState<K, InputT, AccumT, OutputT> copy() {\n+      InMemoryCombiningState<K, InputT, AccumT, OutputT> that =\n+          new InMemoryCombiningState<>(key, combineFn);\n       if (!this.isCleared) {\n         that.isCleared = this.isCleared;\n         that.addAccum(accum);\n@@ -468,47 +467,22 @@ public void clear() {\n     }\n \n     @Override\n-    public boolean contains(T t) {\n-      return contents.contains(t);\n+    public ReadableState<Boolean> contains(T t) {\n+      return ReadableStates.immediate(contents.contains(t));\n     }\n \n     @Override\n-    public boolean addIfAbsent(T t) {\n-      return contents.add(t);\n+    public ReadableState<Boolean> addIfAbsent(T t) {\n+      boolean alreadyContained = contents.contains(t);\n+      contents.add(t);\n+      return ReadableStates.immediate(!alreadyContained);\n     }\n \n     @Override\n     public void remove(T t) {\n       contents.remove(t);\n     }\n \n-    @Override\n-    public SetState<T> readLater(Iterable<T> elements) {\n-      return this;\n-    }\n-\n-    @Override\n-    public boolean containsAny(Iterable<T> elements) {\n-      elements = checkNotNull(elements);\n-      for (T t : elements) {\n-        if (contents.contains(t)) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    }\n-\n-    @Override\n-    public boolean containsAll(Iterable<T> elements) {\n-      elements = checkNotNull(elements);\n-      for (T t : elements) {\n-        if (!contents.contains(t)) {\n-          return false;\n-        }\n-      }\n-      return true;\n-    }\n-\n     @Override\n     public InMemorySet<T> readLater() {\n       return this;\n@@ -565,8 +539,8 @@ public void clear() {\n     }\n \n     @Override\n-    public V get(K key) {\n-      return contents.get(key);\n+    public ReadableState<V> get(K key) {\n+      return ReadableStates.immediate(contents.get(key));\n     }\n \n     @Override\n@@ -575,13 +549,13 @@ public void put(K key, V value) {\n     }\n \n     @Override\n-    public V putIfAbsent(K key, V value) {\n+    public ReadableState<V> putIfAbsent(K key, V value) {\n       V v = contents.get(key);\n       if (v == null) {\n         v = contents.put(key, value);\n       }\n \n-      return v;\n+      return ReadableStates.immediate(v);\n     }\n \n     @Override\n@@ -590,42 +564,18 @@ public void remove(K key) {\n     }\n \n     @Override\n-    public Iterable<V> get(Iterable<K> keys) {\n-      List<V> values = new ArrayList<>();\n-      for (K k : keys) {\n-        values.add(contents.get(k));\n-      }\n-      return values;\n+    public ReadableState<Iterable<K>> keys() {\n+      return ReadableStates.immediate((Iterable<K>) contents.keySet());\n     }\n \n     @Override\n-    public MapState<K, V> getLater(K k) {\n-      return this;\n-    }\n-\n-    @Override\n-    public MapState<K, V> getLater(Iterable<K> keys) {\n-      return this;\n-    }\n-\n-    @Override\n-    public Iterable<K> keys() {\n-      return contents.keySet();\n-    }\n-\n-    @Override\n-    public Iterable<V> values() {\n-      return contents.values();\n-    }\n-\n-    @Override\n-    public MapState<K, V> iterateLater() {\n-      return this;\n+    public ReadableState<Iterable<V>> values() {\n+      return ReadableStates.immediate((Iterable<V>) contents.values());\n     }\n \n     @Override\n-    public Iterable<Map.Entry<K, V>> iterate() {\n-      return contents.entrySet();\n+    public ReadableState<Iterable<Map.Entry<K, V>>> entries() {\n+      return ReadableStates.immediate((Iterable<Map.Entry<K, V>>) contents.entrySet());\n     }\n \n     @Override",
                "changes": 120
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
                "deletions": 2,
                "sha": "3e875c2c07b81771b7d8c5f2e7cf5817b1c69e5a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
                "patch": "@@ -22,7 +22,7 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.ReadableState;\n \n /**\n@@ -113,7 +113,7 @@ public void onMerge(MergingStateAccessor<K, W> context) {\n   private static class GeneralNonEmptyPanes<K, W extends BoundedWindow>\n       extends NonEmptyPanes<K, W> {\n \n-    private static final StateTag<Object, AccumulatorCombiningState<Long, long[], Long>>\n+    private static final StateTag<Object, CombiningState<Long, long[], Long>>\n         PANE_ADDITIONS_TAG =\n         StateTags.makeSystemTagInternal(StateTags.combiningValueFromInputInternal(\n             \"count\", VarLongCoder.of(), Sum.ofLongs()));",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 22,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
                "deletions": 27,
                "sha": "323edf9419e526e677c51731b18255b996528891",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.beam.sdk.transforms.display.HasDisplayData;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n@@ -135,16 +136,15 @@\n     public abstract void outputWithTimestamp(OutputT output, Instant timestamp);\n \n     /**\n-     * Adds the given element to the side output {@code PCollection} with the\n+     * Adds the given element to the output {@code PCollection} with the\n      * given tag.\n      *\n-     * <p>Once passed to {@code sideOutput} the element should not be modified\n+     * <p>Once passed to {@code output} the element should not be modified\n      * in any way.\n      *\n-     * <p>The caller of {@code ParDo} uses {@link ParDo#withOutputTags withOutputTags} to\n-     * specify the tags of side outputs that it consumes. Non-consumed side\n-     * outputs, e.g., outputs for monitoring purposes only, don't necessarily\n-     * need to be specified.\n+     * <p>The caller of {@code ParDo} uses {@link ParDo.SingleOutput#withOutputTags withOutputTags}\n+     * to specify the tags of outputs that it consumes. Outputs that are not consumed, e.g., outputs\n+     * for monitoring purposes only, don't necessarily need to be specified.\n      *\n      * <p>The output element will have the same timestamp and be in the same\n      * windows as the input element passed to {@link OldDoFn#processElement processElement}.\n@@ -157,34 +157,29 @@\n      * to access any information about the input element. The output element\n      * will have a timestamp of negative infinity.\n      *\n-     * @see ParDo#withOutputTags\n+     * @see ParDo.SingleOutput#withOutputTags\n      */\n-    public abstract <T> void sideOutput(TupleTag<T> tag, T output);\n+    public abstract <T> void output(TupleTag<T> tag, T output);\n \n     /**\n-     * Adds the given element to the specified side output {@code PCollection},\n-     * with the given timestamp.\n+     * Adds the given element to the specified output {@code PCollection}, with the given timestamp.\n      *\n-     * <p>Once passed to {@code sideOutputWithTimestamp} the element should not be\n-     * modified in any way.\n+     * <p>Once passed to {@code outputWithTimestamp} the element should not be modified in any way.\n      *\n-     * <p>If invoked from {@link OldDoFn#processElement processElement}, the timestamp\n-     * must not be older than the input element's timestamp minus\n-     * {@link OldDoFn#getAllowedTimestampSkew getAllowedTimestampSkew}.  The output element will\n-     * be in the same windows as the input element.\n+     * <p>If invoked from {@link OldDoFn#processElement processElement}, the timestamp must not be\n+     * older than the input element's timestamp minus {@link OldDoFn#getAllowedTimestampSkew\n+     * getAllowedTimestampSkew}. The output element will be in the same windows as the input\n+     * element.\n      *\n      * <p>If invoked from {@link #startBundle startBundle} or {@link #finishBundle finishBundle},\n-     * this will attempt to use the\n-     * {@link org.apache.beam.sdk.transforms.windowing.WindowFn}\n-     * of the input {@code PCollection} to determine what windows the element\n-     * should be in, throwing an exception if the {@code WindowFn} attempts\n-     * to access any information about the input element except for the\n-     * timestamp.\n+     * this will attempt to use the {@link org.apache.beam.sdk.transforms.windowing.WindowFn} of the\n+     * input {@code PCollection} to determine what windows the element should be in, throwing an\n+     * exception if the {@code WindowFn} attempts to access any information about the input element\n+     * except for the timestamp.\n      *\n-     * @see ParDo#withOutputTags\n+     * @see ParDo.SingleOutput#withOutputTags\n      */\n-    public abstract <T> void sideOutputWithTimestamp(\n-        TupleTag<T> tag, T output, Instant timestamp);\n+    public abstract <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp);\n \n     /**\n      * Creates an {@link Aggregator} in the {@link OldDoFn} context with the\n@@ -247,11 +242,11 @@ protected final void setupDelegateAggregators() {\n      * window of the main input element.\n      *\n      * <p>See\n-     * {@link org.apache.beam.sdk.transforms.windowing.WindowFn#getSideInputWindow}\n+     * {@link WindowMappingFn#getSideInputWindow}\n      * for how this corresponding window is determined.\n      *\n      * @throws IllegalArgumentException if this is not a side input\n-     * @see ParDo#withSideInputs\n+     * @see ParDo.SingleOutput#withSideInputs\n      */\n     public abstract <T> T sideInput(PCollectionView<T> view);\n ",
                "changes": 49
            },
            {
                "status": "modified",
                "additions": 66,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
                "deletions": 70,
                "sha": "d132af672fb2e41e54fa173b09f99768f2f126bf",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
                "patch": "@@ -97,70 +97,57 @@ public Result invokeProcessElement(\n       final WindowedValue<InputT> element,\n       final TrackerT tracker) {\n     final ProcessContext processContext = new ProcessContext(element, tracker);\n-    DoFn.ProcessContinuation cont =\n-        invoker.invokeProcessElement(\n-            new DoFnInvoker.ArgumentProvider<InputT, OutputT>() {\n-              @Override\n-              public DoFn<InputT, OutputT>.ProcessContext processContext(\n-                  DoFn<InputT, OutputT> doFn) {\n-                return processContext;\n-              }\n-\n-              @Override\n-              public RestrictionTracker<?> restrictionTracker() {\n-                return tracker;\n-              }\n-\n-              // Unsupported methods below.\n-\n-              @Override\n-              public BoundedWindow window() {\n-                throw new UnsupportedOperationException(\n-                    \"Access to window of the element not supported in Splittable DoFn\");\n-              }\n-\n-              @Override\n-              public DoFn<InputT, OutputT>.Context context(DoFn<InputT, OutputT> doFn) {\n-                throw new IllegalStateException(\n-                    \"Should not access context() from @\"\n-                        + DoFn.ProcessElement.class.getSimpleName());\n-              }\n-\n-              @Override\n-              public DoFn<InputT, OutputT>.OnTimerContext onTimerContext(\n-                  DoFn<InputT, OutputT> doFn) {\n-                throw new UnsupportedOperationException(\n-                    \"Access to timers not supported in Splittable DoFn\");\n-              }\n-\n-              @Override\n-              public State state(String stateId) {\n-                throw new UnsupportedOperationException(\n-                    \"Access to state not supported in Splittable DoFn\");\n-              }\n-\n-              @Override\n-              public Timer timer(String timerId) {\n-                throw new UnsupportedOperationException(\n-                    \"Access to timers not supported in Splittable DoFn\");\n-              }\n-            });\n-    RestrictionT residual;\n-    RestrictionT forcedCheckpoint = processContext.extractCheckpoint();\n-    if (cont.shouldResume()) {\n-      if (forcedCheckpoint == null) {\n-        // If no checkpoint was forced, the call returned voluntarily (i.e. all tryClaim() calls\n-        // succeeded) - but we still need to have a checkpoint to resume from.\n-        residual = tracker.checkpoint();\n-      } else {\n-        // A checkpoint was forced - i.e. the call probably (but not guaranteed) returned because of\n-        // a failed tryClaim() call.\n-        residual = forcedCheckpoint;\n-      }\n-    } else {\n-      residual = null;\n-    }\n-    return new Result(residual, cont);\n+    invoker.invokeProcessElement(\n+        new DoFnInvoker.ArgumentProvider<InputT, OutputT>() {\n+          @Override\n+          public DoFn<InputT, OutputT>.ProcessContext processContext(\n+              DoFn<InputT, OutputT> doFn) {\n+            return processContext;\n+          }\n+\n+          @Override\n+          public RestrictionTracker<?> restrictionTracker() {\n+            return tracker;\n+          }\n+\n+          // Unsupported methods below.\n+\n+          @Override\n+          public BoundedWindow window() {\n+            throw new UnsupportedOperationException(\n+                \"Access to window of the element not supported in Splittable DoFn\");\n+          }\n+\n+          @Override\n+          public DoFn<InputT, OutputT>.Context context(DoFn<InputT, OutputT> doFn) {\n+            throw new IllegalStateException(\n+                \"Should not access context() from @\"\n+                    + DoFn.ProcessElement.class.getSimpleName());\n+          }\n+\n+          @Override\n+          public DoFn<InputT, OutputT>.OnTimerContext onTimerContext(\n+              DoFn<InputT, OutputT> doFn) {\n+            throw new UnsupportedOperationException(\n+                \"Access to timers not supported in Splittable DoFn\");\n+          }\n+\n+          @Override\n+          public State state(String stateId) {\n+            throw new UnsupportedOperationException(\n+                \"Access to state not supported in Splittable DoFn\");\n+          }\n+\n+          @Override\n+          public Timer timer(String timerId) {\n+            throw new UnsupportedOperationException(\n+                \"Access to timers not supported in Splittable DoFn\");\n+          }\n+        });\n+\n+    tracker.checkDone();\n+    return new Result(\n+        processContext.extractCheckpoint(), processContext.getLastReportedWatermark());\n   }\n \n   private class ProcessContext extends DoFn<InputT, OutputT>.ProcessContext {\n@@ -176,6 +163,7 @@ public Timer timer(String timerId) {\n     private RestrictionT checkpoint;\n     // A handle on the scheduled action to take a checkpoint.\n     private Future<?> scheduledCheckpoint;\n+    private Instant lastReportedWatermark;\n \n     public ProcessContext(WindowedValue<InputT> element, TrackerT tracker) {\n       fn.super();\n@@ -226,8 +214,7 @@ public InputT element() {\n     public <T> T sideInput(PCollectionView<T> view) {\n       return sideInputReader.get(\n           view,\n-          view.getWindowingStrategyInternal()\n-              .getWindowFn()\n+          view.getWindowMappingFn()\n               .getSideInputWindow(Iterables.getOnlyElement(element.getWindows())));\n     }\n \n@@ -241,6 +228,15 @@ public PaneInfo pane() {\n       return element.getPane();\n     }\n \n+    @Override\n+    public synchronized void updateWatermark(Instant watermark) {\n+      lastReportedWatermark = watermark;\n+    }\n+\n+    public synchronized Instant getLastReportedWatermark() {\n+      return lastReportedWatermark;\n+    }\n+\n     @Override\n     public PipelineOptions getPipelineOptions() {\n       return pipelineOptions;\n@@ -258,13 +254,13 @@ public void outputWithTimestamp(OutputT value, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T value) {\n-      sideOutputWithTimestamp(tag, value, element.getTimestamp());\n+    public <T> void output(TupleTag<T> tag, T value) {\n+      outputWithTimestamp(tag, value, element.getTimestamp());\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T value, Instant timestamp) {\n-      output.sideOutputWindowedValue(\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T value, Instant timestamp) {\n+      output.outputWindowedValue(\n           tag, value, timestamp, element.getWindows(), element.getPane());\n       noteOutput();\n     }",
                "changes": 136
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
                "deletions": 5,
                "sha": "35d6737fa561d99742d2dec82e7bebfb2fa4dc11",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
                "patch": "@@ -25,7 +25,7 @@\n \n /**\n  * An object that can output a value with all of its windowing information to the main output or\n- * a side output.\n+ * any tagged output.\n  */\n public interface OutputWindowedValue<OutputT> {\n   /** Outputs a value with windowing information to the main output. */\n@@ -35,10 +35,10 @@ void outputWindowedValue(\n       Collection<? extends BoundedWindow> windows,\n       PaneInfo pane);\n \n-  /** Outputs a value with windowing information to a side output. */\n-  <SideOutputT> void sideOutputWindowedValue(\n-      TupleTag<SideOutputT> tag,\n-      SideOutputT output,\n+  /** Outputs a value with windowing information to a tagged output. */\n+  <AdditionalOutputT> void outputWindowedValue(\n+      TupleTag<AdditionalOutputT> tag,\n+      AdditionalOutputT output,\n       Instant timestamp,\n       Collection<? extends BoundedWindow> windows,\n       PaneInfo pane);",
                "changes": 10
            },
            {
                "status": "added",
                "additions": 127,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
                "deletions": 0,
                "sha": "3ae3f5068c4607d24ed31f90f02b853871fe522b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
                "patch": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.core;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.runners.core.SplittableParDo.ProcessFn;\n+\n+import com.google.common.collect.Iterables;\n+import java.util.Collection;\n+import java.util.Collections;\n+import org.apache.beam.runners.core.StateNamespaces.WindowNamespace;\n+import org.apache.beam.runners.core.TimerInternals.TimerData;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.joda.time.Instant;\n+\n+/** Runs a {@link ProcessFn} by constructing the appropriate contexts and passing them in. */\n+public class ProcessFnRunner<InputT, OutputT, RestrictionT>\n+    implements PushbackSideInputDoFnRunner<\n+        KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT> {\n+  private final DoFnRunner<\n+          KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+      underlying;\n+  private final Collection<PCollectionView<?>> views;\n+  private final ReadyCheckingSideInputReader sideInputReader;\n+\n+  ProcessFnRunner(\n+      DoFnRunner<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+          underlying,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader) {\n+    this.underlying = underlying;\n+    this.views = views;\n+    this.sideInputReader = sideInputReader;\n+  }\n+\n+  @Override\n+  public void startBundle() {\n+    underlying.startBundle();\n+  }\n+\n+  @Override\n+  public Iterable<WindowedValue<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>>\n+      processElementInReadyWindows(\n+          WindowedValue<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>\n+              windowedKWI) {\n+    checkTrivialOuterWindows(windowedKWI);\n+    BoundedWindow window = getUnderlyingWindow(windowedKWI.getValue());\n+    if (!isReady(window)) {\n+      return Collections.singletonList(windowedKWI);\n+    }\n+    underlying.processElement(windowedKWI);\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public void finishBundle() {\n+    underlying.finishBundle();\n+  }\n+\n+  @Override\n+  public void onTimer(\n+      String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain) {\n+    throw new UnsupportedOperationException(\"User timers unsupported in ProcessFn\");\n+  }\n+\n+  private static <T> void checkTrivialOuterWindows(\n+      WindowedValue<KeyedWorkItem<String, T>> windowedKWI) {\n+    // In practice it will be in 0 or 1 windows (ValueInEmptyWindows or ValueInGlobalWindow)\n+    Collection<? extends BoundedWindow> outerWindows = windowedKWI.getWindows();\n+    if (!outerWindows.isEmpty()) {\n+      checkArgument(\n+          outerWindows.size() == 1,\n+          \"The KeyedWorkItem itself must not be in multiple windows, but was in: %s\",\n+          outerWindows);\n+      BoundedWindow onlyWindow = Iterables.getOnlyElement(outerWindows);\n+      checkArgument(\n+          onlyWindow instanceof GlobalWindow,\n+          \"KeyedWorkItem must be in the Global window, but was in: %s\",\n+          onlyWindow);\n+    }\n+  }\n+\n+  private static <T> BoundedWindow getUnderlyingWindow(KeyedWorkItem<String, T> kwi) {\n+    if (Iterables.isEmpty(kwi.elementsIterable())) {\n+      // ProcessFn sets only a single timer.\n+      TimerData timer = Iterables.getOnlyElement(kwi.timersIterable());\n+      return ((WindowNamespace) timer.getNamespace()).getWindow();\n+    } else {\n+      // KWI must have a single element in elementsIterable, because it follows a GBK by a\n+      // uniquely generated key.\n+      // Additionally, windows must be exploded before GBKIntoKeyedWorkItems, so there's also\n+      // only a single window.\n+      WindowedValue<T> value = Iterables.getOnlyElement(kwi.elementsIterable());\n+      return Iterables.getOnlyElement(value.getWindows());\n+    }\n+  }\n+\n+  private boolean isReady(BoundedWindow mainInputWindow) {\n+    for (PCollectionView<?> view : views) {\n+      BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(mainInputWindow);\n+      if (!sideInputReader.isReady(view, sideInputWindow)) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+}",
                "changes": 127
            },
            {
                "status": "modified",
                "additions": 14,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "deletions": 92,
                "sha": "bab1dc7317fc7cc9f043fa8562c738c724d8a98f",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "patch": "@@ -17,113 +17,35 @@\n  */\n package org.apache.beam.runners.core;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Iterables;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.Set;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.values.PCollectionView;\n import org.joda.time.Instant;\n \n /**\n- * A {@link DoFnRunner} that can refuse to process elements that are not ready, instead returning\n- * them via the {@link #processElementInReadyWindows(WindowedValue)}.\n+ * Interface for runners of {@link DoFn}'s that support pushback when reading side inputs,\n+ * i.e. return elements that could not be processed because they require reading a side input\n+ * window that is not ready.\n  */\n-public class PushbackSideInputDoFnRunner<InputT, OutputT> implements DoFnRunner<InputT, OutputT> {\n-  private final DoFnRunner<InputT, OutputT> underlying;\n-  private final Collection<PCollectionView<?>> views;\n-  private final ReadyCheckingSideInputReader sideInputReader;\n-\n-  private Set<BoundedWindow> notReadyWindows;\n-\n-  public static <InputT, OutputT> PushbackSideInputDoFnRunner<InputT, OutputT> create(\n-      DoFnRunner<InputT, OutputT> underlying,\n-      Collection<PCollectionView<?>> views,\n-      ReadyCheckingSideInputReader sideInputReader) {\n-    return new PushbackSideInputDoFnRunner<>(underlying, views, sideInputReader);\n-  }\n-\n-  private PushbackSideInputDoFnRunner(\n-      DoFnRunner<InputT, OutputT> underlying,\n-      Collection<PCollectionView<?>> views,\n-      ReadyCheckingSideInputReader sideInputReader) {\n-    this.underlying = underlying;\n-    this.views = views;\n-    this.sideInputReader = sideInputReader;\n-  }\n-\n-  @Override\n-  public void startBundle() {\n-    notReadyWindows = new HashSet<>();\n-    underlying.startBundle();\n-  }\n+public interface PushbackSideInputDoFnRunner<InputT, OutputT> {\n+  /** Calls the underlying {@link DoFn.StartBundle} method. */\n+  void startBundle();\n \n   /**\n-   * Call the underlying {@link DoFnRunner#processElement(WindowedValue)} for the provided element\n+   * Call the underlying {@link DoFn.ProcessElement} method for the provided element\n    * for each window the element is in that is ready.\n    *\n    * @param elem the element to process in all ready windows\n    * @return each element that could not be processed because it requires a side input window\n    * that is not ready.\n    */\n-  public Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n-    if (views.isEmpty()) {\n-      // When there are no side inputs, we can preserve the compressed representation.\n-      processElement(elem);\n-      return Collections.emptyList();\n-    }\n-    ImmutableList.Builder<WindowedValue<InputT>> pushedBack = ImmutableList.builder();\n-    for (WindowedValue<InputT> windowElem : elem.explodeWindows()) {\n-      BoundedWindow mainInputWindow = Iterables.getOnlyElement(windowElem.getWindows());\n-      if (isReady(mainInputWindow)) {\n-        // When there are any side inputs, we have to process the element in each window\n-        // individually, to disambiguate access to per-window side inputs.\n-        processElement(windowElem);\n-      } else {\n-        notReadyWindows.add(mainInputWindow);\n-        pushedBack.add(windowElem);\n-      }\n-    }\n-    return pushedBack.build();\n-  }\n-\n-  private boolean isReady(BoundedWindow mainInputWindow) {\n-    if (notReadyWindows.contains(mainInputWindow)) {\n-      return false;\n-    }\n-    for (PCollectionView<?> view : views) {\n-      BoundedWindow sideInputWindow =\n-          view.getWindowingStrategyInternal().getWindowFn().getSideInputWindow(mainInputWindow);\n-      if (!sideInputReader.isReady(view, sideInputWindow)) {\n-        return false;\n-      }\n-    }\n-    return true;\n-  }\n+  Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem);\n \n-  @Override\n-  public void processElement(WindowedValue<InputT> elem) {\n-    underlying.processElement(elem);\n-  }\n+  /** Calls the underlying {@link DoFn.OnTimer} method. */\n+  void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+               TimeDomain timeDomain);\n \n-  @Override\n-  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n-      TimeDomain timeDomain) {\n-    underlying.onTimer(timerId, window, timestamp, timeDomain);\n-  }\n-\n-  /**\n-   * Call the underlying {@link DoFnRunner#finishBundle()}.\n-   */\n-  @Override\n-  public void finishBundle() {\n-    notReadyWindows = null;\n-    underlying.finishBundle();\n-  }\n+  /** Calls the underlying {@link DoFn.FinishBundle} method. */\n+  void finishBundle();\n }\n-",
                "changes": 106
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "deletions": 2,
                "sha": "8493474c8d285d1f7d2bdd95102e13c5965af33f",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "patch": "@@ -514,8 +514,7 @@ public PipelineOptions getPipelineOptions() {\n         public <T> T sideInput(PCollectionView<T> view) {\n           return sideInputReader.get(\n               view,\n-              view.getWindowingStrategyInternal()\n-                  .getWindowFn()\n+              view.getWindowMappingFn()\n                   .getSideInputWindow(mainInputWindow));\n         }\n ",
                "changes": 3
            },
            {
                "status": "modified",
                "additions": 9,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
                "deletions": 9,
                "sha": "26e920abd1bc51184eb2d58807a8160733daa85a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
                "patch": "@@ -31,7 +31,7 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.ValueState;\n import org.apache.beam.sdk.values.PCollectionView;\n \n@@ -71,10 +71,10 @@\n       PCollectionView<?>,\n       StateTag<\n           Object,\n-          AccumulatorCombiningState<\n-              BoundedWindow,\n-              Set<BoundedWindow>,\n-              Set<BoundedWindow>>>> availableWindowsTags;\n+          CombiningState<\n+                        BoundedWindow,\n+                        Set<BoundedWindow>,\n+                        Set<BoundedWindow>>>> availableWindowsTags;\n \n   /**\n    * State tag for the actual contents of each side input per window.\n@@ -106,10 +106,10 @@ public SideInputHandler(\n \n       StateTag<\n           Object,\n-          AccumulatorCombiningState<\n-              BoundedWindow,\n-              Set<BoundedWindow>,\n-              Set<BoundedWindow>>> availableTag = StateTags.combiningValue(\n+          CombiningState<\n+                        BoundedWindow,\n+                        Set<BoundedWindow>,\n+                        Set<BoundedWindow>>> availableTag = StateTags.combiningValue(\n           \"side-input-available-windows-\" + sideInput.getTagInternal().getId(),\n           SetCoder.of(windowCoder),\n           new WindowSetCombineFn());",
                "changes": 18
            },
            {
                "status": "modified",
                "additions": 45,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "deletions": 31,
                "sha": "141bf20ee6e41606a9cf96e93bce2d6378cc9797",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Set;\n@@ -105,7 +106,7 @@ public SimpleDoFnRunner(\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       StepContext stepContext,\n       AggregatorFactory aggregatorFactory,\n       WindowingStrategy<?, ?> windowingStrategy) {\n@@ -132,7 +133,7 @@ public SimpleDoFnRunner(\n             sideInputReader,\n             outputManager,\n             mainOutputTag,\n-            sideOutputTags,\n+            additionalOutputTags,\n             stepContext,\n             aggregatorFactory,\n             windowingStrategy.getWindowFn());\n@@ -256,7 +257,7 @@ public DoFnContext(\n         SideInputReader sideInputReader,\n         OutputManager outputManager,\n         TupleTag<OutputT> mainOutputTag,\n-        List<TupleTag<?>> sideOutputTags,\n+        List<TupleTag<?>> additionalOutputTags,\n         StepContext stepContext,\n         AggregatorFactory aggregatorFactory,\n         WindowFn<?, ?> windowFn) {\n@@ -269,8 +270,8 @@ public DoFnContext(\n       this.outputTags = Sets.newHashSet();\n \n       outputTags.add(mainOutputTag);\n-      for (TupleTag<?> sideOutputTag : sideOutputTags) {\n-        outputTags.add(sideOutputTag);\n+      for (TupleTag<?> additionalOutputTag : additionalOutputTags) {\n+        outputTags.add(additionalOutputTag);\n       }\n \n       this.stepContext = stepContext;\n@@ -354,35 +355,35 @@ void outputWindowedValue(WindowedValue<OutputT> windowedElem) {\n       }\n     }\n \n-    private <T> void sideOutputWindowedValue(\n+    private <T> void outputWindowedValue(\n         TupleTag<T> tag,\n         T output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      sideOutputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n+      outputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n     }\n \n-    private <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n+    private <T> void outputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n       if (!outputTags.contains(tag)) {\n         // This tag wasn't declared nor was it seen before during this execution.\n         // Thus, this must be a new, undeclared and unconsumed output.\n         // To prevent likely user errors, enforce the limit on the number of side\n         // outputs.\n         if (outputTags.size() >= MAX_SIDE_OUTPUTS) {\n           throw new IllegalArgumentException(\n-              \"the number of side outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n+              \"the number of outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n         }\n         outputTags.add(tag);\n       }\n \n       outputManager.output(tag, windowedElem);\n       if (stepContext != null) {\n-        stepContext.noteSideOutput(tag, windowedElem);\n+        stepContext.noteOutput(tag, windowedElem);\n       }\n     }\n \n-    // Following implementations of output, outputWithTimestamp, and sideOutput\n+    // Following implementations of output, outputWithTimestamp, and output\n     // are only accessible in DoFn.startBundle and DoFn.finishBundle, and will be shadowed by\n     // ProcessContext's versions in DoFn.processElement.\n     @Override\n@@ -396,15 +397,15 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutput cannot be null\");\n-      sideOutputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"TupleTag passed to output cannot be null\");\n+      outputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutputWithTimestamp cannot be null\");\n-      sideOutputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"TupleTag passed to outputWithTimestamp cannot be null\");\n+      outputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n@@ -532,14 +533,19 @@ public InputT element() {\n         }\n       }\n       return context.sideInput(\n-          view, view.getWindowingStrategyInternal().getWindowFn().getSideInputWindow(window));\n+          view, view.getWindowMappingFn().getSideInputWindow(window));\n     }\n \n     @Override\n     public PaneInfo pane() {\n       return windowedValue.getPane();\n     }\n \n+    @Override\n+    public void updateWatermark(Instant watermark) {\n+      throw new UnsupportedOperationException(\"Only splittable DoFn's can use updateWatermark()\");\n+    }\n+\n     @Override\n     public void output(OutputT output) {\n       context.outputWindowedValue(windowedValue.withValue(output));\n@@ -553,16 +559,16 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"Tag passed to sideOutput cannot be null\");\n-      context.sideOutputWindowedValue(tag, windowedValue.withValue(output));\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"Tag passed to output cannot be null\");\n+      context.outputWindowedValue(tag, windowedValue.withValue(output));\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"Tag passed to sideOutputWithTimestamp cannot be null\");\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"Tag passed to outputWithTimestamp cannot be null\");\n       checkTimestamp(timestamp);\n-      context.sideOutputWindowedValue(\n+      context.outputWindowedValue(\n           tag, output, timestamp, windowedValue.getWindows(), windowedValue.getPane());\n     }\n \n@@ -575,8 +581,12 @@ public Instant timestamp() {\n       return windowedValue.getWindows();\n     }\n \n+    @SuppressWarnings(\"deprecation\") // Allowed Skew is deprecated for users, but must be respected\n     private void checkTimestamp(Instant timestamp) {\n-      if (timestamp.isBefore(windowedValue.getTimestamp().minus(fn.getAllowedTimestampSkew()))) {\n+      // The documentation of getAllowedTimestampSkew explicitly permits Long.MAX_VALUE to be used\n+      // for infinite skew. Defend against underflow in that case for timestamps before the epoch\n+      if (fn.getAllowedTimestampSkew().getMillis() != Long.MAX_VALUE\n+          && timestamp.isBefore(windowedValue.getTimestamp().minus(fn.getAllowedTimestampSkew()))) {\n         throw new IllegalArgumentException(\n             String.format(\n                 \"Cannot output with timestamp %s. Output timestamps must be no earlier than the \"\n@@ -766,22 +776,26 @@ public PipelineOptions getPipelineOptions() {\n \n     @Override\n     public void output(OutputT output) {\n-      context.outputWithTimestamp(output, timestamp);\n+      context.outputWindowedValue(\n+          output, timestamp(), Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override\n     public void outputWithTimestamp(OutputT output, Instant timestamp) {\n-      context.outputWithTimestamp(output, timestamp);\n+      context.outputWindowedValue(\n+          output, timestamp, Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      context.outputWindowedValue(\n+          tag, output, timestamp, Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.outputWindowedValue(\n+          tag, output, timestamp, Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override",
                "changes": 76
            },
            {
                "status": "modified",
                "additions": 35,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "deletions": 30,
                "sha": "6320a3ac04cbfb0630ff600156619e0dc1307dea",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "patch": "@@ -60,19 +60,24 @@\n   /** The context used for running the {@link OldDoFn}. */\n   private final DoFnContext<InputT, OutputT> context;\n \n-  public SimpleOldDoFnRunner(PipelineOptions options, OldDoFn<InputT, OutputT> fn,\n+  public SimpleOldDoFnRunner(\n+      PipelineOptions options,\n+      OldDoFn<InputT, OutputT> fn,\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n-      TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> sideOutputTags, StepContext stepContext,\n-      AggregatorFactory aggregatorFactory, WindowingStrategy<?, ?> windowingStrategy) {\n+      TupleTag<OutputT> mainOutputTag,\n+      List<TupleTag<?>> additionalOutputTags,\n+      StepContext stepContext,\n+      AggregatorFactory aggregatorFactory,\n+      WindowingStrategy<?, ?> windowingStrategy) {\n     this.fn = fn;\n     this.context = new DoFnContext<>(\n         options,\n         fn,\n         sideInputReader,\n         outputManager,\n         mainOutputTag,\n-        sideOutputTags,\n+        additionalOutputTags,\n         stepContext,\n         aggregatorFactory,\n         windowingStrategy == null ? null : windowingStrategy.getWindowFn());\n@@ -177,7 +182,7 @@ public DoFnContext(PipelineOptions options,\n                        SideInputReader sideInputReader,\n                        OutputManager outputManager,\n                        TupleTag<OutputT> mainOutputTag,\n-                       List<TupleTag<?>> sideOutputTags,\n+                       List<TupleTag<?>> additionalOutputTags,\n                        StepContext stepContext,\n                        AggregatorFactory aggregatorFactory,\n                        WindowFn<?, ?> windowFn) {\n@@ -190,8 +195,8 @@ public DoFnContext(PipelineOptions options,\n       this.outputTags = Sets.newHashSet();\n \n       outputTags.add(mainOutputTag);\n-      for (TupleTag<?> sideOutputTag : sideOutputTags) {\n-        outputTags.add(sideOutputTag);\n+      for (TupleTag<?> additionalOutputTag : additionalOutputTags) {\n+        outputTags.add(additionalOutputTag);\n       }\n \n       this.stepContext = stepContext;\n@@ -273,34 +278,34 @@ void outputWindowedValue(WindowedValue<OutputT> windowedElem) {\n       }\n     }\n \n-    private <T> void sideOutputWindowedValue(TupleTag<T> tag,\n+    private <T> void outputWindowedValue(TupleTag<T> tag,\n                                                T output,\n                                                Instant timestamp,\n                                                Collection<? extends BoundedWindow> windows,\n                                                PaneInfo pane) {\n-      sideOutputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n+      outputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n     }\n \n-    private <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n+    private <T> void outputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n       if (!outputTags.contains(tag)) {\n         // This tag wasn't declared nor was it seen before during this execution.\n         // Thus, this must be a new, undeclared and unconsumed output.\n         // To prevent likely user errors, enforce the limit on the number of side\n         // outputs.\n         if (outputTags.size() >= MAX_SIDE_OUTPUTS) {\n           throw new IllegalArgumentException(\n-              \"the number of side outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n+              \"the number of outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n         }\n         outputTags.add(tag);\n       }\n \n       outputManager.output(tag, windowedElem);\n       if (stepContext != null) {\n-        stepContext.noteSideOutput(tag, windowedElem);\n+        stepContext.noteOutput(tag, windowedElem);\n       }\n     }\n \n-    // Following implementations of output, outputWithTimestamp, and sideOutput\n+    // Following implementations of output, outputWithTimestamp, and output\n     // are only accessible in OldDoFn.startBundle and OldDoFn.finishBundle, and will be shadowed by\n     // ProcessContext's versions in OldDoFn.processElement.\n     @Override\n@@ -314,15 +319,15 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutput cannot be null\");\n-      sideOutputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"TupleTag passed to output cannot be null\");\n+      outputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutputWithTimestamp cannot be null\");\n-      sideOutputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"TupleTag passed to outputWithTimestamp cannot be null\");\n+      outputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n@@ -389,7 +394,7 @@ public InputT element() {\n         }\n       }\n       return context.sideInput(\n-          view, view.getWindowingStrategyInternal().getWindowFn().getSideInputWindow(window));\n+          view, view.getWindowMappingFn().getSideInputWindow(window));\n     }\n \n     @Override\n@@ -428,16 +433,16 @@ void outputWindowedValue(\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"Tag passed to sideOutput cannot be null\");\n-      context.sideOutputWindowedValue(tag, windowedValue.withValue(output));\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"Tag passed to output cannot be null\");\n+      context.outputWindowedValue(tag, windowedValue.withValue(output));\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"Tag passed to sideOutputWithTimestamp cannot be null\");\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"Tag passed to outputWithTimestamp cannot be null\");\n       checkTimestamp(timestamp);\n-      context.sideOutputWindowedValue(\n+      context.outputWindowedValue(\n           tag, output, timestamp, windowedValue.getWindows(), windowedValue.getPane());\n     }\n \n@@ -471,13 +476,13 @@ public void outputWindowedValue(OutputT output, Instant timestamp,\n         }\n \n         @Override\n-        public <SideOutputT> void sideOutputWindowedValue(\n-            TupleTag<SideOutputT> tag,\n-            SideOutputT output,\n+        public <AdditionalOutputT> void outputWindowedValue(\n+            TupleTag<AdditionalOutputT> tag,\n+            AdditionalOutputT output,\n             Instant timestamp,\n             Collection<? extends BoundedWindow> windows,\n             PaneInfo pane) {\n-          context.sideOutputWindowedValue(tag, output, timestamp, windows, pane);\n+          context.outputWindowedValue(tag, output, timestamp, windows, pane);\n         }\n \n         @Override",
                "changes": 65
            },
            {
                "status": "added",
                "additions": 115,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
                "deletions": 0,
                "sha": "50d301bc18e6ac7f363aabbe0d6c329f85110af1",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
                "patch": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.core;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.joda.time.Instant;\n+\n+/**\n+ * A {@link DoFnRunner} that can refuse to process elements that are not ready, instead returning\n+ * them via the {@link #processElementInReadyWindows(WindowedValue)}.\n+ */\n+public class SimplePushbackSideInputDoFnRunner<InputT, OutputT>\n+    implements PushbackSideInputDoFnRunner<InputT, OutputT> {\n+  private final DoFnRunner<InputT, OutputT> underlying;\n+  private final Collection<PCollectionView<?>> views;\n+  private final ReadyCheckingSideInputReader sideInputReader;\n+\n+  private Set<BoundedWindow> notReadyWindows;\n+\n+  public static <InputT, OutputT> SimplePushbackSideInputDoFnRunner<InputT, OutputT> create(\n+      DoFnRunner<InputT, OutputT> underlying,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader) {\n+    return new SimplePushbackSideInputDoFnRunner<>(underlying, views, sideInputReader);\n+  }\n+\n+  private SimplePushbackSideInputDoFnRunner(\n+      DoFnRunner<InputT, OutputT> underlying,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader) {\n+    this.underlying = underlying;\n+    this.views = views;\n+    this.sideInputReader = sideInputReader;\n+  }\n+\n+  @Override\n+  public void startBundle() {\n+    notReadyWindows = new HashSet<>();\n+    underlying.startBundle();\n+  }\n+\n+  @Override\n+  public Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n+    if (views.isEmpty()) {\n+      // When there are no side inputs, we can preserve the compressed representation.\n+      underlying.processElement(elem);\n+      return Collections.emptyList();\n+    }\n+    ImmutableList.Builder<WindowedValue<InputT>> pushedBack = ImmutableList.builder();\n+    for (WindowedValue<InputT> windowElem : elem.explodeWindows()) {\n+      BoundedWindow mainInputWindow = Iterables.getOnlyElement(windowElem.getWindows());\n+      if (isReady(mainInputWindow)) {\n+        // When there are any side inputs, we have to process the element in each window\n+        // individually, to disambiguate access to per-window side inputs.\n+        underlying.processElement(windowElem);\n+      } else {\n+        notReadyWindows.add(mainInputWindow);\n+        pushedBack.add(windowElem);\n+      }\n+    }\n+    return pushedBack.build();\n+  }\n+\n+  private boolean isReady(BoundedWindow mainInputWindow) {\n+    if (notReadyWindows.contains(mainInputWindow)) {\n+      return false;\n+    }\n+    for (PCollectionView<?> view : views) {\n+      BoundedWindow sideInputWindow =\n+          view.getWindowMappingFn().getSideInputWindow(mainInputWindow);\n+      if (!sideInputReader.isReady(view, sideInputWindow)) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+                      TimeDomain timeDomain) {\n+    underlying.onTimer(timerId, window, timestamp, timeDomain);\n+  }\n+\n+  @Override\n+  public void finishBundle() {\n+    notReadyWindows = null;\n+    underlying.finishBundle();\n+  }\n+}\n+",
                "changes": 115
            },
            {
                "status": "modified",
                "additions": 86,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "deletions": 68,
                "sha": "31d89eec265b72fbe5aebacb073003ebb20d036b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "patch": "@@ -19,10 +19,8 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkNotNull;\n-import static com.google.common.base.Preconditions.checkState;\n \n import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Iterables;\n import java.util.List;\n import java.util.UUID;\n@@ -82,14 +80,14 @@\n @Experimental(Experimental.Kind.SPLITTABLE_DO_FN)\n public class SplittableParDo<InputT, OutputT, RestrictionT>\n     extends PTransform<PCollection<InputT>, PCollectionTuple> {\n-  private final ParDo.BoundMulti<InputT, OutputT> parDo;\n+  private final ParDo.MultiOutput<InputT, OutputT> parDo;\n \n   /**\n    * Creates the transform for the given original multi-output {@link ParDo}.\n    *\n    * @param parDo The splittable {@link ParDo} transform.\n    */\n-  public SplittableParDo(ParDo.BoundMulti<InputT, OutputT> parDo) {\n+  public SplittableParDo(ParDo.MultiOutput<InputT, OutputT> parDo) {\n     checkNotNull(parDo, \"parDo must not be null\");\n     this.parDo = parDo;\n     checkArgument(\n@@ -115,10 +113,10 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n             fn,\n             input.getCoder(),\n             restrictionCoder,\n-            input.getWindowingStrategy(),\n+            (WindowingStrategy<InputT, ?>) input.getWindowingStrategy(),\n             parDo.getSideInputs(),\n             parDo.getMainOutputTag(),\n-            parDo.getSideOutputTags()));\n+            parDo.getAdditionalOutputTags()));\n   }\n \n   private static <InputT, OutputT, RestrictionT>\n@@ -138,6 +136,12 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n             .setCoder(splitCoder)\n             .apply(\"Split restriction\", ParDo.of(new SplitRestrictionFn<InputT, RestrictionT>(fn)))\n             .setCoder(splitCoder)\n+            // ProcessFn requires all input elements to be in a single window and have a single\n+            // element per work item. This must precede the unique keying so each key has a single\n+            // associated element.\n+            .apply(\n+                \"Explode windows\",\n+                ParDo.of(new ExplodeWindowsFn<ElementAndRestriction<InputT, RestrictionT>>()))\n             .apply(\n                 \"Assign unique key\",\n                 WithKeys.of(new RandomUniqueKeyFn<ElementAndRestriction<InputT, RestrictionT>>()))\n@@ -157,6 +161,18 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n     return keyedWorkItems;\n   }\n \n+  /**\n+   * A {@link DoFn} that forces each of its outputs to be in a single window, by indicating to the\n+   * runner that it observes the window of its input element, so the runner is forced to apply it to\n+   * each input in a single window and thus its output is also in a single window.\n+   */\n+  private static class ExplodeWindowsFn<InputT> extends DoFn<InputT, InputT> {\n+    @ProcessElement\n+    public void process(ProcessContext c, BoundedWindow window) {\n+      c.output(c.element());\n+    }\n+  }\n+\n   /**\n    * Runner-specific primitive {@link GroupByKey GroupByKey-like} {@link PTransform} that produces\n    * {@link KeyedWorkItem KeyedWorkItems} so that downstream transforms can access state and timers.\n@@ -185,33 +201,34 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n     private final DoFn<InputT, OutputT> fn;\n     private final Coder<InputT> elementCoder;\n     private final Coder<RestrictionT> restrictionCoder;\n-    private final WindowingStrategy<?, ?> windowingStrategy;\n+    private final WindowingStrategy<InputT, ?> windowingStrategy;\n     private final List<PCollectionView<?>> sideInputs;\n     private final TupleTag<OutputT> mainOutputTag;\n-    private final TupleTagList sideOutputTags;\n+    private final TupleTagList additionalOutputTags;\n \n     /**\n      * @param fn the splittable {@link DoFn}.\n      * @param windowingStrategy the {@link WindowingStrategy} of the input collection.\n      * @param sideInputs list of side inputs that should be available to the {@link DoFn}.\n      * @param mainOutputTag {@link TupleTag Tag} of the {@link DoFn DoFn's} main output.\n-     * @param sideOutputTags {@link TupleTagList Tags} of the {@link DoFn DoFn's} side outputs.\n+     * @param additionalOutputTags {@link TupleTagList Tags} of the {@link DoFn DoFn's} additional\n+     *     outputs.\n      */\n     public ProcessElements(\n         DoFn<InputT, OutputT> fn,\n         Coder<InputT> elementCoder,\n         Coder<RestrictionT> restrictionCoder,\n-        WindowingStrategy<?, ?> windowingStrategy,\n+        WindowingStrategy<InputT, ?> windowingStrategy,\n         List<PCollectionView<?>> sideInputs,\n         TupleTag<OutputT> mainOutputTag,\n-        TupleTagList sideOutputTags) {\n+        TupleTagList additionalOutputTags) {\n       this.fn = fn;\n       this.elementCoder = elementCoder;\n       this.restrictionCoder = restrictionCoder;\n       this.windowingStrategy = windowingStrategy;\n       this.sideInputs = sideInputs;\n       this.mainOutputTag = mainOutputTag;\n-      this.sideOutputTags = sideOutputTags;\n+      this.additionalOutputTags = additionalOutputTags;\n     }\n \n     public DoFn<InputT, OutputT> getFn() {\n@@ -226,14 +243,14 @@ public ProcessElements(\n       return mainOutputTag;\n     }\n \n-    public TupleTagList getSideOutputTags() {\n-      return sideOutputTags;\n+    public TupleTagList getAdditionalOutputTags() {\n+      return additionalOutputTags;\n     }\n \n     public ProcessFn<InputT, OutputT, RestrictionT, TrackerT> newProcessFn(\n         DoFn<InputT, OutputT> fn) {\n       return new SplittableParDo.ProcessFn<>(\n-          fn, elementCoder, restrictionCoder, windowingStrategy.getWindowFn().windowCoder());\n+          fn, elementCoder, restrictionCoder, windowingStrategy);\n     }\n \n     @Override\n@@ -244,11 +261,11 @@ public PCollectionTuple expand(\n       PCollectionTuple outputs =\n           PCollectionTuple.ofPrimitiveOutputsInternal(\n               input.getPipeline(),\n-              TupleTagList.of(mainOutputTag).and(sideOutputTags.getAll()),\n+              TupleTagList.of(mainOutputTag).and(additionalOutputTags.getAll()),\n               windowingStrategy,\n               input.isBounded().and(signature.isBoundedPerElement()));\n \n-      // Set output type descriptor similarly to how ParDo.BoundMulti does it.\n+      // Set output type descriptor similarly to how ParDo.MultiOutput does it.\n       outputs.get(mainOutputTag).setTypeDescriptor(fn.getOutputTypeDescriptor());\n \n       return outputs;\n@@ -260,7 +277,7 @@ public PCollectionTuple expand(\n             input,\n         TypedPValue<T> output)\n         throws CannotProvideCoderException {\n-      // Similar logic to ParDo.BoundMulti.getDefaultOutputCoder.\n+      // Similar logic to ParDo.MultiOutput.getDefaultOutputCoder.\n       @SuppressWarnings(\"unchecked\")\n       KeyedWorkItemCoder<String, ElementAndRestriction<InputT, RestrictionT>> kwiCoder =\n           (KeyedWorkItemCoder) input.getCoder();\n@@ -316,6 +333,13 @@ public void processElement(ProcessContext context) {\n    * The heart of splittable {@link DoFn} execution: processes a single (element, restriction) pair\n    * by creating a tracker for the restriction and checkpointing/resuming processing later if\n    * necessary.\n+   *\n+   * <p>Takes {@link KeyedWorkItem} and assumes that the KeyedWorkItem contains a single element\n+   * (or a single timer set by {@link ProcessFn itself}, in a single window. This is necessary\n+   * because {@link ProcessFn} sets timers, and timers are namespaced to a single window and it\n+   * should be the window of the input element.\n+   *\n+   * <p>See also: https://issues.apache.org/jira/browse/BEAM-1983\n    */\n   @VisibleForTesting\n   public static class ProcessFn<\n@@ -324,14 +348,12 @@ public void processElement(ProcessContext context) {\n     /**\n      * The state cell containing a watermark hold for the output of this {@link DoFn}. The hold is\n      * acquired during the first {@link DoFn.ProcessElement} call for each element and restriction,\n-     * and is released when the {@link DoFn.ProcessElement} call returns {@link\n-     * DoFn.ProcessContinuation#stop}.\n+     * and is released when the {@link DoFn.ProcessElement} call returns and there is no residual\n+     * restriction captured by the {@link SplittableProcessElementInvoker}.\n      *\n      * <p>A hold is needed to avoid letting the output watermark immediately progress together with\n      * the input watermark when the first {@link DoFn.ProcessElement} call for this element\n      * completes.\n-     *\n-     * <p>The hold is updated with the future output watermark reported by ProcessContinuation.\n      */\n     private static final StateTag<Object, WatermarkHoldState<GlobalWindow>> watermarkHoldTag =\n         StateTags.makeSystemTagInternal(\n@@ -352,7 +374,9 @@ public void processElement(ProcessContext context) {\n     private StateTag<Object, ValueState<RestrictionT>> restrictionTag;\n \n     private final DoFn<InputT, OutputT> fn;\n-    private final Coder<? extends BoundedWindow> windowCoder;\n+    private final Coder<InputT> elementCoder;\n+    private final Coder<RestrictionT> restrictionCoder;\n+    private final WindowingStrategy<InputT, ?> inputWindowingStrategy;\n \n     private transient StateInternalsFactory<String> stateInternalsFactory;\n     private transient TimerInternalsFactory<String> timerInternalsFactory;\n@@ -365,11 +389,16 @@ public ProcessFn(\n         DoFn<InputT, OutputT> fn,\n         Coder<InputT> elementCoder,\n         Coder<RestrictionT> restrictionCoder,\n-        Coder<? extends BoundedWindow> windowCoder) {\n+        WindowingStrategy<InputT, ?> inputWindowingStrategy) {\n       this.fn = fn;\n-      this.windowCoder = windowCoder;\n+      this.elementCoder = elementCoder;\n+      this.restrictionCoder = restrictionCoder;\n+      this.inputWindowingStrategy = inputWindowingStrategy;\n       this.elementTag =\n-          StateTags.value(\"element\", WindowedValue.getFullCoder(elementCoder, this.windowCoder));\n+          StateTags.value(\n+              \"element\",\n+              WindowedValue.getFullCoder(\n+                  elementCoder, inputWindowingStrategy.getWindowFn().windowCoder()));\n       this.restrictionTag = StateTags.value(\"restriction\", restrictionCoder);\n     }\n \n@@ -390,6 +419,18 @@ public void setProcessElementInvoker(\n       return fn;\n     }\n \n+    public Coder<InputT> getElementCoder() {\n+      return elementCoder;\n+    }\n+\n+    public Coder<RestrictionT> getRestrictionCoder() {\n+      return restrictionCoder;\n+    }\n+\n+    public WindowingStrategy<InputT, ?> getInputWindowingStrategy() {\n+      return inputWindowingStrategy;\n+    }\n+\n     @Setup\n     public void setup() throws Exception {\n       invoker = DoFnInvokers.invokerFor(fn);\n@@ -423,7 +464,18 @@ public void processElement(final ProcessContext c) {\n       // Subsequent calls are timer firings and the element has to be retrieved from the state.\n       TimerInternals.TimerData timer = Iterables.getOnlyElement(c.element().timersIterable(), null);\n       boolean isSeedCall = (timer == null);\n-      StateNamespace stateNamespace = isSeedCall ? StateNamespaces.global() : timer.getNamespace();\n+      StateNamespace stateNamespace;\n+      if (isSeedCall) {\n+        WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue =\n+            Iterables.getOnlyElement(c.element().elementsIterable());\n+        BoundedWindow window = Iterables.getOnlyElement(windowedValue.getWindows());\n+        stateNamespace =\n+            StateNamespaces.window(\n+                (Coder<BoundedWindow>) inputWindowingStrategy.getWindowFn().windowCoder(), window);\n+      } else {\n+        stateNamespace = timer.getNamespace();\n+      }\n+\n       ValueState<WindowedValue<InputT>> elementState =\n           stateInternals.state(stateNamespace, elementTag);\n       ValueState<RestrictionT> restrictionState =\n@@ -433,15 +485,8 @@ public void processElement(final ProcessContext c) {\n \n       ElementAndRestriction<WindowedValue<InputT>, RestrictionT> elementAndRestriction;\n       if (isSeedCall) {\n-        // The element and restriction are available in c.element().\n-        // elementsIterable() will, by construction of SplittableParDo, contain the same value\n-        // potentially in several different windows. We implode this into a single WindowedValue\n-        // in order to simplify the rest of the code and avoid iterating over elementsIterable()\n-        // explicitly. The windows of this WindowedValue will be propagated to windows of the\n-        // output. This is correct because a splittable DoFn is not allowed to inspect the window\n-        // of its element.\n         WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue =\n-            implodeWindows(c.element().elementsIterable());\n+            Iterables.getOnlyElement(c.element().elementsIterable());\n         WindowedValue<InputT> element = windowedValue.withValue(windowedValue.getValue().element());\n         elementState.write(element);\n         elementAndRestriction =\n@@ -461,50 +506,23 @@ public void processElement(final ProcessContext c) {\n               invoker, elementAndRestriction.element(), tracker);\n \n       // Save state for resuming.\n-      if (!result.getContinuation().shouldResume()) {\n+      if (result.getResidualRestriction() == null) {\n         // All work for this element/restriction is completed. Clear state and release hold.\n         elementState.clear();\n         restrictionState.clear();\n         holdState.clear();\n         return;\n       }\n       restrictionState.write(result.getResidualRestriction());\n-      Instant futureOutputWatermark = result.getContinuation().getWatermark();\n+      Instant futureOutputWatermark = result.getFutureOutputWatermark();\n       if (futureOutputWatermark == null) {\n         futureOutputWatermark = elementAndRestriction.element().getTimestamp();\n       }\n-      Instant wakeupTime =\n-          timerInternals.currentProcessingTime().plus(result.getContinuation().resumeDelay());\n       holdState.add(futureOutputWatermark);\n       // Set a timer to continue processing this element.\n       timerInternals.setTimer(\n-          TimerInternals.TimerData.of(stateNamespace, wakeupTime, TimeDomain.PROCESSING_TIME));\n-    }\n-\n-    /**\n-     * Does the opposite of {@link WindowedValue#explodeWindows()} - creates a single {@link\n-     * WindowedValue} from a collection of {@link WindowedValue}'s that is known to contain copies\n-     * of the same value with the same timestamp, but different window sets.\n-     *\n-     * <p>This is only legal to do because we know that {@link RandomUniqueKeyFn} created unique\n-     * keys for every {@link ElementAndRestriction}, so if there's multiple {@link WindowedValue}'s\n-     * for the same key, that means only that the windows of that {@link ElementAndRestriction} are\n-     * being delivered separately rather than all at once. It is also legal to do because splittable\n-     * {@link DoFn} is not allowed to access the window of its element, so we can propagate the full\n-     * set of windows of its input to its output.\n-     */\n-    private static <InputT, RestrictionT>\n-        WindowedValue<ElementAndRestriction<InputT, RestrictionT>> implodeWindows(\n-            Iterable<WindowedValue<ElementAndRestriction<InputT, RestrictionT>>> values) {\n-      WindowedValue<ElementAndRestriction<InputT, RestrictionT>> first =\n-          Iterables.getFirst(values, null);\n-      checkState(first != null, \"Got a KeyedWorkItem with no elements and no timers\");\n-      ImmutableList.Builder<BoundedWindow> windows = ImmutableList.builder();\n-      for (WindowedValue<ElementAndRestriction<InputT, RestrictionT>> value : values) {\n-        windows.addAll(value.getWindows());\n-      }\n-      return WindowedValue.of(\n-          first.getValue(), first.getTimestamp(), windows.build(), first.getPane());\n+          TimerInternals.TimerData.of(\n+              stateNamespace, timerInternals.currentProcessingTime(), TimeDomain.PROCESSING_TIME));\n     }\n \n     private DoFn<InputT, OutputT>.Context wrapContext(final Context baseContext) {\n@@ -525,12 +543,12 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n         }\n \n         @Override\n-        public <T> void sideOutput(TupleTag<T> tag, T output) {\n+        public <T> void output(TupleTag<T> tag, T output) {\n           throwUnsupportedOutput();\n         }\n \n         @Override\n-        public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+        public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n           throwUnsupportedOutput();\n         }\n ",
                "changes": 154
            },
            {
                "status": "modified",
                "additions": 11,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
                "deletions": 11,
                "sha": "ced6c015039f81062376051faaa77c60b4d2f74d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.joda.time.Instant;\n \n /**\n  * A runner-specific hook for invoking a {@link DoFn.ProcessElement} method for a splittable {@link\n@@ -31,34 +32,33 @@\n     InputT, OutputT, RestrictionT, TrackerT extends RestrictionTracker<RestrictionT>> {\n   /** Specifies how to resume a splittable {@link DoFn.ProcessElement} call. */\n   public class Result {\n-    @Nullable private final RestrictionT residualRestriction;\n-    private final DoFn.ProcessContinuation continuation;\n+    @Nullable\n+    private final RestrictionT residualRestriction;\n+    private final Instant futureOutputWatermark;\n \n     public Result(\n-        @Nullable RestrictionT residualRestriction, DoFn.ProcessContinuation continuation) {\n+        @Nullable RestrictionT residualRestriction, Instant futureOutputWatermark) {\n       this.residualRestriction = residualRestriction;\n-      this.continuation = continuation;\n+      this.futureOutputWatermark = futureOutputWatermark;\n     }\n \n-    /**\n-     * Can be {@code null} only if {@link #getContinuation} specifies the call should not resume.\n-     */\n+    /** If {@code null}, means the call should not resume. */\n     @Nullable\n     public RestrictionT getResidualRestriction() {\n       return residualRestriction;\n     }\n \n-    public DoFn.ProcessContinuation getContinuation() {\n-      return continuation;\n+    public Instant getFutureOutputWatermark() {\n+      return futureOutputWatermark;\n     }\n   }\n \n   /**\n    * Invokes the {@link DoFn.ProcessElement} method using the given {@link DoFnInvoker} for the\n    * original {@link DoFn}, on the given element and with the given {@link RestrictionTracker}.\n    *\n-   * @return Information on how to resume the call: residual restriction and a {@link\n-   *     DoFn.ProcessContinuation}.\n+   * @return Information on how to resume the call: residual restriction and a\n+   * future output watermark.\n    */\n   public abstract Result invokeProcessElement(\n       DoFnInvoker<InputT, OutputT> invoker, WindowedValue<InputT> element, TrackerT tracker);",
                "changes": 22
            },
            {
                "status": "modified",
                "additions": 8,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
                "deletions": 8,
                "sha": "34108507d051258085136f643b5c23d4c2eca369",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
                "patch": "@@ -24,9 +24,9 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -159,7 +159,7 @@\n    * Prefetch all combining value state for {@code address} across all merging windows in {@code\n    * context}.\n    */\n-  public static <K, StateT extends CombiningState<?, ?>, W extends BoundedWindow> void\n+  public static <K, StateT extends GroupingState<?, ?>, W extends BoundedWindow> void\n       prefetchCombiningValues(MergingStateAccessor<K, W> context,\n           StateTag<? super K, StateT> address) {\n     for (StateT state : context.accessInEachMergingWindow(address).values()) {\n@@ -172,7 +172,7 @@\n    */\n   public static <K, InputT, AccumT, OutputT, W extends BoundedWindow> void mergeCombiningValues(\n       MergingStateAccessor<K, W> context,\n-      StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address) {\n+      StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address) {\n     mergeCombiningValues(\n         context.accessInEachMergingWindow(address).values(), context.access(address));\n   }\n@@ -182,8 +182,8 @@\n    * {@code result}.\n    */\n   public static <InputT, AccumT, OutputT, W extends BoundedWindow> void mergeCombiningValues(\n-      Collection<AccumulatorCombiningState<InputT, AccumT, OutputT>> sources,\n-      AccumulatorCombiningState<InputT, AccumT, OutputT> result) {\n+      Collection<CombiningState<InputT, AccumT, OutputT>> sources,\n+      CombiningState<InputT, AccumT, OutputT> result) {\n     if (sources.isEmpty()) {\n       // Nothing to merge.\n       return;\n@@ -194,18 +194,18 @@\n     }\n     // Prefetch.\n     List<ReadableState<AccumT>> futures = new ArrayList<>(sources.size());\n-    for (AccumulatorCombiningState<InputT, AccumT, OutputT> source : sources) {\n+    for (CombiningState<InputT, AccumT, OutputT> source : sources) {\n       prefetchRead(source);\n     }\n     // Read.\n     List<AccumT> accumulators = new ArrayList<>(futures.size());\n-    for (AccumulatorCombiningState<InputT, AccumT, OutputT> source : sources) {\n+    for (CombiningState<InputT, AccumT, OutputT> source : sources) {\n       accumulators.add(source.getAccum());\n     }\n     // Merge (possibly update and return one of the existing accumulators).\n     AccumT merged = result.mergeAccumulators(accumulators);\n     // Clear sources.\n-    for (AccumulatorCombiningState<InputT, AccumT, OutputT> source : sources) {\n+    for (CombiningState<InputT, AccumT, OutputT> source : sources) {\n       source.clear();\n     }\n     // Update result.",
                "changes": 16
            },
            {
                "status": "modified",
                "additions": 7,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
                "deletions": 7,
                "sha": "12c59adb8e449aef268950203e749a4e02dd19a1",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
                "patch": "@@ -28,8 +28,8 @@\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -94,20 +94,20 @@\n         StateTag<? super K, MapState<KeyT, ValueT>> spec,\n         Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder);\n \n-    <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT> bindCombiningValue(\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+    <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT> bindCombiningValue(\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n         Coder<AccumT> accumCoder,\n         CombineFn<InputT, AccumT, OutputT> combineFn);\n \n     <InputT, AccumT, OutputT>\n-    AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValue(\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+    CombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValue(\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n         Coder<AccumT> accumCoder,\n         KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn);\n \n     <InputT, AccumT, OutputT>\n-    AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValueWithContext(\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+    CombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValueWithContext(\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n         Coder<AccumT> accumCoder,\n         KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT>\n             combineFn);",
                "changes": 14
            },
            {
                "status": "modified",
                "additions": 16,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
                "deletions": 16,
                "sha": "77ae8f533d173abb2ff6dd46eec49f4149c0f469",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
                "patch": "@@ -30,8 +30,8 @@\n import org.apache.beam.sdk.transforms.CombineWithContext.KeyedCombineFnWithContext;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -84,29 +84,29 @@\n \n       @Override\n       public <InputT, AccumT, OutputT>\n-          AccumulatorCombiningState<InputT, AccumT, OutputT> bindCombiningValue(\n+      CombiningState<InputT, AccumT, OutputT> bindCombining(\n               String id,\n-              StateSpec<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+              StateSpec<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n               Coder<AccumT> accumCoder,\n               CombineFn<InputT, AccumT, OutputT> combineFn) {\n         return binder.bindCombiningValue(tagForSpec(id, spec), accumCoder, combineFn);\n       }\n \n       @Override\n       public <InputT, AccumT, OutputT>\n-          AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValue(\n+      CombiningState<InputT, AccumT, OutputT> bindKeyedCombining(\n               String id,\n-              StateSpec<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+              StateSpec<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n               Coder<AccumT> accumCoder,\n               KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n         return binder.bindKeyedCombiningValue(tagForSpec(id, spec), accumCoder, combineFn);\n       }\n \n       @Override\n       public <InputT, AccumT, OutputT>\n-          AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValueWithContext(\n+      CombiningState<InputT, AccumT, OutputT> bindKeyedCombiningWithContext(\n               String id,\n-              StateSpec<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+              StateSpec<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n               Coder<AccumT> accumCoder,\n               KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n         return binder.bindKeyedCombiningValueWithContext(\n@@ -158,37 +158,37 @@ private StateTags() { }\n    * multiple {@code InputT}s into a single {@code OutputT}.\n    */\n   public static <InputT, AccumT, OutputT>\n-    StateTag<Object, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+    StateTag<Object, CombiningState<InputT, AccumT, OutputT>>\n     combiningValue(\n       String id, Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.combiningValue(accumCoder, combineFn));\n+        new StructuredId(id), StateSpecs.combining(accumCoder, combineFn));\n   }\n \n   /**\n    * Create a state tag for values that use a {@link KeyedCombineFn} to automatically merge\n    * multiple {@code InputT}s into a single {@code OutputT}.\n    */\n   public static <K, InputT, AccumT,\n-      OutputT> StateTag<K, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+      OutputT> StateTag<K, CombiningState<InputT, AccumT, OutputT>>\n       keyedCombiningValue(String id, Coder<AccumT> accumCoder,\n           KeyedCombineFn<K, InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.keyedCombiningValue(accumCoder, combineFn));\n+        new StructuredId(id), StateSpecs.keyedCombining(accumCoder, combineFn));\n   }\n \n   /**\n    * Create a state tag for values that use a {@link KeyedCombineFnWithContext} to automatically\n    * merge multiple {@code InputT}s into a single {@code OutputT}.\n    */\n   public static <K, InputT, AccumT, OutputT>\n-      StateTag<K, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+      StateTag<K, CombiningState<InputT, AccumT, OutputT>>\n       keyedCombiningValueWithContext(\n           String id,\n           Coder<AccumT> accumCoder,\n           KeyedCombineFnWithContext<K, InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.keyedCombiningValueWithContext(accumCoder, combineFn));\n+        new StructuredId(id), StateSpecs.keyedCombiningWithContext(accumCoder, combineFn));\n   }\n \n   /**\n@@ -199,11 +199,11 @@ private StateTags() { }\n    * should only be used to initialize static values.\n    */\n   public static <InputT, AccumT, OutputT>\n-      StateTag<Object, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+      StateTag<Object, CombiningState<InputT, AccumT, OutputT>>\n       combiningValueFromInputInternal(\n           String id, Coder<InputT> inputCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.combiningValueFromInputInternal(inputCoder, combineFn));\n+        new StructuredId(id), StateSpecs.combiningFromInputInternal(inputCoder, combineFn));\n   }\n \n   /**\n@@ -255,7 +255,7 @@ private StateTags() { }\n \n   public static <K, InputT, AccumT, OutputT> StateTag<Object, BagState<AccumT>>\n       convertToBagTagInternal(\n-          StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> combiningTag) {\n+          StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> combiningTag) {\n     return new SimpleStateTag<>(\n         new StructuredId(combiningTag.getId()),\n         StateSpecs.convertToBagSpecInternal(combiningTag.getSpec()));",
                "changes": 32
            },
            {
                "status": "modified",
                "additions": 96,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
                "deletions": 0,
                "sha": "4f158224287fa9aa6e8bf400899a821d09d636c4",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
                "patch": "@@ -17,15 +17,21 @@\n  */\n package org.apache.beam.runners.core;\n \n+import java.util.Map;\n+import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.NonMergingWindowFn;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.state.State;\n+import org.apache.beam.sdk.util.state.StateSpec;\n import org.joda.time.Instant;\n \n /**\n@@ -168,4 +174,94 @@ boolean isForWindow(\n \n     void clearForWindow(W window);\n   }\n+\n+  /**\n+   * A {@link StatefulDoFnRunner.CleanupTimer} implemented via {@link TimerInternals}.\n+   */\n+  public static class TimeInternalsCleanupTimer implements StatefulDoFnRunner.CleanupTimer {\n+\n+    public static final String GC_TIMER_ID = \"__StatefulParDoGcTimerId\";\n+\n+    /**\n+     * The amount of milliseconds by which to delay cleanup. We use this to ensure that state is\n+     * still available when a user timer for {@code window.maxTimestamp()} fires.\n+     */\n+    public static final long GC_DELAY_MS = 1;\n+\n+    private final TimerInternals timerInternals;\n+    private final WindowingStrategy<?, ?> windowingStrategy;\n+    private final Coder<BoundedWindow> windowCoder;\n+\n+    public TimeInternalsCleanupTimer(\n+        TimerInternals timerInternals,\n+        WindowingStrategy<?, ?> windowingStrategy) {\n+      this.windowingStrategy = windowingStrategy;\n+      WindowFn<?, ?> windowFn = windowingStrategy.getWindowFn();\n+      windowCoder = (Coder<BoundedWindow>) windowFn.windowCoder();\n+      this.timerInternals = timerInternals;\n+    }\n+\n+    @Override\n+    public Instant currentInputWatermarkTime() {\n+      return timerInternals.currentInputWatermarkTime();\n+    }\n+\n+    @Override\n+    public void setForWindow(BoundedWindow window) {\n+      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n+      // make sure this fires after any window.maxTimestamp() timers\n+      gcTime = gcTime.plus(GC_DELAY_MS);\n+      timerInternals.setTimer(StateNamespaces.window(windowCoder, window),\n+          GC_TIMER_ID, gcTime, TimeDomain.EVENT_TIME);\n+    }\n+\n+    @Override\n+    public boolean isForWindow(\n+        String timerId,\n+        BoundedWindow window,\n+        Instant timestamp,\n+        TimeDomain timeDomain) {\n+      boolean isEventTimer = timeDomain.equals(TimeDomain.EVENT_TIME);\n+      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n+      gcTime = gcTime.plus(GC_DELAY_MS);\n+      return isEventTimer && GC_TIMER_ID.equals(timerId) && gcTime.equals(timestamp);\n+    }\n+  }\n+\n+  /**\n+   * A {@link StatefulDoFnRunner.StateCleaner} implemented via {@link StateInternals}.\n+   */\n+  public static class StateInternalsStateCleaner<W extends BoundedWindow>\n+      implements StatefulDoFnRunner.StateCleaner<W> {\n+\n+    private final DoFn<?, ?> fn;\n+    private final DoFnSignature signature;\n+    private final StateInternals<?> stateInternals;\n+    private final Coder<W> windowCoder;\n+\n+    public StateInternalsStateCleaner(\n+        DoFn<?, ?> fn,\n+        StateInternals<?> stateInternals,\n+        Coder<W> windowCoder) {\n+      this.fn = fn;\n+      this.signature = DoFnSignatures.getSignature(fn.getClass());\n+      this.stateInternals = stateInternals;\n+      this.windowCoder = windowCoder;\n+    }\n+\n+    @Override\n+    public void clearForWindow(W window) {\n+      for (Map.Entry<String, DoFnSignature.StateDeclaration> entry :\n+          signature.stateDeclarations().entrySet()) {\n+        try {\n+          StateSpec<?, ?> spec = (StateSpec<?, ?>) entry.getValue().field().get(fn);\n+          State state = stateInternals.state(StateNamespaces.window(windowCoder, window),\n+              StateTags.tagForSpec(entry.getKey(), (StateSpec) spec));\n+          state.clear();\n+        } catch (IllegalAccessException e) {\n+          throw new RuntimeException(e);\n+        }\n+      }\n+    }\n+  }\n }",
                "changes": 96
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
                "deletions": 4,
                "sha": "f618d889d7b8d2c836ead5079214d6fc7d5856ad",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
                "patch": "@@ -25,9 +25,9 @@\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.AppliedCombineFn;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.ReadableState;\n \n /**\n@@ -71,7 +71,7 @@ public void onMerge(OnMergeContext c) throws Exception {\n       AccumT, OutputT, W>\n       combining(\n           final Coder<K> keyCoder, final AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn) {\n-    final StateTag<K, AccumulatorCombiningState<InputT, AccumT, OutputT>> bufferTag;\n+    final StateTag<K, CombiningState<InputT, AccumT, OutputT>> bufferTag;\n     if (combineFn.getFn() instanceof KeyedCombineFnWithContext) {\n       bufferTag = StateTags.makeSystemTagInternal(\n           StateTags.<K, InputT, AccumT, OutputT>keyedCombiningValueWithContext(\n@@ -97,10 +97,10 @@ public void onMerge(OnMergeContext c) throws Exception {\n     };\n   }\n \n-  private StateTag<? super K, ? extends CombiningState<InputT, OutputT>> bufferTag;\n+  private StateTag<? super K, ? extends GroupingState<InputT, OutputT>> bufferTag;\n \n   public SystemReduceFn(\n-      StateTag<? super K, ? extends CombiningState<InputT, OutputT>> bufferTag) {\n+      StateTag<? super K, ? extends GroupingState<InputT, OutputT>> bufferTag) {\n     this.bufferTag = bufferTag;\n   }\n ",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
                "deletions": 4,
                "sha": "50050653c486797ac6be2dcd49e8e525bfc8c75e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
                "patch": "@@ -49,11 +49,11 @@ void outputWindowedValue(OutputT output, Instant timestamp,\n       Collection<? extends BoundedWindow> windows, PaneInfo pane);\n \n   /**\n-   * Output the value to a side output at the specified timestamp in the listed windows.\n+   * Output the value to a tagged output at the specified timestamp in the listed windows.\n    */\n-  <SideOutputT> void sideOutputWindowedValue(\n-      TupleTag<SideOutputT> tag,\n-      SideOutputT output,\n+  <AdditionalOutputT> void outputWindowedValue(\n+      TupleTag<AdditionalOutputT> tag,\n+      AdditionalOutputT output,\n       Instant timestamp,\n       Collection<? extends BoundedWindow> windows,\n       PaneInfo pane);",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
                "deletions": 4,
                "sha": "1b36bf9c487222df2d7268d13b4ea563673de4dc",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
                "patch": "@@ -62,13 +62,13 @@ public void outputWindowedValue(\n       }\n \n       @Override\n-      public <SideOutputT> void sideOutputWindowedValue(\n-          TupleTag<SideOutputT> tag,\n-          SideOutputT output,\n+      public <AdditionalOutputT> void outputWindowedValue(\n+          TupleTag<AdditionalOutputT> tag,\n+          AdditionalOutputT output,\n           Instant timestamp,\n           Collection<? extends BoundedWindow> windows,\n           PaneInfo pane) {\n-        windowingInternals.sideOutputWindowedValue(tag, output, timestamp, windows, pane);\n+        windowingInternals.outputWindowedValue(tag, output, timestamp, windows, pane);\n       }\n     };\n   }",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
                "deletions": 5,
                "sha": "b416788fd06f800cc6d0486bc50f473eaa0aa87a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
                "patch": "@@ -30,12 +30,12 @@\n import org.apache.beam.runners.core.triggers.TriggerStateMachine.OnceTriggerStateMachine;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.coders.InstantCoder;\n-import org.apache.beam.sdk.transforms.Combine;\n+import org.apache.beam.sdk.transforms.Combine.Holder;\n import org.apache.beam.sdk.transforms.Min;\n import org.apache.beam.sdk.transforms.SerializableFunction;\n import org.apache.beam.sdk.util.TimeDomain;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.joda.time.format.PeriodFormat;\n@@ -55,8 +55,8 @@\n   protected static final List<SerializableFunction<Instant, Instant>> IDENTITY =\n       ImmutableList.<SerializableFunction<Instant, Instant>>of();\n \n-  protected static final StateTag<Object, AccumulatorCombiningState<Instant,\n-                                              Combine.Holder<Instant>, Instant>> DELAYED_UNTIL_TAG =\n+  protected static final StateTag<Object, CombiningState<Instant,\n+                                                Holder<Instant>, Instant>> DELAYED_UNTIL_TAG =\n       StateTags.makeSystemTagInternal(StateTags.combiningValueFromInputInternal(\n           \"delayed\", InstantCoder.of(), Min.<Instant>naturalOrder()));\n \n@@ -169,7 +169,7 @@ public void prefetchOnElement(StateAccessor<?> state) {\n \n   @Override\n   public void onElement(OnElementContext c) throws Exception {\n-    CombiningState<Instant, Instant> delayUntilState = c.state().access(DELAYED_UNTIL_TAG);\n+    GroupingState<Instant, Instant> delayUntilState = c.state().access(DELAYED_UNTIL_TAG);\n     Instant oldDelayUntil = delayUntilState.read();\n \n     // Since processing time can only advance, resulting in target wake-up times we would",
                "changes": 10
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
                "deletions": 2,
                "sha": "11323cc69eb27477713664ea827f131468e6939e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
                "patch": "@@ -27,15 +27,15 @@\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.coders.VarLongCoder;\n import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n \n /**\n  * {@link TriggerStateMachine}s that fire based on properties of the elements in the current pane.\n  */\n @Experimental(Experimental.Kind.TRIGGER)\n public class AfterPaneStateMachine extends OnceTriggerStateMachine {\n \n-private static final StateTag<Object, AccumulatorCombiningState<Long, long[], Long>>\n+private static final StateTag<Object, CombiningState<Long, long[], Long>>\n       ELEMENTS_IN_PANE_TAG =\n       StateTags.makeSystemTagInternal(StateTags.combiningValueFromInputInternal(\n           \"count\", VarLongCoder.of(), Sum.ofLongs()));",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
                "deletions": 8,
                "sha": "1b117d2e6923ba96e51965de725568a14c8b1a67",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
                "patch": "@@ -31,18 +31,16 @@\n  * lower-bound, sometimes heuristically established, on event times that have been fully processed\n  * by the pipeline.\n  *\n- * <p>For sources that provide non-heuristic watermarks (e.g.\n- * {@link org.apache.beam.sdk.io.PubsubIO} when using arrival times as event times), the\n- * watermark is a strict guarantee that no data with an event time earlier than\n+ * <p>For sources that provide non-heuristic watermarks (e.g. PubsubIO when using arrival times as\n+ * event times), the watermark is a strict guarantee that no data with an event time earlier than\n  * that watermark will ever be observed in the pipeline. In this case, it's safe to assume that any\n  * pane triggered by an {@code AfterWatermark} trigger with a reference point at or beyond the end\n  * of the window will be the last pane ever for that window.\n  *\n- * <p>For sources that provide heuristic watermarks (e.g.\n- * {@link org.apache.beam.sdk.io.PubsubIO} when using user-supplied event times), the\n- * watermark itself becomes an <i>estimate</i> that no data with an event time earlier than that\n- * watermark (i.e. \"late data\") will ever be observed in the pipeline. These heuristics can\n- * often be quite accurate, but the chance of seeing late data for any given window is non-zero.\n+ * <p>For sources that provide heuristic watermarks (e.g. PubsubIO when using user-supplied event\n+ * times), the watermark itself becomes an <i>estimate</i> that no data with an event time earlier\n+ * than that watermark (i.e. \"late data\") will ever be observed in the pipeline. These heuristics\n+ * can often be quite accurate, but the chance of seeing late data for any given window is non-zero.\n  * Thus, if absolute correctness over time is important to your use case, you may want to consider\n  * using a trigger that accounts for late data. The default trigger,\n  * {@code Repeatedly.forever(AfterWatermark.pastEndOfWindow())}, which fires",
                "changes": 14
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
                "deletions": 5,
                "sha": "d0a89236f9b6713dfc40e10933c6798c806513c3",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
                "patch": "@@ -677,9 +677,9 @@ public void outputWindowedValue(\n         }\n \n         @Override\n-        public <SideOutputT> void sideOutputWindowedValue(\n-            TupleTag<SideOutputT> tag,\n-            SideOutputT output,\n+        public <AdditionalOutputT> void outputWindowedValue(\n+            TupleTag<AdditionalOutputT> tag,\n+            AdditionalOutputT output,\n             Instant timestamp,\n             Collection<? extends BoundedWindow> windows,\n             PaneInfo pane) {\n@@ -729,12 +729,12 @@ public void outputWithTimestamp(KV<K, OutputT> output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n+    public <T> void output(TupleTag<T> tag, T output) {\n       throw new UnsupportedOperationException();\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n       throw new UnsupportedOperationException();\n     }\n ",
                "changes": 10
            },
            {
                "status": "modified",
                "additions": 47,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java",
                "deletions": 58,
                "sha": "34ddae6fc47a3c9160d280048b7428bf250e449f",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java",
                "patch": "@@ -17,7 +17,9 @@\n  */\n package org.apache.beam.runners.core;\n \n+import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasItems;\n import static org.hamcrest.Matchers.not;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -26,7 +28,6 @@\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n-import java.util.Collections;\n import java.util.Map;\n import java.util.Objects;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n@@ -35,9 +36,9 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.SetState;\n@@ -61,7 +62,7 @@\n \n   private static final StateTag<Object, ValueState<String>> STRING_VALUE_ADDR =\n       StateTags.value(\"stringValue\", StringUtf8Coder.of());\n-  private static final StateTag<Object, AccumulatorCombiningState<Integer, int[], Integer>>\n+  private static final StateTag<Object, CombiningState<Integer, int[], Integer>>\n       SUM_INTEGER_ADDR = StateTags.combiningValueFromInputInternal(\n           \"sumInteger\", VarIntCoder.of(), Sum.ofIntegers());\n   private static final StateTag<Object, BagState<String>> STRING_BAG_ADDR =\n@@ -112,10 +113,10 @@ public void testBag() throws Exception {\n \n     assertThat(value.read(), Matchers.emptyIterable());\n     value.add(\"hello\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"hello\"));\n+    assertThat(value.read(), containsInAnyOrder(\"hello\"));\n \n     value.add(\"world\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"hello\", \"world\"));\n+    assertThat(value.read(), containsInAnyOrder(\"hello\", \"world\"));\n \n     value.clear();\n     assertThat(value.read(), Matchers.emptyIterable());\n@@ -147,7 +148,7 @@ public void testMergeBagIntoSource() throws Exception {\n     StateMerging.mergeBags(Arrays.asList(bag1, bag2), bag1);\n \n     // Reading the merged bag gets both the contents\n-    assertThat(bag1.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(bag1.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(bag2.read(), Matchers.emptyIterable());\n   }\n \n@@ -164,7 +165,7 @@ public void testMergeBagIntoNewNamespace() throws Exception {\n     StateMerging.mergeBags(Arrays.asList(bag1, bag2, bag3), bag3);\n \n     // Reading the merged bag gets both the contents\n-    assertThat(bag3.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(bag3.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(bag1.read(), Matchers.emptyIterable());\n     assertThat(bag2.read(), Matchers.emptyIterable());\n   }\n@@ -179,41 +180,32 @@ public void testSet() throws Exception {\n \n     // empty\n     assertThat(value.read(), Matchers.emptyIterable());\n-    assertFalse(value.contains(\"A\"));\n-    assertFalse(value.containsAny(Collections.singletonList(\"A\")));\n+    assertFalse(value.contains(\"A\").read());\n \n     // add\n     value.add(\"A\");\n     value.add(\"B\");\n     value.add(\"A\");\n-    assertFalse(value.addIfAbsent(\"B\"));\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"A\", \"B\"));\n+    assertFalse(value.addIfAbsent(\"B\").read());\n+    assertThat(value.read(), containsInAnyOrder(\"A\", \"B\"));\n \n     // remove\n     value.remove(\"A\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"B\"));\n+    assertThat(value.read(), containsInAnyOrder(\"B\"));\n     value.remove(\"C\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"B\"));\n+    assertThat(value.read(), containsInAnyOrder(\"B\"));\n \n     // contains\n-    assertFalse(value.contains(\"A\"));\n-    assertTrue(value.contains(\"B\"));\n+    assertFalse(value.contains(\"A\").read());\n+    assertTrue(value.contains(\"B\").read());\n     value.add(\"C\");\n     value.add(\"D\");\n \n-    // containsAny\n-    assertTrue(value.containsAny(Arrays.asList(\"A\", \"C\")));\n-    assertFalse(value.containsAny(Arrays.asList(\"A\", \"E\")));\n-\n-    // containsAll\n-    assertTrue(value.containsAll(Arrays.asList(\"B\", \"C\")));\n-    assertFalse(value.containsAll(Arrays.asList(\"A\", \"B\")));\n-\n     // readLater\n-    assertThat(value.readLater().read(), Matchers.containsInAnyOrder(\"B\", \"C\", \"D\"));\n-    SetState<String> later = value.readLater(Arrays.asList(\"A\", \"C\", \"D\"));\n-    assertTrue(later.containsAll(Arrays.asList(\"C\", \"D\")));\n-    assertFalse(later.contains(\"A\"));\n+    assertThat(value.readLater().read(), containsInAnyOrder(\"B\", \"C\", \"D\"));\n+    SetState<String> later = value.readLater();\n+    assertThat(later.read(), hasItems(\"C\", \"D\"));\n+    assertFalse(later.contains(\"A\").read());\n \n     // clear\n     value.clear();\n@@ -248,7 +240,7 @@ public void testMergeSetIntoSource() throws Exception {\n     StateMerging.mergeSets(Arrays.asList(set1, set2), set1);\n \n     // Reading the merged set gets both the contents\n-    assertThat(set1.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(set1.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(set2.read(), Matchers.emptyIterable());\n   }\n \n@@ -266,7 +258,7 @@ public void testMergeSetIntoNewNamespace() throws Exception {\n     StateMerging.mergeSets(Arrays.asList(set1, set2, set3), set3);\n \n     // Reading the merged set gets both the contents\n-    assertThat(set3.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(set3.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(set1.read(), Matchers.emptyIterable());\n     assertThat(set2.read(), Matchers.emptyIterable());\n   }\n@@ -330,55 +322,52 @@ public void testMap() throws Exception {\n     assertThat(value, not(equalTo(underTest.state(NAMESPACE_2, STRING_MAP_ADDR))));\n \n     // put\n-    assertThat(value.iterate(), Matchers.emptyIterable());\n+    assertThat(value.entries().read(), Matchers.emptyIterable());\n     value.put(\"A\", 1);\n     value.put(\"B\", 2);\n     value.put(\"A\", 11);\n-    assertThat(value.putIfAbsent(\"B\", 22), equalTo(2));\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(MapEntry.of(\"A\", 11),\n+    assertThat(value.putIfAbsent(\"B\", 22).read(), equalTo(2));\n+    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(\"A\", 11),\n         MapEntry.of(\"B\", 2)));\n \n     // remove\n     value.remove(\"A\");\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(MapEntry.of(\"B\", 2)));\n+    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(\"B\", 2)));\n     value.remove(\"C\");\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(MapEntry.of(\"B\", 2)));\n+    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(\"B\", 2)));\n \n     // get\n-    assertNull(value.get(\"A\"));\n-    assertThat(value.get(\"B\"), equalTo(2));\n+    assertNull(value.get(\"A\").read());\n+    assertThat(value.get(\"B\").read(), equalTo(2));\n     value.put(\"C\", 3);\n     value.put(\"D\", 4);\n-    assertThat(value.get(\"C\"), equalTo(3));\n-    assertThat(value.get(Collections.singletonList(\"D\")), Matchers.containsInAnyOrder(4));\n-    assertThat(value.get(Arrays.asList(\"B\", \"C\")), Matchers.containsInAnyOrder(2, 3));\n+    assertThat(value.get(\"C\").read(), equalTo(3));\n \n     // iterate\n     value.put(\"E\", 5);\n     value.remove(\"C\");\n-    assertThat(value.keys(), Matchers.containsInAnyOrder(\"B\", \"D\", \"E\"));\n-    assertThat(value.values(), Matchers.containsInAnyOrder(2, 4, 5));\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(\n-        MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n+    assertThat(value.keys().read(), containsInAnyOrder(\"B\", \"D\", \"E\"));\n+    assertThat(value.values().read(), containsInAnyOrder(2, 4, 5));\n+    assertThat(\n+        value.entries().read(),\n+        containsInAnyOrder(MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n \n     // readLater\n-    assertThat(value.getLater(\"B\").get(\"B\"), equalTo(2));\n-    assertNull(value.getLater(\"A\").get(\"A\"));\n-    MapState<String, Integer> later = value.getLater(Arrays.asList(\"C\", \"D\"));\n-    assertNull(later.get(\"C\"));\n-    assertThat(later.get(\"D\"), equalTo(4));\n-    assertThat(value.iterateLater().iterate(), Matchers.containsInAnyOrder(\n-        MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n+    assertThat(value.get(\"B\").readLater().read(), equalTo(2));\n+    assertNull(value.get(\"A\").readLater().read());\n+    assertThat(\n+        value.entries().readLater().read(),\n+        containsInAnyOrder(MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n \n     // clear\n     value.clear();\n-    assertThat(value.iterate(), Matchers.emptyIterable());\n+    assertThat(value.entries().read(), Matchers.emptyIterable());\n     assertThat(underTest.state(NAMESPACE_1, STRING_MAP_ADDR), Matchers.sameInstance(value));\n   }\n \n   @Test\n   public void testCombiningValue() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     // State instances are cached, but depend on the namespace.\n     assertEquals(value, underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR));\n@@ -398,7 +387,7 @@ public void testCombiningValue() throws Exception {\n \n   @Test\n   public void testCombiningIsEmpty() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     assertThat(value.isEmpty().read(), Matchers.is(true));\n     ReadableState<Boolean> readFuture = value.isEmpty();\n@@ -411,9 +400,9 @@ public void testCombiningIsEmpty() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoSource() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n \n     value1.add(5);\n@@ -432,11 +421,11 @@ public void testMergeCombiningValueIntoSource() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoNewNamespace() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value3 =\n+    CombiningState<Integer, int[], Integer> value3 =\n         underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR);\n \n     value1.add(5);",
                "changes": 105
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
                "deletions": 2,
                "sha": "2e5cd6dee85707025e9f7e74a9b393ba544eaeff",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
                "patch": "@@ -57,10 +57,10 @@ public void output(OutputT output) {\n     public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n+    public <T> void output(TupleTag<T> tag, T output) {\n     }\n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output,\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output,\n         Instant timestamp) {\n     }\n     @Override",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java",
                "deletions": 2,
                "sha": "425de073589d17a6d57726b5bbcff90f69b0fec0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java",
                "patch": "@@ -160,12 +160,12 @@ public void outputWithTimestamp(String output, Instant timestamp) {\n       }\n \n       @Override\n-      public <T> void sideOutput(TupleTag<T> tag, T output) {\n+      public <T> void output(TupleTag<T> tag, T output) {\n         throw new UnsupportedOperationException();\n       }\n \n       @Override\n-      public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n         throw new UnsupportedOperationException();\n       }\n ",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
                "deletions": 22,
                "sha": "541e2383913e6a05156b977240617379df07eb15",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
                "patch": "@@ -17,15 +17,11 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.resume;\n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.stop;\n import static org.hamcrest.Matchers.greaterThan;\n import static org.hamcrest.Matchers.lessThan;\n import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.assertTrue;\n \n import java.util.Collection;\n import java.util.concurrent.Executors;\n@@ -54,28 +50,18 @@ private SomeFn(Duration sleepBeforeEachOutput) {\n     }\n \n     @ProcessElement\n-    public ProcessContinuation process(ProcessContext context, OffsetRangeTracker tracker)\n+    public void process(ProcessContext context, OffsetRangeTracker tracker)\n         throws Exception {\n-      OffsetRange range = tracker.currentRestriction();\n-      for (int i = (int) range.getFrom(); i < range.getTo(); ++i) {\n-        if (!tracker.tryClaim(i)) {\n-          return resume();\n-        }\n+      for (long i = tracker.currentRestriction().getFrom(); tracker.tryClaim(i); ++i) {\n         Thread.sleep(sleepBeforeEachOutput.getMillis());\n         context.output(\"\" + i);\n       }\n-      return stop();\n     }\n \n     @GetInitialRestriction\n     public OffsetRange getInitialRestriction(Integer element) {\n       throw new UnsupportedOperationException(\"Should not be called in this test\");\n     }\n-\n-    @NewTracker\n-    public OffsetRangeTracker newTracker(OffsetRange range) {\n-      throw new UnsupportedOperationException(\"Should not be called in this test\");\n-    }\n   }\n \n   private SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result\n@@ -94,9 +80,9 @@ public void outputWindowedValue(\n                   PaneInfo pane) {}\n \n               @Override\n-              public <SideOutputT> void sideOutputWindowedValue(\n-                  TupleTag<SideOutputT> tag,\n-                  SideOutputT output,\n+              public <AdditionalOutputT> void outputWindowedValue(\n+                  TupleTag<AdditionalOutputT> tag,\n+                  AdditionalOutputT output,\n                   Instant timestamp,\n                   Collection<? extends BoundedWindow> windows,\n                   PaneInfo pane) {}\n@@ -116,7 +102,6 @@ public void outputWindowedValue(\n   public void testInvokeProcessElementOutputBounded() throws Exception {\n     SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result res =\n         runTest(10000, Duration.ZERO);\n-    assertTrue(res.getContinuation().shouldResume());\n     OffsetRange residualRange = res.getResidualRestriction();\n     // Should process the first 100 elements.\n     assertEquals(1000, residualRange.getFrom());\n@@ -127,7 +112,6 @@ public void testInvokeProcessElementOutputBounded() throws Exception {\n   public void testInvokeProcessElementTimeBounded() throws Exception {\n     SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result res =\n         runTest(10000, Duration.millis(100));\n-    assertTrue(res.getContinuation().shouldResume());\n     OffsetRange residualRange = res.getResidualRestriction();\n     // Should process ideally around 30 elements - but due to timing flakiness, we can't enforce\n     // that precisely. Just test that it's not egregiously off.\n@@ -140,7 +124,6 @@ public void testInvokeProcessElementTimeBounded() throws Exception {\n   public void testInvokeProcessElementVoluntaryReturn() throws Exception {\n     SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result res =\n         runTest(5, Duration.millis(100));\n-    assertFalse(res.getContinuation().shouldResume());\n     assertNull(res.getResidualRestriction());\n   }\n }",
                "changes": 27
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "deletions": 6,
                "sha": "0d4d992a9386fad91f27398033538d992e7b2d93",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "patch": "@@ -65,6 +65,7 @@\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -360,8 +361,9 @@ public void testOnElementCombiningWithContext() throws Exception {\n         WindowingStrategy.of(FixedWindows.of(Duration.millis(2)))\n             .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n \n-    WindowingStrategy<?, IntervalWindow> sideInputWindowingStrategy =\n-        WindowingStrategy.of(FixedWindows.of(Duration.millis(4)));\n+    WindowMappingFn<?> sideInputWindowMappingFn =\n+        FixedWindows.of(Duration.millis(4)).getDefaultWindowMappingFn();\n+    when(mockView.getWindowMappingFn()).thenReturn((WindowMappingFn) sideInputWindowMappingFn);\n \n     TestOptions options = PipelineOptionsFactory.as(TestOptions.class);\n     options.setValue(expectedValue);\n@@ -384,10 +386,6 @@ public Integer answer(InvocationOnMock invocation) throws Throwable {\n               }\n             });\n \n-    @SuppressWarnings({\"rawtypes\", \"unchecked\", \"unused\"})\n-    Object suppressWarningsVar = when(mockView.getWindowingStrategyInternal())\n-        .thenReturn((WindowingStrategy) sideInputWindowingStrategy);\n-\n     SumAndVerifyContextFn combineFn = new SumAndVerifyContextFn(mockView, expectedValue);\n     ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(\n         mainInputWindowingStrategy, mockTriggerStateMachine, combineFn.<String>asKeyedFn(),",
                "changes": 10
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "deletions": 5,
                "sha": "914550e2b570b8f1fc736430f5a9d4a458c6d6db",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "patch": "@@ -38,6 +38,7 @@\n import java.util.Set;\n import javax.annotation.Nullable;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachineRunner;\n@@ -60,7 +61,6 @@\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.AppliedCombineFn;\n@@ -574,13 +574,13 @@ public void outputWindowedValue(\n     }\n \n     @Override\n-    public <SideOutputT> void sideOutputWindowedValue(\n-        TupleTag<SideOutputT> tag,\n-        SideOutputT output,\n+    public <AdditionalOutputT> void outputWindowedValue(\n+        TupleTag<AdditionalOutputT> tag,\n+        AdditionalOutputT output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs\");\n+      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use tagged outputs\");\n     }\n   }\n ",
                "changes": 10
            },
            {
                "status": "modified",
                "additions": 12,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java",
                "deletions": 10,
                "sha": "335aedecd190d8cdce9331940e17b75bffa2a9f2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java",
                "patch": "@@ -51,20 +51,22 @@\n   private WindowingStrategy<Object, IntervalWindow> windowingStrategy1 =\n       WindowingStrategy.of(FixedWindows.of(new Duration(WINDOW_MSECS_1)));\n \n-  private PCollectionView<Iterable<String>> view1 = PCollectionViewTesting.testingView(\n-      new TupleTag<Iterable<WindowedValue<String>>>() {},\n-      new PCollectionViewTesting.IdentityViewFn<String>(),\n-      StringUtf8Coder.of(),\n-      windowingStrategy1);\n+  private PCollectionView<Iterable<String>> view1 =\n+      PCollectionViewTesting.testingView(\n+          new TupleTag<Iterable<WindowedValue<String>>>() {},\n+          new PCollectionViewTesting.IdentityViewFn<String>(),\n+          StringUtf8Coder.of(),\n+          windowingStrategy1);\n \n   private WindowingStrategy<Object, IntervalWindow> windowingStrategy2 =\n       WindowingStrategy.of(FixedWindows.of(new Duration(WINDOW_MSECS_2)));\n \n-  private PCollectionView<Iterable<String>> view2 = PCollectionViewTesting.testingView(\n-      new TupleTag<Iterable<WindowedValue<String>>>() {},\n-      new PCollectionViewTesting.IdentityViewFn<String>(),\n-      StringUtf8Coder.of(),\n-      windowingStrategy2);\n+  private PCollectionView<Iterable<String>> view2 =\n+      PCollectionViewTesting.testingView(\n+          new TupleTag<Iterable<WindowedValue<String>>>() {},\n+          new PCollectionViewTesting.IdentityViewFn<String>(),\n+          StringUtf8Coder.of(),\n+          windowingStrategy2);\n \n   @Test\n   public void testIsEmpty() {",
                "changes": 22
            },
            {
                "status": "modified",
                "additions": 145,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "deletions": 0,
                "sha": "4ae533257b556a8759ff38516484caee866bda5b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "patch": "@@ -19,14 +19,18 @@\n \n import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.isA;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.ListMultimap;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n import org.apache.beam.runners.core.BaseExecutionContext.StepContext;\n+import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.DoFn;\n@@ -45,6 +49,7 @@\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n+import org.joda.time.format.PeriodFormat;\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -234,6 +239,115 @@ public void testOnTimerCalled() {\n                 TimeDomain.EVENT_TIME)));\n   }\n \n+  /**\n+   * Demonstrates that attempting to output an element before the timestamp of the current element\n+   * with zero {@link DoFn#getAllowedTimestampSkew() allowed timestamp skew} throws.\n+   */\n+  @Test\n+  public void testBackwardsInTimeNoSkew() {\n+    SkewingDoFn fn = new SkewingDoFn(Duration.ZERO);\n+    DoFnRunner<Duration, Duration> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            new ListOutputManager(),\n+            new TupleTag<Duration>(),\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    runner.startBundle();\n+    // An element output at the current timestamp is fine.\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.ZERO, new Instant(0)));\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(isA(IllegalArgumentException.class));\n+    thrown.expectMessage(\"must be no earlier\");\n+    thrown.expectMessage(\n+        String.format(\"timestamp of the current input (%s)\", new Instant(0).toString()));\n+    thrown.expectMessage(\n+        String.format(\n+            \"the allowed skew (%s)\", PeriodFormat.getDefault().print(Duration.ZERO.toPeriod())));\n+    // An element output before (current time - skew) is forbidden\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));\n+  }\n+\n+  /**\n+   * Demonstrates that attempting to output an element before the timestamp of the current element\n+   * plus the value of {@link DoFn#getAllowedTimestampSkew()} throws, but between that value and\n+   * the current timestamp succeeds.\n+   */\n+  @Test\n+  public void testSkew() {\n+    SkewingDoFn fn = new SkewingDoFn(Duration.standardMinutes(10L));\n+    DoFnRunner<Duration, Duration> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            new ListOutputManager(),\n+            new TupleTag<Duration>(),\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    runner.startBundle();\n+    // Outputting between \"now\" and \"now - allowed skew\" succeeds.\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.standardMinutes(5L), new Instant(0)));\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(isA(IllegalArgumentException.class));\n+    thrown.expectMessage(\"must be no earlier\");\n+    thrown.expectMessage(\n+        String.format(\"timestamp of the current input (%s)\", new Instant(0).toString()));\n+    thrown.expectMessage(\n+        String.format(\n+            \"the allowed skew (%s)\",\n+            PeriodFormat.getDefault().print(Duration.standardMinutes(10L).toPeriod())));\n+    // Outputting before \"now - allowed skew\" fails.\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.standardHours(1L), new Instant(0)));\n+  }\n+\n+  /**\n+   * Demonstrates that attempting to output an element with a timestamp before the current one\n+   * always succeeds when {@link DoFn#getAllowedTimestampSkew()} is equal to\n+   * {@link Long#MAX_VALUE} milliseconds.\n+   */\n+  @Test\n+  public void testInfiniteSkew() {\n+    SkewingDoFn fn = new SkewingDoFn(Duration.millis(Long.MAX_VALUE));\n+    DoFnRunner<Duration, Duration> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            new ListOutputManager(),\n+            new TupleTag<Duration>(),\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    runner.startBundle();\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(\n+            Duration.millis(1L), BoundedWindow.TIMESTAMP_MIN_VALUE.plus(Duration.millis(1))));\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(\n+            // This is the maximum amount a timestamp in beam can move (from the maximum timestamp\n+            // to the minimum timestamp).\n+            Duration.millis(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis())\n+                .minus(Duration.millis(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis())),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE));\n+  }\n+\n   static class ThrowingDoFn extends DoFn<String, String> {\n     final Exception exceptionToThrow = new UnsupportedOperationException(\"Expected exception\");\n \n@@ -296,4 +410,35 @@ public void onTimer(OnTimerContext context) {\n               context.timeDomain()));\n     }\n   }\n+\n+\n+  /**\n+   * A {@link DoFn} that outputs elements with timestamp equal to the input timestamp minus the\n+   * input element.\n+   */\n+  private static class SkewingDoFn extends DoFn<Duration, Duration> {\n+    private final Duration allowedSkew;\n+\n+    private SkewingDoFn(Duration allowedSkew) {\n+      this.allowedSkew = allowedSkew;\n+    }\n+\n+    @ProcessElement\n+    public void processElement(ProcessContext context) {\n+      context.outputWithTimestamp(context.element(), context.timestamp().minus(context.element()));\n+    }\n+\n+    @Override\n+    public Duration getAllowedTimestampSkew() {\n+      return allowedSkew;\n+    }\n+  }\n+\n+  private static class ListOutputManager implements OutputManager {\n+    private ListMultimap<TupleTag<?>, WindowedValue<?>> outputs = ArrayListMultimap.create();\n+    @Override\n+    public <T> void output(TupleTag<T> tag, WindowedValue<T> output) {\n+      outputs.put(tag, output);\n+    }\n+  }\n }",
                "changes": 145
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java",
                "deletions": 2,
                "sha": "8ded2dcdfb3997cbe08e4522227cba993ab28bb2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java",
                "patch": "@@ -64,10 +64,10 @@ public void testSystemDoFnInternalExceptionsNotWrapped() {\n \n   private DoFnRunner<String, String> createRunner(OldDoFn<String, String> fn) {\n     // Pass in only necessary parameters for the test\n-    List<TupleTag<?>> sideOutputTags = Arrays.asList();\n+    List<TupleTag<?>> additionalOutputTags = Arrays.asList();\n     StepContext context = mock(StepContext.class);\n     return new SimpleOldDoFnRunner<>(\n-          null, fn, null, null, null, sideOutputTags, context, null, null);\n+        null, fn, null, null, null, additionalOutputTags, context, null, null);\n   }\n \n   static class ThrowingDoFn extends OldDoFn<String, String> {",
                "changes": 4
            },
            {
                "status": "renamed",
                "additions": 10,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java",
                "previous_filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "deletions": 10,
                "sha": "ba3f9263f8848199650f7466776c2938df9c6e07",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java",
                "patch": "@@ -55,10 +55,10 @@\n import org.mockito.MockitoAnnotations;\n \n /**\n- * Tests for {@link PushbackSideInputDoFnRunner}.\n+ * Tests for {@link SimplePushbackSideInputDoFnRunner}.\n  */\n @RunWith(JUnit4.class)\n-public class PushbackSideInputDoFnRunnerTest {\n+public class SimplePushbackSideInputDoFnRunnerTest {\n   @Mock private ReadyCheckingSideInputReader reader;\n   private TestDoFnRunner<Integer, Integer> underlying;\n   private PCollectionView<Integer> singletonView;\n@@ -78,10 +78,10 @@ public void setup() {\n     underlying = new TestDoFnRunner<>();\n   }\n \n-  private PushbackSideInputDoFnRunner<Integer, Integer> createRunner(\n+  private SimplePushbackSideInputDoFnRunner<Integer, Integer> createRunner(\n       ImmutableList<PCollectionView<?>> views) {\n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n-        PushbackSideInputDoFnRunner.create(underlying, views, reader);\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n+        SimplePushbackSideInputDoFnRunner.create(underlying, views, reader);\n     runner.startBundle();\n     return runner;\n   }\n@@ -102,7 +102,7 @@ public void processElementSideInputNotReady() {\n     when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))\n         .thenReturn(false);\n \n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of(singletonView));\n \n     WindowedValue<Integer> oneWindow =\n@@ -122,7 +122,7 @@ public void processElementSideInputNotReadyMultipleWindows() {\n     when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))\n         .thenReturn(false);\n \n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of(singletonView));\n \n     WindowedValue<Integer> multiWindow =\n@@ -150,7 +150,7 @@ public void processElementSideInputNotReadySomeWindows() {\n                 org.mockito.AdditionalMatchers.not(Mockito.eq(GlobalWindow.INSTANCE))))\n         .thenReturn(true);\n \n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of(singletonView));\n \n     IntervalWindow littleWindow = new IntervalWindow(new Instant(-500L), new Instant(0L));\n@@ -181,7 +181,7 @@ public void processElementSideInputReadyAllWindows() {\n         .thenReturn(true);\n \n     ImmutableList<PCollectionView<?>> views = ImmutableList.<PCollectionView<?>>of(singletonView);\n-    PushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(views);\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(views);\n \n     WindowedValue<Integer> multiWindow =\n         WindowedValue.of(\n@@ -202,7 +202,7 @@ public void processElementSideInputReadyAllWindows() {\n \n   @Test\n   public void processElementNoSideInputs() {\n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of());\n \n     WindowedValue<Integer> multiWindow =",
                "changes": 20
            },
            {
                "status": "modified",
                "additions": 117,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "deletions": 215,
                "sha": "1a444534dc29d169afcf874dcd51696089bff199",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "patch": "@@ -17,11 +17,9 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.resume;\n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.stop;\n-import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n import static org.hamcrest.Matchers.hasItem;\n+import static org.hamcrest.Matchers.hasItems;\n import static org.hamcrest.Matchers.not;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -32,32 +30,41 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.List;\n import java.util.NoSuchElementException;\n import java.util.concurrent.Executors;\n import javax.annotation.Nullable;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.BigEndianIntegerCoder;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.InstantCoder;\n import org.apache.beam.sdk.coders.SerializableCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.testing.ValueInSingleWindow;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.BoundedPerElement;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n import org.apache.beam.sdk.transforms.DoFnTester;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.splittabledofn.HasDefaultTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRange;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.apache.beam.sdk.values.TupleTagList;\n+import org.apache.beam.sdk.values.ValueInSingleWindow;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.junit.Rule;\n@@ -72,10 +79,20 @@\n   private static final Duration MAX_BUNDLE_DURATION = Duration.standardSeconds(5);\n \n   // ----------------- Tests for whether the transform sets boundedness correctly --------------\n-  private static class SomeRestriction implements Serializable {}\n+  private static class SomeRestriction\n+      implements Serializable, HasDefaultTracker<SomeRestriction, SomeRestrictionTracker> {\n+    @Override\n+    public SomeRestrictionTracker newTracker() {\n+      return new SomeRestrictionTracker(this);\n+    }\n+  }\n \n   private static class SomeRestrictionTracker implements RestrictionTracker<SomeRestriction> {\n-    private final SomeRestriction someRestriction = new SomeRestriction();\n+    private final SomeRestriction someRestriction;\n+\n+    public SomeRestrictionTracker(SomeRestriction someRestriction) {\n+      this.someRestriction = someRestriction;\n+    }\n \n     @Override\n     public SomeRestriction currentRestriction() {\n@@ -86,8 +103,12 @@ public SomeRestriction currentRestriction() {\n     public SomeRestriction checkpoint() {\n       return someRestriction;\n     }\n+\n+    @Override\n+    public void checkDone() {}\n   }\n \n+  @BoundedPerElement\n   private static class BoundedFakeFn extends DoFn<Integer, String> {\n     @ProcessElement\n     public void processElement(ProcessContext context, SomeRestrictionTracker tracker) {}\n@@ -96,29 +117,17 @@ public void processElement(ProcessContext context, SomeRestrictionTracker tracke\n     public SomeRestriction getInitialRestriction(Integer element) {\n       return null;\n     }\n-\n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return null;\n-    }\n   }\n \n+  @UnboundedPerElement\n   private static class UnboundedFakeFn extends DoFn<Integer, String> {\n     @ProcessElement\n-    public ProcessContinuation processElement(\n-        ProcessContext context, SomeRestrictionTracker tracker) {\n-      return stop();\n-    }\n+    public void processElement(ProcessContext context, SomeRestrictionTracker tracker) {}\n \n     @GetInitialRestriction\n     public SomeRestriction getInitialRestriction(Integer element) {\n       return null;\n     }\n-\n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return null;\n-    }\n   }\n \n   private static PCollection<Integer> makeUnboundedCollection(Pipeline pipeline) {\n@@ -135,7 +144,7 @@ public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n \n   private static final TupleTag<String> MAIN_OUTPUT_TAG = new TupleTag<String>() {};\n \n-  private ParDo.BoundMulti<Integer, String> makeParDo(DoFn<Integer, String> fn) {\n+  private ParDo.MultiOutput<Integer, String> makeParDo(DoFn<Integer, String> fn) {\n     return ParDo.of(fn).withOutputTags(MAIN_OUTPUT_TAG, TupleTagList.empty());\n   }\n \n@@ -186,11 +195,6 @@ public void testBoundednessForUnboundedFn() {\n \n   // ------------------------------- Tests for ProcessFn ---------------------------------\n \n-  enum WindowExplosion {\n-    EXPLODE_WINDOWS,\n-    DO_NOT_EXPLODE_WINDOWS\n-  }\n-\n   /**\n    * A helper for testing {@link SplittableParDo.ProcessFn} on 1 element (but possibly over multiple\n    * {@link DoFn.ProcessElement} calls).\n@@ -204,7 +208,7 @@ public void testBoundednessForUnboundedFn() {\n     private Instant currentProcessingTime;\n \n     private InMemoryTimerInternals timerInternals;\n-    private InMemoryStateInternals<String> stateInternals;\n+    private TestInMemoryStateInternals<String> stateInternals;\n \n     ProcessFnTester(\n         Instant currentProcessingTime,\n@@ -214,12 +218,16 @@ public void testBoundednessForUnboundedFn() {\n         int maxOutputsPerBundle,\n         Duration maxBundleDuration)\n         throws Exception {\n+      // The exact windowing strategy doesn't matter in this test, but it should be able to\n+      // encode IntervalWindow's because that's what all tests here use.\n+      WindowingStrategy<InputT, BoundedWindow> windowingStrategy =\n+          (WindowingStrategy) WindowingStrategy.of(FixedWindows.of(Duration.standardSeconds(1)));\n       final SplittableParDo.ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn =\n           new SplittableParDo.ProcessFn<>(\n-              fn, inputCoder, restrictionCoder, IntervalWindow.getCoder());\n+              fn, inputCoder, restrictionCoder, windowingStrategy);\n       this.tester = DoFnTester.of(processFn);\n       this.timerInternals = new InMemoryTimerInternals();\n-      this.stateInternals = InMemoryStateInternals.forKey(\"dummy\");\n+      this.stateInternals = new TestInMemoryStateInternals<>(\"dummy\");\n       processFn.setStateInternalsFactory(\n           new StateInternalsFactory<String>() {\n             @Override\n@@ -281,24 +289,13 @@ void startElement(InputT element, RestrictionT restriction) throws Exception {\n               ElementAndRestriction.of(element, restriction),\n               currentProcessingTime,\n               GlobalWindow.INSTANCE,\n-              PaneInfo.ON_TIME_AND_ONLY_FIRING),\n-          WindowExplosion.DO_NOT_EXPLODE_WINDOWS);\n+              PaneInfo.ON_TIME_AND_ONLY_FIRING));\n     }\n \n-    void startElement(\n-        WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue,\n-        WindowExplosion explosion)\n+    void startElement(WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue)\n         throws Exception {\n-      switch (explosion) {\n-        case EXPLODE_WINDOWS:\n-          tester.processElement(\n-              KeyedWorkItems.elementsWorkItem(\"key\", windowedValue.explodeWindows()));\n-          break;\n-        case DO_NOT_EXPLODE_WINDOWS:\n-          tester.processElement(\n-              KeyedWorkItems.elementsWorkItem(\"key\", Arrays.asList(windowedValue)));\n-          break;\n-      }\n+      tester.processElement(\n+          KeyedWorkItems.elementsWorkItem(\"key\", Collections.singletonList(windowedValue)));\n     }\n \n     /**\n@@ -331,6 +328,9 @@ boolean advanceProcessingTimeBy(Duration duration) throws Exception {\n       return tester.takeOutputElements();\n     }\n \n+    public Instant getWatermarkHold() {\n+      return stateInternals.earliestWatermarkHold();\n+    }\n   }\n \n   private static class OutputWindowedValueToDoFnTester<OutputT>\n@@ -347,13 +347,13 @@ public void outputWindowedValue(\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      sideOutputWindowedValue(tester.getMainOutputTag(), output, timestamp, windows, pane);\n+      outputWindowedValue(tester.getMainOutputTag(), output, timestamp, windows, pane);\n     }\n \n     @Override\n-    public <SideOutputT> void sideOutputWindowedValue(\n-        TupleTag<SideOutputT> tag,\n-        SideOutputT output,\n+    public <AdditionalOutputT> void outputWindowedValue(\n+        TupleTag<AdditionalOutputT> tag,\n+        AdditionalOutputT output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n@@ -376,218 +376,125 @@ public void process(ProcessContext c, SomeRestrictionTracker tracker) {\n     public SomeRestriction getInitialRestriction(Integer elem) {\n       return new SomeRestriction();\n     }\n-\n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return new SomeRestrictionTracker();\n-    }\n   }\n \n   @Test\n-  public void testTrivialProcessFnPropagatesOutputsWindowsAndTimestamp() throws Exception {\n-    // Tests that ProcessFn correctly propagates windows and timestamp of the element\n+  public void testTrivialProcessFnPropagatesOutputWindowAndTimestamp() throws Exception {\n+    // Tests that ProcessFn correctly propagates the window and timestamp of the element\n     // inside the KeyedWorkItem.\n     // The underlying DoFn is actually monolithic, so this doesn't test splitting.\n     DoFn<Integer, String> fn = new ToStringFn();\n \n     Instant base = Instant.now();\n \n-    IntervalWindow w1 =\n+    IntervalWindow w =\n         new IntervalWindow(\n             base.minus(Duration.standardMinutes(1)), base.plus(Duration.standardMinutes(1)));\n-    IntervalWindow w2 =\n-        new IntervalWindow(\n-            base.minus(Duration.standardMinutes(2)), base.plus(Duration.standardMinutes(2)));\n-    IntervalWindow w3 =\n-        new IntervalWindow(\n-            base.minus(Duration.standardMinutes(3)), base.plus(Duration.standardMinutes(3)));\n-\n-    for (WindowExplosion explosion : WindowExplosion.values()) {\n-      ProcessFnTester<Integer, String, SomeRestriction, SomeRestrictionTracker> tester =\n-          new ProcessFnTester<>(\n-              base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class),\n-              MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);\n-      tester.startElement(\n-          WindowedValue.of(\n-              ElementAndRestriction.of(42, new SomeRestriction()),\n-              base,\n-              Arrays.asList(w1, w2, w3),\n-              PaneInfo.ON_TIME_AND_ONLY_FIRING),\n-          explosion);\n-\n-      for (IntervalWindow w : new IntervalWindow[] {w1, w2, w3}) {\n-        assertEquals(\n-            Arrays.asList(\n-                TimestampedValue.of(\"42a\", base),\n-                TimestampedValue.of(\"42b\", base),\n-                TimestampedValue.of(\"42c\", base)),\n-            tester.peekOutputElementsInWindow(w));\n-      }\n-    }\n+\n+    ProcessFnTester<Integer, String, SomeRestriction, SomeRestrictionTracker> tester =\n+        new ProcessFnTester<>(\n+            base,\n+            fn,\n+            BigEndianIntegerCoder.of(),\n+            SerializableCoder.of(SomeRestriction.class),\n+            MAX_OUTPUTS_PER_BUNDLE,\n+            MAX_BUNDLE_DURATION);\n+    tester.startElement(\n+        WindowedValue.of(\n+            ElementAndRestriction.of(42, new SomeRestriction()),\n+            base,\n+            Collections.singletonList(w),\n+            PaneInfo.ON_TIME_AND_ONLY_FIRING));\n+\n+    assertEquals(\n+        Arrays.asList(\n+            TimestampedValue.of(\"42a\", base),\n+            TimestampedValue.of(\"42b\", base),\n+            TimestampedValue.of(\"42c\", base)),\n+        tester.peekOutputElementsInWindow(w));\n   }\n \n-  /** A simple splittable {@link DoFn} that outputs the given element every 5 seconds forever. */\n-  private static class SelfInitiatedResumeFn extends DoFn<Integer, String> {\n+  private static class WatermarkUpdateFn extends DoFn<Instant, String> {\n     @ProcessElement\n-    public ProcessContinuation process(ProcessContext c, SomeRestrictionTracker tracker) {\n-      c.output(c.element().toString());\n-      return resume().withResumeDelay(Duration.standardSeconds(5)).withWatermark(c.timestamp());\n+    public void process(ProcessContext c, OffsetRangeTracker tracker) {\n+      for (long i = tracker.currentRestriction().getFrom(); tracker.tryClaim(i); ++i) {\n+        c.updateWatermark(c.element().plus(Duration.standardSeconds(i)));\n+        c.output(String.valueOf(i));\n+      }\n     }\n \n     @GetInitialRestriction\n-    public SomeRestriction getInitialRestriction(Integer elem) {\n-      return new SomeRestriction();\n+    public OffsetRange getInitialRestriction(Instant elem) {\n+      throw new IllegalStateException(\"Expected to be supplied explicitly in this test\");\n     }\n \n     @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return new SomeRestrictionTracker();\n+    public OffsetRangeTracker newTracker(OffsetRange range) {\n+      return new OffsetRangeTracker(range);\n     }\n   }\n \n   @Test\n-  public void testResumeSetsTimer() throws Exception {\n-    DoFn<Integer, String> fn = new SelfInitiatedResumeFn();\n+  public void testUpdatesWatermark() throws Exception {\n+    DoFn<Instant, String> fn = new WatermarkUpdateFn();\n     Instant base = Instant.now();\n-    ProcessFnTester<Integer, String, SomeRestriction, SomeRestrictionTracker> tester =\n-        new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class),\n-            MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);\n \n-    tester.startElement(42, new SomeRestriction());\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-\n-    // Should resume after 5 seconds: advancing by 3 seconds should have no effect.\n-    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertTrue(tester.takeOutputElements().isEmpty());\n-\n-    // 6 seconds should be enough - should invoke the fn again.\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-\n-    // Should again resume after 5 seconds: advancing by 3 seconds should again have no effect.\n-    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertTrue(tester.takeOutputElements().isEmpty());\n-\n-    // 6 seconds should again be enough.\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-  }\n-\n-  private static class SomeCheckpoint implements Serializable {\n-    private int firstUnprocessedIndex;\n-\n-    private SomeCheckpoint(int firstUnprocessedIndex) {\n-      this.firstUnprocessedIndex = firstUnprocessedIndex;\n-    }\n-  }\n-\n-  private static class SomeCheckpointTracker implements RestrictionTracker<SomeCheckpoint> {\n-    private SomeCheckpoint current;\n-    private boolean isActive = true;\n-\n-    private SomeCheckpointTracker(SomeCheckpoint current) {\n-      this.current = current;\n-    }\n+    ProcessFnTester<Instant, String, OffsetRange, OffsetRangeTracker> tester =\n+        new ProcessFnTester<>(\n+            base,\n+            fn,\n+            InstantCoder.of(),\n+            SerializableCoder.of(OffsetRange.class),\n+            3,\n+            MAX_BUNDLE_DURATION);\n \n-    @Override\n-    public SomeCheckpoint currentRestriction() {\n-      return current;\n-    }\n+    tester.startElement(base, new OffsetRange(0, 8));\n+    assertThat(tester.takeOutputElements(), hasItems(\"0\", \"1\", \"2\"));\n+    assertEquals(base.plus(Duration.standardSeconds(2)), tester.getWatermarkHold());\n \n-    public boolean tryUpdateCheckpoint(int firstUnprocessedIndex) {\n-      if (!isActive) {\n-        return false;\n-      }\n-      current = new SomeCheckpoint(firstUnprocessedIndex);\n-      return true;\n-    }\n+    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n+    assertThat(tester.takeOutputElements(), hasItems(\"3\", \"4\", \"5\"));\n+    assertEquals(base.plus(Duration.standardSeconds(5)), tester.getWatermarkHold());\n \n-    @Override\n-    public SomeCheckpoint checkpoint() {\n-      isActive = false;\n-      return current;\n-    }\n+    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n+    assertThat(tester.takeOutputElements(), hasItems(\"6\", \"7\"));\n+    assertEquals(null, tester.getWatermarkHold());\n   }\n \n   /**\n-   * A splittable {@link DoFn} that generates the sequence [init, init + total) in batches of given\n-   * size.\n+   * A splittable {@link DoFn} that generates the sequence [init, init + total).\n    */\n   private static class CounterFn extends DoFn<Integer, String> {\n-    private final int numTotalOutputs;\n-    private final int numOutputsPerCall;\n-\n-    private CounterFn(int numTotalOutputs, int numOutputsPerCall) {\n-      this.numTotalOutputs = numTotalOutputs;\n-      this.numOutputsPerCall = numOutputsPerCall;\n-    }\n-\n     @ProcessElement\n-    public ProcessContinuation process(ProcessContext c, SomeCheckpointTracker tracker) {\n-      int start = tracker.currentRestriction().firstUnprocessedIndex;\n-      for (int i = 0; i < numOutputsPerCall; ++i) {\n-        int index = start + i;\n-        if (!tracker.tryUpdateCheckpoint(index + 1)) {\n-          return resume();\n-        }\n-        if (index >= numTotalOutputs) {\n-          return stop();\n-        }\n-        c.output(String.valueOf(c.element() + index));\n+    public void process(ProcessContext c, OffsetRangeTracker tracker) {\n+      for (long i = tracker.currentRestriction().getFrom();\n+          tracker.tryClaim(i); ++i) {\n+        c.output(String.valueOf(c.element() + i));\n       }\n-      return resume();\n     }\n \n     @GetInitialRestriction\n-    public SomeCheckpoint getInitialRestriction(Integer elem) {\n+    public OffsetRange getInitialRestriction(Integer elem) {\n       throw new UnsupportedOperationException(\"Expected to be supplied explicitly in this test\");\n     }\n-\n-    @NewTracker\n-    public SomeCheckpointTracker newTracker(SomeCheckpoint restriction) {\n-      return new SomeCheckpointTracker(restriction);\n-    }\n-  }\n-\n-  @Test\n-  public void testResumeCarriesOverState() throws Exception {\n-    DoFn<Integer, String> fn = new CounterFn(3, 1);\n-    Instant base = Instant.now();\n-    ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester =\n-        new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class),\n-            MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);\n-\n-    tester.startElement(42, new SomeCheckpoint(0));\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n-    assertThat(tester.takeOutputElements(), contains(\"43\"));\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n-    assertThat(tester.takeOutputElements(), contains(\"44\"));\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n-    // After outputting all 3 items, should not output anything more.\n-    assertEquals(0, tester.takeOutputElements().size());\n-    // Should also not ask to resume.\n-    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n   }\n \n   @Test\n   public void testCheckpointsAfterNumOutputs() throws Exception {\n     int max = 100;\n-    // Create an fn that attempts to 2x output more than checkpointing allows.\n-    DoFn<Integer, String> fn = new CounterFn(2 * max + max / 2, 2 * max);\n+    DoFn<Integer, String> fn = new CounterFn();\n     Instant base = Instant.now();\n     int baseIndex = 42;\n \n-    ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester =\n+    ProcessFnTester<Integer, String, OffsetRange, OffsetRangeTracker> tester =\n         new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class),\n+            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class),\n             max, MAX_BUNDLE_DURATION);\n \n     List<String> elements;\n \n-    tester.startElement(baseIndex, new SomeCheckpoint(0));\n+    // Create an fn that attempts to 2x output more than checkpointing allows.\n+    tester.startElement(baseIndex, new OffsetRange(0, 2 * max + max / 2));\n     elements = tester.takeOutputElements();\n     assertEquals(max, elements.size());\n     // Should output the range [0, max)\n@@ -617,18 +524,18 @@ public void testCheckpointsAfterDuration() throws Exception {\n     // But bound bundle duration - the bundle should terminate.\n     Duration maxBundleDuration = Duration.standardSeconds(1);\n     // Create an fn that attempts to 2x output more than checkpointing allows.\n-    DoFn<Integer, String> fn = new CounterFn(max, max);\n+    DoFn<Integer, String> fn = new CounterFn();\n     Instant base = Instant.now();\n     int baseIndex = 42;\n \n-    ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester =\n+    ProcessFnTester<Integer, String, OffsetRange, OffsetRangeTracker> tester =\n         new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class),\n+            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class),\n             max, maxBundleDuration);\n \n     List<String> elements;\n \n-    tester.startElement(baseIndex, new SomeCheckpoint(0));\n+    tester.startElement(baseIndex, new OffsetRange(0, Long.MAX_VALUE));\n     // Bundle should terminate, and should do at least some processing.\n     elements = tester.takeOutputElements();\n     assertFalse(elements.isEmpty());\n@@ -658,11 +565,6 @@ public SomeRestriction getInitialRestriction(Integer element) {\n       return new SomeRestriction();\n     }\n \n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return new SomeRestrictionTracker();\n-    }\n-\n     @Setup\n     public void setup() {\n       assertEquals(State.BEFORE_SETUP, state);",
                "changes": 332
            },
            {
                "status": "modified",
                "additions": 20,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java",
                "deletions": 93,
                "sha": "46cbd7db266ce0af8e260f47a0e4ad0780b9b69b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java",
                "patch": "@@ -24,26 +24,20 @@\n \n import com.google.common.base.MoreObjects;\n import java.util.Collections;\n-import java.util.Map;\n import org.apache.beam.runners.core.BaseExecutionContext.StepContext;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.VarIntCoder;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.NullSideInputReader;\n-import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.State;\n import org.apache.beam.sdk.util.state.StateSpec;\n import org.apache.beam.sdk.util.state.StateSpecs;\n import org.apache.beam.sdk.util.state.ValueState;\n@@ -124,8 +118,8 @@ public void testLateDropping() throws Exception {\n         mockStepContext,\n         aggregatorFactory,\n         WINDOWING_STRATEGY,\n-        new TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n-        new StateInternalsStateCleaner<>(\n+        new StatefulDoFnRunner.TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n+        new StatefulDoFnRunner.StateInternalsStateCleaner<>(\n             fn, stateInternals, (Coder) WINDOWING_STRATEGY.getWindowFn().windowCoder()));\n \n     runner.startBundle();\n@@ -153,8 +147,8 @@ public void testGarbageCollect() throws Exception {\n         mockStepContext,\n         aggregatorFactory,\n         WINDOWING_STRATEGY,\n-        new TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n-        new StateInternalsStateCleaner<>(\n+        new StatefulDoFnRunner.TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n+        new StatefulDoFnRunner.StateInternalsStateCleaner<>(\n             fn, stateInternals, (Coder) WINDOWING_STRATEGY.getWindowFn().windowCoder()));\n \n     Instant elementTime = new Instant(1);\n@@ -179,8 +173,16 @@ public void testGarbageCollect() throws Exception {\n         2, (int) stateInternals.state(windowNamespace(WINDOW_2), stateTag).read());\n \n     // advance watermark past end of WINDOW_1 + allowed lateness\n+    // the cleanup timer is set to window.maxTimestamp() + allowed lateness + 1\n+    // to ensure that state is still available when a user timer for window.maxTimestamp() fires\n     advanceInputWatermark(\n-        timerInternals, WINDOW_1.maxTimestamp().plus(ALLOWED_LATENESS + 1), runner);\n+        timerInternals,\n+        WINDOW_1.maxTimestamp()\n+            .plus(ALLOWED_LATENESS)\n+            .plus(StatefulDoFnRunner.TimeInternalsCleanupTimer.GC_DELAY_MS)\n+            .plus(1), // so the watermark is past the GC horizon, not on it\n+        runner);\n+\n     assertTrue(\n         stateInternals.isEmptyForTesting(\n             stateInternals.state(windowNamespace(WINDOW_1), stateTag)));\n@@ -190,7 +192,13 @@ public void testGarbageCollect() throws Exception {\n \n     // advance watermark past end of WINDOW_2 + allowed lateness\n     advanceInputWatermark(\n-        timerInternals, WINDOW_2.maxTimestamp().plus(ALLOWED_LATENESS + 1), runner);\n+        timerInternals,\n+        WINDOW_2.maxTimestamp()\n+            .plus(ALLOWED_LATENESS)\n+            .plus(StatefulDoFnRunner.TimeInternalsCleanupTimer.GC_DELAY_MS)\n+            .plus(1), // so the watermark is past the GC horizon, not on it\n+        runner);\n+\n     assertTrue(\n         stateInternals.isEmptyForTesting(\n             stateInternals.state(windowNamespace(WINDOW_2), stateTag)));\n@@ -263,85 +271,4 @@ public String getName() {\n       return Sum.ofLongs();\n     }\n   }\n-\n-  /**\n-   * A {@link StatefulDoFnRunner.CleanupTimer} implemented by TimerInternals.\n-   */\n-  public static class TimeInternalsCleanupTimer implements StatefulDoFnRunner.CleanupTimer {\n-\n-    public static final String GC_TIMER_ID = \"__StatefulParDoGcTimerId\";\n-\n-    private final TimerInternals timerInternals;\n-    private final WindowingStrategy<?, ?> windowingStrategy;\n-    private final Coder<BoundedWindow> windowCoder;\n-\n-    public TimeInternalsCleanupTimer(\n-        TimerInternals timerInternals,\n-        WindowingStrategy<?, ?> windowingStrategy) {\n-      this.windowingStrategy = windowingStrategy;\n-      WindowFn<?, ?> windowFn = windowingStrategy.getWindowFn();\n-      windowCoder = (Coder<BoundedWindow>) windowFn.windowCoder();\n-      this.timerInternals = timerInternals;\n-    }\n-\n-    @Override\n-    public Instant currentInputWatermarkTime() {\n-      return timerInternals.currentInputWatermarkTime();\n-    }\n-\n-    @Override\n-    public void setForWindow(BoundedWindow window) {\n-      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n-      timerInternals.setTimer(StateNamespaces.window(windowCoder, window),\n-          GC_TIMER_ID, gcTime, TimeDomain.EVENT_TIME);\n-    }\n-\n-    @Override\n-    public boolean isForWindow(\n-        String timerId,\n-        BoundedWindow window,\n-        Instant timestamp,\n-        TimeDomain timeDomain) {\n-      boolean isEventTimer = timeDomain.equals(TimeDomain.EVENT_TIME);\n-      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n-      return isEventTimer && GC_TIMER_ID.equals(timerId) && gcTime.equals(timestamp);\n-    }\n-  }\n-\n-  /**\n-   * A {@link StatefulDoFnRunner.StateCleaner} implemented by StateInternals.\n-   */\n-  public static class StateInternalsStateCleaner<W extends BoundedWindow>\n-      implements StatefulDoFnRunner.StateCleaner<W> {\n-\n-    private final DoFn<?, ?> fn;\n-    private final DoFnSignature signature;\n-    private final StateInternals<?> stateInternals;\n-    private final Coder<W> windowCoder;\n-\n-    public StateInternalsStateCleaner(\n-        DoFn<?, ?> fn,\n-        StateInternals<?> stateInternals,\n-        Coder<W> windowCoder) {\n-      this.fn = fn;\n-      this.signature = DoFnSignatures.getSignature(fn.getClass());\n-      this.stateInternals = stateInternals;\n-      this.windowCoder = windowCoder;\n-    }\n-\n-    @Override\n-    public void clearForWindow(W window) {\n-      for (Map.Entry<String, DoFnSignature.StateDeclaration> entry :\n-          signature.stateDeclarations().entrySet()) {\n-        try {\n-          StateSpec<?, ?> spec = (StateSpec<?, ?>) entry.getValue().field().get(fn);\n-          State state = stateInternals.state(StateNamespaces.window(windowCoder, window),\n-              StateTags.tagForSpec(entry.getKey(), (StateSpec) spec));\n-          state.clear();\n-        } catch (IllegalAccessException e) {\n-          throw new RuntimeException(e);\n-        }\n-      }\n-    }\n-  }\n }",
                "changes": 113
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/pom.xml",
                "deletions": 53,
                "sha": "fc28fd6ea667ff032de8885c67a88c44453f35d8",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/pom.xml",
                "patch": "@@ -61,7 +61,7 @@\n         </configuration>\n         <executions>\n           <execution>\n-            <id>runnable-on-service-tests</id>\n+            <id>validates-runner-tests</id>\n             <phase>integration-test</phase>\n             <goals>\n               <goal>test</goal>\n@@ -81,58 +81,7 @@\n                   ]\n                 </beamTestPipelineOptions>\n               </systemPropertyVariables>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n-\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-shade-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <id>bundle-and-repackage</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>shade</goal>\n-            </goals>\n-            <configuration>\n-              <shadeTestJar>true</shadeTestJar>\n-              <artifactSet>\n-                <includes>\n-                  <include>com.google.guava:guava</include>\n-                </includes>\n-              </artifactSet>\n-              <filters>\n-                <filter>\n-                  <artifact>*:*</artifact>\n-                  <excludes>\n-                    <exclude>META-INF/*.SF</exclude>\n-                    <exclude>META-INF/*.DSA</exclude>\n-                    <exclude>META-INF/*.RSA</exclude>\n-                  </excludes>\n-                </filter>\n-              </filters>\n-              <relocations>\n-                <!-- TODO: Once ready, change the following pattern to 'com'\n-                     only, exclude 'org.apache.beam.**', and remove\n-                     the second relocation. -->\n-                <relocation>\n-                  <pattern>com.google.common</pattern>\n-                  <excludes>\n-                    <!-- com.google.common is too generic, need to exclude guava-testlib -->\n-                    <exclude>com.google.common.**.testing.*</exclude>\n-                  </excludes>\n-                  <shadedPattern>org.apache.beam.runners.direct.repackaged.com.google.common</shadedPattern>\n-                </relocation>\n-                <relocation>\n-                  <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>org.apache.beam.runners.direct.repackaged.com.google.thirdparty</shadedPattern>\n-                </relocation>\n-              </relocations>\n-              <transformers>\n-                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n-              </transformers>\n+              <threadCount>4</threadCount>\n             </configuration>\n           </execution>\n         </executions>",
                "changes": 55
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
                "deletions": 2,
                "sha": "0c2afe8c013046434721bb2139881b436f377f69",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
                "patch": "@@ -117,7 +117,7 @@ public BoundedReadEvaluator(\n         ExecutorService executor) {\n       this.evaluationContext = evaluationContext;\n       this.outputPCollection =\n-          (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs()).getValue();\n+          (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs().values());\n       this.resultBuilder = StepTransformResult.withoutHold(transform);\n       this.minimumDynamicSplitSize = minimumDynamicSplitSize;\n       this.produceSplitExecutor = executor;\n@@ -196,7 +196,7 @@ public void processElement(WindowedValue<BoundedSourceShard<OutputT>> element)\n       long estimatedBytes = source.getEstimatedSizeBytes(options);\n       long bytesPerBundle = estimatedBytes / targetParallelism;\n       List<? extends BoundedSource<T>> bundles =\n-          source.splitIntoBundles(bytesPerBundle, options);\n+          source.split(bytesPerBundle, options);\n       ImmutableList.Builder<CommittedBundle<BoundedSourceShard<T>>> shards =\n           ImmutableList.builder();\n       for (BoundedSource<T> bundle : bundles) {",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 22,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "deletions": 24,
                "sha": "0665812e685f12ad503e7a73ca6ba92cd09c38ae",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "patch": "@@ -26,7 +26,7 @@\n import java.util.Map;\n import javax.annotation.Nullable;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryBag;\n-import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryCombiningValue;\n+import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryCombiningState;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryMap;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemorySet;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryState;\n@@ -45,8 +45,8 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.util.CombineFnUtil;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -306,19 +306,18 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n-              InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT, OutputT>>\n-                  existingState = (\n-                  InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n-                                            OutputT>>) underlying.get().get(namespace, address, c);\n+              InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>> existingState =\n+                  (InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>>)\n+                      underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryCombiningValue<>(\n+              return new InMemoryCombiningState<>(\n                   key, combineFn.asKeyedFn());\n             }\n           }\n@@ -367,27 +366,26 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindKeyedCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n-              InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT, OutputT>>\n-                  existingState = (\n-                  InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n-                                            OutputT>>) underlying.get().get(namespace, address, c);\n+              InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>> existingState =\n+                  (InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>>)\n+                      underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryCombiningValue<>(key, combineFn);\n+              return new InMemoryCombiningState<>(key, combineFn);\n             }\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n           bindKeyedCombiningValueWithContext(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n             return bindKeyedCombiningValue(\n@@ -449,9 +447,9 @@ public Instant readThroughAndGetEarliestHold(StateTable<K> readTo) {\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n             return underlying.get(namespace, address, c);\n           }\n@@ -476,18 +474,18 @@ public Instant readThroughAndGetEarliestHold(StateTable<K> readTo) {\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindKeyedCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n             return underlying.get(namespace, address, c);\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n           bindKeyedCombiningValueWithContext(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n             return bindKeyedCombiningValue(",
                "changes": 46
            },
            {
                "status": "modified",
                "additions": 12,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "deletions": 4,
                "sha": "112024341aa93bb052748cc386388c4c203b750d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "patch": "@@ -19,8 +19,9 @@\n \n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.SplittableParDo.GBKIntoKeyedWorkItems;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n-import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n \n@@ -33,8 +34,15 @@\n         PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>,\n         GBKIntoKeyedWorkItems<KeyT, InputT>> {\n   @Override\n-  public PTransform<PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>>\n-      getReplacementTransform(GBKIntoKeyedWorkItems<KeyT, InputT> transform) {\n-    return new DirectGroupByKey.DirectGroupByKeyOnly<>();\n+  public PTransformReplacement<\n+          PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>>\n+      getReplacementTransform(\n+          AppliedPTransform<\n+                  PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>,\n+                  GBKIntoKeyedWorkItems<KeyT, InputT>>\n+              transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        new DirectGroupByKey.DirectGroupByKeyOnly<KeyT, InputT>());\n   }\n }",
                "changes": 16
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "deletions": 3,
                "sha": "c3421363bbe8a3a1a4161fe612cbd01d6e9265e2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n \n /**\n  * Tracks the {@link AppliedPTransform AppliedPTransforms} that consume each {@link PValue} in the\n@@ -83,8 +82,8 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n     if (node.getInputs().isEmpty()) {\n       rootTransforms.add(appliedTransform);\n     } else {\n-      for (TaggedPValue value : node.getInputs()) {\n-        primitiveConsumers.put(value.getValue(), appliedTransform);\n+      for (PValue value : node.getInputs().values()) {\n+        primitiveConsumers.put(value, appliedTransform);\n       }\n     }\n   }",
                "changes": 5
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "deletions": 0,
                "sha": "36050023b399c6157fe7e06babe2c17bf6f22e81",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.KeyedWorkItemCoder;\n+import org.apache.beam.runners.core.construction.ForwardingPTransform;\n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.IterableCoder;",
                "changes": 1
            },
            {
                "status": "modified",
                "additions": 10,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "deletions": 4,
                "sha": "4eb036300c4e8737267faa0e2d103abbdd937ad6",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "patch": "@@ -17,10 +17,11 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n \n@@ -29,8 +30,13 @@\n     extends SingleInputOutputOverrideFactory<\n         PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupByKey<K, V>> {\n   @Override\n-  public PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> getReplacementTransform(\n-      GroupByKey<K, V> transform) {\n-    return new DirectGroupByKey<>(transform);\n+  public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+      getReplacementTransform(\n+          AppliedPTransform<\n+                  PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupByKey<K, V>>\n+              transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        new DirectGroupByKey<>(transform.getTransform()));\n   }\n }",
                "changes": 14
            },
            {
                "status": "modified",
                "additions": 58,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
                "deletions": 71,
                "sha": "fb126fb393b36fdd42a0b62ab4a629cd2c37bd34",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
                "patch": "@@ -20,12 +20,10 @@\n import static java.util.Arrays.asList;\n \n import com.google.auto.value.AutoValue;\n-import com.google.common.base.Objects;\n import com.google.common.collect.ImmutableList;\n import java.util.ArrayList;\n import java.util.Map;\n import java.util.Map.Entry;\n-import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutorService;\n@@ -35,9 +33,11 @@\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.metrics.DistributionData;\n import org.apache.beam.sdk.metrics.DistributionResult;\n+import org.apache.beam.sdk.metrics.GaugeData;\n+import org.apache.beam.sdk.metrics.GaugeResult;\n+import org.apache.beam.sdk.metrics.MetricFiltering;\n import org.apache.beam.sdk.metrics.MetricKey;\n import org.apache.beam.sdk.metrics.MetricName;\n-import org.apache.beam.sdk.metrics.MetricNameFilter;\n import org.apache.beam.sdk.metrics.MetricQueryResults;\n import org.apache.beam.sdk.metrics.MetricResult;\n import org.apache.beam.sdk.metrics.MetricResults;\n@@ -195,6 +195,28 @@ public DistributionResult extract(DistributionData data) {\n         }\n       };\n \n+  private static final MetricAggregation<GaugeData, GaugeResult> GAUGE =\n+      new MetricAggregation<GaugeData, GaugeResult>() {\n+        @Override\n+        public GaugeData zero() {\n+          return GaugeData.empty();\n+        }\n+\n+        @Override\n+        public GaugeData combine(Iterable<GaugeData> updates) {\n+          GaugeData result = GaugeData.empty();\n+          for (GaugeData update : updates) {\n+            result = result.combine(update);\n+          }\n+          return result;\n+        }\n+\n+        @Override\n+        public GaugeResult extract(GaugeData data) {\n+          return data.extractResult();\n+        }\n+      };\n+\n   /** The current values of counters in memory. */\n   private MetricsMap<MetricKey, DirectMetric<Long, Long>> counters =\n       new MetricsMap<>(new MetricsMap.Factory<MetricKey, DirectMetric<Long, Long>>() {\n@@ -212,13 +234,23 @@ public DistributionResult extract(DistributionData data) {\n           return new DirectMetric<>(DISTRIBUTION);\n         }\n       });\n+  private MetricsMap<MetricKey, DirectMetric<GaugeData, GaugeResult>> gauges =\n+      new MetricsMap<>(\n+          new MetricsMap.Factory<MetricKey, DirectMetric<GaugeData, GaugeResult>>() {\n+            @Override\n+            public DirectMetric<GaugeData, GaugeResult> createInstance(\n+                MetricKey unusedKey) {\n+              return new DirectMetric<>(GAUGE);\n+            }\n+          });\n \n   @AutoValue\n   abstract static class DirectMetricQueryResults implements MetricQueryResults {\n     public static MetricQueryResults create(\n         Iterable<MetricResult<Long>> counters,\n-        Iterable<MetricResult<DistributionResult>> distributions) {\n-      return new AutoValue_DirectMetrics_DirectMetricQueryResults(counters, distributions);\n+        Iterable<MetricResult<DistributionResult>> distributions,\n+        Iterable<MetricResult<GaugeResult>> gauges) {\n+      return new AutoValue_DirectMetrics_DirectMetricQueryResults(counters, distributions, gauges);\n     }\n   }\n \n@@ -250,15 +282,22 @@ public MetricQueryResults queryMetrics(MetricsFilter filter) {\n         : distributions.entries()) {\n       maybeExtractResult(filter, distributionResults, distribution);\n     }\n+    ImmutableList.Builder<MetricResult<GaugeResult>> gaugeResults =\n+        ImmutableList.builder();\n+    for (Entry<MetricKey, DirectMetric<GaugeData, GaugeResult>> gauge\n+        : gauges.entries()) {\n+      maybeExtractResult(filter, gaugeResults, gauge);\n+    }\n \n-    return DirectMetricQueryResults.create(counterResults.build(), distributionResults.build());\n+    return DirectMetricQueryResults.create(counterResults.build(), distributionResults.build(),\n+        gaugeResults.build());\n   }\n \n   private <ResultT> void maybeExtractResult(\n       MetricsFilter filter,\n       ImmutableList.Builder<MetricResult<ResultT>> resultsBuilder,\n       Map.Entry<MetricKey, ? extends DirectMetric<?, ResultT>> entry) {\n-    if (matches(filter, entry.getKey())) {\n+    if (MetricFiltering.matches(filter, entry.getKey())) {\n       resultsBuilder.add(DirectMetricResult.create(\n           entry.getKey().metricName(),\n           entry.getKey().stepName(),\n@@ -267,70 +306,6 @@ public MetricQueryResults queryMetrics(MetricsFilter filter) {\n     }\n   }\n \n-  // Matching logic is implemented here rather than in MetricsFilter because we would like\n-  // MetricsFilter to act as a \"dumb\" value-object, with the possibility of replacing it with\n-  // a Proto/JSON/etc. schema object.\n-  private boolean matches(MetricsFilter filter, MetricKey key) {\n-    return matchesName(key.metricName(), filter.names())\n-        && matchesScope(key.stepName(), filter.steps());\n-  }\n-\n-  /**\n-  * {@code subPathMatches(haystack, needle)} returns true if {@code needle}\n-  * represents a path within {@code haystack}. For example, \"foo/bar\" is in \"a/foo/bar/b\",\n-  * but not \"a/fool/bar/b\" or \"a/foo/bart/b\".\n-  */\n-  public boolean subPathMatches(String haystack, String needle) {\n-    int location = haystack.indexOf(needle);\n-    int end = location + needle.length();\n-    if (location == -1) {\n-      return false;  // needle not found\n-    } else if (location != 0 && haystack.charAt(location - 1) != '/') {\n-      return false; // the first entry in needle wasn't exactly matched\n-    } else if (end != haystack.length() && haystack.charAt(end) != '/') {\n-      return false; // the last entry in needle wasn't exactly matched\n-    } else {\n-      return true;\n-    }\n-  }\n-\n-  /**\n-   * {@code matchesScope(actualScope, scopes)} returns true if the scope of a metric is matched\n-   * by any of the filters in {@code scopes}. A metric scope is a path of type \"A/B/D\". A\n-   * path is matched by a filter if the filter is equal to the path (e.g. \"A/B/D\", or\n-   * if it represents a subpath within it (e.g. \"A/B\" or \"B/D\", but not \"A/D\"). */\n-  public boolean matchesScope(String actualScope, Set<String> scopes) {\n-    if (scopes.isEmpty() || scopes.contains(actualScope)) {\n-      return true;\n-    }\n-\n-    // If there is no perfect match, a stage name-level match is tried.\n-    // This is done by a substring search over the levels of the scope.\n-    // e.g. a scope \"A/B/C/D\" is matched by \"A/B\", but not by \"A/C\".\n-    for (String scope : scopes) {\n-      if (subPathMatches(actualScope, scope)) {\n-        return true;\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n-  private boolean matchesName(MetricName metricName, Set<MetricNameFilter> nameFilters) {\n-    if (nameFilters.isEmpty()) {\n-      return true;\n-    }\n-\n-    for (MetricNameFilter nameFilter : nameFilters) {\n-      if ((nameFilter.getName() == null || nameFilter.getName().equals(metricName.name()))\n-          && Objects.equal(metricName.namespace(), nameFilter.getNamespace())) {\n-        return true;\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n   /** Apply metric updates that represent physical counter deltas to the current metric values. */\n   public void updatePhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n     for (MetricUpdate<Long> counter : updates.counterUpdates()) {\n@@ -340,6 +315,10 @@ public void updatePhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n       distributions.get(distribution.getKey())\n           .updatePhysical(bundle, distribution.getUpdate());\n     }\n+    for (MetricUpdate<GaugeData> gauge : updates.gaugeUpdates()) {\n+      gauges.get(gauge.getKey())\n+          .updatePhysical(bundle, gauge.getUpdate());\n+    }\n   }\n \n   public void commitPhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n@@ -350,6 +329,10 @@ public void commitPhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n       distributions.get(distribution.getKey())\n           .commitPhysical(bundle, distribution.getUpdate());\n     }\n+    for (MetricUpdate<GaugeData> gauge : updates.gaugeUpdates()) {\n+      gauges.get(gauge.getKey())\n+          .commitPhysical(bundle, gauge.getUpdate());\n+    }\n   }\n \n   /** Apply metric updates that represent new logical values from a bundle being committed. */\n@@ -361,5 +344,9 @@ public void commitLogical(CommittedBundle<?> bundle, MetricUpdates updates) {\n       distributions.get(distribution.getKey())\n           .commitLogical(bundle, distribution.getUpdate());\n     }\n+    for (MetricUpdate<GaugeData> gauge : updates.gaugeUpdates()) {\n+      gauges.get(gauge.getKey())\n+          .commitLogical(bundle, gauge.getUpdate());\n+    }\n   }\n }",
                "changes": 129
            },
            {
                "status": "modified",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
                "deletions": 11,
                "sha": "3b66cc661aae491342728030e31fc2b0be1f5525",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
                "patch": "@@ -27,17 +27,6 @@\n  * Options that can be used to configure the {@link org.apache.beam.runners.direct.DirectRunner}.\n  */\n public interface DirectOptions extends PipelineOptions, ApplicationNameOptions {\n-  @Default.Boolean(true)\n-  @Description(\n-      \"If the pipeline should shut down producers which have reached the maximum \"\n-          + \"representable watermark. If this is set to true, a pipeline in which all PTransforms \"\n-          + \"have reached the maximum watermark will be shut down, even if there are unbounded \"\n-          + \"sources that could produce additional (late) data. By default, if the pipeline \"\n-          + \"contains any unbounded PCollections, it will run until explicitly shut down.\")\n-  boolean isShutdownUnboundedProducersWithMaxWatermark();\n-\n-  void setShutdownUnboundedProducersWithMaxWatermark(boolean shutdown);\n-\n   @Default.Boolean(true)\n   @Description(\n       \"If the pipeline should block awaiting completion of the pipeline. If set to true, \"",
                "changes": 11
            },
            {
                "status": "modified",
                "additions": 63,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "deletions": 69,
                "sha": "43147a02564d49a974072420eaff932e7000cc35",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "patch": "@@ -22,11 +22,11 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n-import java.io.IOException;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import javax.annotation.Nullable;\n@@ -35,7 +35,6 @@\n import org.apache.beam.runners.core.construction.PTransformMatchers;\n import org.apache.beam.runners.direct.DirectRunner.DirectPipelineResult;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.DirectTestStreamFactory;\n-import org.apache.beam.runners.direct.ViewEvaluatorFactory.ViewOverrideFactory;\n import org.apache.beam.sdk.AggregatorRetrievalException;\n import org.apache.beam.sdk.AggregatorValues;\n import org.apache.beam.sdk.Pipeline;\n@@ -45,21 +44,19 @@\n import org.apache.beam.sdk.metrics.MetricResults;\n import org.apache.beam.sdk.metrics.MetricsEnvironment;\n import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.runners.PTransformMatcher;\n-import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.runners.PTransformOverride;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.ParDo.BoundMulti;\n+import org.apache.beam.sdk.transforms.ParDo.MultiOutput;\n import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n@@ -170,7 +167,7 @@\n   /** The set of {@link PTransform PTransforms} that execute a UDF. Useful for some enforcements. */\n   private static final Set<Class<? extends PTransform>> CONTAINS_UDF =\n       ImmutableSet.of(\n-          Read.Bounded.class, Read.Unbounded.class, ParDo.Bound.class, ParDo.BoundMulti.class);\n+          Read.Bounded.class, Read.Unbounded.class, ParDo.SingleOutput.class, MultiOutput.class);\n \n   enum Enforcement {\n     ENCODABILITY {\n@@ -223,8 +220,8 @@ public static BundleFactory bundleFactoryFor(Set<Enforcement> enforcements, Dire\n         enabledParDoEnforcements.add(ImmutabilityEnforcementFactory.create());\n       }\n       Collection<ModelEnforcementFactory> parDoEnforcements = enabledParDoEnforcements.build();\n-      enforcements.put(ParDo.Bound.class, parDoEnforcements);\n-      enforcements.put(ParDo.BoundMulti.class, parDoEnforcements);\n+      enforcements.put(ParDo.SingleOutput.class, parDoEnforcements);\n+      enforcements.put(MultiOutput.class, parDoEnforcements);\n       return enforcements.build();\n     }\n \n@@ -261,10 +258,7 @@ void setClockSupplier(Supplier<Clock> supplier) {\n \n   @Override\n   public DirectPipelineResult run(Pipeline pipeline) {\n-    for (Map.Entry<PTransformMatcher, PTransformOverrideFactory> override :\n-        defaultTransformOverrides().entrySet()) {\n-      pipeline.replace(override.getKey(), override.getValue());\n-    }\n+    pipeline.replaceAll(defaultTransformOverrides());\n     MetricsEnvironment.setMetricsSupported(true);\n     DirectGraphVisitor graphVisitor = new DirectGraphVisitor();\n     pipeline.traverseTopologically(graphVisitor);\n@@ -322,40 +316,37 @@ public DirectPipelineResult run(Pipeline pipeline) {\n    * iteration order based on the order at which elements are added to it.\n    */\n   @SuppressWarnings(\"rawtypes\")\n-  private Map<PTransformMatcher, PTransformOverrideFactory> defaultTransformOverrides() {\n-    return ImmutableMap.<PTransformMatcher, PTransformOverrideFactory>builder()\n-        .put(\n-            PTransformMatchers.writeWithRunnerDeterminedSharding(),\n-            new WriteWithShardingFactory()) /* Uses a view internally. */\n-        .put(\n-            PTransformMatchers.classEqualTo(CreatePCollectionView.class),\n-            new ViewOverrideFactory()) /* Uses pardos and GBKs */\n-        .put(\n-            PTransformMatchers.classEqualTo(TestStream.class),\n-            new DirectTestStreamFactory(this)) /* primitive */\n-        /* Single-output ParDos are implemented in terms of Multi-output ParDos. Any override\n-        that is applied to a multi-output ParDo must first have all matching Single-output ParDos\n-        converted to match.\n-         */\n-        .put(PTransformMatchers.splittableParDoSingle(), new ParDoSingleViaMultiOverrideFactory())\n-        .put(PTransformMatchers.stateOrTimerParDoSingle(), new ParDoSingleViaMultiOverrideFactory())\n-        // SplittableParMultiDo is implemented in terms of nonsplittable single ParDos\n-        .put(PTransformMatchers.splittableParDoMulti(), new ParDoMultiOverrideFactory())\n-        // state and timer pardos are implemented in terms of nonsplittable single ParDos\n-        .put(PTransformMatchers.stateOrTimerParDoMulti(), new ParDoMultiOverrideFactory())\n-        .put(\n-            PTransformMatchers.classEqualTo(ParDo.Bound.class),\n-            new ParDoSingleViaMultiOverrideFactory()) /* returns a BoundMulti */\n-        .put(\n-            PTransformMatchers.classEqualTo(BoundMulti.class),\n-            /* returns one of two primitives; SplittableParDos are replaced above. */\n-            new ParDoMultiOverrideFactory())\n-        .put(\n-            PTransformMatchers.classEqualTo(GBKIntoKeyedWorkItems.class),\n-            new DirectGBKIntoKeyedWorkItemsOverrideFactory()) /* Returns a GBKO */\n-        .put(\n-            PTransformMatchers.classEqualTo(GroupByKey.class),\n-            new DirectGroupByKeyOverrideFactory()) /* returns two chained primitives. */\n+  private List<PTransformOverride> defaultTransformOverrides() {\n+    return ImmutableList.<PTransformOverride>builder()\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.writeWithRunnerDeterminedSharding(),\n+                new WriteWithShardingFactory())) /* Uses a view internally. */\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(CreatePCollectionView.class),\n+                new ViewOverrideFactory())) /* Uses pardos and GBKs */\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(TestStream.class),\n+                new DirectTestStreamFactory(this))) /* primitive */\n+        // SplittableParMultiDo is implemented in terms of nonsplittable simple ParDos and extra\n+        // primitives\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.splittableParDoMulti(), new ParDoMultiOverrideFactory()))\n+        // state and timer pardos are implemented in terms of simple ParDos and extra primitives\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.stateOrTimerParDoMulti(), new ParDoMultiOverrideFactory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(GBKIntoKeyedWorkItems.class),\n+                new DirectGBKIntoKeyedWorkItemsOverrideFactory())) /* Returns a GBKO */\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(GroupByKey.class),\n+                new DirectGroupByKeyOverrideFactory())) /* returns two chained primitives. */\n         .build();\n   }\n \n@@ -429,19 +420,34 @@ public MetricResults metrics() {\n      * exception. Future calls to {@link #getState()} will return\n      * {@link org.apache.beam.sdk.PipelineResult.State#FAILED}.\n      *\n-     * <p>NOTE: if the {@link Pipeline} contains an {@link IsBounded#UNBOUNDED unbounded}\n-     * {@link PCollection}, and the {@link PipelineRunner} was created with\n-     * {@link DirectOptions#isShutdownUnboundedProducersWithMaxWatermark()} set to false,\n-     * this method will never return.\n-     *\n-     * <p>See also {@link PipelineExecutor#awaitCompletion()}.\n+     * <p>See also {@link PipelineExecutor#waitUntilFinish(Duration)}.\n      */\n     @Override\n     public State waitUntilFinish() {\n-      if (!state.isTerminal()) {\n+      return waitUntilFinish(Duration.ZERO);\n+    }\n+\n+    @Override\n+    public State cancel() {\n+      this.state = executor.getPipelineState();\n+      if (!this.state.isTerminal()) {\n+        executor.stop();\n+        this.state = executor.getPipelineState();\n+      }\n+      return executor.getPipelineState();\n+    }\n+\n+    @Override\n+    public State waitUntilFinish(Duration duration) {\n+      State startState = this.state;\n+      if (!startState.isTerminal()) {\n         try {\n-          executor.awaitCompletion();\n-          state = State.DONE;\n+          state = executor.waitUntilFinish(duration);\n+        } catch (UserCodeException uce) {\n+          // Emulates the behavior of Pipeline#run(), where a stack trace caused by a\n+          // UserCodeException is truncated and replaced with the stack starting at the call to\n+          // waitToFinish\n+          throw new Pipeline.PipelineExecutionException(uce.getCause());\n         } catch (Exception e) {\n           if (e instanceof InterruptedException) {\n             Thread.currentThread().interrupt();\n@@ -452,19 +458,7 @@ public State waitUntilFinish() {\n           throw new RuntimeException(e);\n         }\n       }\n-      return state;\n-    }\n-\n-    @Override\n-    public State cancel() throws IOException {\n-      throw new UnsupportedOperationException(\"DirectPipelineResult does not support cancel.\");\n-    }\n-\n-    @Override\n-    public State waitUntilFinish(Duration duration) {\n-      throw new UnsupportedOperationException(\n-          \"DirectPipelineResult does not support waitUntilFinish with a Duration parameter. See\"\n-              + \" BEAM-596.\");\n+      return this.state;\n     }\n   }\n ",
                "changes": 132
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
                "deletions": 3,
                "sha": "e537962e0ff1f04e36c1d82dbbab27576a1f7c2a",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
                "patch": "@@ -31,16 +31,16 @@\n class DoFnLifecycleManagerRemovingTransformEvaluator<InputT> implements TransformEvaluator<InputT> {\n   private static final Logger LOG =\n       LoggerFactory.getLogger(DoFnLifecycleManagerRemovingTransformEvaluator.class);\n-  private final ParDoEvaluator<InputT, ?> underlying;\n+  private final ParDoEvaluator<InputT> underlying;\n   private final DoFnLifecycleManager lifecycleManager;\n \n   public static <InputT> DoFnLifecycleManagerRemovingTransformEvaluator<InputT> wrapping(\n-      ParDoEvaluator<InputT, ?> underlying, DoFnLifecycleManager lifecycleManager) {\n+      ParDoEvaluator<InputT> underlying, DoFnLifecycleManager lifecycleManager) {\n     return new DoFnLifecycleManagerRemovingTransformEvaluator<>(underlying, lifecycleManager);\n   }\n \n   private DoFnLifecycleManagerRemovingTransformEvaluator(\n-      ParDoEvaluator<InputT, ?> underlying, DoFnLifecycleManager lifecycleManager) {\n+      ParDoEvaluator<InputT> underlying, DoFnLifecycleManager lifecycleManager) {\n     this.underlying = underlying;\n     this.lifecycleManager = lifecycleManager;\n   }",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "deletions": 31,
                "sha": "54ce027279c3de9e4e8df9549795f33133222731",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "patch": "@@ -50,10 +50,8 @@\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.joda.time.Instant;\n \n /**\n@@ -402,37 +400,11 @@ void forceRefresh() {\n \n   /**\n    * Returns true if the step will not produce additional output.\n-   *\n-   * <p>If the provided transform produces only {@link IsBounded#BOUNDED}\n-   * {@link PCollection PCollections}, returns true if the watermark is at\n-   * {@link BoundedWindow#TIMESTAMP_MAX_VALUE positive infinity}.\n-   *\n-   * <p>If the provided transform produces any {@link IsBounded#UNBOUNDED}\n-   * {@link PCollection PCollections}, returns the value of\n-   * {@link DirectOptions#isShutdownUnboundedProducersWithMaxWatermark()}.\n    */\n   public boolean isDone(AppliedPTransform<?, ?, ?> transform) {\n-    // if the PTransform's watermark isn't at the max value, it isn't done\n-    if (watermarkManager\n-        .getWatermarks(transform)\n-        .getOutputWatermark()\n-        .isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {\n-      return false;\n-    }\n-    // If the PTransform has any unbounded outputs, and unbounded producers should not be shut down,\n-    // the PTransform may produce additional output. It is not done.\n-    for (TaggedPValue output : transform.getOutputs()) {\n-      if (output.getValue() instanceof PCollection) {\n-        IsBounded bounded = ((PCollection<?>) output.getValue()).isBounded();\n-        if (bounded.equals(IsBounded.UNBOUNDED)\n-            && !options.isShutdownUnboundedProducersWithMaxWatermark()) {\n-          return false;\n-        }\n-      }\n-    }\n-    // The PTransform's watermark was at positive infinity and all of its outputs are known to be\n-    // done. It is done.\n-    return true;\n+    // the PTransform is done only if watermark is at the max value\n+    Instant stepWatermark = watermarkManager.getWatermarks(transform).getOutputWatermark();\n+    return !stepWatermark.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE);\n   }\n \n   /**",
                "changes": 34
            },
            {
                "status": "modified",
                "additions": 105,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "deletions": 36,
                "sha": "c802c58be6e52bbba5b7e9c068e3c115b38e8065",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "patch": "@@ -25,6 +25,8 @@\n import com.google.common.cache.CacheBuilder;\n import com.google.common.cache.CacheLoader;\n import com.google.common.cache.LoadingCache;\n+import com.google.common.cache.RemovalListener;\n+import com.google.common.cache.RemovalNotification;\n import com.google.common.collect.Iterables;\n import java.util.ArrayList;\n import java.util.Collection;\n@@ -48,13 +50,16 @@\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.WatermarkManager.FiredTimers;\n import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult.State;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PValue;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -99,6 +104,7 @@\n    * {@link CompletionCallback} decrement this value.\n    */\n   private final AtomicLong outstandingWork = new AtomicLong();\n+  private AtomicReference<State> pipelineState = new AtomicReference<>(State.RUNNING);\n \n   public static ExecutorServiceParallelExecutor create(\n       int targetParallelism,\n@@ -138,7 +144,10 @@ private ExecutorServiceParallelExecutor(\n     // Executing TransformExecutorServices have a strong reference to their TransformExecutorService\n     // which stops the TransformExecutorServices from being prematurely garbage collected\n     executorServices =\n-        CacheBuilder.newBuilder().weakValues().build(serialTransformExecutorServiceCacheLoader());\n+        CacheBuilder.newBuilder()\n+            .weakValues()\n+            .removalListener(shutdownExecutorServiceListener())\n+            .build(serialTransformExecutorServiceCacheLoader());\n \n     this.allUpdates = new ConcurrentLinkedQueue<>();\n     this.visibleUpdates = new LinkedBlockingQueue<>();\n@@ -159,6 +168,19 @@ public TransformExecutorService load(StepAndKey stepAndKey) throws Exception {\n     };\n   }\n \n+  private RemovalListener<StepAndKey, TransformExecutorService> shutdownExecutorServiceListener() {\n+    return new RemovalListener<StepAndKey, TransformExecutorService>() {\n+      @Override\n+      public void onRemoval(\n+          RemovalNotification<StepAndKey, TransformExecutorService> notification) {\n+        TransformExecutorService service = notification.getValue();\n+        if (service != null) {\n+          service.shutdown();\n+        }\n+      }\n+    };\n+  }\n+\n   @Override\n   public void start(Collection<AppliedPTransform<?, ?, ?>> roots) {\n     int numTargetSplits = Math.max(3, targetParallelism);\n@@ -179,7 +201,7 @@ public void start(Collection<AppliedPTransform<?, ?, ?>> roots) {\n   }\n \n   @SuppressWarnings(\"unchecked\")\n-  public void scheduleConsumption(\n+  private void scheduleConsumption(\n       AppliedPTransform<?, ?, ?> consumer,\n       CommittedBundle<?> bundle,\n       CompletionCallback onComplete) {\n@@ -219,7 +241,9 @@ public void scheduleConsumption(\n             onComplete,\n             transformExecutor);\n     outstandingWork.incrementAndGet();\n-    transformExecutor.schedule(callable);\n+    if (!pipelineState.get().isTerminal()) {\n+      transformExecutor.schedule(callable);\n+    }\n   }\n \n   private boolean isKeyed(PValue pvalue) {\n@@ -234,20 +258,66 @@ private void scheduleConsumers(ExecutorUpdate update) {\n   }\n \n   @Override\n-  public void awaitCompletion() throws Exception {\n-    VisibleExecutorUpdate update;\n-    do {\n-      // Get an update; don't block forever if another thread has handled it\n-      update = visibleUpdates.poll(2L, TimeUnit.SECONDS);\n-      if (update == null && executorService.isShutdown()) {\n+  public State waitUntilFinish(Duration duration) throws Exception {\n+    Instant completionTime;\n+    if (duration.equals(Duration.ZERO)) {\n+      completionTime = new Instant(Long.MAX_VALUE);\n+    } else {\n+      completionTime = Instant.now().plus(duration);\n+    }\n+\n+    VisibleExecutorUpdate update = null;\n+    while (Instant.now().isBefore(completionTime)\n+        && (update == null || isTerminalStateUpdate(update))) {\n+      // Get an update; don't block forever if another thread has handled it. The call to poll will\n+      // wait the entire timeout; this call primarily exists to relinquish any core.\n+      update = visibleUpdates.poll(25L, TimeUnit.MILLISECONDS);\n+      if (update == null && pipelineState.get().isTerminal()) {\n         // there are no updates to process and no updates will ever be published because the\n         // executor is shutdown\n-        return;\n+        return pipelineState.get();\n       } else if (update != null && update.exception.isPresent()) {\n         throw update.exception.get();\n       }\n-    } while (update == null || !update.isDone());\n+    }\n+    return pipelineState.get();\n+  }\n+\n+  @Override\n+  public State getPipelineState() {\n+    return pipelineState.get();\n+  }\n+\n+  private boolean isTerminalStateUpdate(VisibleExecutorUpdate update) {\n+    return !(update.getNewState() == null && update.getNewState().isTerminal());\n+  }\n+\n+  @Override\n+  public void stop() {\n+    shutdownIfNecessary(State.CANCELLED);\n+    while (!visibleUpdates.offer(VisibleExecutorUpdate.cancelled())) {\n+      // Make sure \"This Pipeline was Cancelled\" notification arrives.\n+      visibleUpdates.poll();\n+    }\n+  }\n+\n+  private void shutdownIfNecessary(State newState) {\n+    if (!newState.isTerminal()) {\n+      return;\n+    }\n+    LOG.debug(\"Pipeline has terminated. Shutting down.\");\n+    pipelineState.compareAndSet(State.RUNNING, newState);\n+    // Stop accepting new work before shutting down the executor. This ensures that thread don't try\n+    // to add work to the shutdown executor.\n+    executorServices.invalidateAll();\n+    executorServices.cleanUp();\n+    parallelExecutorService.shutdown();\n     executorService.shutdown();\n+    try {\n+      registry.cleanup();\n+    } catch (Exception e) {\n+      visibleUpdates.add(VisibleExecutorUpdate.fromException(e));\n+    }\n   }\n \n   /**\n@@ -341,29 +411,35 @@ public static ExecutorUpdate fromException(Exception e) {\n   }\n \n   /**\n-   * An update of interest to the user. Used in {@link #awaitCompletion} to decide whether to\n+   * An update of interest to the user. Used in {@link #waitUntilFinish} to decide whether to\n    * return normally or throw an exception.\n    */\n   private static class VisibleExecutorUpdate {\n     private final Optional<? extends Exception> exception;\n-    private final boolean done;\n+    @Nullable\n+    private final State newState;\n \n     public static VisibleExecutorUpdate fromException(Exception e) {\n-      return new VisibleExecutorUpdate(false, e);\n+      return new VisibleExecutorUpdate(null, e);\n     }\n \n     public static VisibleExecutorUpdate finished() {\n-      return new VisibleExecutorUpdate(true, null);\n+      return new VisibleExecutorUpdate(State.DONE, null);\n+    }\n+\n+    public static VisibleExecutorUpdate cancelled() {\n+      return new VisibleExecutorUpdate(State.CANCELLED, null);\n     }\n \n-    private VisibleExecutorUpdate(boolean done, @Nullable Exception exception) {\n+    private VisibleExecutorUpdate(State newState, @Nullable Exception exception) {\n       this.exception = Optional.fromNullable(exception);\n-      this.done = done;\n+      this.newState = newState;\n     }\n \n-    public boolean isDone() {\n-      return done;\n+    public State getNewState() {\n+      return newState;\n     }\n+\n   }\n \n   private class MonitorRunnable implements Runnable {\n@@ -458,8 +534,8 @@ private void fireTimers() throws Exception {\n                   .createKeyedBundle(\n                       transformTimers.getKey(),\n                       (PCollection)\n-                          Iterables.getOnlyElement(transformTimers.getTransform().getInputs())\n-                              .getValue())\n+                          Iterables.getOnlyElement(\n+                              transformTimers.getTransform().getInputs().values()))\n                   .add(WindowedValue.valueInGlobalWindow(work))\n                   .commit(evaluationContext.now());\n           scheduleConsumption(\n@@ -475,22 +551,15 @@ private void fireTimers() throws Exception {\n     }\n \n     private boolean shouldShutdown() {\n-      boolean shouldShutdown = exceptionThrown || evaluationContext.isDone();\n-      if (shouldShutdown) {\n-        LOG.debug(\"Pipeline has terminated. Shutting down.\");\n-        executorService.shutdown();\n-        try {\n-          registry.cleanup();\n-        } catch (Exception e) {\n-          visibleUpdates.add(VisibleExecutorUpdate.fromException(e));\n-        }\n-        if (evaluationContext.isDone()) {\n-          while (!visibleUpdates.offer(VisibleExecutorUpdate.finished())) {\n-            visibleUpdates.poll();\n-          }\n-        }\n+      State nextState = State.UNKNOWN;\n+      if (exceptionThrown) {\n+        nextState = State.FAILED;\n+      } else if (evaluationContext.isDone()) {\n+        visibleUpdates.offer(VisibleExecutorUpdate.finished());\n+        nextState = State.DONE;\n       }\n-      return shouldShutdown;\n+      shutdownIfNecessary(nextState);\n+      return pipelineState.get().isTerminal();\n     }\n \n     /**",
                "changes": 141
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
                "deletions": 1,
                "sha": "7c6d2a166269de523091b87a062218835203e3b8",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
                "patch": "@@ -57,7 +57,7 @@ public void cleanup() throws Exception {}\n           application) {\n     final UncommittedBundle<InputT> outputBundle =\n         evaluationContext.createBundle(\n-            (PCollection<InputT>) Iterables.getOnlyElement(application.getOutputs()).getValue());\n+            (PCollection<InputT>) Iterables.getOnlyElement(application.getOutputs().values()));\n     final TransformResult<InputT> result =\n         StepTransformResult.<InputT>withoutHold(application).addOutput(outputBundle).build();\n     return new FlattenEvaluator<>(outputBundle, result);",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "deletions": 6,
                "sha": "ce7b12a4ed80685ce4f7affce553d437b98e2d89",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.runners.core.TimerInternals;\n import org.apache.beam.runners.core.UnsupportedSideInputReader;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.runners.direct.DirectExecutionContext.DirectStepContext;\n@@ -47,7 +48,6 @@\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n@@ -162,7 +162,7 @@ public void processElement(WindowedValue<KeyedWorkItem<K, V>> element) throws Ex\n           evaluationContext.createKeyedBundle(\n               structuralKey,\n               (PCollection<KV<K, Iterable<V>>>)\n-                  Iterables.getOnlyElement(application.getOutputs()).getValue());\n+                  Iterables.getOnlyElement(application.getOutputs().values()));\n       outputBundles.add(bundle);\n       CopyOnAccessInMemoryStateInternals<K> stateInternals =\n           (CopyOnAccessInMemoryStateInternals<K>) stepContext.stateInternals();\n@@ -264,13 +264,13 @@ public void outputWindowedValue(\n     }\n \n     @Override\n-    public <SideOutputT> void sideOutputWindowedValue(\n-        TupleTag<SideOutputT> tag,\n-        SideOutputT output,\n+    public <AdditionalOutputT> void outputWindowedValue(\n+        TupleTag<AdditionalOutputT> tag,\n+        AdditionalOutputT output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs\");\n+      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use tagged outputs\");\n     }\n   }\n }",
                "changes": 12
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "deletions": 2,
                "sha": "ac0b14ffba34e629e2c68084fca5fb86fccdeea7",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "patch": "@@ -105,7 +105,7 @@ public GroupByKeyOnlyEvaluator(\n       this.application = application;\n       this.keyCoder =\n           getKeyCoder(\n-              ((PCollection<KV<K, V>>) Iterables.getOnlyElement(application.getInputs()).getValue())\n+              ((PCollection<KV<K, V>>) Iterables.getOnlyElement(application.getInputs().values()))\n                   .getCoder());\n       this.groupingMap = new HashMap<>();\n     }\n@@ -158,7 +158,7 @@ public void processElement(WindowedValue<KV<K, V>> element) {\n             evaluationContext.createKeyedBundle(\n                 StructuralKey.of(key, keyCoder),\n                 (PCollection<KeyedWorkItem<K, V>>)\n-                    Iterables.getOnlyElement(application.getOutputs()).getValue());\n+                    Iterables.getOnlyElement(application.getOutputs().values()));\n         bundle.add(WindowedValue.valueInGlobalWindow(groupedKv));\n         resultBuilder.addOutput(bundle);\n       }",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 9,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "deletions": 9,
                "sha": "f9b6eba215f253b56eeb8545b96c3a3c9bf0f030",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "patch": "@@ -21,7 +21,7 @@\n \n import com.google.common.collect.ImmutableSet;\n import java.util.HashSet;\n-import java.util.List;\n+import java.util.Map;\n import java.util.Set;\n import org.apache.beam.runners.core.SplittableParDo;\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupAlsoByWindow;\n@@ -32,7 +32,7 @@\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A pipeline visitor that tracks all keyed {@link PValue PValues}. A {@link PValue} is keyed if it\n@@ -83,9 +83,9 @@ public void leaveCompositeTransform(TransformHierarchy.Node node) {\n     if (node.isRootNode()) {\n       finalized = true;\n     } else if (PRODUCES_KEYED_OUTPUTS.contains(node.getTransform().getClass())) {\n-      List<TaggedPValue> outputs = node.getOutputs();\n-      for (TaggedPValue output : outputs) {\n-        keyedValues.add(output.getValue());\n+      Map<TupleTag<?>, PValue> outputs = node.getOutputs();\n+      for (PValue output : outputs.values()) {\n+        keyedValues.add(output);\n       }\n     }\n   }\n@@ -96,8 +96,8 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {}\n   @Override\n   public void visitValue(PValue value, TransformHierarchy.Node producer) {\n     boolean inputsAreKeyed = true;\n-    for (TaggedPValue input : producer.getInputs()) {\n-      inputsAreKeyed = inputsAreKeyed && keyedValues.contains(input.getValue());\n+    for (PValue input : producer.getInputs().values()) {\n+      inputsAreKeyed = inputsAreKeyed && keyedValues.contains(input);\n     }\n     if (PRODUCES_KEYED_OUTPUTS.contains(producer.getTransform().getClass())\n         || (isKeyPreserving(producer.getTransform()) && inputsAreKeyed)) {\n@@ -116,8 +116,8 @@ private static boolean isKeyPreserving(PTransform<?, ?> transform) {\n     // The most obvious alternative would be a package-private marker interface, but\n     // better to make this obviously hacky so it is less likely to proliferate. Meanwhile\n     // we intend to allow explicit expression of key-preserving DoFn in the model.\n-    if (transform instanceof ParDo.BoundMulti) {\n-      ParDo.BoundMulti<?, ?> parDo = (ParDo.BoundMulti<?, ?>) transform;\n+    if (transform instanceof ParDo.MultiOutput) {\n+      ParDo.MultiOutput<?, ?> parDo = (ParDo.MultiOutput<?, ?>) transform;\n       return parDo.getFn() instanceof ParDoMultiOverrideFactory.ToKeyedWorkItem;\n     } else {\n       return false;",
                "changes": 18
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
                "deletions": 7,
                "sha": "96dbc2bc2aeaafc8c7ae40038a408707c231c0cf",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.beam.runners.direct;\n \n-import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -28,9 +27,9 @@\n  *\n  * <p>ModelEnforcement is performed on a per-element and per-bundle basis. The\n  * {@link ModelEnforcement} is provided with the input bundle as part of\n- * {@link ModelEnforcementFactory#forBundle(CommittedBundle, AppliedPTransform)}, each element\n- * before and after that element is provided to an underlying {@link TransformEvaluator}, and the\n- * output {@link TransformResult} and committed output bundles after the\n+ * {@link ModelEnforcementFactory#forBundle(DirectRunner.CommittedBundle, AppliedPTransform)} each\n+ * element before and after that element is provided to an underlying {@link TransformEvaluator},\n+ * and the output {@link TransformResult} and committed output bundles after the\n  * {@link TransformEvaluator} has completed.\n  *\n  * <p>Typically, {@link ModelEnforcement} will obtain required metadata (such as the {@link Coder}\n@@ -54,10 +53,10 @@\n   /**\n    * Called after a bundle has been completed and {@link TransformEvaluator#finishBundle()} has been\n    * called, producing the provided {@link TransformResult} and\n-   * {@link CommittedBundle output bundles}.\n+   * {@link DirectRunner.CommittedBundle output bundles}.\n    */\n   void afterFinish(\n-      CommittedBundle<T> input,\n+      DirectRunner.CommittedBundle<T> input,\n       TransformResult<T> result,\n-      Iterable<? extends CommittedBundle<?>> outputs);\n+      Iterable<? extends DirectRunner.CommittedBundle<?>> outputs);\n }",
                "changes": 13
            },
            {
                "status": "modified",
                "additions": 93,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "deletions": 36,
                "sha": "cab11db2abe43a46078677e91bc812d713f28664",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "patch": "@@ -26,9 +26,11 @@\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n+import org.apache.beam.runners.core.SimplePushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n import org.apache.beam.runners.direct.DirectExecutionContext.DirectStepContext;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n+import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n@@ -40,9 +42,53 @@\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n \n-class ParDoEvaluator<InputT, OutputT> implements TransformEvaluator<InputT> {\n+class ParDoEvaluator<InputT> implements TransformEvaluator<InputT> {\n \n-  public static <InputT, OutputT> ParDoEvaluator<InputT, OutputT> create(\n+  public interface DoFnRunnerFactory<InputT, OutputT> {\n+    PushbackSideInputDoFnRunner<InputT, OutputT> createRunner(\n+        PipelineOptions options,\n+        DoFn<InputT, OutputT> fn,\n+        List<PCollectionView<?>> sideInputs,\n+        ReadyCheckingSideInputReader sideInputReader,\n+        OutputManager outputManager,\n+        TupleTag<OutputT> mainOutputTag,\n+        List<TupleTag<?>> additionalOutputTags,\n+        DirectStepContext stepContext,\n+        AggregatorContainer.Mutator aggregatorChanges,\n+        WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy);\n+  }\n+\n+  public static <InputT, OutputT> DoFnRunnerFactory<InputT, OutputT> defaultRunnerFactory() {\n+    return new DoFnRunnerFactory<InputT, OutputT>() {\n+      @Override\n+      public PushbackSideInputDoFnRunner<InputT, OutputT> createRunner(\n+          PipelineOptions options,\n+          DoFn<InputT, OutputT> fn,\n+          List<PCollectionView<?>> sideInputs,\n+          ReadyCheckingSideInputReader sideInputReader,\n+          OutputManager outputManager,\n+          TupleTag<OutputT> mainOutputTag,\n+          List<TupleTag<?>> additionalOutputTags,\n+          DirectStepContext stepContext,\n+          AggregatorContainer.Mutator aggregatorChanges,\n+          WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy) {\n+        DoFnRunner<InputT, OutputT> underlying =\n+            DoFnRunners.simpleRunner(\n+                options,\n+                fn,\n+                sideInputReader,\n+                outputManager,\n+                mainOutputTag,\n+                additionalOutputTags,\n+                stepContext,\n+                aggregatorChanges,\n+                windowingStrategy);\n+        return SimplePushbackSideInputDoFnRunner.create(underlying, sideInputs, sideInputReader);\n+      }\n+    };\n+  }\n+\n+  public static <InputT, OutputT> ParDoEvaluator<InputT> create(\n       EvaluationContext evaluationContext,\n       DirectStepContext stepContext,\n       AppliedPTransform<?, ?, ?> application,\n@@ -51,10 +97,44 @@\n       StructuralKey<?> key,\n       List<PCollectionView<?>> sideInputs,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n-      Map<TupleTag<?>, PCollection<?>> outputs) {\n+      List<TupleTag<?>> additionalOutputTags,\n+      Map<TupleTag<?>, PCollection<?>> outputs,\n+      DoFnRunnerFactory<InputT, OutputT> runnerFactory) {\n     AggregatorContainer.Mutator aggregatorChanges = evaluationContext.getAggregatorMutator();\n \n+    BundleOutputManager outputManager = createOutputManager(evaluationContext, key, outputs);\n+\n+    ReadyCheckingSideInputReader sideInputReader =\n+        evaluationContext.createSideInputReader(sideInputs);\n+\n+    PushbackSideInputDoFnRunner<InputT, OutputT> runner = runnerFactory.createRunner(\n+        evaluationContext.getPipelineOptions(),\n+        fn,\n+        sideInputs,\n+        sideInputReader,\n+        outputManager,\n+        mainOutputTag,\n+        additionalOutputTags,\n+        stepContext,\n+        aggregatorChanges,\n+        windowingStrategy);\n+\n+    return create(runner, stepContext, application, aggregatorChanges, outputManager);\n+  }\n+\n+  public static <InputT, OutputT> ParDoEvaluator<InputT> create(\n+      PushbackSideInputDoFnRunner<InputT, OutputT> runner,\n+      DirectStepContext stepContext,\n+      AppliedPTransform<?, ?, ?> application,\n+      AggregatorContainer.Mutator aggregatorChanges,\n+      BundleOutputManager outputManager) {\n+    return new ParDoEvaluator<>(runner, application, aggregatorChanges, outputManager, stepContext);\n+  }\n+\n+  static BundleOutputManager createOutputManager(\n+      EvaluationContext evaluationContext,\n+      StructuralKey<?> key,\n+      Map<TupleTag<?>, PCollection<?>> outputs) {\n     Map<TupleTag<?>, UncommittedBundle<?>> outputBundles = new HashMap<>();\n     for (Map.Entry<TupleTag<?>, PCollection<?>> outputEntry : outputs.entrySet()) {\n       // Just trust the context's decision as to whether the output should be keyed.\n@@ -68,38 +148,11 @@\n             outputEntry.getKey(), evaluationContext.createBundle(outputEntry.getValue()));\n       }\n     }\n-    BundleOutputManager outputManager = BundleOutputManager.create(outputBundles);\n-\n-    ReadyCheckingSideInputReader sideInputReader =\n-        evaluationContext.createSideInputReader(sideInputs);\n-\n-    DoFnRunner<InputT, OutputT> underlying =\n-        DoFnRunners.simpleRunner(\n-            evaluationContext.getPipelineOptions(),\n-            fn,\n-            sideInputReader,\n-            outputManager,\n-            mainOutputTag,\n-            sideOutputTags,\n-            stepContext,\n-            aggregatorChanges,\n-            windowingStrategy);\n-    PushbackSideInputDoFnRunner<InputT, OutputT> runner =\n-        PushbackSideInputDoFnRunner.create(underlying, sideInputs, sideInputReader);\n-\n-    try {\n-      runner.startBundle();\n-    } catch (Exception e) {\n-      throw UserCodeException.wrap(e);\n-    }\n-\n-    return new ParDoEvaluator<>(\n-        evaluationContext, runner, application, aggregatorChanges, outputManager, stepContext);\n+    return BundleOutputManager.create(outputBundles);\n   }\n \n   ////////////////////////////////////////////////////////////////////////////////////////////////\n \n-  private final EvaluationContext evaluationContext;\n   private final PushbackSideInputDoFnRunner<InputT, ?> fnRunner;\n   private final AppliedPTransform<?, ?, ?> transform;\n   private final AggregatorContainer.Mutator aggregatorChanges;\n@@ -109,19 +162,23 @@\n   private final ImmutableList.Builder<WindowedValue<InputT>> unprocessedElements;\n \n   private ParDoEvaluator(\n-      EvaluationContext evaluationContext,\n       PushbackSideInputDoFnRunner<InputT, ?> fnRunner,\n       AppliedPTransform<?, ?, ?> transform,\n       AggregatorContainer.Mutator aggregatorChanges,\n       BundleOutputManager outputManager,\n       DirectStepContext stepContext) {\n-    this.evaluationContext = evaluationContext;\n     this.fnRunner = fnRunner;\n     this.transform = transform;\n     this.outputManager = outputManager;\n     this.stepContext = stepContext;\n     this.aggregatorChanges = aggregatorChanges;\n     this.unprocessedElements = ImmutableList.builder();\n+\n+    try {\n+      fnRunner.startBundle();\n+    } catch (Exception e) {\n+      throw UserCodeException.wrap(e);\n+    }\n   }\n \n   public BundleOutputManager getOutputManager() {\n@@ -153,11 +210,11 @@ public void onTimer(TimerData timer, BoundedWindow window) {\n     } catch (Exception e) {\n       throw UserCodeException.wrap(e);\n     }\n-    StepTransformResult.Builder resultBuilder;\n+    StepTransformResult.Builder<InputT> resultBuilder;\n     CopyOnAccessInMemoryStateInternals<?> state = stepContext.commitState();\n     if (state != null) {\n       resultBuilder =\n-          StepTransformResult.withHold(transform, state.getEarliestWatermarkHold())\n+          StepTransformResult.<InputT>withHold(transform, state.getEarliestWatermarkHold())\n               .withState(state);\n     } else {\n       resultBuilder = StepTransformResult.withoutHold(transform);",
                "changes": 129
            },
            {
                "status": "modified",
                "additions": 22,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "deletions": 17,
                "sha": "b00c2b67a5d8b29e6d2cf64fc2786f366a319742",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "patch": "@@ -32,20 +32,24 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-/** A {@link TransformEvaluatorFactory} for {@link ParDo.BoundMulti}. */\n+/** A {@link TransformEvaluatorFactory} for {@link ParDo.MultiOutput}. */\n final class ParDoEvaluatorFactory<InputT, OutputT> implements TransformEvaluatorFactory {\n \n   private static final Logger LOG = LoggerFactory.getLogger(ParDoEvaluatorFactory.class);\n   private final LoadingCache<DoFn<?, ?>, DoFnLifecycleManager> fnClones;\n   private final EvaluationContext evaluationContext;\n+  private final ParDoEvaluator.DoFnRunnerFactory<InputT, OutputT> runnerFactory;\n \n-  ParDoEvaluatorFactory(EvaluationContext evaluationContext) {\n+  ParDoEvaluatorFactory(\n+      EvaluationContext evaluationContext,\n+      ParDoEvaluator.DoFnRunnerFactory<InputT, OutputT> runnerFactory) {\n     this.evaluationContext = evaluationContext;\n+    this.runnerFactory = runnerFactory;\n     fnClones =\n         CacheBuilder.newBuilder()\n             .build(\n@@ -62,13 +66,13 @@ public DoFnLifecycleManager load(DoFn<?, ?> key) throws Exception {\n       AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle) throws Exception {\n \n     @SuppressWarnings(\"unchecked\")\n-    AppliedPTransform<PCollection<InputT>, PCollectionTuple, ParDo.BoundMulti<InputT, OutputT>>\n+    AppliedPTransform<PCollection<InputT>, PCollectionTuple, ParDo.MultiOutput<InputT, OutputT>>\n         parDoApplication =\n             (AppliedPTransform<\n-                    PCollection<InputT>, PCollectionTuple, ParDo.BoundMulti<InputT, OutputT>>)\n+                    PCollection<InputT>, PCollectionTuple, ParDo.MultiOutput<InputT, OutputT>>)\n                 application;\n \n-    ParDo.BoundMulti<InputT, OutputT> transform = parDoApplication.getTransform();\n+    ParDo.MultiOutput<InputT, OutputT> transform = parDoApplication.getTransform();\n     final DoFn<InputT, OutputT> doFn = transform.getFn();\n \n     @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n@@ -80,7 +84,7 @@ public DoFnLifecycleManager load(DoFn<?, ?> key) throws Exception {\n                 doFn,\n                 transform.getSideInputs(),\n                 transform.getMainOutputTag(),\n-                transform.getSideOutputTags().getAll());\n+                transform.getAdditionalOutputTags().getAll());\n     return evaluator;\n   }\n \n@@ -103,7 +107,7 @@ public void cleanup() throws Exception {\n       DoFn<InputT, OutputT> doFn,\n       List<PCollectionView<?>> sideInputs,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags)\n+      List<TupleTag<?>> additionalOutputTags)\n       throws Exception {\n     String stepName = evaluationContext.getStepName(application);\n     DirectStepContext stepContext =\n@@ -119,19 +123,19 @@ public void cleanup() throws Exception {\n             inputBundleKey,\n             sideInputs,\n             mainOutputTag,\n-            sideOutputTags,\n+            additionalOutputTags,\n             stepContext,\n             fnManager.<InputT, OutputT>get(),\n             fnManager),\n         fnManager);\n   }\n \n-  ParDoEvaluator<InputT, OutputT> createParDoEvaluator(\n+  ParDoEvaluator<InputT> createParDoEvaluator(\n       AppliedPTransform<PCollection<InputT>, PCollectionTuple, ?> application,\n       StructuralKey<?> key,\n       List<PCollectionView<?>> sideInputs,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       DirectStepContext stepContext,\n       DoFn<InputT, OutputT> fn,\n       DoFnLifecycleManager fnManager)\n@@ -141,14 +145,15 @@ public void cleanup() throws Exception {\n           evaluationContext,\n           stepContext,\n           application,\n-          ((PCollection<InputT>) Iterables.getOnlyElement(application.getInputs()).getValue())\n+          ((PCollection<InputT>) Iterables.getOnlyElement(application.getInputs().values()))\n               .getWindowingStrategy(),\n           fn,\n           key,\n           sideInputs,\n           mainOutputTag,\n-          sideOutputTags,\n-          pcollections(application.getOutputs()));\n+          additionalOutputTags,\n+          pcollections(application.getOutputs()),\n+          runnerFactory);\n     } catch (Exception e) {\n       try {\n         fnManager.remove();\n@@ -162,10 +167,10 @@ public void cleanup() throws Exception {\n     }\n   }\n \n-  private Map<TupleTag<?>, PCollection<?>> pcollections(List<TaggedPValue> outputs) {\n+  static Map<TupleTag<?>, PCollection<?>> pcollections(Map<TupleTag<?>, PValue> outputs) {\n     Map<TupleTag<?>, PCollection<?>> pcs = new HashMap<>();\n-    for (TaggedPValue output : outputs) {\n-      pcs.put(output.getTag(), (PCollection<?>) output.getValue());\n+    for (Map.Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {\n+      pcs.put(output.getKey(), (PCollection<?>) output.getValue());\n     }\n     return pcs;\n   }",
                "changes": 39
            },
            {
                "status": "modified",
                "additions": 28,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "deletions": 25,
                "sha": "b08aa8ebc4d19801c97f446d00926d6fc292965f",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "patch": "@@ -19,24 +19,23 @@\n \n import static com.google.common.base.Preconditions.checkState;\n \n-import com.google.common.collect.Iterables;\n-import java.util.List;\n import java.util.Map;\n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.KeyedWorkItemCoder;\n import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.core.SplittableParDo;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.ReplacementOutputs;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.ParDo.BoundMulti;\n+import org.apache.beam.sdk.transforms.ParDo.MultiOutput;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.AfterPane;\n@@ -50,7 +49,7 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.apache.beam.sdk.values.TupleTagList;\n import org.apache.beam.sdk.values.TypedPValue;\n \n@@ -61,11 +60,21 @@\n  */\n class ParDoMultiOverrideFactory<InputT, OutputT>\n     implements PTransformOverrideFactory<\n-        PCollection<? extends InputT>, PCollectionTuple, BoundMulti<InputT, OutputT>> {\n+        PCollection<? extends InputT>, PCollectionTuple, MultiOutput<InputT, OutputT>> {\n   @Override\n+  public PTransformReplacement<PCollection<? extends InputT>, PCollectionTuple>\n+      getReplacementTransform(\n+          AppliedPTransform<\n+                  PCollection<? extends InputT>, PCollectionTuple, MultiOutput<InputT, OutputT>>\n+              transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        getReplacementTransform(transform.getTransform()));\n+  }\n+\n   @SuppressWarnings(\"unchecked\")\n-  public PTransform<PCollection<? extends InputT>, PCollectionTuple> getReplacementTransform(\n-      BoundMulti<InputT, OutputT> transform) {\n+  private PTransform<PCollection<? extends InputT>, PCollectionTuple> getReplacementTransform(\n+      MultiOutput<InputT, OutputT> transform) {\n \n     DoFn<InputT, OutputT> fn = transform.getFn();\n     DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());\n@@ -75,32 +84,26 @@\n         || signature.timerDeclarations().size() > 0) {\n       // Based on the fact that the signature is stateful, DoFnSignatures ensures\n       // that it is also keyed\n-      ParDo.BoundMulti<KV<?, ?>, OutputT> keyedTransform =\n-          (ParDo.BoundMulti<KV<?, ?>, OutputT>) transform;\n+      MultiOutput<KV<?, ?>, OutputT> keyedTransform =\n+          (MultiOutput<KV<?, ?>, OutputT>) transform;\n \n       return new GbkThenStatefulParDo(keyedTransform);\n     } else {\n       return transform;\n     }\n   }\n \n-  @Override\n-  public PCollection<? extends InputT> getInput(\n-      List<TaggedPValue> inputs, Pipeline p) {\n-    return (PCollection<? extends InputT>) Iterables.getOnlyElement(inputs).getValue();\n-  }\n-\n   @Override\n   public Map<PValue, ReplacementOutput> mapOutputs(\n-      List<TaggedPValue> outputs, PCollectionTuple newOutput) {\n+      Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput) {\n     return ReplacementOutputs.tagged(outputs, newOutput);\n   }\n \n   static class GbkThenStatefulParDo<K, InputT, OutputT>\n       extends PTransform<PCollection<KV<K, InputT>>, PCollectionTuple> {\n-    private final ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo;\n+    private final MultiOutput<KV<K, InputT>, OutputT> underlyingParDo;\n \n-    public GbkThenStatefulParDo(ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo) {\n+    public GbkThenStatefulParDo(MultiOutput<KV<K, InputT>, OutputT> underlyingParDo) {\n       this.underlyingParDo = underlyingParDo;\n     }\n \n@@ -135,8 +138,8 @@ public PCollectionTuple expand(PCollection<KV<K, InputT>> input) {\n               //  - ensure this GBK holds to the minimum of those timestamps (via OutputTimeFn)\n               //  - discard past panes as it is \"just a stream\" of elements\n               .apply(\n-                  Window.<KV<K, WindowedValue<KV<K, InputT>>>>triggering(\n-                          Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n+                  Window.<KV<K, WindowedValue<KV<K, InputT>>>>configure()\n+                      .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n                       .discardingFiredPanes()\n                       .withAllowedLateness(inputWindowingStrategy.getAllowedLateness())\n                       .withOutputTimeFn(OutputTimeFns.outputAtEarliestInputTimestamp()))\n@@ -165,17 +168,17 @@ public PCollectionTuple expand(PCollection<KV<K, InputT>> input) {\n \n   static class StatefulParDo<K, InputT, OutputT>\n       extends PTransform<PCollection<? extends KeyedWorkItem<K, KV<K, InputT>>>, PCollectionTuple> {\n-    private final transient ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo;\n+    private final transient MultiOutput<KV<K, InputT>, OutputT> underlyingParDo;\n     private final transient PCollection<KV<K, InputT>> originalInput;\n \n     public StatefulParDo(\n-        ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo,\n+        MultiOutput<KV<K, InputT>, OutputT> underlyingParDo,\n         PCollection<KV<K, InputT>> originalInput) {\n       this.underlyingParDo = underlyingParDo;\n       this.originalInput = originalInput;\n     }\n \n-    public ParDo.BoundMulti<KV<K, InputT>, OutputT> getUnderlyingParDo() {\n+    public MultiOutput<KV<K, InputT>, OutputT> getUnderlyingParDo() {\n       return underlyingParDo;\n     }\n \n@@ -193,7 +196,7 @@ public PCollectionTuple expand(PCollection<? extends KeyedWorkItem<K, KV<K, Inpu\n           PCollectionTuple.ofPrimitiveOutputsInternal(\n               input.getPipeline(),\n               TupleTagList.of(underlyingParDo.getMainOutputTag())\n-                  .and(underlyingParDo.getSideOutputTags().getAll()),\n+                  .and(underlyingParDo.getAdditionalOutputTags().getAll()),\n               input.getWindowingStrategy(),\n               input.isBounded());\n ",
                "changes": 53
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "deletions": 70,
                "sha": "f8597299217b41c90f02d96fd3d436c22e51dd66",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "patch": "@@ -1,70 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.direct;\n-\n-import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n-import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.ParDo.Bound;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionTuple;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.beam.sdk.values.TupleTagList;\n-\n-/**\n- * A {@link PTransformOverrideFactory} that overrides single-output {@link ParDo} to implement\n- * it in terms of multi-output {@link ParDo}.\n- */\n-class ParDoSingleViaMultiOverrideFactory<InputT, OutputT>\n-    extends SingleInputOutputOverrideFactory<\n-            PCollection<? extends InputT>, PCollection<OutputT>, Bound<InputT, OutputT>> {\n-  @Override\n-  public PTransform<PCollection<? extends InputT>, PCollection<OutputT>> getReplacementTransform(\n-      Bound<InputT, OutputT> transform) {\n-    return new ParDoSingleViaMulti<>(transform);\n-  }\n-\n-  static class ParDoSingleViaMulti<InputT, OutputT>\n-      extends PTransform<PCollection<? extends InputT>, PCollection<OutputT>> {\n-    private static final String MAIN_OUTPUT_TAG = \"main\";\n-\n-    private final ParDo.Bound<InputT, OutputT> underlyingParDo;\n-\n-    public ParDoSingleViaMulti(ParDo.Bound<InputT, OutputT> underlyingParDo) {\n-      this.underlyingParDo = underlyingParDo;\n-    }\n-\n-    @Override\n-    public PCollection<OutputT> expand(PCollection<? extends InputT> input) {\n-\n-      // Output tags for ParDo need only be unique up to applied transform\n-      TupleTag<OutputT> mainOutputTag = new TupleTag<OutputT>(MAIN_OUTPUT_TAG);\n-\n-      PCollectionTuple outputs =\n-          input.apply(\n-              ParDo.of(underlyingParDo.getFn())\n-                  .withSideInputs(underlyingParDo.getSideInputs())\n-                  .withOutputTags(mainOutputTag, TupleTagList.empty()));\n-      PCollection<OutputT> output = outputs.get(mainOutputTag);\n-\n-      output.setTypeDescriptor(underlyingParDo.getFn().getOutputTypeDescriptor());\n-      return output;\n-    }\n-  }\n-}",
                "changes": 70
            },
            {
                "status": "modified",
                "additions": 21,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
                "deletions": 2,
                "sha": "82f59a7c328820450c695e97a23f49515a60cca1",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
                "patch": "@@ -19,8 +19,11 @@\n \n import java.util.Collection;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult.State;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n+import org.joda.time.Duration;\n \n /**\n  * An executor that schedules and executes {@link AppliedPTransform AppliedPTransforms} for both\n@@ -40,8 +43,24 @@\n    * root {@link AppliedPTransform AppliedPTransforms} have completed, and all\n    * {@link CommittedBundle Bundles} have been consumed. Jobs may also terminate abnormally.\n    *\n-   * @throws Throwable whenever an executor thread throws anything, transfers the throwable to the\n+   * <p>Waits for up to the provided duration, or forever if the provided duration is less than or\n+   * equal to zero.\n+   *\n+   * @return The terminal state of the Pipeline.\n+   * @throws Exception whenever an executor thread throws anything, transfers to the\n    *                   waiting thread and rethrows it\n    */\n-  void awaitCompletion() throws Exception;\n+  State waitUntilFinish(Duration duration) throws Exception;\n+\n+  /**\n+   * Gets the current state of the {@link Pipeline}.\n+   */\n+  State getPipelineState();\n+\n+  /**\n+   * Shuts down the executor.\n+   *\n+   * <p>The executor may continue to run for a short time after this method returns.\n+   */\n+  void stop();\n }",
                "changes": 23
            },
            {
                "status": "modified",
                "additions": 80,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "deletions": 28,
                "sha": "7efdb5263f02ed14b01aaf43ce76d00a9ec24b9d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "patch": "@@ -18,25 +18,34 @@\n package org.apache.beam.runners.direct;\n \n import java.util.Collection;\n+import java.util.List;\n import java.util.concurrent.Executors;\n+import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.ElementAndRestriction;\n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.OutputAndTimeBoundedSplittableProcessElementInvoker;\n import org.apache.beam.runners.core.OutputWindowedValue;\n+import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.SplittableParDo;\n+import org.apache.beam.runners.core.SplittableParDo.ProcessFn;\n import org.apache.beam.runners.core.StateInternals;\n import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.runners.core.TimerInternals;\n import org.apache.beam.runners.core.TimerInternalsFactory;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n+import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n+import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n@@ -51,7 +60,11 @@\n \n   SplittableProcessElementsEvaluatorFactory(EvaluationContext evaluationContext) {\n     this.evaluationContext = evaluationContext;\n-    this.delegateFactory = new ParDoEvaluatorFactory<>(evaluationContext);\n+    this.delegateFactory =\n+        new ParDoEvaluatorFactory<>(\n+            evaluationContext,\n+            SplittableProcessElementsEvaluatorFactory\n+                .<InputT, OutputT, RestrictionT>processFnRunnerFactory());\n   }\n \n   @Override\n@@ -82,12 +95,12 @@ public void cleanup() throws Exception {\n     final SplittableParDo.ProcessElements<InputT, OutputT, RestrictionT, TrackerT> transform =\n         application.getTransform();\n \n-    SplittableParDo.ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn =\n+    ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn =\n         transform.newProcessFn(transform.getFn());\n \n     DoFnLifecycleManager fnManager = DoFnLifecycleManager.of(processFn);\n     processFn =\n-        ((SplittableParDo.ProcessFn<InputT, OutputT, RestrictionT, TrackerT>)\n+        ((ProcessFn<InputT, OutputT, RestrictionT, TrackerT>)\n             fnManager\n                 .<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n                     get());\n@@ -98,14 +111,14 @@ public void cleanup() throws Exception {\n             .getExecutionContext(application, inputBundle.getKey())\n             .getOrCreateStepContext(stepName, stepName);\n \n-    ParDoEvaluator<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+    final ParDoEvaluator<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>\n         parDoEvaluator =\n             delegateFactory.createParDoEvaluator(\n                 application,\n                 inputBundle.getKey(),\n                 transform.getSideInputs(),\n                 transform.getMainOutputTag(),\n-                transform.getSideOutputTags().getAll(),\n+                transform.getAdditionalOutputTags().getAll(),\n                 stepContext,\n                 processFn,\n                 fnManager);\n@@ -127,34 +140,36 @@ public TimerInternals timerInternalsForKey(String key) {\n           }\n         });\n \n-    final OutputManager outputManager = parDoEvaluator.getOutputManager();\n+    OutputWindowedValue<OutputT> outputWindowedValue =\n+        new OutputWindowedValue<OutputT>() {\n+          private final OutputManager outputManager = parDoEvaluator.getOutputManager();\n+\n+          @Override\n+          public void outputWindowedValue(\n+              OutputT output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            outputManager.output(\n+                transform.getMainOutputTag(), WindowedValue.of(output, timestamp, windows, pane));\n+          }\n+\n+          @Override\n+          public <AdditionalOutputT> void outputWindowedValue(\n+              TupleTag<AdditionalOutputT> tag,\n+              AdditionalOutputT output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));\n+          }\n+        };\n     processFn.setProcessElementInvoker(\n         new OutputAndTimeBoundedSplittableProcessElementInvoker<\n             InputT, OutputT, RestrictionT, TrackerT>(\n             transform.getFn(),\n             evaluationContext.getPipelineOptions(),\n-            new OutputWindowedValue<OutputT>() {\n-              @Override\n-              public void outputWindowedValue(\n-                  OutputT output,\n-                  Instant timestamp,\n-                  Collection<? extends BoundedWindow> windows,\n-                  PaneInfo pane) {\n-                outputManager.output(\n-                    transform.getMainOutputTag(),\n-                    WindowedValue.of(output, timestamp, windows, pane));\n-              }\n-\n-              @Override\n-              public <SideOutputT> void sideOutputWindowedValue(\n-                  TupleTag<SideOutputT> tag,\n-                  SideOutputT output,\n-                  Instant timestamp,\n-                  Collection<? extends BoundedWindow> windows,\n-                  PaneInfo pane) {\n-                outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));\n-              }\n-            },\n+            outputWindowedValue,\n             evaluationContext.createSideInputReader(transform.getSideInputs()),\n             // TODO: For better performance, use a higher-level executor?\n             Executors.newSingleThreadScheduledExecutor(Executors.defaultThreadFactory()),\n@@ -163,4 +178,41 @@ public void outputWindowedValue(\n \n     return DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(parDoEvaluator, fnManager);\n   }\n+\n+  private static <InputT, OutputT, RestrictionT>\n+  ParDoEvaluator.DoFnRunnerFactory<\n+                KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+          processFnRunnerFactory() {\n+    return new ParDoEvaluator.DoFnRunnerFactory<\n+            KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>() {\n+      @Override\n+      public PushbackSideInputDoFnRunner<\n+          KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+      createRunner(\n+          PipelineOptions options,\n+          DoFn<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT> fn,\n+          List<PCollectionView<?>> sideInputs,\n+          ReadyCheckingSideInputReader sideInputReader,\n+          OutputManager outputManager,\n+          TupleTag<OutputT> mainOutputTag,\n+          List<TupleTag<?>> additionalOutputTags,\n+          DirectExecutionContext.DirectStepContext stepContext,\n+          AggregatorContainer.Mutator aggregatorChanges,\n+          WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy) {\n+        ProcessFn<InputT, OutputT, RestrictionT, ?> processFn =\n+            (ProcessFn) fn;\n+        return DoFnRunners.newProcessFnRunner(\n+            processFn,\n+            options,\n+            sideInputs,\n+            sideInputReader,\n+            outputManager,\n+            mainOutputTag,\n+            additionalOutputTags,\n+            stepContext,\n+            aggregatorChanges,\n+            windowingStrategy);\n+      }\n+    };\n+  }\n }",
                "changes": 108
            },
            {
                "status": "modified",
                "additions": 21,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "deletions": 10,
                "sha": "8793ae887f966f6b97b6f8557ba64b91d26c1a0d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "patch": "@@ -17,19 +17,22 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import static com.google.common.base.Preconditions.checkState;\n+\n import com.google.auto.value.AutoValue;\n import com.google.common.cache.CacheBuilder;\n import com.google.common.cache.CacheLoader;\n import com.google.common.cache.LoadingCache;\n-import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.Map.Entry;\n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.StateNamespaces;\n+import org.apache.beam.runners.core.StateNamespaces.WindowNamespace;\n import org.apache.beam.runners.core.StateTag;\n import org.apache.beam.runners.core.StateTags;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n@@ -50,7 +53,7 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n \n /** A {@link TransformEvaluatorFactory} for stateful {@link ParDo}. */\n@@ -62,7 +65,9 @@\n   private final ParDoEvaluatorFactory<KV<K, InputT>, OutputT> delegateFactory;\n \n   StatefulParDoEvaluatorFactory(EvaluationContext evaluationContext) {\n-    this.delegateFactory = new ParDoEvaluatorFactory<>(evaluationContext);\n+    this.delegateFactory =\n+        new ParDoEvaluatorFactory<>(\n+            evaluationContext, ParDoEvaluator.<KV<K, InputT>, OutputT>defaultRunnerFactory());\n     this.cleanupRegistry =\n         CacheBuilder.newBuilder()\n             .weakValues()\n@@ -117,7 +122,7 @@ public void cleanup() throws Exception {\n             doFn,\n             application.getTransform().getUnderlyingParDo().getSideInputs(),\n             application.getTransform().getUnderlyingParDo().getMainOutputTag(),\n-            application.getTransform().getUnderlyingParDo().getSideOutputTags().getAll());\n+            application.getTransform().getUnderlyingParDo().getAdditionalOutputTags().getAll());\n \n     return new StatefulParDoEvaluator<>(delegateEvaluator);\n   }\n@@ -137,8 +142,9 @@ public Runnable load(\n       String stepName = evaluationContext.getStepName(transformOutputWindow.getTransform());\n \n       Map<TupleTag<?>, PCollection<?>> taggedValues = new HashMap<>();\n-      for (TaggedPValue pv : transformOutputWindow.getTransform().getOutputs()) {\n-        taggedValues.put(pv.getTag(), (PCollection<?>) pv.getValue());\n+      for (Entry<TupleTag<?>, PValue> pv :\n+          transformOutputWindow.getTransform().getOutputs().entrySet()) {\n+        taggedValues.put(pv.getKey(), (PCollection<?>) pv.getValue());\n       }\n       PCollection<?> pc =\n           taggedValues\n@@ -228,15 +234,20 @@ public StatefulParDoEvaluator(\n     @Override\n     public void processElement(WindowedValue<KeyedWorkItem<K, KV<K, InputT>>> gbkResult)\n         throws Exception {\n-\n-      BoundedWindow window = Iterables.getOnlyElement(gbkResult.getWindows());\n-\n       for (WindowedValue<KV<K, InputT>> windowedValue : gbkResult.getValue().elementsIterable()) {\n         delegateEvaluator.processElement(windowedValue);\n       }\n \n       for (TimerData timer : gbkResult.getValue().timersIterable()) {\n-        delegateEvaluator.onTimer(timer, window);\n+        checkState(\n+            timer.getNamespace() instanceof WindowNamespace,\n+            \"Expected Timer %s to be in a %s, but got %s\",\n+            timer,\n+            WindowNamespace.class.getSimpleName(),\n+            timer.getNamespace().getClass().getName());\n+        WindowNamespace<?> windowNamespace = (WindowNamespace) timer.getNamespace();\n+        BoundedWindow timerWindow = windowNamespace.getWindow();\n+        delegateEvaluator.onTimer(timer, timerWindow);\n       }\n     }\n ",
                "changes": 31
            },
            {
                "status": "modified",
                "additions": 8,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "deletions": 12,
                "sha": "cba754ee9438607c4ddc292908c571d1b1610f78",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.beam.runners.core.construction.ReplacementOutputs;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.testing.TestStream.ElementEvent;\n@@ -48,8 +47,8 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n \n@@ -108,7 +107,7 @@ public void processElement(WindowedValue<TestStreamIndex<T>> element) throws Exc\n       if (event.getType().equals(EventType.ELEMENT)) {\n         UncommittedBundle<T> bundle =\n             context.createBundle(\n-                (PCollection<T>) Iterables.getOnlyElement(application.getOutputs()).getValue());\n+                (PCollection<T>) Iterables.getOnlyElement(application.getOutputs().values()));\n         for (TimestampedValue<T> elem : ((ElementEvent<T>) event).getElements()) {\n           bundle.add(\n               WindowedValue.timestampedValueInGlobalWindow(elem.getValue(), elem.getTimestamp()));\n@@ -170,19 +169,16 @@ public Clock get() {\n     }\n \n     @Override\n-    public PTransform<PBegin, PCollection<T>> getReplacementTransform(\n-        TestStream<T> transform) {\n-      return new DirectTestStream<>(runner, transform);\n-    }\n-\n-    @Override\n-    public PBegin getInput(List<TaggedPValue> inputs, Pipeline p) {\n-      return p.begin();\n+    public PTransformReplacement<PBegin, PCollection<T>> getReplacementTransform(\n+        AppliedPTransform<PBegin, PCollection<T>, TestStream<T>> transform) {\n+      return PTransformReplacement.of(\n+          transform.getPipeline().begin(),\n+          new DirectTestStream<T>(runner, transform.getTransform()));\n     }\n \n     @Override\n     public Map<PValue, ReplacementOutput> mapOutputs(\n-        List<TaggedPValue> outputs, PCollection<T> newOutput) {\n+        Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n ",
                "changes": 20
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
                "deletions": 5,
                "sha": "c7bc46f5da39029d06de71796b21ae9d47908ebc",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.beam.runners.direct;\n \n import javax.annotation.Nullable;\n-import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n@@ -48,13 +47,14 @@\n    */\n   @Nullable\n   <InputT> TransformEvaluator<InputT> forApplication(\n-      AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle)\n+      AppliedPTransform<?, ?, ?> application, DirectRunner.CommittedBundle<?> inputBundle)\n       throws Exception;\n \n   /**\n-   * Cleans up any state maintained by this {@link TransformEvaluatorFactory}. Called after a {@link\n-   * Pipeline} is shut down. No more calls to {@link #forApplication(AppliedPTransform,\n-   * CommittedBundle)} will be made after a call to {@link #cleanup()}.\n+   * Cleans up any state maintained by this {@link TransformEvaluatorFactory}. Called after a\n+   * {@link Pipeline} is shut down. No more calls to\n+   * {@link #forApplication(AppliedPTransform, DirectRunner.CommittedBundle)} will be made after\n+   * a call to {@link #cleanup()}.\n    */\n   void cleanup() throws Exception;\n }",
                "changes": 10
            },
            {
                "status": "modified",
                "additions": 5,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
                "deletions": 2,
                "sha": "d06c4601c2808f6c93fea690978633db4c7d4c67",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupByKeyOnly;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.ParDoMultiOverrideFactory.StatefulParDo;\n+import org.apache.beam.runners.direct.ViewOverrideFactory.WriteView;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Flatten.PCollections;\n@@ -51,10 +52,12 @@ public static TransformEvaluatorRegistry defaultRegistry(EvaluationContext ctxt)\n         ImmutableMap.<Class<? extends PTransform>, TransformEvaluatorFactory>builder()\n             .put(Read.Bounded.class, new BoundedReadEvaluatorFactory(ctxt))\n             .put(Read.Unbounded.class, new UnboundedReadEvaluatorFactory(ctxt))\n-            .put(ParDo.BoundMulti.class, new ParDoEvaluatorFactory<>(ctxt))\n+            .put(\n+                ParDo.MultiOutput.class,\n+                new ParDoEvaluatorFactory<>(ctxt, ParDoEvaluator.defaultRunnerFactory()))\n             .put(StatefulParDo.class, new StatefulParDoEvaluatorFactory<>(ctxt))\n             .put(PCollections.class, new FlattenEvaluatorFactory(ctxt))\n-            .put(ViewEvaluatorFactory.WriteView.class, new ViewEvaluatorFactory(ctxt))\n+            .put(WriteView.class, new ViewEvaluatorFactory(ctxt))\n             .put(Window.Assign.class, new WindowEvaluatorFactory(ctxt))\n             // Runner-specific primitives used in expansion of GroupByKey\n             .put(DirectGroupByKeyOnly.class, new GroupByKeyOnlyEvaluatorFactory(ctxt))",
                "changes": 7
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
                "deletions": 0,
                "sha": "c6f770f8012e1f2f946a4e1994a950656d242075",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
                "patch": "@@ -32,4 +32,10 @@\n    * {@link TransformExecutor TransformExecutors} to be evaluated.\n    */\n   void complete(TransformExecutor<?> completed);\n+\n+  /**\n+   * Cancel any outstanding work, if possible. Any future calls to schedule should ignore any\n+   * work.\n+   */\n+  void shutdown();\n }",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 48,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
                "deletions": 9,
                "sha": "53087bfa32b72020544eaa7f623abc4a384e3a73",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
                "patch": "@@ -21,7 +21,11 @@\n import java.util.Queue;\n import java.util.concurrent.ConcurrentLinkedQueue;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicReference;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * Static factory methods for constructing instances of {@link TransformExecutorService}.\n@@ -36,15 +40,15 @@ private TransformExecutorServices() {\n    * parallel.\n    */\n   public static TransformExecutorService parallel(ExecutorService executor) {\n-    return new ParallelEvaluationState(executor);\n+    return new ParallelTransformExecutor(executor);\n   }\n \n   /**\n    * Returns an EvaluationState that evaluates {@link TransformExecutor TransformExecutors} in\n    * serial.\n    */\n   public static TransformExecutorService serial(ExecutorService executor) {\n-    return new SerialEvaluationState(executor);\n+    return new SerialTransformExecutor(executor);\n   }\n \n   /**\n@@ -54,21 +58,47 @@ public static TransformExecutorService serial(ExecutorService executor) {\n    * <p>A principal use of this is for the evaluation of an unkeyed Step. Unkeyed computations are\n    * processed in parallel.\n    */\n-  private static class ParallelEvaluationState implements TransformExecutorService {\n+  private static class ParallelTransformExecutor implements TransformExecutorService {\n+    private static final Logger LOG = LoggerFactory.getLogger(ParallelTransformExecutor.class);\n+\n     private final ExecutorService executor;\n+    private final AtomicBoolean active = new AtomicBoolean(true);\n \n-    private ParallelEvaluationState(ExecutorService executor) {\n+    private ParallelTransformExecutor(ExecutorService executor) {\n       this.executor = executor;\n     }\n \n     @Override\n     public void schedule(TransformExecutor<?> work) {\n-      executor.submit(work);\n+      if (active.get()) {\n+        try {\n+          executor.submit(work);\n+        } catch (RejectedExecutionException rejected) {\n+          boolean stillActive = active.get();\n+          if (stillActive) {\n+            throw new IllegalStateException(\n+                String.format(\n+                    \"Execution of Work %s was rejected, but the %s is still active\",\n+                    work, ParallelTransformExecutor.class.getSimpleName()));\n+          } else {\n+            LOG.debug(\n+                \"Rejected execution of Work {} on executor {}. \"\n+                    + \"Suppressed exception because evaluator is not active\",\n+                work,\n+                this);\n+          }\n+        }\n+      }\n     }\n \n     @Override\n     public void complete(TransformExecutor<?> completed) {\n     }\n+\n+    @Override\n+    public void shutdown() {\n+      active.set(false);\n+    }\n   }\n \n   /**\n@@ -79,13 +109,14 @@ public void complete(TransformExecutor<?> completed) {\n    * <p>A principal use of this is for the serial evaluation of a (Step, Key) pair.\n    * Keyed computations are processed serially per step.\n    */\n-  private static class SerialEvaluationState implements TransformExecutorService {\n+  private static class SerialTransformExecutor implements TransformExecutorService {\n     private final ExecutorService executor;\n \n     private AtomicReference<TransformExecutor<?>> currentlyEvaluating;\n     private final Queue<TransformExecutor<?>> workQueue;\n+    private boolean active = true;\n \n-    private SerialEvaluationState(ExecutorService executor) {\n+    private SerialTransformExecutor(ExecutorService executor) {\n       this.executor = executor;\n       this.currentlyEvaluating = new AtomicReference<>();\n       this.workQueue = new ConcurrentLinkedQueue<>();\n@@ -113,12 +144,20 @@ public void complete(TransformExecutor<?> completed) {\n       updateCurrentlyEvaluating();\n     }\n \n+    @Override\n+    public void shutdown() {\n+      synchronized (this) {\n+        active = false;\n+      }\n+      workQueue.clear();\n+    }\n+\n     private void updateCurrentlyEvaluating() {\n       if (currentlyEvaluating.get() == null) {\n         // Only synchronize if we need to update what's currently evaluating\n         synchronized (this) {\n           TransformExecutor<?> newWork = workQueue.poll();\n-          if (newWork != null) {\n+          if (active && newWork != null) {\n             if (currentlyEvaluating.compareAndSet(null, newWork)) {\n               executor.submit(newWork);\n             } else {\n@@ -131,7 +170,7 @@ private void updateCurrentlyEvaluating() {\n \n     @Override\n     public String toString() {\n-      return MoreObjects.toStringHelper(SerialEvaluationState.class)\n+      return MoreObjects.toStringHelper(SerialTransformExecutor.class)\n           .add(\"currentlyEvaluating\", currentlyEvaluating)\n           .add(\"workQueue\", workQueue)\n           .toString();",
                "changes": 57
            },
            {
                "status": "modified",
                "additions": 25,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "deletions": 14,
                "sha": "d3609f89dc4509a4387298c65c571f90349af2f7",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.CoderUtils;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n@@ -120,7 +121,7 @@ public void processElement(\n         WindowedValue<UnboundedSourceShard<OutputT, CheckpointMarkT>> element) throws IOException {\n       UncommittedBundle<OutputT> output =\n           evaluationContext.createBundle(\n-              (PCollection<OutputT>) getOnlyElement(transform.getOutputs()).getValue());\n+              (PCollection<OutputT>) getOnlyElement(transform.getOutputs().values()));\n       UnboundedSourceShard<OutputT, CheckpointMarkT> shard = element.getValue();\n       UnboundedReader<OutputT> reader = null;\n       try {\n@@ -139,7 +140,24 @@ public void processElement(\n             numElements++;\n           } while (numElements < ARBITRARY_MAX_ELEMENTS && reader.advance());\n           Instant watermark = reader.getWatermark();\n-          UnboundedSourceShard<OutputT, CheckpointMarkT> residual = finishRead(reader, shard);\n+\n+          CheckpointMarkT finishedCheckpoint = finishRead(reader, shard);\n+          UnboundedSourceShard<OutputT, CheckpointMarkT> residual;\n+          // Sometimes resume from a checkpoint even if it's not required\n+          if (ThreadLocalRandom.current().nextDouble(1.0) >= readerReuseChance) {\n+            UnboundedReader<OutputT> toClose = reader;\n+            // Prevent double-close. UnboundedReader is AutoCloseable, which does not require\n+            // idempotency of close. Nulling out the reader here prevents trying to re-close it\n+            // if the call to close throws an IOException.\n+            reader = null;\n+            toClose.close();\n+            residual =\n+                UnboundedSourceShard.of(\n+                    shard.getSource(), shard.getDeduplicator(), null, finishedCheckpoint);\n+          } else {\n+            residual = shard.withCheckpoint(finishedCheckpoint);\n+          }\n+\n           resultBuilder\n               .addOutput(output)\n               .addUnprocessedElements(\n@@ -171,7 +189,7 @@ public void processElement(\n       if (existing == null) {\n         CheckpointMarkT checkpoint = shard.getCheckpoint();\n         if (checkpoint != null) {\n-          checkpoint.finalizeCheckpoint();\n+          checkpoint = CoderUtils.clone(shard.getSource().getCheckpointMarkCoder(), checkpoint);\n         }\n         return shard\n             .getSource()\n@@ -195,7 +213,7 @@ private boolean startReader(\n      * Checkpoint the current reader, finalize the previous checkpoint, and return the residual\n      * {@link UnboundedSourceShard}.\n      */\n-    private UnboundedSourceShard<OutputT, CheckpointMarkT> finishRead(\n+    private CheckpointMarkT finishRead(\n         UnboundedReader<OutputT> reader, UnboundedSourceShard<OutputT, CheckpointMarkT> shard)\n         throws IOException {\n       final CheckpointMark oldMark = shard.getCheckpoint();\n@@ -209,7 +227,7 @@ private boolean startReader(\n       // committing the output.\n       if (!reader.getWatermark().isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {\n         PCollection<OutputT> outputPc =\n-            (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs()).getValue();\n+            (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs().values());\n         evaluationContext.scheduleAfterOutputWouldBeProduced(\n             outputPc,\n             GlobalWindow.INSTANCE,\n@@ -226,14 +244,7 @@ public void run() {\n               }\n             });\n       }\n-\n-      // Sometimes resume from a checkpoint even if it's not required\n-      if (ThreadLocalRandom.current().nextDouble(1.0) >= readerReuseChance) {\n-        reader.close();\n-        return UnboundedSourceShard.of(shard.getSource(), shard.getDeduplicator(), null, mark);\n-      } else {\n-        return shard.withCheckpoint(mark);\n-      }\n+      return mark;\n     }\n \n     @Override\n@@ -290,7 +301,7 @@ public void run() {\n         throws Exception {\n       UnboundedSource<OutputT, ?> source = transform.getTransform().getSource();\n       List<? extends UnboundedSource<OutputT, ?>> splits =\n-          source.generateInitialSplits(targetParallelism, evaluationContext.getPipelineOptions());\n+          source.split(targetParallelism, evaluationContext.getPipelineOptions());\n       UnboundedReadDeduplicator deduplicator =\n           source.requiresDeduping()\n               ? UnboundedReadDeduplicator.CachedIdDeduplicator.create()",
                "changes": 39
            },
            {
                "status": "modified",
                "additions": 9,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "deletions": 73,
                "sha": "8cbe8fc4e08c5c7a133f8029306ac1830344914e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "patch": "@@ -20,31 +20,25 @@\n import com.google.common.collect.Iterables;\n import java.util.ArrayList;\n import java.util.List;\n-import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n import org.apache.beam.runners.direct.CommittedResult.OutputType;\n import org.apache.beam.runners.direct.DirectRunner.PCollectionViewWriter;\n import org.apache.beam.runners.direct.StepTransformResult.Builder;\n-import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.runners.direct.ViewOverrideFactory.WriteView;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n-import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.Values;\n import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n-import org.apache.beam.sdk.transforms.WithKeys;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n \n /**\n- * The {@link DirectRunner} {@link TransformEvaluatorFactory} for the\n- * {@link CreatePCollectionView} primitive {@link PTransform}.\n+ * The {@link DirectRunner} {@link TransformEvaluatorFactory} for the {@link CreatePCollectionView}\n+ * primitive {@link PTransform}.\n  *\n  * <p>The {@link ViewEvaluatorFactory} produces {@link TransformEvaluator TransformEvaluators} for\n- * the {@link WriteView} {@link PTransform}, which is part of the\n- * {@link DirectCreatePCollectionView} composite transform. This transform is an override for the\n- * {@link CreatePCollectionView} transform that applies windowing and triggers before the view is\n- * written.\n+ * the {@link WriteView} {@link PTransform}, which is part of the {@link DirectRunner} override.\n+ * This transform is an override for the {@link CreatePCollectionView} transform that applies\n+ * windowing and triggers before the view is written.\n  */\n class ViewEvaluatorFactory implements TransformEvaluatorFactory {\n   private final EvaluationContext context;\n@@ -70,9 +64,9 @@ public void cleanup() throws Exception {}\n       final AppliedPTransform<PCollection<Iterable<InT>>, PCollectionView<OuT>, WriteView<InT, OuT>>\n           application) {\n     PCollection<Iterable<InT>> input =\n-        (PCollection<Iterable<InT>>) Iterables.getOnlyElement(application.getInputs()).getValue();\n+        (PCollection<Iterable<InT>>) Iterables.getOnlyElement(application.getInputs().values());\n     final PCollectionViewWriter<InT, OuT> writer = context.createPCollectionViewWriter(input,\n-        (PCollectionView<OuT>) Iterables.getOnlyElement(application.getOutputs()).getValue());\n+        (PCollectionView<OuT>) Iterables.getOnlyElement(application.getOutputs().values()));\n     return new TransformEvaluator<Iterable<InT>>() {\n       private final List<WindowedValue<InT>> elements = new ArrayList<>();\n \n@@ -90,67 +84,9 @@ public void processElement(WindowedValue<Iterable<InT>> element) {\n         if (!elements.isEmpty()) {\n           resultBuilder = resultBuilder.withAdditionalOutput(OutputType.PCOLLECTION_VIEW);\n         }\n-        return resultBuilder\n-            .build();\n+        return resultBuilder.build();\n       }\n     };\n   }\n \n-  public static class ViewOverrideFactory<ElemT, ViewT>\n-      extends SingleInputOutputOverrideFactory<\n-                PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>> {\n-    @Override\n-    public PTransform<PCollection<ElemT>, PCollectionView<ViewT>> getReplacementTransform(\n-        CreatePCollectionView<ElemT, ViewT> transform) {\n-      return new DirectCreatePCollectionView<>(transform);\n-    }\n-  }\n-\n-  /**\n-   * An in-process override for {@link CreatePCollectionView}.\n-   */\n-  private static class DirectCreatePCollectionView<ElemT, ViewT>\n-      extends ForwardingPTransform<PCollection<ElemT>, PCollectionView<ViewT>> {\n-    private final CreatePCollectionView<ElemT, ViewT> og;\n-\n-    private DirectCreatePCollectionView(CreatePCollectionView<ElemT, ViewT> og) {\n-      this.og = og;\n-    }\n-\n-    @Override\n-    public PCollectionView<ViewT> expand(PCollection<ElemT> input) {\n-      return input.apply(WithKeys.<Void, ElemT>of((Void) null))\n-          .setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()))\n-          .apply(GroupByKey.<Void, ElemT>create())\n-          .apply(Values.<Iterable<ElemT>>create())\n-          .apply(new WriteView<ElemT, ViewT>(og));\n-    }\n-\n-    @Override\n-    protected PTransform<PCollection<ElemT>, PCollectionView<ViewT>> delegate() {\n-      return og;\n-    }\n-  }\n-\n-  /**\n-   * An in-process implementation of the {@link CreatePCollectionView} primitive.\n-   *\n-   * <p>This implementation requires the input {@link PCollection} to be an iterable\n-   * of {@code WindowedValue<ElemT>}, which is provided\n-   * to {@link PCollectionView#getViewFn()} for conversion to {@link ViewT}.\n-   */\n-  public static final class WriteView<ElemT, ViewT>\n-      extends PTransform<PCollection<Iterable<ElemT>>, PCollectionView<ViewT>> {\n-    private final CreatePCollectionView<ElemT, ViewT> og;\n-\n-    WriteView(CreatePCollectionView<ElemT, ViewT> og) {\n-      this.og = og;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"deprecation\")\n-    public PCollectionView<ViewT> expand(PCollection<Iterable<ElemT>> input) {\n-      return og.getView();\n-    }\n-  }\n }",
                "changes": 82
            },
            {
                "status": "added",
                "additions": 114,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
                "deletions": 0,
                "sha": "d4fd18fa1414cda6555c9fdbe4f084e9fd5952ae",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
                "patch": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.direct;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+import org.apache.beam.runners.core.construction.ForwardingPTransform;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.WithKeys;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ * A {@link PTransformOverrideFactory} that provides overrides for the {@link CreatePCollectionView}\n+ * {@link PTransform}.\n+ */\n+class ViewOverrideFactory<ElemT, ViewT>\n+    implements PTransformOverrideFactory<\n+        PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>> {\n+\n+  @Override\n+  public PTransformReplacement<PCollection<ElemT>, PCollectionView<ViewT>> getReplacementTransform(\n+      AppliedPTransform<\n+              PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>>\n+          transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        new GroupAndWriteView<>(transform.getTransform()));\n+  }\n+\n+  @Override\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, PCollectionView<ViewT> newOutput) {\n+    return Collections.emptyMap();\n+  }\n+\n+  /** The {@link DirectRunner} composite override for {@link CreatePCollectionView}. */\n+  static class GroupAndWriteView<ElemT, ViewT>\n+      extends ForwardingPTransform<PCollection<ElemT>, PCollectionView<ViewT>> {\n+    private final CreatePCollectionView<ElemT, ViewT> og;\n+\n+    private GroupAndWriteView(CreatePCollectionView<ElemT, ViewT> og) {\n+      this.og = og;\n+    }\n+\n+    @Override\n+    public PCollectionView<ViewT> expand(PCollection<ElemT> input) {\n+      return input\n+          .apply(WithKeys.<Void, ElemT>of((Void) null))\n+          .setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()))\n+          .apply(GroupByKey.<Void, ElemT>create())\n+          .apply(Values.<Iterable<ElemT>>create())\n+          .apply(new WriteView<ElemT, ViewT>(og));\n+    }\n+\n+    @Override\n+    protected PTransform<PCollection<ElemT>, PCollectionView<ViewT>> delegate() {\n+      return og;\n+    }\n+  }\n+\n+  /**\n+   * The {@link DirectRunner} implementation of the {@link CreatePCollectionView} primitive.\n+   *\n+   * <p>This implementation requires the input {@link PCollection} to be an iterable of {@code\n+   * WindowedValue<ElemT>}, which is provided to {@link PCollectionView#getViewFn()} for conversion\n+   * to {@link ViewT}.\n+   */\n+  static final class WriteView<ElemT, ViewT>\n+      extends PTransform<PCollection<Iterable<ElemT>>, PCollectionView<ViewT>> {\n+    private final CreatePCollectionView<ElemT, ViewT> og;\n+\n+    WriteView(CreatePCollectionView<ElemT, ViewT> og) {\n+      this.og = og;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"deprecation\")\n+    public PCollectionView<ViewT> expand(PCollection<Iterable<ElemT>> input) {\n+      return og.getView();\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    public PCollectionView<ViewT> getView() {\n+      return og.getView();\n+    }\n+  }\n+}",
                "changes": 114
            },
            {
                "status": "modified",
                "additions": 10,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
                "deletions": 9,
                "sha": "8c043629b08587f72b4ba2e074bcd058e64cdfad",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
                "patch": "@@ -61,7 +61,8 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Instant;\n \n /**\n@@ -818,13 +819,13 @@ private TransformWatermarks getTransformWatermark(AppliedPTransform<?, ?, ?> tra\n \n   private Collection<Watermark> getInputProcessingWatermarks(AppliedPTransform<?, ?, ?> transform) {\n     ImmutableList.Builder<Watermark> inputWmsBuilder = ImmutableList.builder();\n-    List<TaggedPValue> inputs = transform.getInputs();\n+    Map<TupleTag<?>, PValue> inputs = transform.getInputs();\n     if (inputs.isEmpty()) {\n       inputWmsBuilder.add(THE_END_OF_TIME);\n     }\n-    for (TaggedPValue pvalue : inputs) {\n+    for (PValue pvalue : inputs.values()) {\n       Watermark producerOutputWatermark =\n-          getTransformWatermark(graph.getProducer(pvalue.getValue()))\n+          getTransformWatermark(graph.getProducer(pvalue))\n               .synchronizedProcessingOutputWatermark;\n       inputWmsBuilder.add(producerOutputWatermark);\n     }\n@@ -833,13 +834,13 @@ private TransformWatermarks getTransformWatermark(AppliedPTransform<?, ?, ?> tra\n \n   private List<Watermark> getInputWatermarks(AppliedPTransform<?, ?, ?> transform) {\n     ImmutableList.Builder<Watermark> inputWatermarksBuilder = ImmutableList.builder();\n-    List<TaggedPValue> inputs = transform.getInputs();\n+    Map<TupleTag<?>, PValue> inputs = transform.getInputs();\n     if (inputs.isEmpty()) {\n       inputWatermarksBuilder.add(THE_END_OF_TIME);\n     }\n-    for (TaggedPValue pvalue : inputs) {\n+    for (PValue pvalue : inputs.values()) {\n       Watermark producerOutputWatermark =\n-          getTransformWatermark(graph.getProducer(pvalue.getValue())).outputWatermark;\n+          getTransformWatermark(graph.getProducer(pvalue)).outputWatermark;\n       inputWatermarksBuilder.add(producerOutputWatermark);\n     }\n     List<Watermark> inputCollectionWatermarks = inputWatermarksBuilder.build();\n@@ -1023,8 +1024,8 @@ synchronized void refreshAll() {\n     WatermarkUpdate updateResult = myWatermarks.refresh();\n     if (updateResult.isAdvanced()) {\n       Set<AppliedPTransform<?, ?, ?>> additionalRefreshes = new HashSet<>();\n-      for (TaggedPValue outputPValue : toRefresh.getOutputs()) {\n-        additionalRefreshes.addAll(graph.getPrimitiveConsumers(outputPValue.getValue()));\n+      for (PValue outputPValue : toRefresh.getOutputs().values()) {\n+        additionalRefreshes.addAll(graph.getPrimitiveConsumers(outputPValue));\n       }\n       return additionalRefreshes;\n     }",
                "changes": 19
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
                "deletions": 1,
                "sha": "25509249a609a4ed8bcc77802caec425d7da2171",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
                "patch": "@@ -57,7 +57,7 @@\n     WindowFn<? super InputT, ?> fn = transform.getTransform().getWindowFn();\n     UncommittedBundle<InputT> outputBundle =\n         evaluationContext.createBundle(\n-            (PCollection<InputT>) Iterables.getOnlyElement(transform.getOutputs()).getValue());\n+            (PCollection<InputT>) Iterables.getOnlyElement(transform.getOutputs().values()));\n     if (fn == null) {\n       return PassthroughTransformEvaluator.create(transform, outputBundle);\n     }",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 13,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "deletions": 13,
                "sha": "a23ab94f54d742fd71c6a609e655b657ec8d4612",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "patch": "@@ -21,25 +21,26 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Supplier;\n import com.google.common.base.Suppliers;\n-import com.google.common.collect.Iterables;\n import java.io.Serializable;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ThreadLocalRandom;\n-import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.sdk.io.Write;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Count;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PDone;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} that overrides {@link Write} {@link PTransform PTransforms}\n@@ -52,19 +53,17 @@\n   @VisibleForTesting static final int MIN_SHARDS_FOR_LOG = 3;\n \n   @Override\n-  public PTransform<PCollection<InputT>, PDone> getReplacementTransform(\n-      Write<InputT> transform) {\n+  public PTransformReplacement<PCollection<InputT>, PDone> getReplacementTransform(\n+      AppliedPTransform<PCollection<InputT>, PDone, Write<InputT>> transform) {\n \n-      return transform.withSharding(new LogElementShardsWithDrift<InputT>());\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        transform.getTransform().withSharding(new LogElementShardsWithDrift<InputT>()));\n   }\n \n   @Override\n-  public PCollection<InputT> getInput(List<TaggedPValue> inputs, Pipeline p) {\n-    return (PCollection<InputT>) Iterables.getOnlyElement(inputs).getValue();\n-  }\n-\n-  @Override\n-  public Map<PValue, ReplacementOutput> mapOutputs(List<TaggedPValue> outputs, PDone newOutput) {\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, PDone newOutput) {\n     return Collections.emptyMap();\n   }\n \n@@ -74,6 +73,7 @@\n     @Override\n     public PCollectionView<Integer> expand(PCollection<T> records) {\n       return records\n+          .apply(Window.<T>into(new GlobalWindows()))\n           .apply(\"CountRecords\", Count.<T>globally())\n           .apply(\"GenerateShardCount\", ParDo.of(new CalculateShardsFn()))\n           .apply(View.<Integer>asSingleton());",
                "changes": 26
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "deletions": 2,
                "sha": "2b5b46d64eca628c72c3c9c84394aaff87a48ec2",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "patch": "@@ -265,7 +265,7 @@ public void getInitialInputsSplitsIntoBundles() throws Exception {\n   public void boundedSourceInMemoryTransformEvaluatorShardsOfSource() throws Exception {\n     PipelineOptions options = PipelineOptionsFactory.create();\n     List<? extends BoundedSource<Long>> splits =\n-        source.splitIntoBundles(source.getEstimatedSizeBytes(options) / 2, options);\n+        source.split(source.getEstimatedSizeBytes(options) / 2, options);\n \n     UncommittedBundle<BoundedSourceShard<Long>> rootBundle = bundleFactory.createRootBundle();\n     for (BoundedSource<Long> split : splits) {\n@@ -365,7 +365,7 @@ public TestSource(Coder<T> coder, int firstSplitIndex, T... elems) {\n     }\n \n     @Override\n-    public List<? extends OffsetBasedSource<T>> splitIntoBundles(\n+    public List<? extends OffsetBasedSource<T>> split(\n         long desiredBundleSizeBytes, PipelineOptions options) throws Exception {\n       return ImmutableList.of(this);\n     }",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 17,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "deletions": 17,
                "sha": "68c6613848796a19df0e267a85be7282df1a7ada",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "patch": "@@ -45,9 +45,9 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.ValueState;\n@@ -201,24 +201,24 @@ public void testMapStateWithUnderlying() {\n     StateTag<Object, MapState<String, Integer>> valueTag =\n         StateTags.map(\"foo\", StringUtf8Coder.of(), VarIntCoder.of());\n     MapState<String, Integer> underlyingValue = underlying.state(namespace, valueTag);\n-    assertThat(underlyingValue.iterate(), emptyIterable());\n+    assertThat(underlyingValue.entries().read(), emptyIterable());\n \n     underlyingValue.put(\"hello\", 1);\n-    assertThat(underlyingValue.get(\"hello\"), equalTo(1));\n+    assertThat(underlyingValue.get(\"hello\").read(), equalTo(1));\n \n     CopyOnAccessInMemoryStateInternals<String> internals =\n         CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);\n     MapState<String, Integer> copyOnAccessState = internals.state(namespace, valueTag);\n-    assertThat(copyOnAccessState.get(\"hello\"), equalTo(1));\n+    assertThat(copyOnAccessState.get(\"hello\").read(), equalTo(1));\n \n     copyOnAccessState.put(\"world\", 4);\n-    assertThat(copyOnAccessState.get(\"hello\"), equalTo(1));\n-    assertThat(copyOnAccessState.get(\"world\"), equalTo(4));\n-    assertThat(underlyingValue.get(\"hello\"), equalTo(1));\n-    assertNull(underlyingValue.get(\"world\"));\n+    assertThat(copyOnAccessState.get(\"hello\").read(), equalTo(1));\n+    assertThat(copyOnAccessState.get(\"world\").read(), equalTo(4));\n+    assertThat(underlyingValue.get(\"hello\").read(), equalTo(1));\n+    assertNull(underlyingValue.get(\"world\").read());\n \n     MapState<String, Integer> reReadUnderlyingValue = underlying.state(namespace, valueTag);\n-    assertThat(underlyingValue.iterate(), equalTo(reReadUnderlyingValue.iterate()));\n+    assertThat(underlyingValue.entries().read(), equalTo(reReadUnderlyingValue.entries().read()));\n   }\n \n   @Test\n@@ -229,25 +229,25 @@ public void testAccumulatorCombiningStateWithUnderlying() throws CannotProvideCo\n \n     StateNamespace namespace = new StateNamespaceForTest(\"foo\");\n     CoderRegistry reg = pipeline.getCoderRegistry();\n-    StateTag<Object, AccumulatorCombiningState<Long, long[], Long>> stateTag =\n+    StateTag<Object, CombiningState<Long, long[], Long>> stateTag =\n         StateTags.combiningValue(\"summer\",\n             sumLongFn.getAccumulatorCoder(reg, reg.getDefaultCoder(Long.class)), sumLongFn);\n-    CombiningState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(0L));\n \n     underlyingValue.add(1L);\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n     CopyOnAccessInMemoryStateInternals<String> internals =\n         CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);\n-    CombiningState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n+    GroupingState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n     assertThat(copyOnAccessState.read(), equalTo(1L));\n \n     copyOnAccessState.add(4L);\n     assertThat(copyOnAccessState.read(), equalTo(5L));\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n-    CombiningState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(reReadUnderlyingValue.read()));\n   }\n \n@@ -259,28 +259,28 @@ public void testKeyedAccumulatorCombiningStateWithUnderlying() throws Exception\n \n     StateNamespace namespace = new StateNamespaceForTest(\"foo\");\n     CoderRegistry reg = pipeline.getCoderRegistry();\n-    StateTag<String, AccumulatorCombiningState<Long, long[], Long>> stateTag =\n+    StateTag<String, CombiningState<Long, long[], Long>> stateTag =\n         StateTags.keyedCombiningValue(\n             \"summer\",\n             sumLongFn.getAccumulatorCoder(\n                 reg, StringUtf8Coder.of(), reg.getDefaultCoder(Long.class)),\n             sumLongFn);\n-    CombiningState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(0L));\n \n     underlyingValue.add(1L);\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n     CopyOnAccessInMemoryStateInternals<String> internals =\n         CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);\n-    CombiningState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n+    GroupingState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n     assertThat(copyOnAccessState.read(), equalTo(1L));\n \n     copyOnAccessState.add(4L);\n     assertThat(copyOnAccessState.read(), equalTo(5L));\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n-    CombiningState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(reReadUnderlyingValue.read()));\n   }\n ",
                "changes": 34
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "deletions": 4,
                "sha": "b44c89046a3ce1955a24edbc6eb3f3250e89e371",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "patch": "@@ -45,7 +45,6 @@\n import org.apache.beam.sdk.values.PDone;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.hamcrest.Matchers;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -101,9 +100,9 @@ public void getRootTransformsContainsRootTransforms() {\n             graph.getProducer(created), graph.getProducer(counted), graph.getProducer(unCounted)));\n     for (AppliedPTransform<?, ?, ?> root : graph.getRootTransforms())  {\n       // Root transforms will have no inputs\n-      assertThat(root.getInputs(), emptyIterable());\n+      assertThat(root.getInputs().entrySet(), emptyIterable());\n       assertThat(\n-          Iterables.getOnlyElement(root.getOutputs()).getValue(),\n+          Iterables.getOnlyElement(root.getOutputs().values()),\n           Matchers.<POutput>isOneOf(created, counted, unCounted));\n     }\n   }\n@@ -121,7 +120,7 @@ public void getRootTransformsContainsEmptyFlatten() {\n         Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(graph.getProducer(empty)));\n     AppliedPTransform<?, ?, ?> onlyRoot = Iterables.getOnlyElement(graph.getRootTransforms());\n     assertThat(onlyRoot.getTransform(), Matchers.<PTransform<?, ?>>equalTo(flatten));\n-    assertThat(onlyRoot.getInputs(), Matchers.<TaggedPValue>emptyIterable());\n+    assertThat(onlyRoot.getInputs().entrySet(), emptyIterable());\n     assertThat(onlyRoot.getOutputs(), equalTo(empty.expand()));\n   }\n ",
                "changes": 7
            },
            {
                "status": "modified",
                "additions": 10,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java",
                "deletions": 2,
                "sha": "28fef4c7e5d54b96890034b3dec9523b60b1f71c",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java",
                "patch": "@@ -23,8 +23,11 @@\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VarIntCoder;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.PTransformReplacement;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.hamcrest.Matchers;\n@@ -45,7 +48,12 @@ public void getInputSucceeds() {\n         p.apply(\n             Create.of(KV.of(\"foo\", 1))\n                 .withCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())));\n-    PCollection<?> reconstructed = factory.getInput(input.expand(), p);\n-    assertThat(reconstructed, Matchers.<PCollection<?>>equalTo(input));\n+    PCollection<KV<String, Iterable<Integer>>> grouped =\n+        input.apply(GroupByKey.<String, Integer>create());\n+    AppliedPTransform<?, ?, ?> producer = DirectGraphs.getProducer(grouped);\n+    PTransformReplacement<\n+            PCollection<KV<String, Integer>>, PCollection<KV<String, Iterable<Integer>>>>\n+        replacement = factory.getReplacementTransform((AppliedPTransform) producer);\n+    assertThat(replacement.getInput(), Matchers.<PCollection<?>>equalTo(input));\n   }\n }",
                "changes": 12
            },
            {
                "status": "modified",
                "additions": 56,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java",
                "deletions": 68,
                "sha": "ee51e9a904acfcb918e753a9213cb025e862d753",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java",
                "patch": "@@ -23,22 +23,21 @@\n import static org.apache.beam.sdk.metrics.MetricNameFilter.inNamespace;\n import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.assertTrue;\n \n import com.google.common.collect.ImmutableList;\n-import java.util.HashSet;\n-import java.util.Set;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.metrics.DistributionData;\n import org.apache.beam.sdk.metrics.DistributionResult;\n+import org.apache.beam.sdk.metrics.GaugeData;\n+import org.apache.beam.sdk.metrics.GaugeResult;\n import org.apache.beam.sdk.metrics.MetricKey;\n import org.apache.beam.sdk.metrics.MetricName;\n import org.apache.beam.sdk.metrics.MetricQueryResults;\n import org.apache.beam.sdk.metrics.MetricUpdates;\n import org.apache.beam.sdk.metrics.MetricUpdates.MetricUpdate;\n import org.apache.beam.sdk.metrics.MetricsFilter;\n+import org.joda.time.Instant;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -60,6 +59,7 @@\n   private static final MetricName NAME1 = MetricName.named(\"ns1\", \"name1\");\n   private static final MetricName NAME2 = MetricName.named(\"ns1\", \"name2\");\n   private static final MetricName NAME3 = MetricName.named(\"ns2\", \"name1\");\n+  private static final MetricName NAME4 = MetricName.named(\"ns2\", \"name2\");\n \n   private DirectMetrics metrics = new DirectMetrics();\n \n@@ -77,14 +77,20 @@ public void testApplyCommittedNoFilter() {\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME2), 8L)),\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME1),\n-                DistributionData.create(8, 2, 3, 5)))));\n+                DistributionData.create(8, 2, 3, 5))),\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"step1\", NAME4), GaugeData.create(15L)))\n+        ));\n     metrics.commitLogical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step2\", NAME1), 7L),\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME2), 4L)),\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME1),\n-                DistributionData.create(4, 1, 4, 4)))));\n+                DistributionData.create(4, 1, 4, 4))),\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"step1\", NAME4), GaugeData.create(27L)))\n+    ));\n \n     MetricQueryResults results = metrics.queryMetrics(MetricsFilter.builder().build());\n     assertThat(results.counters(), containsInAnyOrder(\n@@ -99,6 +105,12 @@ public void testApplyCommittedNoFilter() {\n         attemptedMetricsResult(\"ns1\", \"name1\", \"step1\", DistributionResult.ZERO)));\n     assertThat(results.distributions(), contains(\n         committedMetricsResult(\"ns1\", \"name1\", \"step1\", DistributionResult.create(12, 3, 3, 5))));\n+    assertThat(results.gauges(), contains(\n+        attemptedMetricsResult(\"ns2\", \"name2\", \"step1\", GaugeResult.empty())\n+    ));\n+    assertThat(results.gauges(), contains(\n+        committedMetricsResult(\"ns2\", \"name2\", \"step1\", GaugeResult.create(27L, Instant.now()))\n+    ));\n   }\n \n   @SuppressWarnings(\"unchecked\")\n@@ -108,12 +120,16 @@ public void testApplyAttemptedCountersQueryOneNamespace() {\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME1), 5L),\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME3), 8L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n     metrics.updatePhysical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step2\", NAME1), 7L),\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME3), 4L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n \n     MetricQueryResults results = metrics.queryMetrics(\n         MetricsFilter.builder().addNameFilter(inNamespace(\"ns1\")).build());\n@@ -129,54 +145,54 @@ public void testApplyAttemptedCountersQueryOneNamespace() {\n             committedMetricsResult(\"ns1\", \"name1\", \"step2\", 0L)));\n   }\n \n-  private boolean matchesSubPath(String actualScope, String subPath) {\n-    return metrics.subPathMatches(actualScope, subPath);\n-  }\n-\n+  @SuppressWarnings(\"unchecked\")\n   @Test\n-  public void testMatchesSubPath() {\n-    assertTrue(\"Match of the first element\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Top1\"));\n-    assertTrue(\"Match of the first elements\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1\"));\n-    assertTrue(\"Match of the last elements\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Inner1/Bottom1\"));\n-    assertFalse(\"Substring match but no subpath match\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"op1/Outer1/Inner1\"));\n-    assertFalse(\"Substring match from start - but no subpath match\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Top\"));\n-  }\n+  public void testApplyAttemptedQueryCompositeScope() {\n+    metrics.updatePhysical(bundle1, MetricUpdates.create(\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 5L),\n+            MetricUpdate.create(MetricKey.create(\"Outer1/Inner2\", NAME1), 8L)),\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()));\n+    metrics.updatePhysical(bundle1, MetricUpdates.create(\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 12L),\n+            MetricUpdate.create(MetricKey.create(\"Outer2/Inner2\", NAME1), 18L)),\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()));\n \n-  private boolean matchesScopeWithSingleFilter(String actualScope, String filter) {\n-    Set<String> scopeFilter = new HashSet<String>();\n-    scopeFilter.add(filter);\n-    return metrics.matchesScope(actualScope, scopeFilter);\n-  }\n+    MetricQueryResults results = metrics.queryMetrics(\n+        MetricsFilter.builder().addStep(\"Outer1\").build());\n \n-  @Test\n-  public void testMatchesScope() {\n-    assertTrue(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1\"));\n-    assertTrue(matchesScopeWithSingleFilter(\n-        \"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1/Inner1/Bottom1\"));\n-    assertTrue(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1\"));\n-    assertTrue(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1/Inner1\"));\n-    assertFalse(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Inner1\"));\n-    assertFalse(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1/Inn\"));\n+    assertThat(results.counters(),\n+        containsInAnyOrder(\n+            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 12L),\n+            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 8L)));\n+\n+    assertThat(results.counters(),\n+        containsInAnyOrder(\n+            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 0L),\n+            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 0L)));\n   }\n \n+\n   @SuppressWarnings(\"unchecked\")\n   @Test\n   public void testPartialScopeMatchingInMetricsQuery() {\n     metrics.updatePhysical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"Top1/Outer1/Inner1\", NAME1), 5L),\n             MetricUpdate.create(MetricKey.create(\"Top1/Outer1/Inner2\", NAME1), 8L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n     metrics.updatePhysical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"Top2/Outer1/Inner1\", NAME1), 12L),\n             MetricUpdate.create(MetricKey.create(\"Top1/Outer2/Inner2\", NAME1), 18L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n \n     MetricQueryResults results = metrics.queryMetrics(\n         MetricsFilter.builder().addStep(\"Top1/Outer1\").build());\n@@ -194,32 +210,4 @@ public void testPartialScopeMatchingInMetricsQuery() {\n             attemptedMetricsResult(\"ns1\", \"name1\", \"Top1/Outer1/Inner2\", 8L),\n             attemptedMetricsResult(\"ns1\", \"name1\", \"Top1/Outer2/Inner2\", 18L)));\n   }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  @Test\n-  public void testApplyAttemptedQueryCompositeScope() {\n-    metrics.updatePhysical(bundle1, MetricUpdates.create(\n-        ImmutableList.of(\n-            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 5L),\n-            MetricUpdate.create(MetricKey.create(\"Outer1/Inner2\", NAME1), 8L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n-    metrics.updatePhysical(bundle1, MetricUpdates.create(\n-        ImmutableList.of(\n-            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 12L),\n-            MetricUpdate.create(MetricKey.create(\"Outer2/Inner2\", NAME1), 18L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n-\n-    MetricQueryResults results = metrics.queryMetrics(\n-        MetricsFilter.builder().addStep(\"Outer1\").build());\n-\n-    assertThat(results.counters(),\n-        containsInAnyOrder(\n-            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 12L),\n-            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 8L)));\n-\n-    assertThat(results.counters(),\n-        containsInAnyOrder(\n-            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 0L),\n-            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 0L)));\n-  }\n }",
                "changes": 124
            },
            {
                "status": "modified",
                "additions": 85,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "deletions": 4,
                "sha": "ed19be2876945d4b4869aff1b69e4496ab13e4e7",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "patch": "@@ -32,9 +32,17 @@\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.ArrayBlockingQueue;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.beam.runners.direct.DirectRunner.DirectPipelineResult;\n import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.PipelineResult.State;\n import org.apache.beam.sdk.coders.AtomicCoder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n@@ -49,6 +57,7 @@\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.transforms.Count;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n@@ -65,6 +74,7 @@\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.TypeDescriptor;\n import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.internal.matchers.ThrowableMessageMatcher;\n@@ -182,9 +192,10 @@ public void byteArrayCountShouldSucceed() {\n     TypeDescriptor<byte[]> td = new TypeDescriptor<byte[]>() {\n     };\n     PCollection<byte[]> foos =\n-        p.apply(Create.of(1, 1, 1, 2, 2, 3)).apply(MapElements.via(getBytes).withOutputType(td));\n+        p.apply(Create.of(1, 1, 1, 2, 2, 3))\n+            .apply(MapElements.into(td).via(getBytes));\n     PCollection<byte[]> msync =\n-        p.apply(Create.of(1, -2, -8, -16)).apply(MapElements.via(getBytes).withOutputType(td));\n+        p.apply(Create.of(1, -2, -8, -16)).apply(MapElements.into(td).via(getBytes));\n     PCollection<byte[]> bytes =\n         PCollectionList.of(foos).and(msync).apply(Flatten.<byte[]>pCollections());\n     PCollection<KV<byte[], Long>> counts = bytes.apply(Count.<byte[]>perElement());\n@@ -221,6 +232,76 @@ public void splitsInputs() {\n     p.run();\n   }\n \n+  @Test\n+  public void cancelShouldStopPipeline() throws Exception {\n+    PipelineOptions opts = TestPipeline.testingPipelineOptions();\n+    opts.as(DirectOptions.class).setBlockOnRun(false);\n+    opts.setRunner(DirectRunner.class);\n+\n+    final Pipeline p = Pipeline.create(opts);\n+    p.apply(CountingInput.unbounded().withRate(1L, Duration.standardSeconds(1)));\n+\n+    final BlockingQueue<PipelineResult> resultExchange = new ArrayBlockingQueue<>(1);\n+    Runnable cancelRunnable = new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          resultExchange.take().cancel();\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+          throw new IllegalStateException(e);\n+        } catch (IOException e) {\n+          throw new IllegalStateException(e);\n+        }\n+      }\n+    };\n+\n+    Callable<PipelineResult> runPipelineRunnable = new Callable<PipelineResult>() {\n+      @Override\n+      public PipelineResult call() {\n+        PipelineResult res = p.run();\n+        try {\n+          resultExchange.put(res);\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+          throw new IllegalStateException(e);\n+        }\n+        return res;\n+      }\n+    };\n+\n+    ExecutorService executor = Executors.newCachedThreadPool();\n+    executor.submit(cancelRunnable);\n+    Future<PipelineResult> result = executor.submit(runPipelineRunnable);\n+\n+    // If cancel doesn't work, this will hang forever\n+    result.get().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void testWaitUntilFinishTimeout() throws Exception {\n+    DirectOptions options = PipelineOptionsFactory.as(DirectOptions.class);\n+    options.setBlockOnRun(false);\n+    options.setRunner(DirectRunner.class);\n+    Pipeline p = Pipeline.create(options);\n+    p\n+      .apply(Create.of(1L))\n+      .apply(ParDo.of(\n+          new DoFn<Long, Long>() {\n+            @ProcessElement\n+            public void hang(ProcessContext context) throws InterruptedException {\n+              // Hangs \"forever\"\n+              Thread.sleep(Long.MAX_VALUE);\n+            }\n+          }));\n+    PipelineResult result = p.run();\n+    // The pipeline should never complete;\n+    assertThat(result.getState(), is(State.RUNNING));\n+    // Must time out, otherwise this test will never complete\n+    result.waitUntilFinish(Duration.millis(1L));\n+    assertThat(result.getState(), is(State.RUNNING));\n+  }\n+\n   @Test\n   public void transformDisplayDataExceptionShouldFail() {\n     DoFn<Integer, Integer> brokenDoFn = new DoFn<Integer, Integer>() {\n@@ -468,13 +549,13 @@ public MustSplitSource(BoundedSource<T> underlying) {\n     }\n \n     @Override\n-    public List<? extends BoundedSource<T>> splitIntoBundles(\n+    public List<? extends BoundedSource<T>> split(\n         long desiredBundleSizeBytes, PipelineOptions options) throws Exception {\n       // Must have more than\n       checkState(\n           desiredBundleSizeBytes < getEstimatedSizeBytes(options),\n           \"Must split into more than one source\");\n-      return underlying.splitIntoBundles(desiredBundleSizeBytes, options);\n+      return underlying.split(desiredBundleSizeBytes, options);\n     }\n \n     @Override",
                "changes": 89
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
                "deletions": 4,
                "sha": "1ac4d6d2773a6f87f3376a1597f67743a2ab6039",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
                "patch": "@@ -53,7 +53,7 @@ public void setup() {\n \n   @Test\n   public void delegatesToUnderlying() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     DoFn<?, ?> original = lifecycleManager.get();\n     TransformEvaluator<Object> evaluator =\n         DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager);\n@@ -72,7 +72,7 @@ public void delegatesToUnderlying() throws Exception {\n \n   @Test\n   public void removesOnExceptionInProcessElement() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     doThrow(Exception.class).when(underlying).processElement(any(WindowedValue.class));\n \n     DoFn<?, ?> original = lifecycleManager.get();\n@@ -91,7 +91,7 @@ public void removesOnExceptionInProcessElement() throws Exception {\n \n   @Test\n   public void removesOnExceptionInOnTimer() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     doThrow(Exception.class)\n         .when(underlying)\n         .onTimer(any(TimerData.class), any(BoundedWindow.class));\n@@ -114,7 +114,7 @@ public void removesOnExceptionInOnTimer() throws Exception {\n \n   @Test\n   public void removesOnExceptionInFinishBundle() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     doThrow(Exception.class).when(underlying).finishBundle();\n \n     DoFn<?, ?> original = lifecycleManager.get();",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "deletions": 56,
                "sha": "7a65493467e36e8945d77c12f9fe194f78b3a3bd",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "patch": "@@ -414,8 +414,7 @@ public void createKeyedBundleKeyed() {\n   }\n \n   @Test\n-  public void isDoneWithUnboundedPCollectionAndShutdown() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(true);\n+  public void isDoneWithUnboundedPCollection() {\n     assertThat(context.isDone(unboundedProducer), is(false));\n \n     context.handleResult(\n@@ -426,34 +425,8 @@ public void isDoneWithUnboundedPCollectionAndShutdown() {\n     assertThat(context.isDone(unboundedProducer), is(true));\n   }\n \n-  @Test\n-  public void isDoneWithUnboundedPCollectionAndNotShutdown() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(graph.getProducer(unbounded)), is(false));\n-\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(graph.getProducer(unbounded)).build());\n-    assertThat(context.isDone(graph.getProducer(unbounded)), is(false));\n-  }\n-\n-  @Test\n-  public void isDoneWithOnlyBoundedPCollections() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(createdProducer), is(false));\n-\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(createdProducer).build());\n-    context.extractFiredTimers();\n-    assertThat(context.isDone(createdProducer), is(true));\n-  }\n-\n   @Test\n   public void isDoneWithPartiallyDone() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(true);\n     assertThat(context.isDone(), is(false));\n \n     UncommittedBundle<Integer> rootBundle = context.createBundle(created);\n@@ -484,34 +457,6 @@ public void isDoneWithPartiallyDone() {\n     assertThat(context.isDone(), is(true));\n   }\n \n-  @Test\n-  public void isDoneWithUnboundedAndNotShutdown() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(), is(false));\n-\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(createdProducer).build());\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(unboundedProducer).build());\n-    context.handleResult(\n-        context.createBundle(created).commit(Instant.now()),\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(downstreamProducer).build());\n-    context.extractFiredTimers();\n-    assertThat(context.isDone(), is(false));\n-\n-    context.handleResult(\n-        context.createBundle(created).commit(Instant.now()),\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(viewProducer).build());\n-    context.extractFiredTimers();\n-    assertThat(context.isDone(), is(false));\n-  }\n-\n   private static class TestBoundedWindow extends BoundedWindow {\n     private final Instant ts;\n ",
                "changes": 57
            },
            {
                "status": "modified",
                "additions": 8,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "deletions": 7,
                "sha": "e99e4bff329bd96a919def05ac491e44e8e4c427",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "patch": "@@ -70,7 +70,7 @@\n   @Mock private EvaluationContext evaluationContext;\n   private PCollection<Integer> inputPc;\n   private TupleTag<Integer> mainOutputTag;\n-  private List<TupleTag<?>> sideOutputTags;\n+  private List<TupleTag<?>> additionalOutputTags;\n   private BundleFactory bundleFactory;\n \n   @Rule\n@@ -81,7 +81,7 @@ public void setup() {\n     MockitoAnnotations.initMocks(this);\n     inputPc = p.apply(Create.of(1, 2, 3));\n     mainOutputTag = new TupleTag<Integer>() {};\n-    sideOutputTags = TupleTagList.empty().getAll();\n+    additionalOutputTags = TupleTagList.empty().getAll();\n \n     bundleFactory = ImmutableListBundleFactory.create();\n   }\n@@ -98,7 +98,7 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n     UncommittedBundle<Integer> outputBundle = bundleFactory.createBundle(output);\n     when(evaluationContext.createBundle(output)).thenReturn(outputBundle);\n \n-    ParDoEvaluator<Integer, Integer> evaluator =\n+    ParDoEvaluator<Integer> evaluator =\n         createEvaluator(singletonView, fn, output);\n \n     IntervalWindow nonGlobalWindow = new IntervalWindow(new Instant(0), new Instant(10_000L));\n@@ -130,7 +130,7 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n             WindowedValue.timestampedValueInGlobalWindow(6, new Instant(2468L))));\n   }\n \n-  private ParDoEvaluator<Integer, Integer> createEvaluator(\n+  private ParDoEvaluator<Integer> createEvaluator(\n       PCollectionView<Integer> singletonView,\n       RecorderFn fn,\n       PCollection<Integer> output) {\n@@ -162,14 +162,15 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n         evaluationContext,\n         stepContext,\n         transform,\n-        ((PCollection<?>) Iterables.getOnlyElement(transform.getInputs()).getValue())\n+        ((PCollection<?>) Iterables.getOnlyElement(transform.getInputs().values()))\n             .getWindowingStrategy(),\n         fn,\n         null /* key */,\n         ImmutableList.<PCollectionView<?>>of(singletonView),\n         mainOutputTag,\n-        sideOutputTags,\n-        ImmutableMap.<TupleTag<?>, PCollection<?>>of(mainOutputTag, output));\n+        additionalOutputTags,\n+        ImmutableMap.<TupleTag<?>, PCollection<?>>of(mainOutputTag, output),\n+        ParDoEvaluator.<Integer, Integer>defaultRunnerFactory());\n   }\n \n   private static class RecorderFn extends DoFn<Integer, Integer> {",
                "changes": 15
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java",
                "deletions": 46,
                "sha": "59577a82b3b931ff658a0cf30d468051c82f2873",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java",
                "patch": "@@ -1,46 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.beam.runners.direct;\n-\n-import static org.junit.Assert.assertThat;\n-\n-import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.hamcrest.Matchers;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.JUnit4;\n-\n-/**\n- * Tests for {@link ParDoSingleViaMultiOverrideFactory}.\n- */\n-@RunWith(JUnit4.class)\n-public class ParDoSingleViaMultiOverrideFactoryTest {\n-  private ParDoSingleViaMultiOverrideFactory<Integer, Integer> factory =\n-      new ParDoSingleViaMultiOverrideFactory<>();\n-\n-  @Test\n-  public void getInputSucceeds() {\n-    TestPipeline p = TestPipeline.create();\n-    PCollection<Integer> input = p.apply(Create.of(1, 2, 3));\n-    PCollection<?> reconstructed = factory.getInput(input.expand(), p);\n-    assertThat(reconstructed, Matchers.<PCollection<?>>equalTo(input));\n-  }\n-}",
                "changes": 46
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java",
                "deletions": 15,
                "sha": "f4de8839c28c4ea550ce6a2c1c4c548e226e2e4d",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n import org.joda.time.Instant;\n import org.junit.Before;\n import org.junit.Rule;\n@@ -203,25 +204,13 @@ public void getNotReadyThrows() throws Exception {\n         .get(mapView, GlobalWindow.INSTANCE);\n   }\n \n-  @Test\n-  public void withPCollectionViewsErrorsForContainsNotInViews() {\n-    PCollectionView<Map<String, Iterable<String>>> newView =\n-        PCollectionViews.multimapView(\n-            pipeline,\n-            WindowingStrategy.globalDefault(),\n-            KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()));\n-\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"with unknown views \" + ImmutableList.of(newView).toString());\n-\n-    container.createReaderForViews(ImmutableList.<PCollectionView<?>>of(newView));\n-  }\n-\n   @Test\n   public void withViewsForViewNotInContainerFails() {\n+    PCollection<KV<String, String>> input =\n+        pipeline.apply(Create.empty(new TypeDescriptor<KV<String, String>>() {}));\n     PCollectionView<Map<String, Iterable<String>>> newView =\n         PCollectionViews.multimapView(\n-            pipeline,\n+            input,\n             WindowingStrategy.globalDefault(),\n             KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()));\n ",
                "changes": 19
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "deletions": 2,
                "sha": "ecb81307270307a8478dab2066c37f02192f22c5",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "patch": "@@ -243,7 +243,7 @@ public void testUnprocessedElements() throws Exception {\n         mainInput\n             .apply(\n                 new ParDoMultiOverrideFactory.GbkThenStatefulParDo<>(\n-                    ParDo.withSideInputs(sideInput)\n+                    ParDo\n                         .of(\n                             new DoFn<KV<String, Integer>, Integer>() {\n                               @StateId(stateId)\n@@ -253,6 +253,7 @@ public void testUnprocessedElements() throws Exception {\n                               @ProcessElement\n                               public void process(ProcessContext c) {}\n                             })\n+                        .withSideInputs(sideInput)\n                         .withOutputTags(mainOutput, TupleTagList.empty())))\n             .get(mainOutput)\n             .setCoder(VarIntCoder.of());\n@@ -307,7 +308,7 @@ public void process(ProcessContext c) {}\n         BUNDLE_FACTORY\n             .createBundle(\n                 (PCollection<KeyedWorkItem<String, KV<String, Integer>>>)\n-                    Iterables.getOnlyElement(producingTransform.getInputs()).getValue())\n+                    Iterables.getOnlyElement(producingTransform.getInputs().values()))\n             .add(gbkOutputElement)\n             .commit(Instant.now());\n     TransformEvaluator<KeyedWorkItem<String, KV<String, Integer>>> evaluator =",
                "changes": 5
            },
            {
                "status": "modified",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "deletions": 11,
                "sha": "b9c6e64de77cf2cc1ac8a8a56c972514868f905e",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "patch": "@@ -27,20 +27,16 @@\n import java.util.Collection;\n import java.util.Collections;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n-import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.DirectTestStreamFactory;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.DirectTestStreamFactory.DirectTestStream;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.TestClock;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.TestStreamIndex;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.VarIntCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.hamcrest.Matchers;\n import org.joda.time.Duration;\n@@ -179,11 +175,4 @@ public void producesElementsInSequence() throws Exception {\n     assertThat(fifthResult.getWatermarkHold(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE));\n     assertThat(fifthResult.getUnprocessedElements(), Matchers.emptyIterable());\n   }\n-\n-  @Test\n-  public void overrideFactoryGetInputSucceeds() {\n-    DirectTestStreamFactory<?> factory = new DirectTestStreamFactory<>(runner);\n-    PBegin begin = factory.getInput(Collections.<TaggedPValue>emptyList(), p);\n-    assertThat(begin.getPipeline(), Matchers.<Pipeline>equalTo(p));\n-  }\n }",
                "changes": 11
            },
            {
                "status": "modified",
                "additions": 48,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java",
                "deletions": 0,
                "sha": "77652b2c2694104a338f646d43fb06c8f63fd105",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java",
                "patch": "@@ -63,6 +63,31 @@ public void parallelScheduleMultipleSchedulesBothImmediately() {\n     parallel.complete(second);\n   }\n \n+  @Test\n+  public void parallelRejectedStillActiveThrows() {\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> first = mock(TransformExecutor.class);\n+\n+    TransformExecutorService parallel =\n+        TransformExecutorServices.parallel(executorService);\n+    executorService.shutdown();\n+    thrown.expect(IllegalStateException.class);\n+    thrown.expectMessage(\"still active\");\n+    parallel.schedule(first);\n+  }\n+\n+  @Test\n+  public void parallelRejectedShutdownSucceeds() {\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> first = mock(TransformExecutor.class);\n+\n+    TransformExecutorService parallel =\n+        TransformExecutorServices.parallel(executorService);\n+    executorService.shutdown();\n+    parallel.shutdown();\n+    parallel.schedule(first);\n+  }\n+\n   @Test\n   public void serialScheduleTwoWaitsForFirstToComplete() {\n     @SuppressWarnings(\"unchecked\")\n@@ -97,4 +122,27 @@ public void serialCompleteNotExecutingTaskThrows() {\n \n     serial.complete(second);\n   }\n+\n+  /**\n+   * Tests that a Serial {@link TransformExecutorService} does not schedule follow up work if the\n+   * executor is shut down when the initial work completes.\n+   */\n+  @Test\n+  public void serialShutdownCompleteActive() {\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> first = mock(TransformExecutor.class);\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> second = mock(TransformExecutor.class);\n+\n+    TransformExecutorService serial = TransformExecutorServices.serial(executorService);\n+    serial.schedule(first);\n+    verify(first).run();\n+\n+    serial.schedule(second);\n+    verify(second, never()).run();\n+\n+    serial.shutdown();\n+    serial.complete(first);\n+    verify(second, never()).run();\n+  }\n }",
                "changes": 48
            },
            {
                "status": "modified",
                "additions": 77,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "deletions": 11,
                "sha": "567ee984c31608177762bc71482ed804e9e307f0",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import static com.google.common.base.Preconditions.checkState;\n import static org.apache.beam.runners.direct.DirectGraphs.getProducer;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.equalTo;\n@@ -75,6 +76,7 @@\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n import org.mockito.invocation.InvocationOnMock;\n@@ -95,8 +97,8 @@\n   private UnboundedSource<Long, ?> source;\n   private DirectGraph graph;\n \n-  @Rule\n-  public TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+  @Rule public TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n \n   @Before\n   public void setup() {\n@@ -373,6 +375,41 @@ public void evaluatorClosesReaderAndResumesFromCheckpoint() throws Exception {\n     secondEvaluator.finishBundle();\n \n     assertThat(TestUnboundedSource.readerClosedCount, equalTo(2));\n+    assertThat(\n+        Iterables.getOnlyElement(residual.getElements()).getValue().getCheckpoint().isFinalized(),\n+        is(true));\n+  }\n+\n+  @Test\n+  public void evaluatorThrowsInCloseRethrows() throws Exception {\n+    ContiguousSet<Long> elems = ContiguousSet.create(Range.closed(0L, 20L), DiscreteDomain.longs());\n+    TestUnboundedSource<Long> source =\n+        new TestUnboundedSource<>(BigEndianLongCoder.of(), elems.toArray(new Long[0]))\n+            .throwsOnClose();\n+\n+    PCollection<Long> pcollection = p.apply(Read.from(source));\n+    AppliedPTransform<?, ?, ?> sourceTransform =\n+        DirectGraphs.getGraph(p).getProducer(pcollection);\n+\n+    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n+    UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);\n+    when(context.createBundle(pcollection)).thenReturn(output);\n+\n+    WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> shard =\n+        WindowedValue.valueInGlobalWindow(\n+            UnboundedSourceShard.unstarted(source, NeverDeduplicator.create()));\n+    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> inputBundle =\n+        bundleFactory\n+            .<UnboundedSourceShard<Long, TestCheckpointMark>>createRootBundle()\n+            .add(shard)\n+            .commit(Instant.now());\n+    UnboundedReadEvaluatorFactory factory =\n+        new UnboundedReadEvaluatorFactory(context, 0.0 /* never reuse */);\n+    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator =\n+        factory.forApplication(sourceTransform, inputBundle);\n+    thrown.expect(IOException.class);\n+    thrown.expectMessage(\"throws on close\");\n+    evaluator.processElement(shard);\n   }\n \n   /**\n@@ -398,26 +435,32 @@ public Instant apply(Long input) {\n     private final Coder<T> coder;\n     private final List<T> elems;\n     private boolean dedupes = false;\n+    private boolean throwOnClose;\n \n     public TestUnboundedSource(Coder<T> coder, T... elems) {\n+      this(coder, false, Arrays.asList(elems));\n+    }\n+\n+   private TestUnboundedSource(Coder<T> coder, boolean throwOnClose, List<T> elems) {\n       readerAdvancedCount = 0;\n       readerClosedCount = 0;\n       this.coder = coder;\n-      this.elems = Arrays.asList(elems);\n+      this.elems = elems;\n+      this.throwOnClose = throwOnClose;\n     }\n \n     @Override\n-    public List<? extends UnboundedSource<T, TestCheckpointMark>> generateInitialSplits(\n+    public List<? extends UnboundedSource<T, TestCheckpointMark>> split(\n         int desiredNumSplits, PipelineOptions options) throws Exception {\n       return ImmutableList.of(this);\n     }\n \n     @Override\n     public UnboundedSource.UnboundedReader<T> createReader(\n         PipelineOptions options, @Nullable TestCheckpointMark checkpointMark) {\n-      if (checkpointMark != null) {\n-        assertThat(checkpointMark.isFinalized(), is(true));\n-      }\n+      checkState(\n+          checkpointMark == null || checkpointMark.decoded,\n+          \"Cannot resume from a checkpoint that has not been decoded\");\n       return new TestUnboundedReader(elems, checkpointMark == null ? -1 : checkpointMark.index);\n     }\n \n@@ -440,9 +483,14 @@ public void validate() {}\n       return coder;\n     }\n \n+    public TestUnboundedSource<T> throwsOnClose() {\n+      return new TestUnboundedSource<>(coder, true, elems);\n+    }\n+\n     private class TestUnboundedReader extends UnboundedReader<T> {\n       private final List<T> elems;\n       private int index;\n+      private boolean closed = false;\n \n       public TestUnboundedReader(List<T> elems, int startIndex) {\n         this.elems = elems;\n@@ -502,21 +550,37 @@ public Instant getCurrentTimestamp() throws NoSuchElementException {\n \n       @Override\n       public void close() throws IOException {\n-        readerClosedCount++;\n+        try {\n+          readerClosedCount++;\n+          // Enforce the AutoCloseable contract. Close is not idempotent.\n+          assertThat(closed, is(false));\n+          if (throwOnClose) {\n+            throw new IOException(String.format(\"%s throws on close\", TestUnboundedSource.this));\n+          }\n+        } finally {\n+          closed = true;\n+        }\n       }\n     }\n   }\n \n   private static class TestCheckpointMark implements CheckpointMark {\n     final int index;\n     private boolean finalized = false;\n+    private boolean decoded = false;\n \n     private TestCheckpointMark(int index) {\n       this.index = index;\n     }\n \n     @Override\n     public void finalizeCheckpoint() throws IOException {\n+      checkState(\n+          !finalized, \"%s was finalized more than once\", TestCheckpointMark.class.getSimpleName());\n+      checkState(\n+          !decoded,\n+          \"%s was finalized after being decoded\",\n+          TestCheckpointMark.class.getSimpleName());\n       finalized = true;\n     }\n \n@@ -530,15 +594,17 @@ public void encode(\n           TestCheckpointMark value,\n           OutputStream outStream,\n           org.apache.beam.sdk.coders.Coder.Context context)\n-          throws CoderException, IOException {\n+          throws IOException {\n         VarInt.encode(value.index, outStream);\n       }\n \n       @Override\n       public TestCheckpointMark decode(\n           InputStream inStream, org.apache.beam.sdk.coders.Coder.Context context)\n-          throws CoderException, IOException {\n-        return new TestCheckpointMark(VarInt.decodeInt(inStream));\n+          throws IOException {\n+        TestCheckpointMark decoded = new TestCheckpointMark(VarInt.decodeInt(inStream));\n+        decoded.decoded = true;\n+        return decoded;\n       }\n     }\n   }",
                "changes": 88
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "deletions": 11,
                "sha": "fe55a5f470fadbcbfcc3864eb0d0107def88bb56",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.beam.runners.direct;\n \n import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.nullValue;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Mockito.mock;\n@@ -27,7 +26,6 @@\n import com.google.common.collect.ImmutableList;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.PCollectionViewWriter;\n-import org.apache.beam.runners.direct.ViewEvaluatorFactory.ViewOverrideFactory;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VoidCoder;\n@@ -63,14 +61,15 @@ public void testInMemoryEvaluator() throws Exception {\n     PCollection<String> input = p.apply(Create.of(\"foo\", \"bar\"));\n     CreatePCollectionView<String, Iterable<String>> createView =\n         CreatePCollectionView.of(\n-            PCollectionViews.iterableView(p, input.getWindowingStrategy(), StringUtf8Coder.of()));\n+            PCollectionViews.iterableView(\n+                input, input.getWindowingStrategy(), StringUtf8Coder.of()));\n     PCollection<Iterable<String>> concat =\n         input.apply(WithKeys.<Void, String>of((Void) null))\n             .setCoder(KvCoder.of(VoidCoder.of(), StringUtf8Coder.of()))\n             .apply(GroupByKey.<Void, String>create())\n             .apply(Values.<Iterable<String>>create());\n     PCollectionView<Iterable<String>> view =\n-        concat.apply(new ViewEvaluatorFactory.WriteView<>(createView));\n+        concat.apply(new ViewOverrideFactory.WriteView<>(createView));\n \n     EvaluationContext context = mock(EvaluationContext.class);\n     TestViewWriter<String, Iterable<String>> viewWriter = new TestViewWriter<>();\n@@ -93,13 +92,6 @@ public void testInMemoryEvaluator() throws Exception {\n             WindowedValue.valueInGlobalWindow(\"foo\"), WindowedValue.valueInGlobalWindow(\"bar\")));\n   }\n \n-  @Test\n-  public void overrideFactoryGetInputSucceeds() {\n-    ViewOverrideFactory<String, String> factory = new ViewOverrideFactory<>();\n-    PCollection<String> input = p.apply(Create.of(\"foo\", \"bar\"));\n-    assertThat(factory.getInput(input.expand(), p), equalTo(input));\n-  }\n-\n   private static class TestViewWriter<ElemT, ViewT> implements PCollectionViewWriter<ElemT, ViewT> {\n     private Iterable<WindowedValue<ElemT>> latest;\n ",
                "changes": 14
            },
            {
                "status": "added",
                "additions": 138,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java",
                "deletions": 0,
                "sha": "6875e1a939cb8b05e7fcc84754910dd7ba3431b6",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java",
                "patch": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.direct;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.common.collect.ImmutableSet;\n+import java.io.Serializable;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import org.apache.beam.runners.direct.ViewOverrideFactory.WriteView;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.PTransformReplacement;\n+import org.apache.beam.sdk.runners.TransformHierarchy.Node;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.util.PCollectionViews;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Tests for {@link ViewOverrideFactory}. */\n+@RunWith(JUnit4.class)\n+public class ViewOverrideFactoryTest implements Serializable {\n+  @Rule\n+  public transient TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+\n+  private transient ViewOverrideFactory<Integer, List<Integer>> factory =\n+      new ViewOverrideFactory<>();\n+\n+  @Test\n+  public void replacementSucceeds() {\n+    PCollection<Integer> ints = p.apply(\"CreateContents\", Create.of(1, 2, 3));\n+    final PCollectionView<List<Integer>> view =\n+        PCollectionViews.listView(ints, WindowingStrategy.globalDefault(), ints.getCoder());\n+    PTransformReplacement<PCollection<Integer>, PCollectionView<List<Integer>>>\n+        replacementTransform =\n+            factory.getReplacementTransform(\n+                AppliedPTransform\n+                    .<PCollection<Integer>, PCollectionView<List<Integer>>,\n+                        CreatePCollectionView<Integer, List<Integer>>>\n+                        of(\n+                            \"foo\",\n+                            ints.expand(),\n+                            view.expand(),\n+                            CreatePCollectionView.<Integer, List<Integer>>of(view),\n+                            p));\n+    PCollectionView<List<Integer>> afterReplacement =\n+        ints.apply(replacementTransform.getTransform());\n+    assertThat(\n+        \"The CreatePCollectionView replacement should return the same View\",\n+        afterReplacement,\n+        equalTo(view));\n+\n+    PCollection<Set<Integer>> outputViewContents =\n+        p.apply(\"CreateSingleton\", Create.of(0))\n+            .apply(\n+                \"OutputContents\",\n+                ParDo.of(\n+                        new DoFn<Integer, Set<Integer>>() {\n+                          @ProcessElement\n+                          public void outputSideInput(ProcessContext context) {\n+                            context.output(ImmutableSet.copyOf(context.sideInput(view)));\n+                          }\n+                        })\n+                    .withSideInputs(view));\n+    PAssert.thatSingleton(outputViewContents).isEqualTo(ImmutableSet.of(1, 2, 3));\n+\n+    p.run();\n+  }\n+\n+  @Test\n+  public void replacementGetViewReturnsOriginal() {\n+    final PCollection<Integer> ints = p.apply(\"CreateContents\", Create.of(1, 2, 3));\n+    final PCollectionView<List<Integer>> view =\n+        PCollectionViews.listView(ints, WindowingStrategy.globalDefault(), ints.getCoder());\n+    PTransformReplacement<PCollection<Integer>, PCollectionView<List<Integer>>> replacement =\n+        factory.getReplacementTransform(\n+            AppliedPTransform\n+                .<PCollection<Integer>, PCollectionView<List<Integer>>,\n+                    CreatePCollectionView<Integer, List<Integer>>>\n+                    of(\n+                        \"foo\",\n+                        ints.expand(),\n+                        view.expand(),\n+                        CreatePCollectionView.<Integer, List<Integer>>of(view),\n+                        p));\n+    ints.apply(replacement.getTransform());\n+    final AtomicBoolean writeViewVisited = new AtomicBoolean();\n+    p.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            if (node.getTransform() instanceof WriteView) {\n+              assertThat(\n+                  \"There should only be one WriteView primitive in the graph\",\n+                  writeViewVisited.getAndSet(true),\n+                  is(false));\n+              PCollectionView replacementView = ((WriteView) node.getTransform()).getView();\n+              assertThat(replacementView, Matchers.<PCollectionView>theInstance(view));\n+              assertThat(node.getInputs().entrySet(), hasSize(1));\n+            }\n+          }\n+        });\n+\n+    assertThat(writeViewVisited.get(), is(true));\n+  }\n+}",
                "changes": 138
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java",
                "deletions": 6,
                "sha": "eb58629d3004ee0b658db5788f594350835d7e5b",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java",
                "patch": "@@ -43,8 +43,8 @@\n import org.apache.beam.sdk.transforms.windowing.PaneInfo.Timing;\n import org.apache.beam.sdk.transforms.windowing.SlidingWindows;\n import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.Window.Bound;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n import org.hamcrest.Matchers;\n@@ -112,7 +112,7 @@ public void setup() {\n   @Test\n   public void singleWindowFnSucceeds() throws Exception {\n     Duration windowDuration = Duration.standardDays(7);\n-    Bound<Long> transform = Window.<Long>into(FixedWindows.of(windowDuration));\n+    Window<Long> transform = Window.<Long>into(FixedWindows.of(windowDuration));\n     PCollection<Long> windowed = input.apply(transform);\n \n     CommittedBundle<Long> inputBundle = createInputBundle();\n@@ -151,7 +151,7 @@ public void singleWindowFnSucceeds() throws Exception {\n   public void multipleWindowsWindowFnSucceeds() throws Exception {\n     Duration windowDuration = Duration.standardDays(6);\n     Duration slidingBy = Duration.standardDays(3);\n-    Bound<Long> transform = Window.into(SlidingWindows.of(windowDuration).every(slidingBy));\n+    Window<Long> transform = Window.into(SlidingWindows.of(windowDuration).every(slidingBy));\n     PCollection<Long> windowed = input.apply(transform);\n \n     CommittedBundle<Long> inputBundle = createInputBundle();\n@@ -208,7 +208,7 @@ public void multipleWindowsWindowFnSucceeds() throws Exception {\n \n   @Test\n   public void referencesEarlierWindowsSucceeds() throws Exception {\n-    Bound<Long> transform = Window.into(new EvaluatorTestWindowFn());\n+    Window<Long> transform = Window.into(new EvaluatorTestWindowFn());\n     PCollection<Long> windowed = input.apply(transform);\n \n     CommittedBundle<Long> inputBundle = createInputBundle();\n@@ -313,8 +313,8 @@ public boolean isCompatible(WindowFn<?, ?> other) {\n     }\n \n     @Override\n-    public BoundedWindow getSideInputWindow(BoundedWindow window) {\n-      return null;\n+    public WindowMappingFn<BoundedWindow> getDefaultWindowMappingFn() {\n+      throw new UnsupportedOperationException(\"Cannot be used as a side input\");\n     }\n   }\n }",
                "changes": 12
            },
            {
                "status": "modified",
                "additions": 17,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "deletions": 11,
                "sha": "361850ddb1ac8f6ed1536c083cf4da99c24b2bd1",
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "patch": "@@ -38,11 +38,13 @@\n import java.util.UUID;\n import org.apache.beam.runners.direct.WriteWithShardingFactory.CalculateShardsFn;\n import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.io.Sink;\n import org.apache.beam.sdk.io.TextIO;\n import org.apache.beam.sdk.io.Write;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFnTester;\n@@ -52,7 +54,9 @@\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n-import org.hamcrest.Matchers;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n@@ -118,7 +122,15 @@ public void dynamicallyReshardedWrite() throws Exception {\n   @Test\n   public void withNoShardingSpecifiedReturnsNewTransform() {\n     Write<Object> original = Write.to(new TestSink());\n-    assertThat(factory.getReplacementTransform(original), not(equalTo((Object) original)));\n+    PCollection<Object> objs = (PCollection) p.apply(Create.empty(VoidCoder.of()));\n+\n+    AppliedPTransform<PCollection<Object>, PDone, Write<Object>> originalApplication =\n+        AppliedPTransform.of(\n+            \"write\", objs.expand(), Collections.<TupleTag<?>, PValue>emptyMap(), original, p);\n+\n+    assertThat(\n+        factory.getReplacementTransform(originalApplication).getTransform(),\n+        not(equalTo((Object) original)));\n   }\n \n   @Test\n@@ -170,13 +182,14 @@ public void keyBasedOnCountFnManyElements() throws Exception {\n \n   @Test\n   public void keyBasedOnCountFnFewElementsExtraShards() throws Exception {\n+    long countValue = (long) WriteWithShardingFactory.MIN_SHARDS_FOR_LOG + 3;\n+    PCollection<Long> inputCount = p.apply(Create.of(countValue));\n     PCollectionView<Long> elementCountView =\n         PCollectionViews.singletonView(\n-            p, WindowingStrategy.globalDefault(), true, 0L, VarLongCoder.of());\n+            inputCount, WindowingStrategy.globalDefault(), true, 0L, VarLongCoder.of());\n     CalculateShardsFn fn = new CalculateShardsFn(3);\n     DoFnTester<Long, Integer> fnTester = DoFnTester.of(fn);\n \n-    long countValue = (long) WriteWithShardingFactory.MIN_SHARDS_FOR_LOG + 3;\n     fnTester.setSideInput(elementCountView, GlobalWindow.INSTANCE, countValue);\n \n     List<Integer> kvs = fnTester.processBundle(10L);\n@@ -194,13 +207,6 @@ public void keyBasedOnCountFnManyElementsExtraShards() throws Exception {\n     assertThat(shards, containsInAnyOrder(13));\n   }\n \n-  @Test\n-  public void getInputSucceeds() {\n-    PCollection<String> original = p.apply(Create.of(\"foo\"));\n-    PCollection<?> input = factory.getInput(original.expand(), p);\n-    assertThat(input, Matchers.<PCollection<?>>equalTo(original));\n-  }\n-\n   private static class TestSink extends Sink<Object> {\n     @Override\n     public void validate(PipelineOptions options) {}",
                "changes": 28
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/pom.xml?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/pom.xml",
                "deletions": 126,
                "sha": "661ed432b087b38a82f36ccba44b805e553e237e",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/pom.xml",
                "patch": "@@ -1,126 +0,0 @@\n-<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<!--\n-    Licensed to the Apache Software Foundation (ASF) under one or more\n-    contributor license agreements.  See the NOTICE file distributed with\n-    this work for additional information regarding copyright ownership.\n-    The ASF licenses this file to You under the Apache License, Version 2.0\n-    (the \"License\"); you may not use this file except in compliance with\n-    the License.  You may obtain a copy of the License at\n-\n-       http://www.apache.org/licenses/LICENSE-2.0\n-\n-    Unless required by applicable law or agreed to in writing, software\n-    distributed under the License is distributed on an \"AS IS\" BASIS,\n-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-    See the License for the specific language governing permissions and\n-    limitations under the License.\n--->\n-<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n-\n-  <modelVersion>4.0.0</modelVersion>\n-\n-  <parent>\n-    <groupId>org.apache.beam</groupId>\n-    <artifactId>beam-runners-flink-parent</artifactId>\n-    <version>0.7.0-SNAPSHOT</version>\n-    <relativePath>../pom.xml</relativePath>\n-  </parent>\n-\n-  <artifactId>beam-runners-flink_2.10-examples</artifactId>\n-\n-  <name>Apache Beam :: Runners :: Flink :: Examples</name>\n-\n-  <packaging>jar</packaging>\n-\n-  <properties>\n-    <!-- Default parameters for mvn exec:java -->\n-    <flink.examples.input>kinglear.txt</flink.examples.input>\n-    <flink.examples.output>wordcounts.txt</flink.examples.output>\n-    <flink.examples.parallelism>-1</flink.examples.parallelism>\n-  </properties>\n-\n-  <profiles>\n-    <profile>\n-      <id>disable-runnable-on-service-tests</id>\n-      <activation>\n-        <activeByDefault>true</activeByDefault>\n-      </activation>\n-      <build>\n-        <plugins>\n-          <plugin>\n-            <groupId>org.apache.maven.plugins</groupId>\n-            <artifactId>maven-surefire-plugin</artifactId>\n-            <executions>\n-              <execution>\n-                <id>runnable-on-service-tests</id>\n-                <configuration>\n-                  <skip>true</skip>\n-                </configuration>\n-              </execution>\n-            </executions>\n-          </plugin>\n-        </plugins>\n-      </build>\n-    </profile>\n-  </profiles>\n-\n-  <dependencies>\n-\n-    <dependency>\n-      <groupId>org.apache.beam</groupId>\n-      <artifactId>beam-runners-flink_2.10</artifactId>\n-      <version>${project.version}</version>\n-    </dependency>\n-\n-    <dependency>\n-      <groupId>org.apache.flink</groupId>\n-      <artifactId>flink-connector-kafka-0.8_2.10</artifactId>\n-      <version>${flink.version}</version>\n-    </dependency>\n-\n-  </dependencies>\n-\n-  <build>\n-    <plugins>\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-compiler-plugin</artifactId>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-jar-plugin</artifactId>\n-      </plugin>\n-\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-dependency-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <goals><goal>analyze-only</goal></goals>\n-            <configuration>\n-              <!-- disable for now until dependencies are cleaned up -->\n-              <failOnWarning>false</failOnWarning>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n-\n-      <plugin>\n-        <groupId>org.codehaus.mojo</groupId>\n-        <artifactId>exec-maven-plugin</artifactId>\n-        <configuration>\n-          <executable>java</executable>\n-          <arguments>\n-            <argument>--runner=org.apache.beam.runners.flink.FlinkRunner</argument>\n-            <argument>--parallelism=${flink.examples.parallelism}</argument>\n-            <argument>--input=${flink.examples.input}</argument>\n-            <argument>--output=${flink.examples.output}</argument>\n-          </arguments>\n-        </configuration>\n-      </plugin>\n-\n-    </plugins>\n-\n-  </build>\n-\n-</project>",
                "changes": 126
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "deletions": 456,
                "sha": "89e261b52b3066faadf79eb25540463f9cc7713d",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "patch": "@@ -1,456 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.util.HashSet;\n-import java.util.Set;\n-import org.apache.beam.runners.flink.FlinkPipelineOptions;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.coders.StringDelegateCoder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.GcsOptions;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.options.Validation;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.Distinct;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.Flatten;\n-import org.apache.beam.sdk.transforms.Keys;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Values;\n-import org.apache.beam.sdk.transforms.View;\n-import org.apache.beam.sdk.transforms.WithKeys;\n-import org.apache.beam.sdk.transforms.join.CoGbkResult;\n-import org.apache.beam.sdk.transforms.join.CoGroupByKey;\n-import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;\n-import org.apache.beam.sdk.util.GcsUtil;\n-import org.apache.beam.sdk.util.gcsfs.GcsPath;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PBegin;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionList;\n-import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.PDone;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * An example that computes a basic TF-IDF search table for a directory or GCS prefix.\n- *\n- * <p>Concepts: joining data; side inputs; logging\n- *\n- * <p>To execute this pipeline locally, specify general pipeline configuration:\n- * <pre>{@code\n- *   --project=YOUR_PROJECT_ID\n- * }</pre>\n- * and a local output file or output prefix on GCS:\n- * <pre>{@code\n- *   --output=[YOUR_LOCAL_FILE | gs://YOUR_OUTPUT_PREFIX]\n- * }</pre>\n- *\n- * <p>To execute this pipeline using the Dataflow service, specify pipeline configuration:\n- * <pre>{@code\n- *   --project=YOUR_PROJECT_ID\n- *   --stagingLocation=gs://YOUR_STAGING_DIRECTORY\n- *   --runner=BlockingDataflowRunner\n- * and an output prefix on GCS:\n- *   --output=gs://YOUR_OUTPUT_PREFIX\n- * }</pre>\n- *\n- * <p>The default input is {@code gs://dataflow-samples/shakespeare/} and can be overridden with\n- * {@code --input}.\n- */\n-public class TFIDF {\n-  /**\n-   * Options supported by {@link TFIDF}.\n-   *\n-   * <p>Inherits standard configuration options.\n-   */\n-  private interface Options extends PipelineOptions, FlinkPipelineOptions {\n-    @Description(\"Path to the directory or GCS prefix containing files to read from\")\n-    @Default.String(\"gs://dataflow-samples/shakespeare/\")\n-    String getInput();\n-    void setInput(String value);\n-\n-    @Description(\"Prefix of output URI to write to\")\n-    @Validation.Required\n-    String getOutput();\n-    void setOutput(String value);\n-  }\n-\n-  /**\n-   * Lists documents contained beneath the {@code options.input} prefix/directory.\n-   */\n-  public static Set<URI> listInputDocuments(Options options)\n-      throws URISyntaxException, IOException {\n-    URI baseUri = new URI(options.getInput());\n-\n-    // List all documents in the directory or GCS prefix.\n-    URI absoluteUri;\n-    if (baseUri.getScheme() != null) {\n-      absoluteUri = baseUri;\n-    } else {\n-      absoluteUri = new URI(\n-          \"file\",\n-          baseUri.getAuthority(),\n-          baseUri.getPath(),\n-          baseUri.getQuery(),\n-          baseUri.getFragment());\n-    }\n-\n-    Set<URI> uris = new HashSet<>();\n-    if (absoluteUri.getScheme().equals(\"file\")) {\n-      File directory = new File(absoluteUri);\n-      String[] directoryListing = directory.list();\n-      if (directoryListing == null) {\n-        throw new IOException(\n-            \"Directory \" + absoluteUri + \" is not a valid path or IO Error occurred.\");\n-      }\n-      for (String entry : directoryListing) {\n-        File path = new File(directory, entry);\n-        uris.add(path.toURI());\n-      }\n-    } else if (absoluteUri.getScheme().equals(\"gs\")) {\n-      GcsUtil gcsUtil = options.as(GcsOptions.class).getGcsUtil();\n-      URI gcsUriGlob = new URI(\n-          absoluteUri.getScheme(),\n-          absoluteUri.getAuthority(),\n-          absoluteUri.getPath() + \"*\",\n-          absoluteUri.getQuery(),\n-          absoluteUri.getFragment());\n-      for (GcsPath entry : gcsUtil.expand(GcsPath.fromUri(gcsUriGlob))) {\n-        uris.add(entry.toUri());\n-      }\n-    }\n-\n-    return uris;\n-  }\n-\n-  /**\n-   * Reads the documents at the provided uris and returns all lines\n-   * from the documents tagged with which document they are from.\n-   */\n-  public static class ReadDocuments\n-      extends PTransform<PBegin, PCollection<KV<URI, String>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    // transient because PTransform is not really meant to be serialized.\n-    // see note on PTransform\n-    private final transient Iterable<URI> uris;\n-\n-    public ReadDocuments(Iterable<URI> uris) {\n-      this.uris = uris;\n-    }\n-\n-    @Override\n-    public Coder<?> getDefaultOutputCoder() {\n-      return KvCoder.of(StringDelegateCoder.of(URI.class), StringUtf8Coder.of());\n-    }\n-\n-    @Override\n-    public PCollection<KV<URI, String>> expand(PBegin input) {\n-      Pipeline pipeline = input.getPipeline();\n-\n-      // Create one TextIO.Read transform for each document\n-      // and add its output to a PCollectionList\n-      PCollectionList<KV<URI, String>> urisToLines =\n-          PCollectionList.empty(pipeline);\n-\n-      // TextIO.Read supports:\n-      //  - file: URIs and paths locally\n-      //  - gs: URIs on the service\n-      for (final URI uri : uris) {\n-        String uriString;\n-        if (uri.getScheme().equals(\"file\")) {\n-          uriString = new File(uri).getPath();\n-        } else {\n-          uriString = uri.toString();\n-        }\n-\n-        PCollection<KV<URI, String>> oneUriToLines = pipeline\n-            .apply(\"TextIO.Read(\" + uriString + \")\", TextIO.Read.from(uriString))\n-            .apply(\"WithKeys(\" + uriString + \")\", WithKeys.<URI, String>of(uri));\n-\n-        urisToLines = urisToLines.and(oneUriToLines);\n-      }\n-\n-      return urisToLines.apply(Flatten.<KV<URI, String>>pCollections());\n-    }\n-  }\n-\n-  /**\n-   * A transform containing a basic TF-IDF pipeline. The input consists of KV objects\n-   * where the key is the document's URI and the value is a piece\n-   * of the document's content. The output is mapping from terms to\n-   * scores for each document URI.\n-   */\n-  public static class ComputeTfIdf\n-      extends PTransform<PCollection<KV<URI, String>>, PCollection<KV<String, KV<URI, Double>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    public ComputeTfIdf() { }\n-\n-    @Override\n-    public PCollection<KV<String, KV<URI, Double>>> expand(\n-        PCollection<KV<URI, String>> uriToContent) {\n-\n-      // Compute the total number of documents, and\n-      // prepare this singleton PCollectionView for\n-      // use as a side input.\n-      final PCollectionView<Long> totalDocuments =\n-          uriToContent\n-              .apply(\"GetURIs\", Keys.<URI>create())\n-              .apply(\"DistinctDocs\", Distinct.<URI>create())\n-              .apply(Count.<URI>globally())\n-              .apply(View.<Long>asSingleton());\n-\n-      // Create a collection of pairs mapping a URI to each\n-      // of the words in the document associated with that that URI.\n-      PCollection<KV<URI, String>> uriToWords = uriToContent\n-          .apply(\"SplitWords\", ParDo.of(new DoFn<KV<URI, String>, KV<URI, String>>() {\n-            private static final long serialVersionUID = 0;\n-\n-            @ProcessElement\n-            public void processElement(ProcessContext c) {\n-              URI uri = c.element().getKey();\n-              String line = c.element().getValue();\n-              for (String word : line.split(\"\\\\W+\")) {\n-                // Log INFO messages when the word \u201clove\u201d is found.\n-                if (word.toLowerCase().equals(\"love\")) {\n-                  LOG.info(\"Found {}\", word.toLowerCase());\n-                }\n-\n-                if (!word.isEmpty()) {\n-                  c.output(KV.of(uri, word.toLowerCase()));\n-                }\n-              }\n-            }\n-          }));\n-\n-      // Compute a mapping from each word to the total\n-      // number of documents in which it appears.\n-      PCollection<KV<String, Long>> wordToDocCount = uriToWords\n-          .apply(\"DistinctWords\", Distinct.<KV<URI, String>>create())\n-          .apply(Values.<String>create())\n-          .apply(\"CountDocs\", Count.<String>perElement());\n-\n-      // Compute a mapping from each URI to the total\n-      // number of words in the document associated with that URI.\n-      PCollection<KV<URI, Long>> uriToWordTotal = uriToWords\n-          .apply(\"GetURIs2\", Keys.<URI>create())\n-          .apply(\"CountWords\", Count.<URI>perElement());\n-\n-      // Count, for each (URI, word) pair, the number of\n-      // occurrences of that word in the document associated\n-      // with the URI.\n-      PCollection<KV<KV<URI, String>, Long>> uriAndWordToCount = uriToWords\n-          .apply(\"CountWordDocPairs\", Count.<KV<URI, String>>perElement());\n-\n-      // Adjust the above collection to a mapping from\n-      // (URI, word) pairs to counts into an isomorphic mapping\n-      // from URI to (word, count) pairs, to prepare for a join\n-      // by the URI key.\n-      PCollection<KV<URI, KV<String, Long>>> uriToWordAndCount = uriAndWordToCount\n-          .apply(\"ShiftKeys\", ParDo.of(\n-              new DoFn<KV<KV<URI, String>, Long>, KV<URI, KV<String, Long>>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  URI uri = c.element().getKey().getKey();\n-                  String word = c.element().getKey().getValue();\n-                  Long occurrences = c.element().getValue();\n-                  c.output(KV.of(uri, KV.of(word, occurrences)));\n-                }\n-              }));\n-\n-      // Prepare to join the mapping of URI to (word, count) pairs with\n-      // the mapping of URI to total word counts, by associating\n-      // each of the input PCollection<KV<URI, ...>> with\n-      // a tuple tag. Each input must have the same key type, URI\n-      // in this case. The type parameter of the tuple tag matches\n-      // the types of the values for each collection.\n-      final TupleTag<Long> wordTotalsTag = new TupleTag<>();\n-      final TupleTag<KV<String, Long>> wordCountsTag = new TupleTag<>();\n-      KeyedPCollectionTuple<URI> coGbkInput = KeyedPCollectionTuple\n-          .of(wordTotalsTag, uriToWordTotal)\n-          .and(wordCountsTag, uriToWordAndCount);\n-\n-      // Perform a CoGroupByKey (a sort of pre-join) on the prepared\n-      // inputs. This yields a mapping from URI to a CoGbkResult\n-      // (CoGroupByKey Result). The CoGbkResult is a mapping\n-      // from the above tuple tags to the values in each input\n-      // associated with a particular URI. In this case, each\n-      // KV<URI, CoGbkResult> group a URI with the total number of\n-      // words in that document as well as all the (word, count)\n-      // pairs for particular words.\n-      PCollection<KV<URI, CoGbkResult>> uriToWordAndCountAndTotal = coGbkInput\n-          .apply(\"CoGroupByUri\", CoGroupByKey.<URI>create());\n-\n-      // Compute a mapping from each word to a (URI, term frequency)\n-      // pair for each URI. A word's term frequency for a document\n-      // is simply the number of times that word occurs in the document\n-      // divided by the total number of words in the document.\n-      PCollection<KV<String, KV<URI, Double>>> wordToUriAndTf = uriToWordAndCountAndTotal\n-          .apply(\"ComputeTermFrequencies\", ParDo.of(\n-              new DoFn<KV<URI, CoGbkResult>, KV<String, KV<URI, Double>>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  URI uri = c.element().getKey();\n-                  Long wordTotal = c.element().getValue().getOnly(wordTotalsTag);\n-\n-                  for (KV<String, Long> wordAndCount\n-                      : c.element().getValue().getAll(wordCountsTag)) {\n-                    String word = wordAndCount.getKey();\n-                    Long wordCount = wordAndCount.getValue();\n-                    Double termFrequency = wordCount.doubleValue() / wordTotal.doubleValue();\n-                    c.output(KV.of(word, KV.of(uri, termFrequency)));\n-                  }\n-                }\n-              }));\n-\n-      // Compute a mapping from each word to its document frequency.\n-      // A word's document frequency in a corpus is the number of\n-      // documents in which the word appears divided by the total\n-      // number of documents in the corpus. Note how the total number of\n-      // documents is passed as a side input; the same value is\n-      // presented to each invocation of the DoFn.\n-      PCollection<KV<String, Double>> wordToDf = wordToDocCount\n-          .apply(\"ComputeDocFrequencies\", ParDo\n-              .withSideInputs(totalDocuments)\n-              .of(new DoFn<KV<String, Long>, KV<String, Double>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  String word = c.element().getKey();\n-                  Long documentCount = c.element().getValue();\n-                  Long documentTotal = c.sideInput(totalDocuments);\n-                  Double documentFrequency = documentCount.doubleValue()\n-                      / documentTotal.doubleValue();\n-\n-                  c.output(KV.of(word, documentFrequency));\n-                }\n-              }));\n-\n-      // Join the term frequency and document frequency\n-      // collections, each keyed on the word.\n-      final TupleTag<KV<URI, Double>> tfTag = new TupleTag<>();\n-      final TupleTag<Double> dfTag = new TupleTag<>();\n-      PCollection<KV<String, CoGbkResult>> wordToUriAndTfAndDf = KeyedPCollectionTuple\n-          .of(tfTag, wordToUriAndTf)\n-          .and(dfTag, wordToDf)\n-          .apply(CoGroupByKey.<String>create());\n-\n-      // Compute a mapping from each word to a (URI, TF-IDF) score\n-      // for each URI. There are a variety of definitions of TF-IDF\n-      // (\"term frequency - inverse document frequency\") score;\n-      // here we use a basic version that is the term frequency\n-      // divided by the log of the document frequency.\n-\n-      return wordToUriAndTfAndDf\n-          .apply(\"ComputeTfIdf\", ParDo.of(\n-              new DoFn<KV<String, CoGbkResult>, KV<String, KV<URI, Double>>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  String word = c.element().getKey();\n-                  Double df = c.element().getValue().getOnly(dfTag);\n-\n-                  for (KV<URI, Double> uriAndTf : c.element().getValue().getAll(tfTag)) {\n-                    URI uri = uriAndTf.getKey();\n-                    Double tf = uriAndTf.getValue();\n-                    Double tfIdf = tf * Math.log(1 / df);\n-                    c.output(KV.of(word, KV.of(uri, tfIdf)));\n-                  }\n-                }\n-              }));\n-    }\n-\n-    // Instantiate Logger.\n-    // It is suggested that the user specify the class name of the containing class\n-    // (in this case ComputeTfIdf).\n-    private static final Logger LOG = LoggerFactory.getLogger(ComputeTfIdf.class);\n-  }\n-\n-  /**\n-   * A {@link PTransform} to write, in CSV format, a mapping from term and URI\n-   * to score.\n-   */\n-  public static class WriteTfIdf\n-      extends PTransform<PCollection<KV<String, KV<URI, Double>>>, PDone> {\n-    private static final long serialVersionUID = 0;\n-\n-    private String output;\n-\n-    public WriteTfIdf(String output) {\n-      this.output = output;\n-    }\n-\n-    @Override\n-    public PDone expand(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf) {\n-      return wordToUriAndTfIdf\n-          .apply(\"Format\", ParDo.of(new DoFn<KV<String, KV<URI, Double>>, String>() {\n-            private static final long serialVersionUID = 0;\n-\n-            @ProcessElement\n-            public void processElement(ProcessContext c) {\n-              c.output(String.format(\"%s,\\t%s,\\t%f\",\n-                  c.element().getKey(),\n-                  c.element().getValue().getKey(),\n-                  c.element().getValue().getValue()));\n-            }\n-          }))\n-          .apply(TextIO.Write\n-              .to(output)\n-              .withSuffix(\".csv\"));\n-    }\n-  }\n-\n-  public static void main(String[] args) throws Exception {\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-\n-    options.setRunner(FlinkRunner.class);\n-\n-    Pipeline pipeline = Pipeline.create(options);\n-    pipeline.getCoderRegistry().registerCoder(URI.class, StringDelegateCoder.of(URI.class));\n-\n-    pipeline\n-        .apply(new ReadDocuments(listInputDocuments(options)))\n-        .apply(new ComputeTfIdf())\n-        .apply(new WriteTfIdf(options.getOutput()));\n-\n-    pipeline.run();\n-  }\n-}",
                "changes": 456
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "deletions": 129,
                "sha": "6ae4cf84352425f3cdfeda76bb96c7e3f022f18f",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "patch": "@@ -1,129 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples;\n-\n-import org.apache.beam.runners.flink.FlinkPipelineOptions;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.options.Validation;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.MapElements;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.SimpleFunction;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-\n-/**\n- * Wordcount pipeline.\n- */\n-public class WordCount {\n-\n-  /**\n-   * Function to extract words.\n-   */\n-  public static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-        createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * PTransform counting words.\n-   */\n-  public static class CountWords extends PTransform<PCollection<String>,\n-                    PCollection<KV<String, Long>>> {\n-    @Override\n-    public PCollection<KV<String, Long>> expand(PCollection<String> lines) {\n-\n-      // Convert lines of text into individual words.\n-      PCollection<String> words = lines.apply(\n-          ParDo.of(new ExtractWordsFn()));\n-\n-      // Count the number of times each word occurs.\n-      PCollection<KV<String, Long>> wordCounts =\n-          words.apply(Count.<String>perElement());\n-\n-      return wordCounts;\n-    }\n-  }\n-\n-  /** A SimpleFunction that converts a Word and Count into a printable string. */\n-  public static class FormatAsTextFn extends SimpleFunction<KV<String, Long>, String> {\n-    @Override\n-    public String apply(KV<String, Long> input) {\n-      return input.getKey() + \": \" + input.getValue();\n-    }\n-  }\n-\n-  /**\n-   * Options supported by {@link WordCount}.\n-   *\n-   * <p>Inherits standard configuration options.\n-   */\n-  public interface Options extends PipelineOptions, FlinkPipelineOptions {\n-    @Description(\"Path of the file to read from\")\n-    String getInput();\n-    void setInput(String value);\n-\n-    @Description(\"Path of the file to write to\")\n-    @Validation.Required\n-    String getOutput();\n-    void setOutput(String value);\n-  }\n-\n-  public static void main(String[] args) {\n-\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation()\n-        .as(Options.class);\n-    options.setRunner(FlinkRunner.class);\n-\n-    Pipeline p = Pipeline.create(options);\n-\n-    p.apply(\"ReadLines\", TextIO.Read.from(options.getInput()))\n-        .apply(new CountWords())\n-        .apply(MapElements.via(new FormatAsTextFn()))\n-        .apply(\"WriteCounts\", TextIO.Write.to(options.getOutput()));\n-\n-    p.run();\n-  }\n-\n-}",
                "changes": 129
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "deletions": 400,
                "sha": "d07df29c004d57c1ee7260c2bfbfd25b5e61060e",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "patch": "@@ -1,400 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedSocketSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.coders.AvroCoder;\n-import org.apache.beam.sdk.coders.DefaultCoder;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.Filter;\n-import org.apache.beam.sdk.transforms.Flatten;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Partition;\n-import org.apache.beam.sdk.transforms.Partition.PartitionFn;\n-import org.apache.beam.sdk.transforms.SerializableFunction;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.Top;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionList;\n-import org.joda.time.Duration;\n-\n-/**\n- * To run the example, first open a socket on a terminal by executing the command:\n- * <ul>\n- *   <li><code>nc -lk 9999</code>\n- * </ul>\n- * and then launch the example. Now whatever you type in the terminal is going to be\n- * the input to the program.\n- * */\n-public class AutoComplete {\n-\n-  /**\n-   * A PTransform that takes as input a list of tokens and returns\n-   * the most common tokens per prefix.\n-   */\n-  public static class ComputeTopCompletions\n-      extends PTransform<PCollection<String>, PCollection<KV<String, List<CompletionCandidate>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int candidatesPerPrefix;\n-    private final boolean recursive;\n-\n-    protected ComputeTopCompletions(int candidatesPerPrefix, boolean recursive) {\n-      this.candidatesPerPrefix = candidatesPerPrefix;\n-      this.recursive = recursive;\n-    }\n-\n-    public static ComputeTopCompletions top(int candidatesPerPrefix, boolean recursive) {\n-      return new ComputeTopCompletions(candidatesPerPrefix, recursive);\n-    }\n-\n-    @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> expand(PCollection<String> input) {\n-      PCollection<CompletionCandidate> candidates = input\n-        // First count how often each token appears.\n-        .apply(Count.<String>perElement())\n-\n-        // Map the KV outputs of Count into our own CompletionCandiate class.\n-        .apply(\"CreateCompletionCandidates\", ParDo.of(\n-            new DoFn<KV<String, Long>, CompletionCandidate>() {\n-              private static final long serialVersionUID = 0;\n-\n-              @ProcessElement\n-              public void processElement(ProcessContext c) {\n-                CompletionCandidate cand = new CompletionCandidate(c.element().getKey(),\n-                    c.element().getValue());\n-                c.output(cand);\n-              }\n-            }));\n-\n-      // Compute the top via either a flat or recursive algorithm.\n-      if (recursive) {\n-        return candidates\n-          .apply(new ComputeTopRecursive(candidatesPerPrefix, 1))\n-          .apply(Flatten.<KV<String, List<CompletionCandidate>>>pCollections());\n-      } else {\n-        return candidates\n-          .apply(new ComputeTopFlat(candidatesPerPrefix, 1));\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Lower latency, but more expensive.\n-   */\n-  private static class ComputeTopFlat\n-      extends PTransform<PCollection<CompletionCandidate>,\n-                         PCollection<KV<String, List<CompletionCandidate>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int candidatesPerPrefix;\n-    private final int minPrefix;\n-\n-    public ComputeTopFlat(int candidatesPerPrefix, int minPrefix) {\n-      this.candidatesPerPrefix = candidatesPerPrefix;\n-      this.minPrefix = minPrefix;\n-    }\n-\n-    @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> expand(\n-        PCollection<CompletionCandidate> input) {\n-      return input\n-        // For each completion candidate, map it to all prefixes.\n-        .apply(ParDo.of(new AllPrefixes(minPrefix)))\n-\n-        // Find and return the top candiates for each prefix.\n-        .apply(Top.<String, CompletionCandidate>largestPerKey(candidatesPerPrefix)\n-             .withHotKeyFanout(new HotKeyFanout()));\n-    }\n-\n-    private static class HotKeyFanout implements SerializableFunction<String, Integer> {\n-      private static final long serialVersionUID = 0;\n-\n-      @Override\n-      public Integer apply(String input) {\n-        return (int) Math.pow(4, 5 - input.length());\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Cheaper but higher latency.\n-   *\n-   * <p>Returns two PCollections, the first is top prefixes of size greater\n-   * than minPrefix, and the second is top prefixes of size exactly\n-   * minPrefix.\n-   */\n-  private static class ComputeTopRecursive\n-      extends PTransform<PCollection<CompletionCandidate>,\n-                         PCollectionList<KV<String, List<CompletionCandidate>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int candidatesPerPrefix;\n-    private final int minPrefix;\n-\n-    public ComputeTopRecursive(int candidatesPerPrefix, int minPrefix) {\n-      this.candidatesPerPrefix = candidatesPerPrefix;\n-      this.minPrefix = minPrefix;\n-    }\n-\n-    private class KeySizePartitionFn implements PartitionFn<KV<String, List<CompletionCandidate>>> {\n-      private static final long serialVersionUID = 0;\n-\n-      @Override\n-      public int partitionFor(KV<String, List<CompletionCandidate>> elem, int numPartitions) {\n-        return elem.getKey().length() > minPrefix ? 0 : 1;\n-      }\n-    }\n-\n-    private static class FlattenTops\n-        extends DoFn<KV<String, List<CompletionCandidate>>, CompletionCandidate> {\n-      private static final long serialVersionUID = 0;\n-\n-      @ProcessElement\n-      public void processElement(ProcessContext c) {\n-        for (CompletionCandidate cc : c.element().getValue()) {\n-          c.output(cc);\n-        }\n-      }\n-    }\n-\n-    @Override\n-    public PCollectionList<KV<String, List<CompletionCandidate>>> expand(\n-          PCollection<CompletionCandidate> input) {\n-        if (minPrefix > 10) {\n-          // Base case, partitioning to return the output in the expected format.\n-          return input\n-            .apply(new ComputeTopFlat(candidatesPerPrefix, minPrefix))\n-            .apply(Partition.of(2, new KeySizePartitionFn()));\n-        } else {\n-          // If a candidate is in the top N for prefix a...b, it must also be in the top\n-          // N for a...bX for every X, which is typlically a much smaller set to consider.\n-          // First, compute the top candidate for prefixes of size at least minPrefix + 1.\n-          PCollectionList<KV<String, List<CompletionCandidate>>> larger = input\n-            .apply(new ComputeTopRecursive(candidatesPerPrefix, minPrefix + 1));\n-          // Consider the top candidates for each prefix of length minPrefix + 1...\n-          PCollection<KV<String, List<CompletionCandidate>>> small =\n-            PCollectionList\n-            .of(larger.get(1).apply(ParDo.of(new FlattenTops())))\n-            // ...together with those (previously excluded) candidates of length\n-            // exactly minPrefix...\n-            .and(input.apply(Filter.by(new SerializableFunction<CompletionCandidate, Boolean>() {\n-              private static final long serialVersionUID = 0;\n-\n-              @Override\n-              public Boolean apply(CompletionCandidate c) {\n-                return c.getValue().length() == minPrefix;\n-              }\n-            })))\n-            .apply(\"FlattenSmall\", Flatten.<CompletionCandidate>pCollections())\n-            // ...set the key to be the minPrefix-length prefix...\n-            .apply(ParDo.of(new AllPrefixes(minPrefix, minPrefix)))\n-            // ...and (re)apply the Top operator to all of them together.\n-            .apply(Top.<String, CompletionCandidate>largestPerKey(candidatesPerPrefix));\n-\n-          PCollection<KV<String, List<CompletionCandidate>>> flattenLarger = larger\n-              .apply(\"FlattenLarge\", Flatten.<KV<String, List<CompletionCandidate>>>pCollections());\n-\n-          return PCollectionList.of(flattenLarger).and(small);\n-        }\n-    }\n-  }\n-\n-  /**\n-   * A DoFn that keys each candidate by all its prefixes.\n-   */\n-  private static class AllPrefixes\n-      extends DoFn<CompletionCandidate, KV<String, CompletionCandidate>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int minPrefix;\n-    private final int maxPrefix;\n-    public AllPrefixes(int minPrefix) {\n-      this(minPrefix, Integer.MAX_VALUE);\n-    }\n-    public AllPrefixes(int minPrefix, int maxPrefix) {\n-      this.minPrefix = minPrefix;\n-      this.maxPrefix = maxPrefix;\n-    }\n-    @ProcessElement\n-      public void processElement(ProcessContext c) {\n-      String word = c.element().value;\n-      for (int i = minPrefix; i <= Math.min(word.length(), maxPrefix); i++) {\n-        KV<String, CompletionCandidate> kv = KV.of(word.substring(0, i), c.element());\n-        c.output(kv);\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Class used to store tag-count pairs.\n-   */\n-  @DefaultCoder(AvroCoder.class)\n-  static class CompletionCandidate implements Comparable<CompletionCandidate> {\n-    private long count;\n-    private String value;\n-\n-    public CompletionCandidate(String value, long count) {\n-      this.value = value;\n-      this.count = count;\n-    }\n-\n-    public String getValue() {\n-      return value;\n-    }\n-\n-    // Empty constructor required for Avro decoding.\n-    @SuppressWarnings(\"unused\")\n-    public CompletionCandidate() {}\n-\n-    @Override\n-    public int compareTo(CompletionCandidate o) {\n-      if (this.count < o.count) {\n-        return -1;\n-      } else if (this.count == o.count) {\n-        return this.value.compareTo(o.value);\n-      } else {\n-        return 1;\n-      }\n-    }\n-\n-    @Override\n-    public boolean equals(Object other) {\n-      if (other instanceof CompletionCandidate) {\n-        CompletionCandidate that = (CompletionCandidate) other;\n-        return this.count == that.count && this.value.equals(that.value);\n-      } else {\n-        return false;\n-      }\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-      return Long.valueOf(count).hashCode() ^ value.hashCode();\n-    }\n-\n-    @Override\n-    public String toString() {\n-      return \"CompletionCandidate[\" + value + \", \" + count + \"]\";\n-    }\n-  }\n-\n-  static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-            createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Takes as input a the top candidates per prefix, and emits an entity suitable for writing to\n-   * Datastore.\n-   */\n-  static class FormatForPerTaskLocalFile\n-      extends DoFn<KV<String, List<CompletionCandidate>>, String> {\n-\n-    private static final long serialVersionUID = 0;\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c, BoundedWindow window) {\n-      StringBuilder str = new StringBuilder();\n-      KV<String, List<CompletionCandidate>> elem = c.element();\n-\n-      str.append(elem.getKey() + \" @ \" + window + \" -> \");\n-      for (CompletionCandidate cand: elem.getValue()) {\n-        str.append(cand.toString() + \" \");\n-      }\n-      System.out.println(str.toString());\n-      c.output(str.toString());\n-    }\n-  }\n-\n-  /**\n-   * Options supported by this class.\n-   *\n-   * <p>Inherits standard Dataflow configuration options.\n-   */\n-  private interface Options extends WindowedWordCount.StreamingWordCountOptions {\n-    @Description(\"Whether to use the recursive algorithm\")\n-    @Default.Boolean(true)\n-    Boolean getRecursive();\n-    void setRecursive(Boolean value);\n-  }\n-\n-  public static void main(String[] args) throws IOException {\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-    options.setStreaming(true);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-\n-    WindowFn<Object, ?> windowFn =\n-        FixedWindows.of(Duration.standardSeconds(options.getWindowSize()));\n-\n-    // Create the pipeline.\n-    Pipeline p = Pipeline.create(options);\n-    PCollection<KV<String, List<CompletionCandidate>>> toWrite = p\n-      .apply(\"WordStream\", Read.from(new UnboundedSocketSource<>(\"localhost\", 9999, '\\n', 3)))\n-      .apply(ParDo.of(new ExtractWordsFn()))\n-      .apply(Window.<String>into(windowFn)\n-              .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes())\n-      .apply(ComputeTopCompletions.top(10, options.getRecursive()));\n-\n-    toWrite\n-      .apply(\"FormatForPerTaskFile\", ParDo.of(new FormatForPerTaskLocalFile()))\n-      .apply(TextIO.Write.to(\"./outputAutoComplete.txt\"));\n-\n-    p.run();\n-  }\n-}",
                "changes": 400
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java",
                "deletions": 154,
                "sha": "8fefc9f2a529e12e158c4b34de40d44831826d10",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java",
                "patch": "@@ -1,154 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedSocketSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.join.CoGbkResult;\n-import org.apache.beam.sdk.transforms.join.CoGroupByKey;\n-import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.joda.time.Duration;\n-\n-/**\n- * To run the example, first open two sockets on two terminals by executing the commands:\n- * <ul>\n- *   <li><code>nc -lk 9999</code>, and\n- *   <li><code>nc -lk 9998</code>\n- * </ul>\n- * and then launch the example. Now whatever you type in the terminal is going to be\n- * the input to the program.\n- * */\n-public class JoinExamples {\n-\n-  static PCollection<String> joinEvents(PCollection<String> streamA,\n-                      PCollection<String> streamB) throws Exception {\n-\n-    final TupleTag<String> firstInfoTag = new TupleTag<>();\n-    final TupleTag<String> secondInfoTag = new TupleTag<>();\n-\n-    // transform both input collections to tuple collections, where the keys are country\n-    // codes in both cases.\n-    PCollection<KV<String, String>> firstInfo = streamA.apply(\n-        ParDo.of(new ExtractEventDataFn()));\n-    PCollection<KV<String, String>> secondInfo = streamB.apply(\n-        ParDo.of(new ExtractEventDataFn()));\n-\n-    // country code 'key' -> CGBKR (<event info>, <country name>)\n-    PCollection<KV<String, CoGbkResult>> kvpCollection = KeyedPCollectionTuple\n-        .of(firstInfoTag, firstInfo)\n-        .and(secondInfoTag, secondInfo)\n-        .apply(CoGroupByKey.<String>create());\n-\n-    // Process the CoGbkResult elements generated by the CoGroupByKey transform.\n-    // country code 'key' -> string of <event info>, <country name>\n-    PCollection<KV<String, String>> finalResultCollection =\n-        kvpCollection.apply(\"Process\", ParDo.of(\n-            new DoFn<KV<String, CoGbkResult>, KV<String, String>>() {\n-              private static final long serialVersionUID = 0;\n-\n-              @ProcessElement\n-              public void processElement(ProcessContext c) {\n-                KV<String, CoGbkResult> e = c.element();\n-                String key = e.getKey();\n-\n-                String defaultA = \"NO_VALUE\";\n-\n-                // the following getOnly is a bit tricky because it expects to have\n-                // EXACTLY ONE value in the corresponding stream and for the corresponding key.\n-\n-                String lineA = e.getValue().getOnly(firstInfoTag, defaultA);\n-                for (String lineB : c.element().getValue().getAll(secondInfoTag)) {\n-                  // Generate a string that combines information from both collection values\n-                  c.output(KV.of(key, \"Value A: \" + lineA + \" - Value B: \" + lineB));\n-                }\n-              }\n-            }));\n-\n-    return finalResultCollection\n-        .apply(\"Format\", ParDo.of(new DoFn<KV<String, String>, String>() {\n-          private static final long serialVersionUID = 0;\n-\n-          @ProcessElement\n-          public void processElement(ProcessContext c) {\n-            String result = c.element().getKey() + \" -> \" + c.element().getValue();\n-            System.out.println(result);\n-            c.output(result);\n-          }\n-        }));\n-  }\n-\n-  static class ExtractEventDataFn extends DoFn<String, KV<String, String>> {\n-    private static final long serialVersionUID = 0;\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      String line = c.element().toLowerCase();\n-      String key = line.split(\"\\\\s\")[0];\n-      c.output(KV.of(key, line));\n-    }\n-  }\n-\n-  private interface Options extends WindowedWordCount.StreamingWordCountOptions {\n-\n-  }\n-\n-  public static void main(String[] args) throws Exception {\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-    options.setStreaming(true);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-    WindowFn<Object, ?> windowFn = FixedWindows.of(\n-        Duration.standardSeconds(options.getWindowSize()));\n-\n-    Pipeline p = Pipeline.create(options);\n-\n-    // the following two 'applys' create multiple inputs to our pipeline, one for each\n-    // of our two input sources.\n-    PCollection<String> streamA = p\n-        .apply(\"FirstStream\", Read.from(new UnboundedSocketSource<>(\"localhost\", 9999, '\\n', 3)))\n-        .apply(Window.<String>into(windowFn)\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-    PCollection<String> streamB = p\n-        .apply(\"SecondStream\", Read.from(new UnboundedSocketSource<>(\"localhost\", 9998, '\\n', 3)))\n-        .apply(Window.<String>into(windowFn)\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-\n-    PCollection<String> formattedResults = joinEvents(streamA, streamB);\n-    formattedResults.apply(TextIO.Write.to(\"./outputJoin.txt\"));\n-    p.run();\n-  }\n-\n-}",
                "changes": 154
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java",
                "deletions": 338,
                "sha": "616e276bab75877db9875b280b84f42ed013a18e",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java",
                "patch": "@@ -1,338 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.io.ByteArrayInputStream;\n-import java.io.ByteArrayOutputStream;\n-import java.io.IOException;\n-import java.io.Serializable;\n-import java.util.Properties;\n-import org.apache.beam.runners.flink.FlinkPipelineOptions;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedFlinkSink;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedFlinkSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.coders.AvroCoder;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.Write;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.TypeExtractor;\n-import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08;\n-import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer08;\n-import org.apache.flink.streaming.util.serialization.DeserializationSchema;\n-import org.apache.flink.streaming.util.serialization.SerializationSchema;\n-import org.apache.flink.streaming.util.serialization.SimpleStringSchema;\n-\n-/**\n- * Recipes/Examples that demonstrate how to read/write data from/to Kafka.\n- */\n-public class KafkaIOExamples {\n-\n-\n-  private static final String KAFKA_TOPIC = \"input\";  // Default kafka topic to read from\n-  private static final String KAFKA_AVRO_TOPIC = \"output\";  // Default kafka topic to read from\n-  private static final String KAFKA_BROKER = \"localhost:9092\";  // Default kafka broker to contact\n-  private static final String GROUP_ID = \"myGroup\";  // Default groupId\n-  private static final String ZOOKEEPER = \"localhost:2181\";  // Default zookeeper to connect (Kafka)\n-\n-  /**\n-   * Read/Write String data to Kafka.\n-   */\n-  public static class KafkaString {\n-\n-    /**\n-     * Read String data from Kafka.\n-     */\n-    public static class ReadStringFromKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        FlinkKafkaConsumer08<String> kafkaConsumer =\n-            new FlinkKafkaConsumer08<>(options.getKafkaTopic(),\n-                new SimpleStringSchema(), getKafkaProps(options));\n-\n-        p\n-            .apply(Read.from(UnboundedFlinkSource.of(kafkaConsumer))).setCoder(StringUtf8Coder.of())\n-            .apply(ParDo.of(new PrintFn<>()));\n-\n-        p.run();\n-\n-      }\n-\n-    }\n-\n-    /**\n-     * Write String data to Kafka.\n-     */\n-    public static class WriteStringToKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        PCollection<String> words =\n-            p.apply(Create.of(\"These\", \"are\", \"some\", \"words\"));\n-\n-        FlinkKafkaProducer08<String> kafkaSink =\n-            new FlinkKafkaProducer08<>(options.getKafkaTopic(),\n-                new SimpleStringSchema(), getKafkaProps(options));\n-\n-        words.apply(Write.to(UnboundedFlinkSink.of(kafkaSink)));\n-\n-        p.run();\n-      }\n-\n-    }\n-  }\n-\n-  /**\n-   * Read/Write Avro data to Kafka.\n-   */\n-  public static class KafkaAvro {\n-\n-    /**\n-     * Read Avro data from Kafka.\n-     */\n-    public static class ReadAvroFromKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        FlinkKafkaConsumer08<MyType> kafkaConsumer =\n-            new FlinkKafkaConsumer08<>(options.getKafkaAvroTopic(),\n-                new AvroSerializationDeserializationSchema<>(MyType.class), getKafkaProps(options));\n-\n-        p\n-            .apply(Read.from(UnboundedFlinkSource.of(kafkaConsumer)))\n-                .setCoder(AvroCoder.of(MyType.class))\n-            .apply(ParDo.of(new PrintFn<>()));\n-\n-        p.run();\n-\n-      }\n-\n-    }\n-\n-    /**\n-     * Write Avro data to Kafka.\n-     */\n-    public static class WriteAvroToKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        PCollection<MyType> words =\n-            p.apply(Create.of(\n-                new MyType(\"word\", 1L),\n-                new MyType(\"another\", 2L),\n-                new MyType(\"yet another\", 3L)));\n-\n-        FlinkKafkaProducer08<MyType> kafkaSink =\n-            new FlinkKafkaProducer08<>(options.getKafkaAvroTopic(),\n-                new AvroSerializationDeserializationSchema<>(MyType.class), getKafkaProps(options));\n-\n-        words.apply(Write.to(UnboundedFlinkSink.of(kafkaSink)));\n-\n-        p.run();\n-\n-      }\n-    }\n-\n-    /**\n-     * Serialiation/Deserialiation schema for Avro types.\n-     * @param <T> the type being encoded\n-     */\n-    static class AvroSerializationDeserializationSchema<T>\n-        implements SerializationSchema<T>, DeserializationSchema<T> {\n-\n-      private final Class<T> avroType;\n-\n-      private final AvroCoder<T> coder;\n-      private transient ByteArrayOutputStream out;\n-\n-      AvroSerializationDeserializationSchema(Class<T> clazz) {\n-        this.avroType = clazz;\n-        this.coder = AvroCoder.of(clazz);\n-        this.out = new ByteArrayOutputStream();\n-      }\n-\n-      @Override\n-      public byte[] serialize(T element) {\n-        if (out == null) {\n-          out = new ByteArrayOutputStream();\n-        }\n-        try {\n-          out.reset();\n-          coder.encode(element, out, Coder.Context.NESTED);\n-        } catch (IOException e) {\n-          throw new RuntimeException(\"Avro encoding failed.\", e);\n-        }\n-        return out.toByteArray();\n-      }\n-\n-      @Override\n-      public T deserialize(byte[] message) throws IOException {\n-        return coder.decode(new ByteArrayInputStream(message), Coder.Context.NESTED);\n-      }\n-\n-      @Override\n-      public boolean isEndOfStream(T nextElement) {\n-        return false;\n-      }\n-\n-      @Override\n-      public TypeInformation<T> getProducedType() {\n-        return TypeExtractor.getForClass(avroType);\n-      }\n-    }\n-\n-    /**\n-     * Custom type for Avro serialization.\n-     */\n-    static class MyType implements Serializable {\n-\n-      public MyType() {}\n-\n-      MyType(String word, long count) {\n-        this.word = word;\n-        this.count = count;\n-      }\n-\n-      String word;\n-      long count;\n-\n-      @Override\n-      public String toString() {\n-        return \"MyType{\"\n-            + \"word='\" + word + '\\''\n-            + \", count=\" + count\n-            + '}';\n-      }\n-    }\n-  }\n-\n-  // -------------- Utilities --------------\n-\n-  /**\n-   * Custom options for the Pipeline.\n-   */\n-  public interface KafkaOptions extends FlinkPipelineOptions {\n-    @Description(\"The Kafka topic to read from\")\n-    @Default.String(KAFKA_TOPIC)\n-    String getKafkaTopic();\n-\n-    void setKafkaTopic(String value);\n-\n-    void setKafkaAvroTopic(String value);\n-\n-    @Description(\"The Kafka topic to read from\")\n-    @Default.String(KAFKA_AVRO_TOPIC)\n-    String getKafkaAvroTopic();\n-\n-    @Description(\"The Kafka Broker to read from\")\n-    @Default.String(KAFKA_BROKER)\n-    String getBroker();\n-\n-    void setBroker(String value);\n-\n-    @Description(\"The Zookeeper server to connect to\")\n-    @Default.String(ZOOKEEPER)\n-    String getZookeeper();\n-\n-    void setZookeeper(String value);\n-\n-    @Description(\"The groupId\")\n-    @Default.String(GROUP_ID)\n-    String getGroup();\n-\n-    void setGroup(String value);\n-  }\n-\n-  /**\n-   * Initializes some options for the Flink runner.\n-   * @param args The command line args\n-   * @return the pipeline\n-   */\n-  private static Pipeline initializePipeline(String[] args) {\n-    KafkaOptions options =\n-        PipelineOptionsFactory.fromArgs(args).as(KafkaOptions.class);\n-\n-    options.setStreaming(true);\n-    options.setRunner(FlinkRunner.class);\n-\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-\n-    return Pipeline.create(options);\n-  }\n-\n-  /**\n-   * Gets KafkaOptions from the Pipeline.\n-   * @param p the pipeline\n-   * @return KafkaOptions\n-   */\n-  private static KafkaOptions getOptions(Pipeline p) {\n-    return p.getOptions().as(KafkaOptions.class);\n-  }\n-\n-  /**\n-   * Helper method to set the Kafka props from the pipeline options.\n-   * @param options KafkaOptions\n-   * @return Kafka props\n-   */\n-  private static Properties getKafkaProps(KafkaOptions options) {\n-\n-    Properties props = new Properties();\n-    props.setProperty(\"zookeeper.connect\", options.getZookeeper());\n-    props.setProperty(\"bootstrap.servers\", options.getBroker());\n-    props.setProperty(\"group.id\", options.getGroup());\n-\n-    return props;\n-  }\n-\n-  /**\n-   * Print contents to stdout.\n-   * @param <T> type of the input\n-   */\n-  private static class PrintFn<T> extends DoFn<T, T> {\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) throws Exception {\n-      System.out.println(c.element().toString());\n-    }\n-  }\n-\n-}",
                "changes": 338
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java",
                "deletions": 164,
                "sha": "ee0e874b713a40cef19695c0f6fe675019b0de4f",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java",
                "patch": "@@ -1,164 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.util.Properties;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedFlinkSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08;\n-import org.apache.flink.streaming.util.serialization.SimpleStringSchema;\n-import org.joda.time.Duration;\n-\n-/**\n- * Wordcount example using Kafka topic.\n- */\n-public class KafkaWindowedWordCountExample {\n-\n-  static final String KAFKA_TOPIC = \"test\";  // Default kafka topic to read from\n-  static final String KAFKA_BROKER = \"localhost:9092\";  // Default kafka broker to contact\n-  static final String GROUP_ID = \"myGroup\";  // Default groupId\n-  static final String ZOOKEEPER = \"localhost:2181\";  // Default zookeeper to connect to for Kafka\n-\n-  /**\n-   * Function to extract words.\n-   */\n-  public static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-        createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Function to format KV as String.\n-   */\n-  public static class FormatAsStringFn extends DoFn<KV<String, Long>, String> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      String row = c.element().getKey() + \" - \" + c.element().getValue() + \" @ \"\n-          + c.timestamp().toString();\n-      System.out.println(row);\n-      c.output(row);\n-    }\n-  }\n-\n-  /**\n-   * Pipeline options.\n-   */\n-  public interface KafkaStreamingWordCountOptions\n-      extends WindowedWordCount.StreamingWordCountOptions {\n-    @Description(\"The Kafka topic to read from\")\n-    @Default.String(KAFKA_TOPIC)\n-    String getKafkaTopic();\n-\n-    void setKafkaTopic(String value);\n-\n-    @Description(\"The Kafka Broker to read from\")\n-    @Default.String(KAFKA_BROKER)\n-    String getBroker();\n-\n-    void setBroker(String value);\n-\n-    @Description(\"The Zookeeper server to connect to\")\n-    @Default.String(ZOOKEEPER)\n-    String getZookeeper();\n-\n-    void setZookeeper(String value);\n-\n-    @Description(\"The groupId\")\n-    @Default.String(GROUP_ID)\n-    String getGroup();\n-\n-    void setGroup(String value);\n-\n-  }\n-\n-  public static void main(String[] args) {\n-    PipelineOptionsFactory.register(KafkaStreamingWordCountOptions.class);\n-    KafkaStreamingWordCountOptions options = PipelineOptionsFactory.fromArgs(args)\n-        .as(KafkaStreamingWordCountOptions.class);\n-    options.setJobName(\"KafkaExample - WindowSize: \" + options.getWindowSize() + \" seconds\");\n-    options.setStreaming(true);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-    System.out.println(options.getKafkaTopic() + \" \" + options.getZookeeper() + \" \"\n-        + options.getBroker() + \" \" + options.getGroup());\n-    Pipeline pipeline = Pipeline.create(options);\n-\n-    Properties p = new Properties();\n-    p.setProperty(\"zookeeper.connect\", options.getZookeeper());\n-    p.setProperty(\"bootstrap.servers\", options.getBroker());\n-    p.setProperty(\"group.id\", options.getGroup());\n-\n-    // this is the Flink consumer that reads the input to\n-    // the program from a kafka topic.\n-    FlinkKafkaConsumer08<String> kafkaConsumer = new FlinkKafkaConsumer08<>(\n-        options.getKafkaTopic(),\n-        new SimpleStringSchema(), p);\n-\n-    PCollection<String> words = pipeline\n-        .apply(\"StreamingWordCount\", Read.from(UnboundedFlinkSource.of(kafkaConsumer)))\n-        .apply(ParDo.of(new ExtractWordsFn()))\n-        .apply(Window.<String>into(FixedWindows.of(\n-            Duration.standardSeconds(options.getWindowSize())))\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-\n-    PCollection<KV<String, Long>> wordCounts =\n-        words.apply(Count.<String>perElement());\n-\n-    wordCounts.apply(ParDo.of(new FormatAsStringFn()))\n-        .apply(TextIO.Write.to(\"./outputKafka.txt\"));\n-\n-    pipeline.run();\n-  }\n-}",
                "changes": 164
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java",
                "deletions": 141,
                "sha": "792c214acc8960b8c3eceebe55061ebaf0616457",
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java",
                "patch": "@@ -1,141 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.io.IOException;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedSocketSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.SlidingWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.joda.time.Duration;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * To run the example, first open a socket on a terminal by executing the command:\n- * <ul>\n- *   <li><code>nc -lk 9999</code>\n- * </ul>\n- * and then launch the example. Now whatever you type in the terminal is going to be\n- * the input to the program.\n- * */\n-public class WindowedWordCount {\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(WindowedWordCount.class);\n-\n-  static final long WINDOW_SIZE = 10;  // Default window duration in seconds\n-  static final long SLIDE_SIZE = 5;  // Default window slide in seconds\n-\n-  static class FormatAsStringFn extends DoFn<KV<String, Long>, String> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      String row = c.element().getKey() + \" - \" + c.element().getValue() + \" @ \"\n-          + c.timestamp().toString();\n-      c.output(row);\n-    }\n-  }\n-\n-  static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-        createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Pipeline options.\n-   */\n-  public interface StreamingWordCountOptions\n-      extends org.apache.beam.runners.flink.examples.WordCount.Options {\n-    @Description(\"Sliding window duration, in seconds\")\n-    @Default.Long(WINDOW_SIZE)\n-    Long getWindowSize();\n-\n-    void setWindowSize(Long value);\n-\n-    @Description(\"Window slide, in seconds\")\n-    @Default.Long(SLIDE_SIZE)\n-    Long getSlide();\n-\n-    void setSlide(Long value);\n-  }\n-\n-  public static void main(String[] args) throws IOException {\n-    StreamingWordCountOptions options = PipelineOptionsFactory.fromArgs(args).withValidation()\n-        .as(StreamingWordCountOptions.class);\n-    options.setStreaming(true);\n-    options.setWindowSize(10L);\n-    options.setSlide(5L);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-    LOG.info(\"Windpwed WordCount with Sliding Windows of \" + options.getWindowSize()\n-        + \" sec. and a slide of \" + options.getSlide());\n-\n-    Pipeline pipeline = Pipeline.create(options);\n-\n-    PCollection<String> words = pipeline\n-        .apply(\"StreamingWordCount\",\n-            Read.from(new UnboundedSocketSource<>(\"localhost\", 9999, '\\n', 3)))\n-        .apply(ParDo.of(new ExtractWordsFn()))\n-        .apply(Window.<String>into(SlidingWindows.of(\n-            Duration.standardSeconds(options.getWindowSize()))\n-            .every(Duration.standardSeconds(options.getSlide())))\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-\n-    PCollection<KV<String, Long>> wordCounts =\n-        words.apply(Count.<String>perElement());\n-\n-    wordCounts.apply(ParDo.of(new FormatAsStringFn()))\n-        .apply(TextIO.Write.to(\"./outputWordCount.txt\"));\n-\n-    pipeline.run();\n-  }\n-}",
                "changes": 141
            },
            {
                "status": "modified",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/pom.xml",
                "deletions": 0,
                "sha": "808219b7457367704576ad90f9d35051dea528f2",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/pom.xml"
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/pom.xml?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/runner/pom.xml",
                "deletions": 0,
                "sha": "f2c2d018174554f4a9f488e8000a3f1c34c9e9d9",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/pom.xml"
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java",
                "deletions": 0,
                "sha": "8b90c73a26fbd2537381097ce92aff3697567604",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java"
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java",
                "deletions": 0,
                "sha": "301d84171901fd70ad2d8eab103a49118e26f803",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java"
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java",
                "deletions": 0,
                "sha": "ac20c34ff204f5dbfd98bce3f6cec23c3a424398",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "deletions": 0,
                "sha": "b745f0bd441acb8fdc7cc5b5f4c0a1223c23c4bc",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "deletions": 0,
                "sha": "854b67460ae2872cd2e2c2101de90cdf8e371c92",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "deletions": 0,
                "sha": "ff9521c009f21a44054395bf693b60cac3288269",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "deletions": 0,
                "sha": "98dd0fb858ebb806557498872b5e1a86fb5fa3bc",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "deletions": 0,
                "sha": "bf4395fcdcca1b559bd1ad7241b239b61704cc97",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "deletions": 0,
                "sha": "ba00036f89e3ceb6a351d5e77677fb0bc401124c",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "deletions": 0,
                "sha": "ef9afeaed52b5bfe1f828f294171bf92b4300d02",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "deletions": 0,
                "sha": "65f416d94fc1309c747c62177dd889647626de16",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "deletions": 0,
                "sha": "096f0302a387a493fd44110d86cce808d6384d74",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "deletions": 0,
                "sha": "681459a819d745bad2c55649aff72db55f8a0cd2",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "deletions": 0,
                "sha": "0682b5695f74b8bf9bd6bd80279215b43e98329b",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "deletions": 0,
                "sha": "0459ef775166586b7766dc50425995dee0098023",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "deletions": 0,
                "sha": "123d5e720b7b5be4b8c2ee3cac182aa94391b6df",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "deletions": 0,
                "sha": "1a943a3dbb1b910437c83bbb40da3d9cab17e42f",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "deletions": 0,
                "sha": "f955f2a573ff4aadc6cba4d5febab68119dc75af",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "deletions": 0,
                "sha": "3acc3eafca13a02dbd24797d1b733d504aed8da3",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "deletions": 0,
                "sha": "8f50105a55b98869ed239c7e4d1a15839ec48381",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "deletions": 0,
                "sha": "ad547506f5658394ce728a24bff64ff62569f3af",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "deletions": 0,
                "sha": "57f1e599ee50250e5fffbe75c5c308955bfbf069",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "deletions": 0,
                "sha": "fb2493bbe7e1624ac429b421c175c30a2ead3a44",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "deletions": 0,
                "sha": "447b1e507e1a5f0847965cc4b9ea43633ad50dff",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "deletions": 0,
                "sha": "c3a5095bc1eb61db1eb99d183f91e7026f8b984d",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "deletions": 0,
                "sha": "51582afda7fbc3fcc4442a428fea8d6a17c6efa8",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "deletions": 0,
                "sha": "26fd0b4f784562558ad202a71bf2df4197795cc4",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "deletions": 0,
                "sha": "c68f155d35557b700e512c6a94bd55951665f5f7",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "deletions": 0,
                "sha": "84b3adc38756902ed50a0b104fb126c1e2d47a7d",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "deletions": 0,
                "sha": "9071cc52ac1d409ddb1b56346dd5669ed5a6eeeb",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "deletions": 0,
                "sha": "847a00abdd82a8b7550ba620bde9fe66c76e1f6a",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "deletions": 0,
                "sha": "1d1ff9fc7ca9d5d215b0090375eacf9f97cfc0f7",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "deletions": 0,
                "sha": "3e4f742dfb0a6cd855eb600747345f8957cbc003",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "deletions": 0,
                "sha": "c317182ffde949be7ccf77bbedc4b018787e180d",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "deletions": 0,
                "sha": "c8193d29c9f134db723102ba2cebc8ca24bbea4f",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "deletions": 0,
                "sha": "12222b499d74616f469ddd06f08d3334a23327aa",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "deletions": 0,
                "sha": "9f1121225b8814e9ce509776c99e10a6441a46c9",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "deletions": 0,
                "sha": "af4b35491ba41f9bffd1110eed0f30bb0494cd6f",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "deletions": 0,
                "sha": "9b449aabc8b59f287725ccd444cec4deace21886",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "deletions": 0,
                "sha": "e210ed9d7b982c660e884483e8ae68fa139be936",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java"
            },
            {
                "status": "renamed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "deletions": 0,
                "sha": "667ef4591a3f0f4a9acac976a24e124f9aa87262",
                "changes": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java"
            }
        ],
        "unit_tests": [
            "PTransformsTest.java",
            "DirectGraphVisitorTest.java",
            "LeaderBoardTest.java",
            "TopWikipediaSessionsTest.java",
            "ViewOverrideFactoryTest.java",
            "SideInputContainerTest.java",
            "FlinkRunnerRegistrarTest.java",
            "SimplePushbackSideInputDoFnRunnerTest.java",
            "FlattenPCollectionTranslatorTest.java",
            "SimpleDoFnRunnerTest.java",
            "StateTagTest.java",
            "AutoCompleteTest.java",
            "ReplacementOutputsTest.java",
            "FlattenEvaluatorFactoryTest.java",
            "PCollectionsTest.java",
            "TestApexRunner.java",
            "TriggerExampleTest.java",
            "EncodedValueComparatorTest.java",
            "TriggersTest.java",
            "SimpleOldDoFnRunnerTest.java",
            "WindowAssignTranslatorTest.java",
            "UnboundedReadFromBoundedSourceTest.java",
            "HourlyTeamScoreTest.java",
            "KeyedPValueTrackingVisitorTest.java",
            "DistinctExampleTest.java",
            "GroupByKeyTranslatorTest.java",
            "DirectMetricsTest.java",
            "ParDoEvaluatorTest.java",
            "DeduplicatedFlattenFactoryTest.java",
            "BoundedReadEvaluatorFactoryTest.java",
            "UnconsumedReadsTest.java",
            "TfIdfTest.java",
            "OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
            "StatefulDoFnRunnerTest.java",
            "TestInMemoryStateInternals.java",
            "PTransformReplacementsTest.java",
            "EmptyFlattenAsCreateFactoryTest.java",
            "MinimalWordCountJava8Test.java",
            "WindowingStrategiesTest.java",
            "PTransformMatchersTest.java",
            "GroupAlsoByWindowViaOutputBufferDoFnTest.java",
            "SplittableParDoTest.java",
            "CodersTest.java",
            "UnsupportedOverrideFactoryTest.java",
            "SingleInputOutputOverrideFactoryTest.java",
            "ReduceFnRunnerTest.java",
            "ParDoTranslatorTest.java",
            "JoinExamplesTest.java",
            "OldDoFnTest.java",
            "GroupByKeyOnlyEvaluatorFactoryTest.java",
            "DirectGroupByKeyOverrideFactoryTest.java",
            "TestStreamEvaluatorFactoryTest.java",
            "UnboundedReadEvaluatorFactoryTest.java",
            "WindowEvaluatorFactoryTest.java",
            "WriteWithShardingFactoryTest.java",
            "AfterPaneStateMachineTest.java",
            "DirectRunnerTest.java",
            "ApexRunnerTest.java",
            "SdkComponentsTest.java",
            "AfterWatermarkStateMachineTest.java",
            "FilterExamplesTest.java",
            "EvaluationContextTest.java",
            "TransformExecutorServicesTest.java",
            "ViewEvaluatorFactoryTest.java",
            "ParDoSingleViaMultiOverrideFactoryTest.java",
            "CopyOnAccessInMemoryStateInternalsTest.java",
            "DebuggingWordCountTest.java",
            "ApexGroupByKeyOperatorTest.java",
            "DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
            "TestStreamEvaluatorFactory.java",
            "BigQueryTornadoesTest.java",
            "ApexYarnLauncherTest.java",
            "InMemoryStateInternalsTest.java",
            "SideInputHandlerTest.java",
            "UserScoreTest.java",
            "WordCountTest.java",
            "TestFlinkRunner.java",
            "CombinePerKeyExamplesTest.java",
            "StatefulParDoEvaluatorFactoryTest.java",
            "WatermarkManagerTest.java",
            "MaxPerKeyExamplesTest.java",
            "ApexStateInternalsTest.java",
            "GameStatsTest.java",
            "ForwardingPTransformTest.java"
        ]
    },
    {
        "buggy": false,
        "test_file": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/LeaderBoardTest.java",
        "buggy_files": [
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnRunner.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/TranslationContext.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/GroupByKeyTranslator.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/options/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/values/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/runners/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/coders/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/coders/protobuf/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/annotations/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/util/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/util/state/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/util/gcsfs/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/util/common/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/windowing/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/display/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/splittabledofn/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/reflect/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/io/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/io/fs/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/package-info.java",
            "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/runners/core/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/control/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/logging/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/fn/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/fake/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/data/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/channel/package-info.java",
            "sdks/java/harness/src/main/java/org/apache/beam/fn/harness/stream/package-info.java",
            "sdks/java/io/jdbc/src/main/java/org/apache/beam/sdk/io/jdbc/package-info.java",
            "sdks/java/io/elasticsearch/src/main/java/org/apache/beam/sdk/io/elasticsearch/package-info.java",
            "sdks/java/io/mongodb/src/main/java/org/apache/beam/sdk/io/mongodb/package-info.java",
            "sdks/java/io/hdfs/src/main/java/org/apache/beam/sdk/io/hdfs/package-info.java",
            "sdks/java/io/hadoop-common/src/main/java/org/apache/beam/sdk/io/hadoop/package-info.java",
            "sdks/java/io/kinesis/src/main/java/org/apache/beam/sdk/io/kinesis/package-info.java",
            "sdks/java/io/hadoop/input-format/src/main/java/org/apache/beam/sdk/io/hadoop/inputformat/package-info.java",
            "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/package-info.java",
            "sdks/java/io/hbase/src/main/java/org/apache/beam/sdk/io/hbase/package-info.java",
            "sdks/java/io/mqtt/src/main/java/org/apache/beam/sdk/io/mqtt/package-info.java",
            "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigtable/package-info.java",
            "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/storage/package-info.java",
            "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/package-info.java",
            "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/package-info.java",
            "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/package-info.java",
            "sdks/java/io/jms/src/main/java/org/apache/beam/sdk/io/jms/package-info.java",
            "sdks/java/extensions/gcp-core/src/main/java/org/apache/beam/sdk/options/package-info.java",
            "sdks/java/extensions/gcp-core/src/main/java/org/apache/beam/sdk/util/package-info.java",
            "sdks/java/extensions/gcp-core/src/main/java/org/apache/beam/sdk/testing/package-info.java",
            "sdks/java/extensions/jackson/src/main/java/org/apache/beam/sdk/extensions/jackson/package-info.java",
            "sdks/java/extensions/join-library/src/main/java/org/apache/beam/sdk/extensions/joinlibrary/package-info.java",
            "sdks/java/extensions/sorter/src/main/java/org/apache/beam/sdk/extensions/sorter/package-info.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/package-info.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/package-info.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/package-info.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/package-info.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/package-info.java",
            "runners/apex/src/test/java/org/apache/beam/runners/apex/examples/package-info.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/package-info.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/package-info.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/package-info.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/functions/package-info.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/io/package-info.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/package-info.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/utils/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/state/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/package-info.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/package-info.java",
            "runners/flink/src/test/java/org/apache/beam/runners/flink/streaming/package-info.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/package-info.java",
            "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/package-info.java",
            "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/package-info.java",
            "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/package-info.java",
            "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/package-info.java",
            "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/testing/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/coders/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/util/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/io/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/metrics/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/sink/package-info.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/stateful/package-info.java",
            "runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/examples/WordCount.java",
            "examples/java/src/main/java/org/apache/beam/examples/WordCount.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/NoOpStepContext.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/WindowAssignTranslator.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
            "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
            "runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
            "README.md",
            "sdks/python/README.md",
            "sdks/python/target/build/lib.linux-x86_64-2.7/apache_beam/tests/data/README.md",
            "sdks/python/apache_beam/tests/data/README.md",
            "sdks/python/apache_beam/examples/complete/game/README.md",
            "sdks/java/core/src/main/proto/README.md",
            "sdks/java/io/jdbc/src/test/README.md",
            "sdks/java/io/hdfs/README.md",
            "sdks/java/io/hadoop/README.md",
            "sdks/java/extensions/join-library/README.md",
            "sdks/java/extensions/sorter/README.md",
            "sdks/java/javadoc/README.md",
            "runners/apex/README.md",
            "runners/gearpump/README.md",
            "runners/spark/README.md",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
            "examples/java/README.md",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
            "examples/java/src/main/java/org/apache/beam/examples/complete/README.md",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
            "examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
            "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/io/ValuesSource.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
            "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
            ".travis.yml",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
            "examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
            ".gitignore",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/DistinctExample.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/TopWikipediaSessions.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/SideInputContainer.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
            "examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
            "examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
            "runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
            "examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
            "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
            "pom.xml",
            "sdks/pom.xml",
            "sdks/common/pom.xml",
            "sdks/common/fn-api/pom.xml",
            "sdks/common/runner-api/pom.xml",
            "sdks/python/pom.xml",
            "sdks/java/pom.xml",
            "sdks/java/maven-archetypes/pom.xml",
            "sdks/java/maven-archetypes/starter/pom.xml",
            "sdks/java/maven-archetypes/starter/src/main/resources/archetype-resources/pom.xml",
            "sdks/java/maven-archetypes/starter/src/test/resources/projects/basic/reference/pom.xml",
            "sdks/java/maven-archetypes/examples-java8/pom.xml",
            "sdks/java/maven-archetypes/examples-java8/src/main/resources/archetype-resources/pom.xml",
            "sdks/java/maven-archetypes/examples/pom.xml",
            "sdks/java/maven-archetypes/examples/src/main/resources/archetype-resources/pom.xml",
            "sdks/java/build-tools/pom.xml",
            "sdks/java/core/pom.xml",
            "sdks/java/java8tests/pom.xml",
            "sdks/java/harness/pom.xml",
            "sdks/java/io/pom.xml",
            "sdks/java/io/jdbc/pom.xml",
            "sdks/java/io/elasticsearch/pom.xml",
            "sdks/java/io/mongodb/pom.xml",
            "sdks/java/io/common/pom.xml",
            "sdks/java/io/hdfs/pom.xml",
            "sdks/java/io/hadoop-common/pom.xml",
            "sdks/java/io/kinesis/pom.xml",
            "sdks/java/io/hadoop/pom.xml",
            "sdks/java/io/hadoop/jdk1.8-tests/pom.xml",
            "sdks/java/io/hadoop/input-format/pom.xml",
            "sdks/java/io/kafka/pom.xml",
            "sdks/java/io/hbase/pom.xml",
            "sdks/java/io/mqtt/pom.xml",
            "sdks/java/io/google-cloud-platform/pom.xml",
            "sdks/java/io/jms/pom.xml",
            "sdks/java/extensions/pom.xml",
            "sdks/java/extensions/gcp-core/pom.xml",
            "sdks/java/extensions/jackson/pom.xml",
            "sdks/java/extensions/join-library/pom.xml",
            "sdks/java/extensions/sorter/pom.xml",
            "sdks/java/javadoc/pom.xml",
            "runners/pom.xml",
            "runners/direct-java/pom.xml",
            "runners/apex/pom.xml",
            "runners/gearpump/pom.xml",
            "runners/core-java/pom.xml",
            "runners/flink/pom.xml",
            "runners/core-construction-java/pom.xml",
            "runners/google-cloud-dataflow-java/pom.xml",
            "runners/spark/pom.xml",
            "examples/pom.xml",
            "examples/java8/pom.xml",
            "examples/java/pom.xml",
            "examples/java8/src/main/java/org/apache/beam/examples/complete/game/HourlyTeamScore.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
            "examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
            "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
            "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
            "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
            "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java"
        ],
        "fixed": true
    }
]