[{"commit": "https://github.com/apache/pig/commit/01b7a50657b46d346f0a8f472c92fdba72819a24", "parent": "https://github.com/apache/pig/commit/8cecd74b7f64f7e964ea7164637da1011e1ab92f", "message": "PIG-5372: SAMPLE/RANDOM(udf) before skewed join failing with NPE (knoguchi)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1852183 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_1", "file": [{"additions": 1, "raw_url": "https://github.com/apache/pig/raw/01b7a50657b46d346f0a8f472c92fdba72819a24/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/01b7a50657b46d346f0a8f472c92fdba72819a24/CHANGES.txt", "sha": "ceba83de46d545efee0e2147fb16796ef78f5cfe", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=01b7a50657b46d346f0a8f472c92fdba72819a24", "patch": "@@ -87,6 +87,7 @@ PIG-5251: Bump joda-time to 2.9.9 (dbist13 via rohini)\n OPTIMIZATIONS\n  \n BUG FIXES\n+PIG-5372: SAMPLE/RANDOM(udf) before skewed join failing with NPE (knoguchi)\n \n PIG-5374: Use CircularFifoBuffer in InterRecordReader (szita)\n ", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/01b7a50657b46d346f0a8f472c92fdba72819a24/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java", "blob_url": "https://github.com/apache/pig/blob/01b7a50657b46d346f0a8f472c92fdba72819a24/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java", "sha": "1209989808511324633db5a30f7da6cae5d4adb2", "changes": 2, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java?ref=01b7a50657b46d346f0a8f472c92fdba72819a24", "patch": "@@ -112,8 +112,6 @@ public int getPartition(PigNullableWritable wrappedKey, Writable value, int numP\n     @Override\n     public void setConf(Configuration job) {\n         conf = job;\n-        PigMapReduce.sJobConfInternal.set(conf);\n-        PigMapReduce.sJobConf = conf;\n     }\n \n     @Override", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/01b7a50657b46d346f0a8f472c92fdba72819a24/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java", "blob_url": "https://github.com/apache/pig/blob/01b7a50657b46d346f0a8f472c92fdba72819a24/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java", "sha": "ec931f6507bcc7a2da1ca54806512225bf677237", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java?ref=01b7a50657b46d346f0a8f472c92fdba72819a24", "patch": "@@ -93,10 +93,10 @@\n             conf.set(\"yarn.resourcemanager.principal\", mapConf.get(\"yarn.resourcemanager.principal\"));\n         }\n \n-        if (PigMapReduce.sJobConfInternal.get().get(\"fs.file.impl\")!=null)\n-            conf.set(\"fs.file.impl\", PigMapReduce.sJobConfInternal.get().get(\"fs.file.impl\"));\n-        if (PigMapReduce.sJobConfInternal.get().get(\"fs.hdfs.impl\")!=null)\n-            conf.set(\"fs.hdfs.impl\", PigMapReduce.sJobConfInternal.get().get(\"fs.hdfs.impl\"));\n+        if (mapConf.get(\"fs.file.impl\")!=null)\n+            conf.set(\"fs.file.impl\", mapConf.get(\"fs.file.impl\"));\n+        if (mapConf.get(\"fs.hdfs.impl\")!=null)\n+            conf.set(\"fs.hdfs.impl\", mapConf.get(\"fs.hdfs.impl\"));\n \n         copyTmpFileConfigurationValues(PigMapReduce.sJobConfInternal.get(), conf);\n ", "filename": "src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java"}, {"additions": 38, "raw_url": "https://github.com/apache/pig/raw/01b7a50657b46d346f0a8f472c92fdba72819a24/test/org/apache/pig/test/TestSkewedJoin.java", "blob_url": "https://github.com/apache/pig/blob/01b7a50657b46d346f0a8f472c92fdba72819a24/test/org/apache/pig/test/TestSkewedJoin.java", "sha": "e1ad73cb04d95b21b1ce7bcc64ce420188ed522d", "changes": 47, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestSkewedJoin.java?ref=01b7a50657b46d346f0a8f472c92fdba72819a24", "patch": "@@ -207,7 +207,6 @@ public void testSkewedJoinMapLeftEmpty() throws IOException{\n         assertEquals(0, count);\n     }\n \n-\n     @Test\n     public void testSkewedJoinWithGroup() throws IOException{\n         pigServer.registerQuery(\"A = LOAD '\" + INPUT_FILE1 + \"' as (id, name, n);\");\n@@ -354,7 +353,7 @@ public void testSkewedJoinNullKeys() throws IOException {\n         try {\n             DataBag dbfrj = BagFactory.getInstance().newDefaultBag();\n             {\n-                pigServer.registerQuery(\"C = join A by id, B by id using 'skewed';\");\n+                pigServer.registerQuery(\"C = join A by id, B by id using 'skewed' parallel 2;\");\n                 Iterator<Tuple> iter = pigServer.openIterator(\"C\");\n \n                 while(iter.hasNext()) {\n@@ -375,23 +374,23 @@ public void testSkewedJoinOuter() throws IOException {\n         pigServer.registerQuery(\"B = LOAD '\" + INPUT_FILE5 + \"' as (id,name);\");\n         DataBag dbfrj = BagFactory.getInstance().newDefaultBag();\n         {\n-            pigServer.registerQuery(\"C = join A by id left, B by id using 'skewed';\");\n+            pigServer.registerQuery(\"C = join A by id left, B by id using 'skewed' parallel 2;\");\n             Iterator<Tuple> iter = pigServer.openIterator(\"C\");\n \n             while(iter.hasNext()) {\n                 dbfrj.add(iter.next());\n             }\n         }\n         {\n-            pigServer.registerQuery(\"C = join A by id right, B by id using 'skewed';\");\n+            pigServer.registerQuery(\"C = join A by id right, B by id using 'skewed' parallel 2;\");\n             Iterator<Tuple> iter = pigServer.openIterator(\"C\");\n \n             while(iter.hasNext()) {\n                 dbfrj.add(iter.next());\n             }\n         }\n         {\n-            pigServer.registerQuery(\"C = join A by id full, B by id using 'skewed';\");\n+            pigServer.registerQuery(\"C = join A by id full, B by id using 'skewed' parallel 2;\");\n             Iterator<Tuple> iter = pigServer.openIterator(\"C\");\n \n             while(iter.hasNext()) {\n@@ -413,7 +412,7 @@ public void testSkewedJoinOneValue() throws IOException {\n \n         DataBag dbfrj = BagFactory.getInstance().newDefaultBag(), dbrj = BagFactory.getInstance().newDefaultBag();\n         {\n-            pigServer.registerQuery(\"E = join C by id, D by id using 'skewed';\");\n+            pigServer.registerQuery(\"E = join C by id, D by id using 'skewed' parallel 2;\");\n             Iterator<Tuple> iter = pigServer.openIterator(\"E\");\n \n             while(iter.hasNext()) {\n@@ -487,7 +486,7 @@ public void testSkewedJoinEmptyInput() throws IOException {\n         pigServer.registerQuery(\"a = load 'left.dat' as (nums:chararray);\");\n         pigServer.registerQuery(\"b = load 'right.dat' as (number:chararray,text:chararray);\");\n         pigServer.registerQuery(\"c = filter a by nums == '7';\");\n-        pigServer.registerQuery(\"d = join c by nums LEFT OUTER, b by number USING 'skewed';\");\n+        pigServer.registerQuery(\"d = join c by nums LEFT OUTER, b by number USING 'skewed' parallel 2;\");\n \n         Iterator<Tuple> iter = pigServer.openIterator(\"d\");\n \n@@ -515,7 +514,7 @@ public void testRecursiveFileListing() throws IOException {\n \n         pigServer.registerQuery(\"a = load 'foo' as (nums:chararray);\");\n         pigServer.registerQuery(\"b = load 'foo' as (nums:chararray);\");\n-        pigServer.registerQuery(\"d = join a by nums, b by nums USING 'skewed';\");\n+        pigServer.registerQuery(\"d = join a by nums, b by nums USING 'skewed' parallel 2;\");\n \n         Iterator<Tuple> iter = pigServer.openIterator(\"d\");\n         int count = 0;\n@@ -569,7 +568,7 @@ public void testNonExistingInputPathInSkewJoin() throws Exception {\n           \"exists = LOAD '\" + INPUT_FILE2 + \"' AS (a:long, x:chararray);\" +\n           \"missing = LOAD '/non/existing/directory' AS (a:long);\" +\n           \"missing = FOREACH ( GROUP missing BY a ) GENERATE $0 AS a, COUNT_STAR($1);\" +\n-          \"joined = JOIN exists BY a, missing BY a USING 'skewed';\";\n+          \"joined = JOIN exists BY a, missing BY a USING 'skewed' parallel 2;\";\n \n         String logFile = Util.createTempFileDelOnExit(\"tmp\", \".log\").getAbsolutePath();\n         Logger logger = Logger.getLogger(\"org.apache.pig\");\n@@ -619,4 +618,34 @@ public void testNonExistingInputPathInSkewJoin() throws Exception {\n         }\n     }\n \n+    // PIG-5372\n+    @Test\n+    public void testSkewedJoinWithRANDOMudf() throws IOException{\n+        pigServer.registerQuery(\"A = LOAD '\" + INPUT_FILE1 + \"' as (id, name, n);\");\n+        pigServer.registerQuery(\"B = LOAD '\" + INPUT_FILE2 + \"' as (id, name);\");\n+        pigServer.registerQuery(\"A2 = FOREACH A GENERATE id, RANDOM() as randnum;\");\n+\n+        DataBag dbfrj = BagFactory.getInstance().newDefaultBag(), dbshj = BagFactory.getInstance().newDefaultBag();\n+        {\n+            pigServer.registerQuery(\"D = join A2 by id, B by id using 'skewed' parallel 2;\");\n+            Iterator<Tuple> iter = pigServer.openIterator(\"D\");\n+\n+            while(iter.hasNext()) {\n+                dbfrj.add(iter.next());\n+            }\n+        }\n+        {\n+            pigServer.registerQuery(\"D = join A2 by id, B by id;\");\n+            Iterator<Tuple> iter = pigServer.openIterator(\"D\");\n+\n+            while(iter.hasNext()) {\n+                dbshj.add(iter.next());\n+            }\n+        }\n+        assertTrue(dbfrj.size()>0);\n+        assertTrue(dbshj.size()>0);\n+        assertEquals(dbfrj.size(), dbshj.size());\n+    }\n+\n+\n }", "filename": "test/org/apache/pig/test/TestSkewedJoin.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/518b5d0a958f22b35e108e609ada523fee3f6a69", "parent": "https://github.com/apache/pig/commit/e07bcade84ff174680a754f607e5b39c4c003fb7", "message": "PIG-5307: NPE in TezOperDependencyParallelismEstimator (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1810606 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_2", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/518b5d0a958f22b35e108e609ada523fee3f6a69/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/518b5d0a958f22b35e108e609ada523fee3f6a69/CHANGES.txt", "sha": "99df4cb76dcc3a25ded7b8e722b77e601f7223d7", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=518b5d0a958f22b35e108e609ada523fee3f6a69", "patch": "@@ -52,6 +52,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-5307: NPE in TezOperDependencyParallelismEstimator (rohini)\n+\n PIG-5272: BagToTuple output schema is incorrect (juen1jp via rohini)\n \n PIG-5271: StackOverflowError when compiling in Tez mode (with union and replicated join) (knoguchi)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/518b5d0a958f22b35e108e609ada523fee3f6a69/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/TezOperDependencyParallelismEstimator.java", "blob_url": "https://github.com/apache/pig/blob/518b5d0a958f22b35e108e609ada523fee3f6a69/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/TezOperDependencyParallelismEstimator.java", "sha": "b4223db592fdb0ac2f1e1553db56ce9771595f0f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/TezOperDependencyParallelismEstimator.java?ref=518b5d0a958f22b35e108e609ada523fee3f6a69", "patch": "@@ -217,7 +217,7 @@ public TezParallelismFactorVisitor(TezOperator tezOp, TezOperator successor) {\n         public void visitFilter(POFilter fl) throws VisitorException {\n             if (fl.getPlan().size()==1 && fl.getPlan().getRoots().get(0) instanceof ConstantExpression) {\n                 ConstantExpression cons = (ConstantExpression)fl.getPlan().getRoots().get(0);\n-                if (cons.getValue().equals(Boolean.TRUE)) {\n+                if (Boolean.TRUE.equals(cons.getValue())) {\n                     // skip all true condition\n                     return;\n                 }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/TezOperDependencyParallelismEstimator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/dc54571be938015c2d1b00694c845101a8404127", "parent": "https://github.com/apache/pig/commit/2babe53b41bf374701851f53e97e27883a8b1bc9", "message": "PIG-4967: NPE in PigJobControl.run() when job status is null\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1758708 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_3", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/dc54571be938015c2d1b00694c845101a8404127/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/dc54571be938015c2d1b00694c845101a8404127/CHANGES.txt", "sha": "9c75f6d49ae35b3b8ed8cd8559a3bce55d210f76", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=dc54571be938015c2d1b00694c845101a8404127", "patch": "@@ -40,6 +40,8 @@ OPTIMIZATIONS\n \u00a0\n BUG FIXES\n \n+PIG-4967: NPE in PigJobControl.run() when job status is null (water via daijy)\n+\n PIG-4972: StreamingIO_1 fail on perl 5.22 (daijy)\n \n PIG-4933: TestDataBagAccess.testBagConstantFlatten1/TestLogicalPlanBuilder.testQuery90 broken after PIG-2315 (knoguchi)", "filename": "CHANGES.txt"}, {"additions": 12, "raw_url": "https://github.com/apache/pig/raw/dc54571be938015c2d1b00694c845101a8404127/shims/src/hadoop23/org/apache/pig/backend/hadoop23/PigJobControl.java", "blob_url": "https://github.com/apache/pig/blob/dc54571be938015c2d1b00694c845101a8404127/shims/src/hadoop23/org/apache/pig/backend/hadoop23/PigJobControl.java", "sha": "6439611ae1b983266a9832bd54e653862bdc9564", "changes": 13, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/shims/src/hadoop23/org/apache/pig/backend/hadoop23/PigJobControl.java?ref=dc54571be938015c2d1b00694c845101a8404127", "patch": "@@ -179,7 +179,18 @@ public void run() {\n           }\n           while(it.hasNext()) {\n             ControlledJob j = it.next();\n-            log.debug(\"Checking state of job \"+j);\n+\n+            // TODO: Need to re-visit the following try...catch\n+            // when Pig picks up a Hadoop release with MAPREDUCE-6762 applied\n+            // as its dependency.\n+            try {\n+              log.debug(\"Checking state of job \" + j);\n+            } catch(NullPointerException npe) {\n+              log.warn(\"Failed to get job name \" +\n+                \"when checking state of job. \" +\n+                \"Check if job status is null.\", npe);\n+            }\n+\n             switch(checkState(j)) {\n             case SUCCESS:\n               getJobs(successfulJobs).add(j);", "filename": "shims/src/hadoop23/org/apache/pig/backend/hadoop23/PigJobControl.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/5de019a0785911c2881dc816a2d616a10327938f", "parent": "https://github.com/apache/pig/commit/9562f478075624f15889f95b266296d6f607618e", "message": "PIG-4832: Fix TestPrumeColumn NPE failure\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1736096 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_4", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/5de019a0785911c2881dc816a2d616a10327938f/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/5de019a0785911c2881dc816a2d616a10327938f/CHANGES.txt", "sha": "433cf0dc5055c9017e32d24516f928c1fbec59e1", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=5de019a0785911c2881dc816a2d616a10327938f", "patch": "@@ -99,6 +99,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4832: Fix TestPrumeColumn NPE failure (kellyzly via daijy)\n+\n PIG-4833 TestBuiltin.testURIWithCurlyBrace in TEZ failing after PIG-4819 (knoguchi)\n \n PIG-4819: RANDOM() udf can lead to missing or redundant records (knoguchi)", "filename": "CHANGES.txt"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/5de019a0785911c2881dc816a2d616a10327938f/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java", "blob_url": "https://github.com/apache/pig/blob/5de019a0785911c2881dc816a2d616a10327938f/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java", "sha": "e0e0bc2d657c5cf6ac63175ef2f2351f97c2c090", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java?ref=5de019a0785911c2881dc816a2d616a10327938f", "patch": "@@ -247,9 +247,11 @@ public Schema getSchema() {\n \n \n     public StoreFuncInterface getStoreFunc() {\n-        if(storer == null){\n-            storer = (StoreFuncInterface)PigContext.instantiateFuncFromSpec(sFile.getFuncSpec());\n+        if (storer == null) {\n+            storer = (StoreFuncInterface) PigContext.instantiateFuncFromSpec(sFile.getFuncSpec());\n             storer.setStoreFuncUDFContextSignature(signature);\n+        }\n+        if (sDecorator == null) {\n             // Init the Decorator we use for writing Tuples\n             setStoreFuncDecorator(new StoreFuncDecorator(storer, signature));\n         }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/d2b7e212a2fcb7b989198923e9e76ee49ebf850d", "parent": "https://github.com/apache/pig/commit/b6f197003cb7e74ee47e278e40ad62e7be654300", "message": "PIG-5064: NPE in TestScriptUDF#testPythonBuiltinModuleImport1 when JAVA_HOME is not set\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1770733 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_5", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/d2b7e212a2fcb7b989198923e9e76ee49ebf850d/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/d2b7e212a2fcb7b989198923e9e76ee49ebf850d/CHANGES.txt", "sha": "196539f5a46e3d304fcee18aa5994ff7c988885d", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=d2b7e212a2fcb7b989198923e9e76ee49ebf850d", "patch": "@@ -155,6 +155,8 @@ OPTIMIZATIONS\n \n BUG FIXES\n \n+PIG-5064: NPE in TestScriptUDF#testPythonBuiltinModuleImport1 when JAVA_HOME is not set (water via daijy)\n+\n PIG-5048: HiveUDTF fail if it is the first expression in projection (nkollar via daijy)\n \n PIG-4951: Rename PIG_ATS_ENABLED constant (szita via daijy)", "filename": "CHANGES.txt"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/d2b7e212a2fcb7b989198923e9e76ee49ebf850d/test/org/apache/pig/test/TestScriptUDF.java", "blob_url": "https://github.com/apache/pig/blob/d2b7e212a2fcb7b989198923e9e76ee49ebf850d/test/org/apache/pig/test/TestScriptUDF.java", "sha": "8dc5818d3f574a77588e387a6abb87ea63a869cd", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestScriptUDF.java?ref=d2b7e212a2fcb7b989198923e9e76ee49ebf850d", "patch": "@@ -247,7 +247,11 @@ public void testPythonBuiltinModuleImport1() throws Exception {\n         Assert.assertTrue(t.get(0).toString().equals(System.getenv(input[0])));\n         Assert.assertTrue(iter.hasNext());\n         t = iter.next();\n-        Assert.assertTrue(t.get(0).toString().equals(System.getenv(input[1])));\n+        if (System.getenv(input[1]) != null) {  // JAVA_HOME is set, t.get(0) is not null\n+            Assert.assertTrue(t.get(0).toString().equals(System.getenv(input[1])));\n+        } else {  // JAVA_HOME is not set, t.get(0) is null\n+            Assert.assertNull(t.get(0));\n+        }\n         Assert.assertFalse(iter.hasNext());\n     }\n ", "filename": "test/org/apache/pig/test/TestScriptUDF.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/348d0839580ff7b0e647c326012580bfd00ed906", "parent": "https://github.com/apache/pig/commit/27d4ee118de18acaf267d459cf90192bad7d047f", "message": "PIG-4722: [Pig on Tez] NPE while running Combiner (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1714256 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_6", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/348d0839580ff7b0e647c326012580bfd00ed906/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/348d0839580ff7b0e647c326012580bfd00ed906/CHANGES.txt", "sha": "54b5ead7dfe8408efc26e35e3bdb67841df2d19d", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=348d0839580ff7b0e647c326012580bfd00ed906", "patch": "@@ -69,6 +69,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4722: [Pig on Tez] NPE while running Combiner (rohini)\n+\n PIG-4730: [Pig on Tez] Total parallelism estimation does not account load parallelism (rohini)\n \n PIG-4689: CSV Writes incorrect header if two CSV files are created in one script (nielsbasjes via daijy)", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/348d0839580ff7b0e647c326012580bfd00ed906/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java", "blob_url": "https://github.com/apache/pig/blob/348d0839580ff7b0e647c326012580bfd00ed906/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java", "sha": "164b75fc8dfc2aa00b8f2e85f200183c5ad15e81", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java?ref=348d0839580ff7b0e647c326012580bfd00ed906", "patch": "@@ -299,8 +299,9 @@ public Result processInput() throws ExecException {\n             }\n \n             // Should be removed once the model is clear\n-            if (getReporter() != null) {\n-                getReporter().progress();\n+            PigProgressable progRep = getReporter();\n+            if (progRep != null) {\n+                progRep.progress();\n             }\n \n             if (!isInputAttached()) {", "filename": "src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/a44b85a0ab941cdd1d2d7f6e457303aef1e57501", "parent": "https://github.com/apache/pig/commit/8f273342fdd3e8c4292f2460d07b99978a2be5c6", "message": "PIG-4635: NPE while running pig script in tez mode\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1705335 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_7", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/CHANGES.txt", "sha": "1085ffaa11a5d01c36c7e90f3ee0ae8032ff6b2a", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=a44b85a0ab941cdd1d2d7f6e457303aef1e57501", "patch": "@@ -47,6 +47,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4635: NPE while running pig script in tez mode (daijy)\n+\n PIG-4683: Nested order is broken after PIG-3591 in some cases (daijy)\n \n PIG-4679: Performance degradation due to InputSizeReducerEstimator since PIG-3754 (daijy)", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperator.java", "blob_url": "https://github.com/apache/pig/blob/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperator.java", "sha": "52764ab09991369c716aea647a9526ef05c9e73b", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperator.java?ref=a44b85a0ab941cdd1d2d7f6e457303aef1e57501", "patch": "@@ -189,6 +189,7 @@\n     private transient VertexGroupInfo vertexGroupInfo;\n     // Mapping of OperatorKey of POStore OperatorKey to vertexGroup TezOperator\n     private Map<OperatorKey, OperatorKey> vertexGroupStores = null;\n+    private boolean isVertexGroup = false;\n \n     public static class LoaderInfo implements Serializable {\n         private List<POLoad> loads = null;\n@@ -487,7 +488,7 @@ public void setVertexGroupMembers(List<OperatorKey> vertexGroupMembers) {\n     // Union is the only operator that uses alias vertex (VertexGroup) now. But\n     // more operators could be added to the list in the future.\n     public boolean isVertexGroup() {\n-        return vertexGroupInfo != null;\n+        return isVertexGroup;\n     }\n \n     public VertexGroupInfo getVertexGroupInfo() {\n@@ -496,6 +497,7 @@ public VertexGroupInfo getVertexGroupInfo() {\n \n     public void setVertexGroupInfo(VertexGroupInfo vertexGroup) {\n         this.vertexGroupInfo = vertexGroup;\n+        this.isVertexGroup = true;\n     }\n \n     public void addVertexGroupStore(OperatorKey storeKey, OperatorKey vertexGroupKey) {", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperator.java"}, {"additions": 7, "raw_url": "https://github.com/apache/pig/raw/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezPrinter.java", "blob_url": "https://github.com/apache/pig/blob/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezPrinter.java", "sha": "ce5d98176d0546575aa1ef0bca323f9673210091", "changes": 10, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezPrinter.java?ref=a44b85a0ab941cdd1d2d7f6e457303aef1e57501", "patch": "@@ -55,9 +55,13 @@ public void setVerbose(boolean verbose) {\n     public void visitTezOp(TezOperator tezOper) throws VisitorException {\n         if (tezOper.isVertexGroup()) {\n             VertexGroupInfo info = tezOper.getVertexGroupInfo();\n-            mStream.println(\"Tez vertex group \"\n-                    + tezOper.getOperatorKey().toString() + \"\\t<-\\t \"\n-                    + info.getInputs() + \"\\t->\\t \" + info.getOutput());\n+            mStream.print(\"Tez vertex group \"\n+                    + tezOper.getOperatorKey().toString());\n+            if (info!=null) {\n+                mStream.println(\"\\t<-\\t \" + info.getInputs() + \"\\t->\\t \" + info.getOutput());\n+            } else {\n+                mStream.println();\n+            }\n             mStream.println(\"# No plan on vertex group\");\n         } else {\n             mStream.println(\"Tez vertex \" + tezOper.getOperatorKey().toString());", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezPrinter.java"}, {"additions": 34, "raw_url": "https://github.com/apache/pig/raw/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/test/org/apache/pig/tez/TestTezGraceParallelism.java", "blob_url": "https://github.com/apache/pig/blob/a44b85a0ab941cdd1d2d7f6e457303aef1e57501/test/org/apache/pig/tez/TestTezGraceParallelism.java", "sha": "cf8d19c55fa2de818f3776675c4f0b9951336872", "changes": 34, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/tez/TestTezGraceParallelism.java?ref=a44b85a0ab941cdd1d2d7f6e457303aef1e57501", "patch": "@@ -26,6 +26,7 @@\n import java.io.IOException;\n import java.io.PrintWriter;\n import java.io.StringWriter;\n+import java.util.Arrays;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Random;\n@@ -248,4 +249,37 @@ public void testJoinWithDifferentDepth2() throws IOException{\n             Util.removeLogAppender(PigGraceShuffleVertexManager.class, \"testJoinWithDifferentDepth2\");\n         }\n     }\n+\n+    @Test\n+    // See PIG-4635 for a NPE in TezOperDependencyParallelismEstimator\n+    public void testJoinWithUnion() throws IOException{\n+        NodeIdGenerator.reset();\n+        PigServer.resetScope();\n+        StringWriter writer = new StringWriter();\n+        Util.createLogAppender(\"testJoinWithUnion\", writer, PigGraceShuffleVertexManager.class);\n+        try {\n+            // DAG: 29 -> 32 -> 41 \\\n+            //                       -> 70 (vertex group) -> 61\n+            //      42 -> 45 -> 54 /\n+            pigServer.registerQuery(\"A = load '\" + INPUT_DIR + \"/\" + INPUT_FILE2 + \"' as (name:chararray, gender:chararray);\");\n+            pigServer.registerQuery(\"B = distinct A;\");\n+            pigServer.registerQuery(\"C = group B by name;\");\n+            pigServer.registerQuery(\"D = load '\" + INPUT_DIR + \"/\" + INPUT_FILE2 + \"' as (name:chararray, gender:chararray);\");\n+            pigServer.registerQuery(\"E = distinct D;\");\n+            pigServer.registerQuery(\"F = group E by name;\");\n+            pigServer.registerQuery(\"G = union C, F;\");\n+            pigServer.registerQuery(\"H = distinct G;\");\n+            Iterator<Tuple> iter = pigServer.openIterator(\"H\");\n+            int count = 0;\n+            while (iter.hasNext()) {\n+                iter.next();\n+                count++;\n+            }\n+            assertEquals(count, 20);\n+            assertTrue(writer.toString().contains(\"time to set parallelism for scope-41\"));\n+            assertTrue(writer.toString().contains(\"time to set parallelism for scope-54\"));\n+        } finally {\n+            Util.removeLogAppender(PigGraceShuffleVertexManager.class, \"testJoinWithUnion\");\n+        }\n+    }\n }", "filename": "test/org/apache/pig/tez/TestTezGraceParallelism.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0", "parent": "https://github.com/apache/pig/commit/9b7af31b09099f9954729cddbdf71519ab8b1621", "message": "PIG-4497: [Pig on Tez] NPE for null scalar (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1671155 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_8", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0/CHANGES.txt", "sha": "47c0d77b1714a463fb55a5aaf9776bd59d31eb54", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=8d7d90161e41654a03b73624e5bfe6f1e68ddaf0", "patch": "@@ -58,6 +58,8 @@ PIG-4333: Split BigData tests into multiple groups (rohini)\n  \n BUG FIXES\n \n+PIG-4497: [Pig on Tez] NPE for null scalar (rohini)\n+\n PIG-4493: Pig on Tez gives wrong results if Union is followed by Split (rohini)\n \n PIG-4491: Streaming Python Bytearray Bugs (jeremykarn via daijy)", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/udf/ReadScalarsTez.java", "blob_url": "https://github.com/apache/pig/blob/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/udf/ReadScalarsTez.java", "sha": "df04f2f476b8824cd836f4e2a04badb7542ded2b", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/udf/ReadScalarsTez.java?ref=8d7d90161e41654a03b73624e5bfe6f1e68ddaf0", "patch": "@@ -100,6 +100,9 @@ public void attachInputs(Map<String, LogicalInput> inputs,\n \n     @Override\n     public Object exec(Tuple input) throws IOException {\n+        if (t == null) {\n+            return null;\n+        }\n         int pos = (Integer) input.get(0);\n         Object obj = t.get(pos);\n         return obj;", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/udf/ReadScalarsTez.java"}, {"additions": 22, "raw_url": "https://github.com/apache/pig/raw/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0/test/org/apache/pig/test/TestScalarAliasesLocal.java", "blob_url": "https://github.com/apache/pig/blob/8d7d90161e41654a03b73624e5bfe6f1e68ddaf0/test/org/apache/pig/test/TestScalarAliasesLocal.java", "sha": "7c88219bb90faf5fc7aaee92ac57ad74917ef65f", "changes": 22, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestScalarAliasesLocal.java?ref=8d7d90161e41654a03b73624e5bfe6f1e68ddaf0", "patch": "@@ -25,8 +25,10 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.List;\n \n import org.apache.pig.PigServer;\n+import org.apache.pig.builtin.mock.Storage;\n import org.apache.pig.data.BagFactory;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n@@ -556,4 +558,24 @@ public void testScalarWithNoProjection() throws Exception{\n         );\n     }\n \n+    @Test\n+    public void testScalarNullValue() throws Exception{\n+        Storage.Data data = Storage.resetData(pigServer);\n+        data.set(\"input\", Storage.tuple(\"a\", 1), Storage.tuple(\"b\", 2));\n+\n+        pigServer.setBatchOn();\n+        pigServer.registerQuery(\"A = load 'input' using mock.Storage() as (a:chararray, b:int);\");\n+        pigServer.registerQuery(\"B = FILTER A by a == 'c';\");\n+        pigServer.registerQuery(\"C = FOREACH A generate a, b + B.b;\");\n+        pigServer.registerQuery(\"store C into 'output' using mock.Storage();\");\n+\n+        pigServer.executeBatch();\n+\n+        List<Tuple> actualResults = data.get(\"output\");\n+        List<Tuple> expectedResults = Util.getTuplesFromConstantTupleStrings(\n+                new String[] {\"('a', null)\", \"('b', null)\"});\n+        Util.checkQueryOutputsAfterSort(actualResults.iterator(), expectedResults);\n+\n+    }\n+\n }", "filename": "test/org/apache/pig/test/TestScalarAliasesLocal.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/6e19aa5c51829f525149c5ef6de69395599deacb", "parent": "https://github.com/apache/pig/commit/a58d3088c37a153fe565f0b9dd7cb208e6c5ac56", "message": "PIG-4873: InputSplit.getLocations return null and result a NPE in Pig\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1744313 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_9", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/6e19aa5c51829f525149c5ef6de69395599deacb/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/6e19aa5c51829f525149c5ef6de69395599deacb/CHANGES.txt", "sha": "5d917b0533962ce629770571128ad9920ba996d9", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=6e19aa5c51829f525149c5ef6de69395599deacb", "patch": "@@ -127,6 +127,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4873: InputSplit.getLocations return null and result a NPE in Pig (daijy)\n+\n PIG-4895: User UDFs relying on mapreduce.job.maps broken in Tez (rohini)\n \n PIG-4883: MapKeyType of splitter was set wrongly in specific multiquery case (kellyzly via rohini)", "filename": "CHANGES.txt"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/6e19aa5c51829f525149c5ef6de69395599deacb/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java", "blob_url": "https://github.com/apache/pig/blob/6e19aa5c51829f525149c5ef6de69395599deacb/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java", "sha": "e866b28c1bcdadcb5c0201d80c9648868dd6b3ca", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java?ref=6e19aa5c51829f525149c5ef6de69395599deacb", "patch": "@@ -463,9 +463,11 @@ public String toString() {\n             for (int i = 0; i < wrappedSplits.length; i++) {\n                 st.append(\"Input split[\"+i+\"]:\\n   Length = \"+ wrappedSplits[i].getLength()+\"\\n   ClassName: \" +\n                     wrappedSplits[i].getClass().getName() + \"\\n   Locations:\\n\");\n-                for (String location :  wrappedSplits[i].getLocations())\n-                    st.append(\"    \"+location+\"\\n\");\n-                st.append(\"\\n-----------------------\\n\");\n+                if (wrappedSplits[i]!=null && wrappedSplits[i].getLocations()!=null) {\n+                    for (String location :  wrappedSplits[i].getLocations())\n+                        st.append(\"    \"+location+\"\\n\");\n+                    st.append(\"\\n-----------------------\\n\");\n+                }\n           }\n         } catch (IOException e) {\n           return null;", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/fbda3661b25d640233ff3ddcf0e69832170c11b4", "parent": "https://github.com/apache/pig/commit/8cc8c06ac886613f3c695209d92bf584576ad887", "message": "PIG-4880: Overlapping of parameter substitution names inside&outside a macro fails with NPE (knoguchi)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1743527 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_10", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/fbda3661b25d640233ff3ddcf0e69832170c11b4/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/fbda3661b25d640233ff3ddcf0e69832170c11b4/CHANGES.txt", "sha": "cfc219cb85e38587a9bbbdb6fc4326271ca98d2b", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=fbda3661b25d640233ff3ddcf0e69832170c11b4", "patch": "@@ -119,6 +119,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4880: Overlapping of parameter substitution names inside&outside a macro fails with NPE (knoguchi)\n+\n PIG-4881: TestBuiltin.testUniqueID failing on hadoop-1.x (knoguchi)\n \n PIG-4888: Line number off when reporting syntax error inside a macro (knoguchi)", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/fbda3661b25d640233ff3ddcf0e69832170c11b4/src/org/apache/pig/parser/PigMacro.java", "blob_url": "https://github.com/apache/pig/blob/fbda3661b25d640233ff3ddcf0e69832170c11b4/src/org/apache/pig/parser/PigMacro.java", "sha": "1f84af48c5346690b461a1f1a65f576f43bf6cbd", "changes": 11, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/parser/PigMacro.java?ref=fbda3661b25d640233ff3ddcf0e69832170c11b4", "patch": "@@ -168,14 +168,9 @@ private String substituteParams(String[] inputs, String[] outputs,\n \n             Map<String, String> paramVal = pc.getParamVal();\n             for (Map.Entry<String, String> e : pigContext.getParamVal().entrySet()) {\n-                if (paramVal.containsKey(e.getKey())) {\n-                    throw new ParserException(\n-                        \"Macro contains argument or return value \" + e.getKey() + \" which conflicts \" +\n-                        \"with a Pig parameter of the same name.\"\n-                    );\n-                } else {\n-                    paramVal.put(e.getKey(), e.getValue());\n-                }\n+                // overwrite=false since macro parameters should have precedence\n+                // over commandline parameters (if keys overlap)\n+                pc.processOrdLine(e.getKey(), e.getValue(), false);\n             }\n             \n             ParameterSubstitutionPreprocessor psp = new ParameterSubstitutionPreprocessor(pc);", "filename": "src/org/apache/pig/parser/PigMacro.java"}, {"additions": 129, "raw_url": "https://github.com/apache/pig/raw/fbda3661b25d640233ff3ddcf0e69832170c11b4/test/org/apache/pig/test/TestMacroExpansion.java", "blob_url": "https://github.com/apache/pig/blob/fbda3661b25d640233ff3ddcf0e69832170c11b4/test/org/apache/pig/test/TestMacroExpansion.java", "sha": "0328b072166536e02a0568b574f3cdfe36f9ad49", "changes": 129, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestMacroExpansion.java?ref=fbda3661b25d640233ff3ddcf0e69832170c11b4", "patch": "@@ -2280,6 +2280,135 @@ public void testNestedImport() throws Exception {\n         verify(script, expected);\n     }\n \n+    // When  declare-in-macro, macro param and command-line param contain the\n+    // same name, last declare wins\n+    @Test\n+    public void testParamOverLap1() throws Exception {\n+        String macro =\n+            \"DEFINE mygroupby(REL, key, number) RETURNS G {\\n\" +\n+            \"    %declare number 333;\\n\"  +\n+            \"    $G = GROUP $REL by $key parallel $number;\\n\" +\n+            \"};\";\n+        createFile(\"my_macro.pig\", macro);\n+\n+        String script =\n+            \"%declare number 111;\\n\" +\n+            \"IMPORT 'my_macro.pig';\\n\" +\n+            \"data = LOAD '1234.txt' USING PigStorage() AS (i: int);\\n\" +\n+            \"result = mygroupby(data, i, 222);\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\";\n+\n+        String expected =\n+            \"data = LOAD '1234.txt' USING PigStorage() AS i:int;\\n\" +\n+            \"result = GROUP data by (i) parallel 333;\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\\n\";\n+\n+        verify(script, expected);\n+    }\n+\n+    // When  default-in-macro, macro param and command-line param contain the\n+    // same name, then default should be ignored and macro param to be taken\n+    @Test\n+    public void testParamOverLap2() throws Exception {\n+        String macro =\n+            \"DEFINE mygroupby(REL, key, number) RETURNS G {\\n\" +\n+            \"    %default number 333;\\n\"  +\n+            \"    $G = GROUP $REL by $key parallel $number;\\n\" +\n+            \"};\";\n+        createFile(\"my_macro.pig\", macro);\n+\n+        String script =\n+            \"%declare number 111;\\n\" +\n+            \"IMPORT 'my_macro.pig';\\n\" +\n+            \"data = LOAD '1234.txt' USING PigStorage() AS (i: int);\\n\" +\n+            \"result = mygroupby(data, i, 222);\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\";\n+\n+        String expected =\n+            \"data = LOAD '1234.txt' USING PigStorage() AS i:int;\\n\" +\n+            \"result = GROUP data by (i) parallel 222;\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\\n\";\n+\n+        verify(script, expected);\n+    }\n+\n+    // Overlapping of  macro param and command-line param used to be disallowed.\n+    // Now, simply taking the macro param when this happens\n+    @Test\n+    public void testParamOverLap3() throws Exception {\n+        String macro =\n+            \"DEFINE mygroupby(REL, key, number) RETURNS G {\\n\" +\n+            \"    $G = GROUP $REL by $key parallel $number;\\n\" +\n+            \"};\";\n+        createFile(\"my_macro.pig\", macro);\n+\n+        String script =\n+            \"%default number 111;\\n\" +\n+            \"IMPORT 'my_macro.pig';\\n\" +\n+            \"data = LOAD '1234.txt' USING PigStorage() AS (i: int);\\n\" +\n+            \"result = mygroupby(data, i, 222);\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\";\n+\n+        String expected =\n+            \"data = LOAD '1234.txt' USING PigStorage() AS i:int;\\n\" +\n+            \"result = GROUP data by (i) parallel 222;\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\\n\";\n+\n+        verify(script, expected);\n+    }\n+\n+    // Testing inline declare and commandline param overlap.\n+    // testParamOverLap1 should cover this case as well but creating a specific\n+    // case since this pair used to fail with NPE\n+    @Test\n+    public void testParamOverLap4() throws Exception {\n+        String macro =\n+            \"DEFINE mygroupby(REL, key) RETURNS G {\\n\" +\n+            \"    %declare number 333;\\n\"  +\n+            \"    $G = GROUP $REL by $key parallel $number;\\n\" +\n+            \"};\";\n+        createFile(\"my_macro.pig\", macro);\n+\n+        String script =\n+            \"%default number 111;\\n\" +\n+            \"IMPORT 'my_macro.pig';\\n\" +\n+            \"data = LOAD '1234.txt' USING PigStorage() AS (i: int);\\n\" +\n+            \"result = mygroupby(data, i);\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\";\n+\n+        String expected =\n+            \"data = LOAD '1234.txt' USING PigStorage() AS i:int;\\n\" +\n+            \"result = GROUP data by (i) parallel 333;\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\\n\";\n+\n+        verify(script, expected);\n+    }\n+\n+    // default-in-macro should yield to command-line param\n+    @Test\n+    public void testParamOverLap5() throws Exception {\n+        String macro =\n+            \"DEFINE mygroupby(REL, key) RETURNS G {\\n\" +\n+            \"    %default number 333;\\n\"  +\n+            \"    $G = GROUP $REL by $key parallel $number;\\n\" +\n+            \"};\";\n+        createFile(\"my_macro.pig\", macro);\n+\n+        String script =\n+            \"%declare number 111;\\n\" +\n+            \"IMPORT 'my_macro.pig';\\n\" +\n+            \"data = LOAD '1234.txt' USING PigStorage() AS (i: int);\\n\" +\n+            \"result = mygroupby(data, i);\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\";\n+\n+        String expected =\n+            \"data = LOAD '1234.txt' USING PigStorage() AS i:int;\\n\" +\n+            \"result = GROUP data by (i) parallel 111;\\n\" +\n+            \"STORE result INTO 'test.out' USING PigStorage();\\n\";\n+\n+        verify(script, expected);\n+    }\n+\n     //-------------------------------------------------------------------------\n     \n     private void testMacro(String content) throws Exception {", "filename": "test/org/apache/pig/test/TestMacroExpansion.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/5a9b9ebdf1554898bd3a3d59d6571f20cbc692b9", "parent": "https://github.com/apache/pig/commit/f919fc3b742dae415a59539ab3abd08c6b8a0400", "message": "PIG-4169: NPE in ConstantCalculator\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1625205 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_11", "file": [{"additions": 5, "raw_url": "https://github.com/apache/pig/raw/5a9b9ebdf1554898bd3a3d59d6571f20cbc692b9/src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java", "blob_url": "https://github.com/apache/pig/blob/5a9b9ebdf1554898bd3a3d59d6571f20cbc692b9/src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java", "sha": "860fa474ea59bf6126a37d46740fc9d6d3df0f06", "changes": 11, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java?ref=5a9b9ebdf1554898bd3a3d59d6571f20cbc692b9", "patch": "@@ -26,8 +26,8 @@\n \n import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;\n+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;\n-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;\n import org.apache.pig.impl.PigContext;\n import org.apache.pig.impl.logicalLayer.FrontendException;\n@@ -149,15 +149,14 @@ protected void execute(LogicalExpression op) throws FrontendException {\n                     PhysicalOperator root = expPhysicalPlan.getLeaves().get(0);\n                     try {\n                         UDFContext.getUDFContext().addJobConf(ConfigurationUtil.toConfiguration(pc.getProperties(), true));\n-                        Result ret= root.getNext(root.getResultType());\n-                        if (ret.result != null) {\n-                            val = ret.result;\n-                            valSet = true;\n-                        }\n+                        PigHadoopLogger pigHadoopLogger = PigHadoopLogger.getInstance();\n+                        PhysicalOperator.setPigLogger(pigHadoopLogger);\n+                        val = root.getNext(root.getResultType()).result;\n                         UDFContext.getUDFContext().addJobConf(null);\n                     } catch (ExecException e) {\n                         throw new FrontendException(e);\n                     }\n+                    valSet = true;\n                 } else if (op instanceof UserFuncExpression) {\n                     // If solo UDF, calculate UDF\n                     UserFuncExpression udf = (UserFuncExpression)op;", "filename": "src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/779945ca83b00b13835184b8da25548cd5e0e0c7", "parent": "https://github.com/apache/pig/commit/484de09de12b17bed1d76fef8958f38163822e0c", "message": "PIG-4169: NPE in ConstantCalculator (cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1624611 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_12", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/779945ca83b00b13835184b8da25548cd5e0e0c7/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/779945ca83b00b13835184b8da25548cd5e0e0c7/CHANGES.txt", "sha": "234073e4fd15efb3ec54f8e6b265b3f6811d607d", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=779945ca83b00b13835184b8da25548cd5e0e0c7", "patch": "@@ -70,6 +70,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-4169: NPE in ConstantCalculator (cheolsoo)\n+\n PIG-4161: check for latest Hive snapshot dependencies (daijy)\n \n PIG-4102: Adding e2e tests and several improvements for Orc predicate pushdown (daijy)", "filename": "CHANGES.txt"}, {"additions": 6, "raw_url": "https://github.com/apache/pig/raw/779945ca83b00b13835184b8da25548cd5e0e0c7/src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java", "blob_url": "https://github.com/apache/pig/blob/779945ca83b00b13835184b8da25548cd5e0e0c7/src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java", "sha": "761781c1e3ab01b84316b45a7c23d6c873592ee8", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java?ref=779945ca83b00b13835184b8da25548cd5e0e0c7", "patch": "@@ -27,6 +27,7 @@\n import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;\n+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;\n import org.apache.pig.impl.PigContext;\n import org.apache.pig.impl.logicalLayer.FrontendException;\n@@ -148,12 +149,15 @@ protected void execute(LogicalExpression op) throws FrontendException {\n                     PhysicalOperator root = expPhysicalPlan.getLeaves().get(0);\n                     try {\n                         UDFContext.getUDFContext().addJobConf(ConfigurationUtil.toConfiguration(pc.getProperties(), true));\n-                        val = root.getNext(root.getResultType()).result;\n+                        Result ret= root.getNext(root.getResultType());\n+                        if (ret.result != null) {\n+                            val = ret.result;\n+                            valSet = true;\n+                        }\n                         UDFContext.getUDFContext().addJobConf(null);\n                     } catch (ExecException e) {\n                         throw new FrontendException(e);\n                     }\n-                    valSet = true;\n                 } else if (op instanceof UserFuncExpression) {\n                     // If solo UDF, calculate UDF\n                     UserFuncExpression udf = (UserFuncExpression)op;", "filename": "src/org/apache/pig/newplan/logical/rules/ConstantCalculator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/6567cc0704041f2b7b1f6fa9f7e264a568fd5975", "parent": "https://github.com/apache/pig/commit/0fb27a9a7a5cb7fdd277db66b647e09f319bef32", "message": "PIG-4156: [PATCH] fix NPE when running scripts stored on hdfs://\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1623608 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_13", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/6567cc0704041f2b7b1f6fa9f7e264a568fd5975/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/6567cc0704041f2b7b1f6fa9f7e264a568fd5975/CHANGES.txt", "sha": "4f8376b59e9bd16905750bb3f2fcc43735300546", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=6567cc0704041f2b7b1f6fa9f7e264a568fd5975", "patch": "@@ -70,6 +70,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-4156: [PATCH] fix NPE when running scripts stored on hdfs:// (acoliver via daijy)\n+\n PIG-4159: TestGroupConstParallelTez and TestJobSubmissionTez should be excluded in Hadoop 20 unit tests (cheolsoo)\n \n PIG-4154: ScriptState#setScript(File) does not close resources (lars_francke via daijy)", "filename": "CHANGES.txt"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/6567cc0704041f2b7b1f6fa9f7e264a568fd5975/src/org/apache/pig/impl/io/FileLocalizer.java", "blob_url": "https://github.com/apache/pig/blob/6567cc0704041f2b7b1f6fa9f7e264a568fd5975/src/org/apache/pig/impl/io/FileLocalizer.java", "sha": "2ffa4ebed3fa4a22065c28334e0f3361f1b4e03f", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/impl/io/FileLocalizer.java?ref=6567cc0704041f2b7b1f6fa9f7e264a568fd5975", "patch": "@@ -800,7 +800,8 @@ public static FetchFileRet fetchFile(Properties properties, String filePath) thr\n                 && uri.getScheme() == null )||\n                 // For Windows local files\n                 (uri.getScheme() == null && uri.getPath().matches(\"^/[A-Za-z]:.*\")) ||\n-                uri.getScheme().equals(\"local\") ) {\n+                (uri.getScheme() != null && uri.getScheme().equals(\"local\")) \n+            ) {\n             srcFs = localFs;\n         } else {\n             srcFs = path.getFileSystem(conf);", "filename": "src/org/apache/pig/impl/io/FileLocalizer.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/20eac9b2672f80eb24502a176a9df2024ba70fbe", "parent": "https://github.com/apache/pig/commit/63efe61aa797cb65b470019b2a6beb6c74826e61", "message": "PIG-4017: NPE thrown from JobControlCompiler.shipToHdfs (cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1603315 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_14", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/20eac9b2672f80eb24502a176a9df2024ba70fbe/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/20eac9b2672f80eb24502a176a9df2024ba70fbe/CHANGES.txt", "sha": "dba86c9ade6aa95d040f79ba8e05a19186f49384", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=20eac9b2672f80eb24502a176a9df2024ba70fbe", "patch": "@@ -193,6 +193,8 @@ PIG-3882: Multiquery off mode execution is not done in batch and very inefficien\n  \n BUG FIXES\n \n+PIG-4017: NPE thrown from JobControlCompiler.shipToHdfs (cheolsoo)\n+\n PIG-3997: Issue on Pig docs: Testing and Diagnostics (zjffdu via cheolsoo)\n \n PIG-3998: Documentation fix: invalid page links, wrong Groovy udf example (lbendig via cheolsoo)", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/20eac9b2672f80eb24502a176a9df2024ba70fbe/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java", "blob_url": "https://github.com/apache/pig/blob/20eac9b2672f80eb24502a176a9df2024ba70fbe/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java", "sha": "37341ce948d89c3b66d7a4860e5ba83a98d3c4a8", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java?ref=20eac9b2672f80eb24502a176a9df2024ba70fbe", "patch": "@@ -1687,7 +1687,9 @@ private static Path shipToHDFS(\n         } finally {\n             org.apache.commons.io.IOUtils.closeQuietly(is);\n             // IOUtils should not close stream to HDFS quietly\n-            os.close();\n+            if (os != null) {\n+                os.close();\n+            }\n         }\n         return dst;\n     }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/18c844e6e3254586d4b5ba9840c054c66c513adc", "parent": "https://github.com/apache/pig/commit/5fe78aa2504d9dc1267895976a5a6cb6a5d84940", "message": "PIG-4774: Fix NPE in SUM,AVG,MIN,MAX UDFs for null bag input (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1723972 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_15", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/CHANGES.txt", "sha": "92003cd62079ad44d3b712e1e4cf2fb6904d63a1", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -77,6 +77,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4774: Fix NPE in SUM,AVG,MIN,MAX UDFs for null bag input (rohini)\n+\n PIG-4757: Job stats on successfully read/output records wrong with multiple inputs/outputs (rohini)\n \n PIG-4769: UnionOptimizer hits errors when merging vertex group into split (rohini)", "filename": "CHANGES.txt"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AVG.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AVG.java", "sha": "f14dce1086bb89b43cc07306b64899621d176c42", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AVG.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -27,14 +27,14 @@\n import org.apache.pig.EvalFunc;\n import org.apache.pig.FuncSpec;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataByteArray;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.FrontendException;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n@@ -79,14 +79,17 @@ public Double exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -230,7 +233,7 @@ static protected Double sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n \n         // if we were handed an empty bag, return NULL\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n ", "filename": "src/org/apache/pig/builtin/AVG.java"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicBigDecimalMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicBigDecimalMathBase.java", "sha": "e16221096d7e5f4d4e5060bcc0bbe22b57614c5e", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicBigDecimalMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -18,8 +18,8 @@\n package org.apache.pig.builtin;\n \n import java.io.IOException;\n-import java.util.Iterator;\n import java.math.BigDecimal;\n+import java.util.Iterator;\n \n import org.apache.pig.Accumulator;\n import org.apache.pig.PigException;\n@@ -88,7 +88,7 @@ protected static BigDecimal doTupleWork(Tuple input, KnownOpProvider opProvider)\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         BigDecimal sofar = AlgebraicBigDecimalMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicBigDecimalMathBase.java"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicBigIntegerMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicBigIntegerMathBase.java", "sha": "7b9be4a6e3e87c1896c072cca5cbc06238e8e0e2", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicBigIntegerMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -18,8 +18,8 @@\n package org.apache.pig.builtin;\n \n import java.io.IOException;\n-import java.util.Iterator;\n import java.math.BigInteger;\n+import java.util.Iterator;\n \n import org.apache.pig.Accumulator;\n import org.apache.pig.PigException;\n@@ -88,7 +88,7 @@ protected static BigInteger doTupleWork(Tuple input, KnownOpProvider opProvider)\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         BigInteger sofar = AlgebraicBigIntegerMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicBigIntegerMathBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicByteArrayMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicByteArrayMathBase.java", "sha": "b6e694dba74d1c5cc8ccfd97b1b41a7b2fc33589", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicByteArrayMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -67,7 +67,7 @@ protected static Double doTupleWork(Tuple input, KnownOpProvider opProvider, byt\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         double sofar = AlgebraicByteArrayMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicByteArrayMathBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicDoubleMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicDoubleMathBase.java", "sha": "b6dc77c6c18138de51b235d8c539f15052e00fb9", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicDoubleMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -64,7 +64,7 @@ protected static Double doTupleWork(Tuple input, KnownOpProvider opProvider) thr\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         double sofar = AlgebraicDoubleMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicDoubleMathBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicFloatMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicFloatMathBase.java", "sha": "7e360cc57b77760b29edaed93132e23e9fd1b121", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicFloatMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -64,7 +64,7 @@ protected static Float doTupleWork(Tuple input, KnownOpProvider opProvider) thro\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         Float sofar = AlgebraicFloatMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicFloatMathBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicIntMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicIntMathBase.java", "sha": "4b58f9d241e2d79eaa527aefe639863eb7718f3b", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicIntMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -64,7 +64,7 @@ protected static Integer doTupleWork(Tuple input, KnownOpProvider opProvider) th\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         int sofar = AlgebraicIntMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicIntMathBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicLongMathBase.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/AlgebraicLongMathBase.java", "sha": "38674b25345a0d3352653ad191839ad5561c036d", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/AlgebraicLongMathBase.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -64,7 +64,7 @@ protected static Long doTupleWork(Tuple input, KnownOpProvider opProvider) throw\n         DataBag values = (DataBag)input.get(0);\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n         Long sofar = AlgebraicLongMathBase.getSeed(opProvider.getOp());", "filename": "src/org/apache/pig/builtin/AlgebraicLongMathBase.java"}, {"additions": 6, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/BigDecimalAvg.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/BigDecimalAvg.java", "sha": "f9452aef3105b49d9401966d62d4da301ec36590", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/BigDecimalAvg.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -18,20 +18,20 @@\n package org.apache.pig.builtin;\n \n import java.io.IOException;\n-import java.util.Iterator;\n import java.math.BigDecimal;\n import java.math.MathContext;\n+import java.util.Iterator;\n \n import org.apache.pig.Accumulator;\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n@@ -61,14 +61,17 @@ public BigDecimal exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -199,7 +202,7 @@ static protected BigDecimal sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n \n         // if we were handed an empty bag, return NULL\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n ", "filename": "src/org/apache/pig/builtin/BigDecimalAvg.java"}, {"additions": 6, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/BigIntegerAvg.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/BigIntegerAvg.java", "sha": "a388adb83dca6fecf92c343ade401952738209c9", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/BigIntegerAvg.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -18,21 +18,21 @@\n package org.apache.pig.builtin;\n \n import java.io.IOException;\n-import java.util.Iterator;\n import java.math.BigDecimal;\n import java.math.BigInteger;\n import java.math.MathContext;\n+import java.util.Iterator;\n \n import org.apache.pig.Accumulator;\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n@@ -62,14 +62,17 @@ public BigDecimal exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -201,7 +204,7 @@ static protected BigInteger sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n \n         // if we were handed an empty bag, return NULL\n-        if (values.size() == 0) {\n+        if (values == null || values.size() == 0) {\n             return null;\n         }\n ", "filename": "src/org/apache/pig/builtin/BigIntegerAvg.java"}, {"additions": 6, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/DateTimeMax.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/DateTimeMax.java", "sha": "ce08bc4aab9abc097951304c7435f2068b22332c", "changes": 10, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/DateTimeMax.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -20,8 +20,6 @@\n import java.io.IOException;\n import java.util.Iterator;\n \n-import org.joda.time.DateTime;\n-\n import org.apache.pig.Accumulator;\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n@@ -32,6 +30,7 @@\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n+import org.joda.time.DateTime;\n \n /**\n  * This method should never be used directly, use {@link MAX}.\n@@ -47,14 +46,17 @@ public DateTime exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -120,7 +122,7 @@ static protected DateTime max(Tuple input) throws ExecException {\n \n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -153,7 +155,7 @@ static protected DateTime max(Tuple input) throws ExecException {\n \n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.DATETIME)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.DATETIME));\n     }\n \n ", "filename": "src/org/apache/pig/builtin/DateTimeMax.java"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/DateTimeMin.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/DateTimeMin.java", "sha": "20739e1a8fd0b4783c357aee7240ddb14e722afa", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/DateTimeMin.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -20,8 +20,6 @@\n import java.io.IOException;\n import java.util.Iterator;\n \n-import org.joda.time.DateTime;\n-\n import org.apache.pig.Accumulator;\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n@@ -32,6 +30,7 @@\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n+import org.joda.time.DateTime;\n \n /**\n  * This method should never be used directly, use {@link MAX}.\n@@ -47,14 +46,17 @@ public DateTime exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -120,7 +122,7 @@ static protected DateTime min(Tuple input) throws ExecException {\n \n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n ", "filename": "src/org/apache/pig/builtin/DateTimeMin.java"}, {"additions": 23, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/DoubleAvg.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/DoubleAvg.java", "sha": "8ae937ca542c56955fae215f0ffb54693a2456b4", "changes": 46, "status": "modified", "deletions": 23, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/DoubleAvg.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -18,28 +18,25 @@\n package org.apache.pig.builtin;\n \n import java.io.IOException;\n-import java.util.HashMap;\n import java.util.Iterator;\n \n import org.apache.pig.Accumulator;\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n-import org.apache.pig.FuncSpec;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n-import org.apache.pig.impl.logicalLayer.FrontendException;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n  * This method should never be used directly, use {@link AVG}.\n  */\n public class DoubleAvg extends EvalFunc<Double> implements Algebraic, Accumulator<Double> {\n-    \n+\n     private static TupleFactory mTupleFactory = TupleFactory.getInstance();\n \n     @Override\n@@ -56,21 +53,24 @@ public Double exec(Tuple input) throws IOException {\n             Double avg = null;\n             if (count > 0)\n                 avg = new Double(sum / count);\n-    \n+\n             return avg;\n         } catch (ExecException ee) {\n             throw ee;\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -91,7 +91,7 @@ public Tuple exec(Tuple input) throws IOException {\n                 t.set(0, d);\n                 if (d != null){\n                     t.set(1, 1L);\n-                }else{    \n+                } else {\n                     t.set(1, 0L);\n                 }\n                 return t;\n@@ -100,9 +100,9 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n-                \n+\n         }\n     }\n \n@@ -117,7 +117,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);            \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -145,7 +145,7 @@ public Double exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);            \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -166,7 +166,7 @@ static protected Tuple combine(DataBag values) throws ExecException {\n             Tuple t = it.next();\n             Double d = (Double)t.get(0);\n             // we count nulls in avg as contributing 0\n-            // a departure from SQL for performance of \n+            // a departure from SQL for performance of\n             // COUNT() which implemented by just inspecting\n             // size of the bag\n             if(d == null) {\n@@ -200,9 +200,9 @@ static protected long count(Tuple input) throws ExecException {\n \n     static protected Double sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n-        \n+\n         // if we were handed an empty bag, return NULL\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -228,17 +228,17 @@ static protected Double sum(Tuple input) throws ExecException, IOException {\n             return null;\n         }\n     }\n-    \n+\n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE));\n     }\n-    \n+\n     /* Accumulator interface */\n-    \n+\n     private Double intermediateSum = null;\n     private Double intermediateCount = null;\n-    \n+\n     @Override\n     public void accumulate(Tuple b) throws IOException {\n         try {\n@@ -251,7 +251,7 @@ public void accumulate(Tuple b) throws IOException {\n                 intermediateSum = 0.0;\n                 intermediateCount = 0.0;\n             }\n-            \n+\n             double count = (Long)count(b);\n \n             if (count > 0) {\n@@ -263,9 +263,9 @@ public void accumulate(Tuple b) throws IOException {\n         } catch (Exception e) {\n             int errCode = 2106;\n             String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-            throw new ExecException(msg, errCode, PigException.BUG, e);           \n+            throw new ExecException(msg, errCode, PigException.BUG, e);\n         }\n-    }        \n+    }\n \n     @Override\n     public void cleanup() {\n@@ -280,5 +280,5 @@ public Double getValue() {\n             avg = new Double(intermediateSum / intermediateCount);\n         }\n         return avg;\n-    }    \n+    }\n }", "filename": "src/org/apache/pig/builtin/DoubleAvg.java"}, {"additions": 21, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/FloatAvg.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/FloatAvg.java", "sha": "ddf0b127798ff9090f10ff3fdc59997a085d916d", "changes": 39, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/FloatAvg.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -24,19 +24,19 @@\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n  * This method should never be used directly, use {@link AVG}.\n  */\n public class FloatAvg extends EvalFunc<Double> implements Algebraic, Accumulator<Double> {\n-    \n+\n     private static TupleFactory mTupleFactory = TupleFactory.getInstance();\n \n     @Override\n@@ -53,21 +53,24 @@ public Double exec(Tuple input) throws IOException {\n             Double avg = null;\n             if (count > 0)\n                 avg = new Double(sum / count);\n-    \n+\n             return avg;\n         } catch (ExecException ee) {\n             throw ee;\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -96,9 +99,9 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n-                \n+\n         }\n     }\n \n@@ -113,7 +116,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -141,7 +144,7 @@ public Double exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -162,7 +165,7 @@ static protected Tuple combine(DataBag values) throws ExecException {\n             Tuple t = it.next();\n             Double d = (Double)t.get(0);\n             // we count nulls in avg as contributing 0\n-            // a departure from SQL for performance of \n+            // a departure from SQL for performance of\n             // COUNT() which implemented by just inspecting\n             // size of the bag\n             if(d == null) {\n@@ -199,7 +202,7 @@ static protected Double sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n \n         // if we were handed an empty bag, return NULL\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -225,17 +228,17 @@ static protected Double sum(Tuple input) throws ExecException, IOException {\n             return null;\n         }\n     }\n-    \n+\n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE));\n     }\n-    \n+\n     /* Accumulator interface */\n \n     private Double intermediateSum = null;\n     private Double intermediateCount = null;\n-    \n+\n     @Override\n     public void accumulate(Tuple b) throws IOException {\n         try {\n@@ -248,9 +251,9 @@ public void accumulate(Tuple b) throws IOException {\n                 intermediateSum = 0.0;\n                 intermediateCount = 0.0;\n             }\n-            \n+\n             double count = (Long)count(b);\n-            \n+\n             if (count > 0) {\n                 intermediateCount += count;\n                 intermediateSum += sum;\n@@ -260,9 +263,9 @@ public void accumulate(Tuple b) throws IOException {\n         } catch (Exception e) {\n             int errCode = 2106;\n             String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-            throw new ExecException(msg, errCode, PigException.BUG, e);           \n+            throw new ExecException(msg, errCode, PigException.BUG, e);\n         }\n-    }        \n+    }\n \n     @Override\n     public void cleanup() {\n@@ -277,6 +280,6 @@ public Double getValue() {\n             avg = new Double(intermediateSum / intermediateCount);\n         }\n         return avg;\n-    }    \n+    }\n \n }", "filename": "src/org/apache/pig/builtin/FloatAvg.java"}, {"additions": 20, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/IntAvg.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/IntAvg.java", "sha": "7e224c6e33a484f01958cc982192d1276443387e", "changes": 37, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/IntAvg.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -24,19 +24,19 @@\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n  * This method should never be used directly, use {@link AVG}.\n  */\n public class IntAvg extends EvalFunc<Double> implements Algebraic, Accumulator<Double> {\n-    \n+\n     private static TupleFactory mTupleFactory = TupleFactory.getInstance();\n \n     @Override\n@@ -53,29 +53,32 @@ public Double exec(Tuple input) throws IOException {\n             Double avg = null;\n             if (count > 0)\n                 avg = new Double(sum / count);\n-    \n+\n             return avg;\n         } catch (ExecException ee) {\n             throw ee;\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n \n     static public class Initial extends EvalFunc<Tuple> {\n         @Override\n         public Tuple exec(Tuple input) throws IOException {\n-            \n+\n             try {\n                 Tuple t = mTupleFactory.newTuple(2);\n                 // input is a bag with one tuple containing\n@@ -97,9 +100,9 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n-                \n+\n         }\n     }\n \n@@ -114,7 +117,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -142,7 +145,7 @@ public Double exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -163,7 +166,7 @@ static protected Tuple combine(DataBag values) throws ExecException {\n             Tuple t = it.next();\n             Long l = (Long)t.get(0);\n             // we count nulls in avg as contributing 0\n-            // a departure from SQL for performance of \n+            // a departure from SQL for performance of\n             // COUNT() which implemented by just inspecting\n             // size of the bag\n             if(l == null) {\n@@ -200,7 +203,7 @@ static protected Long sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n \n         // if we were handed an empty bag, return NULL\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -211,7 +214,7 @@ static protected Long sum(Tuple input) throws ExecException, IOException {\n             try {\n                 Integer i = (Integer)(t.get(0));\n                 // we count nulls in avg as contributing 0\n-                // a departure from SQL for performance of \n+                // a departure from SQL for performance of\n                 // COUNT() which implemented by just inspecting\n                 // size of the bag\n                 if (i == null) continue;\n@@ -230,17 +233,17 @@ static protected Long sum(Tuple input) throws ExecException, IOException {\n             return null;\n         }\n     }\n-    \n+\n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE));\n     }\n \n     /* Accumulator interface */\n \n     private Long intermediateSum = null;\n     private Double intermediateCount = null;\n-    \n+\n     @Override\n     public void accumulate(Tuple b) throws IOException {\n         try {\n@@ -253,7 +256,7 @@ public void accumulate(Tuple b) throws IOException {\n                 intermediateSum = 0L;\n                 intermediateCount = 0.0;\n             }\n-            \n+\n             double count = (Long)count(b);\n \n             if (count > 0) {\n@@ -265,9 +268,9 @@ public void accumulate(Tuple b) throws IOException {\n         } catch (Exception e) {\n             int errCode = 2106;\n             String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-            throw new ExecException(msg, errCode, PigException.BUG, e);           \n+            throw new ExecException(msg, errCode, PigException.BUG, e);\n         }\n-    }        \n+    }\n \n     @Override\n     public void cleanup() {", "filename": "src/org/apache/pig/builtin/IntAvg.java"}, {"additions": 20, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/LongAvg.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/LongAvg.java", "sha": "de16c957492d13399953e2f4714602b152c4133b", "changes": 37, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/LongAvg.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -24,19 +24,19 @@\n import org.apache.pig.Algebraic;\n import org.apache.pig.EvalFunc;\n import org.apache.pig.PigException;\n+import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.data.DataBag;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.Tuple;\n import org.apache.pig.data.TupleFactory;\n import org.apache.pig.impl.logicalLayer.schema.Schema;\n-import org.apache.pig.backend.executionengine.ExecException;\n \n \n /**\n  * This method should never be used directly, use {@link AVG}.\n  */\n public class LongAvg extends EvalFunc<Double> implements Algebraic, Accumulator<Double> {\n-    \n+\n     private static TupleFactory mTupleFactory = TupleFactory.getInstance();\n \n     @Override\n@@ -53,21 +53,24 @@ public Double exec(Tuple input) throws IOException {\n             Double avg = null;\n             if (count > 0)\n                 avg = new Double(sum / count);\n-    \n+\n             return avg;\n         } catch (ExecException ee) {\n             throw ee;\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -96,9 +99,9 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n-                \n+\n         }\n     }\n \n@@ -113,7 +116,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -141,7 +144,7 @@ public Double exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -162,7 +165,7 @@ static protected Tuple combine(DataBag values) throws ExecException {\n             Tuple t = it.next();\n             Long l = (Long)t.get(0);\n             // we count nulls in avg as contributing 0\n-            // a departure from SQL for performance of \n+            // a departure from SQL for performance of\n             // COUNT() which implemented by just inspecting\n             // size of the bag\n             if(l == null) {\n@@ -199,7 +202,7 @@ static protected Long sum(Tuple input) throws ExecException, IOException {\n         DataBag values = (DataBag)input.get(0);\n \n         // if we were handed an empty bag, return NULL\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -225,17 +228,17 @@ static protected Long sum(Tuple input) throws ExecException, IOException {\n             return null;\n         }\n     }\n-    \n+\n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.DOUBLE));\n     }\n-    \n+\n     /* Accumulator interface */\n-   \n+\n     private Long intermediateSum = null;\n     private Double intermediateCount = null;\n-    \n+\n     @Override\n     public void accumulate(Tuple b) throws IOException {\n         try {\n@@ -248,7 +251,7 @@ public void accumulate(Tuple b) throws IOException {\n                 intermediateSum = 0L;\n                 intermediateCount = 0.0;\n             }\n-            \n+\n             double count = (Long)count(b);\n \n             if (count > 0) {\n@@ -260,9 +263,9 @@ public void accumulate(Tuple b) throws IOException {\n         } catch (Exception e) {\n             int errCode = 2106;\n             String msg = \"Error while computing average in \" + this.getClass().getSimpleName();\n-            throw new ExecException(msg, errCode, PigException.BUG, e);           \n+            throw new ExecException(msg, errCode, PigException.BUG, e);\n         }\n-    }        \n+    }\n \n     @Override\n     public void cleanup() {", "filename": "src/org/apache/pig/builtin/LongAvg.java"}, {"additions": 15, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/StringMax.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/StringMax.java", "sha": "788bc779e53172b4954d062af9a7bbfc40beba1a", "changes": 27, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/StringMax.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -45,14 +45,17 @@ public String exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -77,7 +80,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing max in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -94,7 +97,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing max in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -108,17 +111,17 @@ public String exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing max in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n \n     static protected String max(Tuple input) throws ExecException {\n         DataBag values = (DataBag)input.get(0);\n-        \n+\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -129,7 +132,7 @@ static protected String max(Tuple input) throws ExecException {\n             Tuple t = it.next();\n             curMax = (String)(t.get(0));\n         }\n-        \n+\n         for (; it.hasNext();) {\n             Tuple t = it.next();\n             try {\n@@ -138,26 +141,26 @@ static protected String max(Tuple input) throws ExecException {\n                 if( s.compareTo(curMax) > 0) {\n                     curMax = s;\n                 }\n-                \n+\n             } catch (RuntimeException exp) {\n                 int errCode = 2103;\n                 String msg = \"Problem while computing max of strings.\";\n                 throw new ExecException(msg, errCode, PigException.BUG, exp);\n             }\n         }\n-    \n+\n         return curMax;\n     }\n \n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY));\n     }\n \n \n     /* accumulator interface */\n     private String intermediateMax = null;\n-    \n+\n     @Override\n     public void accumulate(Tuple b) throws IOException {\n         try {\n@@ -168,14 +171,14 @@ public void accumulate(Tuple b) throws IOException {\n             // check if it lexicographically follows curMax\n             if (intermediateMax == null || intermediateMax.compareTo(curMax) < 0) {\n                 intermediateMax = curMax;\n-            }            \n+            }\n \n         } catch (ExecException ee) {\n             throw ee;\n         } catch (Exception e) {\n             int errCode = 2106;\n             String msg = \"Error while computing max in \" + this.getClass().getSimpleName();\n-            throw new ExecException(msg, errCode, PigException.BUG, e);           \n+            throw new ExecException(msg, errCode, PigException.BUG, e);\n         }\n     }\n ", "filename": "src/org/apache/pig/builtin/StringMax.java"}, {"additions": 16, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/StringMin.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/src/org/apache/pig/builtin/StringMin.java", "sha": "f71c2eb4494ec42eb6f832e62e51a7b62b9b4356", "changes": 29, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/StringMin.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -46,14 +46,17 @@ public String exec(Tuple input) throws IOException {\n         }\n     }\n \n+    @Override\n     public String getInitial() {\n         return Initial.class.getName();\n     }\n \n+    @Override\n     public String getIntermed() {\n         return Intermediate.class.getName();\n     }\n \n+    @Override\n     public String getFinal() {\n         return Final.class.getName();\n     }\n@@ -78,7 +81,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing min in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -95,7 +98,7 @@ public Tuple exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing min in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n@@ -109,17 +112,17 @@ public String exec(Tuple input) throws IOException {\n             } catch (Exception e) {\n                 int errCode = 2106;\n                 String msg = \"Error while computing min in \" + this.getClass().getSimpleName();\n-                throw new ExecException(msg, errCode, PigException.BUG, e);           \n+                throw new ExecException(msg, errCode, PigException.BUG, e);\n             }\n         }\n     }\n \n     static protected String min(Tuple input) throws ExecException {\n         DataBag values = (DataBag)input.get(0);\n-        \n+\n         // if we were handed an empty bag, return NULL\n         // this is in compliance with SQL standard\n-        if(values.size() == 0) {\n+        if(values == null || values.size() == 0) {\n             return null;\n         }\n \n@@ -130,7 +133,7 @@ static protected String min(Tuple input) throws ExecException {\n             Tuple t = it.next();\n             curMin = (String)(t.get(0));\n         }\n-        \n+\n         for (; it.hasNext();) {\n             Tuple t = it.next();\n             try {\n@@ -139,25 +142,25 @@ static protected String min(Tuple input) throws ExecException {\n                 if( s.compareTo(curMin) < 0) {\n                     curMin = s;\n                 }\n-                \n+\n             } catch (RuntimeException exp) {\n                 int errCode = 2103;\n                 String msg = \"Problem while computing min of strings.\";\n                 throw new ExecException(msg, errCode, PigException.BUG, exp);\n             }\n         }\n-    \n+\n         return curMin;\n     }\n \n     @Override\n     public Schema outputSchema(Schema input) {\n-        return new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY)); \n+        return new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY));\n     }\n-    \n+\n     /* accumulator interface */\n     private String intermediateMin = null;\n-    \n+\n     @Override\n     public void accumulate(Tuple b) throws IOException {\n         try {\n@@ -168,14 +171,14 @@ public void accumulate(Tuple b) throws IOException {\n             // check if it lexicographically follows curMax\n             if (intermediateMin == null || intermediateMin.compareTo(curMin) > 0) {\n                 intermediateMin = curMin;\n-            }            \n+            }\n \n         } catch (ExecException ee) {\n             throw ee;\n         } catch (Exception e) {\n             int errCode = 2106;\n             String msg = \"Error while computing max in \" + this.getClass().getSimpleName();\n-            throw new ExecException(msg, errCode, PigException.BUG, e);           \n+            throw new ExecException(msg, errCode, PigException.BUG, e);\n         }\n     }\n ", "filename": "src/org/apache/pig/builtin/StringMin.java"}, {"additions": 18, "raw_url": "https://github.com/apache/pig/raw/18c844e6e3254586d4b5ba9840c054c66c513adc/test/org/apache/pig/test/TestBuiltin.java", "blob_url": "https://github.com/apache/pig/blob/18c844e6e3254586d4b5ba9840c054c66c513adc/test/org/apache/pig/test/TestBuiltin.java", "sha": "50afa26f2d32be15fad3b50203190c4dfc955074", "changes": 19, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestBuiltin.java?ref=18c844e6e3254586d4b5ba9840c054c66c513adc", "patch": "@@ -143,6 +143,8 @@\n     private TupleFactory tupleFactory = TupleFactory.getInstance();\n     private BagFactory bagFactory = DefaultBagFactory.getInstance();\n \n+    private static Tuple NULL_INPUT_TUPLE;\n+\n     // some inputs\n     private static Integer[] intInput = { 3, 1, 2, 4, 5, 7, null, 6, 8, 9, 10 };\n     private static Long[] intAsLong = { 3L, 1L, 2L, 4L, 5L, 7L, null, 6L, 8L, 9L, 10L };\n@@ -222,6 +224,9 @@ public void setUp() throws Exception {\n         // first set up EvalFuncMap and expectedMap\n         setupEvalFuncMap();\n \n+        NULL_INPUT_TUPLE = TupleFactory.getInstance().newTuple(1);\n+        NULL_INPUT_TUPLE.set(0, null);\n+\n         expectedMap.put(\"SUM\", new Double(55));\n         expectedMap.put(\"DoubleSum\", new Double(170.567391834593));\n         expectedMap.put(\"IntSum\", new Long(55));\n@@ -955,6 +960,9 @@ public void testAVG() throws Exception {\n             } else {\n                 assertEquals(msg, (Double)output, (Double)getExpected(avgTypes[k]), 0.00001);\n             }\n+\n+            // Check null input\n+            assertNull(avg.exec(NULL_INPUT_TUPLE));\n         }\n     }\n \n@@ -1423,6 +1431,9 @@ else if (inputType == \"BigInteger\")\n             else {\n                 assertEquals(msg, (Double)output, (Double)getExpected(sumTypes[k]), 0.00001);\n             }\n+\n+            // Check null input\n+            assertNull(sum.exec(NULL_INPUT_TUPLE));\n         }\n     }\n \n@@ -1488,6 +1499,9 @@ public void testMIN() throws Exception {\n             String msg = \"[Testing \" + minTypes[k] + \" on input type: \" + getInputType(minTypes[k]) + \" ( (output) \" +\n                            output + \" == \" + getExpected(minTypes[k]) + \" (expected) )]\";\n             assertForInputType(inputType, msg, getExpected(minTypes[k]), output);\n+\n+            // Check null input\n+            assertNull(min.exec(NULL_INPUT_TUPLE));\n         }\n     }\n \n@@ -1554,6 +1568,9 @@ public void testMAX() throws Exception {\n             String msg = \"[Testing \" + maxTypes[k] + \" on input type: \" + getInputType(maxTypes[k]) + \" ( (output) \" +\n                            output + \" == \" + getExpected(maxTypes[k]) + \" (expected) )]\";\n             assertForInputType(inputType, msg, getExpected(maxTypes[k]), output);\n+\n+            // Check null input\n+            assertNull(max.exec(NULL_INPUT_TUPLE));\n         }\n     }\n \n@@ -3223,6 +3240,6 @@ public void testToMapSchema() throws Exception {\n         pigServer.registerQuery(\"B = foreach A generate TOMAP(a);\");\n         s = pigServer.dumpSchema(\"B\");\n         Assert.assertEquals(s.toString(), \"{map[]}\");\n-        \n+\n     }\n }", "filename": "test/org/apache/pig/test/TestBuiltin.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/23dc5ce42e783a32fd5713de05e9e330cc429731", "parent": "https://github.com/apache/pig/commit/c2aedcc66486ddc721a32dc4984547f049aa5541", "message": "PIG-3806: PigServer constructor throws NPE after PIG-3765 (aniket486)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1576549 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_16", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/23dc5ce42e783a32fd5713de05e9e330cc429731/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/23dc5ce42e783a32fd5713de05e9e330cc429731/CHANGES.txt", "sha": "70b6d59884318e7be39758c61b14231baeb4a70c", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=23dc5ce42e783a32fd5713de05e9e330cc429731", "patch": "@@ -99,6 +99,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-3806: PigServer constructor throws NPE after PIG-3765 (aniket486)\n+\n PIG-3801: Auto local mode does not call storeSchema (aniket486)\n \n PIG-3754: InputSizeReducerEstimator.getTotalInputFileSize reports incorrect size (aniket486)", "filename": "CHANGES.txt"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/23dc5ce42e783a32fd5713de05e9e330cc429731/src/org/apache/pig/PigServer.java", "blob_url": "https://github.com/apache/pig/blob/23dc5ce42e783a32fd5713de05e9e330cc429731/src/org/apache/pig/PigServer.java", "sha": "54515206ca52220ccb68faf19ff54c6a034d2255", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/PigServer.java?ref=23dc5ce42e783a32fd5713de05e9e330cc429731", "patch": "@@ -233,6 +233,8 @@ public PigServer(PigContext context, boolean connect) throws ExecException {\n         if (connect) {\n             pigContext.connect();\n         }\n+        \n+        this.filter = new BlackAndWhitelistFilter(this);\n \n         addJarsFromProperties();\n         markPredeployedJarsFromProperties();\n@@ -244,8 +246,6 @@ public PigServer(PigContext context, boolean connect) throws ExecException {\n         if (ScriptState.get() == null) {\n             ScriptState.start(pigContext.getExecutionEngine().instantiateScriptState());\n         }\n-\n-        this.filter = new BlackAndWhitelistFilter(this);\n     }\n \n     private void addJarsFromProperties() throws ExecException {", "filename": "src/org/apache/pig/PigServer.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/a249156f72465e3320fde609a4958af886d68029", "parent": "https://github.com/apache/pig/commit/750ec4a3d456a58c732114fb809499338e715eaf", "message": "PIG-3657: New partition filter extractor fails with NPE (cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1556906 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_17", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/a249156f72465e3320fde609a4958af886d68029/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/a249156f72465e3320fde609a4958af886d68029/CHANGES.txt", "sha": "8a1a55416db28006123ac83491dc8c25d8883064", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=a249156f72465e3320fde609a4958af886d68029", "patch": "@@ -64,6 +64,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-3657: New partition filter extractor fails with NPE (cheolsoo)\n+\n PIG-3617: problem with temp file deletion in MAPREDUCE operator (nezihyigitbasi via cheolsoo)\n \n PIG-3649: POPartialAgg incorrectly calculates size reduction when multiple values aggregated (tmwoodruff via daijy)", "filename": "CHANGES.txt"}, {"additions": 15, "raw_url": "https://github.com/apache/pig/raw/a249156f72465e3320fde609a4958af886d68029/src/org/apache/pig/newplan/FilterExtractor.java", "blob_url": "https://github.com/apache/pig/blob/a249156f72465e3320fde609a4958af886d68029/src/org/apache/pig/newplan/FilterExtractor.java", "sha": "2a327d37af69e195757e3599e934f6afc6bc5aee", "changes": 18, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/newplan/FilterExtractor.java?ref=a249156f72465e3320fde609a4958af886d68029", "patch": "@@ -180,23 +180,35 @@ private LogicalExpression addToFilterPlan(LogicalExpression op) throws FrontendE\n     }\n \n     private LogicalExpression andLogicalExpressions(\n-            LogicalExpressionPlan plan, LogicalExpression a, LogicalExpression b) {\n+            LogicalExpressionPlan plan, LogicalExpression a, LogicalExpression b) throws FrontendException {\n         if (a == null) {\n             return b;\n         }\n         if (b == null) {\n             return a;\n         }\n+        if (!plan.ops.contains(a)) {\n+            a = a.deepCopy(plan);\n+        }\n+        if (!plan.ops.contains(b)) {\n+            b = b.deepCopy(plan);\n+        }\n         LogicalExpression andOp = new AndExpression(plan, a, b);\n         return andOp;\n     }\n \n     private LogicalExpression orLogicalExpressions(\n-            LogicalExpressionPlan plan, LogicalExpression a, LogicalExpression b) {\n+            LogicalExpressionPlan plan, LogicalExpression a, LogicalExpression b) throws FrontendException {\n         // Or 2 operators if they are not null\n         if (a == null || b == null) {\n             return null;\n         }\n+        if (!plan.ops.contains(a)) {\n+            a = a.deepCopy(plan);\n+        }\n+        if (!plan.ops.contains(b)) {\n+            b = b.deepCopy(plan);\n+        }\n         LogicalExpression orOp = new OrExpression(plan, a, b);\n         return orOp;\n     }\n@@ -234,7 +246,7 @@ private KeyState checkPushDown(BinaryExpression binExpr) throws FrontendExceptio\n             //              AND (leftState.filterExpr OR rightState.pushdownExpr)\n             //              AND (leftState.filterExpr OR rightState.filterExpr)\n             state.pushdownExpr = orLogicalExpressions(pushdownExprPlan, leftState.pushdownExpr, rightState.pushdownExpr);\n-            if(state.pushdownExpr == null) {\n+            if (state.pushdownExpr == null) {\n                 // Whatever we did so far on the right tree is all wasted :(\n                 // Undo all the mutation (AND OR distributions) until now\n                 removeFromFilteredPlan(leftState.filterExpr);", "filename": "src/org/apache/pig/newplan/FilterExtractor.java"}, {"additions": 15, "raw_url": "https://github.com/apache/pig/raw/a249156f72465e3320fde609a4958af886d68029/test/org/apache/pig/test/TestNewPartitionFilterPushDown.java", "blob_url": "https://github.com/apache/pig/blob/a249156f72465e3320fde609a4958af886d68029/test/org/apache/pig/test/TestNewPartitionFilterPushDown.java", "sha": "703d86f738dd882605cb4ab1bc85cd55d34625d0", "changes": 15, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestNewPartitionFilterPushDown.java?ref=a249156f72465e3320fde609a4958af886d68029", "patch": "@@ -61,6 +61,7 @@\n import org.apache.pig.newplan.logical.expression.ProjectExpression;\n import org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer;\n import org.apache.pig.newplan.logical.relational.LOFilter;\n+import org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor;\n import org.apache.pig.newplan.logical.relational.LogicalPlan;\n import org.apache.pig.newplan.logical.rules.LoadTypeCastInserter;\n import org.apache.pig.newplan.logical.rules.PartitionFilterOptimizer;\n@@ -466,6 +467,10 @@ private void testFull(String q, String partFilter, String filterExpr, boolean un\n                         (it.next() instanceof LOFilter));\n             }\n         }\n+\n+        // Test that the filtered plan can be translated to physical plan (PIG-3657)\n+        LogToPhyTranslationVisitor translator = new LogToPhyTranslationVisitor(newLogicalPlan);\n+        translator.visit();\n     }\n \n     /**\n@@ -671,6 +676,16 @@ public void testOrWithNonPartitionCondition() throws Exception {\n         negativeTest(q, Arrays.asList(\"srcid\", \"mrkt\", \"dstid\"));\n     }\n \n+    // PIG-3657\n+    @Test\n+    public void testFilteredPlanWithLogToPhyTranslator() throws Exception {\n+        String q = \"a = load 'foo' using \" + TestLoader.class.getName() +\n+                \"('srcid:int, mrkt:chararray', 'srcid') as (f1, f2);\" +\n+                \"b = filter a by (f1 < 5 or (f1 == 10 and f2 == 'UK'));\" +\n+                \"store b into 'out';\";\n+        testFull(q, \"((srcid < 5) or (srcid == 10))\", \"((f1 < 5) or (f2 == 'UK'))\", false);\n+    }\n+\n     //// helper methods ///////\n     private FilterExtractor test(String query, List<String> partitionCols,\n             String expPartFilterString, String expFilterString)", "filename": "test/org/apache/pig/test/TestNewPartitionFilterPushDown.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/9bdf3e10dcd1d78c32911488df61c7a200532824", "parent": "https://github.com/apache/pig/commit/ea8fd2df27c0a3b20bfc52f6c1a308f9f944a7d4", "message": "PIG-4712: [Pig on Tez] NPE in Bloom UDF after Union (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1710672 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_18", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/9bdf3e10dcd1d78c32911488df61c7a200532824/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/9bdf3e10dcd1d78c32911488df61c7a200532824/CHANGES.txt", "sha": "1227b5d828077b56466c32c5b94709e99baceb2a", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=9bdf3e10dcd1d78c32911488df61c7a200532824", "patch": "@@ -57,6 +57,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4712: [Pig on Tez] NPE in Bloom UDF after Union (rohini)\n+\n PIG-4707: [Pig on Tez] Streaming job hangs with pig.exec.mapPartAgg=true (rohini)\n \n PIG-4703: TezOperator.stores shall not ship to backend (daijy)", "filename": "CHANGES.txt"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/9bdf3e10dcd1d78c32911488df61c7a200532824/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java", "blob_url": "https://github.com/apache/pig/blob/9bdf3e10dcd1d78c32911488df61c7a200532824/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java", "sha": "818bb4c01fee40bd4a05dfa1387ed1f6c4b643f1", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java?ref=9bdf3e10dcd1d78c32911488df61c7a200532824", "patch": "@@ -566,6 +566,8 @@ public POUserFunc clone() throws CloneNotSupportedException {\n             requestedParallelism, null, funcSpec.clone());\n         clone.setResultType(resultType);\n         clone.signature = signature;\n+        clone.cacheFiles = cacheFiles;\n+        clone.shipFiles = shipFiles;\n         return clone;\n     }\n ", "filename": "src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java"}, {"additions": 24, "raw_url": "https://github.com/apache/pig/raw/9bdf3e10dcd1d78c32911488df61c7a200532824/test/e2e/pig/tests/nightly.conf", "blob_url": "https://github.com/apache/pig/blob/9bdf3e10dcd1d78c32911488df61c7a200532824/test/e2e/pig/tests/nightly.conf", "sha": "644de3bd92b48c1325309eed88b546ecb0e695b1", "changes": 24, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/e2e/pig/tests/nightly.conf?ref=9bdf3e10dcd1d78c32911488df61c7a200532824", "patch": "@@ -5148,6 +5148,30 @@ store C into ':OUTPATH:';\\,\n                                 C = load ':INPATH:/singlefile/votertab10k'as (name:chararray, age:int, reg:chararray, contrib:float);\n                                 D = join C by name, B by name;\n                                 store D into ':OUTPATH:';\",\n+                    },{\n+                        'num' => 4,\n+                        'pig' => \"set pig.optimizer.rules.disabled PushUpFilter;\n+                                define bb BuildBloom('Hash.JENKINS_HASH', 'fixed', '128', '3');\n+                                A = LOAD ':INPATH:/singlefile/studenttab10k' AS (name:chararray, age:int, gpa:double);\n+                                B = filter A by name == 'alice allen';\n+                                C = group B all;\n+                                D = foreach C generate bb(B.name);\n+                                store D into ':HDFSTMP:/mybloom_4';\n+                                exec;\n+                                define bloom Bloom(':HDFSTMP:/mybloom_4');\n+                                E = LOAD ':INPATH:/singlefile/studenttab10k' AS (name:chararray, age:int, gpa:double);\n+                                F = LOAD ':INPATH:/singlefile/studenttab10k' AS (name:chararray, age:int, gpa:double);\n+                                G = union E, F;\n+                                -- PushUpFilter is disabled to avoid filter being pushed before union\n+                                H = filter G by bloom(name);\n+                                store H into ':OUTPATH:';\",\n+                        'notmq' => 1,\n+                        'verify_pig_script' => \"\n+                                A = LOAD ':INPATH:/singlefile/studenttab10k' AS (name, age:int ,gpa:double);\n+                                B = LOAD ':INPATH:/singlefile/studenttab10k' AS (name, age:int ,gpa:double);\n+                                C = UNION A,B;\n+                                D = filter C by name == 'alice allen';\n+                                store D into ':OUTPATH:';\",\n                     }\n                 ],\n             },{", "filename": "test/e2e/pig/tests/nightly.conf"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13", "parent": "https://github.com/apache/pig/commit/95830c900eacd18919bd4e47e561f0f52ff335ee", "message": "PIG:3302: JSONStorage throws NPE if map has null values (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1482145 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_19", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/CHANGES.txt", "sha": "ccaf586525b8bf2dc6c46d11cd8f356e571b8ddd", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13", "patch": "@@ -176,6 +176,8 @@ PIG-3013: BinInterSedes improve chararray sort performance (rohini)\n \n BUG FIXES\n \n+PIG:3302: JSONStorage throws NPE if map has null values (rohini)\n+\n PIG-3309: TestJsonLoaderStorage fails with IBM JDK 6/7 (lrangel via daijy)\n \n PIG-3097: HiveColumnarLoader doesn't correctly load partitioned Hive table (maczech via daijy)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/src/org/apache/pig/builtin/JsonLoader.java", "blob_url": "https://github.com/apache/pig/blob/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/src/org/apache/pig/builtin/JsonLoader.java", "sha": "fe67767cf8242dde9b13cc7065979c74265968e0", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/JsonLoader.java?ref=0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13", "patch": "@@ -225,7 +225,7 @@ private Object readField(JsonParser p,\n             Map<String, String> m = new HashMap<String, String>();\n             while (p.nextToken() != JsonToken.END_OBJECT) {\n                 String k = p.getCurrentName();\n-                String v = p.getText();\n+                String v = p.getCurrentToken() == JsonToken.VALUE_NULL ? null : p.getText();\n                 m.put(k, v);\n             }\n             return m;", "filename": "src/org/apache/pig/builtin/JsonLoader.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/src/org/apache/pig/builtin/JsonStorage.java", "blob_url": "https://github.com/apache/pig/blob/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/src/org/apache/pig/builtin/JsonStorage.java", "sha": "d1f3d2dc08aebbb96139d594366627da84f063cc", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/JsonStorage.java?ref=0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13", "patch": "@@ -199,7 +199,7 @@ private void writeField(JsonGenerator json,\n             json.writeFieldName(field.getName());\n             json.writeStartObject();\n             for (Map.Entry<String, Object> e : ((Map<String, Object>)d).entrySet()) {\n-                json.writeStringField(e.getKey(), e.getValue().toString());\n+                json.writeStringField(e.getKey(), e.getValue() == null ? null : e.getValue().toString());\n             }\n             json.writeEndObject();\n             return;", "filename": "src/org/apache/pig/builtin/JsonStorage.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/test/org/apache/pig/test/data/jsonStorage1.result", "blob_url": "https://github.com/apache/pig/blob/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/test/org/apache/pig/test/data/jsonStorage1.result", "sha": "587fead95cc5323cdd64e4473390f62d06bf09b6", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/jsonStorage1.result?ref=0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13", "patch": "@@ -1,2 +1,2 @@\n {\"a0\":1,\"a1\":[{\"a10\":1,\"a11\":\"tom\"},{\"a10\":2,\"a11\":\"jerry\"}],\"a2\":{\"a20\":1.01,\"a21\":\"sun\"},\"a3\":{\"key3\":\"c\",\"key2\":\"b\",\"key1\":\"a\"}}\n-{\"a0\":2,\"a1\":[{\"a10\":6,\"a11\":\"cat\"},{\"a10\":7,\"a11\":\"dog\"},{\"a10\":8,\"a11\":\"pig\"}],\"a2\":{\"a20\":2.3,\"a21\":\"moon\"},\"a3\":{\"key4\":\"value4\",\"key1\":\"value1\"}}\n+{\"a0\":2,\"a1\":[{\"a10\":6,\"a11\":\"cat\"},{\"a10\":7,\"a11\":\"dog\"},{\"a10\":8,\"a11\":\"pig\"}],\"a2\":{\"a20\":2.3,\"a21\":\"moon\"},\"a3\":{\"key4\":\"value4\",\"key1\":\"value1\",\"key2\":null}}", "filename": "test/org/apache/pig/test/data/jsonStorage1.result"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/test/org/apache/pig/test/data/jsonStorage1.txt", "blob_url": "https://github.com/apache/pig/blob/0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13/test/org/apache/pig/test/data/jsonStorage1.txt", "sha": "1df3eb88d3c6b227eb75914c866019b612047d6c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/jsonStorage1.txt?ref=0cf5c7aa85471942cbc7e1d94a5108bdd4a28f13", "patch": "@@ -1,2 +1,2 @@\n 1\t{(1,tom),(2,jerry)}\t(1.01,sun)\t[key3#c,key2#b,key1#a]\n-2\t{(6,cat),(7,dog),(8,pig)}\t(2.3,moon)\t[key4#value4,key1#value1]\n+2\t{(6,cat),(7,dog),(8,pig)}\t(2.3,moon)\t[key4#value4,key1#value1,key2#]", "filename": "test/org/apache/pig/test/data/jsonStorage1.txt"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/e6326d2618b61075ba62c78049b005705ec894f2", "parent": "https://github.com/apache/pig/commit/5c43bd4b687bba27d2abca79e2fc08a973626f39", "message": "PIG-3132: NPE when illustrating a relation with HCatLoader\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1460784 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_20", "file": [{"additions": 17, "raw_url": "https://github.com/apache/pig/raw/e6326d2618b61075ba62c78049b005705ec894f2/test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java", "blob_url": "https://github.com/apache/pig/blob/e6326d2618b61075ba62c78049b005705ec894f2/test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java", "sha": "4d8151f7bc2fdca14ca543707eea4965cc267e9c", "changes": 17, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java?ref=e6326d2618b61075ba62c78049b005705ec894f2", "patch": "@@ -1,3 +1,20 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.pig.test.utils;\n \n import java.io.IOException;", "filename": "test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/f878f90c267ec45ef1091d98b7d92e77020d5ea4", "parent": "https://github.com/apache/pig/commit/e59573212a5c450ace0e8de8077bfef6f148fbff", "message": "PIG-3132: NPE when illustrating a relation with HCatLoader\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1457884 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_21", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/f878f90c267ec45ef1091d98b7d92e77020d5ea4/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/f878f90c267ec45ef1091d98b7d92e77020d5ea4/CHANGES.txt", "sha": "3cfdf3bf19430b05c51a10a420746257790707dc", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=f878f90c267ec45ef1091d98b7d92e77020d5ea4", "patch": "@@ -249,6 +249,8 @@ OPTIMIZATIONS\n \n BUG FIXES\n \n+PIG-3132: NPE when illustrating a relation with HCatLoader (daijy)\n+\n PIG-3194: Changes to ObjectSerializer.java break compatibility with Hadoop 0.20.2 (prkommireddi via dvryaboy)\n \n PIG-3241: ConcurrentModificationException in POPartialAgg (dvryaboy)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/f878f90c267ec45ef1091d98b7d92e77020d5ea4/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java", "blob_url": "https://github.com/apache/pig/blob/f878f90c267ec45ef1091d98b7d92e77020d5ea4/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java", "sha": "55bd896245c8e3c42b8c25d2d8d78e12f853ea58", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java?ref=f878f90c267ec45ef1091d98b7d92e77020d5ea4", "patch": "@@ -93,7 +93,7 @@ public void setUp() throws IOException{\n         loader = new ReadToEndLoader((LoadFunc)\n                 PigContext.instantiateFuncFromSpec(lFile.getFuncSpec()), \n                 ConfigurationUtil.toConfiguration(pc.getProperties()), \n-                lFile.getFileName(),0);\n+                lFile.getFileName(),0, signature);\n     }\n     \n     /**", "filename": "src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java"}, {"additions": 19, "raw_url": "https://github.com/apache/pig/raw/f878f90c267ec45ef1091d98b7d92e77020d5ea4/src/org/apache/pig/impl/io/ReadToEndLoader.java", "blob_url": "https://github.com/apache/pig/blob/f878f90c267ec45ef1091d98b7d92e77020d5ea4/src/org/apache/pig/impl/io/ReadToEndLoader.java", "sha": "8435a3590e79f51b2e6043fe08bf8e22251a4edf", "changes": 20, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/impl/io/ReadToEndLoader.java?ref=f878f90c267ec45ef1091d98b7d92e77020d5ea4", "patch": "@@ -106,6 +106,8 @@\n     private InputFormat inputFormat = null;\n     \n     private PigContext pigContext;\n+    \n+    private String udfContextSignature = null;\n \n     /**\n      * @param wrappedLoadFunc\n@@ -133,6 +135,16 @@ public ReadToEndLoader(LoadFunc wrappedLoadFunc, Configuration conf,\n         this.pigContext = pigContext;\n         init();\n     }\n+    \n+    public ReadToEndLoader(LoadFunc wrappedLoadFunc, Configuration conf,\n+            String inputLocation, int splitIndex, String signature) throws IOException {\n+        this.udfContextSignature = signature;\n+        this.wrappedLoadFunc = wrappedLoadFunc;\n+        this.inputLocation = inputLocation;\n+        this.conf = conf;\n+        this.curSplitIndex = splitIndex;\n+        init();\n+    }\n \n     /**\n      * This constructor takes an array of split indexes (toReadSplitIdxs) of the \n@@ -167,6 +179,7 @@ private void init() throws IOException {\n \n         // let's initialize the wrappedLoadFunc \n         Job job = new Job(conf);\n+        wrappedLoadFunc.setUDFContextSignature(this.udfContextSignature);\n         wrappedLoadFunc.setLocation(inputLocation, \n                 job);\n         // The above setLocation call could write to the conf within\n@@ -277,7 +290,7 @@ public void prepareToRead(RecordReader reader, PigSplit split) {\n \n     @Override\n     public void setLocation(String location, Job job) throws IOException {\n-        //no-op\n+        wrappedLoadFunc.setLocation(location, job);\n     }\n \n     @Override\n@@ -313,4 +326,9 @@ public void setPartitionFilter(Expression partitionFilter) throws IOException {\n              ((LoadMetadata) wrappedLoadFunc).setPartitionFilter(partitionFilter);\n         }\n     }\n+    \n+    @Override\n+    public void setUDFContextSignature(String signature) {\n+        this.udfContextSignature = signature;\n+    }\n }", "filename": "src/org/apache/pig/impl/io/ReadToEndLoader.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/f878f90c267ec45ef1091d98b7d92e77020d5ea4/src/org/apache/pig/pen/LocalMapReduceSimulator.java", "blob_url": "https://github.com/apache/pig/blob/f878f90c267ec45ef1091d98b7d92e77020d5ea4/src/org/apache/pig/pen/LocalMapReduceSimulator.java", "sha": "416c78fd0a3fbb0d5b9d5ec9e5b60ca282557639", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/pen/LocalMapReduceSimulator.java?ref=f878f90c267ec45ef1091d98b7d92e77020d5ea4", "patch": "@@ -105,6 +105,7 @@ public void launchPig(PhysicalPlan php, Map<LOLoad, DataBag> baseData,\n         // jc is null only when mrp.size == 0\n         boolean needFileInput;\n         final ArrayList<OperatorKey> emptyInpTargets = new ArrayList<OperatorKey>();\n+        pc.getProperties().setProperty(\"pig.illustrating\", \"true\");\n         while(mrp.size() != 0) {\n             jc = jcc.compile(mrp, \"Illustrator\");\n             if(jc == null) {", "filename": "src/org/apache/pig/pen/LocalMapReduceSimulator.java"}, {"additions": 10, "raw_url": "https://github.com/apache/pig/raw/f878f90c267ec45ef1091d98b7d92e77020d5ea4/test/org/apache/pig/test/TestExampleGenerator.java", "blob_url": "https://github.com/apache/pig/blob/f878f90c267ec45ef1091d98b7d92e77020d5ea4/test/org/apache/pig/test/TestExampleGenerator.java", "sha": "f9b5203c4329b963f500136cb9e2e76bdd607f43", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestExampleGenerator.java?ref=f878f90c267ec45ef1091d98b7d92e77020d5ea4", "patch": "@@ -33,6 +33,7 @@\n import org.apache.pig.data.DataBag;\n import org.apache.pig.impl.PigContext;\n import org.apache.pig.newplan.Operator;\n+import org.apache.pig.test.utils.UDFContextTestLoaderWithSignature;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n@@ -394,5 +395,14 @@ public void testFilterGroupCountStore() throws Exception {\n     \n         assertNotNull(derivedData);\n     }\n+    \n+    @Test\n+    public void testLoaderWithContext() throws Exception {\n+        PigServer pigServer = new PigServer(pigContext);\n+        pigServer.registerQuery(\"A = load \" + A.toString() + \" using \" + UDFContextTestLoaderWithSignature.class.getName() + \"('a') as (x, y);\");\n+        Map<Operator, DataBag> derivedData = pigServer.getExamples(\"A\");\n+        \n+        assertNotNull(derivedData);\n+    }\n \n }", "filename": "test/org/apache/pig/test/TestExampleGenerator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/8cd6c1c4895dd81dbcb987e215586049eb5a7680", "parent": "https://github.com/apache/pig/commit/1edcdf86f7f5fa7fec7e78cb210fde35f2779f71", "message": "PIG-4315: MergeJoin or Split followed by order by gives NPE in Tez (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1700912 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_22", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/CHANGES.txt", "sha": "ef6e4878ac5fa11828fc7a322cedcb536373a62f", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -38,6 +38,8 @@ PIG-4639: Add better parser for Apache HTTPD access log (nielsbasjes via daijy)\n \n BUG FIXES\n \n+PIG-4315: MergeJoin or Split followed by order by gives NPE in Tez (rohini)\n+\n PIG-4654: Reduce tez memory.reserve-fraction and clear spillables for better memory utilization (rohini)\n \n PIG-4628: Pig 0.14 job with order by fails in mapreduce mode with Oozie (knoguchi)", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java", "sha": "6fbcec68e9fcbc99c34c1be86f20f638ea50c0fe", "changes": 8, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -193,10 +193,8 @@ public boolean disconnect(TezOperator from, TezOperator to) {\n     public void moveTree(TezOperator root, TezOperPlan newPlan) throws PlanException {\n         List<TezOperator> list = new ArrayList<TezOperator>();\n         list.add(root);\n-        int prevSize = 0;\n         int pos = 0;\n-        while (list.size() > prevSize) {\n-            prevSize = list.size();\n+        while (list.size() > pos) {\n             TezOperator node = list.get(pos);\n             if (getSuccessors(node)!=null) {\n                 for (TezOperator succ : getSuccessors(node)) {\n@@ -243,11 +241,11 @@ public void moveTree(TezOperator root, TezOperPlan newPlan) throws PlanException\n         }\n     }\n \n-    // This method is used in PigGraceShuffleVertexManager to get a list of grandparents. \n+    // This method is used in PigGraceShuffleVertexManager to get a list of grandparents.\n     // Also need to exclude grandparents which also a parent (a is both parent and grandparent in the diagram below)\n     //    a   ->    c\n     //      \\  b  /\n-    // \n+    //\n     public static List<TezOperator> getGrandParentsForGraceParallelism(TezOperPlan tezPlan, TezOperator op) {\n         List<TezOperator> grandParents = new ArrayList<TezOperator>();\n         List<TezOperator> preds = tezPlan.getPredecessors(op);", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POFRJoinTez.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POFRJoinTez.java", "sha": "dec86863b4ad5238fe093d67ed1f974fe95c1c3c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POFRJoinTez.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -73,7 +73,7 @@ public POFRJoinTez(POFRJoin copy, List<String> inputKeys) throws ExecException {\n \n     @Override\n     public void replaceInput(String oldInputKey, String newInputKey) {\n-        if (inputKeys.remove(oldInputKey)) {\n+        while (inputKeys.remove(oldInputKey)) {\n             inputKeys.add(newInputKey);\n         }\n     }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POFRJoinTez.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffleTezLoad.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffleTezLoad.java", "sha": "bb88af4a23d0819309f14ac11ff9620ff2dfa09f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffleTezLoad.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -73,7 +73,7 @@ public POShuffleTezLoad(POPackage pack) {\n \n     @Override\n     public void replaceInput(String oldInputKey, String newInputKey) {\n-        if (inputKeys.remove(oldInputKey)) {\n+        while (inputKeys.remove(oldInputKey)) {\n             inputKeys.add(newInputKey);\n         }\n     }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffleTezLoad.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffledValueInputTez.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffledValueInputTez.java", "sha": "cdde2d16351575c26912cf35dfdfcf1ebad69329", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffledValueInputTez.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -71,7 +71,7 @@ public POShuffledValueInputTez(OperatorKey k) {\n \n     @Override\n     public void replaceInput(String oldInputKey, String newInputKey) {\n-        if (inputKeys.remove(oldInputKey)) {\n+        while (inputKeys.remove(oldInputKey)) {\n             inputKeys.add(newInputKey);\n         }\n     }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POShuffledValueInputTez.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POValueOutputTez.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POValueOutputTez.java", "sha": "f8065e1de41db9ec4878f875407bdd88a4860a3e", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POValueOutputTez.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -96,7 +96,7 @@ public void initialize(ProcessorContext processorContext)\n \n     @Override\n     public void replaceOutput(String oldOutputKey, String newOutputKey) {\n-        if (outputKeys.remove(oldOutputKey)) {\n+        while (outputKeys.remove(oldOutputKey)) {\n             outputKeys.add(newOutputKey);\n         }\n     }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/operator/POValueOutputTez.java"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/MultiQueryOptimizerTez.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/MultiQueryOptimizerTez.java", "sha": "7fce1b32995c9dd08503a187c0bad55b61b09a80", "changes": 39, "status": "modified", "deletions": 36, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/MultiQueryOptimizerTez.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -23,20 +23,14 @@\n import java.util.Map.Entry;\n import java.util.Set;\n \n-import org.apache.commons.lang.ArrayUtils;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;\n-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;\n-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.TezEdgeDescriptor;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOpPlanVisitor;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperPlan;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POValueOutputTez;\n-import org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez;\n-import org.apache.pig.backend.hadoop.executionengine.tez.runtime.TezInput;\n-import org.apache.pig.backend.hadoop.executionengine.tez.runtime.TezOutput;\n import org.apache.pig.backend.hadoop.executionengine.tez.util.TezCompilerUtil;\n import org.apache.pig.impl.plan.OperatorKey;\n import org.apache.pig.impl.plan.PlanException;\n@@ -214,22 +208,12 @@ private void removeSplittee(TezOperPlan plan, TezOperator splitter,\n \n         if (plan.getPredecessors(splittee) != null) {\n             for (TezOperator pred : new ArrayList<TezOperator>(plan.getPredecessors(splittee))) {\n-                List<TezOutput> tezOutputs = PlanHelper.getPhysicalOperators(pred.plan,\n-                        TezOutput.class);\n-                for (TezOutput tezOut : tezOutputs) {\n-                    if (ArrayUtils.contains(tezOut.getTezOutputs(), spliteeKey)) {\n-                        tezOut.replaceOutput(spliteeKey, splitterKey);\n-                    }\n-                }\n-\n                 TezEdgeDescriptor edge = pred.outEdges.remove(splittee.getOperatorKey());\n                 if (edge == null) {\n                     throw new VisitorException(\"Edge description is empty\");\n                 }\n-                pred.outEdges.put(splitter.getOperatorKey(), edge);\n-                splitter.inEdges.put(pred.getOperatorKey(), edge);\n                 plan.disconnect(pred, splittee);\n-                plan.connect(pred, splitter);\n+                TezCompilerUtil.connectTezOpToNewSuccesor(plan, pred, splitter, edge, spliteeKey);\n             }\n         }\n \n@@ -244,27 +228,10 @@ private void removeSplittee(TezOperPlan plan, TezOperator splitter,\n \n                 // Do not connect again in case of self join/cross/cogroup or union\n                 if (splitterSuccs == null || !splitterSuccs.contains(succTezOperator)) {\n-                    TezCompilerUtil.connect(plan, splitter, succTezOperator, edge);\n+                    TezCompilerUtil.connectTezOpToNewPredecessor(plan, succTezOperator, splitter, edge, null);\n                 }\n \n-                try {\n-                    List<TezInput> inputs = PlanHelper.getPhysicalOperators(succTezOperator.plan, TezInput.class);\n-                    for (TezInput input : inputs) {\n-                        input.replaceInput(spliteeKey,\n-                                splitterKey);\n-                    }\n-                    List<POUserFunc> userFuncs = PlanHelper.getPhysicalOperators(succTezOperator.plan, POUserFunc.class);\n-                    for (POUserFunc userFunc : userFuncs) {\n-                        if (userFunc.getFunc() instanceof ReadScalarsTez) {\n-                            TezInput tezInput = (TezInput)userFunc.getFunc();\n-                            tezInput.replaceInput(spliteeKey,\n-                                    splitterKey);\n-                            userFunc.getFuncSpec().setCtorArgs(tezInput.getTezInputs());\n-                        }\n-                    }\n-                } catch (VisitorException e) {\n-                    throw new PlanException(e);\n-                }\n+                TezCompilerUtil.replaceInput(succTezOperator, spliteeKey, splitterKey);\n \n                 if (succTezOperator.isUnion()) {\n                     int index = succTezOperator.getUnionMembers().indexOf(splittee.getOperatorKey());", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/MultiQueryOptimizerTez.java"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java", "sha": "94a578c27dce31ac6eddf34f2f48e33ad767500a", "changes": 32, "status": "modified", "deletions": 28, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -26,7 +26,6 @@\n import java.util.Map.Entry;\n import java.util.Set;\n \n-import org.apache.commons.lang.ArrayUtils;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;\n@@ -43,6 +42,7 @@\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez;\n import org.apache.pig.backend.hadoop.executionengine.tez.runtime.TezInput;\n import org.apache.pig.backend.hadoop.executionengine.tez.runtime.TezOutput;\n+import org.apache.pig.backend.hadoop.executionengine.tez.util.TezCompilerUtil;\n import org.apache.pig.builtin.RoundRobinPartitioner;\n import org.apache.pig.impl.plan.OperatorKey;\n import org.apache.pig.impl.plan.PlanException;\n@@ -274,14 +274,9 @@ private void connectUnionNonMemberPredecessorsToSplit(TezOperator unionOp,\n                 }\n \n                 for (TezOperator actualPred : actualPreds) {\n-                    List<TezOutput> tezOutputs = PlanHelper.getPhysicalOperators(actualPred.plan,\n-                                    TezOutput.class);\n \n-                    for (TezOutput tezOut : tezOutputs) {\n-                        if (ArrayUtils.contains(tezOut.getTezOutputs(), unionOpKey)) {\n-                            tezOut.replaceOutput(unionOpKey, splitPredKey.toString());\n-                        }\n-                    }\n+                    TezCompilerUtil.replaceOutput(actualPred, unionOpKey, splitPredKey.toString());\n+\n                     TezEdgeDescriptor edge = actualPred.outEdges.remove(unionOp.getOperatorKey());\n                     if (edge == null) {\n                         throw new VisitorException(\"Edge description is empty\");\n@@ -343,27 +338,8 @@ private void connectSplitOpToUnionSuccessors(TezOperator unionOp,\n                 }\n \n                 for (TezOperator actualSucc : actualSuccs) {\n-                    LinkedList<TezInput> inputs = PlanHelper.getPhysicalOperators(succ.plan, TezInput.class);\n-                    for (TezInput tezInput : inputs) {\n-                        for (String inputKey : tezInput.getTezInputs()) {\n-                            if (inputKey.equals(unionOpKey)) {\n-                                tezInput.replaceInput(inputKey, splitPredOpKey);\n-                            }\n-                        }\n-                    }\n \n-                    List<POUserFunc> userFuncs = PlanHelper.getPhysicalOperators(succ.plan, POUserFunc.class);\n-                    for (POUserFunc userFunc : userFuncs) {\n-                        if (userFunc.getFunc() instanceof ReadScalarsTez) {\n-                            TezInput tezInput = (TezInput)userFunc.getFunc();\n-                            for (String inputKey : tezInput.getTezInputs()) {\n-                                if (inputKey.equals(unionOpKey)) {\n-                                    tezInput.replaceInput(inputKey, splitPredOpKey);\n-                                    userFunc.getFuncSpec().setCtorArgs(tezInput.getTezInputs());\n-                                }\n-                            }\n-                        }\n-                    }\n+                    TezCompilerUtil.replaceInput(succ, unionOpKey, splitPredOpKey);\n \n                     TezEdgeDescriptor edge = actualSucc.inEdges.remove(unionOp.getOperatorKey());\n                     if (edge == null) {", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java"}, {"additions": 67, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/util/TezCompilerUtil.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/src/org/apache/pig/backend/hadoop/executionengine/tez/util/TezCompilerUtil.java", "sha": "71083de0ac2ca6f45672d30eead4b93c922d9158", "changes": 75, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/util/TezCompilerUtil.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -22,10 +22,12 @@\n import java.util.LinkedList;\n import java.util.List;\n \n+import org.apache.commons.lang.ArrayUtils;\n import org.apache.pig.PigException;\n import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;\n+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;\n@@ -37,6 +39,9 @@\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POLocalRearrangeTez;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez;\n import org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POValueOutputTez;\n+import org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez;\n+import org.apache.pig.backend.hadoop.executionengine.tez.runtime.TezInput;\n+import org.apache.pig.backend.hadoop.executionengine.tez.runtime.TezOutput;\n import org.apache.pig.builtin.RoundRobinPartitioner;\n import org.apache.pig.data.DataType;\n import org.apache.pig.data.TupleFactory;\n@@ -123,19 +128,73 @@ static public TezEdgeDescriptor connect(TezOperPlan plan, TezOperator from, TezO\n \n     static public void connect(TezOperPlan plan, TezOperator from, TezOperator to, TezEdgeDescriptor edge) throws PlanException {\n         plan.connect(from, to);\n-        if (from.plan.getLeaves()!=null && !from.plan.getLeaves().isEmpty()) {\n-            PhysicalOperator leaf = from.plan.getLeaves().get(0);\n-            // It could be POStoreTez incase of sampling job in order by\n-            if (leaf instanceof POLocalRearrangeTez) {\n-                POLocalRearrangeTez lr = (POLocalRearrangeTez) leaf;\n-                lr.setOutputKey(to.getOperatorKey().toString());\n-            }\n-        }\n+\n         // Add edge descriptors to old and new operators\n         to.inEdges.put(from.getOperatorKey(), edge);\n         from.outEdges.put(to.getOperatorKey(), edge);\n     }\n \n+    static public void connectTezOpToNewPredecessor(TezOperPlan plan,\n+            TezOperator tezOp, TezOperator newPredecessor,\n+            TezEdgeDescriptor edge, String oldInputKey) throws PlanException {\n+        plan.connect(newPredecessor, tezOp);\n+        // Add edge descriptors to old and new operators\n+        tezOp.inEdges.put(newPredecessor.getOperatorKey(), edge);\n+        newPredecessor.outEdges.put(tezOp.getOperatorKey(), edge);\n+\n+        if (oldInputKey != null) {\n+            replaceInput(tezOp, oldInputKey, newPredecessor.getOperatorKey().toString());\n+        }\n+    }\n+\n+    public static void replaceInput(TezOperator tezOp, String oldInputKey,\n+            String newInputKey) throws PlanException {\n+        try {\n+            List<TezInput> inputs = PlanHelper.getPhysicalOperators(tezOp.plan, TezInput.class);\n+            for (TezInput input : inputs) {\n+                input.replaceInput(oldInputKey, newInputKey);\n+            }\n+            List<POUserFunc> userFuncs = PlanHelper.getPhysicalOperators(tezOp.plan, POUserFunc.class);\n+            for (POUserFunc userFunc : userFuncs) {\n+                if (userFunc.getFunc() instanceof ReadScalarsTez) {\n+                    TezInput input = (TezInput)userFunc.getFunc();\n+                    input.replaceInput(oldInputKey, newInputKey);\n+                    userFunc.getFuncSpec().setCtorArgs(input.getTezInputs());\n+                }\n+            }\n+        } catch (VisitorException e) {\n+            throw new PlanException(e);\n+        }\n+    }\n+\n+    static public void connectTezOpToNewSuccesor(TezOperPlan plan,\n+            TezOperator tezOp, TezOperator newSuccessor,\n+            TezEdgeDescriptor edge, String oldOutputKey) throws PlanException {\n+        plan.connect(tezOp, newSuccessor);\n+        // Add edge descriptors to old and new operators\n+        newSuccessor.inEdges.put(tezOp.getOperatorKey(), edge);\n+        tezOp.outEdges.put(newSuccessor.getOperatorKey(), edge);\n+\n+        if (oldOutputKey != null) {\n+            replaceOutput(tezOp, oldOutputKey, newSuccessor.getOperatorKey().toString());\n+        }\n+    }\n+\n+    public static void replaceOutput(TezOperator tezOp, String oldOutputKey,\n+            String newOutputKey) throws PlanException {\n+        try {\n+            List<TezOutput> tezOutputs = PlanHelper.getPhysicalOperators(tezOp.plan,\n+                    TezOutput.class);\n+            for (TezOutput tezOut : tezOutputs) {\n+                if (ArrayUtils.contains(tezOut.getTezOutputs(), oldOutputKey)) {\n+                    tezOut.replaceOutput(oldOutputKey, newOutputKey);\n+                }\n+            }\n+        } catch (VisitorException e) {\n+            throw new PlanException(e);\n+        }\n+    }\n+\n     static public POForEach getForEach(POProject project, int rp, String scope, NodeIdGenerator nig) {\n         PhysicalPlan forEachPlan = new PhysicalPlan();\n         forEachPlan.add(project);", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/util/TezCompilerUtil.java"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/8cd6c1c4895dd81dbcb987e215586049eb5a7680/test/org/apache/pig/test/TestHBaseStorage.java", "blob_url": "https://github.com/apache/pig/blob/8cd6c1c4895dd81dbcb987e215586049eb5a7680/test/org/apache/pig/test/TestHBaseStorage.java", "sha": "8d2ad85388416ec825165fca16db7ba9cc5693ae", "changes": 11, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestHBaseStorage.java?ref=8cd6c1c4895dd81dbcb987e215586049eb5a7680", "patch": "@@ -37,7 +37,6 @@\n import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.util.Bytes;\n-import org.apache.pig.ExecType;\n import org.apache.pig.PigServer;\n import org.apache.pig.backend.executionengine.ExecException;\n import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;\n@@ -49,7 +48,6 @@\n import org.junit.After;\n import org.junit.AfterClass;\n import org.junit.Assert;\n-import org.junit.Assume;\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -835,7 +833,6 @@ public void testLoadWithProjection_2() throws IOException {\n      */\n     @Test\n     public void testMergeJoin() throws IOException {\n-        Assume.assumeTrue(\"Skip this test for TEZ. See PIG-4315\", pig.getPigContext().getExecType().equals(ExecType.LOCAL));\n         prepareTable(TESTTABLE_1, true, DataFormat.HBaseBinary);\n         prepareTable(TESTTABLE_2, true, DataFormat.HBaseBinary);\n         pig.registerQuery(\"a = load 'hbase://\" + TESTTABLE_1 + \"' using \"\n@@ -1078,7 +1075,7 @@ public void testStoreToHBase_1_with_timestamp() throws IOException {\n             long col_a_ts = getColTimestamp(result, TESTCOLUMN_A);\n             long col_b_ts = getColTimestamp(result, TESTCOLUMN_B);\n             long col_c_ts = getColTimestamp(result, TESTCOLUMN_C);\n-            \n+\n             Assert.assertEquals(timestamp, col_a_ts);\n             Assert.assertEquals(timestamp, col_b_ts);\n             Assert.assertEquals(timestamp, col_c_ts);\n@@ -1125,7 +1122,7 @@ public void testStoreToHBase_1_with_datetime_timestamp() throws IOException {\n             long col_a_ts = getColTimestamp(result, TESTCOLUMN_A);\n             long col_b_ts = getColTimestamp(result, TESTCOLUMN_B);\n             long col_c_ts = getColTimestamp(result, TESTCOLUMN_C);\n-            \n+\n             Assert.assertEquals(timestamp, col_a_ts);\n             Assert.assertEquals(timestamp, col_b_ts);\n             Assert.assertEquals(timestamp, col_c_ts);\n@@ -1172,7 +1169,7 @@ public void testStoreToHBase_1_with_bytearray_timestamp() throws IOException {\n             long col_a_ts = getColTimestamp(result, TESTCOLUMN_A);\n             long col_b_ts = getColTimestamp(result, TESTCOLUMN_B);\n             long col_c_ts = getColTimestamp(result, TESTCOLUMN_C);\n-            \n+\n             Assert.assertEquals(timestamp, col_a_ts);\n             Assert.assertEquals(timestamp, col_b_ts);\n             Assert.assertEquals(timestamp, col_c_ts);\n@@ -1217,7 +1214,7 @@ public void testStoreToHBase_1_with_delete() throws IOException {\n             long col_a_ts = getColTimestamp(result, TESTCOLUMN_A);\n             long col_b_ts = getColTimestamp(result, TESTCOLUMN_B);\n             long col_c_ts = getColTimestamp(result, TESTCOLUMN_C);\n-            \n+\n             Assert.assertEquals(\"00\".substring(v.length()) + v, rowKey);\n             Assert.assertEquals(i, col_a);\n             Assert.assertEquals(i + 0.0, col_b, 1e-6);", "filename": "test/org/apache/pig/test/TestHBaseStorage.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/aba9f47589d8a11b8d6ad428343eb780464496af", "parent": "https://github.com/apache/pig/commit/369b81c18afad938b9d33915cb63d5591cbe38c1", "message": "Tiny fix: NPE introduced by PIG-2332\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1208338 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_23", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/aba9f47589d8a11b8d6ad428343eb780464496af/src/org/apache/pig/LoadFunc.java", "blob_url": "https://github.com/apache/pig/blob/aba9f47589d8a11b8d6ad428343eb780464496af/src/org/apache/pig/LoadFunc.java", "sha": "1cae5cf820b2b4137484b6ac7dedf34a3409b9cb", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/LoadFunc.java?ref=aba9f47589d8a11b8d6ad428343eb780464496af", "patch": "@@ -303,6 +303,7 @@ public void setUDFContextSignature(String signature) {\n      */\n     public final void warn(String msg, Enum warningEnum) {\n         Counter counter = PigStatusReporter.getInstance().getCounter(warningEnum);\n-        counter.increment(1);\n+        if (counter!=null)\n+            counter.increment(1);\n     }\n }", "filename": "src/org/apache/pig/LoadFunc.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/e881d29d478e27b2f7957451288aa409eeb321ba", "parent": "https://github.com/apache/pig/commit/795b645a521c852052c4206a307fb3b6f49a0c45", "message": "PIG-2313: NPE in ILLUSTRATE trying to get StatusReporter in STORE\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1196882 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_24", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/CHANGES.txt", "sha": "598899e865d17e5dcfc46e6ecdee8f42445067e2", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -158,6 +158,8 @@ PIG-2228: support partial aggregation in map task (thejas)\n \n BUG FIXES\n \n+PIG-2313: NPE in ILLUSTRATE trying to get StatusReporter in STORE (daijy)\n+\n PIG-2335: bin/pig does not work with bash 3.0 (azaroth)\n \n PIG-2275: NullPointerException from ILLUSTRATE (daijy)", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java", "sha": "8bdfa2cea5575cac9f262dafbfa997c897e92e89", "changes": 32, "status": "modified", "deletions": 32, "contents_url": "https://api.github.com/repos/apache/pig/contents/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -60,38 +60,6 @@ public Context getIllustratorContext(Configuration conf, DataBag input,\n     public boolean inIllustrator(Context context) {\n         return (context instanceof PigMapBase.IllustratorContext);\n     }\n-\n-    /**\n-     * Dummy implementation of StatusReporter for illustrate mode\n-     *\n-     */\n-    @SuppressWarnings(\"deprecation\")\n-    public static class IllustrateDummyReporter extends StatusReporter{\n-\n-\n-        Counters countGen = new Counters();\n-        \n-        @Override\n-        public Counter getCounter(Enum<?> arg0) {\n-            return countGen.findCounter(arg0);\n-        }\n-\n-        @Override\n-        public Counter getCounter(String group, String name) {\n-            return countGen.findCounter(group, name);\n-        }\n-\n-        @Override\n-        public void progress() {\n-            //no-op\n-        }\n-\n-        @Override\n-        public void setStatus(String arg0) {\n-            //no-op\n-        }\n-        \n-    }\n     \n     public class IllustratorContext extends Context {\n         private DataBag input;", "filename": "shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java", "sha": "50d3b1b50db86ce871db50ce09a735d1a52b7889", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -70,7 +70,7 @@ public IllustratorContext(Job job,\n                   POPackage pkg\n                   ) throws IOException, InterruptedException {\n                 super(job.getJobConf(), new TaskAttemptID(), new FakeRawKeyValueIterator(input.iterator().hasNext()),\n-                    null, null, null, null, null, null, PigNullableWritable.class, NullableTuple.class);\n+                    null, null, null, null, new IllustrateDummyReporter(), null, PigNullableWritable.class, NullableTuple.class);\n                 bos = new ByteArrayOutputStream();\n                 dos = new DataOutputStream(bos);\n                 org.apache.hadoop.mapreduce.Job nwJob = new org.apache.hadoop.mapreduce.Job(job.getJobConf());", "filename": "shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java", "sha": "cbbdb0c6435ab3ce83a01847539715d08d197863", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -85,7 +85,7 @@ public Context getIllustratorContext(Configuration conf, DataBag input,\n         public IllustratorContext(Configuration conf, DataBag input,\n               List<Pair<PigNullableWritable, Writable>> output,\n               InputSplit split) throws IOException, InterruptedException {\n-            super(conf, new TaskAttemptID(), null, null, null, null, split);\n+            super(conf, new TaskAttemptID(), null, null, null, new IllustrateDummyReporter(), split);\n             if (output == null)\n                 throw new IOException(\"Null output can not be used\");\n             this.input = input; this.output = output;", "filename": "shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java", "sha": "783ae552f070ca79bc550d1f03dae3c183757315", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -112,7 +112,7 @@ public IllustratorContextImpl(Job job,\n                   POPackage pkg\n                   ) throws IOException, InterruptedException {\n                 super(job.getJobConf(), new TaskAttemptID(), new FakeRawKeyValueIterator(input.iterator().hasNext()),\n-                    null, null, null, null, null, null, PigNullableWritable.class, NullableTuple.class);\n+                    null, null, null, null, new IllustrateDummyReporter(), null, PigNullableWritable.class, NullableTuple.class);\n                 bos = new ByteArrayOutputStream();\n                 dos = new DataOutputStream(bos);\n                 org.apache.hadoop.mapreduce.Job nwJob = new org.apache.hadoop.mapreduce.Job(job.getJobConf());", "filename": "shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java"}, {"additions": 59, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/IllustrateDummyReporter.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/IllustrateDummyReporter.java", "sha": "c3ebeeb906aec9d0297f13eb68839d361f0a5d80", "changes": 59, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/IllustrateDummyReporter.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;\n+\n+import org.apache.hadoop.mapred.Counters;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n+\n+/**\n+ * Dummy implementation of StatusReporter for illustrate mode\n+ *\n+ */\n+@SuppressWarnings(\"deprecation\")\n+public class IllustrateDummyReporter extends StatusReporter{\n+\n+\n+    Counters countGen = new Counters();\n+    \n+    @Override\n+    public Counter getCounter(Enum<?> arg0) {\n+        return countGen.findCounter(arg0);\n+    }\n+\n+    @Override\n+    public Counter getCounter(String group, String name) {\n+        return countGen.findCounter(group, name);\n+    }\n+\n+    @Override\n+    public void progress() {\n+        //no-op\n+    }\n+\n+    @Override\n+    public void setStatus(String arg0) {\n+        //no-op\n+    }\n+\n+    public float getProgress() {\n+        return 0;\n+    }\n+    \n+}\n\\ No newline at end of file", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/IllustrateDummyReporter.java"}, {"additions": 17, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java", "sha": "ac74a68d5b381e3627063e0e330c7a466c2e3cd7", "changes": 30, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -124,14 +124,16 @@ public void cleanup(Context context) throws IOException, InterruptedException {\n             runPipeline(leaf);\n         }\n \n-        for (POStore store: stores) {\n-            if (!initialized) {\n-                MapReducePOStoreImpl impl \n-                    = new MapReducePOStoreImpl(context);\n-                store.setStoreImpl(impl);\n-                store.setUp();\n+        if (!inIllustrator) {\n+            for (POStore store: stores) {\n+                if (!initialized) {\n+                    MapReducePOStoreImpl impl \n+                        = new MapReducePOStoreImpl(context);\n+                    store.setStoreImpl(impl);\n+                    store.setUp();\n+                }\n+                store.tearDown();\n             }\n-            store.tearDown();\n         }\n         \n         //Calling EvalFunc.finish()\n@@ -227,12 +229,14 @@ protected void map(Text key, Tuple inpTuple, Context context) throws IOException\n             pigReporter.setRep(context);\n             PhysicalOperator.setReporter(pigReporter);\n            \n-            for (POStore store: stores) {\n-                MapReducePOStoreImpl impl \n-                    = new MapReducePOStoreImpl(context);\n-                store.setStoreImpl(impl);\n-                if (!pigContext.inIllustrator)\n-                    store.setUp();\n+            if (!inIllustrator) {\n+                for (POStore store: stores) {\n+                    MapReducePOStoreImpl impl \n+                        = new MapReducePOStoreImpl(context);\n+                    store.setStoreImpl(impl);\n+                    if (!pigContext.inIllustrator)\n+                        store.setUp();\n+                }\n             }\n             \n             boolean aggregateWarning = \"true\".equalsIgnoreCase(pigContext.getProperties().getProperty(\"aggregate.warning\"));", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java"}, {"additions": 9, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java", "sha": "d41f8d6232a0dad43f45addc60eff61f7e576a52", "changes": 16, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -518,14 +518,16 @@ protected void cleanup(Context context) throws IOException, InterruptedException\n                 runPipeline(leaf);\n             }\n \n-            for (POStore store: stores) {\n-                if (!initialized) {\n-                    MapReducePOStoreImpl impl \n-                        = new MapReducePOStoreImpl(context);\n-                    store.setStoreImpl(impl);\n-                    store.setUp();\n+            if (!inIllustrator) {\n+                for (POStore store: stores) {\n+                    if (!initialized) {\n+                        MapReducePOStoreImpl impl \n+                            = new MapReducePOStoreImpl(context);\n+                        store.setStoreImpl(impl);\n+                        store.setUp();\n+                    }\n+                    store.tearDown();\n                 }\n-                store.tearDown();\n             }\n                         \n             //Calling EvalFunc.finish()", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java"}, {"additions": 18, "raw_url": "https://github.com/apache/pig/raw/e881d29d478e27b2f7957451288aa409eeb321ba/test/org/apache/pig/test/TestExampleGenerator.java", "blob_url": "https://github.com/apache/pig/blob/e881d29d478e27b2f7957451288aa409eeb321ba/test/org/apache/pig/test/TestExampleGenerator.java", "sha": "4babccbd292da3874ac8c8a3cbfb344659d6bbfd", "changes": 18, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestExampleGenerator.java?ref=e881d29d478e27b2f7957451288aa409eeb321ba", "patch": "@@ -390,4 +390,22 @@ public void testFilterWithUDF() throws ExecException, IOException {\n         assertTrue(derivedData != null);\n     }\n \n+    @Test\n+    public void testFilterGroupCountStore() throws Exception {\n+        File out = File.createTempFile(\"testFilterGroupCountStoreOutput\", \"\");\n+        out.deleteOnExit();\n+        out.delete();\n+    \n+        PigServer pigServer = new PigServer(pigContext);\n+        pigServer.setBatchOn();\n+        pigServer.registerQuery(\"A = load \" + A.toString() + \" as (x, y);\");\n+        pigServer.registerQuery(\"B = filter A by x < 5;\");\n+        pigServer.registerQuery(\"C = group B by x;\");\n+        pigServer.registerQuery(\"D = foreach C generate group as x, COUNT(B) as the_count;\");\n+        pigServer.registerQuery(\"store D into '\" +  out.getAbsolutePath() + \"';\");\n+        Map<Operator, DataBag> derivedData = pigServer.getExamples(null);\n+    \n+        assertTrue(derivedData != null);\n+    }\n+\n }", "filename": "test/org/apache/pig/test/TestExampleGenerator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/73878540198b2ab90a8422ec2e452c5a0df360c0", "parent": "https://github.com/apache/pig/commit/d1e1d1b93ea4863b222fca636a5155cac1a35777", "message": "PIG-2170: NPE thrown during illustrate\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1154068 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_25", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/73878540198b2ab90a8422ec2e452c5a0df360c0/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/73878540198b2ab90a8422ec2e452c5a0df360c0/CHANGES.txt", "sha": "a82fca94d3ead8b200ec141293f03d000e9bc72b", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=73878540198b2ab90a8422ec2e452c5a0df360c0", "patch": "@@ -99,6 +99,8 @@ PIG-2011: Speed up TestTypedMap.java (dvryaboy)\n \n BUG FIXES\n \n+PIG-2170: NPE thrown during illustrate (thejas)\n+\n PIG-2186: PigStorage new warnings about missing schema file \n  can be confusing (thejas)\n ", "filename": "CHANGES.txt"}, {"additions": 43, "raw_url": "https://github.com/apache/pig/raw/73878540198b2ab90a8422ec2e452c5a0df360c0/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java", "blob_url": "https://github.com/apache/pig/blob/73878540198b2ab90a8422ec2e452c5a0df360c0/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java", "sha": "6c4e78e0898a3476aee7648121fd2df1bb44666b", "changes": 49, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/pig/contents/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java?ref=73878540198b2ab90a8422ec2e452c5a0df360c0", "patch": "@@ -24,7 +24,10 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapred.Counters;\n import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.StatusReporter;\n import org.apache.hadoop.mapreduce.TaskAttemptID;\n import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase;\n import org.apache.pig.data.DataBag;\n@@ -34,7 +37,7 @@\n \n abstract public class PigMapBase extends PigGenericMapBase {\n     /**\n-     * \n+     *  \n      * Get mapper's illustrator context\n      * \n      * @param conf  Configuration\n@@ -52,6 +55,40 @@ public Context getIllustratorContext(Configuration conf, DataBag input,\n         return new IllustratorContext(conf, input, output, split);\n     }\n     \n+    \n+\n+    /**\n+     * Dummy implementation of StatusReporter for illustrate mode\n+     *\n+     */\n+    @SuppressWarnings(\"deprecation\")\n+    public static class IllustrateDummyReporter extends StatusReporter{\n+\n+\n+        Counters countGen = new Counters();\n+        \n+        @Override\n+        public Counter getCounter(Enum<?> arg0) {\n+            return countGen.findCounter(arg0);\n+        }\n+\n+        @Override\n+        public Counter getCounter(String group, String name) {\n+            return countGen.findCounter(group, name);\n+        }\n+\n+        @Override\n+        public void progress() {\n+            //no-op\n+        }\n+\n+        @Override\n+        public void setStatus(String arg0) {\n+            //no-op\n+        }\n+        \n+    }\n+    \n     public class IllustratorContext extends Context {\n         private DataBag input;\n         List<Pair<PigNullableWritable, Writable>> output;\n@@ -62,12 +99,12 @@ public Context getIllustratorContext(Configuration conf, DataBag input,\n         public IllustratorContext(Configuration conf, DataBag input,\n               List<Pair<PigNullableWritable, Writable>> output,\n               InputSplit split) throws IOException, InterruptedException {\n-              super(conf, new TaskAttemptID(), null, null, null, null, split);\n-              if (output == null)\n-                  throw new IOException(\"Null output can not be used\");\n-              this.input = input; this.output = output;\n+            super(conf, new TaskAttemptID(), null, null, null, new IllustrateDummyReporter(), split);\n+            if (output == null)\n+                throw new IOException(\"Null output can not be used\");\n+            this.input = input; this.output = output;\n         }\n-        \n+\n         @Override\n         public boolean nextKeyValue() throws IOException, InterruptedException {\n             if (input == null) {", "filename": "shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java"}, {"additions": 10, "raw_url": "https://github.com/apache/pig/raw/73878540198b2ab90a8422ec2e452c5a0df360c0/src/org/apache/pig/pen/IllustratorAttacher.java", "blob_url": "https://github.com/apache/pig/blob/73878540198b2ab90a8422ec2e452c5a0df360c0/src/org/apache/pig/pen/IllustratorAttacher.java", "sha": "63089f50b7acacd7e5a6362c96dd3f6727d7fae7", "changes": 22, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/pen/IllustratorAttacher.java?ref=73878540198b2ab90a8422ec2e452c5a0df360c0", "patch": "@@ -283,56 +283,56 @@ public void visitProject(POProject proj) throws VisitorException{\n     @Override\n     public void visitGreaterThan(GreaterThanExpr grt) throws VisitorException{\n         setIllustrator(grt, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(grt.getIllustrator().getSubExpResult());\n     }\n     \n     @Override\n     public void visitLessThan(LessThanExpr lt) throws VisitorException{\n         setIllustrator(lt, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(lt.getIllustrator().getSubExpResult());\n     }\n     \n     @Override\n     public void visitGTOrEqual(GTOrEqualToExpr gte) throws VisitorException{\n         setIllustrator(gte, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(gte.getIllustrator().getSubExpResult());\n     }\n     \n     @Override\n     public void visitLTOrEqual(LTOrEqualToExpr lte) throws VisitorException{\n         setIllustrator(lte, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(lte.getIllustrator().getSubExpResult());\n     }\n     \n     @Override\n     public void visitEqualTo(EqualToExpr eq) throws VisitorException{\n         setIllustrator(eq, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(eq.getIllustrator().getSubExpResult());\n     }\n     \n     @Override\n     public void visitNotEqualTo(NotEqualToExpr eq) throws VisitorException{\n         setIllustrator(eq, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(eq.getIllustrator().getSubExpResult());\n     }\n     \n     @Override\n     public void visitRegexp(PORegexp re) throws VisitorException{\n         setIllustrator(re, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(re.getIllustrator().getSubExpResult());\n     }\n \n     @Override\n     public void visitIsNull(POIsNull isNull) throws VisitorException {\n         setIllustrator(isNull, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(isNull.getIllustrator().getSubExpResult());\n     }\n     \n@@ -349,15 +349,13 @@ public void visitOr(POOr or) throws VisitorException {\n     @Override\n     public void visitNot(PONot not) throws VisitorException {\n         setIllustrator(not, 0);\n-        if (!revisit)\n+        if (!revisit && subExpResults != null)\n             subExpResults.add(not.getIllustrator().getSubExpResult());\n     }\n \n     @Override\n     public void visitBinCond(POBinCond binCond) {\n-        setIllustrator(binCond, 0);\n-        if (!revisit)\n-            subExpResults.add(binCond.getIllustrator().getSubExpResult());\n+\n     }\n \n     @Override", "filename": "src/org/apache/pig/pen/IllustratorAttacher.java"}, {"additions": 49, "raw_url": "https://github.com/apache/pig/raw/73878540198b2ab90a8422ec2e452c5a0df360c0/test/org/apache/pig/test/TestExampleGenerator.java", "blob_url": "https://github.com/apache/pig/blob/73878540198b2ab90a8422ec2e452c5a0df360c0/test/org/apache/pig/test/TestExampleGenerator.java", "sha": "9a56344f2670c44a6f3e4bb4208f2a0dc70f61a4", "changes": 77, "status": "modified", "deletions": 28, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestExampleGenerator.java?ref=73878540198b2ab90a8422ec2e452c5a0df360c0", "patch": "@@ -18,14 +18,15 @@\n \n package org.apache.pig.test;\n \n+import static org.junit.Assert.assertTrue;\n+\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.IOException;\n import java.util.Map;\n+import java.util.Properties;\n import java.util.Random;\n \n-import junit.framework.TestCase;\n-\n import org.apache.pig.ExecType;\n import org.apache.pig.PigServer;\n import org.apache.pig.backend.executionengine.ExecException;\n@@ -34,22 +35,20 @@\n import org.apache.pig.impl.io.FileLocalizer;\n import org.apache.pig.newplan.Operator;\n import org.junit.AfterClass;\n-import org.junit.Before;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.JUnit4;\n \n-@RunWith(JUnit4.class)\n-public class TestExampleGenerator extends TestCase {\n \n-    static MiniCluster cluster = MiniCluster.buildCluster();\n-    PigContext pigContext = new PigContext(ExecType.MAPREDUCE, cluster\n-            .getProperties());\n+public class TestExampleGenerator {\n \n-    Random rand = new Random();\n-    int MAX = 100;\n-    String A, B;\n+    \n+    static PigContext pigContext = new PigContext(ExecType.LOCAL, new Properties());\n \n+   \n+    static int MAX = 100;\n+    static String A, B;\n+    static  File fileA, fileB;\n+    \n     {\n         try {\n             pigContext.connect();\n@@ -59,32 +58,26 @@\n         }\n     }\n \n-    @Before\n-    public void setUp() throws Exception {\n-        File fileA, fileB;\n+    @BeforeClass\n+    public static void oneTimeSetup() throws Exception {\n+       \n \n         fileA = File.createTempFile(\"dataA\", \".dat\");\n         fileB = File.createTempFile(\"dataB\", \".dat\");\n \n         writeData(fileA);\n         writeData(fileB);\n+     \n \n-        A = \"'\" + FileLocalizer.hadoopify(fileA.toString(), pigContext) + \"'\";\n-        B = \"'\" + FileLocalizer.hadoopify(fileB.toString(), pigContext) + \"'\";\n-        System.out.println(\"A : \" + A + \"\\n\" + \"B : \" + B);\n-        System.out.println(\"Test data created.\");\n         fileA.deleteOnExit();\n         fileB.deleteOnExit();\n-          \n-    }\n-\n-    \n-    @AfterClass\n-    public static void oneTimeTearDown() throws Exception {\n-        cluster.shutDown();\n+        A = \"'\" + fileA.getPath() + \"'\";\n+        B = \"'\" + fileB.getPath() + \"'\";\n+        System.out.println(\"A : \" + A + \"\\n\" + \"B : \" + B);\n+        System.out.println(\"Test data created.\");\n     }\n \n-    private void writeData(File dataFile) throws Exception {\n+    private static void writeData(File dataFile) throws Exception {\n         // File dataFile = File.createTempFile(name, \".dat\");\n         FileOutputStream dat = new FileOutputStream(dataFile);\n \n@@ -176,6 +169,34 @@ public void testForeach() throws ExecException, IOException {\n \n         assertTrue(derivedData != null);\n     }\n+    \n+    //see PIG-2170\n+    @Test\n+    public void testForeachBinCondWithBooleanExp() throws ExecException, IOException {\n+        PigServer pigServer = new PigServer(pigContext);\n+\n+        pigServer.registerQuery(\"A = load \" + A\n+                + \" using PigStorage() as (x : int, y : int);\");\n+        pigServer.registerQuery(\"B = foreach A generate  (x + 1 > y ? 0 : 1);\");\n+\n+        Map<Operator, DataBag> derivedData = pigServer.getExamples(\"B\");\n+\n+        assertTrue(derivedData != null);\n+    }\n+    \n+    @Test\n+    public void testForeachWithTypeCastCounter() throws ExecException, IOException {\n+        PigServer pigServer = new PigServer(pigContext);\n+        //cast error results in counter being incremented and was resulting\n+        // in a NPE exception in illustrate\n+        pigServer.registerQuery(\"A = load \" + A\n+                + \" using PigStorage() as (x : int, y : int);\");\n+        pigServer.registerQuery(\"B = foreach A generate x, (int)'InvalidInt';\");\n+\n+        Map<Operator, DataBag> derivedData = pigServer.getExamples(\"B\");\n+\n+        assertTrue(derivedData != null);\n+    }\n \n     @Test\n     public void testJoin() throws IOException, ExecException {", "filename": "test/org/apache/pig/test/TestExampleGenerator.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/4c05bdb44dd6c9eba704f1568f076d218a430565", "parent": "https://github.com/apache/pig/commit/52477dc84cb7b8ca8ed5be9a0a77bdd9a9770d18", "message": "PIG-2006: Regression: NPE when Pig processes an empty script file\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1096189 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_26", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/4c05bdb44dd6c9eba704f1568f076d218a430565/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/4c05bdb44dd6c9eba704f1568f076d218a430565/CHANGES.txt", "sha": "e2cfd67b9ac211c003e002abf6bee3685985611a", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=4c05bdb44dd6c9eba704f1568f076d218a430565", "patch": "@@ -44,6 +44,8 @@ PIG-1876: Typed map for Pig (daijy)\n \n IMPROVEMENTS\n \n+PIG-2006: Regression: NPE when Pig processes an empty script file, fix test case (xuefu)\n+\n PIG-2006: Regression: NPE when Pig processes an empty script file (xuefu)\n \n PIG-2007: Parsing error when map key referred directly from udf in nested foreach (xuefu)", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/4c05bdb44dd6c9eba704f1568f076d218a430565/test/org/apache/pig/test/TestPigRunner.java", "blob_url": "https://github.com/apache/pig/blob/4c05bdb44dd6c9eba704f1568f076d218a430565/test/org/apache/pig/test/TestPigRunner.java", "sha": "6c0fb1d49cd3f5854938054861562b27ec0e4146", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestPigRunner.java?ref=4c05bdb44dd6c9eba704f1568f076d218a430565", "patch": "@@ -604,7 +604,6 @@ public void fsCommandTest() throws Exception {\n     @Test // PIG-2006\n     public void testEmptyFile() throws IOException {\n         File f1 = new File(\"myscript.pig\");\n-        f1.deleteOnExit();\n         \n         FileWriter fw1 = new FileWriter(f1);\n         fw1.close();", "filename": "test/org/apache/pig/test/TestPigRunner.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/53c1c1bc1836988fc9656d310eeed940b8bb84cc", "parent": "https://github.com/apache/pig/commit/7e469c81debea1a24260f4f12ef84441c53a416b", "message": "PIG-2006: Regression: NPE when Pig processes an empty script file\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1096087 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_27", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/53c1c1bc1836988fc9656d310eeed940b8bb84cc/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/53c1c1bc1836988fc9656d310eeed940b8bb84cc/CHANGES.txt", "sha": "623d1fb5cd91a5dac92fe957d9bdeef821625b11", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=53c1c1bc1836988fc9656d310eeed940b8bb84cc", "patch": "@@ -44,6 +44,8 @@ PIG-1876: Typed map for Pig (daijy)\n \n IMPROVEMENTS\n \n+PIG-2006: Regression: NPE when Pig processes an empty script file (xuefu)\n+\n PIG-2007: Parsing error when map key referred directly from udf in nested foreach (xuefu)\n \n PIG-2000: Pig gives incorrect error message dealing with scalar projection (xuefu)", "filename": "CHANGES.txt"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/7e469c81debea1a24260f4f12ef84441c53a416b", "parent": "https://github.com/apache/pig/commit/90d008ac79eec001314ad4f041f0955a17632315", "message": "PIG-2006: Regression: NPE when Pig processes an empty script file\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1096086 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_28", "file": [{"additions": 4, "raw_url": "https://github.com/apache/pig/raw/7e469c81debea1a24260f4f12ef84441c53a416b/src/org/apache/pig/scripting/ScriptEngine.java", "blob_url": "https://github.com/apache/pig/blob/7e469c81debea1a24260f4f12ef84441c53a416b/src/org/apache/pig/scripting/ScriptEngine.java", "sha": "69a7cbcbd3d7ba77725d3158ba3f485dd18214d1", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/scripting/ScriptEngine.java?ref=7e469c81debea1a24260f4f12ef84441c53a416b", "patch": "@@ -83,6 +83,8 @@ private SupportedScriptLang(String[] shebangs, String[] extensions, String engin\n          * @param firstLine The first line of the file (possibly containing #!...)\n          */\n         public boolean accepts(String file, String firstLine) {\n+            if( firstLine == null )\n+            \treturn false;\n             \n             for (String shebang : shebangs) {\n                 Pattern p = Pattern.compile(\"^#!.*/\" + shebang + \"\\\\s*$\");\n@@ -109,6 +111,8 @@ public String getEngineClassName() {\n     private static final Pattern shebangPattern = Pattern.compile(\"^#!.+\");\n     \n     private static boolean declaresShebang(String firstLine) {\n+    \tif( firstLine == null )\n+    \t\treturn false;\n         return shebangPattern.matcher(firstLine).matches();\n     }\n     ", "filename": "src/org/apache/pig/scripting/ScriptEngine.java"}, {"additions": 17, "raw_url": "https://github.com/apache/pig/raw/7e469c81debea1a24260f4f12ef84441c53a416b/test/org/apache/pig/test/TestPigRunner.java", "blob_url": "https://github.com/apache/pig/blob/7e469c81debea1a24260f4f12ef84441c53a416b/test/org/apache/pig/test/TestPigRunner.java", "sha": "7e27529fd9d68a5c44c54096cb11f846619915f2", "changes": 17, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestPigRunner.java?ref=7e469c81debea1a24260f4f12ef84441c53a416b", "patch": "@@ -23,6 +23,7 @@\n \n import java.io.File;\n import java.io.FileWriter;\n+import java.io.IOException;\n import java.io.PrintWriter;\n import java.util.HashMap;\n import java.util.Iterator;\n@@ -600,6 +601,22 @@ public void fsCommandTest() throws Exception {\n         }\n     }\n     \n+    @Test // PIG-2006\n+    public void testEmptyFile() throws IOException {\n+        File f1 = new File(\"myscript.pig\");\n+        f1.deleteOnExit();\n+        \n+        FileWriter fw1 = new FileWriter(f1);\n+        fw1.close();\n+\n+        String[] args = { \"-x\", \"local\", \"-c\", \"myscript.pig\" };\n+        PigStats stats = PigRunner.run(args, null);\n+       \n+        Assert.assertTrue(stats.isSuccessful());\n+        Assert.assertEquals( 0, stats.getReturnCode() );\n+        Assert.assertEquals( null, stats.getErrorMessage() );\n+    }\n+    \n     @Test\n     public void returnCodeTest() throws Exception {\n         PrintWriter w = new PrintWriter(new FileWriter(PIG_FILE));", "filename": "test/org/apache/pig/test/TestPigRunner.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/97a51f0bac943d7e6b34627424b5d16f5fdd50a3", "parent": "https://github.com/apache/pig/commit/cdce20e257efb05347a3d2a9b04caabbda04c754", "message": "PIG-1993: PigStorageSchema throw NPE with ColumnPruning\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1091988 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_29", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/97a51f0bac943d7e6b34627424b5d16f5fdd50a3/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/97a51f0bac943d7e6b34627424b5d16f5fdd50a3/CHANGES.txt", "sha": "14ee89df938109938a1c3d538da2797bf7209e7f", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=97a51f0bac943d7e6b34627424b5d16f5fdd50a3", "patch": "@@ -429,6 +429,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)\n \n BUG FIXES\n \n+PIG-1993: PigStorageSchema throw NPE with ColumnPruning (daijy)\n+\n PIG-1935: New logical plan: Should not push up filter in front of Bincond (daijy)\n \n PIG-1912: non-deterministic output when a file is loaded multiple times (daijy)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/97a51f0bac943d7e6b34627424b5d16f5fdd50a3/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java", "blob_url": "https://github.com/apache/pig/blob/97a51f0bac943d7e6b34627424b5d16f5fdd50a3/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java", "sha": "f920263f6f586f59026e693d7f85b9c7e22c578a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java?ref=97a51f0bac943d7e6b34627424b5d16f5fdd50a3", "patch": "@@ -91,7 +91,7 @@ public Tuple getNext() throws IOException {\n             // We walk the requiredColumns array to find required fields,\n             // and cast those.\n             for (int i = 0; i < fieldSchemas.length; i++) {\n-                if (mRequiredColumns == null || mRequiredColumns[i]) {\n+                if (mRequiredColumns == null || (mRequiredColumns.length>i && mRequiredColumns[i])) {\n                     Object val = null;\n                     if(tup.get(tupleIdx) != null){\n                         byte[] bytes = ((DataByteArray) tup.get(tupleIdx)).get();", "filename": "contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java"}, {"additions": 28, "raw_url": "https://github.com/apache/pig/raw/97a51f0bac943d7e6b34627424b5d16f5fdd50a3/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java", "blob_url": "https://github.com/apache/pig/blob/97a51f0bac943d7e6b34627424b5d16f5fdd50a3/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java", "sha": "068347843832e0efdf6bc89fff5b528e888b7919", "changes": 28, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java?ref=97a51f0bac943d7e6b34627424b5d16f5fdd50a3", "patch": "@@ -219,4 +219,32 @@ public void testByteArrayConversion() throws IOException {\n \n \n     }\n+    \n+    // See PIG-1993\n+    @Test\n+    public void testColumnPrune() throws IOException {\n+        Util.createInputFile(cluster, \"originput2\",\n+                new String[] {\"peter\\t1\", \"samir\\t2\", \"michael\\t4\",\n+                \"peter\\t2\", \"peter\\t4\", \"samir\\t1\", \"john\\t\"\n+        });\n+        Util.createInputFile(cluster, \".pig_schema\",\n+                new String[] {\n+                \"{\\\"fields\\\":[{\\\"name\\\":\\\"name\\\",\\\"type\\\":55,\\\"schema\\\":null,\" +\n+                \"\\\"description\\\":\\\"autogenerated from Pig Field Schema\\\"},\" +\n+                \"{\\\"name\\\":\\\"val\\\",\\\"type\\\":10,\\\"schema\\\":null,\\\"description\\\":\"+\n+                \"\\\"autogenerated from Pig Field Schema\\\"}],\\\"version\\\":0,\" +\n+                \"\\\"sortKeys\\\":[],\\\"sortKeyOrders\\\":[]}\"\n+        });\n+        pig.registerQuery(\"Events = LOAD 'originput2' USING org.apache.pig.piggybank.storage.PigStorageSchema();\");\n+        pig.registerQuery(\"EventsName = foreach Events generate name;\");\n+        Iterator<Tuple> sessions = pig.openIterator(\"EventsName\");\n+        sessions.next().toString().equals(\"(1)\");\n+        sessions.next().toString().equals(\"(2)\");\n+        sessions.next().toString().equals(\"(4)\");\n+        sessions.next().toString().equals(\"(2)\");\n+        sessions.next().toString().equals(\"(4)\");\n+        sessions.next().toString().equals(\"(1)\");\n+        sessions.next().toString().equals(\"()\");\n+        Assert.assertFalse(sessions.hasNext());\n+    }\n }", "filename": "contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/edc5ec1d076a05591ab8cdfa2881eb317f98e202", "parent": "https://github.com/apache/pig/commit/ce385ecec65e9ac62cec3410046dc41ec66645a5", "message": "PIG-1988: Importing an empty macro file causing NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1091242 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_30", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/edc5ec1d076a05591ab8cdfa2881eb317f98e202/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/edc5ec1d076a05591ab8cdfa2881eb317f98e202/CHANGES.txt", "sha": "15d13c4258b9b1e7db03a202956ee40b9c310c68", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=edc5ec1d076a05591ab8cdfa2881eb317f98e202", "patch": "@@ -142,6 +142,8 @@ PIG-1696: Performance: Use System.arraycopy() instead of manually copying the by\n \n BUG FIXES\n \n+PIG-1988: Importing an empty macro file causing NPE (rding)\n+\n PIG-1977: \"Stream closed\" error while reading Pig temp files (results of intermediate jobs) (rding)\n \n PIG-1963: in nested foreach, accumutive udf taking input from order-by does not get results in order (thejas)", "filename": "CHANGES.txt"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/edc5ec1d076a05591ab8cdfa2881eb317f98e202/src/org/apache/pig/parser/QueryParserUtils.java", "blob_url": "https://github.com/apache/pig/blob/edc5ec1d076a05591ab8cdfa2881eb317f98e202/src/org/apache/pig/parser/QueryParserUtils.java", "sha": "9a3f26db7746e5a96a41f4fc5d6957d869f6c987", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/parser/QueryParserUtils.java?ref=edc5ec1d076a05591ab8cdfa2881eb317f98e202", "patch": "@@ -191,7 +191,10 @@ static void replaceNodeWithNodeList(Tree oldNode, CommonTree newTree,\n \n         for (int i = 0; i < count; i++) {\n             if (i == idx) {\n-                parent.addChildren(macroList);\n+                // add only there is something to add\n+                if (macroList != null) {\n+                    parent.addChildren(macroList);\n+                }\n             } else {\n                 parent.addChild((Tree) childList.get(i));\n             }", "filename": "src/org/apache/pig/parser/QueryParserUtils.java"}, {"additions": 30, "raw_url": "https://github.com/apache/pig/raw/edc5ec1d076a05591ab8cdfa2881eb317f98e202/test/org/apache/pig/test/TestMacroExpansion.java", "blob_url": "https://github.com/apache/pig/blob/edc5ec1d076a05591ab8cdfa2881eb317f98e202/test/org/apache/pig/test/TestMacroExpansion.java", "sha": "3536751e15982f9c3fecb03112b099337fc31197", "changes": 31, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestMacroExpansion.java?ref=edc5ec1d076a05591ab8cdfa2881eb317f98e202", "patch": "@@ -1663,6 +1663,31 @@ public void test35() throws Exception {\n         testMacro( query, expected );\n     }\n     \n+    // PIG-1988\n+    @Test\n+    public void test36() throws Exception {\n+        File f = new File(\"mymacro.pig\");\n+        f.deleteOnExit();\n+        \n+        FileWriter fw = new FileWriter(f);\n+        fw.append(\" \");\n+        fw.close();\n+\n+        String query = \"import 'mymacro.pig';\" +\n+            \"define macro1() returns dummy {}; \" + \n+            \"A = load '1.txt' as (a0:int, a1:chararray);\" +\n+            \"dummy = macro1();\" +\n+            \"B = group A by a0;\" + \n+            \"store B into 'output';\";\n+        \n+        String expected = \n+            \"A = load '1.txt' as (a0:int, a1:chararray);\\n\" +\n+            \"B = group A by (a0);\\n\" +\n+            \"store B INTO 'output';\\n\";\n+        \n+        verify(query, expected);\n+    }\n+    \n     @Test\n     public void testCommentInMacro() throws Exception {\n         String query = \"a = load 'testComplexCast' as (m);\\n\" +\n@@ -1734,7 +1759,11 @@ private void verify(String s, String expected) throws Exception {\n         \n         String[] args = { \"-Dpig.import.search.path=/tmp\", \"-x\", \"local\", \"-c\", \"myscript.pig\" };\n         PigStats stats = PigRunner.run(args, null);\n- \n+        \n+        if (!stats.isSuccessful()) {\n+            System.out.println(\"error msg: \" + stats.getErrorMessage());\n+        }\n+        \n         assertTrue(stats.isSuccessful());\n         \n         String[] args2 = { \"-Dpig.import.search.path=/tmp\", \"-x\", \"local\", \"-r\", \"myscript.pig\" };", "filename": "test/org/apache/pig/test/TestMacroExpansion.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/20f7c2c14519092b727904d57171b4d8a0a4732f", "parent": "https://github.com/apache/pig/commit/8c7e23cc630a3127d80cb6764d4d5e05d6d765ca", "message": "PIG-1843: NPE in schema generation\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1071857 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_31", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/20f7c2c14519092b727904d57171b4d8a0a4732f/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/20f7c2c14519092b727904d57171b4d8a0a4732f/CHANGES.txt", "sha": "d796576317136fe0291a8cac7459ba4b49b030af", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=20f7c2c14519092b727904d57171b4d8a0a4732f", "patch": "@@ -299,6 +299,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)\n \n BUG FIXES\n \n+PIG-1843: NPE in schema generation (daijy)\n+\n PIG-1820: New logical plan: FilterLogicExpressionSimplifier fail to deal with UDF (daijy)\n \n PIG-1854: Pig returns exit code 0 for the failed Pig script (rding)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/20f7c2c14519092b727904d57171b4d8a0a4732f/src/org/apache/pig/EvalFunc.java", "blob_url": "https://github.com/apache/pig/blob/20f7c2c14519092b727904d57171b4d8a0a4732f/src/org/apache/pig/EvalFunc.java", "sha": "13640657c4258336dd1469007b39db5d77c8d3b3", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/EvalFunc.java?ref=20f7c2c14519092b727904d57171b4d8a0a4732f", "patch": "@@ -73,7 +73,7 @@\n     private static int nextSchemaId; // for assigning unique ids to UDF columns\n     protected String getSchemaName(String name, Schema input) {\n         String alias = name + \"_\";\n-        if (input.getAliases().size() > 0){\n+        if (input!=null && input.getAliases().size() > 0){\n             alias += input.getAliases().iterator().next() + \"_\";\n         }\n ", "filename": "src/org/apache/pig/EvalFunc.java"}, {"additions": 21, "raw_url": "https://github.com/apache/pig/raw/20f7c2c14519092b727904d57171b4d8a0a4732f/test/org/apache/pig/test/TestEvalPipeline2.java", "blob_url": "https://github.com/apache/pig/blob/20f7c2c14519092b727904d57171b4d8a0a4732f/test/org/apache/pig/test/TestEvalPipeline2.java", "sha": "9d285f9499223426d8a38116e73767712cd63443", "changes": 25, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestEvalPipeline2.java?ref=20f7c2c14519092b727904d57171b4d8a0a4732f", "patch": "@@ -973,13 +973,13 @@ public void testBinStorageByteCast() throws Exception{\n     \n     // See PIG-1761\n     @Test\n-    public void testBagDereferenceInMiddle() throws Exception{\n+    public void testBagDereferenceInMiddle1() throws Exception{\n         String[] input1 = {\n                 \"foo@apache#44\",\n         };\n         \n-        Util.createInputFile(cluster, \"table_testBagDereferenceInMiddle\", input1);\n-        pigServer.registerQuery(\"a = load 'table_testBagDereferenceInMiddle' as (a0:chararray);\");\n+        Util.createInputFile(cluster, \"table_testBagDereferenceInMiddle1\", input1);\n+        pigServer.registerQuery(\"a = load 'table_testBagDereferenceInMiddle1' as (a0:chararray);\");\n         pigServer.registerQuery(\"b = foreach a generate UPPER(REGEX_EXTRACT_ALL(a0, '.*@(.*)#.*').$0);\");\n         \n         Iterator<Tuple> iter = pigServer.openIterator(\"b\");\n@@ -988,6 +988,23 @@ public void testBagDereferenceInMiddle() throws Exception{\n         assertTrue(t.get(0).equals(\"APACHE\"));\n     }\n     \n+    // See PIG-1843\n+    @Test\n+    public void testBagDereferenceInMiddle2() throws Exception{\n+        String[] input1 = {\n+                \"foo apache\",\n+        };\n+        \n+        Util.createInputFile(cluster, \"table_testBagDereferenceInMiddle2\", input1);\n+        pigServer.registerQuery(\"a = load 'table_testBagDereferenceInMiddle2' as (a0:chararray);\");\n+        pigServer.registerQuery(\"b = foreach a generate \" + MapGenerate.class.getName() + \" (STRSPLIT(a0).$0);\");\n+        \n+        Iterator<Tuple> iter = pigServer.openIterator(\"b\");\n+        Tuple t = iter.next();\n+        assertTrue(t.size()==1);\n+        assertTrue(t.toString().equals(\"([key#1])\"));\n+    }\n+    \n     // See PIG-1766\n     @Test\n     public void testForEachSameOriginColumn1() throws Exception{\n@@ -1048,7 +1065,7 @@ public Map exec(Tuple input) throws IOException {\n         \n         @Override\n         public Schema outputSchema(Schema input) {\n-            return new Schema(new Schema.FieldSchema(null, DataType.MAP));\n+            return new Schema(new Schema.FieldSchema(getSchemaName(\"parselong\", input), DataType.MAP));\n         }\n     }\n     ", "filename": "test/org/apache/pig/test/TestEvalPipeline2.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd", "parent": "https://github.com/apache/pig/commit/46abdac090dad98b87b33b8d9663c897354bf64d", "message": "PIG-1647: Logical simplifier throws a NPE (yanz)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@1001838 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_32", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd/CHANGES.txt", "sha": "30784b3126d6c135f09ff6988132413a0577a7b0", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd", "patch": "@@ -207,6 +207,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)\n \n BUG FIXES\n \n+PIG-1647: Logical simplifier throws a NPE (yanz)\n+\n PIG-1642: Order by doesn't use estimation to determine the parallelism (rding)\n \n PIG-1644: New logical plan: Plan.connect with position is misused in some", "filename": "CHANGES.txt"}, {"additions": 30, "raw_url": "https://github.com/apache/pig/raw/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd/src/org/apache/pig/newplan/logical/rules/DNFPlanGenerator.java", "blob_url": "https://github.com/apache/pig/blob/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd/src/org/apache/pig/newplan/logical/rules/DNFPlanGenerator.java", "sha": "9d471eef540eef18a119d98ab2356ac06a9a4883", "changes": 40, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/newplan/logical/rules/DNFPlanGenerator.java?ref=f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd", "patch": "@@ -147,20 +147,30 @@ else if (!isRhsOr) {\n                     int lsize = lhsChildren.length, rsize = rhsChildren.length;\n                     LogicalExpression[][] grandChildrenL = new LogicalExpression[lsize][];;\n                     for (int i = 0; i < lsize; i++) {\n-                        if (lhsChildren[i] instanceof AndExpression || lhsChildren[i] instanceof DNFExpression) grandChildrenL[i] = dnfPlan.getSuccessors(\n-                                        lhsChildren[i]).toArray(\n-                                        new LogicalExpression[0]);\n-                        else {\n+                        if (lhsChildren[i] instanceof AndExpression) {\n+                            grandChildrenL[i] = lhsChildren[i].getPlan().getSuccessors(\n+                              lhsChildren[i]).toArray(\n+                              new LogicalExpression[0]);\n+                        } else if (lhsChildren[i] instanceof DNFExpression) {\n+                            grandChildrenL[i] = dnfPlan.getSuccessors(\n+                              lhsChildren[i]).toArray(\n+                              new LogicalExpression[0]);\n+                        } else {\n                             grandChildrenL[i] = new LogicalExpression[1];\n                             grandChildrenL[i][0] = (LogicalExpression) lhsChildren[i];\n                         }\n                     }\n                     LogicalExpression[][] grandChildrenR = new LogicalExpression[rsize][];;\n                     for (int i = 0; i < rsize; i++) {\n-                        if (rhsChildren[i] instanceof AndExpression || rhsChildren[i] instanceof DNFExpression) grandChildrenR[i] = dnfPlan.getSuccessors(\n-                                        rhsChildren[i]).toArray(\n-                                        new LogicalExpression[0]);\n-                        else {\n+                        if (rhsChildren[i] instanceof AndExpression) {\n+                            grandChildrenR[i] = rhsChildren[i].getPlan().getSuccessors(\n+                              rhsChildren[i]).toArray(\n+                              new LogicalExpression[0]);\n+                        } else if (rhsChildren[i] instanceof DNFExpression) {\n+                            grandChildrenR[i] = dnfPlan.getSuccessors(\n+                              rhsChildren[i]).toArray(\n+                              new LogicalExpression[0]);\n+                        } else {\n                             grandChildrenR[i] = new LogicalExpression[1];\n                             grandChildrenR[i][0] = (LogicalExpression) rhsChildren[i];\n                         }\n@@ -248,9 +258,14 @@ private void mergeSimpleOr(LogicalExpression current,\n         int size = orChildren.length;\n         LogicalExpression[][] grandChildrenOr = new LogicalExpression[size][];;\n         for (int i = 0; i < size; i++) {\n-            if (orChildren[i] instanceof AndExpression || orChildren[i] instanceof DNFExpression) grandChildrenOr[i] = dnfPlan.getSuccessors(\n+            if (orChildren[i] instanceof DNFExpression)\n+              grandChildrenOr[i] = dnfPlan.getSuccessors(\n                             orChildren[i]).toArray(\n                             new LogicalExpression[0]);\n+            else if (orChildren[i] instanceof AndExpression)\n+              grandChildrenOr[i] = orChildren[i].getPlan().getSuccessors(\n+                  orChildren[i]).toArray(\n+                  new LogicalExpression[0]);\n             else {\n                 grandChildrenOr[i] = new LogicalExpression[1];\n                 grandChildrenOr[i][0] = (LogicalExpression) orChildren[i];\n@@ -302,9 +317,14 @@ private void mergeAndOr(LogicalExpression current,\n         boolean andDNF = and.getPlan() == dnfPlan, orDNF = or.getPlan() == dnfPlan;\n         LogicalExpression[][] grandChildrenOr = new LogicalExpression[orSize][];;\n         for (int i = 0; i < orSize; i++) {\n-            if (orChildren[i] instanceof AndExpression || orChildren[i] instanceof DNFExpression) grandChildrenOr[i] = dnfPlan.getSuccessors(\n+            if (orChildren[i] instanceof DNFExpression)\n+              grandChildrenOr[i] = dnfPlan.getSuccessors(\n                             orChildren[i]).toArray(\n                             new LogicalExpression[0]);\n+            else if (orChildren[i] instanceof AndExpression)\n+              grandChildrenOr[i] = orChildren[i].getPlan().getSuccessors(\n+                  orChildren[i]).toArray(\n+                      new LogicalExpression[0]);\n             else {\n                 grandChildrenOr[i] = new LogicalExpression[1];\n                 grandChildrenOr[i][0] = (LogicalExpression) orChildren[i];", "filename": "src/org/apache/pig/newplan/logical/rules/DNFPlanGenerator.java"}, {"additions": 104, "raw_url": "https://github.com/apache/pig/raw/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd/test/org/apache/pig/test/TestFilterSimplification.java", "blob_url": "https://github.com/apache/pig/blob/f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd/test/org/apache/pig/test/TestFilterSimplification.java", "sha": "73773e7e061133cc51c8f26b2fe7fec85dbc4148", "changes": 104, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestFilterSimplification.java?ref=f8c3ad6ea19798e28de4c5bc8d76b05beb17acfd", "patch": "@@ -666,6 +666,110 @@ public void test3() throws Exception {\n         assertTrue(expected.isEqual(newLogicalPlan));\n     }\n \n+    @Test\n+    public void test4() throws Exception {\n+        LogicalPlanTester lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(\"b = filter (load 'd.txt' as (a:chararray, b:long, c:map[], d:chararray, e:chararray)) by a == 'v' and b == 117L and c#'p1' == 'h' and c#'p2' == 'to' and ((d is not null and d != '') or (e is not null and e != ''));\"); \n+\n+        org.apache.pig.impl.logicalLayer.LogicalPlan plan = lpt.buildPlan(\"store b into 'empty';\");\n+        LogicalPlan newLogicalPlan = migratePlan(plan);\n+\n+        PlanOptimizer optimizer = new MyPlanOptimizer(newLogicalPlan, 10);\n+        optimizer.optimize();\n+\n+        lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(\"b = filter (load 'd.txt' as (a:chararray, b:long, c:map[], d:chararray, e:chararray)) by a == 'v' and b == 117L and c#'p1' == 'h' and c#'p2' == 'to' and ((d is not null and d != '') or (e is not null and e != ''));\"); \n+        plan = lpt.buildPlan(\"store b into 'empty';\");\n+        LogicalPlan expected = migratePlan(plan);\n+\n+        assertTrue(expected.isEqual(newLogicalPlan));\n+\n+        // mirror of the above\n+        lpt.buildPlan(\"b = filter (load 'd.txt' as (a:chararray, b:long, c:map[], d:chararray, e:chararray)) by ((d is not null and d != '') or (e is not null and e != '')) and a == 'v' and b == 117L and c#'p1' == 'h' and c#'p2' == 'to';\"); \n+\n+        plan = lpt.buildPlan(\"store b into 'empty';\");\n+        newLogicalPlan = migratePlan(plan);\n+\n+        optimizer = new MyPlanOptimizer(newLogicalPlan, 10);\n+        optimizer.optimize();\n+\n+        lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(\"b = filter (load 'd.txt' as (a:chararray, b:long, c:map[], d:chararray, e:chararray)) by ((d is not null and d != '') or (e is not null and e != '')) and a == 'v' and b == 117L and c#'p1' == 'h' and c#'p2' == 'to';\"); \n+        plan = lpt.buildPlan(\"store b into 'empty';\");\n+        expected = migratePlan(plan);\n+\n+        assertTrue(expected.isEqual(newLogicalPlan));\n+\n+    }\n+\n+    @Test\n+    public void test5() throws Exception {\n+        // 2-level combo: 8 possibilities\n+        boolean[] booleans = {false, true};\n+        for (boolean b1 : booleans)\n+        for (boolean b2 : booleans)\n+        for (boolean b3 : booleans)\n+            comboRunner2(b1, b2, b3);\n+    }\n+\n+    private void comboRunner2(boolean b1, boolean b2, boolean b3) throws Exception {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"b = filter (load 'd.txt' as (a:int, b:int, c:int, d:int)) by (((a < 1) \" + (b1 ? \"and\" : \"or\") + \" (b < 2)) \" + (b2 ? \"and\" : \"or\") + \" ((c < 3) \" + (b3 ? \"and\" : \"or\") + \" (d < 4)));\");  \n+        String query = sb.toString();\n+\n+        LogicalPlanTester lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(query); \n+\n+        org.apache.pig.impl.logicalLayer.LogicalPlan plan = lpt.buildPlan(\"store b into 'empty';\");\n+        LogicalPlan newLogicalPlan = migratePlan(plan);\n+\n+        PlanOptimizer optimizer = new MyPlanOptimizer(newLogicalPlan, 10);\n+        optimizer.optimize();\n+\n+        lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(query); \n+        plan = lpt.buildPlan(\"store b into 'empty';\");\n+        LogicalPlan expected = migratePlan(plan);\n+\n+        assertTrue(expected.isEqual(newLogicalPlan));\n+    }\n+\n+    @Test\n+    public void test6() throws Exception {\n+        // 3-level combo: 128 possibilities\n+        boolean[] booleans = {false, true};\n+        for (boolean b1 : booleans)\n+        for (boolean b2 : booleans)\n+        for (boolean b3 : booleans)\n+        for (boolean b4 : booleans)\n+        for (boolean b5 : booleans)\n+        for (boolean b6 : booleans)\n+        for (boolean b7 : booleans)\n+            comboRunner3(b1, b2, b3, b4, b5, b6, b7);\n+    }\n+\n+    private void comboRunner3(boolean b1, boolean b2, boolean b3, boolean b4, boolean b5, boolean b6, boolean b7) throws Exception {\n+        StringBuilder sb = new StringBuilder();\n+        sb.append(\"b = filter (load 'd.txt' as (a:int, b:int, c:int, d:int, e:int, f:int, g:int, h:int)) by ((((a < 1) \" + (b1 ? \"and\" : \"or\") + \" (b < 2)) \" + (b2 ? \"and\" : \"or\") + \" ((c < 3) \" + (b3 ? \"and\" : \"or\") + \" (d < 4))) \" + (b4 ? \"and\" : \"or\") + \" (((e < 5) \" + (b5 ? \"and\" : \"or\") + \" (f < 6)) \" + (b6 ? \"and\" : \"or\") + \" ((g < 7) \" + (b7 ? \"and\" : \"or\") + \" (h < 8))));\");  \n+        String query = sb.toString();\n+\n+        LogicalPlanTester lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(query); \n+\n+        org.apache.pig.impl.logicalLayer.LogicalPlan plan = lpt.buildPlan(\"store b into 'empty';\");\n+        LogicalPlan newLogicalPlan = migratePlan(plan);\n+\n+        PlanOptimizer optimizer = new MyPlanOptimizer(newLogicalPlan, 10);\n+        optimizer.optimize();\n+\n+        lpt = new LogicalPlanTester(pc);\n+        lpt.buildPlan(query); \n+        plan = lpt.buildPlan(\"store b into 'empty';\");\n+        LogicalPlan expected = migratePlan(plan);\n+\n+        assertTrue(expected.isEqual(newLogicalPlan));\n+    }\n+\n     public class MyPlanOptimizer extends LogicalPlanOptimizer {\n \n         protected MyPlanOptimizer(OperatorPlan p, int iterations) {", "filename": "test/org/apache/pig/test/TestFilterSimplification.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/1d64916466db0da7c57a00a49f3235e7b190a074", "parent": "https://github.com/apache/pig/commit/6c0cdfeb9d2daf997df8ccea091873706ef3b160", "message": "PIG-810: Fixed NPE in PigStats (gates)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@774989 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_33", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/1d64916466db0da7c57a00a49f3235e7b190a074/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/1d64916466db0da7c57a00a49f3235e7b190a074/CHANGES.txt", "sha": "7175634f5ea7e879154ab9c4e3b01273594f21fe", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=1d64916466db0da7c57a00a49f3235e7b190a074", "patch": "@@ -44,6 +44,8 @@ PIG-626: Add access to hadoop counters (shubhamc via gates).\n \n BUG FIXES\n \n+PIG-810: Fixed NPE in PigStats (gates)\n+\n PIG-804: problem with lineage with double map redirection (pradeepkth)\n \n PIG-733: Order by sampling dumps entire sample to hdfs which causes dfs", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/1d64916466db0da7c57a00a49f3235e7b190a074/src/org/apache/pig/tools/pigstats/PigStats.java", "blob_url": "https://github.com/apache/pig/blob/1d64916466db0da7c57a00a49f3235e7b190a074/src/org/apache/pig/tools/pigstats/PigStats.java", "sha": "f0b87c7074e838c8a3fd0cb4aedc9a874f71ce09", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/tools/pigstats/PigStats.java?ref=1d64916466db0da7c57a00a49f3235e7b190a074", "patch": "@@ -172,7 +172,7 @@ else if(mode == ExecType.LOCAL)\n             \n         }\n         \n-        lastJobID = lastJob.getAssignedJobID().toString();\n+        if (lastJob != null) lastJobID = lastJob.getAssignedJobID().toString();\n         return stats;\n     }\n     ", "filename": "src/org/apache/pig/tools/pigstats/PigStats.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/72c1c418f346ea777be91ed5f6534a039526dda1", "parent": "https://github.com/apache/pig/commit/f54829f022c922315de4123ec54adeaac47594aa", "message": "PIG-4503: [Pig on Tez] NPE in UnionOptimizer with multiple levels of union (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1674348 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_34", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/CHANGES.txt", "sha": "6c2882643ac34c27173202e604b37f2e9d984fb1", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -62,6 +62,8 @@ PIG-4333: Split BigData tests into multiple groups (rohini)\n  \n BUG FIXES\n \n+PIG-4503: [Pig on Tez] NPE in UnionOptimizer with multiple levels of union (rohini)\n+\n PIG-4509: [Pig on Tez] Unassigned applications not killed on shutdown (rohini)\n \n PIG-4508: [Pig on Tez] PigProcessor check for commit only on MROutput (rohini)", "filename": "CHANGES.txt"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java", "sha": "41bddcfb8055737fcd455b6488a24c529d07d137", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -153,13 +153,15 @@ public void visitTezOp(TezOperator tezOp) throws VisitorException {\n             TezOperator existingVertexGroup = null;\n             if (successors != null) {\n                 for (TezOperator succ : successors) {\n-                    if (succ.isVertexGroup() && succ.getVertexGroupInfo().getSFile().equals(unionStoreOutputs.get(i).getSFile())) {\n+                    if (succ.isVertexGroup() && unionStoreOutputs.get(i).getSFile().equals(succ.getVertexGroupInfo().getSFile())) {\n                         existingVertexGroup = succ;\n                     }\n                 }\n             }\n             if (existingVertexGroup != null) {\n                 storeVertexGroupOps[i] = existingVertexGroup;\n+                existingVertexGroup.getVertexGroupMembers().remove(unionOp.getOperatorKey());\n+                existingVertexGroup.getVertexGroupInfo().removeInput(unionOp.getOperatorKey());\n             } else {\n                 storeVertexGroupOps[i] = new TezOperator(OperatorKey.genOpKey(scope));\n                 storeVertexGroupOps[i].setVertexGroupInfo(new VertexGroupInfo(unionStoreOutputs.get(i)));\n@@ -471,8 +473,8 @@ private void connectVertexGroupsToSuccessors(TezOperator unionOp,\n             TezOperator succOpVertexGroup = null;\n             for (TezOperator succ : successors) {\n                 if (succ.isVertexGroup()\n-                        && succ.getVertexGroupInfo().getOutput()\n-                                .equals(succOp.getOperatorKey().toString())) {\n+                        && succOp.getOperatorKey().toString()\n+                                .equals(succ.getVertexGroupInfo().getOutput())) {\n                     succOpVertexGroup = succ;\n                     break;\n                 }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/UnionOptimizer.java"}, {"additions": 26, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10-OPTOFF.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10-OPTOFF.gld", "sha": "3fc7248cd795009ff1a7129bef5c40ce83b99a5f", "changes": 48, "status": "modified", "deletions": 22, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10-OPTOFF.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -4,16 +4,16 @@\n #--------------------------------------------------\n # TEZ DAG plan: pig-0_scope-0\n #--------------------------------------------------\n-Tez vertex scope-31\t->\tTez vertex scope-33,\n-Tez vertex scope-32\t->\tTez vertex scope-33,\n-Tez vertex scope-33\t->\tTez vertex scope-38,\n-Tez vertex scope-37\t->\tTez vertex scope-38,\n-Tez vertex scope-38\t->\tTez vertex scope-42,\n-Tez vertex scope-42\n+Tez vertex scope-37\t->\tTez vertex scope-39,\n+Tez vertex scope-38\t->\tTez vertex scope-39,\n+Tez vertex scope-39\t->\tTez vertex scope-44,\n+Tez vertex scope-43\t->\tTez vertex scope-44,\n+Tez vertex scope-44\t->\tTez vertex scope-53,\n+Tez vertex scope-53\n \n-Tez vertex scope-31\n+Tez vertex scope-37\n # Plan on vertex\n-POValueOutputTez - scope-35\t->\t [scope-33]\n+POValueOutputTez - scope-41\t->\t [scope-39]\n |\n |---a: New For Each(false,false)[bag] - scope-7\n     |   |\n@@ -26,9 +26,9 @@ POValueOutputTez - scope-35\t->\t [scope-33]\n     |   |---Project[bytearray][1] - scope-4\n     |\n     |---a: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-0\n-Tez vertex scope-32\n+Tez vertex scope-38\n # Plan on vertex\n-POValueOutputTez - scope-36\t->\t [scope-33]\n+POValueOutputTez - scope-42\t->\t [scope-39]\n |\n |---c: New For Each(false,false)[bag] - scope-15\n     |   |\n@@ -41,14 +41,14 @@ POValueOutputTez - scope-36\t->\t [scope-33]\n     |   |---Project[bytearray][0] - scope-12\n     |\n     |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-8\n-Tez vertex scope-33\n+Tez vertex scope-39\n # Plan on vertex\n-POValueOutputTez - scope-40\t->\t [scope-38]\n+POValueOutputTez - scope-46\t->\t [scope-44]\n |\n-|---POShuffledValueInputTez - scope-34\t<-\t [scope-31, scope-32]\n-Tez vertex scope-37\n+|---POShuffledValueInputTez - scope-40\t<-\t [scope-37, scope-38]\n+Tez vertex scope-43\n # Plan on vertex\n-POValueOutputTez - scope-41\t->\t [scope-38]\n+POValueOutputTez - scope-47\t->\t [scope-44]\n |\n |---d: New For Each(false,false)[bag] - scope-24\n     |   |\n@@ -61,15 +61,19 @@ POValueOutputTez - scope-41\t->\t [scope-38]\n     |   |---Project[bytearray][1] - scope-21\n     |\n     |---d: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-17\n-Tez vertex scope-38\n+Tez vertex scope-44\n # Plan on vertex\n-f: Local Rearrange[tuple]{int}(false) - scope-28\t->\t scope-42\n+e: Split - scope-54\n+|   |\n+|   e: Store(file:///tmp/output1:org.apache.pig.builtin.PigStorage) - scope-29\n |   |\n-|   Project[int][0] - scope-29\n+|   f: Local Rearrange[tuple]{int}(false) - scope-34\t->\t scope-53\n+|   |   |\n+|   |   Project[int][0] - scope-35\n |\n-|---POShuffledValueInputTez - scope-39\t<-\t [scope-33, scope-37]\n-Tez vertex scope-42\n+|---POShuffledValueInputTez - scope-45\t<-\t [scope-39, scope-43]\n+Tez vertex scope-53\n # Plan on vertex\n-f: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-30\n+f: Store(file:///tmp/output2:org.apache.pig.builtin.PigStorage) - scope-36\n |\n-|---f: Package(Packager)[tuple]{int} - scope-27\n+|---f: Package(Packager)[tuple]{int} - scope-33", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10-OPTOFF.gld"}, {"additions": 33, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10.gld", "sha": "dfcc8e7fee0d026e06753ef573fb6e2686289f9a", "changes": 51, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -4,17 +4,22 @@\n #--------------------------------------------------\n # TEZ DAG plan: pig-0_scope-0\n #--------------------------------------------------\n-Tez vertex scope-37\t->\tTez vertex group scope-43,\n-Tez vertex scope-31\t->\tTez vertex group scope-43,\n-Tez vertex scope-32\t->\tTez vertex group scope-43,\n-Tez vertex group scope-43\t->\tTez vertex scope-42,\n-Tez vertex scope-42\n+Tez vertex scope-43\t->\tTez vertex group scope-55,Tez vertex group scope-56,\n+Tez vertex scope-37\t->\tTez vertex group scope-55,Tez vertex group scope-56,\n+Tez vertex scope-38\t->\tTez vertex group scope-55,Tez vertex group scope-56,\n+Tez vertex group scope-56\t->\tTez vertex scope-53,\n+Tez vertex scope-53\n+Tez vertex group scope-55\n \n-Tez vertex scope-37\n+Tez vertex scope-43\n # Plan on vertex\n-f: Local Rearrange[tuple]{int}(false) - scope-46\t->\t scope-42\n+e: Split - scope-61\n+|   |\n+|   e: Store(file:///tmp/output1:org.apache.pig.builtin.PigStorage) - scope-62\t->\t scope-29\n |   |\n-|   Project[int][0] - scope-47\n+|   f: Local Rearrange[tuple]{int}(false) - scope-63\t->\t scope-53\n+|   |   |\n+|   |   Project[int][0] - scope-64\n |\n |---d: New For Each(false,false)[bag] - scope-24\n     |   |\n@@ -27,11 +32,15 @@ f: Local Rearrange[tuple]{int}(false) - scope-46\t->\t scope-42\n     |   |---Project[bytearray][1] - scope-21\n     |\n     |---d: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-17\n-Tez vertex scope-31\n+Tez vertex scope-37\n # Plan on vertex\n-f: Local Rearrange[tuple]{int}(false) - scope-49\t->\t scope-42\n+e: Split - scope-66\n |   |\n-|   Project[int][0] - scope-50\n+|   e: Store(file:///tmp/output1:org.apache.pig.builtin.PigStorage) - scope-67\t->\t scope-29\n+|   |\n+|   f: Local Rearrange[tuple]{int}(false) - scope-68\t->\t scope-53\n+|   |   |\n+|   |   Project[int][0] - scope-69\n |\n |---a: New For Each(false,false)[bag] - scope-7\n     |   |\n@@ -44,11 +53,15 @@ f: Local Rearrange[tuple]{int}(false) - scope-49\t->\t scope-42\n     |   |---Project[bytearray][1] - scope-4\n     |\n     |---a: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-0\n-Tez vertex scope-32\n+Tez vertex scope-38\n # Plan on vertex\n-f: Local Rearrange[tuple]{int}(false) - scope-51\t->\t scope-42\n+e: Split - scope-70\n+|   |\n+|   e: Store(file:///tmp/output1:org.apache.pig.builtin.PigStorage) - scope-71\t->\t scope-29\n |   |\n-|   Project[int][0] - scope-52\n+|   f: Local Rearrange[tuple]{int}(false) - scope-72\t->\t scope-53\n+|   |   |\n+|   |   Project[int][0] - scope-73\n |\n |---c: New For Each(false,false)[bag] - scope-15\n     |   |\n@@ -61,10 +74,12 @@ f: Local Rearrange[tuple]{int}(false) - scope-51\t->\t scope-42\n     |   |---Project[bytearray][0] - scope-12\n     |\n     |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-8\n-Tez vertex group scope-43\t<-\t [scope-37, scope-31, scope-32]\t->\t scope-42\n+Tez vertex group scope-56\t<-\t [scope-43, scope-37, scope-38]\t->\t scope-53\n # No plan on vertex group\n-Tez vertex scope-42\n+Tez vertex scope-53\n # Plan on vertex\n-f: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-30\n+f: Store(file:///tmp/output2:org.apache.pig.builtin.PigStorage) - scope-36\n |\n-|---f: Package(Packager)[tuple]{int} - scope-27\n+|---f: Package(Packager)[tuple]{int} - scope-33\n+Tez vertex group scope-55\t<-\t [scope-43, scope-37, scope-38]\t->\t null\n+# No plan on vertex group", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10.gld"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-11.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-11.gld", "sha": "aca935cb452377018ec5049345832af4df998f41", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-11.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -54,5 +54,5 @@ e: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-42\t->\t sc\n     |   |---Project[bytearray][0] - scope-12\n     |\n     |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-8\n-Tez vertex group scope-38\t<-\t [scope-29, scope-33, scope-27, scope-28]\t->\t null\n+Tez vertex group scope-38\t<-\t [scope-33, scope-27, scope-28]\t->\t null\n # No plan on vertex group", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-11.gld"}, {"additions": 87, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17-OPTOFF.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17-OPTOFF.gld", "sha": "87f8efe0c1c8ad05f3059318d32970cf518639ba", "changes": 87, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17-OPTOFF.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -0,0 +1,87 @@\n+#--------------------------------------------------\n+# There are 1 DAGs in the session\n+#--------------------------------------------------\n+#--------------------------------------------------\n+# TEZ DAG plan: pig-0_scope-0\n+#--------------------------------------------------\n+Tez vertex scope-44\t->\tTez vertex scope-45,Tez vertex scope-46,\n+Tez vertex scope-45\t->\tTez vertex scope-47,\n+Tez vertex scope-46\t->\tTez vertex scope-47,\n+Tez vertex scope-47\n+\n+Tez vertex scope-44\n+# Plan on vertex\n+POValueOutputTez - scope-51\t->\t [scope-45, scope-46]\n+|\n+|---d: New For Each(false,false)[bag] - scope-15\n+    |   |\n+    |   Cast[int] - scope-10\n+    |   |\n+    |   |---Project[bytearray][0] - scope-9\n+    |   |\n+    |   Cast[chararray] - scope-13\n+    |   |\n+    |   |---Project[bytearray][1] - scope-12\n+    |\n+    |---d: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-8\n+Tez vertex scope-45\n+# Plan on vertex\n+POValueOutputTez - scope-49\t->\t [scope-47]\n+|\n+|---e: Filter[bag] - scope-17\n+    |   |\n+    |   Equal To[boolean] - scope-22\n+    |   |\n+    |   |---Project[int][0] - scope-18\n+    |   |\n+    |   |---POUserFunc(org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez)[int] - scope-21\n+    |       |\n+    |       |---Constant(0) - scope-19\n+    |\n+    |---a: New For Each(false,false)[bag] - scope-7\n+        |   |\n+        |   Cast[int] - scope-2\n+        |   |\n+        |   |---Project[bytearray][0] - scope-1\n+        |   |\n+        |   Cast[chararray] - scope-5\n+        |   |\n+        |   |---Project[bytearray][1] - scope-4\n+        |\n+        |---a: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-0\n+Tez vertex scope-46\n+# Plan on vertex\n+POValueOutputTez - scope-50\t->\t [scope-47]\n+|\n+|---c: New For Each(false,false)[bag] - scope-41\n+    |   |\n+    |   Project[int][1] - scope-37\n+    |   |\n+    |   Project[chararray][0] - scope-39\n+    |\n+    |---Filter[bag] - scope-31\n+        |   |\n+        |   Equal To[boolean] - scope-36\n+        |   |\n+        |   |---Project[int][1] - scope-32\n+        |   |\n+        |   |---POUserFunc(org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez)[int] - scope-35\n+        |       |\n+        |       |---Constant(0) - scope-33\n+        |\n+        |---b: New For Each(false,false)[bag] - scope-30\n+            |   |\n+            |   Cast[chararray] - scope-25\n+            |   |\n+            |   |---Project[bytearray][0] - scope-24\n+            |   |\n+            |   Cast[int] - scope-28\n+            |   |\n+            |   |---Project[bytearray][1] - scope-27\n+            |\n+            |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-23\n+Tez vertex scope-47\n+# Plan on vertex\n+e: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-43\n+|\n+|---POShuffledValueInputTez - scope-48\t<-\t [scope-45, scope-46]", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17-OPTOFF.gld"}, {"additions": 84, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17.gld", "sha": "8be94ab46ec8cd59ac8262c08e15b58bded9eba7", "changes": 84, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -0,0 +1,84 @@\n+#--------------------------------------------------\n+# There are 1 DAGs in the session\n+#--------------------------------------------------\n+#--------------------------------------------------\n+# TEZ DAG plan: pig-0_scope-0\n+#--------------------------------------------------\n+Tez vertex scope-44\t->\tTez vertex scope-45,Tez vertex scope-46,\n+Tez vertex scope-45\t->\tTez vertex group scope-52,\n+Tez vertex scope-46\t->\tTez vertex group scope-52,\n+Tez vertex group scope-52\n+\n+Tez vertex scope-44\n+# Plan on vertex\n+POValueOutputTez - scope-51\t->\t [scope-45, scope-46]\n+|\n+|---d: New For Each(false,false)[bag] - scope-15\n+    |   |\n+    |   Cast[int] - scope-10\n+    |   |\n+    |   |---Project[bytearray][0] - scope-9\n+    |   |\n+    |   Cast[chararray] - scope-13\n+    |   |\n+    |   |---Project[bytearray][1] - scope-12\n+    |\n+    |---d: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-8\n+Tez vertex scope-45\n+# Plan on vertex\n+e: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-53\t->\t scope-43\n+|\n+|---e: Filter[bag] - scope-17\n+    |   |\n+    |   Equal To[boolean] - scope-22\n+    |   |\n+    |   |---Project[int][0] - scope-18\n+    |   |\n+    |   |---POUserFunc(org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez)[int] - scope-21\n+    |       |\n+    |       |---Constant(0) - scope-19\n+    |\n+    |---a: New For Each(false,false)[bag] - scope-7\n+        |   |\n+        |   Cast[int] - scope-2\n+        |   |\n+        |   |---Project[bytearray][0] - scope-1\n+        |   |\n+        |   Cast[chararray] - scope-5\n+        |   |\n+        |   |---Project[bytearray][1] - scope-4\n+        |\n+        |---a: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-0\n+Tez vertex scope-46\n+# Plan on vertex\n+e: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-54\t->\t scope-43\n+|\n+|---c: New For Each(false,false)[bag] - scope-41\n+    |   |\n+    |   Project[int][1] - scope-37\n+    |   |\n+    |   Project[chararray][0] - scope-39\n+    |\n+    |---Filter[bag] - scope-31\n+        |   |\n+        |   Equal To[boolean] - scope-36\n+        |   |\n+        |   |---Project[int][1] - scope-32\n+        |   |\n+        |   |---POUserFunc(org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez)[int] - scope-35\n+        |       |\n+        |       |---Constant(0) - scope-33\n+        |\n+        |---b: New For Each(false,false)[bag] - scope-30\n+            |   |\n+            |   Cast[chararray] - scope-25\n+            |   |\n+            |   |---Project[bytearray][0] - scope-24\n+            |   |\n+            |   Cast[int] - scope-28\n+            |   |\n+            |   |---Project[bytearray][1] - scope-27\n+            |\n+            |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-23\n+Tez vertex group scope-52\t<-\t [scope-45, scope-46]\t->\t null\n+# No plan on vertex group", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17.gld"}, {"additions": 71, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18-OPTOFF.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18-OPTOFF.gld", "sha": "d63afeadbe7491c9b274dafc1368fab110e5ddc0", "changes": 71, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18-OPTOFF.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -0,0 +1,71 @@\n+#--------------------------------------------------\n+# There are 1 DAGs in the session\n+#--------------------------------------------------\n+#--------------------------------------------------\n+# TEZ DAG plan: pig-0_scope-0\n+#--------------------------------------------------\n+Tez vertex scope-33\t->\tTez vertex scope-35,\n+Tez vertex scope-34\t->\tTez vertex scope-35,\n+Tez vertex scope-35\t->\tTez vertex scope-39,\n+Tez vertex scope-39\n+\n+Tez vertex scope-33\n+# Plan on vertex\n+POValueOutputTez - scope-37\t->\t [scope-35]\n+|\n+|---a: New For Each(false,false)[bag] - scope-15\n+    |   |\n+    |   Cast[int] - scope-10\n+    |   |\n+    |   |---Project[bytearray][0] - scope-9\n+    |   |\n+    |   Cast[chararray] - scope-13\n+    |   |\n+    |   |---Project[bytearray][1] - scope-12\n+    |\n+    |---a: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-8\n+Tez vertex scope-34\n+# Plan on vertex\n+POValueOutputTez - scope-38\t->\t [scope-35]\n+|\n+|---c: New For Each(false,false)[bag] - scope-23\n+    |   |\n+    |   Cast[int] - scope-18\n+    |   |\n+    |   |---Project[bytearray][1] - scope-17\n+    |   |\n+    |   Cast[chararray] - scope-21\n+    |   |\n+    |   |---Project[bytearray][0] - scope-20\n+    |\n+    |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-16\n+Tez vertex scope-35\n+# Plan on vertex\n+POValueOutputTez - scope-40\t->\t [scope-39]\n+|\n+|---POShuffledValueInputTez - scope-36\t<-\t [scope-33, scope-34]\n+Tez vertex scope-39\n+# Plan on vertex\n+e: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-32\n+|\n+|---e: Filter[bag] - scope-26\n+    |   |\n+    |   Equal To[boolean] - scope-31\n+    |   |\n+    |   |---Project[int][0] - scope-27\n+    |   |\n+    |   |---POUserFunc(org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez)[int] - scope-30\n+    |       |\n+    |       |---Constant(0) - scope-28\n+    |\n+    |---d: New For Each(false,false)[bag] - scope-7\n+        |   |\n+        |   Cast[int] - scope-2\n+        |   |\n+        |   |---Project[bytearray][0] - scope-1\n+        |   |\n+        |   Cast[chararray] - scope-5\n+        |   |\n+        |   |---Project[bytearray][1] - scope-4\n+        |\n+        |---d: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-0", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18-OPTOFF.gld"}, {"additions": 68, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18.gld", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18.gld", "sha": "5c32197613d818b53bed4662ebeb98857f464fe0", "changes": 68, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18.gld?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -0,0 +1,68 @@\n+#--------------------------------------------------\n+# There are 1 DAGs in the session\n+#--------------------------------------------------\n+#--------------------------------------------------\n+# TEZ DAG plan: pig-0_scope-0\n+#--------------------------------------------------\n+Tez vertex scope-33\t->\tTez vertex group scope-41,\n+Tez vertex scope-34\t->\tTez vertex group scope-41,\n+Tez vertex group scope-41\t->\tTez vertex scope-39,\n+Tez vertex scope-39\n+\n+Tez vertex scope-33\n+# Plan on vertex\n+POValueOutputTez - scope-42\t->\t [scope-39]\n+|\n+|---a: New For Each(false,false)[bag] - scope-15\n+    |   |\n+    |   Cast[int] - scope-10\n+    |   |\n+    |   |---Project[bytearray][0] - scope-9\n+    |   |\n+    |   Cast[chararray] - scope-13\n+    |   |\n+    |   |---Project[bytearray][1] - scope-12\n+    |\n+    |---a: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-8\n+Tez vertex scope-34\n+# Plan on vertex\n+POValueOutputTez - scope-43\t->\t [scope-39]\n+|\n+|---c: New For Each(false,false)[bag] - scope-23\n+    |   |\n+    |   Cast[int] - scope-18\n+    |   |\n+    |   |---Project[bytearray][1] - scope-17\n+    |   |\n+    |   Cast[chararray] - scope-21\n+    |   |\n+    |   |---Project[bytearray][0] - scope-20\n+    |\n+    |---b: Load(file:///tmp/input:org.apache.pig.builtin.PigStorage) - scope-16\n+Tez vertex group scope-41\t<-\t [scope-33, scope-34]\t->\t scope-39\n+# No plan on vertex group\n+Tez vertex scope-39\n+# Plan on vertex\n+e: Store(file:///tmp/output:org.apache.pig.builtin.PigStorage) - scope-32\n+|\n+|---e: Filter[bag] - scope-26\n+    |   |\n+    |   Equal To[boolean] - scope-31\n+    |   |\n+    |   |---Project[int][0] - scope-27\n+    |   |\n+    |   |---POUserFunc(org.apache.pig.backend.hadoop.executionengine.tez.plan.udf.ReadScalarsTez)[int] - scope-30\n+    |       |\n+    |       |---Constant(0) - scope-28\n+    |\n+    |---d: New For Each(false,false)[bag] - scope-7\n+        |   |\n+        |   Cast[int] - scope-2\n+        |   |\n+        |   |---Project[bytearray][0] - scope-1\n+        |   |\n+        |   Cast[chararray] - scope-5\n+        |   |\n+        |   |---Project[bytearray][1] - scope-4\n+        |\n+        |---d: Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-0", "filename": "test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18.gld"}, {"additions": 35, "raw_url": "https://github.com/apache/pig/raw/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/tez/TestTezCompiler.java", "blob_url": "https://github.com/apache/pig/blob/72c1c418f346ea777be91ed5f6534a039526dda1/test/org/apache/pig/tez/TestTezCompiler.java", "sha": "d42a27f124947fd961f93007c7444a95482272fc", "changes": 36, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/tez/TestTezCompiler.java?ref=72c1c418f346ea777be91ed5f6534a039526dda1", "patch": "@@ -618,13 +618,15 @@ public void testUnionUnion() throws Exception {\n                 \"d = load 'file:///tmp/input1' as (x:int, y:chararray);\" +\n                 \"e = union onschema c, d;\" +\n                 \"f = group e by x;\" +\n-                \"store f into 'file:///tmp/output';\";\n+                \"store e into 'file:///tmp/output1';\" +\n+                \"store f into 'file:///tmp/output2';\";\n \n         setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, \"\" + true);\n         run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10.gld\");\n         resetScope();\n         setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, \"\" + false);\n         run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-10-OPTOFF.gld\");\n+\n     }\n \n     @Test\n@@ -732,6 +734,38 @@ public void testUnionSplitSkewedJoin() throws Exception {\n         run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-16-OPTOFF.gld\");\n     }\n \n+    @Test\n+    public void testUnionScalar() throws Exception {\n+        String query =\n+                \"a = load 'file:///tmp/input' as (x:int, y:chararray);\" +\n+                \"b = load 'file:///tmp/input' as (y:chararray, x:int);\" +\n+                \"c = union onschema a, b;\" +\n+                \"d = load 'file:///tmp/input1' as (x:int, z:chararray);\" +\n+                \"e = filter c by x == d.x;\" +\n+                \"store e into 'file:///tmp/output';\";\n+\n+        setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, \"\" + true);\n+        run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17.gld\");\n+        resetScope();\n+        setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, \"\" + false);\n+        run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-17-OPTOFF.gld\");\n+\n+        query =\n+                \"a = load 'file:///tmp/input' as (x:int, y:chararray);\" +\n+                \"b = load 'file:///tmp/input' as (y:chararray, x:int);\" +\n+                \"c = union onschema a, b;\" +\n+                \"d = load 'file:///tmp/input1' as (x:int, z:chararray);\" +\n+                \"e = filter d by x == c.x;\" +\n+                \"store e into 'file:///tmp/output';\";\n+\n+        resetScope();\n+        setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, \"\" + true);\n+        run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18.gld\");\n+        resetScope();\n+        setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, \"\" + false);\n+        run(query, \"test/org/apache/pig/test/data/GoldenFiles/tez/TEZC-Union-18-OPTOFF.gld\");\n+    }\n+\n     @Test\n     public void testRank() throws Exception {\n         String query =", "filename": "test/org/apache/pig/tez/TestTezCompiler.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/4dc2910286badf25a3aa43c5ee78ee3cde306b84", "parent": "https://github.com/apache/pig/commit/8277042ff356c58c64cfcdaa0a590890a7de4ffd", "message": "PIG-4220: MapReduce-based Rank failing with NPE due to missing Counters (knoguchi)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1629766 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_35", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/4dc2910286badf25a3aa43c5ee78ee3cde306b84/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/4dc2910286badf25a3aa43c5ee78ee3cde306b84/CHANGES.txt", "sha": "30ba06b03fe0569a71be01bb557a99d6056c8e4a", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=4dc2910286badf25a3aa43c5ee78ee3cde306b84", "patch": "@@ -92,6 +92,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-4220: MapReduce-based Rank failing with NPE due to missing Counters (knoguchi)\n+\n PIG-3985: Multiquery execution of RANK with RANK BY causes NPE (rohini)\n \n PIG-4218: Pig OrcStorage fail to load a map with null key (daijy)", "filename": "CHANGES.txt"}, {"additions": 19, "raw_url": "https://github.com/apache/pig/raw/4dc2910286badf25a3aa43c5ee78ee3cde306b84/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduceCounter.java", "blob_url": "https://github.com/apache/pig/blob/4dc2910286badf25a3aa43c5ee78ee3cde306b84/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduceCounter.java", "sha": "38668bc2c0eca058318ab357a6bcb8143da14085", "changes": 41, "status": "modified", "deletions": 22, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduceCounter.java?ref=4dc2910286badf25a3aa43c5ee78ee3cde306b84", "patch": "@@ -60,6 +60,13 @@ public void setup(Context context) throws IOException, InterruptedException {\n                     pOperator = mp.getPredecessors(pOperator).get(0);\n                 }\n             }\n+\n+            PigStatusReporter reporter = PigStatusReporter.getInstance();\n+            if (reporter != null) {\n+                reporter.incrCounter(\n+                        JobControlCompiler.PIG_MAP_RANK_NAME\n+                        + context.getJobID().toString(), taskID, 0);\n+            }\n         }\n \n         /**\n@@ -69,15 +76,11 @@ public void setup(Context context) throws IOException, InterruptedException {\n         public void collect(Context context, Tuple tuple)\n         throws InterruptedException, IOException {\n             context.write(null, tuple);\n-            try {\n-                PigStatusReporter reporter = PigStatusReporter.getInstance();\n-                if (reporter != null) {\n-                    reporter.incrCounter(\n-                            JobControlCompiler.PIG_MAP_RANK_NAME\n-                            + context.getJobID().toString(), taskID, 1);\n-                }\n-            } catch (Exception ex) {\n-                log.error(\"Error on incrementer of PigMapCounter\");\n+            PigStatusReporter reporter = PigStatusReporter.getInstance();\n+            if (reporter != null) {\n+                reporter.incrCounter(\n+                        JobControlCompiler.PIG_MAP_RANK_NAME\n+                        + context.getJobID().toString(), taskID, 1);\n             }\n         }\n     }\n@@ -116,6 +119,7 @@ protected void setup(Context context) throws IOException, InterruptedException {\n             }\n \n             this.context = context;\n+            incrementCounter(0L);\n         }\n \n         /**\n@@ -127,21 +131,14 @@ protected void setup(Context context) throws IOException, InterruptedException {\n          * @param increment is the value to add to the corresponding global counter.\n          **/\n         public static void incrementCounter(Long increment) {\n-            try {\n-                PigStatusReporter reporter = PigStatusReporter.getInstance();\n-                if (reporter != null) {\n-\n-                    if(leaf instanceof POCounter){\n-                        reporter.incrCounter(\n-                                JobControlCompiler.PIG_MAP_RANK_NAME\n-                                + context.getJobID().toString(), taskID, increment);\n-                    }\n-\n+            PigStatusReporter reporter = PigStatusReporter.getInstance();\n+            if (reporter != null) {\n+                if(leaf instanceof POCounter){\n+                    reporter.incrCounter(\n+                            JobControlCompiler.PIG_MAP_RANK_NAME\n+                            + context.getJobID().toString(), taskID, increment);\n                 }\n-            } catch (Exception ex) {\n-                log.error(\"Error on incrementer of PigReduceCounter\");\n             }\n-\n         }\n     }\n }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduceCounter.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "parent": "https://github.com/apache/pig/commit/0026db31c02138f1685c2ce6afe39dfb1058d480", "message": "PIG-3985: Multiquery execution of RANK with RANK BY causes NPE (rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1629020 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_36", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/CHANGES.txt", "sha": "07a83a71c160b4d532cb5f65c76ec92294da039d", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -92,6 +92,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-3985: Multiquery execution of RANK with RANK BY causes NPE (rohini)\n+\n PIG-4218: Pig OrcStorage fail to load a map with null key (daijy)\n \n PIG-4164: After Pig job finish, Pig client spend too much time retry to connect to AM (daijy)", "filename": "CHANGES.txt"}, {"additions": 22, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java", "sha": "b5611db44512b2b4765badb4fee81abe57c935c9", "changes": 33, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -21,13 +21,15 @@\n import java.util.ArrayList;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROpPlanVisitor;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;\n+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;\n import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;\n import org.apache.pig.impl.plan.NodeIdGenerator;\n import org.apache.pig.impl.plan.Operator;\n@@ -523,23 +525,32 @@ public String getOperationID() {\n     }\n \n     private POCounter getCounterOperation() {\n-        PhysicalOperator operator;\n-        Iterator<PhysicalOperator> it =  this.mapPlan.getLeaves().iterator();\n-\n-        while(it.hasNext()) {\n-            operator = it.next();\n-            if(operator instanceof POCounter)\n-                return (POCounter) operator;\n+        POCounter counter = getCounterOperation(this.mapPlan);\n+        if (counter == null) {\n+            counter = getCounterOperation(this.reducePlan);\n         }\n+        return counter;\n+    }\n \n-        it =  this.reducePlan.getLeaves().iterator();\n+    private POCounter getCounterOperation(PhysicalPlan plan) {\n+        PhysicalOperator operator;\n+        Iterator<PhysicalOperator> it = plan.getLeaves().iterator();\n \n-        while(it.hasNext()) {\n+        while (it.hasNext()) {\n             operator = it.next();\n-            if(operator instanceof POCounter)\n+            if (operator instanceof POCounter) {\n                 return (POCounter) operator;\n+            } else if (operator instanceof POStore) {\n+                List<PhysicalOperator> preds = plan.getPredecessors(operator);\n+                if (preds != null) {\n+                    for (PhysicalOperator pred : preds) {\n+                        if (pred instanceof POCounter) {\n+                            return (POCounter) pred;\n+                        }\n+                    }\n+                }\n+            }\n         }\n-\n         return null;\n     }\n }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java", "sha": "5331766a96d10831efd55a198a0eb6ad25a8ece0", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -121,6 +121,11 @@ public void visitMROp(MapReduceOper mr) throws VisitorException {\n                         + \" uses customPartitioner, do not merge it\");\n                 continue;\n             }\n+            if (successor.isCounterOperation()) {\n+                log.debug(\"Splittee \" + successor.getOperatorKey().getId()\n+                        + \" has POCounter, do not merge it\");\n+                continue;\n+            }\n             if (isMapOnly(successor)) {\n                 if (isSingleLoadMapperPlan(successor.mapPlan)\n                         && isSinglePredecessor(successor)) {", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/TestPOPartialAgg.java", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/TestPOPartialAgg.java", "sha": "b9ce3604a7488453936794a5f7a117838ae4b6c1", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestPOPartialAgg.java?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -17,7 +17,8 @@\n  */\n package org.apache.pig.test;\n \n-import static org.junit.Assert.*;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.fail;\n \n import java.util.ArrayList;\n import java.util.List;\n@@ -324,15 +325,15 @@ private void checkInputAndOutput(String[] inputTups, String[] outputTups,\n \n             res = partAggOp.getNextTuple();\n             assertEquals(POStatus.STATUS_EOP, res.returnStatus);\n-            Util.compareActualAndExpectedResults(outputs, expectedOuts);\n+            Util.checkQueryOutputsAfterSort(outputs, expectedOuts);\n         } else {\n             while (true) {\n                 Result res = partAggOp.getNextTuple();\n                 if (!addResults(res, outputs)) {\n                     break;\n                 }\n             }\n-            Util.compareActualAndExpectedResults(outputs, expectedOuts);\n+            Util.checkQueryOutputsAfterSort(outputs, expectedOuts);\n         }\n \n     }", "filename": "test/org/apache/pig/test/TestPOPartialAgg.java"}, {"additions": 39, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/TestRank3.java", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/TestRank3.java", "sha": "a4a0ab57668468568b07f6137c6912cc244e9eea", "changes": 39, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestRank3.java?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -53,6 +53,12 @@ public void setUp() throws Exception {\n \n             data = resetData(pigServer);\n             data.set(\"empty\");\n+            data.set(\"testsplit\",\n+                    tuple(1, 2),\n+                    tuple(1, 2),\n+                    tuple(3, 1),\n+                    tuple(2, 4),\n+                    tuple(2, 3));\n             data.set(\n                     \"testcascade\",\n                     tuple(3,2,3),\n@@ -156,6 +162,39 @@ public void testRankEmptyRelation() throws Exception {\n       verifyExpected(data.get(\"empty_result\"), expected);\n     }\n \n+    @Test\n+    public void testRankWithSplitInMap() throws Exception {\n+        String query = \"R1 = LOAD 'testsplit' USING mock.Storage() AS (a:int,b:int);\"\n+            + \"R2 = rank R1 by a ;\"\n+            + \"R3 = rank R1 ;\"\n+            + \"R4 = union R2, R3;\"\n+            + \"store R4 into 'R4' using mock.Storage();\";\n+\n+        Util.registerMultiLineQuery(pigServer, query);\n+        List<Tuple> expectedResults = Util\n+                .getTuplesFromConstantTupleStrings(new String[] { \"(1L,1,2)\",\n+                        \"(2L,1,2)\", \"(3L,3,1)\", \"(4L,2,4)\", \"(5L,2,3)\", \"(1L,1,2)\",\n+                        \"(1L,1,2)\", \"(3L,2,3)\", \"(3L,2,4)\", \"(5L,3,1)\" });\n+        Util.checkQueryOutputsAfterSort(data.get(\"R4\"), expectedResults);\n+    }\n+\n+    @Test\n+    public void testRankWithSplitInReduce() throws Exception {\n+        String query = \"R1 = LOAD 'testsplit' USING mock.Storage() AS (a:int,b:int);\"\n+            + \"R1 = ORDER R1 by b;\"\n+            + \"R2 = rank R1 by a ;\"\n+            + \"R3 = rank R1;\"\n+            + \"R4 = union R2, R3;\"\n+            + \"store R4 into 'R4' using mock.Storage();\";\n+\n+        Util.registerMultiLineQuery(pigServer, query);\n+        List<Tuple> expectedResults = Util\n+                .getTuplesFromConstantTupleStrings(new String[] { \"(1L,3,1)\",\n+                        \"(2L,1,2)\", \"(3L,1,2)\", \"(4L,2,3)\", \"(5L,2,4)\", \"(1L,1,2)\",\n+                        \"(1L,1,2)\", \"(3L,2,4)\", \"(3L,2,3)\", \"(5L,3,1)\" });\n+        Util.checkQueryOutputsAfterSort(data.get(\"R4\"), expectedResults);\n+    }\n+\n     public void verifyExpected(List<Tuple> out, Set<Tuple> expected) {\n         for (Tuple tup : out) {\n             assertTrue(expected + \" contains \" + tup, expected.contains(tup));", "filename": "test/org/apache/pig/test/TestRank3.java"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/TestUnionOnSchema.java", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/TestUnionOnSchema.java", "sha": "fa1bacbb73d8b5cb4e6443becd7eccbfb873db0f", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestUnionOnSchema.java?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -492,7 +492,7 @@ public void testUnionOnSchemaAdditionalColumnsWithImplicitSplit() throws IOExcep\n                                 \"(4,5,6,null)\",\n                         });\n         \n-        Util.compareActualAndExpectedResults(list1, expectedRes);\n+        Util.checkQueryOutputsAfterSort(list1, expectedRes);\n         \n         assertEquals(0, list2.size());\n     }\n@@ -852,6 +852,7 @@ public void testUnionOnSchemaUdfTypeEvolution2() throws IOException, ParserExcep\n      * Udf that has schema of tuple column with no inner schema \n      */\n     public static class UDFTupleNullSchema extends EvalFunc <Tuple> {\n+        @Override\n         public Tuple exec(Tuple input) {\n             return input;\n         }", "filename": "test/org/apache/pig/test/TestUnionOnSchema.java"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/Util.java", "blob_url": "https://github.com/apache/pig/blob/04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1/test/org/apache/pig/test/Util.java", "sha": "2e0a46ce8148d00e8eba64007c0879f208827745", "changes": 8, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/Util.java?ref=04e1bbe2c65bcff07bce0d7afc7b2d6c2ff80cb1", "patch": "@@ -540,14 +540,10 @@ static public void checkQueryOutputsAfterSort(Iterator<Tuple> actualResultsIt,\n          while(actualResultsIt.hasNext()){\n              actualResList.add(actualResultsIt.next());\n          }\n-\n-         compareActualAndExpectedResults(actualResList, expectedResList);\n-\n+         checkQueryOutputsAfterSort(actualResList, expectedResList);\n      }\n \n-\n-\n-     static public void compareActualAndExpectedResults(\n+     static public void checkQueryOutputsAfterSort(\n             List<Tuple> actualResList, List<Tuple> expectedResList) {\n          Collections.sort(actualResList);\n          Collections.sort(expectedResList);", "filename": "test/org/apache/pig/test/Util.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/f947c43b34904297f2e5fd967604631cc0c366cb", "parent": "https://github.com/apache/pig/commit/cb3e95936576278a486eae07dda93bb71c0490cd", "message": "PIG-4112: NPE in packager when union + group-by followed by replicated join in Tez (rohini via cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1617200 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_37", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/f947c43b34904297f2e5fd967604631cc0c366cb/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/f947c43b34904297f2e5fd967604631cc0c366cb/CHANGES.txt", "sha": "3dc0179f7e52f0de9d243c7f6d808c3b8801ae87", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=f947c43b34904297f2e5fd967604631cc0c366cb", "patch": "@@ -56,6 +56,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-4112: NPE in packager when union + group-by followed by replicated join in Tez (rohini via cheolsoo)\n+\n PIG-4113: TEZ-1386 breaks hadoop 2 compilation in trunk (cheolsoo)\n \n PIG-4110: TEZ-1382 breaks Hadoop 2 compilation (cheolsoo)", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/f947c43b34904297f2e5fd967604631cc0c366cb/src/org/apache/pig/backend/hadoop/executionengine/tez/TezLauncher.java", "blob_url": "https://github.com/apache/pig/blob/f947c43b34904297f2e5fd967604631cc0c366cb/src/org/apache/pig/backend/hadoop/executionengine/tez/TezLauncher.java", "sha": "725ffd9ceb1c0a3182bb09a81ce0411f19343630", "changes": 3, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/TezLauncher.java?ref=f947c43b34904297f2e5fd967604631cc0c366cb", "patch": "@@ -137,9 +137,6 @@ public void run() {\n             task.setUncaughtExceptionHandler(jctExceptionHandler);\n             task.setContextClassLoader(PigContext.getClassLoader());\n \n-            // TezJobControl always holds a single TezJob. We use JobControl\n-            // only because it is convenient to launch the job via\n-            // ControlledJob.submit().\n             tezStats.setTezJob(runningJob);\n \n             // Mark the times that the jobs were submitted so it's reflected in job", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/TezLauncher.java"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/f947c43b34904297f2e5fd967604631cc0c366cb/src/org/apache/pig/backend/hadoop/executionengine/tez/TezPOPackageAnnotator.java", "blob_url": "https://github.com/apache/pig/blob/f947c43b34904297f2e5fd967604631cc0c366cb/src/org/apache/pig/backend/hadoop/executionengine/tez/TezPOPackageAnnotator.java", "sha": "94122a5dae612c818d33ccfbbb4cafa194ddde60", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/tez/TezPOPackageAnnotator.java?ref=f947c43b34904297f2e5fd967604631cc0c366cb", "patch": "@@ -140,7 +140,7 @@ public LoRearrangeDiscoverer(PhysicalPlan plan, TezOperator pkgTezOp, POPackage\n         @Override\n         public void visitLocalRearrange(POLocalRearrange lrearrange) throws VisitorException {\n             POLocalRearrangeTez lr = (POLocalRearrangeTez) lrearrange;\n-            if (!lr.getOutputKey().equals(pkgTezOp.getOperatorKey().toString())) {\n+            if (!(lr.isConnectedToPackage() && lr.getOutputKey().equals(pkgTezOp.getOperatorKey().toString()))) {\n                 return;\n             }\n             loRearrangeFound++;", "filename": "src/org/apache/pig/backend/hadoop/executionengine/tez/TezPOPackageAnnotator.java"}, {"additions": 10, "raw_url": "https://github.com/apache/pig/raw/f947c43b34904297f2e5fd967604631cc0c366cb/test/e2e/pig/tests/nightly.conf", "blob_url": "https://github.com/apache/pig/blob/f947c43b34904297f2e5fd967604631cc0c366cb/test/e2e/pig/tests/nightly.conf", "sha": "bc288d99d086483d045dbeac7fdc76571e9745a8", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/e2e/pig/tests/nightly.conf?ref=f947c43b34904297f2e5fd967604631cc0c366cb", "patch": "@@ -1493,6 +1493,16 @@ c = union a, b;\n d = foreach c generate (name is not NULL? UPPER(name) : 'FNU LNU') as name, (age < 30 ? -1 : age) as age, (gpa is NULL ? 0.0 : ((gpa > 0.5 AND gpa < 1.0) ? 1 : gpa)) as gpa;\n e = filter d by (name matches '.*MIKE.*') OR (NOT (gpa + 1.5 > 4));\n store e into ':OUTPATH:';\\,\n+            },\n+            {\n+            'num' => 13,\n+            'pig' => q\\a = load ':INPATH:/singlefile/studenttab10k' as (name, age, gpa);\n+b = load ':INPATH:/singlefile/studentcolon10k' using PigStorage(':') as (name, age, gpa);\n+c = union a, b;\n+d = group c by name;\n+e = load ':INPATH:/singlefile/votertab10k' as (name, age, registration, contributions);\n+f = join d by group, e by name using 'replicated';\n+store f into ':OUTPATH:';\\,\n             },\n \t\t]\n \t\t},", "filename": "test/e2e/pig/tests/nightly.conf"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/f9a24a9b9140444e7a0559eb727b66bc036792de", "parent": "https://github.com/apache/pig/commit/beaca12dc346df325f98cd01911b4ae7d51a23e1", "message": "PIG-3988: PigStorage: CommandLineParser is not thread safe (tmwoodruff via rohini) - Fix NPE with commons-cli-1.0.jar\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1601993 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_38", "file": [{"additions": 1, "raw_url": "https://github.com/apache/pig/raw/f9a24a9b9140444e7a0559eb727b66bc036792de/src/org/apache/pig/builtin/PigStorage.java", "blob_url": "https://github.com/apache/pig/blob/f9a24a9b9140444e7a0559eb727b66bc036792de/src/org/apache/pig/builtin/PigStorage.java", "sha": "675f138fc36e2595b430225c2dca3853ed82656e", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/PigStorage.java?ref=f9a24a9b9140444e7a0559eb727b66bc036792de", "patch": "@@ -28,7 +28,6 @@\n import org.apache.commons.cli.GnuParser;\n import org.apache.commons.cli.HelpFormatter;\n import org.apache.commons.cli.Option;\n-import org.apache.commons.cli.OptionBuilder;\n import org.apache.commons.cli.Options;\n import org.apache.commons.cli.ParseException;\n import org.apache.commons.logging.Log;\n@@ -166,7 +165,7 @@ private Options populateValidOptions() {\n         validOptions.addOption(TAG_SOURCE_FILE, false, \"Appends input source file name to beginning of each tuple.\");\n         validOptions.addOption(TAG_SOURCE_PATH, false, \"Appends input source file path to beginning of each tuple.\");\n         validOptions.addOption(\"tagsource\", false, \"Appends input source file name to beginning of each tuple.\");\n-        Option overwrite = new Option(null, \"Overwrites the destination.\");\n+        Option overwrite = new Option(\" \", \"Overwrites the destination.\");\n         overwrite.setLongOpt(\"overwrite\");\n         overwrite.setOptionalArg(true);\n         overwrite.setArgs(1);", "filename": "src/org/apache/pig/builtin/PigStorage.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/01276f04024d2e0d16f31fddef3faf54cdb18d10", "parent": "https://github.com/apache/pig/commit/dfb7e24c95818b95879779b5eee270044f993328", "message": "PIG-3944: PigNullableWritable toString method throws NPE on null value (mauzhang via cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1596062 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_39", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/01276f04024d2e0d16f31fddef3faf54cdb18d10/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/01276f04024d2e0d16f31fddef3faf54cdb18d10/CHANGES.txt", "sha": "7bfc6563cc05b59f87036e44df3a7911e802dfbb", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=01276f04024d2e0d16f31fddef3faf54cdb18d10", "patch": "@@ -133,6 +133,8 @@ PIG-3882: Multiquery off mode execution is not done in batch and very inefficien\n  \n BUG FIXES\n \n+PIG-3944: PigNullableWritable toString method throws NPE on null value (mauzhang via cheolsoo)\n+\n PIG-3936: DBStorage fails on storing nulls for non varchar columns (jeremykarn via cheolsoo) \n \n PIG-3945: Ant not sending hadoopversion to piggybank sub-ant (mrflip via cheolsoo)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/01276f04024d2e0d16f31fddef3faf54cdb18d10/src/org/apache/pig/impl/io/PigNullableWritable.java", "blob_url": "https://github.com/apache/pig/blob/01276f04024d2e0d16f31fddef3faf54cdb18d10/src/org/apache/pig/impl/io/PigNullableWritable.java", "sha": "3941c7ae970db1a37760699fe19e41800dd86ea2", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/impl/io/PigNullableWritable.java?ref=01276f04024d2e0d16f31fddef3faf54cdb18d10", "patch": "@@ -181,6 +181,6 @@ public boolean equals(Object arg0) {\n \n     @Override\n     public String toString() {\n-        return \"Null: \" + mNull + \" index: \" + mIndex + \" \" + mValue.toString();\n+        return \"Null: \" + mNull + \" index: \" + mIndex + (mNull ? \"\" : \" \" + mValue.toString());\n     }\n }", "filename": "src/org/apache/pig/impl/io/PigNullableWritable.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/1611daabf35490a194a5d5171d1e1cfe765046ea", "parent": "https://github.com/apache/pig/commit/3444b733fa4a987e5a91a921d2cd8d0554778097", "message": "PIG-3746: NPE is thrown if Pig fails before PigStats is intialized (cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1569497 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_40", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/1611daabf35490a194a5d5171d1e1cfe765046ea/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/1611daabf35490a194a5d5171d1e1cfe765046ea/CHANGES.txt", "sha": "3c7cb9d46f2ca6011cb23f5a477c199aa3f3af5b", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=1611daabf35490a194a5d5171d1e1cfe765046ea", "patch": "@@ -89,6 +89,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-3746: NPE is thrown if Pig fails before PigStats is intialized (cheolsoo)\n+\n PIG-3747: Update skewed join documentation (cheolsoo)\n \n PIG-3755: auto local mode selection does not check lower bound for size (aniket486)", "filename": "CHANGES.txt"}, {"additions": 12, "raw_url": "https://github.com/apache/pig/raw/1611daabf35490a194a5d5171d1e1cfe765046ea/src/org/apache/pig/tools/pigstats/PigStatsUtil.java", "blob_url": "https://github.com/apache/pig/blob/1611daabf35490a194a5d5171d1e1cfe765046ea/src/org/apache/pig/tools/pigstats/PigStatsUtil.java", "sha": "e690b8dd16365ec18ca174c304407818ff51a3b2", "changes": 15, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/tools/pigstats/PigStatsUtil.java?ref=1611daabf35490a194a5d5171d1e1cfe765046ea", "patch": "@@ -142,15 +142,24 @@ public static PigStats getPigStats(int code) {\n \n \n     public static void setErrorMessage(String msg) {\n-        PigStats.get().setErrorMessage(msg);\n+        PigStats ps = PigStats.get();\n+        if (ps != null) {\n+            ps.setErrorMessage(msg);\n+        }\n     }\n \n     public static void setErrorCode(int code) {\n-        PigStats.get().setErrorCode(code);\n+        PigStats ps = PigStats.get();\n+        if (ps != null) {\n+            ps.setErrorCode(code);\n+        }\n     }\n \n     public static void setErrorThrowable(Throwable t) {\n-        PigStats.get().setErrorThrowable(t);\n+        PigStats ps = PigStats.get();\n+        if (ps != null) {\n+            ps.setErrorThrowable(t);\n+        }\n     }\n \n     private static Pattern pattern = Pattern.compile(\"tmp(-)?[\\\\d]{1,10}$\");", "filename": "src/org/apache/pig/tools/pigstats/PigStatsUtil.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/65b249f8c49bcbcf7aa818dad6de20b9bec70a02", "parent": "https://github.com/apache/pig/commit/ca885a6130d4008b4025084e34e9358837a4ae94", "message": "PIG-3576: NPE due to PIG-3549 when job never gets submitted (lbendig via cheolsoo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1546204 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_41", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/65b249f8c49bcbcf7aa818dad6de20b9bec70a02/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/65b249f8c49bcbcf7aa818dad6de20b9bec70a02/CHANGES.txt", "sha": "b984bf081207dbf96aa6ecba15f63bbee97bd079", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=65b249f8c49bcbcf7aa818dad6de20b9bec70a02", "patch": "@@ -62,8 +62,6 @@ PIG-3285: Jobs using HBaseStorage fail to ship dependency jars (ndimiduk via che\n \n PIG-3582: Document SUM, MIN, MAX, and AVG functions for BigInteger and BigDecimal (harichinnan via cheolsoo)\n \n-PIG-3576: NPE due to PIG-3549 when job never gets submitted (lbendig via cheolsoo)\n-\n PIG-3525: PigStats.get() and ScriptState.get() shouldn't return MR-specific objects (cheolsoo)\n \n PIG-3568: Define the semantics of POStatus.STATUS_NULL (mwagner via cheolsoo)\n@@ -121,6 +119,8 @@ PIG-3480: TFile-based tmpfile compression crashes in some cases (dvryaboy via an\n \n BUG FIXES\n \n+PIG-3576: NPE due to PIG-3549 when job never gets submitted (lbendig via cheolsoo)\n+\n PIG-3567: LogicalPlanPrinter throws OOM for large scripts (aniket486)\n \n PIG-3579: pig.script's deserialized version does not maintain line numbers (jgzhang via aniket486)", "filename": "CHANGES.txt"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/65b249f8c49bcbcf7aa818dad6de20b9bec70a02/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java", "blob_url": "https://github.com/apache/pig/blob/65b249f8c49bcbcf7aa818dad6de20b9bec70a02/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java", "sha": "059db0c04a5509178d72270af91fd66486e4e2bd", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java?ref=65b249f8c49bcbcf7aa818dad6de20b9bec70a02", "patch": "@@ -102,7 +102,7 @@ public void kill() {\n                     RunningJob runningJob = job.getJobClient().getJob(job.getAssignedJobID());\n                     if (runningJob!=null)\n                         runningJob.killJob();\n-                    log.info(\"Job \" + job.getAssignedJobID().toString() + \" killed\");\n+                    log.info(\"Job \" + job.getAssignedJobID() + \" killed\");\n                 }\n             }\n         } catch (Exception e) {\n@@ -536,7 +536,7 @@ private void checkStopOnFailure(boolean stop_on_failure) throws ExecException{\n \n             for (int i=0; i<jc.getFailedJobs().size(); i++) {\n                 Job j = jc.getFailedJobs().get(i);\n-                msg.append(\"JobID: \" + String.valueOf(j.getAssignedJobID()) + \" Reason: \" + j.getMessage());\n+                msg.append(\"JobID: \" + j.getAssignedJobID() + \" Reason: \" + j.getMessage());\n                 if (i!=jc.getFailedJobs().size()-1) {\n                     msg.append(\"\\n\");\n                 }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df", "parent": "https://github.com/apache/pig/commit/52d852b8f8e102b4335f23e3a66c6d31a5f93844", "message": "PIG-3576: NPE due to PIG-3549 when job never gets submitted (lbendig via cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1543100 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_42", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df/CHANGES.txt", "sha": "45cf4674f178f46489942dbfd531a8e83fcfe5fb", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df", "patch": "@@ -54,6 +54,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-3576: NPE due to PIG-3549 when job never gets submitted (lbendig via cheolsoo)\n+\n PIG-3525: PigStats.get() and ScriptState.get() shouldn't return MR-specific objects (cheolsoo)\n \n PIG-3568: Define the semantics of POStatus.STATUS_NULL (mwagner via cheolsoo)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java", "blob_url": "https://github.com/apache/pig/blob/c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java", "sha": "9580c265546de226889318a1e37b134c38ccb982", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java?ref=c0e5bfb7d7c8b3d72d57bbcbb62eb36dc89ce1df", "patch": "@@ -536,7 +536,7 @@ private void checkStopOnFailure(boolean stop_on_failure) throws ExecException{\n \n             for (int i=0; i<jc.getFailedJobs().size(); i++) {\n                 Job j = jc.getFailedJobs().get(i);\n-                msg.append(\"JobID: \" + j.getAssignedJobID().toString() + \" Reason: \" + j.getMessage());\n+                msg.append(\"JobID: \" + String.valueOf(j.getAssignedJobID()) + \" Reason: \" + j.getMessage());\n                 if (i!=jc.getFailedJobs().size()-1) {\n                     msg.append(\"\\n\");\n                 }", "filename": "src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/d3324852207252e156d13fc6ee75d26149ff6731", "parent": "https://github.com/apache/pig/commit/5358546758669c04fcf109210997424eadfc216c", "message": "PIG-3377: New AvroStorage throws NPE when storing untyped map/bag/tuple (jadler via cheolsoo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1536044 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_43", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/d3324852207252e156d13fc6ee75d26149ff6731/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/d3324852207252e156d13fc6ee75d26149ff6731/CHANGES.txt", "sha": "5897047b19536b3afaea6d64045740c3330e4b51", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=d3324852207252e156d13fc6ee75d26149ff6731", "patch": "@@ -40,6 +40,8 @@ OPTIMIZATIONS\n  \n BUG FIXES\n \n+PIG-3377: New AvroStorage throws NPE when storing untyped map/array/bag (jadler via cheolsoo)\n+\n PIG-3542: Javadoc of REGEX_EXTRACT_ALL (nyigitba via daijy)\n \n PIG-3518: Need to ship jruby.jar in the release (daijy)", "filename": "CHANGES.txt"}, {"additions": 13, "raw_url": "https://github.com/apache/pig/raw/d3324852207252e156d13fc6ee75d26149ff6731/src/org/apache/pig/impl/util/avro/AvroStorageSchemaConversionUtilities.java", "blob_url": "https://github.com/apache/pig/blob/d3324852207252e156d13fc6ee75d26149ff6731/src/org/apache/pig/impl/util/avro/AvroStorageSchemaConversionUtilities.java", "sha": "fb89e9c766de24eccd2d7485d39f799cfd0397f8", "changes": 15, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/impl/util/avro/AvroStorageSchemaConversionUtilities.java?ref=d3324852207252e156d13fc6ee75d26149ff6731", "patch": "@@ -427,6 +427,9 @@ private static Schema resourceFieldSchemaToAvroSchema(\n           schema.getFields()[0].getSchema(), name, null,\n           definedRecordNames,\n           doubleColonsToDoubleUnderscores);\n+      if (innerBagSchema == null) {\n+        throw new IOException(\"AvroStorage can't save bags with untyped values; please specify a value type or a schema.\");\n+      }\n       return createNullableUnion(Schema.createArray(innerBagSchema));\n     case DataType.BIGCHARARRAY:\n       return createNullableUnion(Type.STRING);\n@@ -457,10 +460,15 @@ private static Schema resourceFieldSchemaToAvroSchema(\n     case DataType.LONG:\n       return createNullableUnion(Type.LONG);\n     case DataType.MAP:\n+      if (schema == null) {\n+        throw new IOException(\"AvroStorage can't save maps with untyped values; please specify a value type or a schema.\");\n+      }\n       byte innerType = schema.getFields()[0].getType();\n       String desc = schema.getFields()[0].getDescription();\n-      if (desc.equals(\"autogenerated from Pig Field Schema\")) {\n-        desc = null;\n+      if (desc != null) {\n+        if (desc.equals(\"autogenerated from Pig Field Schema\")) {\n+          desc = null;\n+        }\n       }\n       Schema innerSchema;\n       if (DataType.isComplex(innerType)) {\n@@ -480,6 +488,9 @@ private static Schema resourceFieldSchemaToAvroSchema(\n     case DataType.NULL:\n       return Schema.create(Type.NULL);\n     case DataType.TUPLE:\n+      if (schema == null) {\n+        throw new IOException(\"AvroStorage can't save tuples with untyped values; please specify a value type or a schema.\");\n+      }\n       Schema returnSchema = createNullableUnion(\n           resourceSchemaToAvroSchema(schema, name, null,\n               definedRecordNames, doubleColonsToDoubleUnderscores));", "filename": "src/org/apache/pig/impl/util/avro/AvroStorageSchemaConversionUtilities.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b", "parent": "https://github.com/apache/pig/commit/a13db1bc664c212177353daae9e204fd9da679bd", "message": "PIG-3322: AvroStorage give NPE on reading file with union as top level schema (viraj via rohini)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1489264 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_44", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/CHANGES.txt", "sha": "78f65afae8d4995f4f59f8ff83e1c6272f1e3104", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=35fb1eadb7287e3904f1c1fbf9a021db30f37f6b", "patch": "@@ -192,6 +192,8 @@ PIG-3013: BinInterSedes improve chararray sort performance (rohini)\n \n BUG FIXES\n \n+PIG-3322: AvroStorage give NPE on reading file with union as top level schema (viraj via rohini)\n+\n PIG-2828: Handle nulls in DataType.compare (aniket486)\n \n PIG-3335: TestErrorHandling.tesNegative7 fails on MR2 (xuefuz)", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/AvroStorage.java", "blob_url": "https://github.com/apache/pig/blob/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/AvroStorage.java", "sha": "997cf4c182dc0b45c402caeee4e970a7015a3ca9", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/AvroStorage.java?ref=35fb1eadb7287e3904f1c1fbf9a021db30f37f6b", "patch": "@@ -534,7 +534,6 @@ else if (inputs.containsKey(\"schema_file\")) {\n                 AvroStorageLog.details(\"data path=\" + path.toUri().toString());\n                 FileSystem fs = FileSystem.get(path.toUri(), new Configuration());\n                 outputAvroSchema = getAvroSchema(path, fs);\n-                userSpecifiedAvroSchema = outputAvroSchema;\n             } else if (name.equalsIgnoreCase(\"nullable\")) {\n                 nullable = (Boolean) value;\n             } else if (name.equalsIgnoreCase(\"schema\")) {", "filename": "contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/AvroStorage.java"}, {"additions": 6, "raw_url": "https://github.com/apache/pig/raw/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/PigAvroRecordReader.java", "blob_url": "https://github.com/apache/pig/blob/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/PigAvroRecordReader.java", "sha": "531030467ea8d2b92394103b903d8a99ad4c9694", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/PigAvroRecordReader.java?ref=35fb1eadb7287e3904f1c1fbf9a021db30f37f6b", "patch": "@@ -141,7 +141,12 @@ public Writable getCurrentValue() throws IOException, InterruptedException {\n             AvroStorageLog.details(\"Class =\" + obj.getClass());\n             result = (Tuple) obj;\n         } else {\n-            AvroStorageLog.details(\"Wrap calss \" + obj.getClass() + \" as a tuple.\");\n+            if (obj != null) {\n+                AvroStorageLog.details(\"Wrap class \" + obj.getClass() + \" as a tuple.\");\n+            }\n+            else {\n+                AvroStorageLog.details(\"Wrap null as a tuple.\");\n+            }\n             result = wrapAsTuple(obj);\n         }\n         if (schemaToMergedSchemaMap != null) {", "filename": "contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/avro/PigAvroRecordReader.java"}, {"additions": 40, "raw_url": "https://github.com/apache/pig/raw/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java", "blob_url": "https://github.com/apache/pig/blob/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java", "sha": "dad54449161c84100cf2b77b5cf1255f9f849224", "changes": 40, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java?ref=35fb1eadb7287e3904f1c1fbf9a021db30f37f6b", "patch": "@@ -16,6 +16,10 @@\n  */\n package org.apache.pig.piggybank.test.storage.avro;\n \n+import static org.apache.pig.builtin.mock.Storage.resetData;\n+import static org.apache.pig.builtin.mock.Storage.schema;\n+import static org.apache.pig.builtin.mock.Storage.tuple;\n+\n import org.apache.avro.file.DataFileStream;\n import org.apache.avro.generic.GenericDatumReader;\n import org.apache.commons.logging.Log;\n@@ -33,6 +37,8 @@\n import org.apache.pig.backend.executionengine.ExecJob;\n import org.apache.pig.backend.executionengine.ExecJob.JOB_STATUS;\n import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException;\n+import org.apache.pig.builtin.mock.Storage.Data;\n+import org.apache.pig.data.Tuple;\n import org.apache.pig.impl.io.FileLocalizer;\n import org.apache.pig.impl.logicalLayer.FrontendException;\n import org.apache.pig.piggybank.storage.avro.AvroStorage;\n@@ -181,6 +187,7 @@ private static String getInputFile(String file) {\n     final private String testMultipleSchemas1File = getInputFile(\"test_primitive_types/*\");\n     final private String testMultipleSchemas2File = getInputFile(\"test_complex_types/*\");\n     final private String testUserDefinedLoadSchemaFile = getInputFile(\"test_user_defined_load_schema/*\");\n+    final private String testLoadwithNullValues = getInputFile(\"test_loadavrowithnulls.avro\");\n \n     @BeforeClass\n     public static void setup() throws ExecException, IOException {\n@@ -1075,6 +1082,39 @@ public void testCorruptedFile2() throws IOException {\n         verifyResults(output, expected);\n     }\n \n+    @Test\n+    // Schema for the generated avro file test_loadavrowithnulls.avro\n+    // [\"null\",{\"type\":\"record\",\"name\":\"TUPLE_0\",\n+    // \"fields\":[\n+    // {\"name\":\"name\",\"type\":[\"null\",\"string\"],\"doc\":\"autogenerated from Pig Field Schema\"},\n+    // {\"name\":\"age\",\"type\":[\"null\",\"int\"],\"doc\":\"autogenerated from Pig Field Schema\"},\n+    // {\"name\":\"gpa\",\"type\":[\"null\",\"double\"],\"doc\":\"autogenerated from Pig Field Schema\"}]}]\n+    public void testLoadwithNullValues() throws IOException {\n+    //Input is supposed to have empty tuples\n+    PigSchema2Avro.setTupleIndex(0);\n+    Data data = resetData(pigServerLocal);\n+    String output = outbasedir + \"testLoadwithNulls\";\n+    deleteDirectory(new File(output));\n+    String [] queries = {\n+       \" A = load '\" +  testLoadwithNullValues + \"' USING \" +\n+          \" org.apache.pig.piggybank.storage.avro.AvroStorage(); \",\n+       \" B = order A by name;\",\n+       \" store B into '\" +  output +\"' USING mock.Storage();\"\n+       };\n+    testAvroStorage(queries);\n+    List<Tuple> out = data.get(output);\n+    assertEquals(out + \" size\", 4, out.size());\n+\n+    assertEquals(schema(\"name:chararray,age:int,gpa:double\"), data.getSchema(output));\n+\n+    // sorted data ordered by name\n+    assertEquals(tuple((String)null),out.get(0));\n+    assertEquals(tuple((String)null),out.get(1));\n+    assertEquals(tuple(\"calvin ellison\", 24, 0.71), out.get(2));\n+    assertEquals(tuple(\"wendy johnson\", 60, 0.07), out.get(3));\n+\n+   }\n+\n     private static void deleteDirectory (File path) {\n         if ( path.exists()) {\n             File [] files = path.listFiles();", "filename": "contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/avro_test_files/test_loadavrowithnulls.avro", "blob_url": "https://github.com/apache/pig/blob/35fb1eadb7287e3904f1c1fbf9a021db30f37f6b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/avro_test_files/test_loadavrowithnulls.avro", "sha": "1fcaf4c2d277c82af23e4bb1fd79b98ae26ff4a1", "changes": 0, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/avro_test_files/test_loadavrowithnulls.avro?ref=35fb1eadb7287e3904f1c1fbf9a021db30f37f6b", "filename": "contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/avro_test_files/test_loadavrowithnulls.avro"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/7b7a628d475f0a7c3d025b8a7db2df5c35e19c91", "parent": "https://github.com/apache/pig/commit/1d4d72a9780a47a3b5a8929fede677dbff85be6b", "message": "PIG-3132:  NPE when illustrating a relation with HCatLoader (check in missing UDFContextTestLoaderWithSignature.java)\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1458036 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_45", "file": [{"additions": 37, "raw_url": "https://github.com/apache/pig/raw/7b7a628d475f0a7c3d025b8a7db2df5c35e19c91/test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java", "blob_url": "https://github.com/apache/pig/blob/7b7a628d475f0a7c3d025b8a7db2df5c35e19c91/test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java", "sha": "db1c62e0c5b80c977e87603f490adbb956a31bc2", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java?ref=7b7a628d475f0a7c3d025b8a7db2df5c35e19c91", "patch": "@@ -0,0 +1,37 @@\n+package org.apache.pig.test.utils;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.pig.builtin.PigStorage;\n+import org.apache.pig.data.Tuple;\n+import org.apache.pig.impl.util.UDFContext;\n+\n+public class UDFContextTestLoaderWithSignature extends PigStorage {\n+    private String val;\n+    \n+    public UDFContextTestLoaderWithSignature(String v1) {\n+        val = v1;\n+    }\n+    \n+    @Override\n+    public void setLocation(String location, Job job)\n+            throws IOException {\n+        super.setLocation(location, job);\n+        Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass());\n+        if (p.get(signature)==null) {\n+            p.put(\"test_\" + signature, val);\n+        }\n+    }\n+    \n+    @Override\n+    public Tuple getNext() throws IOException {\n+        Tuple t = super.getNext();\n+        if (t!=null) {\n+            Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass());\n+            t.append(p.get(\"test_\" + signature));\n+        }\n+        return t;\n+    }\n+}", "filename": "test/org/apache/pig/test/utils/UDFContextTestLoaderWithSignature.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/cb518031e49814cce0cdaa11adbeeca52b708594", "parent": "https://github.com/apache/pig/commit/90aee49d68f434624032e210d3e91faf1873662b", "message": "PIG-2644: Piggybank's HadoopJobHistoryLoader throws NPE when reading broken history file\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1328551 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_46", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/cb518031e49814cce0cdaa11adbeeca52b708594/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/cb518031e49814cce0cdaa11adbeeca52b708594/CHANGES.txt", "sha": "821cdd2a28de04b843aa235d11d87610c37a8db1", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=cb518031e49814cce0cdaa11adbeeca52b708594", "patch": "@@ -106,6 +106,8 @@ OPTIMIZATIONS\n \n BUG FIXES\n \n+PIG-2644: Piggybank's HadoopJobHistoryLoader throws NPE when reading broken history file (herberts via daijy)\n+\n PIG-2627: Custom partitioner not set when POSplit is involved in Plan (aniket486 via daijy)\n \n PIG-2596: Jython UDF does not handle boolean output (aniket486 via daijy)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/cb518031e49814cce0cdaa11adbeeca52b708594/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java", "blob_url": "https://github.com/apache/pig/blob/cb518031e49814cce0cdaa11adbeeca52b708594/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java", "sha": "c1aba97daa9aa6a9ef04cbe08b3eaa6cda0e1cfe", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java?ref=cb518031e49814cce0cdaa11adbeeca52b708594", "patch": "@@ -482,7 +482,7 @@ private static void populateMapReduceTaskLists (MRJobInfo value,\n             // CHECK_IT: Only one SUCCESSFUL TASK ATTEMPT\n             Map.Entry<String, JobHistory.TaskAttempt> tae = kv.next();\n             JobHistory.TaskAttempt attempt = tae.getValue();\n-            if (attempt.getValues().get(JobHistory.Keys.TASK_STATUS).equals(\n+            if (null != attempt && null != attempt.getValues() && attempt.getValues().containsKey(JobHistory.Keys.TASK_STATUS) && attempt.getValues().get(JobHistory.Keys.TASK_STATUS).equals(\n                     \"SUCCESS\")) {\n                 return attempt.getValues();\n             }", "filename": "contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/4465d83514c1295a7340a667a7d594138d4c9072", "parent": "https://github.com/apache/pig/commit/28b71c5a894a711b094f3c5369a729a8643e3701", "message": "PIG-2027: NPE if Pig don't have permission for log file\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1148855 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_47", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/4465d83514c1295a7340a667a7d594138d4c9072/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/4465d83514c1295a7340a667a7d594138d4c9072/CHANGES.txt", "sha": "1a69b4590c82b33e2cd002bb6f38c370c3cff797", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=4465d83514c1295a7340a667a7d594138d4c9072", "patch": "@@ -89,6 +89,8 @@ PIG-2011: Speed up TestTypedMap.java (dvryaboy)\n \n BUG FIXES\n \n+PIG-2027: NPE if Pig don't have permission for log file (daijy)\n+\n PIG-2171: TestScriptLanguage is broken on trunk (daijy and thejas)\n \n PIG-2172: Fix test failure for ant 1.8.x (daijy)", "filename": "CHANGES.txt"}, {"additions": 2, "raw_url": "https://github.com/apache/pig/raw/4465d83514c1295a7340a667a7d594138d4c9072/src/org/apache/pig/impl/util/LogUtils.java", "blob_url": "https://github.com/apache/pig/blob/4465d83514c1295a7340a667a7d594138d4c9072/src/org/apache/pig/impl/util/LogUtils.java", "sha": "b5b519efebd586d236622c4052992f55d39d8456", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/impl/util/LogUtils.java?ref=4465d83514c1295a7340a667a7d594138d4c9072", "patch": "@@ -169,7 +169,8 @@ public static void writeLog(Throwable t, String logFileName, Log log, boolean ve\n             log.error(bs.toString());\n         } finally {\n             try {\n-                fos.close();\n+                if (fos!=null)\n+                    fos.close();\n             } catch (IOException e) {\n             }\n         }", "filename": "src/org/apache/pig/impl/util/LogUtils.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/adcdeab1773917322a2230b4782a14b7192d2d5d", "parent": "https://github.com/apache/pig/commit/7b650fa16ca82c00b65b04db24fc60fc29bd4a6d", "message": "PIG-2072: NPE when udf has project-star argument and input schema is null\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1103776 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_48", "file": [{"additions": 4, "raw_url": "https://github.com/apache/pig/raw/adcdeab1773917322a2230b4782a14b7192d2d5d/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/adcdeab1773917322a2230b4782a14b7192d2d5d/CHANGES.txt", "sha": "f54ed36f5a29de568faa9a75bfb7a20a3108c870", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=adcdeab1773917322a2230b4782a14b7192d2d5d", "patch": "@@ -44,8 +44,6 @@ BUG FIXES\n \n PIG-2044: Patten match bug in org.apache.pig.newplan.optimizer.Rule (knoguchi via daijy)\n \n-PIG-1938: support project-range as udf argument (thejas)\n-\n PIG-2048: Add zookeeper to pig jar (gbowyer via gates)\n \n PIG-2008: Cache outputFormat in HBaseStorage (thedatachef via gates)\n@@ -72,6 +70,8 @@ PIG-1876: Typed map for Pig (daijy)\n \n IMPROVEMENTS\n \n+PIG-1938: support project-range as udf argument (thejas)\n+\n PIG-2059: PIG doesn't validate incomplete query in batch mode even if -c option is given (xuefu)\n \n PIG-2062: Script silently ended (xuefu)\n@@ -224,6 +224,8 @@ PIG-1696: Performance: Use System.arraycopy() instead of manually copying the by\n \n BUG FIXES\n \n+PIG-2072: NPE when udf has project-star argument and input schema is null (thejas)\n+\n PIG-2075: Bring back TestNewPlanPushUpFilter (daijy)\n \n PIG-1827: When passing a parameter to Pig, if the value contains $ it has to be escaped for no apparent reason (rding)", "filename": "CHANGES.txt"}, {"additions": 9, "raw_url": "https://github.com/apache/pig/raw/adcdeab1773917322a2230b4782a14b7192d2d5d/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java", "blob_url": "https://github.com/apache/pig/blob/adcdeab1773917322a2230b4782a14b7192d2d5d/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java", "sha": "bd4e8947e4a866b35f93a541efc7df6c13bf03b3", "changes": 10, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java?ref=adcdeab1773917322a2230b4782a14b7192d2d5d", "patch": "@@ -973,6 +973,8 @@ private boolean byteArrayFound(UserFuncExpression func, Schema s) throws Visitor\n         for(int i=0;i<s.size();i++){\n             try {\n                 FieldSchema fs=s.getField(i);\n+                if(fs == null)\n+                    return false;\n                 if(fs.type==DataType.BYTEARRAY){\n                     return true;\n                 }\n@@ -1098,7 +1100,10 @@ public static boolean schemaEqualsForMatching(Schema inputSchema,\n \n             FieldSchema inputFieldSchema = i.next();\n             FieldSchema udfFieldSchema = j.next();\n-\n+            if(inputFieldSchema == null)\n+                return false;\n+            \n+            \n             if(ignoreByteArrays && inputFieldSchema.type == DataType.BYTEARRAY) {\n                 continue;\n             }\n@@ -1249,6 +1254,9 @@ private long fitPossible(Schema s1, Schema s2) {\n         int castCnt=0;\n         for(int i=0;i<sFields.size();i++){\n             FieldSchema sFS = sFields.get(i);\n+            if(sFS == null){\n+                return INF;\n+            }\n \n             // if we have a byte array do not include it\n             // in the computation of the score - bytearray", "filename": "src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java"}, {"additions": 24, "raw_url": "https://github.com/apache/pig/raw/adcdeab1773917322a2230b4782a14b7192d2d5d/test/org/apache/pig/test/TestProjectStarRangeInUdf.java", "blob_url": "https://github.com/apache/pig/blob/adcdeab1773917322a2230b4782a14b7192d2d5d/test/org/apache/pig/test/TestProjectStarRangeInUdf.java", "sha": "43cbdee6e742794146ace28a943eacc0c02c4872", "changes": 24, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestProjectStarRangeInUdf.java?ref=adcdeab1773917322a2230b4782a14b7192d2d5d", "patch": "@@ -99,6 +99,30 @@ public void testProjStarExpandInForeach1Negative() throws IOException{\n                 \"org.apache.pig.builtin.CONCAT\");\n     }\n     \n+    @Test\n+    public void testProjStarExpandInForeach1NegativeNoSchema() throws IOException{\n+        \n+        String query;\n+\n+        query =\n+            \"  l1 = load '\" + INP_FILE_5FIELDS + \"' ;\"\n+            + \"f = foreach l1 generate CONCAT(*) as ct;\"\n+            ; \n+        Util.checkExceptionMessage(query, \"f\",\n+                \"Could not infer the matching function for \" +\n+                \"org.apache.pig.builtin.CONCAT\");\n+\n+\n+        query =\n+            \"  l1 = load '\" + INP_FILE_5FIELDS + \"' ;\"\n+            + \"f = foreach l1 generate SIZE(*) as ct;\"\n+            ; \n+        Util.checkExceptionMessage(query, \"f\",\n+                \"Could not infer the matching function for \" +\n+                \"org.apache.pig.builtin.SIZE\");\n+        \n+    }\n+    \n     @Test\n     public void testProjStarExpandInForeach2() throws IOException {\n ", "filename": "test/org/apache/pig/test/TestProjectStarRangeInUdf.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/3505a4289fc9114f214e8e20d5cb59cfd78b16e9", "parent": "https://github.com/apache/pig/commit/ff92a6d6eeec56d455ddd3472b3947a8d8bc3e2c", "message": "PIG-2018: NPE for co-group with group-by column having complex schema and\n different load functions for each input (thejas)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/pig/trunk@1098027 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_49", "file": [{"additions": 4, "raw_url": "https://github.com/apache/pig/raw/3505a4289fc9114f214e8e20d5cb59cfd78b16e9/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/3505a4289fc9114f214e8e20d5cb59cfd78b16e9/CHANGES.txt", "sha": "5fe7c28bc906dd8856c97de0c088c2771130a4ad", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=3505a4289fc9114f214e8e20d5cb59cfd78b16e9", "patch": "@@ -188,6 +188,9 @@ PIG-1696: Performance: Use System.arraycopy() instead of manually copying the by\n \n BUG FIXES\n \n+PIG-2018: NPE for co-group with group-by column having complex schema and \n+ different load functions for each input (thejas)\n+\n PIG-2015: Explain writes out logical plan twice (alangates)\n \n PIG-2017: consumeMap() fails with EmptyStackException (thedatachef via daijy)\n@@ -845,7 +848,7 @@ PIG-1414: Problem with parameter substitution (rding)\n PIG-1407: Logging starts before being configured (azaroth via daijy)\n \n PIG-1391: pig unit tests leave behind files in temp directory because \n-MiniCluster files don't get deleted (tejas)\n+ MiniCluster files don't get deleted (thejas)\n \n PIG-1211: Pig script runs half way after which it reports syntax error\n (pradeepkth)", "filename": "CHANGES.txt"}, {"additions": 0, "raw_url": "https://github.com/apache/pig/raw/3505a4289fc9114f214e8e20d5cb59cfd78b16e9/src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java", "blob_url": "https://github.com/apache/pig/blob/3505a4289fc9114f214e8e20d5cb59cfd78b16e9/src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java", "sha": "51cc7ab6af96faaaad81247b3556c201985ac06a", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java?ref=3505a4289fc9114f214e8e20d5cb59cfd78b16e9", "patch": "@@ -507,7 +507,6 @@ void mapMatchLoadFuncToUid(\n                 //check if all func spec match\n                 if(!funcSpec1.equals(uid2LoadFuncMap.get(fs.uid))){\n                     allMatch = false;\n-                    break;\n                 }\n                 //check if all inner schema match for use later\n                 if(outFS.schema == null ||  !outFS.schema.isEqual(fs.schema)){", "filename": "src/org/apache/pig/newplan/logical/visitor/LineageFindRelVisitor.java"}, {"additions": 13, "raw_url": "https://github.com/apache/pig/raw/3505a4289fc9114f214e8e20d5cb59cfd78b16e9/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java", "blob_url": "https://github.com/apache/pig/blob/3505a4289fc9114f214e8e20d5cb59cfd78b16e9/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java", "sha": "adf3c68527f77b173cf2cc670dfd7695dc77905f", "changes": 13, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java?ref=3505a4289fc9114f214e8e20d5cb59cfd78b16e9", "patch": "@@ -4106,4 +4106,17 @@ public void testUDFNoInnerSchema() throws FrontendException {\n             checkLastForeachCastLoadFunc(query, null, 0);\n         }\n \n+        //see PIG-2018\n+        @Test\n+        public void testCoGroupComplex(){\n+            String query = \n+                \"l1 = load 'x' using PigStorage(':') as (a : (i : int),b,c);\"\n+                + \"l2 = load 'x' as (a,b,c);\"\n+                + \"cg = cogroup l1 by a, l2 by a;\";\n+            try {\n+                createAndProcessLPlan(query);\n+            } catch (FrontendException e) {\n+                fail(\"caught exception creating lp\");\n+            }\n+        }\n }", "filename": "test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c", "parent": "https://github.com/apache/pig/commit/e08f4375537ebf0e4b4a2672444894db0c9a78c5", "message": "PIG-1596: NPE's thrown when attempting to load hbase columns containing null values (zjffdu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@992926 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_50", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c/CHANGES.txt", "sha": "e59239caf1aa17a8cd0217f612d6f5c160bed6db", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=1a1e5e336dee8e0b5b8358b92bfab657f82cae0c", "patch": "@@ -197,6 +197,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)\n \n BUG FIXES\n \n+PIG-1596: NPE's thrown when attempting to load hbase columns containing null values (zjffdu)\n+\n PIG-1597: Development snapshot jar no longer picked up by bin/pig (dvryaboy)\n \n PIG-1599: pig gives generic message for few cases (nrai via rding)", "filename": "CHANGES.txt"}, {"additions": 5, "raw_url": "https://github.com/apache/pig/raw/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java", "blob_url": "https://github.com/apache/pig/blob/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java", "sha": "1b4e5a28ffa6f23f3e08911203c4729b86dda88f", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java?ref=1a1e5e336dee8e0b5b8358b92bfab657f82cae0c", "patch": "@@ -247,7 +247,11 @@ public Tuple getNext() throws IOException {\n                     startIndex++;\n                 }\n                 for (int i=0;i<columnList_.size();++i){\n-                    tuple.set(i+startIndex, new DataByteArray(result.getValue(columnList_.get(i))));\n+                \tbyte[] cell=result.getValue(columnList_.get(i));\n+                \tif (cell!=null)\n+                \t    tuple.set(i+startIndex, new DataByteArray(cell));\n+                \telse\n+                \t    tuple.set(i+startIndex, null);\n                 }\n                 return tuple;\n             }", "filename": "src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java"}, {"additions": 4, "raw_url": "https://github.com/apache/pig/raw/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c/test/org/apache/pig/test/TestHBaseStorage.java", "blob_url": "https://github.com/apache/pig/blob/1a1e5e336dee8e0b5b8358b92bfab657f82cae0c/test/org/apache/pig/test/TestHBaseStorage.java", "sha": "d8f5f6e5bb4a197d77d82815850b1b3f16df7549", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestHBaseStorage.java?ref=1a1e5e336dee8e0b5b8358b92bfab657f82cae0c", "patch": "@@ -171,8 +171,8 @@ public void testLoadFromHBase() throws IOException {\n \t\tprepareTable(TESTTABLE_1, true, DataFormat.UTF8PlainText);\n \t\tpig.registerQuery(\"a = load 'hbase://\" + TESTTABLE_1 + \"' using \"\n \t\t\t\t+ \"org.apache.pig.backend.hadoop.hbase.HBaseStorage('\"\n-\t\t\t\t+ TESTCOLUMN_A + \" \" + TESTCOLUMN_B + \" \" + TESTCOLUMN_C\n-\t\t\t\t+ \"') as (col_a, col_b, col_c);\");\n+\t\t\t\t+ TESTCOLUMN_A + \" \" + TESTCOLUMN_B + \" \" + TESTCOLUMN_C +\" pig:col_d\"\n+\t\t\t\t+ \"') as (col_a, col_b, col_c, col_d);\");\n \t\tIterator<Tuple> it = pig.openIterator(\"a\");\n \t\tint count = 0;\n \t\tLOG.info(\"LoadFromHBase Starting\");\n@@ -182,10 +182,11 @@ public void testLoadFromHBase() throws IOException {\n \t\t\tString col_a = ((DataByteArray) t.get(0)).toString();\n \t\t\tString col_b = ((DataByteArray) t.get(1)).toString();\n \t\t\tString col_c = ((DataByteArray) t.get(2)).toString();\n-\n+\t\t\tObject col_d = t.get(3);       // empty cell\n \t\t\tAssert.assertEquals(count, Integer.parseInt(col_a));\n \t\t\tAssert.assertEquals(count + 0.0, Double.parseDouble(col_b), 1e-6);\n \t\t\tAssert.assertEquals(\"Text_\" + count, col_c);\n+\t\t\tAssert.assertNull(col_d);\n \t\t\tcount++;\n \t\t}\n \t\tAssert.assertEquals(TEST_ROW_COUNT, count);", "filename": "test/org/apache/pig/test/TestHBaseStorage.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/cc58ab80914074ff513e78234f2f2a663e078b04", "parent": "https://github.com/apache/pig/commit/e440118f88700ed4efd8e6b9e14fdea854036549", "message": "PIG-1597: NPEs thrown when attempting to load hbase columns containing null values\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@992677 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_51", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/cc58ab80914074ff513e78234f2f2a663e078b04/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/cc58ab80914074ff513e78234f2f2a663e078b04/CHANGES.txt", "sha": "0a82df22dc789bb98fa33795e91fa90c1f9e2a8a", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=cc58ab80914074ff513e78234f2f2a663e078b04", "patch": "@@ -197,6 +197,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)\n \n BUG FIXES\n \n+PIG-1597: NPE's thrown when attempting to load hbase columns containing null values (dvryaboy)\n+\n PIG-1599: pig gives generic message for few cases (nrai via rding)\n \n PIG-1595: casting relation to scalar- problem with handling of data from non PigStorage loaders (thejas)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/cc58ab80914074ff513e78234f2f2a663e078b04/bin/pig", "blob_url": "https://github.com/apache/pig/blob/cc58ab80914074ff513e78234f2f2a663e078b04/bin/pig", "sha": "ffb155c3d21d7c7cb0f85914b866239141388500", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/bin/pig?ref=cc58ab80914074ff513e78234f2f2a663e078b04", "patch": "@@ -131,7 +131,7 @@ for f in $PIG_HOME/pig-*-core.jar; do\n done\n \n # during development pig jar might be in build\n-for f in $PIG_HOME/build/pig-*-dev.jar; do\n+for f in $PIG_HOME/build/pig-*-SNAPSHOT.jar; do\n     CLASSPATH=${CLASSPATH}:$f;\n done\n ", "filename": "bin/pig"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/9c4d36691393bd0f024d7de5f79c2492cf9469ad", "parent": "https://github.com/apache/pig/commit/28f0849b88b81e2703c633af5082744f1a7bcc45", "message": "PIG-1366: PigStorage's pushProjection implementation results in NPE under certain data conditions (pradeepkth)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@932144 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_52", "file": [{"additions": 3, "raw_url": "https://github.com/apache/pig/raw/9c4d36691393bd0f024d7de5f79c2492cf9469ad/CHANGES.txt", "blob_url": "https://github.com/apache/pig/blob/9c4d36691393bd0f024d7de5f79c2492cf9469ad/CHANGES.txt", "sha": "65cdc4e1b4b077c66a5de982a765cedf9c449f58", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/CHANGES.txt?ref=9c4d36691393bd0f024d7de5f79c2492cf9469ad", "patch": "@@ -39,6 +39,9 @@ PIG-1309: Map-side Cogroup (ashutoshc)\n \n BUG FIXES\n \n+PIG-1366: PigStorage's pushProjection implementation results in NPE under\n+certain data conditions (pradeepkth)\n+\n PIG-1365: WrappedIOException is missing from Pig.jar (pradeepkth)\n \n PIG-1313: PigServer leaks memory over time (billgraham via daijy)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/pig/raw/9c4d36691393bd0f024d7de5f79c2492cf9469ad/src/org/apache/pig/builtin/PigStorage.java", "blob_url": "https://github.com/apache/pig/blob/9c4d36691393bd0f024d7de5f79c2492cf9469ad/src/org/apache/pig/builtin/PigStorage.java", "sha": "de877f55ecfab3b0de9fa955bcc9bb5d304989e8", "changes": 6, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/PigStorage.java?ref=9c4d36691393bd0f024d7de5f79c2492cf9469ad", "patch": "@@ -97,6 +97,7 @@ public PigStorage(String delimiter) {\n \n     @Override\n     public Tuple getNext() throws IOException {\n+        mProtoTuple = new ArrayList<Object>();\n         if (!mRequiredColumnsInitialized) {\n             if (signature!=null) {\n                 Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass());\n@@ -127,7 +128,6 @@ public Tuple getNext() throws IOException {\n                 readField(buf, start, len);\n             }\n             Tuple t =  mTupleFactory.newTupleNoCopy(mProtoTuple);\n-            mProtoTuple = null;\n             return t;\n         } catch (InterruptedException e) {\n             int errCode = 6018;\n@@ -171,10 +171,6 @@ public void putNext(Tuple f) throws IOException {\n     }\n \n     private void readField(byte[] buf, int start, int end) {\n-        if (mProtoTuple == null) {\n-            mProtoTuple = new ArrayList<Object>();\n-        }\n-\n         if (start == end) {\n             // NULL value\n             mProtoTuple.add(null);", "filename": "src/org/apache/pig/builtin/PigStorage.java"}, {"additions": 47, "raw_url": "https://github.com/apache/pig/raw/9c4d36691393bd0f024d7de5f79c2492cf9469ad/test/org/apache/pig/test/TestPigStorage.java", "blob_url": "https://github.com/apache/pig/blob/9c4d36691393bd0f024d7de5f79c2492cf9469ad/test/org/apache/pig/test/TestPigStorage.java", "sha": "356b082d8aa0bee9ef02022c664b311990f91ef4", "changes": 66, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/pig/contents/test/org/apache/pig/test/TestPigStorage.java?ref=9c4d36691393bd0f024d7de5f79c2492cf9469ad", "patch": "@@ -26,46 +26,47 @@\n import java.io.IOException;\n import java.io.PrintWriter;\n import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.Map.Entry;\n \n import junit.framework.Assert;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.pig.ExecType;\n import org.apache.pig.PigServer;\n import org.apache.pig.backend.executionengine.ExecException;\n+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;\n import org.apache.pig.data.Tuple;\n-import org.junit.AfterClass;\n-import org.junit.BeforeClass;\n+import org.apache.pig.impl.io.FileLocalizer;\n+import org.junit.Before;\n import org.junit.Test;\n \n-public class TestPigStorage {\n+public class TestPigStorage  {\n         \n     protected final Log log = LogFactory.getLog(getClass());\n     \n     private static MiniCluster cluster = MiniCluster.buildCluster();\n-    private static PigServer pigServer = null;\n     \n-    \n-    @BeforeClass\n-    public static void setup() {\n-        try {\n-            pigServer = new PigServer(MAPREDUCE, cluster.getProperties());\n-        } catch (ExecException e) {\n-            e.printStackTrace();\n-            Assert.fail();\n-        }\n-    }\n-    \n-    @AfterClass\n-    public static void shutdown() {\n-        pigServer.shutdown();\n+    @Before\n+    public void setup() {\n+        // some tests are in map-reduce mode and some in local - so before\n+        // each test, we will de-initialize FileLocalizer so that temp files\n+        // are created correctly depending on the ExecType in the test.\n+        FileLocalizer.setInitialized(false);\n     }\n     \n     @Test\n-    public void testBlockBoundary() {\n+    public void testBlockBoundary() throws ExecException {\n         \n         // This tests PigStorage loader with records exectly \n         // on the boundary of the file blocks.\n+        Properties props = new Properties();\n+        for (Entry<Object, Object> entry : cluster.getProperties().entrySet()) {\n+            props.put(entry.getKey(), entry.getValue());\n+        }\n+        props.setProperty(\"mapred.max.split.size\", \"20\");\n+        PigServer pigServer = new PigServer(MAPREDUCE, props);\n         String[] inputs = {\n                 \"abcdefgh1\", \"abcdefgh2\", \"abcdefgh3\", \n                 \"abcdefgh4\", \"abcdefgh5\", \"abcdefgh6\",\n@@ -115,5 +116,32 @@ public void testBlockBoundary() {\n             }\n         }\n     } \n+    \n+    /**\n+     * Test to verify that PigStorage works fine in the following scenario:\n+     * The column prune optimization determines only columns 2 and 3 are needed\n+     * and there are records in the data which have only 1 column (malformed data).\n+     * In this case, PigStorage should return an empty tuple to represent columns\n+     * 2 and 3 and {@link POProject} would handle catching any \n+     * {@link IndexOutOfBoundsException} resulting from accessing a field in the\n+     * tuple and substitute a null. \n+     */\n+    @Test\n+    public void testPruneColumnsWithMissingFields() throws IOException {\n+        String inputFileName = \"TestPigStorage-testPruneColumnsWithMissingFields-input.txt\";\n+        Util.createLocalInputFile(\n+                inputFileName, \n+                new String[] {\"1\\t2\\t3\", \"4\", \"5\\t6\\t7\"});\n+        PigServer ps = new PigServer(ExecType.LOCAL);\n+        String script = \"a = load '\" + inputFileName + \"' as (i:int, j:int, k:int);\" +\n+        \t\t\"b = foreach a generate j, k;\";\n+        Util.registerMultiLineQuery(ps, script);\n+        Iterator<Tuple> it = ps.openIterator(\"b\");\n+        assertEquals(Util.createTuple(new Integer[] { 2, 3}), it.next());\n+        assertEquals(Util.createTuple(new Integer[] { null, null}), it.next());\n+        assertEquals(Util.createTuple(new Integer[] { 6, 7}), it.next());\n+        assertFalse(it.hasNext());\n+                \n+    }\n \n }", "filename": "test/org/apache/pig/test/TestPigStorage.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/3a356b5e672dab1a3ad52c7b8d1b396bc3f5965b", "parent": "https://github.com/apache/pig/commit/8eec835de57b4c29519efd3107e277eb948829c2", "message": "PIG-942: Maps are not implicitly casted (fixed a possible NPE from earlier commit for this patch) (pradeepkth)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@818929 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_53", "file": [{"additions": 3, "raw_url": "https://github.com/apache/pig/raw/3a356b5e672dab1a3ad52c7b8d1b396bc3f5965b/src/org/apache/pig/impl/logicalLayer/LOCast.java", "blob_url": "https://github.com/apache/pig/blob/3a356b5e672dab1a3ad52c7b8d1b396bc3f5965b/src/org/apache/pig/impl/logicalLayer/LOCast.java", "sha": "1757078a8b8f87358e73d53af144749559b606f9", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/impl/logicalLayer/LOCast.java?ref=3a356b5e672dab1a3ad52c7b8d1b396bc3f5965b", "patch": "@@ -70,7 +70,9 @@ public Schema getSchema() {\n     public Schema.FieldSchema getFieldSchema() throws FrontendException {\n         if(!mIsFieldSchemaComputed) {\n             mFieldSchema = new Schema.FieldSchema(null, mType);\n-            mFieldSchema.setParent(getExpression().mFieldSchema.canonicalName, getExpression());\n+            Schema.FieldSchema parFs  = getExpression().getFieldSchema();\n+            String canonicalName = (parFs != null ? parFs.canonicalName : null);\n+            mFieldSchema.setParent(canonicalName, getExpression());\n             mIsFieldSchemaComputed = true;\n         }\n         return mFieldSchema;", "filename": "src/org/apache/pig/impl/logicalLayer/LOCast.java"}], "repo": "pig"}, {"commit": "https://github.com/apache/pig/commit/cb9e4699894adae3b547328f0b6663d9672e687c", "parent": "https://github.com/apache/pig/commit/d3b2bab0f3b3b6154dfff8b3b0afef9e4508d150", "message": "PIG-1420: Make CONCAT act on all fields of a tuple, instead of just the first two fields of a tuple (fix NPE)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/pig/trunk@947105 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "pig_54", "file": [{"additions": 2, "raw_url": "https://github.com/apache/pig/raw/cb9e4699894adae3b547328f0b6663d9672e687c/src/org/apache/pig/builtin/CONCAT.java", "blob_url": "https://github.com/apache/pig/blob/cb9e4699894adae3b547328f0b6663d9672e687c/src/org/apache/pig/builtin/CONCAT.java", "sha": "815479d7d33ee74b09034127389a5360a2b038fa", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/CONCAT.java?ref=cb9e4699894adae3b547328f0b6663d9672e687c", "patch": "@@ -45,6 +45,8 @@ public DataByteArray exec(Tuple input) throws IOException {\n \n             DataByteArray db = new DataByteArray();\n             for (int i = 0; i < input.size(); i++) {\n+                if (input.get(i)==null)\n+                    return null;\n                 db.append((DataByteArray)(input.get(i)));\n             }\n             return db;", "filename": "src/org/apache/pig/builtin/CONCAT.java"}, {"additions": 3, "raw_url": "https://github.com/apache/pig/raw/cb9e4699894adae3b547328f0b6663d9672e687c/src/org/apache/pig/builtin/StringConcat.java", "blob_url": "https://github.com/apache/pig/blob/cb9e4699894adae3b547328f0b6663d9672e687c/src/org/apache/pig/builtin/StringConcat.java", "sha": "815e3bf89e22f0a943d891de4c1305bed11bc44b", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/pig/contents/src/org/apache/pig/builtin/StringConcat.java?ref=cb9e4699894adae3b547328f0b6663d9672e687c", "patch": "@@ -39,7 +39,9 @@ public String exec(Tuple input) throws IOException {\n \n           StringBuilder sb = new StringBuilder();\n           for (int i = 0; i < input.size(); i++){\n-            sb.append(String.valueOf(input.get(i)));\n+              if (input.get(i)==null)\n+                  return null;\n+              sb.append(String.valueOf(input.get(i)));\n           }\n           return sb.toString();\n         } catch (ExecException exp) {", "filename": "src/org/apache/pig/builtin/StringConcat.java"}], "repo": "pig"}]
