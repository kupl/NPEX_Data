{
    "samza_02153fa": {
        "bug_id": "samza_02153fa",
        "commit": "https://github.com/apache/samza/commit/02153fa506e38b2e7f01c0374089e200bfe1e363",
        "file": [
            {
                "additions": 111,
                "blob_url": "https://github.com/apache/samza/blob/02153fa506e38b2e7f01c0374089e200bfe1e363/samza-core/src/main/java/org/apache/samza/processor/StreamProcessor.java",
                "changes": 216,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/processor/StreamProcessor.java?ref=02153fa506e38b2e7f01c0374089e200bfe1e363",
                "deletions": 105,
                "filename": "samza-core/src/main/java/org/apache/samza/processor/StreamProcessor.java",
                "patch": "@@ -66,16 +66,16 @@\n   private final Config config;\n   private final long taskShutdownMs;\n   private final String processorId;\n+  private final ExecutorService executorService;\n+  private final Object lock = new Object();\n \n-  private ExecutorService executorService;\n-\n-  private volatile SamzaContainer container = null;\n-  private volatile Throwable containerException = null;\n+  private SamzaContainer container = null;\n+  private Throwable containerException = null;\n+  private boolean processorOnStartCalled = false;\n \n   // Latch used to synchronize between the JobCoordinator thread and the container thread, when the container is\n   // stopped due to re-balancing\n   volatile CountDownLatch jcContainerShutdownLatch;\n-  private volatile boolean processorOnStartCalled = false;\n \n   @VisibleForTesting\n   JobCoordinatorListener jobCoordinatorListener = null;\n@@ -97,7 +97,7 @@\n    */\n   public StreamProcessor(Config config, Map<String, MetricsReporter> customMetricsReporters,\n                          AsyncStreamTaskFactory asyncStreamTaskFactory, StreamProcessorLifecycleListener processorListener) {\n-    this(config, customMetricsReporters, (Object) asyncStreamTaskFactory, processorListener, null);\n+    this(config, customMetricsReporters, asyncStreamTaskFactory, processorListener, null);\n   }\n \n   /**\n@@ -110,7 +110,7 @@ public StreamProcessor(Config config, Map<String, MetricsReporter> customMetrics\n    */\n   public StreamProcessor(Config config, Map<String, MetricsReporter> customMetricsReporters,\n                          StreamTaskFactory streamTaskFactory, StreamProcessorLifecycleListener processorListener) {\n-    this(config, customMetricsReporters, (Object) streamTaskFactory, processorListener, null);\n+    this(config, customMetricsReporters, streamTaskFactory, processorListener, null);\n   }\n \n   /* package private */\n@@ -134,8 +134,9 @@ JobCoordinator getCurrentJobCoordinator() {\n     this.jobCoordinator = (jobCoordinator != null) ? jobCoordinator : getJobCoordinator();\n     this.jobCoordinatorListener = createJobCoordinatorListener();\n     this.jobCoordinator.setListener(jobCoordinatorListener);\n-\n-    processorId = this.jobCoordinator.getProcessorId();\n+    ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(CONTAINER_THREAD_NAME_FORMAT).setDaemon(true).build();\n+    this.executorService = Executors.newSingleThreadExecutor(threadFactory);\n+    this.processorId = this.jobCoordinator.getProcessorId();\n   }\n \n   /**\n@@ -175,124 +176,81 @@ public void start() {\n    * If container is not running, then this method will simply shutdown the {@link JobCoordinator}.\n    *\n    */\n-  public synchronized void stop() {\n-    boolean containerShutdownInvoked = false;\n-    if (container != null) {\n-      try {\n-        LOGGER.info(\"Shutting down the container: {} of stream processor: {}.\", container, processorId);\n-        container.shutdown();\n-        LOGGER.info(\"Waiting {} milliseconds for the container: {} to shutdown.\", taskShutdownMs, container);\n-        containerShutdownInvoked = true;\n-      } catch (Exception exception) {\n-        LOGGER.error(String.format(\"Ignoring the exception during the shutdown of container: %s.\", container), exception);\n+  public void stop() {\n+    synchronized (lock) {\n+      boolean containerShutdownInvoked = false;\n+      if (container != null) {\n+        try {\n+          LOGGER.info(\"Shutting down the container: {} of stream processor: {}.\", container, processorId);\n+          container.shutdown();\n+          containerShutdownInvoked = true;\n+        } catch (Exception exception) {\n+          LOGGER.error(String.format(\"Ignoring the exception during the shutdown of container: %s.\", container), exception);\n+        }\n       }\n-    }\n \n-    if (!containerShutdownInvoked) {\n-      LOGGER.info(\"Shutting down JobCoordinator from StreamProcessor\");\n-      jobCoordinator.stop();\n+      if (!containerShutdownInvoked) {\n+        LOGGER.info(\"Shutting down JobCoordinator from StreamProcessor\");\n+        jobCoordinator.stop();\n+      }\n     }\n   }\n \n   SamzaContainer createSamzaContainer(String processorId, JobModel jobModel) {\n-    return SamzaContainer.apply(\n-        processorId,\n-        jobModel,\n-        config,\n-        ScalaJavaUtil.toScalaMap(customMetricsReporter),\n-        taskFactory);\n+    return SamzaContainer.apply(processorId, jobModel, config, ScalaJavaUtil.toScalaMap(customMetricsReporter), taskFactory);\n   }\n \n   JobCoordinatorListener createJobCoordinatorListener() {\n     return new JobCoordinatorListener() {\n \n       @Override\n       public void onJobModelExpired() {\n-        if (container != null) {\n-          SamzaContainerStatus status = container.getStatus();\n-          if (SamzaContainerStatus.NOT_STARTED.equals(status) || SamzaContainerStatus.STARTED.equals(status)) {\n-            boolean shutdownComplete = false;\n-            try {\n-              LOGGER.info(\"Job model expired. Shutting down the container: {} of stream processor: {}.\", container, processorId);\n-              container.pause();\n-              shutdownComplete = jcContainerShutdownLatch.await(taskShutdownMs, TimeUnit.MILLISECONDS);\n-              LOGGER.info(String.format(\"Shutdown status of container: %s for stream processor: %s is: %s.\", container, processorId, shutdownComplete));\n-            } catch (IllegalContainerStateException icse) {\n-              // Ignored since container is not running\n-              LOGGER.info(String.format(\"Cannot shutdown container: %s for stream processor: %s. Container is not running.\", container, processorId), icse);\n-              shutdownComplete = true;\n-            } catch (InterruptedException e) {\n-              Thread.currentThread().interrupt();\n-              LOGGER.warn(String.format(\"Shutdown of container: %s for stream processor: %s was interrupted\", container, processorId), e);\n-            }\n-            if (!shutdownComplete) {\n-              LOGGER.warn(\"Container: {} shutdown was unsuccessful. Stopping the stream processor: {}.\", container, processorId);\n-              container = null;\n-              stop();\n+        synchronized (lock) {\n+          if (container != null) {\n+            SamzaContainerStatus status = container.getStatus();\n+            if (SamzaContainerStatus.NOT_STARTED.equals(status) || SamzaContainerStatus.STARTED.equals(status)) {\n+              boolean shutdownComplete = false;\n+              try {\n+                LOGGER.info(\"Job model expired. Shutting down the container: {} of stream processor: {}.\", container,\n+                    processorId);\n+                container.pause();\n+                shutdownComplete = jcContainerShutdownLatch.await(taskShutdownMs, TimeUnit.MILLISECONDS);\n+                LOGGER.info(String.format(\"Shutdown status of container: %s for stream processor: %s is: %s.\", container, processorId, shutdownComplete));\n+              } catch (IllegalContainerStateException icse) {\n+                // Ignored since container is not running\n+                LOGGER.info(String.format(\"Cannot shutdown container: %s for stream processor: %s. Container is not running.\", container, processorId), icse);\n+                shutdownComplete = true;\n+              } catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                LOGGER.warn(String.format(\"Shutdown of container: %s for stream processor: %s was interrupted\", container, processorId), e);\n+              } catch (Exception e) {\n+                LOGGER.error(\"Exception occurred when shutting down the container: {}.\", container, e);\n+              }\n+              if (!shutdownComplete) {\n+                LOGGER.warn(\"Container: {} shutdown was unsuccessful. Stopping the stream processor: {}.\", container, processorId);\n+                container = null;\n+                stop();\n+              } else {\n+                LOGGER.info(\"Container: {} shutdown completed for stream processor: {}.\", container, processorId);\n+              }\n             } else {\n-              LOGGER.info(\"Container: {} shutdown completed for stream processor: {}.\", container, processorId);\n+              LOGGER.info(\"Container: {} of the stream processor: {} is not running.\", container, processorId);\n             }\n           } else {\n-            LOGGER.info(\"Container: {} of the stream processor: {} is not running.\", container, processorId);\n+            LOGGER.info(\"Container is not instantiated for stream processor: {}.\", processorId);\n           }\n-        } else {\n-          LOGGER.info(\"Container is not instantiated for stream processor: {}.\", processorId);\n         }\n       }\n \n       @Override\n       public void onNewJobModel(String processorId, JobModel jobModel) {\n-        jcContainerShutdownLatch = new CountDownLatch(1);\n-\n-        SamzaContainerListener containerListener = new SamzaContainerListener() {\n-          @Override\n-          public void onContainerStart() {\n-            if (!processorOnStartCalled) {\n-              // processorListener is called on start only the first time the container starts.\n-              // It is not called after every re-balance of partitions among the processors\n-              processorOnStartCalled = true;\n-              if (processorListener != null) {\n-                processorListener.onStart();\n-              }\n-            } else {\n-              LOGGER.warn(\"Received duplicate container start notification for container: {} in stream processor: {}.\", container, processorId);\n-            }\n-          }\n-\n-          @Override\n-          public void onContainerStop(boolean pauseByJm) {\n-            if (pauseByJm) {\n-              LOGGER.info(\"Container: {} of the stream processor: {} was stopped by the JobCoordinator.\", container, processorId);\n-              if (jcContainerShutdownLatch != null) {\n-                jcContainerShutdownLatch.countDown();\n-              }\n-            } else {  // sp.stop was called or container stopped by itself\n-              LOGGER.info(\"Container: {} stopped. Stopping the stream processor: {}.\", container, processorId);\n-              container = null; // this guarantees that stop() doesn't try to stop container again\n-              stop();\n-            }\n-          }\n-\n-          @Override\n-          public void onContainerFailed(Throwable t) {\n-            if (jcContainerShutdownLatch != null) {\n-              jcContainerShutdownLatch.countDown();\n-            } else {\n-              LOGGER.warn(\"JobCoordinatorLatch was null. It is possible for some component to be waiting.\");\n-            }\n-            containerException = t;\n-            LOGGER.error(String.format(\"Container: %s failed with an exception. Stopping the stream processor: %s. Original exception:\", container, processorId), containerException);\n-            container = null;\n-            stop();\n-          }\n-        };\n-\n-        container = createSamzaContainer(processorId, jobModel);\n-        container.setContainerListener(containerListener);\n-        LOGGER.info(\"Starting the container: {} for the stream processor: {}.\", container, processorId);\n-        ThreadFactory threadFactory = new ThreadFactoryBuilder().setNameFormat(CONTAINER_THREAD_NAME_FORMAT).build();\n-        executorService = Executors.newSingleThreadExecutor(threadFactory);\n-        executorService.submit(container::run);\n+        synchronized (lock) {\n+          jcContainerShutdownLatch = new CountDownLatch(1);\n+          container = createSamzaContainer(processorId, jobModel);\n+          container.setContainerListener(new ContainerListener());\n+          LOGGER.info(\"Starting the container: {} for the stream processor: {}.\", container, processorId);\n+          executorService.submit(container::run);\n+        }\n       }\n \n       @Override\n@@ -324,4 +282,52 @@ public void onCoordinatorFailure(Throwable throwable) {\n   SamzaContainer getContainer() {\n     return container;\n   }\n+\n+  class ContainerListener implements SamzaContainerListener {\n+\n+    @Override\n+    public void onContainerStart() {\n+      if (!processorOnStartCalled) {\n+        // processorListener is called on start only the first time the container starts.\n+        // It is not called after every re-balance of partitions among the processors\n+        processorOnStartCalled = true;\n+        if (processorListener != null) {\n+          processorListener.onStart();\n+        }\n+      } else {\n+        LOGGER.warn(\"Received duplicate container start notification for container: {} in stream processor: {}.\", container, processorId);\n+      }\n+    }\n+\n+    @Override\n+    public void onContainerStop(boolean pauseByJm) {\n+      if (pauseByJm) {\n+        LOGGER.info(\"Container: {} of the stream processor: {} was stopped by the JobCoordinator.\", container, processorId);\n+        if (jcContainerShutdownLatch != null) {\n+          jcContainerShutdownLatch.countDown();\n+        }\n+      } else {  // sp.stop was called or container stopped by itself\n+        LOGGER.info(\"Container: {} stopped. Stopping the stream processor: {}.\", container, processorId);\n+        synchronized (lock) {\n+          container = null; // this guarantees that stop() doesn't try to stop container again\n+          stop();\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public void onContainerFailed(Throwable t) {\n+      if (jcContainerShutdownLatch != null) {\n+        jcContainerShutdownLatch.countDown();\n+      } else {\n+        LOGGER.warn(\"JobCoordinatorLatch was null. It is possible for some component to be waiting.\");\n+      }\n+      synchronized (lock) {\n+        containerException = t;\n+        LOGGER.error(String.format(\"Container: %s failed with an exception. Stopping the stream processor: %s. Original exception:\", container, processorId), containerException);\n+        container = null;\n+        stop();\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/samza/raw/02153fa506e38b2e7f01c0374089e200bfe1e363/samza-core/src/main/java/org/apache/samza/processor/StreamProcessor.java",
                "sha": "73f32e77fcb08f661be12d050265b400cf2f2968",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/samza/blob/02153fa506e38b2e7f01c0374089e200bfe1e363/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java?ref=02153fa506e38b2e7f01c0374089e200bfe1e363",
                "deletions": 43,
                "filename": "samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.Map;\n import java.util.Objects;\n import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import org.I0Itec.zkclient.IZkStateListener;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.samza.checkpoint.CheckpointManager;\n@@ -58,8 +59,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static org.apache.zookeeper.Watcher.Event.KeeperState.*;\n-\n /**\n  * JobCoordinator for stand alone processor managed via Zookeeper.\n  */\n@@ -92,19 +91,19 @@\n   private final ZkJobCoordinatorMetrics metrics;\n   private final Map<String, MetricsReporter> reporters;\n   private final ZkLeaderElector leaderElector;\n+  private final AtomicBoolean initiatedShutdown = new AtomicBoolean(false);\n+  private final StreamMetadataCache streamMetadataCache;\n+  private final SystemAdmins systemAdmins;\n+  private final int debounceTimeMs;\n+  private final Map<TaskName, Integer> changeLogPartitionMap = new HashMap<>();\n \n-  private StreamMetadataCache streamMetadataCache = null;\n-  private SystemAdmins systemAdmins = null;\n-\n-  @VisibleForTesting\n-  ScheduleAfterDebounceTime debounceTimer = null;\n   private JobCoordinatorListener coordinatorListener = null;\n   private JobModel newJobModel;\n-  private int debounceTimeMs;\n   private boolean hasCreatedStreams = false;\n-  private boolean initiatedShutdown = false;\n   private String cachedJobModelVersion = null;\n-  private Map<TaskName, Integer> changeLogPartitionMap = new HashMap<>();\n+\n+  @VisibleForTesting\n+  ScheduleAfterDebounceTime debounceTimer;\n \n   ZkJobCoordinator(Config config, MetricsRegistry metricsRegistry, ZkUtils zkUtils) {\n     this.config = config;\n@@ -142,50 +141,49 @@ public void start() {\n   }\n \n   @Override\n-  public synchronized void stop() {\n+  public void stop() {\n     // Make the shutdown idempotent\n-    if (initiatedShutdown) {\n-      LOG.debug(\"Job Coordinator shutdown is already in progress!\");\n-      return;\n-    }\n+    if (initiatedShutdown.compareAndSet(false, true)) {\n \n-    LOG.info(\"Shutting down Job Coordinator...\");\n-    initiatedShutdown = true;\n-    boolean shutdownSuccessful = false;\n+      LOG.info(\"Shutting down JobCoordinator.\");\n+      boolean shutdownSuccessful = false;\n \n-    // Notify the metrics about abandoning the leadership. Moving it up the chain in the shutdown sequence so that\n-    // in case of unclean shutdown, we get notified about lack of leader and we can set up some alerts around the absence of leader.\n-    metrics.isLeader.set(false);\n+      // Notify the metrics about abandoning the leadership. Moving it up the chain in the shutdown sequence so that\n+      // in case of unclean shutdown, we get notified about lack of leader and we can set up some alerts around the absence of leader.\n+      metrics.isLeader.set(false);\n \n-    try {\n-      // todo: what does it mean for coordinator listener to be null? why not have it part of constructor?\n-      if (coordinatorListener != null) {\n-        coordinatorListener.onJobModelExpired();\n-      }\n+      try {\n+        // todo: what does it mean for coordinator listener to be null? why not have it part of constructor?\n+        if (coordinatorListener != null) {\n+          coordinatorListener.onJobModelExpired();\n+        }\n \n-      debounceTimer.stopScheduler();\n+        debounceTimer.stopScheduler();\n \n-      LOG.debug(\"Shutting down ZkController.\");\n-      zkController.stop();\n+        LOG.debug(\"Shutting down ZkController.\");\n+        zkController.stop();\n \n-      LOG.debug(\"Shutting down system admins.\");\n-      systemAdmins.stop();\n+        LOG.debug(\"Shutting down system admins.\");\n+        systemAdmins.stop();\n \n-      LOG.debug(\"Shutting down metrics.\");\n-      shutdownMetrics();\n+        LOG.debug(\"Shutting down metrics.\");\n+        shutdownMetrics();\n \n-      if (coordinatorListener != null) {\n-        coordinatorListener.onCoordinatorStop();\n-      }\n+        if (coordinatorListener != null) {\n+          coordinatorListener.onCoordinatorStop();\n+        }\n \n-      shutdownSuccessful = true;\n-    } catch (Throwable t) {\n-      LOG.error(\"Encountered errors during job coordinator stop.\", t);\n-      if (coordinatorListener != null) {\n-        coordinatorListener.onCoordinatorFailure(t);\n+        shutdownSuccessful = true;\n+      } catch (Throwable t) {\n+        LOG.error(\"Encountered errors during job coordinator stop.\", t);\n+        if (coordinatorListener != null) {\n+          coordinatorListener.onCoordinatorFailure(t);\n+        }\n+      } finally {\n+        LOG.info(\"Job Coordinator shutdown finished with ShutdownComplete=\" + shutdownSuccessful);\n       }\n-    } finally {\n-      LOG.info(\"Job Coordinator shutdown finished with ShutdownComplete=\" + shutdownSuccessful);\n+    } else {\n+      LOG.info(\"Job Coordinator shutdown is in progress!\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/samza/raw/02153fa506e38b2e7f01c0374089e200bfe1e363/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java",
                "sha": "74abf556593fc14c152f732145c093c40bf9858e",
                "status": "modified"
            }
        ],
        "message": "SAMZA-1647: Fix NPE in onJobModelExpired handler in StreamProcessor.\n\n**Changes:**\n* Switching to using explicit lock in StreamProcessor to make things simpler on state updation.\n* Switch from using synchronized in ZkJobCoordinator to prevent any potential deadlocks\nbetween two threads (where one thread holds the StreamProcessor and other thread has ZkJobCoordinator lock).\n* Misc cleanups in StreamProcessor: Remove volatile qualifiers from state variables in StreamProcessor. Remove reinstantiating the\nexecutorService in onNewJobModel.\n* ZkJobCoordinator cleanups: Make some state variables as immutable.\n\n**NOTE**: The classes in which these changes were made were aynonymous inner classes,\nso to add proper unit tests we need to do big haul of refactor.\n\nAuthor: Shanthoosh Venkataraman <santhoshvenkat1988@gmail.com>\n\nReviewers: Jagadish <jagadish@apache.org>\n\nCloses #493 from shanthoosh/fix_npe_in_jobmodel_expired_handler",
        "parent": "https://github.com/apache/samza/commit/72ad7523fffdcafdc01a0c6922fc94ccd1e482a5",
        "repo": "samza",
        "unit_tests": [
            "TestStreamProcessor.java",
            "TestZkJobCoordinator.java"
        ]
    },
    "samza_04d00f5": {
        "bug_id": "samza_04d00f5",
        "commit": "https://github.com/apache/samza/commit/04d00f5a4291f54b2e64d555d7ef7f9641011c19",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/samza/blob/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-core/src/main/scala/org/apache/samza/util/Util.scala",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/scala/org/apache/samza/util/Util.scala?ref=04d00f5a4291f54b2e64d555d7ef7f9641011c19",
                "deletions": 4,
                "filename": "samza-core/src/main/scala/org/apache/samza/util/Util.scala",
                "patch": "@@ -102,21 +102,29 @@ object Util extends Logging {\n   }\n \n   /**\n-   * Returns a SystemStream object based on the system stream name given. For \n+   * Returns a SystemStream object based on the system stream name given. For\n    * example, kafka.topic would return new SystemStream(\"kafka\", \"topic\").\n    */\n   def getSystemStreamFromNames(systemStreamNames: String): SystemStream = {\n     val idx = systemStreamNames.indexOf('.')\n-    if(idx < 0)\n+    if (idx < 0)\n       throw new IllegalArgumentException(\"No '.' in stream name '\" + systemStreamNames + \"'. Stream names should be in the form 'system.stream'\")\n     new SystemStream(systemStreamNames.substring(0, idx), systemStreamNames.substring(idx + 1, systemStreamNames.length))\n   }\n-  \n+\n   /**\n-   * Returns a SystemStream object based on the system stream name given. For \n+   * Returns a SystemStream object based on the system stream name given. For\n    * example, kafka.topic would return new SystemStream(\"kafka\", \"topic\").\n    */\n   def getNameFromSystemStream(systemStream: SystemStream) = {\n     systemStream.getSystem + \".\" + systemStream.getStream\n   }\n+\n+  /**\n+   * Makes sure that an object is not null, and throws a NullPointerException\n+   * if it is.\n+   */\n+  def notNull[T](obj: T, msg: String) = if (obj == null) {\n+    throw new NullPointerException(msg)\n+  }\n }",
                "raw_url": "https://github.com/apache/samza/raw/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-core/src/main/scala/org/apache/samza/util/Util.scala",
                "sha": "6b2ec49747181cbd3eb40c0cf49fd0077f1a7904",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngineFactory.scala",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngineFactory.scala?ref=04d00f5a4291f54b2e64d555d7ef7f9641011c19",
                "deletions": 1,
                "filename": "samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngineFactory.scala",
                "patch": "@@ -73,7 +73,7 @@ class KeyValueStorageEngineFactory[K, V] extends StorageEngineFactory[K, V] {\n     } else {\n       serialized\n     }\n-    val db = maybeCachedStore\n+    val db = new NullSafeKeyValueStore(maybeCachedStore)\n     val keyValueStorageEngineMetrics = new KeyValueStorageEngineMetrics(storeName, registry)\n \n     // Decide if we should use raw bytes when restoring",
                "raw_url": "https://github.com/apache/samza/raw/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngineFactory.scala",
                "sha": "ea75fab9c315a005cd15efe79cbd4ec0d5f13f38",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/samza/blob/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/main/scala/org/apache/samza/storage/kv/NullSafeKeyValueStore.scala",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/scala/org/apache/samza/storage/kv/NullSafeKeyValueStore.scala?ref=04d00f5a4291f54b2e64d555d7ef7f9641011c19",
                "deletions": 0,
                "filename": "samza-kv/src/main/scala/org/apache/samza/storage/kv/NullSafeKeyValueStore.scala",
                "patch": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.samza.storage.kv\n+\n+import org.apache.samza.util.Util.notNull\n+import scala.collection.JavaConversions._\n+\n+object NullSafeKeyValueStore {\n+  val KEY_ERROR_MSG = \"Null is not a valid key.\"\n+  val VAL_ERROR_MSG = \"Null is not a valid value.\"\n+}\n+\n+class NullSafeKeyValueStore[K, V](store: KeyValueStore[K, V]) extends KeyValueStore[K, V] {\n+  import NullSafeKeyValueStore._\n+\n+  def get(key: K): V = {\n+    notNull(key, KEY_ERROR_MSG)\n+    store.get(key)\n+  }\n+\n+  def put(key: K, value: V) {\n+    notNull(key, KEY_ERROR_MSG)\n+    notNull(value, VAL_ERROR_MSG)\n+    store.put(key, value)\n+  }\n+\n+  def putAll(entries: java.util.List[Entry[K, V]]) {\n+    entries.foreach(entry => {\n+      notNull(entry.getKey, KEY_ERROR_MSG)\n+      notNull(entry.getValue, VAL_ERROR_MSG)\n+    })\n+    store.putAll(entries)\n+  }\n+\n+  def delete(key: K) {\n+    notNull(key, KEY_ERROR_MSG)\n+    store.delete(key)\n+  }\n+\n+  def range(from: K, to: K): KeyValueIterator[K, V] = {\n+    notNull(from, KEY_ERROR_MSG)\n+    notNull(to, KEY_ERROR_MSG)\n+    store.range(from, to)\n+  }\n+\n+  def all(): KeyValueIterator[K, V] = {\n+    store.all\n+  }\n+\n+  def flush {\n+    store.flush\n+  }\n+\n+  def close {\n+    store.close\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/samza/raw/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/main/scala/org/apache/samza/storage/kv/NullSafeKeyValueStore.scala",
                "sha": "00f9af319c780d9513b735016b1c3e016714f5c8",
                "status": "added"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/samza/blob/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala?ref=04d00f5a4291f54b2e64d555d7ef7f9641011c19",
                "deletions": 46,
                "filename": "samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala",
                "patch": "@@ -33,57 +33,42 @@ class SerializedKeyValueStore[K, V](\n   metrics: SerializedKeyValueStoreMetrics = new SerializedKeyValueStoreMetrics) extends KeyValueStore[K, V] with Logging {\n \n   def get(key: K): V = {\n-    val keyBytes = bytesNotNull(key, keySerde)\n+    val keyBytes = toBytesOrNull(key, keySerde)\n     val found = store.get(keyBytes)\n     metrics.gets.inc\n-    metrics.bytesSerialized.inc(keyBytes.size)\n-    if (found == null) {\n-      null.asInstanceOf[V]\n-    } else {\n-      metrics.bytesDeserialized.inc(found.size)\n-      msgSerde.fromBytes(found).asInstanceOf[V]\n-    }\n+    fromBytesOrNull(found, msgSerde)\n   }\n \n   def put(key: K, value: V) {\n     metrics.puts.inc\n-    val keyBytes = bytesNotNull(key, keySerde)\n-    val valBytes = bytesNotNull(value, msgSerde)\n-    metrics.bytesSerialized.inc(keyBytes.size + valBytes.size)\n+    val keyBytes = toBytesOrNull(key, keySerde)\n+    val valBytes = toBytesOrNull(value, msgSerde)\n     store.put(keyBytes, valBytes)\n   }\n \n   def putAll(entries: java.util.List[Entry[K, V]]) {\n     val list = new java.util.ArrayList[Entry[Array[Byte], Array[Byte]]](entries.size())\n     val iter = entries.iterator\n-    var bytesSerialized = 0L\n     while (iter.hasNext) {\n       val curr = iter.next\n-      val keyBytes = bytesNotNull(curr.getKey, keySerde)\n-      val valBytes = bytesNotNull(curr.getValue, msgSerde)\n-      bytesSerialized += keyBytes.size\n-      if (valBytes != null) {\n-        bytesSerialized += valBytes.size\n-      }\n+      val keyBytes = toBytesOrNull(curr.getKey, keySerde)\n+      val valBytes = toBytesOrNull(curr.getValue, msgSerde)\n       list.add(new Entry(keyBytes, valBytes))\n     }\n     store.putAll(list)\n     metrics.puts.inc(list.size)\n-    metrics.bytesSerialized.inc(bytesSerialized)\n   }\n \n   def delete(key: K) {\n     metrics.deletes.inc\n-    val keyBytes = bytesNotNull(key, keySerde)\n-    metrics.bytesSerialized.inc(keyBytes.size)\n+    val keyBytes = toBytesOrNull(key, keySerde)\n     store.delete(keyBytes)\n   }\n \n   def range(from: K, to: K): KeyValueIterator[K, V] = {\n     metrics.ranges.inc\n-    val fromBytes = bytesNotNull(from, keySerde)\n-    val toBytes = bytesNotNull(to, keySerde)\n-    metrics.bytesSerialized.inc(fromBytes.size + toBytes.size)\n+    val fromBytes = toBytesOrNull(from, keySerde)\n+    val toBytes = toBytesOrNull(to, keySerde)\n     new DeserializingIterator(store.range(fromBytes, toBytes))\n   }\n \n@@ -98,21 +83,8 @@ class SerializedKeyValueStore[K, V](\n     def close() = iter.close()\n     def next(): Entry[K, V] = {\n       val nxt = iter.next()\n-      val keyBytes = nxt.getKey\n-      val valBytes = nxt.getValue\n-      val key = if (keyBytes != null) {\n-        metrics.bytesDeserialized.inc(keyBytes.size)\n-        keySerde.fromBytes(keyBytes).asInstanceOf[K]\n-      } else {\n-        warn(\"Got a null key while iterating over a store. This is highly unexpected, since null in key and value is disallowed for key value stores.\")\n-        null.asInstanceOf[K]\n-      }\n-      val value = if (valBytes != null) {\n-        metrics.bytesDeserialized.inc(valBytes.size)\n-        msgSerde.fromBytes(valBytes)\n-      } else {\n-        null.asInstanceOf[V]\n-      }\n+      val key = fromBytesOrNull(nxt.getKey, keySerde)\n+      val value = fromBytesOrNull(nxt.getValue, msgSerde)\n       new Entry(key, value)\n     }\n   }\n@@ -131,13 +103,19 @@ class SerializedKeyValueStore[K, V](\n     store.close\n   }\n \n-  /**\n-   * Null is not allowed for keys and values because some change log systems\n-   * (Kafka) model deletes as null.\n-   */\n-  private def bytesNotNull[T](t: T, serde: Serde[T]): Array[Byte] = if (t != null) {\n-    serde.toBytes(t)\n+  def toBytesOrNull[T](t: T, serde: Serde[T]): Array[Byte] = if (t == null) {\n+    null\n+  } else {\n+    val bytes = serde.toBytes(t)\n+    metrics.bytesSerialized.inc(bytes.size)\n+    bytes\n+  }\n+\n+  def fromBytesOrNull[T](bytes: Array[Byte], serde: Serde[T]): T = if (bytes == null) {\n+    null.asInstanceOf[T]\n   } else {\n-    throw new NullPointerException(\"Null is not a valid key or value.\")\n+    val obj = serde.fromBytes(bytes)\n+    metrics.bytesDeserialized.inc(bytes.size)\n+    obj\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala",
                "sha": "2d3b6e55c20172aa5f149c6d6728cab8229312c3",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/samza/blob/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala",
                "changes": 62,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala?ref=04d00f5a4291f54b2e64d555d7ef7f9641011c19",
                "deletions": 20,
                "filename": "samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala",
                "patch": "@@ -34,24 +34,42 @@ import org.junit.runners.Parameterized\n import org.junit.runners.Parameterized.Parameters\n import org.apache.samza.serializers.StringSerde\n import org.apache.samza.util.TestUtil._\n+import org.apache.samza.serializers.Serde\n \n @RunWith(value = classOf[Parameterized])\n-class TestKeyValueStores(cache: Boolean) {\n-\n+class TestKeyValueStores(typeOfStore: String) {\n   import TestKeyValueStores._\n \n   val letters = \"abcdefghijklmnopqrstuvwxyz\".map(_.toString)\n   val dir = new File(System.getProperty(\"java.io.tmpdir\"), \"leveldb-test-\" + new Random().nextInt(Int.MaxValue))\n   var store: KeyValueStore[Array[Byte], Array[Byte]] = null\n+  var cache = false\n+  var serde = false\n \n   @Before\n   def setup() {\n     dir.mkdirs()\n     val leveldb = new LevelDbKeyValueStore(dir, new Options)\n-    if (cache)\n-      store = new CachedStore(leveldb, CacheSize, BatchSize)\n-    else\n-      store = leveldb\n+    val passThroughSerde = new Serde[Array[Byte]] {\n+      def toBytes(obj: Array[Byte]) = obj\n+      def fromBytes(bytes: Array[Byte]) = bytes\n+    }\n+    store = if (\"cache\".equals(typeOfStore)) {\n+      cache = true\n+      new CachedStore(leveldb, CacheSize, BatchSize)\n+    } else if (\"serde\".equals(typeOfStore)) {\n+      serde = true\n+      new SerializedKeyValueStore(leveldb, passThroughSerde, passThroughSerde)\n+    } else if (\"cache-and-serde\".equals(typeOfStore)) {\n+      val serializedStore = new SerializedKeyValueStore(leveldb, passThroughSerde, passThroughSerde)\n+      serde = true\n+      cache = true\n+      new CachedStore(serializedStore, CacheSize, BatchSize)\n+    } else {\n+      leveldb\n+    }\n+\n+    store = new NullSafeKeyValueStore(store)\n   }\n \n   @After\n@@ -82,19 +100,21 @@ class TestKeyValueStores(cache: Boolean) {\n   }\n \n   @Test\n-  def testNulls() {\n-    val stringSerde = new StringSerde(\"UTF-8\")\n-    val serializedStore = new SerializedKeyValueStore(store, stringSerde, stringSerde)\n-    val expectedNPEMessage = Some(\"Null is not a valid key or value.\")\n-\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.get(null) }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.delete(null) }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.put(null, \"\") }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.put(\"\", null) }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.putAll(List(new Entry(\"\", \"\"), new Entry[String, String](\"\", null))) }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.putAll(List(new Entry[String, String](null, \"\"))) }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.range(\"\", null) }\n-    expect(classOf[NullPointerException], expectedNPEMessage) { serializedStore.range(null, \"\") }\n+  def testNullsWithSerde() {\n+    if (serde) {\n+      val a = b(\"a\")\n+      val keyMsg = Some(NullSafeKeyValueStore.KEY_ERROR_MSG)\n+      val valMsg = Some(NullSafeKeyValueStore.VAL_ERROR_MSG)\n+\n+      expect(classOf[NullPointerException], keyMsg) { store.get(null) }\n+      expect(classOf[NullPointerException], keyMsg) { store.delete(null) }\n+      expect(classOf[NullPointerException], keyMsg) { store.put(null, a) }\n+      expect(classOf[NullPointerException], valMsg) { store.put(a, null) }\n+      expect(classOf[NullPointerException], valMsg) { store.putAll(List(new Entry(a, a), new Entry[Array[Byte], Array[Byte]](a, null))) }\n+      expect(classOf[NullPointerException], keyMsg) { store.putAll(List(new Entry[Array[Byte], Array[Byte]](null, a))) }\n+      expect(classOf[NullPointerException], keyMsg) { store.range(a, null) }\n+      expect(classOf[NullPointerException], keyMsg) { store.range(null, a) }\n+    }\n   }\n \n   @Test\n@@ -137,7 +157,9 @@ class TestKeyValueStores(cache: Boolean) {\n   @Test\n   def testDelete() {\n     val a = b(\"a\")\n+    assertNull(store.get(a))\n     store.put(a, a)\n+    assertTrue(Arrays.equals(a, store.get(a)))\n     store.delete(a)\n     assertNull(store.get(a))\n   }\n@@ -291,5 +313,5 @@ object TestKeyValueStores {\n   val CacheSize = 10\n   val BatchSize = 5\n   @Parameters\n-  def parameters: java.util.Collection[Array[java.lang.Boolean]] = Arrays.asList(Array(java.lang.Boolean.TRUE), Array(java.lang.Boolean.FALSE))\n+  def parameters: java.util.Collection[Array[String]] = Arrays.asList(Array(\"cache\"), Array(\"serde\"), Array(\"cache-and-serde\"), Array(\"levledb\"))\n }",
                "raw_url": "https://github.com/apache/samza/raw/04d00f5a4291f54b2e64d555d7ef7f9641011c19/samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala",
                "sha": "e99d4598abc5eaba443f90253530aa1e67a685b1",
                "status": "modified"
            }
        ],
        "message": "SAMZA-104; Fix NPE in kv store when delete is used with cached store and serde store.",
        "parent": "https://github.com/apache/samza/commit/a268b7e0ba763d639fb01513fdc39ac5a4ef1edd",
        "repo": "samza",
        "unit_tests": [
            "TestUtil.java"
        ]
    },
    "samza_12594fb": {
        "bug_id": "samza_12594fb",
        "commit": "https://github.com/apache/samza/commit/12594fb710260b62f9a21a8be785bd8dd5dcdd01",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/12594fb710260b62f9a21a8be785bd8dd5dcdd01/samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala?ref=12594fb710260b62f9a21a8be785bd8dd5dcdd01",
                "deletions": 1,
                "filename": "samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala",
                "patch": "@@ -169,7 +169,7 @@ private[kafka] class KafkaSystemConsumer(\n       } else {\n         null\n       }\n-      val message = if (msg.message.buffer != null) {\n+      val message = if (!msg.message.isNull) {\n         deserializer.fromBytes(Utils.readBytes(msg.message.payload))\n       } else {\n         null",
                "raw_url": "https://github.com/apache/samza/raw/12594fb710260b62f9a21a8be785bd8dd5dcdd01/samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala",
                "sha": "afbd7cdc1436a0d9a83857c0f410a2977812ee77",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/samza/blob/12594fb710260b62f9a21a8be785bd8dd5dcdd01/samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngine.scala",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngine.scala?ref=12594fb710260b62f9a21a8be785bd8dd5dcdd01",
                "deletions": 1,
                "filename": "samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngine.scala",
                "patch": "@@ -91,7 +91,11 @@ class KeyValueStorageEngine[K, V](\n         batch.clear()\n       }\n \n-      metrics.restoredBytes.inc(keyBytes.size + valBytes.size)\n+      if (valBytes != null) {\n+        metrics.restoredBytes.inc(valBytes.size)\n+      }\n+\n+      metrics.restoredBytes.inc(keyBytes.size)\n       metrics.restoredMessages.inc\n       count += 1\n ",
                "raw_url": "https://github.com/apache/samza/raw/12594fb710260b62f9a21a8be785bd8dd5dcdd01/samza-kv/src/main/scala/org/apache/samza/storage/kv/KeyValueStorageEngine.scala",
                "sha": "f42ea026cd4a2ae633d71149af6dacef68dab352",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/samza/blob/12594fb710260b62f9a21a8be785bd8dd5dcdd01/samza-test/src/test/scala/org/apache/samza/test/integration/TestStatefulTask.scala",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-test/src/test/scala/org/apache/samza/test/integration/TestStatefulTask.scala?ref=12594fb710260b62f9a21a8be785bd8dd5dcdd01",
                "deletions": 12,
                "filename": "samza-test/src/test/scala/org/apache/samza/test/integration/TestStatefulTask.scala",
                "patch": "@@ -250,15 +250,19 @@ class TestStatefulTask {\n     send(task, \"2\")\n     send(task, \"3\")\n     send(task, \"2\")\n+    send(task, \"99\")\n+    send(task, \"-99\")\n \n     // Validate that messages appear in store stream.\n-    val messages = readAll(STATE_TOPIC, 3, \"testShouldStartTaskForFirstTime\")\n+    val messages = readAll(STATE_TOPIC, 5, \"testShouldStartTaskForFirstTime\")\n \n-    assertEquals(4, messages.length)\n+    assertEquals(6, messages.length)\n     assertEquals(\"1\", messages(0))\n     assertEquals(\"2\", messages(1))\n     assertEquals(\"3\", messages(2))\n     assertEquals(\"2\", messages(3))\n+    assertEquals(\"99\", messages(4))\n+    assertEquals(null, messages(5))\n \n     stopJob(job)\n   }\n@@ -292,23 +296,27 @@ class TestStatefulTask {\n     send(task, \"5\")\n \n     // Validate that messages appear in store stream.\n-    val messages = readAll(STATE_TOPIC, 10, \"testShouldRestoreStore\")\n+    val messages = readAll(STATE_TOPIC, 14, \"testShouldRestoreStore\")\n \n-    assertEquals(11, messages.length)\n+    assertEquals(15, messages.length)\n     // From initial start.\n     assertEquals(\"1\", messages(0))\n     assertEquals(\"2\", messages(1))\n     assertEquals(\"3\", messages(2))\n     assertEquals(\"2\", messages(3))\n+    assertEquals(\"99\", messages(4))\n+    assertEquals(null, messages(5))\n     // From second startup.\n-    assertEquals(\"1\", messages(4))\n-    assertEquals(\"2\", messages(5))\n-    assertEquals(\"3\", messages(6))\n+    assertEquals(\"1\", messages(6))\n     assertEquals(\"2\", messages(7))\n+    assertEquals(\"3\", messages(8))\n+    assertEquals(\"2\", messages(9))\n+    assertEquals(\"99\", messages(10))\n+    assertEquals(null, messages(11))\n     // From sending in this method.\n-    assertEquals(\"4\", messages(8))\n-    assertEquals(\"5\", messages(9))\n-    assertEquals(\"5\", messages(10))\n+    assertEquals(\"4\", messages(12))\n+    assertEquals(\"5\", messages(13))\n+    assertEquals(\"5\", messages(14))\n \n     stopJob(job)\n   }\n@@ -376,7 +384,11 @@ class TestStatefulTask {\n \n     while (message == null || message.offset < maxOffsetInclusive) {\n       message = stream.next\n-      messages += new String(message.message, \"UTF-8\")\n+      if (message.message == null) {\n+        messages += null\n+      } else {\n+        messages += new String(message.message, \"UTF-8\")\n+      }\n       System.err.println(\"TestStatefulTask.readAll(): offset=%s, message=%s\" format (message.offset, messages.last))\n     }\n \n@@ -436,7 +448,14 @@ class TestTask extends StreamTask with InitableTask {\n     System.err.println(\"TestTask.process(): %s\" format msg)\n \n     received += msg\n-    store.put(msg, msg)\n+\n+    // A negative string means delete\n+    if (msg.startsWith(\"-\")) {\n+      store.delete(msg.substring(1))\n+    } else {\n+      store.put(msg, msg)\n+    }\n+\n     coordinator.commit\n \n     // Notify sender that we got a message.",
                "raw_url": "https://github.com/apache/samza/raw/12594fb710260b62f9a21a8be785bd8dd5dcdd01/samza-test/src/test/scala/org/apache/samza/test/integration/TestStatefulTask.scala",
                "sha": "7e813870ef07cee889d3be526d62f4e7574ce100",
                "status": "modified"
            }
        ],
        "message": "SAMZA-173; fix NPE when changelog restore includes a message with a null value",
        "parent": "https://github.com/apache/samza/commit/55929095f5abc0911278ca8336bccbc835122c84",
        "repo": "samza",
        "unit_tests": [
            "TestKafkaSystemConsumer.java"
        ]
    },
    "samza_210631c": {
        "bug_id": "samza_210631c",
        "commit": "https://github.com/apache/samza/commit/210631cd5c4487e99978189ee9b8d0b0c8847aba",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/BaseReadableTable.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/table/BaseReadableTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 0,
                "filename": "samza-core/src/main/java/org/apache/samza/table/BaseReadableTable.java",
                "patch": "@@ -19,9 +19,11 @@\n package org.apache.samza.table;\n \n import com.google.common.base.Preconditions;\n+import org.apache.samza.config.MetricsConfig;\n import org.apache.samza.context.Context;\n import org.apache.samza.table.utils.TableReadMetrics;\n import org.apache.samza.table.utils.TableWriteMetrics;\n+import org.apache.samza.util.HighResolutionClock;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -41,6 +43,8 @@\n   protected TableReadMetrics readMetrics;\n   protected TableWriteMetrics writeMetrics;\n \n+  protected HighResolutionClock clock;\n+\n   /**\n    * Construct an instance\n    * @param tableId Id of the table\n@@ -54,6 +58,11 @@ public BaseReadableTable(String tableId) {\n \n   @Override\n   public void init(Context context) {\n+    MetricsConfig metricsConfig = new MetricsConfig(context.getJobContext().getConfig());\n+    clock = metricsConfig.getMetricsTimerEnabled()\n+        ? () -> System.nanoTime()\n+        : () -> 0L;\n+\n     readMetrics = new TableReadMetrics(context, this, tableId);\n     if (this instanceof ReadWriteTable) {\n       writeMetrics = new TableWriteMetrics(context, this, tableId);",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/BaseReadableTable.java",
                "sha": "1dfd54cc23eea9acd68a4e151cd89e23103c491a",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/caching/CachingTable.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/table/caching/CachingTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 14,
                "filename": "samza-core/src/main/java/org/apache/samza/table/caching/CachingTable.java",
                "patch": "@@ -130,7 +130,7 @@ public V get(K key) {\n       return CompletableFuture.completedFuture(value);\n     }\n \n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     missCount.incrementAndGet();\n \n     return rdTable.getAsync(key).handle((result, e) -> {\n@@ -140,7 +140,7 @@ public V get(K key) {\n           if (result != null) {\n             cache.put(key, result);\n           }\n-          updateTimer(readMetrics.getNs, System.nanoTime() - startNs);\n+          updateTimer(readMetrics.getNs, clock.nanoTime() - startNs);\n           return result;\n         }\n       });\n@@ -168,7 +168,7 @@ public V get(K key) {\n       return CompletableFuture.completedFuture(getAllResult);\n     }\n \n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     return rdTable.getAllAsync(missingKeys).handle((records, e) -> {\n         if (e != null) {\n           throw new SamzaException(\"Failed to get records for \" + keys, e);\n@@ -179,7 +179,7 @@ public V get(K key) {\n                 .collect(Collectors.toList()));\n             getAllResult.putAll(records);\n           }\n-          updateTimer(readMetrics.getAllNs, System.nanoTime() - startNs);\n+          updateTimer(readMetrics.getAllNs, clock.nanoTime() - startNs);\n           return getAllResult;\n         }\n       });\n@@ -201,7 +201,7 @@ public void put(K key, V value) {\n     incCounter(writeMetrics.numPuts);\n     Preconditions.checkNotNull(rwTable, \"Cannot write to a read-only table: \" + rdTable);\n \n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     return rwTable.putAsync(key, value).handle((result, e) -> {\n         if (e != null) {\n           throw new SamzaException(String.format(\"Failed to put a record, key=%s, value=%s\", key, value), e);\n@@ -212,7 +212,7 @@ public void put(K key, V value) {\n             cache.put(key, value);\n           }\n         }\n-        updateTimer(writeMetrics.putNs, System.nanoTime() - startNs);\n+        updateTimer(writeMetrics.putNs, clock.nanoTime() - startNs);\n         return result;\n       });\n   }\n@@ -231,7 +231,7 @@ public void putAll(List<Entry<K, V>> records) {\n   @Override\n   public CompletableFuture<Void> putAllAsync(List<Entry<K, V>> records) {\n     incCounter(writeMetrics.numPutAlls);\n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     Preconditions.checkNotNull(rwTable, \"Cannot write to a read-only table: \" + rdTable);\n     return rwTable.putAllAsync(records).handle((result, e) -> {\n         if (e != null) {\n@@ -240,7 +240,7 @@ public void putAll(List<Entry<K, V>> records) {\n           cache.putAll(records);\n         }\n \n-        updateTimer(writeMetrics.putAllNs, System.nanoTime() - startNs);\n+        updateTimer(writeMetrics.putAllNs, clock.nanoTime() - startNs);\n         return result;\n       });\n   }\n@@ -259,15 +259,15 @@ public void delete(K key) {\n   @Override\n   public CompletableFuture<Void> deleteAsync(K key) {\n     incCounter(writeMetrics.numDeletes);\n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     Preconditions.checkNotNull(rwTable, \"Cannot delete from a read-only table: \" + rdTable);\n     return rwTable.deleteAsync(key).handle((result, e) -> {\n         if (e != null) {\n           throw new SamzaException(\"Failed to delete the record for \" + key, e);\n         } else if (!isWriteAround) {\n           cache.delete(key);\n         }\n-        updateTimer(writeMetrics.deleteNs, System.nanoTime() - startNs);\n+        updateTimer(writeMetrics.deleteNs, clock.nanoTime() - startNs);\n         return result;\n       });\n   }\n@@ -284,26 +284,26 @@ public void deleteAll(List<K> keys) {\n   @Override\n   public CompletableFuture<Void> deleteAllAsync(List<K> keys) {\n     incCounter(writeMetrics.numDeleteAlls);\n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     Preconditions.checkNotNull(rwTable, \"Cannot delete from a read-only table: \" + rdTable);\n     return rwTable.deleteAllAsync(keys).handle((result, e) -> {\n         if (e != null) {\n           throw new SamzaException(\"Failed to delete the record for \" + keys, e);\n         } else if (!isWriteAround) {\n           cache.deleteAll(keys);\n         }\n-        updateTimer(writeMetrics.deleteAllNs, System.nanoTime() - startNs);\n+        updateTimer(writeMetrics.deleteAllNs, clock.nanoTime() - startNs);\n         return result;\n       });\n   }\n \n   @Override\n   public synchronized void flush() {\n     incCounter(writeMetrics.numFlushes);\n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     Preconditions.checkNotNull(rwTable, \"Cannot flush a read-only table: \" + rdTable);\n     rwTable.flush();\n-    updateTimer(writeMetrics.flushNs, System.nanoTime() - startNs);\n+    updateTimer(writeMetrics.flushNs, clock.nanoTime() - startNs);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/caching/CachingTable.java",
                "sha": "e63bf614cd02179bbf32ae66cf85049303187f08",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadWriteTable.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadWriteTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 4,
                "filename": "samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadWriteTable.java",
                "patch": "@@ -174,9 +174,9 @@ public void deleteAll(List<K> keys) {\n   public void flush() {\n     try {\n       incCounter(writeMetrics.numFlushes);\n-      long startNs = System.nanoTime();\n+      long startNs = clock.nanoTime();\n       writeFn.flush();\n-      updateTimer(writeMetrics.flushNs, System.nanoTime() - startNs);\n+      updateTimer(writeMetrics.flushNs, clock.nanoTime() - startNs);\n     } catch (Exception e) {\n       String errMsg = \"Failed to flush remote store\";\n       logger.error(errMsg, e);\n@@ -202,7 +202,7 @@ public void close() {\n   protected CompletableFuture<Void> execute(TableRateLimiter<K, V> rateLimiter,\n       K key, V value, BiFunction<K, V, CompletableFuture<Void>> method, Counter counter, Timer timer) {\n     incCounter(counter);\n-    final long startNs = System.nanoTime();\n+    final long startNs = clock.nanoTime();\n     CompletableFuture<Void> ioFuture = rateLimiter.isRateLimited()\n         ? CompletableFuture\n             .runAsync(() -> rateLimiter.throttle(key, value), tableExecutor)\n@@ -223,7 +223,7 @@ public void close() {\n       Collection<Entry<K, V>> records, Function<Collection<Entry<K, V>>, CompletableFuture<Void>> method,\n       Counter counter, Timer timer) {\n     incCounter(counter);\n-    final long startNs = System.nanoTime();\n+    final long startNs = clock.nanoTime();\n     CompletableFuture<Void> ioFuture = rateLimiter.isRateLimited()\n         ? CompletableFuture\n             .runAsync(() -> rateLimiter.throttleRecords(records), tableExecutor)",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadWriteTable.java",
                "sha": "80c2cacec0fbf9c29f8c3f6658c9fced1a4e1bd8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadableTable.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadableTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 4,
                "filename": "samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadableTable.java",
                "patch": "@@ -166,7 +166,7 @@ public V get(K key) {\n   protected <T> CompletableFuture<T> execute(TableRateLimiter<K, V> rateLimiter,\n       K key, Function<K, CompletableFuture<T>> method, Counter counter, Timer timer) {\n     incCounter(counter);\n-    final long startNs = System.nanoTime();\n+    final long startNs = clock.nanoTime();\n     CompletableFuture<T> ioFuture = rateLimiter.isRateLimited()\n         ? CompletableFuture\n             .runAsync(() -> rateLimiter.throttle(key), tableExecutor)\n@@ -187,7 +187,7 @@ public V get(K key) {\n   protected <T> CompletableFuture<T> execute(TableRateLimiter<K, V> rateLimiter,\n       Collection<K> keys, Function<Collection<K>, CompletableFuture<T>> method, Counter counter, Timer timer) {\n     incCounter(counter);\n-    final long startNs = System.nanoTime();\n+    final long startNs = clock.nanoTime();\n     CompletableFuture<T> ioFuture = rateLimiter.isRateLimited()\n         ? CompletableFuture\n             .runAsync(() -> rateLimiter.throttle(keys), tableExecutor)\n@@ -207,12 +207,12 @@ public V get(K key) {\n   protected  <T> CompletableFuture<T> completeExecution(CompletableFuture<T> ioFuture, long startNs, Timer timer) {\n     if (callbackExecutor != null) {\n       ioFuture.thenApplyAsync(r -> {\n-          updateTimer(timer, System.nanoTime() - startNs);\n+          updateTimer(timer, clock.nanoTime() - startNs);\n           return r;\n         }, callbackExecutor);\n     } else {\n       ioFuture.thenApply(r -> {\n-          updateTimer(timer, System.nanoTime() - startNs);\n+          updateTimer(timer, clock.nanoTime() - startNs);\n           return r;\n         });\n     }",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/remote/RemoteReadableTable.java",
                "sha": "84a05b8b4ff30cce29b34400104150a6bf63eb74",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/utils/TableReadMetrics.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/table/utils/TableReadMetrics.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 13,
                "filename": "samza-core/src/main/java/org/apache/samza/table/utils/TableReadMetrics.java",
                "patch": "@@ -18,7 +18,6 @@\n  */\n package org.apache.samza.table.utils;\n \n-import org.apache.samza.config.MetricsConfig;\n import org.apache.samza.context.Context;\n import org.apache.samza.metrics.Counter;\n import org.apache.samza.metrics.Timer;\n@@ -34,7 +33,6 @@\n   public final Timer getAllNs;\n   public final Counter numGets;\n   public final Counter numGetAlls;\n-  public final Timer getCallbackNs;\n   public final Counter numMissedLookups;\n \n   /**\n@@ -47,19 +45,10 @@\n   public TableReadMetrics(Context context, Table table, String tableId) {\n     TableMetricsUtil tableMetricsUtil = new TableMetricsUtil(context, table, tableId);\n     numGets = tableMetricsUtil.newCounter(\"num-gets\");\n+    getNs = tableMetricsUtil.newTimer(\"get-ns\");\n     numGetAlls = tableMetricsUtil.newCounter(\"num-getAlls\");\n+    getAllNs = tableMetricsUtil.newTimer(\"getAll-ns\");\n     numMissedLookups = tableMetricsUtil.newCounter(\"num-missed-lookups\");\n-\n-    MetricsConfig metricsConfig = new MetricsConfig(context.getJobContext().getConfig());\n-    if (metricsConfig.getMetricsTimerEnabled()) {\n-      getNs = tableMetricsUtil.newTimer(\"get-ns\");\n-      getAllNs = tableMetricsUtil.newTimer(\"getAll-ns\");\n-      getCallbackNs = tableMetricsUtil.newTimer(\"get-callback-ns\");\n-    } else {\n-      getNs = null;\n-      getAllNs = null;\n-      getCallbackNs = null;\n-    }\n   }\n \n }",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/utils/TableReadMetrics.java",
                "sha": "e77fcfd5e73ecec9c02b1725160744b0a30a16cb",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/utils/TableWriteMetrics.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/table/utils/TableWriteMetrics.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 27,
                "filename": "samza-core/src/main/java/org/apache/samza/table/utils/TableWriteMetrics.java",
                "patch": "@@ -18,7 +18,6 @@\n  */\n package org.apache.samza.table.utils;\n \n-import org.apache.samza.config.MetricsConfig;\n import org.apache.samza.context.Context;\n import org.apache.samza.metrics.Counter;\n import org.apache.samza.metrics.Timer;\n@@ -27,18 +26,16 @@\n \n public class TableWriteMetrics {\n \n-  public final Timer putNs;\n-  public final Timer putAllNs;\n-  public final Timer deleteNs;\n-  public final Timer deleteAllNs;\n-  public final Timer flushNs;\n   public final Counter numPuts;\n+  public final Timer putNs;\n   public final Counter numPutAlls;\n+  public final Timer putAllNs;\n   public final Counter numDeletes;\n+  public final Timer deleteNs;\n   public final Counter numDeleteAlls;\n+  public final Timer deleteAllNs;\n   public final Counter numFlushes;\n-  public final Timer putCallbackNs;\n-  public final Timer deleteCallbackNs;\n+  public final Timer flushNs;\n \n   /**\n    * Utility class that contains the default set of write metrics.\n@@ -50,28 +47,14 @@\n   public TableWriteMetrics(Context context, Table table, String tableId) {\n     TableMetricsUtil tableMetricsUtil = new TableMetricsUtil(context, table, tableId);\n     numPuts = tableMetricsUtil.newCounter(\"num-puts\");\n+    putNs = tableMetricsUtil.newTimer(\"put-ns\");\n     numPutAlls = tableMetricsUtil.newCounter(\"num-putAlls\");\n+    putAllNs = tableMetricsUtil.newTimer(\"putAll-ns\");\n     numDeletes = tableMetricsUtil.newCounter(\"num-deletes\");\n+    deleteNs = tableMetricsUtil.newTimer(\"delete-ns\");\n     numDeleteAlls = tableMetricsUtil.newCounter(\"num-deleteAlls\");\n+    deleteAllNs = tableMetricsUtil.newTimer(\"deleteAll-ns\");\n     numFlushes = tableMetricsUtil.newCounter(\"num-flushes\");\n-\n-    MetricsConfig metricsConfig = new MetricsConfig(context.getJobContext().getConfig());\n-    if (metricsConfig.getMetricsTimerEnabled()) {\n-      putNs = tableMetricsUtil.newTimer(\"put-ns\");\n-      putAllNs = tableMetricsUtil.newTimer(\"putAll-ns\");\n-      deleteNs = tableMetricsUtil.newTimer(\"delete-ns\");\n-      deleteAllNs = tableMetricsUtil.newTimer(\"deleteAll-ns\");\n-      flushNs = tableMetricsUtil.newTimer(\"flush-ns\");\n-      putCallbackNs = tableMetricsUtil.newTimer(\"put-callback-ns\");\n-      deleteCallbackNs = tableMetricsUtil.newTimer(\"delete-callback-ns\");\n-    } else {\n-      putNs = null;\n-      putAllNs = null;\n-      deleteNs = null;\n-      deleteAllNs = null;\n-      flushNs = null;\n-      putCallbackNs = null;\n-      deleteCallbackNs = null;\n-    }\n+    flushNs = tableMetricsUtil.newTimer(\"flush-ns\");\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/main/java/org/apache/samza/table/utils/TableWriteMetrics.java",
                "sha": "bf65b74037a30775070359e16b338e0576f3414b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/test/java/org/apache/samza/table/caching/TestCachingTable.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/table/caching/TestCachingTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 9,
                "filename": "samza-core/src/test/java/org/apache/samza/table/caching/TestCachingTable.java",
                "patch": "@@ -277,14 +277,14 @@ public void testKeyEviction() {\n   public void testGuavaCacheAndRemoteTable() throws Exception {\n     String tableId = \"testGuavaCacheAndRemoteTable\";\n     Cache<String, String> guavaCache = CacheBuilder.newBuilder().initialCapacity(100).build();\n-    final ReadWriteTable<String, String> guavaTable = new GuavaCacheTable<>(tableId, guavaCache);\n+    final ReadWriteTable<String, String> guavaTable = new GuavaCacheTable<>(tableId + \"-cache\", guavaCache);\n \n     // It is okay to share rateLimitHelper and async helper for read/write in test\n     TableRateLimiter<String, String> rateLimitHelper = mock(TableRateLimiter.class);\n     TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\n     TableWriteFunction<String, String> writeFn = mock(TableWriteFunction.class);\n     final RemoteReadWriteTable<String, String> remoteTable = new RemoteReadWriteTable<>(\n-        tableId, readFn, writeFn, rateLimitHelper, rateLimitHelper,\n+        tableId + \"-remote\", readFn, writeFn, rateLimitHelper, rateLimitHelper,\n         Executors.newSingleThreadExecutor(), Executors.newSingleThreadExecutor());\n \n     final CachingTable<String, String> cachingTable = new CachingTable<>(\n@@ -296,11 +296,11 @@ public void testGuavaCacheAndRemoteTable() throws Exception {\n     // 5 per read/write table (15)\n     verify(metricsRegistry, times(24)).newCounter(any(), anyString());\n \n-    // 3 per readable table (9)\n-    // 7 per read/write table (21)\n+    // 2 per readable table (6)\n+    // 5 per read/write table (15)\n     // 1 per remote readable table (1)\n     // 1 per remote read/write table (1)\n-    verify(metricsRegistry, times(32)).newTimer(any(), anyString());\n+    verify(metricsRegistry, times(23)).newTimer(any(), anyString());\n \n     // 1 per guava table (1)\n     // 3 per caching table (2)\n@@ -424,10 +424,6 @@ public void testTimerDisabled() throws Exception {\n     cachingTable.deleteAsync(\"\");\n     cachingTable.deleteAll(Collections.emptyList());\n     cachingTable.deleteAllAsync(Collections.emptyList());\n-\n-    verify(metricsRegistry, atLeast(1)).newCounter(any(), anyString());\n-    verify(metricsRegistry, atLeast(1)).newGauge(anyString(), any());\n-    verify(metricsRegistry, times(0)).newTimer(any(), anyString());\n   }\n \n   private TableDescriptor createDummyTableDescriptor(String tableId) {",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-core/src/test/java/org/apache/samza/table/caching/TestCachingTable.java",
                "sha": "5a197670a431cee94b86d9742ff2a2b52a47ffd6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadWriteTable.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadWriteTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 2,
                "filename": "samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadWriteTable.java",
                "patch": "@@ -146,9 +146,9 @@ public void flush() {\n \n   private void instrument(Counter counter, Timer timer, Func0 func) {\n     incCounter(counter);\n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     func.apply();\n-    updateTimer(timer, System.nanoTime() - startNs);\n+    updateTimer(timer, clock.nanoTime() - startNs);\n   }\n \n }",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadWriteTable.java",
                "sha": "eae6bb0cf4ba33fefb51fda50343608b6e2a086a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadableTable.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadableTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 2,
                "filename": "samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadableTable.java",
                "patch": "@@ -100,9 +100,9 @@ public void close() {\n \n   private <T> T instrument(Counter counter, Timer timer, Supplier<T> func) {\n     incCounter(counter);\n-    long startNs = System.nanoTime();\n+    long startNs = clock.nanoTime();\n     T result = func.get();\n-    updateTimer(timer, System.nanoTime() - startNs);\n+    updateTimer(timer, clock.nanoTime() - startNs);\n     return result;\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/main/java/org/apache/samza/storage/kv/LocalReadableTable.java",
                "sha": "29ddb15521503e0956971ccebeb65cc10adb35e6",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadWriteTable.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadWriteTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 3,
                "filename": "samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadWriteTable.java",
                "patch": "@@ -212,9 +212,6 @@ public void testTimerDisabled() throws Exception {\n     table.deleteAll(Collections.emptyList());\n     table.deleteAllAsync(Collections.emptyList()).get();\n     table.flush();\n-    verify(metricsRegistry, atLeast(1)).newCounter(anyString(), anyString());\n-    verify(metricsRegistry, times(0)).newTimer(anyString(), anyString());\n-    verify(metricsRegistry, times(0)).newGauge(anyString(), any());\n     Assert.assertEquals(1, numFlushes.getCount());\n     Assert.assertEquals(2, numPuts.getCount());\n     Assert.assertEquals(0, numPutAlls.getCount());",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadWriteTable.java",
                "sha": "044fab48da3ea216d2808e820dcd880635148c03",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/samza/blob/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadableTable.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadableTable.java?ref=210631cd5c4487e99978189ee9b8d0b0c8847aba",
                "deletions": 3,
                "filename": "samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadableTable.java",
                "patch": "@@ -126,9 +126,6 @@ public void testTimerDisabled() throws Exception {\n     table.getAsync(\"\").get();\n     table.getAll(keys);\n     table.getAllAsync(keys).get();\n-    verify(metricsRegistry, atLeast(1)).newCounter(anyString(), anyString());\n-    verify(metricsRegistry, times(0)).newTimer(anyString(), anyString());\n-    verify(metricsRegistry, times(0)).newGauge(anyString(), any());\n     Assert.assertEquals(2, numGets.getCount());\n     Assert.assertEquals(4, numMissedLookups.getCount());\n     Assert.assertEquals(2, numGetAlls.getCount());",
                "raw_url": "https://github.com/apache/samza/raw/210631cd5c4487e99978189ee9b8d0b0c8847aba/samza-kv/src/test/java/org/apache/samza/storage/kv/TestLocalReadableTable.java",
                "sha": "e1c82d92f1f43b13c30a456819023b230d765889",
                "status": "modified"
            }
        ],
        "message": "SAMZA-2015: Refactor timer handling in tables to be consistent with stores\n\nCurrently when timer is disabled, we do not instantiate timer instances for tables, this introduced potential opportunities for NPE in the future. We wanted to refactor to use the same approach used in store implementation based on HighResolutionClock.\n\nAuthor: Wei Song <wsong@linkedin.com>\n\nReviewers: Xinyu Liu <xiliu@linkedin.com>\n\nCloses #835 from weisong44/SAMZA-2015",
        "parent": "https://github.com/apache/samza/commit/bdce47707a32f91032574f1d6d05b6e659a4aa33",
        "repo": "samza",
        "unit_tests": [
            "TestCachingTable.java"
        ]
    },
    "samza_2cd1827": {
        "bug_id": "samza_2cd1827",
        "commit": "https://github.com/apache/samza/commit/2cd18279c61c92597ad6d2892b1f01b5106ffcb1",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/samza/blob/2cd18279c61c92597ad6d2892b1f01b5106ffcb1/samza-core/src/main/scala/org/apache/samza/serializers/IntegerSerde.scala",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/scala/org/apache/samza/serializers/IntegerSerde.scala?ref=2cd18279c61c92597ad6d2892b1f01b5106ffcb1",
                "deletions": 5,
                "filename": "samza-core/src/main/scala/org/apache/samza/serializers/IntegerSerde.scala",
                "patch": "@@ -25,19 +25,19 @@ import org.apache.samza.config.Config\n /**\n  * A serializer for integers\n  */\n-class IntegerSerdeFactory extends SerdeFactory[Integer] {\n-  def getSerde(name: String, config: Config): Serde[Integer] = new IntegerSerde\n+class IntegerSerdeFactory extends SerdeFactory[java.lang.Integer] {\n+  def getSerde(name: String, config: Config): Serde[java.lang.Integer] = new IntegerSerde\n }\n \n-class IntegerSerde extends Serde[Integer] {\n-  def toBytes(obj: Integer): Array[Byte] = if (obj != null) {\n+class IntegerSerde extends Serde[java.lang.Integer] {\n+  def toBytes(obj: java.lang.Integer): Array[Byte] = if (obj != null) {\n     ByteBuffer.allocate(4).putInt(obj.intValue).array\n   } else {\n     null\n   }\n \n   // big-endian by default\n-  def fromBytes(bytes: Array[Byte]): Integer = if (bytes != null) {\n+  def fromBytes(bytes: Array[Byte]): java.lang.Integer = if (bytes != null) {\n     ByteBuffer.wrap(bytes).getInt\n   } else {\n     null",
                "raw_url": "https://github.com/apache/samza/raw/2cd18279c61c92597ad6d2892b1f01b5106ffcb1/samza-core/src/main/scala/org/apache/samza/serializers/IntegerSerde.scala",
                "sha": "46509f7e25bae29829d70c8e7a59dee654da4329",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/samza/blob/2cd18279c61c92597ad6d2892b1f01b5106ffcb1/samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala?ref=2cd18279c61c92597ad6d2892b1f01b5106ffcb1",
                "deletions": 1,
                "filename": "samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala",
                "patch": "@@ -61,7 +61,10 @@ class SerializedKeyValueStore[K, V](\n       val curr = iter.next\n       val keyBytes = keySerde.toBytes(curr.getKey)\n       val valBytes = msgSerde.toBytes(curr.getValue)\n-      bytesSerialized += keyBytes.size + valBytes.size\n+      bytesSerialized += keyBytes.size\n+      if (valBytes != null) {\n+        bytesSerialized += valBytes.size\n+      }\n       list.add(new Entry(keyBytes, valBytes))\n     }\n     store.putAll(list)",
                "raw_url": "https://github.com/apache/samza/raw/2cd18279c61c92597ad6d2892b1f01b5106ffcb1/samza-kv/src/main/scala/org/apache/samza/storage/kv/SerializedKeyValueStore.scala",
                "sha": "53a5cbeddcee61c05beb9c858eb81d11a1cd5d93",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/samza/blob/2cd18279c61c92597ad6d2892b1f01b5106ffcb1/samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala?ref=2cd18279c61c92597ad6d2892b1f01b5106ffcb1",
                "deletions": 0,
                "filename": "samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala",
                "patch": "@@ -25,6 +25,7 @@ import java.util.Random\n \n import scala.collection.JavaConversions._\n \n+import org.apache.samza.serializers.IntegerSerde\n import org.iq80.leveldb.Options\n import org.junit.After\n import org.junit.Assert.assertEquals\n@@ -141,6 +142,16 @@ class TestKeyValueStores(cache: Boolean) {\n     vals.foreach(v => assertNull(store.get(v)))\n   }\n \n+  @Test\n+  def testSerializedValueIsNull {\n+    val serializedStore = new SerializedKeyValueStore(\n+      store,\n+      new IntegerSerde,\n+      new IntegerSerde)\n+\n+    serializedStore.putAll(List(new Entry[java.lang.Integer, java.lang.Integer](0, null)))\n+  }\n+\n   def checkRange(vals: IndexedSeq[String], iter: KeyValueIterator[Array[Byte], Array[Byte]]) {\n     for (v <- vals) {\n       assertTrue(iter.hasNext)",
                "raw_url": "https://github.com/apache/samza/raw/2cd18279c61c92597ad6d2892b1f01b5106ffcb1/samza-kv/src/test/scala/org/apache/samza/storage/kv/TestKeyValueStores.scala",
                "sha": "03a189e71191af364aaec21baf5876c683cbf782",
                "status": "modified"
            }
        ],
        "message": "SAMZA-57; fix npe in serialized key value store.",
        "parent": "https://github.com/apache/samza/commit/63cc71c5f750b8af615e85d46ab35871f6e777b2",
        "repo": "samza",
        "unit_tests": [
            "TestIntegerSerde.java"
        ]
    },
    "samza_2e461a8": {
        "bug_id": "samza_2e461a8",
        "commit": "https://github.com/apache/samza/commit/2e461a8804ec568ff52de2d6e81a8edd4865ce12",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/2e461a8804ec568ff52de2d6e81a8edd4865ce12/samza-yarn/src/main/java/org/apache/samza/job/yarn/YarnClusterResourceManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-yarn/src/main/java/org/apache/samza/job/yarn/YarnClusterResourceManager.java?ref=2e461a8804ec568ff52de2d6e81a8edd4865ce12",
                "deletions": 1,
                "filename": "samza-yarn/src/main/java/org/apache/samza/job/yarn/YarnClusterResourceManager.java",
                "patch": "@@ -712,7 +712,7 @@ private String getFormattedCommand(String logDirExpansionVar,\n   private String getPendingSamzaContainerId(ContainerId containerId) {\n     for (String samzaContainerId: state.pendingYarnContainers.keySet()) {\n       YarnContainer yarnContainer = state.pendingYarnContainers.get(samzaContainerId);\n-      if (yarnContainer.id().equals(containerId)) {\n+      if (yarnContainer != null && yarnContainer.id().equals(containerId)) {\n         return samzaContainerId;\n       }\n     }",
                "raw_url": "https://github.com/apache/samza/raw/2e461a8804ec568ff52de2d6e81a8edd4865ce12/samza-yarn/src/main/java/org/apache/samza/job/yarn/YarnClusterResourceManager.java",
                "sha": "79a9083987b6f3836d63f5f951369f1c94791801",
                "status": "modified"
            }
        ],
        "message": "SAMZA-1699: Fix NPE in ClusterResourceManager\n\nWhen the ClusterResourcedManager receives a notification that a container is started, it moves the container from the \"pending queue\" to its \"running queue\".\nIn the meanwhile, it's possible for another thread to remove the mapping for the key. Here's an example:\n\nNMCallbackThread-1:```\npendingYarnContainers.remove(key);```\n\nNMCallbackThread-2:\n```\nfor (String key : pendingYarnContainers.keySet()) {\n  yarnContainer = pendingYarnContainers.get(key); <-- could be null depending on whether the removal happened before it.\n}```\n\nAuthor: Jagadish <jvenkatraman@linkedin.com>\n\nReviewers: Prateek M<pmaheshw@linkedin.com>\n\nCloses #504 from vjagadish/npe-fix-async",
        "parent": "https://github.com/apache/samza/commit/51729ac6836f5baa522d7ba6a7308504ba027c5f",
        "repo": "samza",
        "unit_tests": [
            "TestYarnClusterResourceManager.java"
        ]
    },
    "samza_4aaa589": {
        "bug_id": "samza_4aaa589",
        "commit": "https://github.com/apache/samza/commit/4aaa589cdd22250719c5c30a19f4f95f4c327110",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/samza/blob/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/main/java/org/apache/samza/coordinator/stream/CoordinatorStreamSystemConsumer.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/coordinator/stream/CoordinatorStreamSystemConsumer.java?ref=4aaa589cdd22250719c5c30a19f4f95f4c327110",
                "deletions": 7,
                "filename": "samza-core/src/main/java/org/apache/samza/coordinator/stream/CoordinatorStreamSystemConsumer.java",
                "patch": "@@ -202,13 +202,11 @@ public void bootstrap() {\n     }\n   }\n \n-  public Set<CoordinatorStreamMessage> getBoostrappedStream() {\n-    log.info(\"Returning the bootstrapped data from the stream\");\n-    if (!isBootstrapped)\n-      bootstrap();\n-    return bootstrappedStreamSet;\n-  }\n-\n+  /**\n+   * Returns the set of bootstrapped {@link CoordinatorStreamMessage}s\n+   * @param type The type of {@link CoordinatorStreamMessage}s to return.\n+   * @return The bootstrapped {@link CoordinatorStreamMessage}s\n+   */\n   public Set<CoordinatorStreamMessage> getBootstrappedStream(String type) {\n     log.debug(\"Bootstrapping coordinator stream for messages of type {}\", type);\n     bootstrap();",
                "raw_url": "https://github.com/apache/samza/raw/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/main/java/org/apache/samza/coordinator/stream/CoordinatorStreamSystemConsumer.java",
                "sha": "ff287235b004ba5341d4099d8018a1b043cce448",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/samza/blob/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/main/java/org/apache/samza/coordinator/stream/messages/CoordinatorStreamMessage.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/coordinator/stream/messages/CoordinatorStreamMessage.java?ref=4aaa589cdd22250719c5c30a19f4f95f4c327110",
                "deletions": 1,
                "filename": "samza-core/src/main/java/org/apache/samza/coordinator/stream/messages/CoordinatorStreamMessage.java",
                "patch": "@@ -43,7 +43,7 @@\n  * </p>\n  *\n  * <pre>\n- * key =&gt; [1, \"set-config\", \"job.name\"] \n+ * key =&gt; [1, \"set-config\", \"job.name\"]\n  *\n  * message =&gt; {\n  *   \"host\": \"192.168.0.1\",\n@@ -312,6 +312,8 @@ public boolean equals(Object obj) {\n     if (messageMap == null) {\n       if (other.messageMap != null)\n         return false;\n+    } else if (getMessageValues() == null) {\n+      return other.getMessageValues() == null;\n     } else if (!getMessageValues().equals(other.getMessageValues()))\n       return false;\n     return true;",
                "raw_url": "https://github.com/apache/samza/raw/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/main/java/org/apache/samza/coordinator/stream/messages/CoordinatorStreamMessage.java",
                "sha": "a0e8794b90eb1cebba3e79c6890ba8421ee1e3d0",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/samza/blob/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamMessage.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamMessage.java?ref=4aaa589cdd22250719c5c30a19f4f95f4c327110",
                "deletions": 0,
                "filename": "samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamMessage.java",
                "patch": "@@ -25,6 +25,7 @@\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.assertFalse;\n \n+import java.util.HashMap;\n import org.apache.samza.coordinator.stream.messages.CoordinatorStreamMessage;\n import org.apache.samza.coordinator.stream.messages.Delete;\n import org.apache.samza.coordinator.stream.messages.SetConfig;\n@@ -80,4 +81,18 @@ public void testHashCodeAndEquality() {\n     assertEquals(message, message1);\n     assertTrue(!message.equals(message2));\n   }\n+\n+  @Test\n+  public void testEqualsNPEonNullValues() {\n+    String[] testKeys = {\"key1\", \"key2\"};\n+    HashMap<String, Object> messageMap = new HashMap<>();\n+    messageMap.put(\"values\", new HashMap<String, String>());\n+    HashMap<String, Object> messageMapWithNullValues = new HashMap<>();\n+    messageMapWithNullValues.put(\"values\", null);\n+    CoordinatorStreamMessage message = new CoordinatorStreamMessage(testKeys, messageMap);\n+    CoordinatorStreamMessage messageWithNullValue = new CoordinatorStreamMessage(testKeys, messageMapWithNullValues);\n+\n+    assertFalse(\"Should not throw NPE and should not be equal to each other.\",\n+        messageWithNullValue.equals(message));\n+  }\n }",
                "raw_url": "https://github.com/apache/samza/raw/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamMessage.java",
                "sha": "3fd803cdbe81093a18c746cb5a1dfd9403ca1ee8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamSystemConsumer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamSystemConsumer.java?ref=4aaa589cdd22250719c5c30a19f4f95f4c327110",
                "deletions": 1,
                "filename": "samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamSystemConsumer.java",
                "patch": "@@ -120,7 +120,7 @@ public void testOrderKeyRewrite() throws InterruptedException {\n \n     consumer.bootstrap();\n \n-    Set<CoordinatorStreamMessage> bootstrappedMessages = consumer.getBoostrappedStream();\n+    Set<CoordinatorStreamMessage> bootstrappedMessages = consumer.getBootstrappedStream(SetConfig.TYPE);\n \n     assertEquals(2, bootstrappedMessages.size()); // First message should have been removed as a duplicate\n     CoordinatorStreamMessage[] coordinatorStreamMessages = bootstrappedMessages.toArray(new CoordinatorStreamMessage[2]);",
                "raw_url": "https://github.com/apache/samza/raw/4aaa589cdd22250719c5c30a19f4f95f4c327110/samza-core/src/test/java/org/apache/samza/coordinator/stream/TestCoordinatorStreamSystemConsumer.java",
                "sha": "6da2c8f403495d529ff33893a110b4a8d028b327",
                "status": "modified"
            }
        ],
        "message": "SAMZA-2143: NPE in CoordinatorStreamMessage#equals and some clean-up to CoordinatorStreamSystemConsumer (#970)",
        "parent": "https://github.com/apache/samza/commit/cb410d2e14de85064e74cedacc1472f15f7d968c",
        "repo": "samza",
        "unit_tests": [
            "TestCoordinatorStreamSystemConsumer.java",
            "TestCoordinatorStreamMessage.java"
        ]
    },
    "samza_5e14946": {
        "bug_id": "samza_5e14946",
        "commit": "https://github.com/apache/samza/commit/5e14946826e6abb8b0a852818f8affce363a7337",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/5e14946826e6abb8b0a852818f8affce363a7337/samza-api/src/main/java/org/apache/samza/system/StreamSpec.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-api/src/main/java/org/apache/samza/system/StreamSpec.java?ref=5e14946826e6abb8b0a852818f8affce363a7337",
                "deletions": 2,
                "filename": "samza-api/src/main/java/org/apache/samza/system/StreamSpec.java",
                "patch": "@@ -137,7 +137,7 @@ public StreamSpec(String id, String physicalName, String systemName, Map<String,\n    * @param config          A map of properties for the stream. These may be System-specfic.\n    */\n   public StreamSpec(String id, String physicalName, String systemName, int partitionCount,  Map<String, String> config) {\n-    validateLogicalIdentifier(\"id\", id);\n+    validateLogicalIdentifier(\"streamId\", id);\n     validateLogicalIdentifier(\"systemName\", systemName);\n \n     if (partitionCount < 1) {\n@@ -197,7 +197,7 @@ public String getOrDefault(String propertyName, String defaultValue) {\n   }\n \n   private void validateLogicalIdentifier(String identifierName, String identifierValue) {\n-    if (!identifierValue.matches(\"[A-Za-z0-9_-]+\")) {\n+    if (identifierValue == null || !identifierValue.matches(\"[A-Za-z0-9_-]+\")) {\n       throw new IllegalArgumentException(String.format(\"Identifier '%s' is '%s'. It must match the expression [A-Za-z0-9_-]+\", identifierName, identifierValue));\n     }\n   }",
                "raw_url": "https://github.com/apache/samza/raw/5e14946826e6abb8b0a852818f8affce363a7337/samza-api/src/main/java/org/apache/samza/system/StreamSpec.java",
                "sha": "0cdeb95b4f6acefbb8f1743fcc2ce063c6c75f45",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/samza/blob/5e14946826e6abb8b0a852818f8affce363a7337/samza-core/src/test/java/org/apache/samza/runtime/TestAbstractApplicationRunner.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/runtime/TestAbstractApplicationRunner.java?ref=5e14946826e6abb8b0a852818f8affce363a7337",
                "deletions": 7,
                "filename": "samza-core/src/test/java/org/apache/samza/runtime/TestAbstractApplicationRunner.java",
                "patch": "@@ -20,17 +20,16 @@\n \n import java.util.HashMap;\n import java.util.Map;\n+import org.apache.samza.application.StreamApplication;\n import org.apache.samza.config.Config;\n import org.apache.samza.config.JobConfig;\n import org.apache.samza.config.MapConfig;\n import org.apache.samza.config.StreamConfig;\n-import org.apache.samza.application.StreamApplication;\n import org.apache.samza.job.ApplicationStatus;\n import org.apache.samza.system.StreamSpec;\n import org.junit.Test;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.*;\n \n public class TestAbstractApplicationRunner {\n   private static final String STREAM_ID = \"t3st-Stream_Id\";\n@@ -119,7 +118,7 @@ public void testgetStreamWithSystemAtBothScopesInConfig() {\n   }\n \n   // System is required. Throw if it cannot be determined.\n-  @Test(expected = Exception.class)\n+  @Test(expected = IllegalArgumentException.class)\n   public void testgetStreamWithOutSystemInConfig() {\n     Config config = buildStreamConfig(STREAM_ID,\n                                       StreamConfig.PHYSICAL_NAME(), TEST_PHYSICAL_NAME);\n@@ -291,8 +290,8 @@ public void testGetStreamSystemNameArgEmpty() {\n     runner.getStreamSpec(STREAM_ID, TEST_PHYSICAL_NAME, \"\");\n   }\n \n-  // Null is not allowed for system name.\n-  @Test(expected = NullPointerException.class)\n+  // Null is not allowed IllegalArgumentException system name.\n+  @Test(expected = IllegalArgumentException.class)\n   public void testGetStreamSystemNameArgNull() {\n     Config config = buildStreamConfig(STREAM_ID,\n                                       StreamConfig.PHYSICAL_NAME(), TEST_PHYSICAL_NAME2,\n@@ -323,7 +322,7 @@ public void testGetStreamStreamIdEmpty() {\n   }\n \n   // Null is not allowed for streamId.\n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = IllegalArgumentException.class)\n   public void testGetStreamStreamIdNull() {\n     Config config = buildStreamConfig(null,\n         StreamConfig.SYSTEM(), TEST_SYSTEM);",
                "raw_url": "https://github.com/apache/samza/raw/5e14946826e6abb8b0a852818f8affce363a7337/samza-core/src/test/java/org/apache/samza/runtime/TestAbstractApplicationRunner.java",
                "sha": "aaacd6ec48246469e062ac39a5dfeb5be645cf26",
                "status": "modified"
            }
        ],
        "message": "SAMZA-1263: Samza Fluent: NPE is streams.id.samza.system is missing\n\nAuthor: Jacob Maes <jmaes@linkedin.com>\n\nReviewers: Xinyu Liu <xiliu@linkedin.com>, Prateek Maheshwari <pmaheshw@linkedin.com>\n\nCloses #165 from jmakes/samza-1263",
        "parent": "https://github.com/apache/samza/commit/435d4fe1f9caf7dd310a788e8158bc94c3f23878",
        "repo": "samza",
        "unit_tests": [
            "TestStreamSpec.java"
        ]
    },
    "samza_b101ff9": {
        "bug_id": "samza_b101ff9",
        "commit": "https://github.com/apache/samza/commit/b101ff91a8f9dae77fb64b846f150bcf8154e598",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/LocalityManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/container/LocalityManager.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 8,
                "filename": "samza-core/src/main/java/org/apache/samza/container/LocalityManager.java",
                "patch": "@@ -25,7 +25,6 @@\n import java.util.Map;\n import org.apache.samza.config.Config;\n import org.apache.samza.config.JobConfig;\n-import org.apache.samza.container.grouper.task.TaskAssignmentManager;\n import org.apache.samza.coordinator.stream.CoordinatorStreamKeySerde;\n import org.apache.samza.coordinator.stream.CoordinatorStreamValueSerde;\n import org.apache.samza.coordinator.stream.messages.SetContainerHostMapping;\n@@ -48,7 +47,6 @@\n   private final Serde<String> keySerde;\n   private final Serde<String> valueSerde;\n   private final MetadataStore metadataStore;\n-  private final TaskAssignmentManager taskAssignmentManager;\n \n   /**\n    * Builds the LocalityManager based upon {@link Config} and {@link MetricsRegistry}.\n@@ -81,7 +79,6 @@ public LocalityManager(Config config, MetricsRegistry metricsRegistry) {\n     this.metadataStore.init();\n     this.keySerde = keySerde;\n     this.valueSerde = valueSerde;\n-    this.taskAssignmentManager = new TaskAssignmentManager(config, metricsRegistry, keySerde, valueSerde);\n   }\n \n   /**\n@@ -128,10 +125,5 @@ public void writeContainerToHostMapping(String containerId, String hostName) {\n \n   public void close() {\n     metadataStore.close();\n-    taskAssignmentManager.close();\n-  }\n-\n-  public TaskAssignmentManager getTaskAssignmentManager() {\n-    return taskAssignmentManager;\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/LocalityManager.java",
                "sha": "63483b75b2ef4915810d55323b92797ee272ef4b",
                "status": "modified"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCount.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCount.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 39,
                "filename": "samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCount.java",
                "patch": "@@ -29,10 +29,13 @@\n import java.util.Set;\n \n import org.apache.samza.SamzaException;\n+import org.apache.samza.config.Config;\n+import org.apache.samza.config.JobConfig;\n import org.apache.samza.container.LocalityManager;\n import org.apache.samza.container.TaskName;\n import org.apache.samza.job.model.ContainerModel;\n import org.apache.samza.job.model.TaskModel;\n+import org.apache.samza.metrics.MetricsRegistryMap;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -50,10 +53,12 @@\n public class GroupByContainerCount implements BalancingTaskNameGrouper {\n   private static final Logger log = LoggerFactory.getLogger(GroupByContainerCount.class);\n   private final int containerCount;\n+  private final Config config;\n \n-  public GroupByContainerCount(int containerCount) {\n+  public GroupByContainerCount(Config config) {\n+    this.containerCount = new JobConfig(config).getContainerCount();\n+    this.config = config;\n     if (containerCount <= 0) throw new IllegalArgumentException(\"Must have at least one container\");\n-    this.containerCount = containerCount;\n   }\n \n   @Override\n@@ -94,51 +99,56 @@ public GroupByContainerCount(int containerCount) {\n       return group(tasks);\n     }\n \n-    TaskAssignmentManager taskAssignmentManager = localityManager.getTaskAssignmentManager();\n-    List<TaskGroup> containers = getPreviousContainers(taskAssignmentManager, tasks.size());\n-    if (containers == null || containers.size() == 1 || containerCount == 1) {\n-      log.info(\"Balancing does not apply. Invoking grouper.\");\n-      Set<ContainerModel> models = group(tasks);\n-      saveTaskAssignments(models, taskAssignmentManager);\n-      return models;\n-    }\n+    TaskAssignmentManager taskAssignmentManager =  new TaskAssignmentManager(config, new MetricsRegistryMap());\n+    taskAssignmentManager.init();\n+    try {\n+      List<TaskGroup> containers = getPreviousContainers(taskAssignmentManager, tasks.size());\n+      if (containers == null || containers.size() == 1 || containerCount == 1) {\n+        log.info(\"Balancing does not apply. Invoking grouper.\");\n+        Set<ContainerModel> models = group(tasks);\n+        saveTaskAssignments(models, taskAssignmentManager);\n+        return models;\n+      }\n \n-    int prevContainerCount = containers.size();\n-    int containerDelta = containerCount - prevContainerCount;\n-    if (containerDelta == 0) {\n-      log.info(\"Container count has not changed. Reusing previous container models.\");\n-      return buildContainerModels(tasks, containers);\n-    }\n-    log.info(\"Container count changed from {} to {}. Balancing tasks.\", prevContainerCount, containerCount);\n+      int prevContainerCount = containers.size();\n+      int containerDelta = containerCount - prevContainerCount;\n+      if (containerDelta == 0) {\n+        log.info(\"Container count has not changed. Reusing previous container models.\");\n+        return buildContainerModels(tasks, containers);\n+      }\n+      log.info(\"Container count changed from {} to {}. Balancing tasks.\", prevContainerCount, containerCount);\n \n-    // Calculate the expected task count per container\n-    int[] expectedTaskCountPerContainer = calculateTaskCountPerContainer(tasks.size(), prevContainerCount, containerCount);\n+      // Calculate the expected task count per container\n+      int[] expectedTaskCountPerContainer = calculateTaskCountPerContainer(tasks.size(), prevContainerCount, containerCount);\n \n-    // Collect excess tasks from over-assigned containers\n-    List<String> taskNamesToReassign = new LinkedList<>();\n-    for (int i = 0; i < prevContainerCount; i++) {\n-      TaskGroup taskGroup = containers.get(i);\n-      while (taskGroup.size() > expectedTaskCountPerContainer[i]) {\n-        taskNamesToReassign.add(taskGroup.removeTask());\n+      // Collect excess tasks from over-assigned containers\n+      List<String> taskNamesToReassign = new LinkedList<>();\n+      for (int i = 0; i < prevContainerCount; i++) {\n+        TaskGroup taskGroup = containers.get(i);\n+        while (taskGroup.size() > expectedTaskCountPerContainer[i]) {\n+          taskNamesToReassign.add(taskGroup.removeTask());\n+        }\n       }\n-    }\n \n-    // Assign tasks to the under-assigned containers\n-    if (containerDelta > 0) {\n-      List<TaskGroup> newContainers = createContainers(prevContainerCount, containerCount);\n-      containers.addAll(newContainers);\n-    } else {\n-      containers = containers.subList(0, containerCount);\n-    }\n-    assignTasksToContainers(expectedTaskCountPerContainer, taskNamesToReassign, containers);\n+      // Assign tasks to the under-assigned containers\n+      if (containerDelta > 0) {\n+        List<TaskGroup> newContainers = createContainers(prevContainerCount, containerCount);\n+        containers.addAll(newContainers);\n+      } else {\n+        containers = containers.subList(0, containerCount);\n+      }\n+      assignTasksToContainers(expectedTaskCountPerContainer, taskNamesToReassign, containers);\n \n-    // Transform containers to containerModel\n-    Set<ContainerModel> models = buildContainerModels(tasks, containers);\n+      // Transform containers to containerModel\n+      Set<ContainerModel> models = buildContainerModels(tasks, containers);\n \n-    // Save the results\n-    saveTaskAssignments(models, taskAssignmentManager);\n+      // Save the results\n+      saveTaskAssignments(models, taskAssignmentManager);\n \n-    return models;\n+      return models;\n+    } finally {\n+      taskAssignmentManager.close();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCount.java",
                "sha": "759f82e0a0be2a770cd42d868491af629508d50f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCountFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCountFactory.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 3,
                "filename": "samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCountFactory.java",
                "patch": "@@ -19,15 +19,13 @@\n package org.apache.samza.container.grouper.task;\n \n import org.apache.samza.config.Config;\n-import org.apache.samza.config.JobConfig;\n-\n \n /**\n  * Factory to build the GroupByContainerCount class.\n  */\n public class GroupByContainerCountFactory implements TaskNameGrouperFactory {\n   @Override\n   public TaskNameGrouper build(Config config) {\n-    return new GroupByContainerCount(new JobConfig(config).getContainerCount());\n+    return new GroupByContainerCount(config);\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerCountFactory.java",
                "sha": "06aba33a24013baf122c404b69df17a9abe2c67d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 1,
                "filename": "samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java",
                "patch": "@@ -83,7 +83,7 @@ public TaskAssignmentManager(Config config, MetricsRegistry metricsRegistry, Ser\n     this.metadataStore = metadataStoreFactory.getMetadataStore(SetTaskContainerMapping.TYPE, config, metricsRegistry);\n   }\n \n-  public void init(Config config, MetricsRegistry metricsRegistry) {\n+  public void init() {\n     this.metadataStore.init();\n   }\n ",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java",
                "sha": "0ada91ca597ddeb1fb2f8be176a46b60ed8d0883",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerCount.java",
                "changes": 106,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerCount.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 44,
                "filename": "samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerCount.java",
                "patch": "@@ -25,50 +25,61 @@\n import java.util.Set;\n import java.util.UUID;\n import org.apache.samza.SamzaException;\n+import org.apache.samza.config.Config;\n+import org.apache.samza.config.JobConfig;\n+import org.apache.samza.config.MapConfig;\n import org.apache.samza.container.LocalityManager;\n import org.apache.samza.job.model.ContainerModel;\n import org.apache.samza.job.model.TaskModel;\n import org.junit.Before;\n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mockito;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n \n import static org.apache.samza.container.mock.ContainerMocks.*;\n import static org.junit.Assert.*;\n import static org.mockito.Mockito.*;\n \n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({TaskAssignmentManager.class, GroupByContainerCount.class})\n public class TestGroupByContainerCount {\n   private TaskAssignmentManager taskAssignmentManager;\n   private LocalityManager localityManager;\n   @Before\n-  public void setup() {\n+  public void setup() throws Exception {\n     taskAssignmentManager = mock(TaskAssignmentManager.class);\n     localityManager = mock(LocalityManager.class);\n-    when(localityManager.getTaskAssignmentManager()).thenReturn(taskAssignmentManager);\n+    PowerMockito.whenNew(TaskAssignmentManager.class).withAnyArguments().thenReturn(taskAssignmentManager);\n+    Mockito.doNothing().when(taskAssignmentManager).init();\n   }\n \n   @Test(expected = IllegalArgumentException.class)\n   public void testGroupEmptyTasks() {\n-    new GroupByContainerCount(1).group(new HashSet());\n+    new GroupByContainerCount(getConfig(1)).group(new HashSet());\n   }\n \n   @Test(expected = IllegalArgumentException.class)\n   public void testGroupFewerTasksThanContainers() {\n     Set<TaskModel> taskModels = new HashSet<>();\n     taskModels.add(getTaskModel(1));\n-    new GroupByContainerCount(2).group(taskModels);\n+    new GroupByContainerCount(getConfig(2)).group(taskModels);\n   }\n \n   @Test(expected = UnsupportedOperationException.class)\n   public void testGrouperResultImmutable() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    Set<ContainerModel> containers = new GroupByContainerCount(3).group(taskModels);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n     containers.remove(containers.iterator().next());\n   }\n \n   @Test\n   public void testGroupHappyPath() {\n     Set<TaskModel> taskModels = generateTaskModels(5);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).group(taskModels);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -95,7 +106,7 @@ public void testGroupHappyPath() {\n   public void testGroupManyTasks() {\n     Set<TaskModel> taskModels = generateTaskModels(21);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).group(taskModels);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -163,11 +174,11 @@ public void testGroupManyTasks() {\n   @Test\n   public void testBalancerAfterContainerIncrease() {\n     Set<TaskModel> taskModels = generateTaskModels(9);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(2).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(2)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(4).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(4)).balance(taskModels, localityManager);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -245,11 +256,11 @@ public void testBalancerAfterContainerIncrease() {\n   @Test\n   public void testBalancerAfterContainerDecrease() {\n     Set<TaskModel> taskModels = generateTaskModels(9);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(4).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(4)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(2).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).balance(taskModels, localityManager);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -320,15 +331,15 @@ public void testBalancerAfterContainerDecrease() {\n    *  T8  T7  T3\n    */\n   @Test\n-  public void testBalancerMultipleReblances() {\n+  public void testBalancerMultipleReblances() throws Exception {\n     // Before\n     Set<TaskModel> taskModels = generateTaskModels(9);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(4).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(4)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n     // First balance\n-    Set<ContainerModel> containers = new GroupByContainerCount(2).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).balance(taskModels, localityManager);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -380,9 +391,9 @@ public void testBalancerMultipleReblances() {\n     TaskAssignmentManager taskAssignmentManager2 = mock(TaskAssignmentManager.class);\n     when(taskAssignmentManager2.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n     LocalityManager localityManager2 = mock(LocalityManager.class);\n-    when(localityManager2.getTaskAssignmentManager()).thenReturn(taskAssignmentManager2);\n+    PowerMockito.whenNew(TaskAssignmentManager.class).withAnyArguments().thenReturn(taskAssignmentManager2);\n \n-    containers = new GroupByContainerCount(3).balance(taskModels, localityManager2);\n+    containers = new GroupByContainerCount(getConfig(3)).balance(taskModels, localityManager2);\n \n     containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -455,11 +466,11 @@ public void testBalancerMultipleReblances() {\n   @Test\n   public void testBalancerAfterContainerSame() {\n     Set<TaskModel> taskModels = generateTaskModels(9);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(2).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(2)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(2).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).balance(taskModels, localityManager);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -529,7 +540,7 @@ public void testBalancerAfterContainerSameCustomAssignment() {\n     prevTaskToContainerMapping.put(getTaskName(8).getTaskName(), \"1\");\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(2).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).balance(taskModels, localityManager);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -595,7 +606,7 @@ public void testBalancerAfterContainerSameCustomAssignmentAndContainerIncrease()\n     prevTaskToContainerMapping.put(getTaskName(5).getTaskName(), \"1\");\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(3).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(3)).balance(taskModels, localityManager);\n \n     Map<String, ContainerModel> containersMap = new HashMap<>();\n     for (ContainerModel container : containers) {\n@@ -636,12 +647,12 @@ public void testBalancerAfterContainerSameCustomAssignmentAndContainerIncrease()\n   @Test\n   public void testBalancerOldContainerCountOne() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(1).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(1)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> groupContainers = new GroupByContainerCount(3).group(taskModels);\n-    Set<ContainerModel> balanceContainers = new GroupByContainerCount(3).balance(taskModels, localityManager);\n+    Set<ContainerModel> groupContainers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n+    Set<ContainerModel> balanceContainers = new GroupByContainerCount(getConfig(3)).balance(taskModels, localityManager);\n \n     // Results should be the same as calling group()\n     assertEquals(groupContainers, balanceContainers);\n@@ -657,12 +668,12 @@ public void testBalancerOldContainerCountOne() {\n   @Test\n   public void testBalancerNewContainerCountOne() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\n-    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).balance(taskModels, localityManager);\n+    Set<ContainerModel> groupContainers = new GroupByContainerCount(getConfig(1)).group(taskModels);\n+    Set<ContainerModel> balanceContainers = new GroupByContainerCount(getConfig(1)).balance(taskModels, localityManager);\n \n     // Results should be the same as calling group()\n     assertEquals(groupContainers, balanceContainers);\n@@ -677,10 +688,10 @@ public void testBalancerNewContainerCountOne() {\n   @Test\n   public void testBalancerEmptyTaskMapping() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    when(taskAssignmentManager.readTaskAssignment()).thenReturn(new HashMap<String, String>());\n+    when(taskAssignmentManager.readTaskAssignment()).thenReturn(new HashMap<>());\n \n-    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\n-    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).balance(taskModels, localityManager);\n+    Set<ContainerModel> groupContainers = new GroupByContainerCount(getConfig(1)).group(taskModels);\n+    Set<ContainerModel> balanceContainers = new GroupByContainerCount(getConfig(1)).balance(taskModels, localityManager);\n \n     // Results should be the same as calling group()\n     assertEquals(groupContainers, balanceContainers);\n@@ -696,12 +707,12 @@ public void testBalancerEmptyTaskMapping() {\n   public void testGroupTaskCountIncrease() {\n     int taskCount = 3;\n     Set<TaskModel> taskModels = generateTaskModels(taskCount);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(2).group(generateTaskModels(taskCount - 1)); // Here's the key step\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(2)).group(generateTaskModels(taskCount - 1)); // Here's the key step\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\n-    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).balance(taskModels, localityManager);\n+    Set<ContainerModel> groupContainers = new GroupByContainerCount(getConfig(1)).group(taskModels);\n+    Set<ContainerModel> balanceContainers = new GroupByContainerCount(getConfig(1)).balance(taskModels, localityManager);\n \n     // Results should be the same as calling group()\n     assertEquals(groupContainers, balanceContainers);\n@@ -717,12 +728,12 @@ public void testGroupTaskCountIncrease() {\n   public void testGroupTaskCountDecrease() {\n     int taskCount = 3;\n     Set<TaskModel> taskModels = generateTaskModels(taskCount);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(generateTaskModels(taskCount + 1)); // Here's the key step\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(3)).group(generateTaskModels(taskCount + 1)); // Here's the key step\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\n-    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).balance(taskModels, localityManager);\n+    Set<ContainerModel> groupContainers = new GroupByContainerCount(getConfig(1)).group(taskModels);\n+    Set<ContainerModel> balanceContainers = new GroupByContainerCount(getConfig(1)).balance(taskModels, localityManager);\n \n     // Results should be the same as calling group()\n     assertEquals(groupContainers, balanceContainers);\n@@ -737,31 +748,31 @@ public void testGroupTaskCountDecrease() {\n   @Test(expected = IllegalArgumentException.class)\n   public void testBalancerNewContainerCountGreaterThanTasks() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    new GroupByContainerCount(5).balance(taskModels, localityManager);     // Should throw\n+    new GroupByContainerCount(getConfig(5)).balance(taskModels, localityManager);     // Should throw\n   }\n \n   @Test(expected = IllegalArgumentException.class)\n   public void testBalancerEmptyTasks() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    new GroupByContainerCount(5).balance(new HashSet<TaskModel>(), localityManager);     // Should throw\n+    new GroupByContainerCount(getConfig(5)).balance(new HashSet<>(), localityManager);     // Should throw\n   }\n \n   @Test(expected = UnsupportedOperationException.class)\n   public void testBalancerResultImmutable() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n-    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\n+    Set<ContainerModel> prevContainers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    Set<ContainerModel> containers = new GroupByContainerCount(2).balance(taskModels, localityManager);\n+    Set<ContainerModel> containers = new GroupByContainerCount(getConfig(2)).balance(taskModels, localityManager);\n     containers.remove(containers.iterator().next());\n   }\n \n@@ -776,18 +787,25 @@ public void testBalancerThrowsOnNonIntegerContainerIds() {\n     Map<String, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\n     when(taskAssignmentManager.readTaskAssignment()).thenReturn(prevTaskToContainerMapping);\n \n-    new GroupByContainerCount(3).balance(taskModels, localityManager); //Should throw\n+    new GroupByContainerCount(getConfig(3)).balance(taskModels, localityManager); //Should throw\n \n   }\n \n   @Test\n   public void testBalancerWithNullLocalityManager() {\n     Set<TaskModel> taskModels = generateTaskModels(3);\n \n-    Set<ContainerModel> groupContainers = new GroupByContainerCount(3).group(taskModels);\n-    Set<ContainerModel> balanceContainers = new GroupByContainerCount(3).balance(taskModels, null);\n+    Set<ContainerModel> groupContainers = new GroupByContainerCount(getConfig(3)).group(taskModels);\n+    Set<ContainerModel> balanceContainers = new GroupByContainerCount(getConfig(3)).balance(taskModels, null);\n \n     // Results should be the same as calling group()\n     assertEquals(groupContainers, balanceContainers);\n   }\n+\n+\n+  Config getConfig(int containerCount) {\n+    Map<String, String> config = new HashMap<>();\n+    config.put(JobConfig.JOB_CONTAINER_COUNT(), String.valueOf(containerCount));\n+    return new MapConfig(config);\n+  }\n }",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerCount.java",
                "sha": "0c2f2fb819166adf520dc0d8c16667061404c67f",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 4,
                "filename": "samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java",
                "patch": "@@ -37,21 +37,25 @@\n import org.apache.samza.job.model.TaskModel;\n import org.junit.Before;\n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n \n import static org.apache.samza.container.mock.ContainerMocks.*;\n import static org.junit.Assert.*;\n import static org.mockito.Mockito.*;\n \n \n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({TaskAssignmentManager.class, GroupByContainerIds.class})\n public class TestGroupByContainerIds {\n \n   @Before\n-  public void setup() {\n+  public void setup() throws Exception {\n     TaskAssignmentManager taskAssignmentManager = mock(TaskAssignmentManager.class);\n     LocalityManager localityManager = mock(LocalityManager.class);\n-    when(localityManager.getTaskAssignmentManager()).thenReturn(taskAssignmentManager);\n-\n-\n+    PowerMockito.whenNew(TaskAssignmentManager.class).withAnyArguments().thenReturn(taskAssignmentManager);\n   }\n \n   private Config buildConfigForContainerCount(int count) {",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java",
                "sha": "5bb78e8e2df278630015358a1e73d2a386c58f0a",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestTaskAssignmentManager.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestTaskAssignmentManager.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 5,
                "filename": "samza-core/src/test/java/org/apache/samza/container/grouper/task/TestTaskAssignmentManager.java",
                "patch": "@@ -68,7 +68,7 @@ public void tearDown() {\n   @Test\n   public void testTaskAssignmentManager() {\n     TaskAssignmentManager taskAssignmentManager = new TaskAssignmentManager(config, new MetricsRegistryMap());\n-    taskAssignmentManager.init(config, new MetricsRegistryMap());\n+    taskAssignmentManager.init();\n \n     Map<String, String> expectedMap = ImmutableMap.of(\"Task0\", \"0\", \"Task1\", \"1\", \"Task2\", \"2\", \"Task3\", \"0\", \"Task4\", \"1\");\n \n@@ -83,9 +83,10 @@ public void testTaskAssignmentManager() {\n     taskAssignmentManager.close();\n   }\n \n-  @Test public void testDeleteMappings() {\n+  @Test\n+  public void testDeleteMappings() {\n     TaskAssignmentManager taskAssignmentManager = new TaskAssignmentManager(config, new MetricsRegistryMap());\n-    taskAssignmentManager.init(config, new MetricsRegistryMap());\n+    taskAssignmentManager.init();\n \n     Map<String, String> expectedMap = ImmutableMap.of(\"Task0\", \"0\", \"Task1\", \"1\");\n \n@@ -104,9 +105,10 @@ public void testTaskAssignmentManager() {\n     taskAssignmentManager.close();\n   }\n \n-  @Test public void testTaskAssignmentManagerEmptyCoordinatorStream() {\n+  @Test\n+  public void testTaskAssignmentManagerEmptyCoordinatorStream() {\n     TaskAssignmentManager taskAssignmentManager = new TaskAssignmentManager(config, new MetricsRegistryMap());\n-    taskAssignmentManager.init(config, new MetricsRegistryMap());\n+    taskAssignmentManager.init();\n \n     Map<String, String> expectedMap = new HashMap<>();\n     Map<String, String> localMap = taskAssignmentManager.readTaskAssignment();",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestTaskAssignmentManager.java",
                "sha": "fcdbf0848c77ba5292346d84daacca2d8962081c",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/coordinator/TestJobModelManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/coordinator/TestJobModelManager.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 2,
                "filename": "samza-core/src/test/java/org/apache/samza/coordinator/TestJobModelManager.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.samza.config.Config;\n import org.apache.samza.config.MapConfig;\n import org.apache.samza.container.LocalityManager;\n+import org.apache.samza.container.grouper.task.GroupByContainerCount;\n import org.apache.samza.container.grouper.task.TaskAssignmentManager;\n import org.apache.samza.coordinator.server.HttpServer;\n import org.apache.samza.coordinator.stream.messages.SetContainerHostMapping;\n@@ -45,12 +46,18 @@\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n+import org.junit.runner.RunWith;\n import org.mockito.ArgumentMatcher;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n import scala.collection.JavaConversions;\n \n /**\n  * Unit tests for {@link JobModelManager}\n  */\n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({TaskAssignmentManager.class, GroupByContainerCount.class})\n public class TestJobModelManager {\n   private final TaskAssignmentManager mockTaskManager = mock(TaskAssignmentManager.class);\n   private final LocalityManager mockLocalityManager = mock(LocalityManager.class);\n@@ -67,7 +74,7 @@\n   private JobModelManager jobModelManager;\n \n   @Before\n-  public void setup() {\n+  public void setup() throws Exception {\n     when(mockLocalityManager.readContainerLocality()).thenReturn(this.localityMappings);\n     when(mockStreamMetadataCache.getStreamMetadata(argThat(new ArgumentMatcher<scala.collection.immutable.Set<SystemStream>>() {\n       @Override\n@@ -77,7 +84,7 @@ public boolean matches(Object argument) {\n       }\n     }), anyBoolean())).thenReturn(mockStreamMetadataMap);\n     when(mockStreamMetadata.getSystemStreamPartitionMetadata()).thenReturn(mockSspMetadataMap);\n-    when(mockLocalityManager.getTaskAssignmentManager()).thenReturn(mockTaskManager);\n+    PowerMockito.whenNew(TaskAssignmentManager.class).withAnyArguments().thenReturn(mockTaskManager);\n     when(mockTaskManager.readTaskAssignment()).thenReturn(Collections.EMPTY_MAP);\n   }\n ",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-core/src/test/java/org/apache/samza/coordinator/TestJobModelManager.java",
                "sha": "1dbf1326532c95e5f1ce673c5f08d58b5402027d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-rest/src/main/java/org/apache/samza/rest/proxy/task/SamzaTaskProxy.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-rest/src/main/java/org/apache/samza/rest/proxy/task/SamzaTaskProxy.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 1,
                "filename": "samza-rest/src/main/java/org/apache/samza/rest/proxy/task/SamzaTaskProxy.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.samza.config.MapConfig;\n import org.apache.samza.config.StorageConfig;\n import org.apache.samza.container.LocalityManager;\n+import org.apache.samza.container.grouper.task.TaskAssignmentManager;\n import org.apache.samza.coordinator.stream.CoordinatorStreamSystemConsumer;\n import org.apache.samza.coordinator.stream.messages.SetContainerHostMapping;\n import org.apache.samza.metrics.MetricsRegistryMap;\n@@ -131,7 +132,8 @@ private Config getCoordinatorSystemConfig(JobInstance jobInstance) {\n   protected List<Task> readTasksFromCoordinatorStream(CoordinatorStreamSystemConsumer consumer) {\n     LocalityManager localityManager = new LocalityManager(consumer.getConfig(), new MetricsRegistryMap());\n     Map<String, Map<String, String>> containerIdToHostMapping = localityManager.readContainerLocality();\n-    Map<String, String> taskNameToContainerIdMapping = localityManager.getTaskAssignmentManager().readTaskAssignment();\n+    TaskAssignmentManager taskAssignmentManager = new TaskAssignmentManager(consumer.getConfig(), new MetricsRegistryMap());\n+    Map<String, String> taskNameToContainerIdMapping = taskAssignmentManager.readTaskAssignment();\n     StorageConfig storageConfig = new StorageConfig(consumer.getConfig());\n     List<String> storeNames = JavaConverters.seqAsJavaListConverter(storageConfig.getStoreNames()).asJava();\n     return taskNameToContainerIdMapping.entrySet()",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-rest/src/main/java/org/apache/samza/rest/proxy/task/SamzaTaskProxy.java",
                "sha": "daf665b163529a7c8081e48cd4ff9c897f59c76e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/samza/blob/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-yarn/src/test/java/org/apache/samza/webapp/TestApplicationMasterRestClient.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-yarn/src/test/java/org/apache/samza/webapp/TestApplicationMasterRestClient.java?ref=b101ff91a8f9dae77fb64b846f150bcf8154e598",
                "deletions": 1,
                "filename": "samza-yarn/src/test/java/org/apache/samza/webapp/TestApplicationMasterRestClient.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.samza.clustermanager.SamzaApplicationState;\n import org.apache.samza.clustermanager.SamzaResource;\n import org.apache.samza.config.Config;\n+import org.apache.samza.config.JobConfig;\n import org.apache.samza.config.MapConfig;\n import org.apache.samza.container.TaskName;\n import org.apache.samza.container.grouper.task.GroupByContainerCount;\n@@ -275,7 +276,9 @@ private YarnAppState createYarnAppState(ContainerId containerId) throws Malforme\n         new TaskModel(new TaskName(\"task2\"),\n             ImmutableSet.of(new SystemStreamPartition(new SystemStream(\"system1\", \"stream1\"), new Partition(1))),\n             new Partition(1)));\n-    GroupByContainerCount grouper = new GroupByContainerCount(2);\n+    Map<String, String> config = new HashMap<>();\n+    config.put(JobConfig.JOB_CONTAINER_COUNT(), String.valueOf(2));\n+    GroupByContainerCount grouper = new GroupByContainerCount(new MapConfig(config));\n     Set<ContainerModel> containerModels = grouper.group(taskModels);\n     HashMap<String, ContainerModel> containers = new HashMap<>();\n     for (ContainerModel containerModel : containerModels) {",
                "raw_url": "https://github.com/apache/samza/raw/b101ff91a8f9dae77fb64b846f150bcf8154e598/samza-yarn/src/test/java/org/apache/samza/webapp/TestApplicationMasterRestClient.java",
                "sha": "d19badc083e29b5dea9c0362f3cda023231bab85",
                "status": "modified"
            }
        ],
        "message": "SAMZA-1933: Fix NPE in LocalityManager.\n\nAuthor: Shanthoosh Venkataraman <spvenkat@usc.edu>\n\nReviewers: Prateek Maheshwari <pmaheshwari@apache.org>\n\nCloses #684 from shanthoosh/fix_NPE_in_task_assignment_manager",
        "parent": "https://github.com/apache/samza/commit/d48cf5f74b2268762d3175d8c068769b418b578e",
        "repo": "samza",
        "unit_tests": [
            "TestLocalityManager.java",
            "TestGroupByContainerCount.java",
            "TestTaskAssignmentManager.java"
        ]
    },
    "samza_bda8df6": {
        "bug_id": "samza_bda8df6",
        "commit": "https://github.com/apache/samza/commit/bda8df6e21c02060831ca05a1a23af1685f0ea07",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/main/scala/org/apache/samza/system/kafka/BrokerProxy.scala",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kafka/src/main/scala/org/apache/samza/system/kafka/BrokerProxy.scala?ref=bda8df6e21c02060831ca05a1a23af1685f0ea07",
                "deletions": 2,
                "filename": "samza-kafka/src/main/scala/org/apache/samza/system/kafka/BrokerProxy.scala",
                "patch": "@@ -77,7 +77,7 @@ abstract class BrokerProxy(\n     sc\n   }\n \n-  def addTopicPartition(tp: TopicAndPartition, lastCheckpointedOffset: String) = {\n+  def addTopicPartition(tp: TopicAndPartition, lastCheckpointedOffset: Option[String]) = {\n     debug(\"Adding new topic and partition %s to queue for %s\" format (tp, host))\n     if (nextOffsets.containsKey(tp)) toss(\"Already consuming TopicPartition %s\" format tp)\n \n@@ -181,7 +181,7 @@ abstract class BrokerProxy(\n       warn(\"Received OffsetOutOfRange exception for %s. Current offset = %s\" format (e.tp, nextOffsets.getOrElse(e.tp, \"not found in map, likely removed in the interim\")))\n \n       try {\n-        val newOffset = offsetGetter.getNextOffset(simpleConsumer, e.tp, null)\n+        val newOffset = offsetGetter.getNextOffset(simpleConsumer, e.tp, Option(null))\n         // Put the new offset into the map (if the tp still exists).  Will catch it on the next go-around\n         nextOffsets.replace(e.tp, newOffset)\n       } catch {",
                "raw_url": "https://github.com/apache/samza/raw/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/main/scala/org/apache/samza/system/kafka/BrokerProxy.scala",
                "sha": "53b2e22e369c0ee678bfb479ed847159f9ddaa0c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/samza/blob/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/main/scala/org/apache/samza/system/kafka/GetOffset.scala",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kafka/src/main/scala/org/apache/samza/system/kafka/GetOffset.scala?ref=bda8df6e21c02060831ca05a1a23af1685f0ea07",
                "deletions": 3,
                "filename": "samza-kafka/src/main/scala/org/apache/samza/system/kafka/GetOffset.scala",
                "patch": "@@ -87,7 +87,7 @@ class GetOffset(default: String, autoOffsetResetTopics: Map[String, String] = Ma\n    * @param lastCheckpointedOffset Null is acceptable. If not null, return the last checkpointed offset, after checking it is valid\n    * @return Next offset to read or throw an exception if one has been received via the simple consumer\n    */\n-  def getNextOffset(sc: DefaultFetchSimpleConsumer, tp: TopicAndPartition, lastCheckpointedOffset: String): Long = {\n+ def getNextOffset(sc: DefaultFetchSimpleConsumer, tp: TopicAndPartition, lastCheckpointedOffset: Option[String]): Long = {\n     val offsetRequest = new OffsetRequest(Map(tp -> new PartitionOffsetRequestInfo(getAutoOffset(tp.topic), 1)))\n     val offsetResponse = sc.getOffsetsBefore(offsetRequest)\n     val partitionOffsetResponse = offsetResponse.partitionErrorAndOffsets.get(tp).getOrElse(toss(\"Unable to find offset information for %s\" format tp))\n@@ -96,9 +96,9 @@ class GetOffset(default: String, autoOffsetResetTopics: Map[String, String] = Ma\n \n     val autoOffset = partitionOffsetResponse.offsets.headOption.getOrElse(toss(\"Got response, but no offsets defined for %s\" format tp))\n \n-    val actualOffset = Option(lastCheckpointedOffset) match {\n+    val actualOffset = lastCheckpointedOffset match {\n       case Some(last) => useLastCheckpointedOffset(sc, last, tp).getOrElse(autoOffset)\n-      case None => autoOffset\n+      case _ => autoOffset\n     }\n \n     info(\"Final offset to be returned for Topic and Partition %s = %d\" format (tp, actualOffset))",
                "raw_url": "https://github.com/apache/samza/raw/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/main/scala/org/apache/samza/system/kafka/GetOffset.scala",
                "sha": "25cd52cbfad33d39f682a9764a5b6b3fbaa9ac97",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala?ref=bda8df6e21c02060831ca05a1a23af1685f0ea07",
                "deletions": 1,
                "filename": "samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala",
                "patch": "@@ -118,7 +118,7 @@ private[kafka] class KafkaSystemConsumer(\n                 val messageSink: MessageSink = sink\n               })\n \n-              brokerProxy.addTopicPartition(head, lastOffset)\n+              brokerProxy.addTopicPartition(head, Option(lastOffset))\n             case None => warn(\"No such topic-partition: %s, dropping.\" format head)\n           }\n           rest",
                "raw_url": "https://github.com/apache/samza/raw/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/main/scala/org/apache/samza/system/kafka/KafkaSystemConsumer.scala",
                "sha": "5dbcd945984f4c71a02e4bbba10e71fce871229c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/samza/blob/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/test/scala/org/apache/samza/system/kafka/TestBrokerProxy.scala",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-kafka/src/test/scala/org/apache/samza/system/kafka/TestBrokerProxy.scala?ref=bda8df6e21c02060831ca05a1a23af1685f0ea07",
                "deletions": 6,
                "filename": "samza-kafka/src/test/scala/org/apache/samza/system/kafka/TestBrokerProxy.scala",
                "patch": "@@ -162,9 +162,9 @@ class TestBrokerProxy extends Logging {\n     val (bp, tp, sink) = getMockBrokerProxy()\n \n     bp.start\n-    bp.addTopicPartition(tp, \"0\")\n+    bp.addTopicPartition(tp, Option(\"0\"))\n     // Add tp2, which should never receive messages since sink disables it.\n-    bp.addTopicPartition(tp2, \"0\")\n+    bp.addTopicPartition(tp2, Option(\"0\"))\n     Thread.sleep(1000)\n     assertEquals(2, sink.receivedMessages.size)\n     assertEquals(42, sink.receivedMessages.get(0)._2.offset)\n@@ -174,10 +174,10 @@ class TestBrokerProxy extends Logging {\n   @Test def brokerProxyThrowsExceptionOnDuplicateTopicPartitions() = {\n     val (bp, tp, _) = getMockBrokerProxy()\n     bp.start\n-    bp.addTopicPartition(tp, \"0\")\n+    bp.addTopicPartition(tp, Option(\"0\"))\n \n     try {\n-      bp.addTopicPartition(tp, \"1\")\n+      bp.addTopicPartition(tp, Option(\"1\"))\n       fail(\"Should have thrown an exception\")\n     } catch {\n       case se: SamzaException => assertEquals(se.getMessage, \"Already consuming TopicPartition [Redbird,2012]\")\n@@ -199,7 +199,7 @@ class TestBrokerProxy extends Logging {\n \n     val mockOffsetGetter = mock(classOf[GetOffset])\n     // This will be used by the simple consumer below, and this is the response that simple consumer needs\n-    when(mockOffsetGetter.getNextOffset(any(classOf[DefaultFetchSimpleConsumer]), Matchers.eq(tp), Matchers.eq(null))).thenReturn(1492l)\n+    when(mockOffsetGetter.getNextOffset(any(classOf[DefaultFetchSimpleConsumer]), Matchers.eq(tp), Matchers.eq(Option(null)))).thenReturn(1492l)\n \n     var callsToCreateSimpleConsumer = 0\n     val mockSimpleConsumer = mock(classOf[DefaultFetchSimpleConsumer])\n@@ -256,7 +256,7 @@ class TestBrokerProxy extends Logging {\n       }\n     }\n \n-    bp.addTopicPartition(tp, \"earliest\")\n+    bp.addTopicPartition(tp, Option(\"earliest\"))\n     bp.start\n     countdownLatch.await()\n     bp.stop",
                "raw_url": "https://github.com/apache/samza/raw/bda8df6e21c02060831ca05a1a23af1685f0ea07/samza-kafka/src/test/scala/org/apache/samza/system/kafka/TestBrokerProxy.scala",
                "sha": "9a3a29edf89244fa0bb8877a747446ec0b9dedc1",
                "status": "modified"
            }
        ],
        "message": "SAMZA-86; Convert GetOffset.getNextOffset to use Option instead of null when no offset is available. Applying patch again, with fixed NPE issue.",
        "parent": "https://github.com/apache/samza/commit/df6bd392acb5dd1600d191a5157dd6cd54519463",
        "repo": "samza",
        "unit_tests": [
            "TestKafkaSystemConsumer.java"
        ]
    },
    "samza_e11ccd2": {
        "bug_id": "samza_e11ccd2",
        "commit": "https://github.com/apache/samza/commit/e11ccd241c6dddd1ffb25e5debd4a639930a4660",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/samza/blob/e11ccd241c6dddd1ffb25e5debd4a639930a4660/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerIds.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerIds.java?ref=e11ccd241c6dddd1ffb25e5debd4a639930a4660",
                "deletions": 9,
                "filename": "samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerIds.java",
                "patch": "@@ -46,12 +46,6 @@ public GroupByContainerIds(int count) {\n \n   @Override\n   public Set<ContainerModel> group(Set<TaskModel> tasks) {\n-    if (tasks.isEmpty())\n-      throw new IllegalArgumentException(\"cannot group an empty set\");\n-\n-    if (startContainerCount > tasks.size())\n-      throw new IllegalArgumentException(\"number of containers=\"  + startContainerCount + \" is bigger than number of tasks=\" + tasks.size());\n-\n     List<String> containerIds = new ArrayList<>(startContainerCount);\n     for (int i = 0; i < startContainerCount; i++) {\n       containerIds.add(String.valueOf(i));\n@@ -60,16 +54,19 @@ public GroupByContainerIds(int count) {\n   }\n \n   public Set<ContainerModel> group(Set<TaskModel> tasks, List<String> containersIds) {\n+    if (containersIds == null)\n+      return this.group(tasks);\n+\n+    if (containersIds.isEmpty())\n+      throw new IllegalArgumentException(\"Must have at least one container\");\n+\n     if (tasks.isEmpty())\n       throw new IllegalArgumentException(\"cannot group an empty set. containersIds=\" + Arrays\n           .toString(containersIds.toArray()));\n \n     if (containersIds.size() > tasks.size())\n       throw new IllegalArgumentException(\"number of containers \"  + containersIds.size() + \" is bigger than number of tasks \" + tasks.size());\n \n-    if (containersIds == null)\n-      return this.group(tasks);\n-\n     int containerCount = containersIds.size();\n \n     // Sort tasks by taskName.",
                "raw_url": "https://github.com/apache/samza/raw/e11ccd241c6dddd1ffb25e5debd4a639930a4660/samza-core/src/main/java/org/apache/samza/container/grouper/task/GroupByContainerIds.java",
                "sha": "651dca7a977d97d154a6b86ad8a315d78e4dac18",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/samza/blob/e11ccd241c6dddd1ffb25e5debd4a639930a4660/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java?ref=e11ccd241c6dddd1ffb25e5debd4a639930a4660",
                "deletions": 0,
                "filename": "samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.samza.container.grouper.task;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -113,6 +114,40 @@ public void testGroupHappyPath() {\n     assertTrue(container1.getTasks().containsKey(getTaskName(3)));\n   }\n \n+  @Test\n+  public void testGroupWithNullContainerIds() {\n+    Set<TaskModel> taskModels = generateTaskModels(5);\n+\n+    Set<ContainerModel> containers = buildSimpleGrouper(2).group(taskModels, null);\n+\n+    Map<String, ContainerModel> containersMap = new HashMap<>();\n+    for (ContainerModel container : containers) {\n+      containersMap.put(container.getProcessorId(), container);\n+    }\n+\n+    assertEquals(2, containers.size());\n+    ContainerModel container0 = containersMap.get(\"0\");\n+    ContainerModel container1 = containersMap.get(\"1\");\n+    assertNotNull(container0);\n+    assertNotNull(container1);\n+    assertEquals(\"0\", container0.getProcessorId());\n+    assertEquals(\"1\", container1.getProcessorId());\n+    assertEquals(3, container0.getTasks().size());\n+    assertEquals(2, container1.getTasks().size());\n+    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\n+    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\n+    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\n+    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\n+    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testGroupWithEmptyContainerIds() {\n+    Set<TaskModel> taskModels = generateTaskModels(5);\n+\n+    buildSimpleGrouper(2).group(taskModels, Collections.emptyList());\n+  }\n+\n   @Test\n   public void testGroupHappyPathWithListOfContainers() {\n     Set<TaskModel> taskModels = generateTaskModels(5);",
                "raw_url": "https://github.com/apache/samza/raw/e11ccd241c6dddd1ffb25e5debd4a639930a4660/samza-core/src/test/java/org/apache/samza/container/grouper/task/TestGroupByContainerIds.java",
                "sha": "cd2cc3d27e3e244dd0399ff1490d8cd38ef267d8",
                "status": "modified"
            }
        ],
        "message": "SAMZA-1347: GroupByContainerIds NPE if containerIds list is null\n\nAuthor: Jacob Maes <jmaes@linkedin.com>\n\nReviewers: Boris Shkolnik <boryas@apache.org>\n\nCloses #233 from jmakes/samza-1347",
        "parent": "https://github.com/apache/samza/commit/26dc77b08c216596dc2b727e90308d14c77fc7a1",
        "repo": "samza",
        "unit_tests": [
            "TestGroupByContainerIds.java"
        ]
    },
    "samza_ebb1b7f": {
        "bug_id": "samza_ebb1b7f",
        "commit": "https://github.com/apache/samza/commit/ebb1b7fea2518e827e589fe8523089322c68c9bf",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/docs/learn/documentation/versioned/container/metrics-table.html",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/docs/learn/documentation/versioned/container/metrics-table.html?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 2,
                "filename": "docs/learn/documentation/versioned/container/metrics-table.html",
                "patch": "@@ -142,6 +142,7 @@ <h1>Samza Metrics Reference</h1>\n     <li><a href=\"#bootstrapping-chooser-metrics\">BootstrappingChooserMetrics</a></li>\n     <li><a href=\"#hdfs-system-producer-metrics\">HdfsSystemProducerMetrics</a></li>\n     <li><a href=\"#elasticsearch-system-producer-metrics\">ElasticsearchSystemProducerMetrics</a></li>\n+    <li><a href=\"#zookeeper-client-metrics\">ZookeeperClientMetrics</a></li>\n     <li><a href=\"#zookeeper-job-coordinator-metrics\">ZkJobCoordinatorMetrics</a></li>\n </ul>\n <p>Words highlighted like <span class=\"system\">this</span> are placeholders for your own variable names defined in configuration file or system variables defined while starting the job.</p>\n@@ -894,7 +895,7 @@ <h1>Samza Metrics Reference</h1>\n     </tr>\n \n     <tr>\n-        <th colspan=\"2\" class=\"section\" id=\"zookeeper-job-coordinator-metrics\">org.apache.samza.zk.ZkJobCoordinatorMetrics</th>\n+        <th colspan=\"2\" class=\"section\" id=\"zookeeper-client-metrics\">org.apache.samza.zk.ZkUtilsMetrics</th>\n     </tr>\n     <tr>\n         <td>reads</td>\n@@ -909,9 +910,12 @@ <h1>Samza Metrics Reference</h1>\n         <td>Number of subscriptions to znodes in Zookeeper</td>\n     </tr>\n     <tr>\n-        <td>zk-connection-error</td>\n+        <td>zk-connection-errors</td>\n         <td>Number of Zookeeper connection errors</td>\n     </tr>\n+    <tr>\n+        <th colspan=\"2\" class=\"section\" id=\"zookeeper-job-coordinator-metrics\">org.apache.samza.zk.ZkJobCoordinatorMetrics</th>\n+    </tr>\n     <tr>\n         <td>is-leader</td>\n         <td>Denotes if the processor is a leader or not</td>",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/docs/learn/documentation/versioned/container/metrics-table.html",
                "sha": "e504fa3fbdd9898c3595b80341cb4d1f71eabe8d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkCoordinationServiceFactory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/zk/ZkCoordinationServiceFactory.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 2,
                "filename": "samza-core/src/main/java/org/apache/samza/zk/ZkCoordinationServiceFactory.java",
                "patch": "@@ -25,21 +25,22 @@\n import org.apache.samza.config.ZkConfig;\n import org.apache.samza.coordinator.CoordinationServiceFactory;\n import org.apache.samza.coordinator.CoordinationUtils;\n+import org.apache.samza.util.NoOpMetricsRegistry;\n import org.apache.zookeeper.client.ConnectStringParser;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n \n public class ZkCoordinationServiceFactory implements CoordinationServiceFactory {\n-  private final static Logger LOG = LoggerFactory.getLogger(ZkCoordinationServiceFactory.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(ZkCoordinationServiceFactory.class);\n \n   public CoordinationUtils getCoordinationService(String groupId, String participantId, Config config) {\n     ZkConfig zkConfig = new ZkConfig(config);\n \n     ZkClient zkClient =\n         createZkClient(zkConfig.getZkConnect(), zkConfig.getZkSessionTimeoutMs(), zkConfig.getZkConnectionTimeoutMs());\n \n-    ZkUtils zkUtils = new ZkUtils(new ZkKeyBuilder(groupId), zkClient, zkConfig.getZkConnectionTimeoutMs());\n+    ZkUtils zkUtils = new ZkUtils(new ZkKeyBuilder(groupId), zkClient, zkConfig.getZkConnectionTimeoutMs(), new NoOpMetricsRegistry());\n \n     return new ZkCoordinationUtils(participantId, zkConfig, zkUtils);\n   }",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkCoordinationServiceFactory.java",
                "sha": "d0633a831607bf0f82adfcda1f4cecc97e70cf22",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 1,
                "filename": "samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java",
                "patch": "@@ -80,7 +80,7 @@ public ZkJobCoordinator(Config config, MetricsRegistry metricsRegistry) {\n             zkConfig.getZkConnect(),\n             zkConfig.getZkSessionTimeoutMs(),\n             zkConfig.getZkConnectionTimeoutMs()),\n-        zkConfig.getZkConnectionTimeoutMs(), metrics);\n+        zkConfig.getZkConnectionTimeoutMs(), metricsRegistry);\n \n     this.processorId = createProcessorId(config);\n     LeaderElector leaderElector = new ZkLeaderElector(processorId, zkUtils);",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinator.java",
                "sha": "94c3054196a479936092206a1c22d5640b0dcd9d",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinatorMetrics.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinatorMetrics.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 9,
                "filename": "samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinatorMetrics.java",
                "patch": "@@ -31,11 +31,6 @@\n \n   private final MetricsRegistry metricsRegistry;\n \n-  public final Counter reads;\n-  public final Counter writes;\n-  public final Counter subscriptions;\n-  public final Counter zkConnectionError;\n-\n   /**\n    * Denotes if the processor is a leader or not\n    */\n@@ -65,10 +60,6 @@\n   public ZkJobCoordinatorMetrics(MetricsRegistry metricsRegistry) {\n     super(metricsRegistry);\n     this.metricsRegistry = metricsRegistry;\n-    this.reads = newCounter(\"reads\");\n-    this.writes = newCounter(\"writes\");\n-    this.subscriptions = newCounter(\"subscriptions\");\n-    this.zkConnectionError = newCounter(\"zk-connection-error\");\n     this.isLeader = newGauge(\"is-leader\", false);\n     this.barrierCreation = newCounter(\"barrier-creation\");\n     this.barrierStateChange = newCounter(\"barrier-state-change\");",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkJobCoordinatorMetrics.java",
                "sha": "3d00897a42a4a2e6597af88095c19138d9f35f0e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkUtils.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/zk/ZkUtils.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 10,
                "filename": "samza-core/src/main/java/org/apache/samza/zk/ZkUtils.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.I0Itec.zkclient.exception.ZkInterruptedException;\n import org.apache.samza.SamzaException;\n import org.apache.samza.job.model.JobModel;\n+import org.apache.samza.metrics.MetricsRegistry;\n import org.apache.samza.serializers.model.SamzaObjectMapper;\n import org.apache.zookeeper.data.Stat;\n import org.codehaus.jackson.map.ObjectMapper;\n@@ -65,19 +66,13 @@\n   private volatile String ephemeralPath = null;\n   private final ZkKeyBuilder keyBuilder;\n   private final int connectionTimeoutMs;\n-  private ZkJobCoordinatorMetrics metrics;\n+  private final ZkUtilsMetrics metrics;\n \n-  public ZkUtils(ZkKeyBuilder zkKeyBuilder, ZkClient zkClient, int connectionTimeoutMs) {\n+  public ZkUtils(ZkKeyBuilder zkKeyBuilder, ZkClient zkClient, int connectionTimeoutMs, MetricsRegistry metricsRegistry) {\n     this.keyBuilder = zkKeyBuilder;\n     this.connectionTimeoutMs = connectionTimeoutMs;\n     this.zkClient = zkClient;\n-  }\n-\n-  public ZkUtils(ZkKeyBuilder zkKeyBuilder, ZkClient zkClient, int connectionTimeoutMs, ZkJobCoordinatorMetrics metrics) {\n-    this.keyBuilder = zkKeyBuilder;\n-    this.connectionTimeoutMs = connectionTimeoutMs;\n-    this.zkClient = zkClient;\n-    this.metrics = metrics;\n+    this.metrics = new ZkUtilsMetrics(metricsRegistry);\n   }\n \n   public void connect() throws ZkInterruptedException {\n@@ -269,7 +264,9 @@ public JobModel getJobModel(String jobModelVersion) {\n    * @return jobmodel version as a string\n    */\n   public String getJobModelVersion() {\n-    return zkClient.<String>readData(keyBuilder.getJobModelVersionPath());\n+    String jobModelVersion = zkClient.readData(keyBuilder.getJobModelVersionPath());\n+    metrics.reads.inc();\n+    return jobModelVersion;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkUtils.java",
                "sha": "ecf118bed37d45756ef89a27fae5544084d9df9f",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkUtilsMetrics.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/main/java/org/apache/samza/zk/ZkUtilsMetrics.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 0,
                "filename": "samza-core/src/main/java/org/apache/samza/zk/ZkUtilsMetrics.java",
                "patch": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.samza.zk;\n+\n+import org.apache.samza.metrics.Counter;\n+import org.apache.samza.metrics.MetricsBase;\n+import org.apache.samza.metrics.MetricsRegistry;\n+\n+/**\n+ * Contains all the metrics published by {@link ZkUtils}.\n+ */\n+public class ZkUtilsMetrics extends MetricsBase {\n+  /**\n+   * Number of data reads from zookeeper.\n+   */\n+  public final Counter reads;\n+\n+  /**\n+   * Number of data writes into zookeeper.\n+   */\n+  public final Counter writes;\n+\n+  /**\n+   * Number of subscriptions created with zookeeper.\n+   */\n+  public final Counter subscriptions;\n+\n+  /**\n+   * Number of zookeeper connection errors in ZkClient.\n+   */\n+  public final Counter zkConnectionError;\n+\n+  public ZkUtilsMetrics(MetricsRegistry metricsRegistry) {\n+    super(metricsRegistry);\n+    this.reads = newCounter(\"reads\");\n+    this.writes = newCounter(\"writes\");\n+    this.subscriptions = newCounter(\"subscriptions\");\n+    this.zkConnectionError = newCounter(\"zk-connection-errors\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/main/java/org/apache/samza/zk/ZkUtilsMetrics.java",
                "sha": "b9f4aa8274257283eebe0dfafb499ba77dd5d539",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkBarrierForVersionUpgrade.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/zk/TestZkBarrierForVersionUpgrade.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 2,
                "filename": "samza-core/src/test/java/org/apache/samza/zk/TestZkBarrierForVersionUpgrade.java",
                "patch": "@@ -53,9 +53,9 @@ public static void test() {\n   @Before\n   public void testSetup() {\n     ZkClient zkClient = new ZkClient(testZkConnectionString, ZkConfig.DEFAULT_SESSION_TIMEOUT_MS, ZkConfig.DEFAULT_CONNECTION_TIMEOUT_MS);\n-    this.zkUtils = new ZkUtils(new ZkKeyBuilder(\"group1\"), zkClient, ZkConfig.DEFAULT_CONNECTION_TIMEOUT_MS, new ZkJobCoordinatorMetrics(new NoOpMetricsRegistry()));\n+    this.zkUtils = new ZkUtils(new ZkKeyBuilder(\"group1\"), zkClient, ZkConfig.DEFAULT_CONNECTION_TIMEOUT_MS, new NoOpMetricsRegistry());\n     ZkClient zkClient1 = new ZkClient(testZkConnectionString, ZkConfig.DEFAULT_SESSION_TIMEOUT_MS, ZkConfig.DEFAULT_CONNECTION_TIMEOUT_MS);\n-    this.zkUtils1 = new ZkUtils(new ZkKeyBuilder(\"group1\"), zkClient1, ZkConfig.DEFAULT_CONNECTION_TIMEOUT_MS, new ZkJobCoordinatorMetrics(new NoOpMetricsRegistry()));\n+    this.zkUtils1 = new ZkUtils(new ZkKeyBuilder(\"group1\"), zkClient1, ZkConfig.DEFAULT_CONNECTION_TIMEOUT_MS, new NoOpMetricsRegistry());\n   }\n \n   @After",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkBarrierForVersionUpgrade.java",
                "sha": "3dd1bd50e844e2722f0d7c6a94bce0f184d19f33",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkLeaderElector.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/zk/TestZkLeaderElector.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 1,
                "filename": "samza-core/src/test/java/org/apache/samza/zk/TestZkLeaderElector.java",
                "patch": "@@ -437,6 +437,6 @@ private ZkUtils getZkUtilsWithNewClient() {\n     return new ZkUtils(\n         KEY_BUILDER,\n         zkClient,\n-        CONNECTION_TIMEOUT_MS, new ZkJobCoordinatorMetrics(new NoOpMetricsRegistry()));\n+        CONNECTION_TIMEOUT_MS, new NoOpMetricsRegistry());\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkLeaderElector.java",
                "sha": "3ff91757d4bc3c76122ef26340c2648a1d5a418a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkProcessorLatch.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/zk/TestZkProcessorLatch.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 1,
                "filename": "samza-core/src/test/java/org/apache/samza/zk/TestZkProcessorLatch.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.I0Itec.zkclient.ZkClient;\n import org.apache.samza.coordinator.Latch;\n import org.apache.samza.testUtils.EmbeddedZookeeper;\n+import org.apache.samza.util.NoOpMetricsRegistry;\n import org.junit.After;\n import org.junit.AfterClass;\n import org.junit.Assert;\n@@ -219,6 +220,7 @@ private ZkUtils getZkUtilsWithNewClient(String processorId) {\n     return new ZkUtils(\n         KEY_BUILDER,\n         zkClient,\n-        CONNECTION_TIMEOUT_MS);\n+        CONNECTION_TIMEOUT_MS,\n+        new NoOpMetricsRegistry());\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkProcessorLatch.java",
                "sha": "b2a553321399ae53d057775095aa83ef2c408917",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkUtils.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-core/src/test/java/org/apache/samza/zk/TestZkUtils.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 2,
                "filename": "samza-core/src/test/java/org/apache/samza/zk/TestZkUtils.java",
                "patch": "@@ -71,7 +71,7 @@ public void testSetup() {\n     zkUtils = new ZkUtils(\n         KEY_BUILDER,\n         zkClient,\n-        SESSION_TIMEOUT_MS, new ZkJobCoordinatorMetrics(new NoOpMetricsRegistry()));\n+        SESSION_TIMEOUT_MS, new NoOpMetricsRegistry());\n \n     zkUtils.connect();\n   }\n@@ -110,7 +110,7 @@ public void testGetProcessorsIDs() {\n     zkUtils.registerProcessorAndGetId(new ProcessorData(\"host1\", \"1\"));\n     List<String> l = zkUtils.getSortedActiveProcessorsIDs();\n     Assert.assertEquals(1, l.size());\n-    new ZkUtils(KEY_BUILDER, zkClient, SESSION_TIMEOUT_MS).registerProcessorAndGetId(new ProcessorData(\"host2\", \"2\"));\n+    new ZkUtils(KEY_BUILDER, zkClient, SESSION_TIMEOUT_MS, new NoOpMetricsRegistry()).registerProcessorAndGetId(new ProcessorData(\"host2\", \"2\"));\n     l = zkUtils.getSortedActiveProcessorsIDs();\n     Assert.assertEquals(2, l.size());\n ",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-core/src/test/java/org/apache/samza/zk/TestZkUtils.java",
                "sha": "a33bf032af834e422741e4a203907c00940ce447",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-test/src/test/java/org/apache/samza/test/processor/TestZkLocalApplicationRunner.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-test/src/test/java/org/apache/samza/test/processor/TestZkLocalApplicationRunner.java?ref=ebb1b7fea2518e827e589fe8523089322c68c9bf",
                "deletions": 4,
                "filename": "samza-test/src/test/java/org/apache/samza/test/processor/TestZkLocalApplicationRunner.java",
                "patch": "@@ -54,7 +54,6 @@\n import org.apache.samza.test.StandaloneIntegrationTestHarness;\n import org.apache.samza.test.StandaloneTestUtils;\n import org.apache.samza.util.NoOpMetricsRegistry;\n-import org.apache.samza.zk.ZkJobCoordinatorMetrics;\n import org.apache.samza.zk.ZkKeyBuilder;\n import org.apache.samza.zk.ZkUtils;\n import org.junit.Rule;\n@@ -94,7 +93,6 @@\n   private LocalApplicationRunner applicationRunner1;\n   private LocalApplicationRunner applicationRunner2;\n   private LocalApplicationRunner applicationRunner3;\n-  private ZkJobCoordinatorMetrics zkJobCoordinatorMetrics;\n \n   // Set 90 seconds as max execution time for each test.\n   @Rule\n@@ -110,8 +108,7 @@ public void setUp() {\n     outputKafkaTopic = String.format(\"test-output-topic-%s\", uniqueTestId);\n     ZkClient zkClient = new ZkClient(zkConnect());\n     ZkKeyBuilder zkKeyBuilder = new ZkKeyBuilder(String.format(\"app-%s-%s\", testStreamAppName, testStreamAppId));\n-    zkJobCoordinatorMetrics = new ZkJobCoordinatorMetrics(new NoOpMetricsRegistry());\n-    zkUtils = new ZkUtils(zkKeyBuilder, zkClient, ZK_CONNECTION_TIMEOUT_MS, zkJobCoordinatorMetrics);\n+    zkUtils = new ZkUtils(zkKeyBuilder, zkClient, ZK_CONNECTION_TIMEOUT_MS, new NoOpMetricsRegistry());\n     zkUtils.connect();\n \n     // Set up stream application configs with different processorIds and same testStreamAppName, testStreamAppId.",
                "raw_url": "https://github.com/apache/samza/raw/ebb1b7fea2518e827e589fe8523089322c68c9bf/samza-test/src/test/java/org/apache/samza/test/processor/TestZkLocalApplicationRunner.java",
                "sha": "2d5da2b3ee647d6781503033121ce158ea8cf675",
                "status": "modified"
            }
        ],
        "message": "SAMZA-1324: Fix NullPointerException in ZkUtils api's.\n\nProblem:\nRead/Write api methods in ZkUtils updates counters/timers in `metrics` field. In a ZkUtils constructor this fields is not initialized properly. Java default for uninitialized field is null resulting in NPE.\n\nFix:\nInitialize private fields of ZkUtils class with appropriate defaults.\n\nAuthor: Shanthoosh Venkataraman <svenkataraman@linkedin.com>\n\nReviewers: Navina Ramesh <navina@apache.org>\n\nCloses #235 from shanthoosh/fix_zkutils_api",
        "parent": "https://github.com/apache/samza/commit/be989935c2fcc6ead114ce308ebaa93855ecb582",
        "repo": "samza",
        "unit_tests": [
            "TestZkJobCoordinator.java",
            "TestZkUtils.java"
        ]
    },
    "samza_fda1e37": {
        "bug_id": "samza_fda1e37",
        "commit": "https://github.com/apache/samza/commit/fda1e37d0862493cb59ed0907c597a2dc973ef6f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/build.gradle",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/build.gradle?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 1,
                "filename": "build.gradle",
                "patch": "@@ -202,7 +202,7 @@ project(':samza-azure') {\n \n   dependencies {\n     compile \"com.microsoft.azure:azure-storage:5.3.1\"\n-    compile \"com.microsoft.azure:azure-eventhubs:0.14.5\"\n+    compile \"com.microsoft.azure:azure-eventhubs:1.0.1\"\n     compile \"com.fasterxml.jackson.core:jackson-core:2.8.8\"\n     compile \"io.dropwizard.metrics:metrics-core:3.1.2\"\n     compile project(':samza-api')",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/build.gradle",
                "sha": "2f27a03163a2ce8b34e75456e48e6c2beee8553e",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/docs/learn/documentation/versioned/jobs/configuration-table.html",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/docs/learn/documentation/versioned/jobs/configuration-table.html?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 5,
                "filename": "docs/learn/documentation/versioned/jobs/configuration-table.html",
                "patch": "@@ -2279,29 +2279,47 @@ <h1>Samza Configuration Reference</h1>\n                 </tr>\n \n                 <tr>\n-                    <td class=\"property\" id=\"eventhub-stream-namespace\">systems.<span class=\"system\">system-name</span>.<br>streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.namespace</td>\n+                    <td class=\"property\" id=\"eventhub-stream-namespace\">streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.namespace</td>\n                     <td class=\"default\"></td>\n                     <td class=\"description\">Namespace of the associated <span class=\"stream\">stream-ids</span>. Required to access the Eventhubs entity per stream.</td>\n                 </tr>\n \n                 <tr>\n-                    <td class=\"property\" id=\"eventhub-stream-entity\">systems.<span class=\"system\">system-name</span>.<br>streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.entitypath</td>\n+                    <td class=\"property\" id=\"eventhub-stream-entity\">streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.entitypath</td>\n                     <td class=\"default\"></td>\n                     <td class=\"description\">Entity of the associated <span class=\"stream\">stream-ids</span>. Required to access the Eventhubs entity per stream.</td>\n                 </tr>\n \n                 <tr>\n-                    <td class=\"property\" id=\"eventhub-stream-sas-keyname\">systems.<span class=\"system\">system-name</span>.<br>streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.sas.keyname</td>\n+                    <td class=\"property\" id=\"eventhub-stream-sas-keyname\">sensitive.streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.sas.keyname</td>\n                     <td class=\"default\"></td>\n                     <td class=\"description\">SAS Keyname of the associated <span class=\"stream\">stream-ids</span>. Required to access the Eventhubs entity per stream.</td>\n                 </tr>\n \n                 <tr>\n-                    <td class=\"property\" id=\"eventhub-stream-sas-token\">systems.<span class=\"system\">system-name</span>.<br>streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.sas.token</td>\n+                    <td class=\"property\" id=\"eventhub-stream-sas-token\">sensitive.streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.sas.token</td>\n                     <td class=\"default\"></td>\n                     <td class=\"description\">SAS Token the associated <span class=\"stream\">stream-ids</span>. Required to access the Eventhubs entity per stream.</td>\n                 </tr>\n \n+                <tr>\n+                    <td class=\"property\" id=\"eventhub-client-threads\">streams.<span class=\"system\">stream-name</span>.<br>eventhubs.numClientThreads</td>\n+                    <td class=\"default\">10</td>\n+                    <td class=\"description\">Number of threads in thread pool that will be used by the EventHubClient. See <a href=\"https://docs.microsoft.com/en-us/java/api/com.microsoft.azure.eventhubs._event_hub_client.create\">here </a> for more details.</td>\n+                </tr>\n+\n+                <tr>\n+                    <td class=\"property\" id=\"eventhub-prefetch-count\">systems.<span class=\"system\">system-name</span>.<br>eventhubs.prefetchCount</td>\n+                    <td class=\"default\">999</td>\n+                    <td class=\"description\">Number of events that EventHub client should prefetch from the server. See <a href=\"https://docs.microsoft.com/en-us/java/api/com.microsoft.azure.eventhubs._partition_receiver.setprefetchcount\">here </a> for more details.</td>\n+                </tr>\n+\n+                <tr>\n+                    <td class=\"property\" id=\"eventhub-max-event-count\">systems.<span class=\"system\">system-name</span>.<br>eventhubs.maxEventCountPerPoll</td>\n+                    <td class=\"default\">50</td>\n+                    <td class=\"description\">Maximum number of events that EventHub client can return in a receive call. See <a href=\"https://docs.microsoft.com/en-us/java/api/com.microsoft.azure.eventhubs._partition_receive_handler.getmaxeventcount#com_microsoft_azure_eventhubs__partition_receive_handler_getMaxEventCount__\">here </a> for more details.</td>\n+                </tr>\n+\n                 <tr>\n                     <td class=\"property\" id=\"eventhub-runtime-timeout\">systems.<span class=\"system\">system-name</span>.<br>eventhubs.runtime.info.timeout</td>\n                     <td class=\"default\">60000</td>\n@@ -2333,7 +2351,7 @@ <h1>Samza Configuration Reference</h1>\n                 </tr>\n \n                 <tr>\n-                    <td class=\"property\" id=\"eventhub-consumer-group\">systems.<span class=\"system\">system-name</span>.<br>streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.consumer.group</td>\n+                    <td class=\"property\" id=\"eventhub-consumer-group\">streams.<span class=\"stream\">stream-id</span>.<br>eventhubs.consumer.group</td>\n                     <td class=\"default\"><code>$Default</code></td>\n                     <td class=\"description\">\n                         Consumer only config. Set the consumer group from the upstream Eventhub that the consumer is part of. Defaults to the <code>$Default</code> group that is initially present in all Eventhub entities (unless removed)",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/docs/learn/documentation/versioned/jobs/configuration-table.html",
                "sha": "4495cb17317c94851efcec3d94153a03b3c69efd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubClientManagerFactory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubClientManagerFactory.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 1,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubClientManagerFactory.java",
                "patch": "@@ -26,7 +26,8 @@ public EventHubClientManager getEventHubClientManager(String systemName, String\n     String entityPath = config.getStreamEntityPath(systemName, streamName);\n     String sasKeyName = config.getStreamSasKeyName(systemName, streamName);\n     String sasToken = config.getStreamSasToken(systemName, streamName);\n+    int numClientThreads = config.getNumClientThreads(systemName);\n \n-    return new SamzaEventHubClientManager(eventHubNamespace, entityPath, sasKeyName, sasToken);\n+    return new SamzaEventHubClientManager(eventHubNamespace, entityPath, sasKeyName, sasToken, numClientThreads);\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubClientManagerFactory.java",
                "sha": "879a07fc09ce6cdc27321f2c171cf28d106ca58e",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubConfig.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubConfig.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 0,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubConfig.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.samza.system.eventhub;\n \n import com.microsoft.azure.eventhubs.EventHubClient;\n+import com.microsoft.azure.eventhubs.PartitionReceiver;\n import org.apache.samza.SamzaException;\n import org.apache.samza.config.Config;\n import org.apache.samza.config.MapConfig;\n@@ -49,6 +50,15 @@\n   public static final String CONFIG_STREAM_CONSUMER_GROUP = \"streams.%s.eventhubs.consumer.group\";\n   public static final String DEFAULT_CONFIG_STREAM_CONSUMER_GROUP = EventHubClient.DEFAULT_CONSUMER_GROUP_NAME;\n \n+  public static final String CONFIG_SYSTEM_NUM_CLIENT_THREADS = \"streams.%s.eventhubs.numClientThreads\";\n+  public static final int DEFAULT_CONFIG_SYSTEM_NUM_CLIENT_THREADS = 10;\n+\n+  public static final String CONFIG_PREFETCH_COUNT = \"systems.%s.eventhubs.prefetchCount\";\n+  public static final int DEFAULT_CONFIG_PREFETCH_COUNT = PartitionReceiver.DEFAULT_PREFETCH_COUNT;\n+\n+  public static final String CONFIG_MAX_EVENT_COUNT_PER_POLL = \"systems.%s.eventhubs.maxEventCountPerPoll\";\n+  public static final int DEFAULT_CONFIG_MAX_EVENT_COUNT_PER_POLL = 50;\n+\n   public static final String CONFIG_PRODUCER_PARTITION_METHOD = \"systems.%s.eventhubs.partition.method\";\n   public static final String DEFAULT_CONFIG_PRODUCER_PARTITION_METHOD = EventHubSystemProducer\n           .PartitioningMethod.EVENT_HUB_HASHING.name();\n@@ -143,6 +153,34 @@ public String getStreamEntityPath(String systemName, String streamName) {\n             \"EntityPath\", systemName, streamName);\n   }\n \n+  /**\n+   * Get the number of client threads, This is used to create the ThreadPool executor that is passed to the\n+   * {@link EventHubClient#create}\n+   * @param systemName Name of the system.\n+   * @return Num of client threads to use.\n+   */\n+  public Integer getNumClientThreads(String systemName) {\n+    return getInt(String.format(CONFIG_SYSTEM_NUM_CLIENT_THREADS, systemName), DEFAULT_CONFIG_SYSTEM_NUM_CLIENT_THREADS);\n+  }\n+\n+  /**\n+   * Get the max event count returned per poll\n+   * @param systemName Name of the system\n+   * @return Max number of events returned per poll\n+   */\n+  public Integer getMaxEventCountPerPoll(String systemName) {\n+    return getInt(String.format(CONFIG_MAX_EVENT_COUNT_PER_POLL, systemName), DEFAULT_CONFIG_MAX_EVENT_COUNT_PER_POLL);\n+  }\n+\n+  /**\n+   * Get the per partition prefetch count for the event hub client\n+   * @param systemName Name of the system.\n+   * @return Per partition Prefetch count for the event hub client.\n+   */\n+  public Integer getPrefetchCount(String systemName) {\n+    return getInt(String.format(CONFIG_PREFETCH_COUNT, systemName), DEFAULT_CONFIG_PREFETCH_COUNT);\n+  }\n+\n   /**\n    * Get the EventHubs max Message size\n    *",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/EventHubConfig.java",
                "sha": "6639dd8c4fcdc7797e7cf670f4a2dd4f6f6f6461",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/SamzaEventHubClientManager.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/SamzaEventHubClientManager.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 12,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/SamzaEventHubClientManager.java",
                "patch": "@@ -19,12 +19,15 @@\n \n package org.apache.samza.system.eventhub;\n \n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import com.microsoft.azure.eventhubs.EventHubClient;\n-import com.microsoft.azure.servicebus.ClientConstants;\n-import com.microsoft.azure.servicebus.ConnectionStringBuilder;\n-import com.microsoft.azure.servicebus.RetryExponential;\n-import com.microsoft.azure.servicebus.RetryPolicy;\n-import com.microsoft.azure.servicebus.ServiceBusException;\n+import com.microsoft.azure.eventhubs.impl.ClientConstants;\n+import com.microsoft.azure.eventhubs.ConnectionStringBuilder;\n+import com.microsoft.azure.eventhubs.impl.RetryExponential;\n+import com.microsoft.azure.eventhubs.RetryPolicy;\n+import com.microsoft.azure.eventhubs.EventHubException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n import org.apache.samza.SamzaException;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -43,6 +46,7 @@\n   private static final Duration MAX_RETRY_BACKOFF = Duration.ofMillis(11000);\n   private static final int MAX_RETRY_COUNT = 100;\n   private static final String SAMZA_EVENTHUB_RETRY = \"SAMZA_CONNECTOR_RETRY\";\n+  private final int numClientThreads;\n \n   private EventHubClient eventHubClient;\n \n@@ -51,30 +55,38 @@\n   private final String sasKeyName;\n   private final String sasKey;\n   private final RetryPolicy retryPolicy;\n+  private ExecutorService eventHubClientExecutor;\n \n-  public SamzaEventHubClientManager(String eventHubNamespace, String entityPath, String sasKeyName, String sasKey) {\n+  public SamzaEventHubClientManager(String eventHubNamespace, String entityPath, String sasKeyName, String sasKey,\n+      Integer numClientThreads) {\n     this(eventHubNamespace, entityPath, sasKeyName, sasKey,\n-            new RetryExponential(MIN_RETRY_BACKOFF, MAX_RETRY_BACKOFF, MAX_RETRY_COUNT, SAMZA_EVENTHUB_RETRY));\n+            new RetryExponential(MIN_RETRY_BACKOFF, MAX_RETRY_BACKOFF, MAX_RETRY_COUNT, SAMZA_EVENTHUB_RETRY), numClientThreads);\n   }\n \n   public SamzaEventHubClientManager(String eventHubNamespace, String entityPath, String sasKeyName, String sasKey,\n-                                    RetryPolicy retryPolicy) {\n+                                    RetryPolicy retryPolicy, int numClientThreads) {\n     this.eventHubNamespace = eventHubNamespace;\n     this.entityPath = entityPath;\n     this.sasKeyName = sasKeyName;\n     this.sasKey = sasKey;\n     this.retryPolicy = retryPolicy;\n+    this.numClientThreads = numClientThreads;\n   }\n \n   @Override\n   public void init() {\n     String remoteHost = String.format(EVENTHUB_REMOTE_HOST_FORMAT, eventHubNamespace);\n     try {\n-      ConnectionStringBuilder connectionStringBuilder =\n-              new ConnectionStringBuilder(eventHubNamespace, entityPath, sasKeyName, sasKey);\n+      ConnectionStringBuilder connectionStringBuilder = new ConnectionStringBuilder()\n+          .setNamespaceName(eventHubNamespace)\n+          .setEventHubName(entityPath)\n+          .setSasKeyName(sasKeyName)\n+          .setSasKey(sasKey);\n \n-      eventHubClient = EventHubClient.createFromConnectionStringSync(connectionStringBuilder.toString(), retryPolicy);\n-    } catch (IOException | ServiceBusException e) {\n+      ThreadFactoryBuilder threadFactoryBuilder = new ThreadFactoryBuilder().setNameFormat(\"Samza EventHubClient Thread-%d\");\n+      eventHubClientExecutor = Executors.newFixedThreadPool(numClientThreads, threadFactoryBuilder.build());\n+      eventHubClient = EventHubClient.createSync(connectionStringBuilder.toString(), retryPolicy, eventHubClientExecutor);\n+    } catch (IOException | EventHubException e) {\n       String msg = String.format(\"Creation of EventHub client failed for eventHub EntityPath: %s on remote host %s:%d\",\n               entityPath, remoteHost, ClientConstants.AMQPS_PORT);\n       LOG.error(msg, e);\n@@ -92,6 +104,7 @@ public void close(long timeoutMS) {\n     try {\n       if (timeoutMS == EventHubClientManager.BLOCK_UNTIL_CLOSE) {\n         eventHubClient.closeSync();\n+        eventHubClientExecutor.shutdown();\n       } else {\n         CompletableFuture<Void> future = eventHubClient.close();\n         future.get(timeoutMS, TimeUnit.MILLISECONDS);",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/SamzaEventHubClientManager.java",
                "sha": "a884a794ec64ae6ee9e48a2afe8c1866d3f24ed7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/admin/EventHubSystemAdmin.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/admin/EventHubSystemAdmin.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 4,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/admin/EventHubSystemAdmin.java",
                "patch": "@@ -19,8 +19,8 @@\n \n package org.apache.samza.system.eventhub.admin;\n \n-import com.microsoft.azure.eventhubs.EventHubPartitionRuntimeInformation;\n import com.microsoft.azure.eventhubs.EventHubRuntimeInformation;\n+import com.microsoft.azure.eventhubs.PartitionRuntimeInformation;\n import org.apache.samza.Partition;\n import org.apache.samza.SamzaException;\n import org.apache.samza.system.SystemAdmin;\n@@ -144,10 +144,10 @@ private EventHubClientManager getOrCreateStreamEventHubClient(String streamName)\n   private Map<Partition, SystemStreamPartitionMetadata> getPartitionMetadata(String streamName, String[] partitionIds) {\n     EventHubClientManager eventHubClientManager = getOrCreateStreamEventHubClient(streamName);\n     Map<Partition, SystemStreamPartitionMetadata> sspMetadataMap = new HashMap<>();\n-    Map<String, CompletableFuture<EventHubPartitionRuntimeInformation>> ehRuntimeInfos = new HashMap<>();\n+    Map<String, CompletableFuture<PartitionRuntimeInformation>> ehRuntimeInfos = new HashMap<>();\n \n     for (String partition : partitionIds) {\n-      CompletableFuture<EventHubPartitionRuntimeInformation> partitionRuntimeInfo = eventHubClientManager\n+      CompletableFuture<PartitionRuntimeInformation> partitionRuntimeInfo = eventHubClientManager\n               .getEventHubClient()\n               .getPartitionRuntimeInformation(partition);\n \n@@ -157,7 +157,7 @@ private EventHubClientManager getOrCreateStreamEventHubClient(String streamName)\n     ehRuntimeInfos.forEach((partitionId, ehPartitionRuntimeInfo) -> {\n         try {\n           long timeoutMs = eventHubConfig.getRuntimeInfoWaitTimeMS(systemName);\n-          EventHubPartitionRuntimeInformation ehPartitionInfo = ehPartitionRuntimeInfo.get(timeoutMs, TimeUnit.MILLISECONDS);\n+          PartitionRuntimeInformation ehPartitionInfo = ehPartitionRuntimeInfo.get(timeoutMs, TimeUnit.MILLISECONDS);\n \n           // Set offsets\n           String startingOffset = EventHubSystemConsumer.START_OF_STREAM;",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/admin/EventHubSystemAdmin.java",
                "sha": "acb1775354c15173336db9855515fc5ae6972e32",
                "status": "modified"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/consumer/EventHubSystemConsumer.java",
                "changes": 179,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/consumer/EventHubSystemConsumer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 81,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/consumer/EventHubSystemConsumer.java",
                "patch": "@@ -1,45 +1,31 @@\n /*\n-* Licensed to the Apache Software Foundation (ASF) under one\n-* or more contributor license agreements.  See the NOTICE file\n-* distributed with this work for additional information\n-* regarding copyright ownership.  The ASF licenses this file\n-* to you under the Apache License, Version 2.0 (the\n-* \"License\"); you may not use this file except in compliance\n-* with the License.  You may obtain a copy of the License at\n-*\n-*   http://www.apache.org/licenses/LICENSE-2.0\n-*\n-* Unless required by applicable law or agreed to in writing,\n-* software distributed under the License is distributed on an\n-* \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-* KIND, either express or implied.  See the License for the\n-* specific language governing permissions and limitations\n-* under the License.\n-*/\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n \n package org.apache.samza.system.eventhub.consumer;\n \n import com.microsoft.azure.eventhubs.EventData;\n-import com.microsoft.azure.eventhubs.EventHubPartitionRuntimeInformation;\n+import com.microsoft.azure.eventhubs.EventHubException;\n+import com.microsoft.azure.eventhubs.EventPosition;\n import com.microsoft.azure.eventhubs.PartitionReceiveHandler;\n import com.microsoft.azure.eventhubs.PartitionReceiver;\n-import com.microsoft.azure.servicebus.ServiceBusException;\n-import org.apache.samza.SamzaException;\n-import org.apache.samza.metrics.Counter;\n-import org.apache.samza.metrics.MetricsRegistry;\n-import org.apache.samza.system.IncomingMessageEnvelope;\n-import org.apache.samza.system.SystemStreamPartition;\n-import org.apache.samza.system.eventhub.EventHubClientManagerFactory;\n-import org.apache.samza.system.eventhub.EventHubClientManager;\n-import org.apache.samza.system.eventhub.EventHubConfig;\n-import org.apache.samza.system.eventhub.Interceptor;\n-import org.apache.samza.system.eventhub.admin.EventHubSystemAdmin;\n-import org.apache.samza.system.eventhub.metrics.SamzaHistogram;\n-import org.apache.samza.system.eventhub.producer.EventHubSystemProducer;\n-import org.apache.samza.util.BlockingEnvelopeMap;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n+import com.microsoft.azure.eventhubs.PartitionRuntimeInformation;\n+import com.microsoft.azure.eventhubs.impl.ClientConstants;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.ArrayList;\n@@ -51,11 +37,26 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.LinkedBlockingQueue;\n-import java.util.concurrent.TimeoutException;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n+import org.apache.samza.SamzaException;\n+import org.apache.samza.metrics.Counter;\n+import org.apache.samza.metrics.MetricsRegistry;\n+import org.apache.samza.system.IncomingMessageEnvelope;\n+import org.apache.samza.system.SystemStreamPartition;\n+import org.apache.samza.system.eventhub.EventHubClientManager;\n+import org.apache.samza.system.eventhub.EventHubClientManagerFactory;\n+import org.apache.samza.system.eventhub.EventHubConfig;\n+import org.apache.samza.system.eventhub.Interceptor;\n+import org.apache.samza.system.eventhub.admin.EventHubSystemAdmin;\n+import org.apache.samza.system.eventhub.metrics.SamzaHistogram;\n+import org.apache.samza.system.eventhub.producer.EventHubSystemProducer;\n+import org.apache.samza.util.BlockingEnvelopeMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n \n /**\n@@ -96,13 +97,12 @@\n  */\n public class EventHubSystemConsumer extends BlockingEnvelopeMap {\n   private static final Logger LOG = LoggerFactory.getLogger(EventHubSystemConsumer.class);\n-  private static final int MAX_EVENT_COUNT_PER_PARTITION_POLL = 50;\n \n   // Overall timeout for EventHubClient exponential backoff policy\n   private static final Duration DEFAULT_EVENTHUB_RECEIVER_TIMEOUT = Duration.ofMinutes(10L);\n   private static final long DEFAULT_SHUTDOWN_TIMEOUT_MILLIS = Duration.ofMinutes(1L).toMillis();\n \n-  public static final String START_OF_STREAM = PartitionReceiver.START_OF_STREAM; // -1\n+  public static final String START_OF_STREAM = ClientConstants.START_OF_STREAM; // -1\n   public static final String END_OF_STREAM = \"-2\";\n   public static final String EVENT_READ_RATE = \"eventReadRate\";\n   public static final String EVENT_BYTE_READ_RATE = \"eventByteReadRate\";\n@@ -122,11 +122,14 @@\n   private final Map<String, SamzaHistogram> readLatencies;\n   private final Map<String, Counter> readErrors;\n \n-  final ConcurrentHashMap<SystemStreamPartition, PartitionReceiveHandler> streamPartitionHandlers = new ConcurrentHashMap<>();\n-  private final ConcurrentHashMap<SystemStreamPartition, PartitionReceiver> streamPartitionReceivers = new ConcurrentHashMap<>();\n+  final ConcurrentHashMap<SystemStreamPartition, PartitionReceiveHandler> streamPartitionHandlers =\n+      new ConcurrentHashMap<>();\n+  private final ConcurrentHashMap<SystemStreamPartition, PartitionReceiver> streamPartitionReceivers =\n+      new ConcurrentHashMap<>();\n   private final ConcurrentHashMap<String, EventHubClientManager> streamEventHubManagers = new ConcurrentHashMap<>();\n   private final ConcurrentHashMap<SystemStreamPartition, String> streamPartitionOffsets = new ConcurrentHashMap<>();\n   private final Map<String, Interceptor> interceptors;\n+  private final Integer prefetchCount;\n   private boolean isStarted = false;\n   private final EventHubConfig config;\n   private final String systemName;\n@@ -135,8 +138,8 @@\n   private final AtomicReference<Throwable> eventHubHandlerError = new AtomicReference<>(null);\n \n   public EventHubSystemConsumer(EventHubConfig config, String systemName,\n-                                EventHubClientManagerFactory eventHubClientManagerFactory,\n-                                Map<String, Interceptor> interceptors, MetricsRegistry registry) {\n+      EventHubClientManagerFactory eventHubClientManagerFactory, Map<String, Interceptor> interceptors,\n+      MetricsRegistry registry) {\n     super(registry, System::currentTimeMillis);\n \n     this.config = config;\n@@ -145,21 +148,24 @@ public EventHubSystemConsumer(EventHubConfig config, String systemName,\n     List<String> streamIds = config.getStreams(systemName);\n     // Create and initiate connections to Event Hubs\n     for (String streamId : streamIds) {\n-      EventHubClientManager eventHubClientManager = eventHubClientManagerFactory\n-              .getEventHubClientManager(systemName, streamId, config);\n+      EventHubClientManager eventHubClientManager =\n+          eventHubClientManagerFactory.getEventHubClientManager(systemName, streamId, config);\n       streamEventHubManagers.put(streamId, eventHubClientManager);\n       eventHubClientManager.init();\n     }\n+    prefetchCount = config.getPrefetchCount(systemName);\n+\n+\n \n     // Initiate metrics\n-    eventReadRates = streamIds.stream()\n-            .collect(Collectors.toMap(Function.identity(), x -> registry.newCounter(x, EVENT_READ_RATE)));\n+    eventReadRates =\n+        streamIds.stream().collect(Collectors.toMap(Function.identity(), x -> registry.newCounter(x, EVENT_READ_RATE)));\n     eventByteReadRates = streamIds.stream()\n-            .collect(Collectors.toMap(Function.identity(), x -> registry.newCounter(x, EVENT_BYTE_READ_RATE)));\n+        .collect(Collectors.toMap(Function.identity(), x -> registry.newCounter(x, EVENT_BYTE_READ_RATE)));\n     readLatencies = streamIds.stream()\n-            .collect(Collectors.toMap(Function.identity(), x -> new SamzaHistogram(registry, x, READ_LATENCY)));\n-    readErrors = streamIds.stream()\n-            .collect(Collectors.toMap(Function.identity(), x -> registry.newCounter(x, READ_ERRORS)));\n+        .collect(Collectors.toMap(Function.identity(), x -> new SamzaHistogram(registry, x, READ_LATENCY)));\n+    readErrors =\n+        streamIds.stream().collect(Collectors.toMap(Function.identity(), x -> registry.newCounter(x, READ_ERRORS)));\n \n     // Locking to ensure that these aggregated metrics will be created only once across multiple system consumers.\n     synchronized (AGGREGATE_METRICS_LOCK) {\n@@ -183,7 +189,9 @@ public void register(SystemStreamPartition systemStreamPartition, String offset)\n \n     if (streamPartitionOffsets.containsKey(systemStreamPartition)) {\n       // Only update if new offset is lower than previous offset\n-      if (END_OF_STREAM.equals(offset)) return;\n+      if (END_OF_STREAM.equals(offset)) {\n+        return;\n+      }\n       String prevOffset = streamPartitionOffsets.get(systemStreamPartition);\n       if (!END_OF_STREAM.equals(prevOffset) && EventHubSystemAdmin.compareOffsets(offset, prevOffset) > -1) {\n         return;\n@@ -192,20 +200,20 @@ public void register(SystemStreamPartition systemStreamPartition, String offset)\n     streamPartitionOffsets.put(systemStreamPartition, offset);\n   }\n \n-  private String getNewestEventHubOffset(EventHubClientManager eventHubClientManager, String streamName, Integer partitionId) {\n-    CompletableFuture<EventHubPartitionRuntimeInformation> partitionRuntimeInfoFuture = eventHubClientManager\n-            .getEventHubClient()\n-            .getPartitionRuntimeInformation(partitionId.toString());\n+  private String getNewestEventHubOffset(EventHubClientManager eventHubClientManager, String streamName,\n+      Integer partitionId) {\n+    CompletableFuture<PartitionRuntimeInformation> partitionRuntimeInfoFuture =\n+        eventHubClientManager.getEventHubClient().getPartitionRuntimeInformation(partitionId.toString());\n     try {\n       long timeoutMs = config.getRuntimeInfoWaitTimeMS(systemName);\n \n-      EventHubPartitionRuntimeInformation partitionRuntimeInformation = partitionRuntimeInfoFuture\n-              .get(timeoutMs, TimeUnit.MILLISECONDS);\n+      PartitionRuntimeInformation partitionRuntimeInformation =\n+          partitionRuntimeInfoFuture.get(timeoutMs, TimeUnit.MILLISECONDS);\n \n       return partitionRuntimeInformation.getLastEnqueuedOffset();\n     } catch (InterruptedException | ExecutionException | TimeoutException e) {\n-      String msg = String.format(\n-              \"Error while fetching EventHubPartitionRuntimeInfo for System:%s, Stream:%s, Partition:%s\",\n+      String msg =\n+          String.format(\"Error while fetching EventHubPartitionRuntimeInfo for System:%s, Stream:%s, Partition:%s\",\n               systemName, streamName, partitionId);\n       throw new SamzaException(msg);\n     }\n@@ -235,21 +243,23 @@ public void start() {\n           // If the offset is greater than the newest offset, use the use current Instant as\n           // offset to fetch in Eventhub.\n           receiver = eventHubClientManager.getEventHubClient()\n-                  .createReceiverSync(consumerGroup, partitionId.toString(), Instant.now());\n+              .createReceiverSync(consumerGroup, partitionId.toString(), EventPosition.fromEnqueuedTime(Instant.now()));\n         } else {\n           // If the offset is less or equal to the newest offset in the system, it can be\n           // used as the starting offset to receive from. EventHub will return the first\n           // message AFTER the offset that was specified in the fetch request.\n           // If no such offset exists Eventhub will return an error.\n           receiver = eventHubClientManager.getEventHubClient()\n-                  .createReceiverSync(consumerGroup, partitionId.toString(), offset,\n-                          !offset.equals(EventHubSystemConsumer.START_OF_STREAM));\n+              .createReceiverSync(consumerGroup, partitionId.toString(),\n+                  EventPosition.fromOffset(offset, !offset.equals(EventHubSystemConsumer.START_OF_STREAM)));\n         }\n \n-        PartitionReceiveHandler handler = new PartitionReceiverHandlerImpl(ssp, eventReadRates.get(streamId),\n-                eventByteReadRates.get(streamId), readLatencies.get(streamId), readErrors.get(streamId),\n-                interceptors.getOrDefault(streamId, null));\n+        receiver.setPrefetchCount(prefetchCount);\n \n+        PartitionReceiveHandler handler =\n+            new PartitionReceiverHandlerImpl(ssp, eventReadRates.get(streamId), eventByteReadRates.get(streamId),\n+                readLatencies.get(streamId), readErrors.get(streamId), interceptors.getOrDefault(streamId, null),\n+                config.getMaxEventCountPerPoll(systemName));\n \n         // Timeout for EventHubClient receive\n         receiver.setReceiveTimeout(DEFAULT_EVENTHUB_RECEIVER_TIMEOUT);\n@@ -261,16 +271,16 @@ public void start() {\n         streamPartitionReceivers.put(ssp, receiver);\n       } catch (Exception e) {\n         throw new SamzaException(\n-                String.format(\"Failed to create receiver for EventHubs: namespace=%s, entity=%s, partitionId=%d\",\n-                        namespace, entityPath, partitionId), e);\n+            String.format(\"Failed to create receiver for EventHubs: namespace=%s, entity=%s, partitionId=%d\", namespace,\n+                entityPath, partitionId), e);\n       }\n       LOG.debug(String.format(\"Connection successfully started for namespace=%s, entity=%s \", namespace, entityPath));\n-\n     }\n   }\n \n   @Override\n-  public Map<SystemStreamPartition, List<IncomingMessageEnvelope>> poll(Set<SystemStreamPartition> systemStreamPartitions, long timeout) throws InterruptedException {\n+  public Map<SystemStreamPartition, List<IncomingMessageEnvelope>> poll(\n+      Set<SystemStreamPartition> systemStreamPartitions, long timeout) throws InterruptedException {\n     Throwable handlerError = eventHubHandlerError.get();\n \n     if (handlerError != null) {\n@@ -305,19 +315,20 @@ private void renewPartitionReceiver(SystemStreamPartition ssp) {\n \n       // Recreate receiver\n       PartitionReceiver receiver = eventHubClientManager.getEventHubClient()\n-              .createReceiverSync(consumerGroup, partitionId.toString(), offset,\n-                      !offset.equals(EventHubSystemConsumer.START_OF_STREAM));\n+          .createReceiverSync(consumerGroup, partitionId.toString(),\n+              EventPosition.fromOffset(offset, !offset.equals(EventHubSystemConsumer.START_OF_STREAM)));\n+\n+      receiver.setPrefetchCount(prefetchCount);\n \n       // Timeout for EventHubClient receive\n       receiver.setReceiveTimeout(DEFAULT_EVENTHUB_RECEIVER_TIMEOUT);\n \n       // Create and start receiver thread with handler\n       receiver.setReceiveHandler(streamPartitionHandlers.get(ssp));\n       streamPartitionReceivers.put(ssp, receiver);\n-\n     } catch (Exception e) {\n       eventHubHandlerError.set(new SamzaException(\n-              String.format(\"Failed to recreate receiver for EventHubs after ReceiverHandlerError (ssp=%s)\", ssp), e));\n+          String.format(\"Failed to recreate receiver for EventHubs after ReceiverHandlerError (ssp=%s)\", ssp), e));\n     }\n   }\n \n@@ -336,9 +347,9 @@ public void stop() {\n   }\n \n   private boolean isErrorTransient(Throwable throwable) {\n-    if (throwable instanceof ServiceBusException) {\n-      ServiceBusException serviceBusException = (ServiceBusException) throwable;\n-      return serviceBusException.getIsTransient();\n+    if (throwable instanceof EventHubException) {\n+      EventHubException eventHubException = (EventHubException) throwable;\n+      return eventHubException.getIsTransient();\n     }\n     return false;\n   }\n@@ -348,24 +359,30 @@ private boolean isErrorTransient(Throwable throwable) {\n     return new LinkedBlockingQueue<>(config.getConsumerBufferCapacity(systemName));\n   }\n \n-  protected class PartitionReceiverHandlerImpl extends PartitionReceiveHandler {\n+  protected class PartitionReceiverHandlerImpl implements PartitionReceiveHandler {\n \n     private final Counter eventReadRate;\n     private final Counter eventByteReadRate;\n     private final SamzaHistogram readLatency;\n     private final Counter errorRate;\n     private final Interceptor interceptor;\n+    private final Integer maxEventCount;\n     SystemStreamPartition ssp;\n \n     PartitionReceiverHandlerImpl(SystemStreamPartition ssp, Counter eventReadRate, Counter eventByteReadRate,\n-                                 SamzaHistogram readLatency, Counter readErrors, Interceptor interceptor) {\n-      super(MAX_EVENT_COUNT_PER_PARTITION_POLL);\n+        SamzaHistogram readLatency, Counter readErrors, Interceptor interceptor, int maxEventCount) {\n       this.ssp = ssp;\n       this.eventReadRate = eventReadRate;\n       this.eventByteReadRate = eventByteReadRate;\n       this.readLatency = readLatency;\n       this.errorRate = readErrors;\n       this.interceptor = interceptor;\n+      this.maxEventCount = maxEventCount;\n+    }\n+\n+    @Override\n+    public int getMaxEventCount() {\n+      return this.maxEventCount;\n     }\n \n     @Override\n@@ -415,8 +432,8 @@ public void onError(Throwable throwable) {\n       errorRate.inc();\n       aggReadErrors.inc();\n \n-      if (throwable instanceof ServiceBusException) {\n-        ServiceBusException busException = (ServiceBusException) throwable;\n+      if (throwable instanceof EventHubException) {\n+        EventHubException busException = (EventHubException) throwable;\n \n         if (busException.getIsTransient()) {\n ",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/consumer/EventHubSystemConsumer.java",
                "sha": "f00944b35e8fb25af1dee2a9a51bdf2d947a96d3",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/metrics/SamzaHistogram.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/metrics/SamzaHistogram.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 24,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/metrics/SamzaHistogram.java",
                "patch": "@@ -1,35 +1,36 @@\n /*\n-* Licensed to the Apache Software Foundation (ASF) under one\n-* or more contributor license agreements.  See the NOTICE file\n-* distributed with this work for additional information\n-* regarding copyright ownership.  The ASF licenses this file\n-* to you under the Apache License, Version 2.0 (the\n-* \"License\"); you may not use this file except in compliance\n-* with the License.  You may obtain a copy of the License at\n-*\n-*   http://www.apache.org/licenses/LICENSE-2.0\n-*\n-* Unless required by applicable law or agreed to in writing,\n-* software distributed under the License is distributed on an\n-* \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-* KIND, either express or implied.  See the License for the\n-* specific language governing permissions and limitations\n-* under the License.\n-*/\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n \n package org.apache.samza.system.eventhub.metrics;\n \n import com.codahale.metrics.ExponentiallyDecayingReservoir;\n import com.codahale.metrics.Histogram;\n import com.codahale.metrics.Snapshot;\n-import org.apache.samza.metrics.Gauge;\n-import org.apache.samza.metrics.MetricsRegistry;\n-\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n+import org.apache.samza.metrics.Gauge;\n+import org.apache.samza.metrics.MetricsRegistry;\n+import org.apache.samza.metrics.MetricsVisitor;\n+\n \n /**\n  * Creates a {@link Histogram} metric using {@link ExponentiallyDecayingReservoir}\n@@ -49,14 +50,34 @@ public SamzaHistogram(MetricsRegistry registry, String group, String name, List<\n     this.histogram = new Histogram(new ExponentiallyDecayingReservoir());\n     this.percentiles = percentiles;\n     this.gauges = this.percentiles.stream()\n-            .filter(x -> x > 0 && x <= 100)\n-            .collect(\n-                    Collectors.toMap(Function.identity(), x -> registry.newGauge(group, name + \"_\" + String.valueOf(0), 0D)));\n+        .filter(x -> x > 0 && x <= 100)\n+        .collect(Collectors.toMap(Function.identity(),\n+            x -> registry.newGauge(group, new HistogramGauge(x, name + \"_\" + String.valueOf(x), 0D))));\n   }\n \n   public void update(long value) {\n     histogram.update(value);\n+  }\n+\n+  public void updateGaugeValues(double percentile) {\n     Snapshot values = histogram.getSnapshot();\n-    percentiles.forEach(x -> gauges.get(x).set(values.getValue(x / 100)));\n+    gauges.get(percentile).set(values.getValue(percentile / 100));\n+  }\n+\n+  /**\n+   * Custom gauge whose value is set based on the underlying Histogram\n+   */\n+  private class HistogramGauge extends Gauge<Double> {\n+    private final Double percentile;\n+\n+    public HistogramGauge(Double percentile, String name, double value) {\n+      super(name, value);\n+      this.percentile = percentile;\n+    }\n+\n+    public void visit(MetricsVisitor visitor) {\n+      updateGaugeValues(percentile);\n+      visitor.gauge(this);\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/metrics/SamzaHistogram.java",
                "sha": "03fc1149b54682001b9afddb5db3ca687c5dfcb2",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/AsyncSystemProducer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/AsyncSystemProducer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 1,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/AsyncSystemProducer.java",
                "patch": "@@ -139,7 +139,6 @@ public synchronized void send(String source, OutgoingMessageEnvelope envelope) {\n \n     // Auto update the metrics and possible throwable when futures are complete.\n     sendResult.handle((aVoid, throwable) -> {\n-        pendingFutures.remove(sendResult);\n         long callbackLatencyMs = System.currentTimeMillis() - afterSendTimeMs;\n         sendCallbackLatency.get(streamId).update(callbackLatencyMs);\n         aggSendCallbackLatency.update(callbackLatencyMs);",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/AsyncSystemProducer.java",
                "sha": "307b8f64cd8146a51936d808fda4dd0fba1e3c04",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/EventHubSystemProducer.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/EventHubSystemProducer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 3,
                "filename": "samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/EventHubSystemProducer.java",
                "patch": "@@ -21,8 +21,9 @@\n \n import com.microsoft.azure.eventhubs.EventData;\n import com.microsoft.azure.eventhubs.EventHubClient;\n+import com.microsoft.azure.eventhubs.EventHubException;\n import com.microsoft.azure.eventhubs.PartitionSender;\n-import com.microsoft.azure.servicebus.ServiceBusException;\n+import com.microsoft.azure.eventhubs.impl.EventDataImpl;\n import java.nio.charset.Charset;\n import java.time.Duration;\n import java.util.ArrayList;\n@@ -164,7 +165,7 @@ public synchronized void start() {\n           } catch (InterruptedException | ExecutionException | TimeoutException e) {\n             String msg = \"Failed to fetch number of Event Hub partitions for partition sender creation\";\n             throw new SamzaException(msg, e);\n-          } catch (ServiceBusException | IllegalArgumentException e) {\n+          } catch (EventHubException | IllegalArgumentException e) {\n             String msg = \"Creation of partition sender failed with exception\";\n             throw new SamzaException(msg, e);\n           }\n@@ -282,7 +283,7 @@ protected EventData createEventData(String streamId, OutgoingMessageEnvelope env\n       eventValue = interceptor.get().intercept(eventValue);\n     }\n \n-    EventData eventData = new EventData(eventValue);\n+    EventData eventData = new EventDataImpl(eventValue);\n \n     eventData.getProperties().put(PRODUCE_TIMESTAMP, Long.toString(System.currentTimeMillis()));\n ",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/main/java/org/apache/samza/system/eventhub/producer/EventHubSystemProducer.java",
                "sha": "3639bbc19e0a3c997071aed6a6138221ce5271e4",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventData.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventData.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 2,
                "filename": "samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventData.java",
                "patch": "@@ -21,15 +21,17 @@\n \n import com.microsoft.azure.eventhubs.EventData;\n \n+import com.microsoft.azure.eventhubs.impl.EventDataImpl;\n import java.nio.charset.Charset;\n import java.util.*;\n \n-public class MockEventData extends EventData {\n+public class MockEventData implements EventData {\n+  EventData eventData;\n \n   private EventData.SystemProperties overridedSystemProperties;\n \n   private MockEventData(byte[] data, String partitionKey, String offset) {\n-    super(data);\n+    eventData = new EventDataImpl(data);\n     HashMap<String, Object> properties = new HashMap<>();\n     properties.put(\"x-opt-offset\", offset);\n     properties.put(\"x-opt-partition-key\", partitionKey);\n@@ -50,6 +52,21 @@ private MockEventData(byte[] data, String partitionKey, String offset) {\n     return result;\n   }\n \n+  @Override\n+  public Object getObject() {\n+    return eventData.getObject();\n+  }\n+\n+  @Override\n+  public byte[] getBytes() {\n+    return eventData.getBytes();\n+  }\n+\n+  @Override\n+  public Map<String, Object> getProperties() {\n+    return eventData.getProperties();\n+  }\n+\n   @Override\n   public EventData.SystemProperties getSystemProperties() {\n     return overridedSystemProperties;",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventData.java",
                "sha": "1e3d4f5cfd38f5e528c84fee7abb1c6c61ee6aaa",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventHubClientManagerFactory.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventHubClientManagerFactory.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 13,
                "filename": "samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventHubClientManagerFactory.java",
                "patch": "@@ -26,7 +26,6 @@\n import org.mockito.stubbing.Answer;\n import org.powermock.api.mockito.PowerMockito;\n \n-import java.time.Instant;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n@@ -39,7 +38,7 @@\n public class MockEventHubClientManagerFactory extends EventHubClientManagerFactory {\n   private Map<SystemStreamPartition, List<EventData>> eventData;\n   private Map<String, Map<String, Map<Integer, List<EventData>>>> receivedData;\n-  private Map<String, String> startingOffsets = new HashMap<>();\n+  private Map<String, EventPosition> startingOffsets = new HashMap<>();\n \n   public MockEventHubClientManagerFactory() {\n     this.receivedData = new HashMap<>();\n@@ -71,7 +70,7 @@ public void sendToHandlers(Map<SystemStreamPartition, PartitionReceiveHandler> h\n     handlers.forEach((ssp, value) -> value.onReceive(eventData.get(ssp)));\n   }\n \n-  public String getPartitionOffset(String partitionId) {\n+  public EventPosition getPartitionOffset(String partitionId) {\n     return startingOffsets.getOrDefault(partitionId, null);\n   }\n \n@@ -101,10 +100,10 @@ public String getPartitionOffset(String partitionId) {\n           }\n           return null;\n         });\n-      EventHubPartitionRuntimeInformation mockPartitionRuntimeInfo = PowerMockito.mock(EventHubPartitionRuntimeInformation.class);\n+      PartitionRuntimeInformation mockPartitionRuntimeInfo = PowerMockito.mock(PartitionRuntimeInformation.class);\n       PowerMockito.when(mockPartitionRuntimeInfo.getLastEnqueuedOffset())\n               .thenReturn(EventHubSystemConsumer.START_OF_STREAM);\n-      CompletableFuture<EventHubPartitionRuntimeInformation> partitionFuture =  new MockPartitionFuture(mockPartitionRuntimeInfo);\n+      CompletableFuture<PartitionRuntimeInformation> partitionFuture =  new MockPartitionFuture(mockPartitionRuntimeInfo);\n \n       // Producer mocks\n       PartitionSender mockPartitionSender0 = PowerMockito.mock(PartitionSender.class);\n@@ -128,16 +127,16 @@ public String getPartitionOffset(String partitionId) {\n \n       try {\n         // Consumer calls\n-        PowerMockito.when(mockEventHubClient.createReceiverSync(anyString(), anyString(), any(Instant.class)))\n+        PowerMockito.when(mockEventHubClient.createReceiverSync(anyString(), anyString(), anyObject()))\n                 .then((Answer<PartitionReceiver>) invocationOnMock -> {\n                     String partitionId = invocationOnMock.getArgumentAt(1, String.class);\n-                    startingOffsets.put(partitionId, EventHubSystemConsumer.END_OF_STREAM);\n+                    startingOffsets.put(partitionId, EventPosition.fromEndOfStream());\n                     return mockPartitionReceiver;\n                   });\n-        PowerMockito.when(mockEventHubClient.createReceiverSync(anyString(), anyString(), anyString(), anyBoolean()))\n+        PowerMockito.when(mockEventHubClient.createReceiverSync(anyString(), anyString(), anyObject()))\n                 .then((Answer<PartitionReceiver>) invocationOnMock -> {\n                     String partitionId = invocationOnMock.getArgumentAt(1, String.class);\n-                    String offset = invocationOnMock.getArgumentAt(2, String.class);\n+                    EventPosition offset = invocationOnMock.getArgumentAt(2, EventPosition.class);\n                     startingOffsets.put(partitionId, offset);\n                     return mockPartitionReceiver;\n                   });\n@@ -196,15 +195,15 @@ public EventHubRuntimeInformation get(long timeout, TimeUnit unit) {\n       }\n     }\n \n-    private class MockPartitionFuture extends CompletableFuture<EventHubPartitionRuntimeInformation> {\n-      EventHubPartitionRuntimeInformation runtimeInformation;\n+    private class MockPartitionFuture extends CompletableFuture<PartitionRuntimeInformation> {\n+      PartitionRuntimeInformation runtimeInformation;\n \n-      MockPartitionFuture(EventHubPartitionRuntimeInformation runtimeInformation) {\n+      MockPartitionFuture(PartitionRuntimeInformation runtimeInformation) {\n         this.runtimeInformation = runtimeInformation;\n       }\n \n       @Override\n-      public EventHubPartitionRuntimeInformation get(long timeout, TimeUnit unit) {\n+      public PartitionRuntimeInformation get(long timeout, TimeUnit unit) {\n         return runtimeInformation;\n       }\n     }",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/MockEventHubClientManagerFactory.java",
                "sha": "6ee9bcfd155a6e0007118b1f3e68b6471a17f1c9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/TestMetricsRegistry.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/test/java/org/apache/samza/system/eventhub/TestMetricsRegistry.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 1,
                "filename": "samza-azure/src/test/java/org/apache/samza/system/eventhub/TestMetricsRegistry.java",
                "patch": "@@ -70,7 +70,12 @@ public Counter newCounter(String group, Counter counter) {\n \n   @Override\n   public <T> Gauge<T> newGauge(String group, Gauge<T> value) {\n-    return null;\n+    if (!gauges.containsKey(group)) {\n+      gauges.put(group, new ArrayList<>());\n+    }\n+\n+    gauges.get(group).add(value);\n+    return value;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/TestMetricsRegistry.java",
                "sha": "d29b975578cedf32af552395d70d6074055fe969",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/consumer/TestEventHubSystemConsumer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/test/java/org/apache/samza/system/eventhub/consumer/TestEventHubSystemConsumer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 3,
                "filename": "samza-azure/src/test/java/org/apache/samza/system/eventhub/consumer/TestEventHubSystemConsumer.java",
                "patch": "@@ -42,7 +42,7 @@\n import static org.apache.samza.system.eventhub.MockEventHubConfigFactory.*;\n \n @RunWith(PowerMockRunner.class)\n-@PrepareForTest({EventHubRuntimeInformation.class, EventHubPartitionRuntimeInformation.class,\n+@PrepareForTest({EventHubRuntimeInformation.class, PartitionRuntimeInformation.class,\n         EventHubClient.class, PartitionReceiver.class, PartitionSender.class})\n public class TestEventHubSystemConsumer {\n   private static final String MOCK_ENTITY_1 = \"mocktopic1\";\n@@ -99,8 +99,8 @@ public void testMultipleRegistersToSameSSP() throws Exception {\n     consumer.register(ssp, EventHubSystemConsumer.START_OF_STREAM);\n     consumer.start();\n \n-    Assert.assertEquals(EventHubSystemConsumer.START_OF_STREAM,\n-            eventHubClientWrapperFactory.getPartitionOffset(String.valueOf(partitionId)));\n+    Assert.assertEquals(EventPosition.fromOffset(EventHubSystemConsumer.START_OF_STREAM, false).toString(),\n+            eventHubClientWrapperFactory.getPartitionOffset(String.valueOf(partitionId)).toString());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/consumer/TestEventHubSystemConsumer.java",
                "sha": "b40d86db469e62352755d0ab1cc2bd46699198e4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/ITestEventHubSystemProducer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/ITestEventHubSystemProducer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 5,
                "filename": "samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/ITestEventHubSystemProducer.java",
                "patch": "@@ -21,13 +21,13 @@\n \n import com.microsoft.azure.eventhubs.EventData;\n import com.microsoft.azure.eventhubs.EventHubClient;\n+import com.microsoft.azure.eventhubs.EventHubException;\n+import com.microsoft.azure.eventhubs.EventPosition;\n import com.microsoft.azure.eventhubs.PartitionReceiver;\n-import com.microsoft.azure.servicebus.ServiceBusException;\n import org.apache.samza.SamzaException;\n import org.apache.samza.config.Config;\n import org.apache.samza.system.*;\n import org.apache.samza.system.eventhub.*;\n-import org.apache.samza.system.eventhub.consumer.EventHubSystemConsumer;\n import org.apache.samza.util.NoOpMetricsRegistry;\n import org.junit.Assert;\n import org.junit.Ignore;\n@@ -92,19 +92,19 @@ public void testSend() {\n   }\n \n   @Test\n-  public void testReceive() throws ServiceBusException {\n+  public void testReceive() throws EventHubException {\n     EventHubClientManagerFactory clientFactory = new EventHubClientManagerFactory();\n     EventHubClientManager wrapper = clientFactory\n             .getEventHubClientManager(SYSTEM_NAME, STREAM_NAME1, new EventHubConfig(createEventHubConfig()));\n     wrapper.init();\n     EventHubClient client = wrapper.getEventHubClient();\n     PartitionReceiver receiver =\n             client.createReceiverSync(EventHubClient.DEFAULT_CONSUMER_GROUP_NAME, \"0\",\n-                    EventHubSystemConsumer.START_OF_STREAM, true);\n+                    EventPosition.fromStartOfStream());\n     receiveMessages(receiver, 300);\n   }\n \n-  private void receiveMessages(PartitionReceiver receiver, int numMessages) throws ServiceBusException {\n+  private void receiveMessages(PartitionReceiver receiver, int numMessages) throws EventHubException {\n     int count = 0;\n     while (count < numMessages) {\n ",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/ITestEventHubSystemProducer.java",
                "sha": "63f6daa535981a3bf7b2e95388d84374b89350e6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/TestEventHubSystemProducer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/TestEventHubSystemProducer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 2,
                "filename": "samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/TestEventHubSystemProducer.java",
                "patch": "@@ -20,9 +20,9 @@\n package org.apache.samza.system.eventhub.producer;\n \n import com.microsoft.azure.eventhubs.EventHubClient;\n-import com.microsoft.azure.eventhubs.EventHubPartitionRuntimeInformation;\n import com.microsoft.azure.eventhubs.EventHubRuntimeInformation;\n import com.microsoft.azure.eventhubs.PartitionReceiver;\n+import com.microsoft.azure.eventhubs.PartitionRuntimeInformation;\n import com.microsoft.azure.eventhubs.PartitionSender;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -49,7 +49,7 @@\n \n \n @RunWith(PowerMockRunner.class)\n-@PrepareForTest({EventHubRuntimeInformation.class, EventHubPartitionRuntimeInformation.class, EventHubClient.class, PartitionReceiver.class, PartitionSender.class})\n+@PrepareForTest({EventHubRuntimeInformation.class, PartitionRuntimeInformation.class, EventHubClient.class, PartitionReceiver.class, PartitionSender.class})\n public class TestEventHubSystemProducer {\n \n   private static final String SOURCE = \"TestEventHubSystemProducer\";",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-azure/src/test/java/org/apache/samza/system/eventhub/producer/TestEventHubSystemProducer.java",
                "sha": "9a3bf7def8a3c0a207d7b11987369f35919d2482",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/samza/blob/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-tools/src/main/java/org/apache/samza/tools/EventHubConsoleConsumer.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/samza/contents/samza-tools/src/main/java/org/apache/samza/tools/EventHubConsoleConsumer.java?ref=fda1e37d0862493cb59ed0907c597a2dc973ef6f",
                "deletions": 29,
                "filename": "samza-tools/src/main/java/org/apache/samza/tools/EventHubConsoleConsumer.java",
                "patch": "@@ -1,38 +1,41 @@\n /*\n-* Licensed to the Apache Software Foundation (ASF) under one\n-* or more contributor license agreements.  See the NOTICE file\n-* distributed with this work for additional information\n-* regarding copyright ownership.  The ASF licenses this file\n-* to you under the Apache License, Version 2.0 (the\n-* \"License\"); you may not use this file except in compliance\n-* with the License.  You may obtain a copy of the License at\n-*\n-*   http://www.apache.org/licenses/LICENSE-2.0\n-*\n-* Unless required by applicable law or agreed to in writing,\n-* software distributed under the License is distributed on an\n-* \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-* KIND, either express or implied.  See the License for the\n-* specific language governing permissions and limitations\n-* under the License.\n-*/\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n \n package org.apache.samza.tools;\n \n+import com.microsoft.azure.eventhubs.ConnectionStringBuilder;\n import com.microsoft.azure.eventhubs.EventData;\n import com.microsoft.azure.eventhubs.EventHubClient;\n+import com.microsoft.azure.eventhubs.EventHubException;\n import com.microsoft.azure.eventhubs.EventHubRuntimeInformation;\n+import com.microsoft.azure.eventhubs.EventPosition;\n import com.microsoft.azure.eventhubs.PartitionReceiver;\n-import com.microsoft.azure.servicebus.ConnectionStringBuilder;\n-import com.microsoft.azure.servicebus.ServiceBusException;\n import java.io.IOException;\n import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executors;\n import org.apache.commons.cli.BasicParser;\n import org.apache.commons.cli.CommandLine;\n import org.apache.commons.cli.CommandLineParser;\n import org.apache.commons.cli.HelpFormatter;\n import org.apache.commons.cli.Options;\n \n+\n /**\n  * Tool to read events from Microsoft Azure event hubs.\n  */\n@@ -59,17 +62,17 @@\n   private static final String OPT_DESC_TOKEN = \"Token corresponding to the key.\";\n \n   public static void main(String[] args)\n-      throws ServiceBusException, IOException, ExecutionException, InterruptedException {\n+      throws EventHubException, IOException, ExecutionException, InterruptedException {\n     Options options = new Options();\n     options.addOption(\n         CommandLineHelper.createOption(OPT_SHORT_EVENTHUB_NAME, OPT_LONG_EVENTHUB_NAME, OPT_ARG_EVENTHUB_NAME, true,\n-        OPT_DESC_EVENTHUB_NAME));\n+            OPT_DESC_EVENTHUB_NAME));\n \n-    options.addOption(\n-        CommandLineHelper.createOption(OPT_SHORT_NAMESPACE, OPT_LONG_NAMESPACE, OPT_ARG_NAMESPACE, true, OPT_DESC_NAMESPACE));\n+    options.addOption(CommandLineHelper.createOption(OPT_SHORT_NAMESPACE, OPT_LONG_NAMESPACE, OPT_ARG_NAMESPACE, true,\n+        OPT_DESC_NAMESPACE));\n \n-    options.addOption(\n-        CommandLineHelper.createOption(OPT_SHORT_KEY_NAME, OPT_LONG_KEY_NAME, OPT_ARG_KEY_NAME, true, OPT_DESC_KEY_NAME));\n+    options.addOption(CommandLineHelper.createOption(OPT_SHORT_KEY_NAME, OPT_LONG_KEY_NAME, OPT_ARG_KEY_NAME, true,\n+        OPT_DESC_KEY_NAME));\n \n     options.addOption(\n         CommandLineHelper.createOption(OPT_SHORT_TOKEN, OPT_LONG_TOKEN, OPT_ARG_TOKEN, true, OPT_DESC_TOKEN));\n@@ -93,17 +96,20 @@ public static void main(String[] args)\n   }\n \n   private static void consumeEvents(String ehName, String namespace, String keyName, String token)\n-      throws ServiceBusException, IOException, ExecutionException, InterruptedException {\n-    ConnectionStringBuilder connStr = new ConnectionStringBuilder(namespace, ehName, keyName, token);\n+      throws EventHubException, IOException, ExecutionException, InterruptedException {\n+    ConnectionStringBuilder connStr = new ConnectionStringBuilder().setNamespaceName(namespace)\n+        .setEventHubName(ehName)\n+        .setSasKeyName(keyName)\n+        .setSasKey(token);\n \n-    EventHubClient client = EventHubClient.createFromConnectionStringSync(connStr.toString());\n+    EventHubClient client = EventHubClient.createSync(connStr.toString(), Executors.newFixedThreadPool(10));\n \n     EventHubRuntimeInformation runTimeInfo = client.getRuntimeInformation().get();\n     int numPartitions = runTimeInfo.getPartitionCount();\n     for (int partition = 0; partition < numPartitions; partition++) {\n       PartitionReceiver receiver =\n           client.createReceiverSync(EventHubClient.DEFAULT_CONSUMER_GROUP_NAME, String.valueOf(partition),\n-              PartitionReceiver.START_OF_STREAM);\n+              EventPosition.fromStartOfStream());\n       receiver.receive(10).handle((records, throwable) -> handleComplete(receiver, records, throwable));\n     }\n   }",
                "raw_url": "https://github.com/apache/samza/raw/fda1e37d0862493cb59ed0907c597a2dc973ef6f/samza-tools/src/main/java/org/apache/samza/tools/EventHubConsoleConsumer.java",
                "sha": "a72c8b77f69fce8b74295365d7f3226e336c93c5",
                "status": "modified"
            }
        ],
        "message": "Upgrade to latest event hub version (1.0.1)\n\n* Upgrade to the latest event hub version 1.0.1\n* Adding configs for prefetchCount and maxEventPerPoll\n* Fix the high cpu usage issue in SamzaHistogram\n* Fixing a race condition in event hub system producer where the future was getting removed while it was being checked for completion resulting in NPE.\n\nAuthor: Srinivasulu Punuru <spunuru@linkedin.com>\n\nReviewers: Prateek Maheshwari <pmaheshwari@apache.org>\n\nCloses #467 from srinipunuru/upgrade-eh-1.0.1",
        "parent": "https://github.com/apache/samza/commit/8e000f3984e6158b504bc9cad0c81ea3d260cea3",
        "repo": "samza",
        "unit_tests": [
            "TestEventHubSystemAdmin.java",
            "TestEventHubSystemConsumer.java",
            "TestEventHubSystemProducer.java"
        ]
    }
}