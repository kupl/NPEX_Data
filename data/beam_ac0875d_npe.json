[
    {
        "repo": "beam",
        "commit": "https://github.com/apache/beam/commit/ac0875de84085e1298575d0887e83e5deee5f418",
        "bug_id": "beam_ac0875d",
        "message": "SparkRunner calls pipeline.run\n\n* Remove SparkStreamingPipelineOptions.\n* Run pipeline with Pipeline.run().\n* Better EmbeddedKafka.\n* Avoid NPE if factory wasn't created.\n* Let EmbeddedKafka/Zookeeper discover ports on their own.",
        "parent": "https://github.com/apache/beam/commit/c314e670e0113cddc40af680f5ce8a5134d61e9a",
        "patched_files": [
            "SparkStreamingPipelineOptions.java",
            "SparkPipelineOptions.java",
            "SparkRunner.java",
            "SparkRunnerRegistrar.java",
            "EmbeddedKafkaCluster.java",
            "TfIdf.java"
        ],
        "file": [
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
                "deletions": 0,
                "sha": "6ef374167fa53d7973e3d757969d0ac4f59ffbce",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
                "patch": "@@ -33,4 +33,10 @@\n   @Default.String(\"local[1]\")\n   String getSparkMaster();\n   void setSparkMaster(String master);\n+\n+  @Description(\"Timeout to wait (in msec) for a streaming execution to stop, -1 runs until \"\n+          + \"execution is stopped\")\n+  @Default.Long(-1)\n+  Long getTimeout();\n+  void setTimeout(Long batchInterval);\n }",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
                "deletions": 10,
                "sha": "d994ec418c6b7801e4dc96c5490a70d47c2757b9",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
                "patch": "@@ -69,8 +69,6 @@\n  * options.setSparkMaster(\"spark://host:port\");\n  * EvaluationResult result = SparkRunner.create(options).run(p);\n  * }\n- *\n- * To create a Spark streaming pipeline runner use {@link SparkStreamingPipelineOptions}\n  */\n public final class SparkRunner extends PipelineRunner<EvaluationResult> {\n \n@@ -146,12 +144,6 @@ private SparkRunner(SparkPipelineOptions options) {\n   @Override\n   public EvaluationResult run(Pipeline pipeline) {\n     try {\n-      // validate streaming configuration\n-      if (mOptions.isStreaming() && !(mOptions instanceof SparkStreamingPipelineOptions)) {\n-        throw new RuntimeException(\"A streaming job must be configured with \"\n-            + SparkStreamingPipelineOptions.class.getSimpleName() + \", found \"\n-            + mOptions.getClass().getSimpleName());\n-      }\n       LOG.info(\"Executing pipeline using the SparkRunner.\");\n       JavaSparkContext jsc = SparkContextFactory.getSparkContext(mOptions\n               .getSparkMaster(), mOptions.getAppName());\n@@ -179,6 +171,9 @@ public EvaluationResult run(Pipeline pipeline) {\n \n         return ctxt;\n       } else {\n+        if (mOptions.getTimeout() > 0) {\n+          LOG.info(\"Timeout is ignored by the SparkRunner in batch.\");\n+        }\n         EvaluationContext ctxt = new EvaluationContext(jsc, pipeline);\n         SparkPipelineTranslator translator = new TransformTranslator.Translator();\n         pipeline.traverseTopologically(new SparkPipelineEvaluator(ctxt, translator));\n@@ -210,9 +205,8 @@ public EvaluationResult run(Pipeline pipeline) {\n   private EvaluationContext\n       createStreamingEvaluationContext(JavaSparkContext jsc, Pipeline pipeline,\n       Duration batchDuration) {\n-    SparkStreamingPipelineOptions streamingOptions = (SparkStreamingPipelineOptions) mOptions;\n     JavaStreamingContext jssc = new JavaStreamingContext(jsc, batchDuration);\n-    return new StreamingEvaluationContext(jsc, pipeline, jssc, streamingOptions.getTimeout());\n+    return new StreamingEvaluationContext(jsc, pipeline, jssc, mOptions.getTimeout());\n   }\n \n   /**",
                "changes": 14
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunnerRegistrar.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunnerRegistrar.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunnerRegistrar.java",
                "deletions": 4,
                "sha": "7a317539e47214b75f386ce486d25e2ef6a71300",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunnerRegistrar.java",
                "patch": "@@ -49,15 +49,13 @@ private SparkRunnerRegistrar() {}\n   }\n \n   /**\n-   * Registers the {@link SparkPipelineOptions} and {@link SparkStreamingPipelineOptions}.\n+   * Registers the {@link SparkPipelineOptions}.\n    */\n   @AutoService(PipelineOptionsRegistrar.class)\n   public static class Options implements PipelineOptionsRegistrar {\n     @Override\n     public Iterable<Class<? extends PipelineOptions>> getPipelineOptions() {\n-      return ImmutableList.<Class<? extends PipelineOptions>>of(\n-          SparkPipelineOptions.class,\n-          SparkStreamingPipelineOptions.class);\n+      return ImmutableList.<Class<? extends PipelineOptions>>of(SparkPipelineOptions.class);\n     }\n   }\n }",
                "changes": 6
            },
            {
                "status": "removed",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/c314e670e0113cddc40af680f5ce8a5134d61e9a/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkStreamingPipelineOptions.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkStreamingPipelineOptions.java?ref=c314e670e0113cddc40af680f5ce8a5134d61e9a",
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkStreamingPipelineOptions.java",
                "deletions": 32,
                "sha": "5944acdf100eeb16a9e6c462bc4fb60ceeca73e4",
                "blob_url": "https://github.com/apache/beam/blob/c314e670e0113cddc40af680f5ce8a5134d61e9a/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkStreamingPipelineOptions.java",
                "patch": "@@ -1,32 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.spark;\n-\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-\n-/**\n- * Options used to configure Spark streaming.\n- */\n-public interface SparkStreamingPipelineOptions extends SparkPipelineOptions {\n-  @Description(\"Timeout to wait (in msec) for the streaming execution so stop, -1 runs until \"\n-          + \"execution is stopped\")\n-  @Default.Long(-1)\n-  Long getTimeout();\n-  void setTimeout(Long batchInterval);\n-}",
                "changes": 32
            },
            {
                "status": "modified",
                "additions": 0,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java",
                "deletions": 2,
                "sha": "50ed5f3f5ce8c8bf04b6dde9288160341c898af6",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java",
                "patch": "@@ -46,8 +46,6 @@\n  * options.setSparkMaster(\"spark://host:port\");\n  * EvaluationResult result = SparkRunner.create(options).run(p);\n  * }\n- *\n- * To create a Spark streaming pipeline runner use {@link SparkStreamingPipelineOptions}\n  */\n public final class TestSparkRunner extends PipelineRunner<EvaluationResult> {\n ",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/DeDupTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/DeDupTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/DeDupTest.java",
                "deletions": 1,
                "sha": "9a167446847e013c4db61527877eabc2203a514b",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/DeDupTest.java",
                "patch": "@@ -56,7 +56,7 @@ public void testRun() throws Exception {\n \n     PAssert.that(output).containsInAnyOrder(EXPECTED_SET);\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n }",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/EmptyInputTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/EmptyInputTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/EmptyInputTest.java",
                "deletions": 1,
                "sha": "c2e331f78364e0b95591b57def8d75f38ec10281",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/EmptyInputTest.java",
                "patch": "@@ -49,7 +49,7 @@ public void test() throws Exception {\n     PCollection<String> inputWords = p.apply(Create.of(empty).withCoder(StringUtf8Coder.of()));\n     PCollection<String> output = inputWords.apply(Combine.globally(new ConcatWords()));\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     assertEquals(\"\", Iterables.getOnlyElement(res.get(output)));\n     res.close();\n   }",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 2,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/SimpleWordCountTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/SimpleWordCountTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/SimpleWordCountTest.java",
                "deletions": 2,
                "sha": "441d92d2af65c6026fc46c8374d3ad0e766c8759",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/SimpleWordCountTest.java",
                "patch": "@@ -67,7 +67,7 @@ public void testInMem() throws Exception {\n \n     PAssert.that(output).containsInAnyOrder(EXPECTED_COUNT_SET);\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n \n@@ -87,7 +87,7 @@ public void testOutputFile() throws Exception {\n     File outputFile = testFolder.newFile();\n     output.apply(\"WriteCounts\", TextIO.Write.to(outputFile.getAbsolutePath()).withoutSharding());\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n \n     assertThat(Sets.newHashSet(FileUtils.readLines(outputFile)),",
                "changes": 4
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/SparkRunnerRegistrarTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/SparkRunnerRegistrarTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/SparkRunnerRegistrarTest.java",
                "deletions": 1,
                "sha": "3ca9df4ad839618dd9d31671cfbe8c207a884b9f",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/SparkRunnerRegistrarTest.java",
                "patch": "@@ -41,7 +41,7 @@\n   @Test\n   public void testOptions() {\n     assertEquals(\n-        ImmutableList.of(SparkPipelineOptions.class, SparkStreamingPipelineOptions.class),\n+        ImmutableList.of(SparkPipelineOptions.class),\n         new SparkRunnerRegistrar.Options().getPipelineOptions());\n   }\n ",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/TfIdfTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/TfIdfTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/TfIdfTest.java",
                "deletions": 1,
                "sha": "074e6aadf1d1384fea44d995abd0902897683c77",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/TfIdfTest.java",
                "patch": "@@ -72,7 +72,7 @@ public void testTfIdf() throws Exception {\n \n     PAssert.that(words).containsInAnyOrder(Arrays.asList(\"a\", \"m\", \"n\", \"b\", \"c\", \"d\"));\n \n-    EvaluationResult res = SparkRunner.create().run(pipeline);\n+    EvaluationResult res = (EvaluationResult) pipeline.run();\n     res.close();\n   }\n ",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java",
                "deletions": 1,
                "sha": "d86242402c7cd05c52dd60e10d794d5123a50dff",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java",
                "patch": "@@ -81,7 +81,7 @@ public void testGeneric() throws Exception {\n     PCollection<GenericRecord> input = p.apply(\n         AvroIO.Read.from(inputFile.getAbsolutePath()).withSchema(schema));\n     input.apply(AvroIO.Write.to(outputDir.getAbsolutePath()).withSchema(schema));\n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n \n     List<GenericRecord> records = readGenericFile();",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
                "deletions": 1,
                "sha": "9c65917ece381eee8731df8fa65082f320df46b3",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
                "patch": "@@ -79,7 +79,7 @@ public void testText() throws Exception {\n     PCollection<String> output = inputWords.apply(new WordCount.CountWords())\n         .apply(MapElements.via(new WordCount.FormatAsTextFn()));\n     output.apply(TextIO.Write.to(outputDir.getAbsolutePath()).withNumShards(3).withSuffix(\".txt\"));\n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n \n     int count = 0;",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java",
                "deletions": 1,
                "sha": "01aa839bde2e38e4e6beddd3d86b2392824d2934",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java",
                "patch": "@@ -92,7 +92,7 @@ public void testSequenceFile() throws Exception {\n     HadoopIO.Write.Bound<IntWritable, Text> write = HadoopIO.Write.to(outputFile.getAbsolutePath(),\n         outputFormatClass, IntWritable.class, Text.class);\n     input.apply(write.withoutSharding());\n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n \n     IntWritable key = new IntWritable();",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombineGloballyTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombineGloballyTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombineGloballyTest.java",
                "deletions": 1,
                "sha": "e4ef7d7052126be8e5037f9ab8e5600b31cd159d",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombineGloballyTest.java",
                "patch": "@@ -55,7 +55,7 @@ public void test() throws Exception {\n     PCollection<String> inputWords = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));\n     PCollection<String> output = inputWords.apply(Combine.globally(new WordMerger()));\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     assertEquals(\"hi there,hi,hi sue bob,hi sue,,bob hi\",\n         Iterables.getOnlyElement(res.get(output)));\n     res.close();",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombinePerKeyTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombinePerKeyTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombinePerKeyTest.java",
                "deletions": 1,
                "sha": "dee92136843ad4c998a62f2bdd205fe26d86e9e8",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/CombinePerKeyTest.java",
                "patch": "@@ -57,7 +57,7 @@ public void testRun() {\n         Pipeline p = Pipeline.create(options);\n         PCollection<String> inputWords = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));\n         PCollection<KV<String, Long>> cnts = inputWords.apply(new SumPerKey<String>());\n-        EvaluationResult res = SparkRunner.create().run(p);\n+        EvaluationResult res = (EvaluationResult) p.run();\n         Map<String, Long> actualCnts = new HashMap<>();\n         for (KV<String, Long> kv : res.get(cnts)) {\n             actualCnts.put(kv.getKey(), kv.getValue());",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/DoFnOutputTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/DoFnOutputTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/DoFnOutputTest.java",
                "deletions": 3,
                "sha": "e4b25bb06f74bb5f3d23482429bcb84ab1730ccd",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/DoFnOutputTest.java",
                "patch": "@@ -41,9 +41,9 @@\n   public void test() throws Exception {\n     SparkPipelineOptions options = PipelineOptionsFactory.as(SparkPipelineOptions.class);\n     options.setRunner(SparkRunner.class);\n-    Pipeline pipeline = Pipeline.create(options);\n+    Pipeline p = Pipeline.create(options);\n \n-    PCollection<String> strings = pipeline.apply(Create.of(\"a\"));\n+    PCollection<String> strings = p.apply(Create.of(\"a\"));\n     // Test that values written from startBundle() and finishBundle() are written to\n     // the output\n     PCollection<String> output = strings.apply(ParDo.of(new OldDoFn<String, String>() {\n@@ -63,7 +63,7 @@ public void finishBundle(Context c) throws Exception {\n \n     PAssert.that(output).containsInAnyOrder(\"start\", \"a\", \"finish\");\n \n-    EvaluationResult res = SparkRunner.create().run(pipeline);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n }",
                "changes": 6
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/MultiOutputWordCountTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/MultiOutputWordCountTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/MultiOutputWordCountTest.java",
                "deletions": 1,
                "sha": "066521bc3386e73c0e4317c8e5ce1f0f62b7bc93",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/MultiOutputWordCountTest.java",
                "patch": "@@ -84,7 +84,7 @@ public void testRun() throws Exception {\n     PCollection<Long> unique = luc.get(lowerCnts).apply(\n         ApproximateUnique.<KV<String, Long>>globally(16));\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     PAssert.that(luc.get(lowerCnts).apply(ParDo.of(new FormatCountsFn())))\n         .containsInAnyOrder(EXPECTED_LOWER_COUNTS);\n     Iterable<KV<String, Long>> actualUpper = res.get(luc.get(upperCnts));",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 1,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SerializationTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SerializationTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SerializationTest.java",
                "deletions": 1,
                "sha": "fb97b8b72fa081464b733bc12c1740c8bad5ccfe",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SerializationTest.java",
                "patch": "@@ -140,7 +140,7 @@ public void testRun() throws Exception {\n \n     PAssert.that(output).containsInAnyOrder(EXPECTED_COUNT_SET);\n \n-    EvaluationResult res = SparkRunner.create().run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n ",
                "changes": 2
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java",
                "deletions": 4,
                "sha": "6cefa49ac04b780601e64316b75d84570ffed174",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java",
                "patch": "@@ -50,19 +50,19 @@\n   public void test() throws Exception {\n     SparkPipelineOptions options = PipelineOptionsFactory.as(SparkPipelineOptions.class);\n     options.setRunner(SparkRunner.class);\n-    Pipeline pipeline = Pipeline.create(options);\n+    Pipeline p = Pipeline.create(options);\n \n-    pipeline.getCoderRegistry().registerCoder(URI.class, StringDelegateCoder.of(URI.class));\n+    p.getCoderRegistry().registerCoder(URI.class, StringDelegateCoder.of(URI.class));\n \n-    pipeline.apply(Create.of(\"a\")).apply(ParDo.of(new OldDoFn<String, String>() {\n+    p.apply(Create.of(\"a\")).apply(ParDo.of(new OldDoFn<String, String>() {\n       @Override\n       public void processElement(ProcessContext c) throws Exception {\n         throw new UserException();\n       }\n     }));\n \n     try {\n-      pipeline.run();\n+      p.run();\n       fail(\"Run should thrown an exception\");\n     } catch (RuntimeException e) {\n       assertNotNull(e.getCause());",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/FlattenStreamingTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/FlattenStreamingTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/FlattenStreamingTest.java",
                "deletions": 4,
                "sha": "deb1b6a60f9bdbf35db764829514fee94d97d79e",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/FlattenStreamingTest.java",
                "patch": "@@ -18,8 +18,8 @@\n package org.apache.beam.runners.spark.translation.streaming;\n \n import org.apache.beam.runners.spark.EvaluationResult;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n import org.apache.beam.runners.spark.SparkRunner;\n-import org.apache.beam.runners.spark.SparkStreamingPipelineOptions;\n import org.apache.beam.runners.spark.io.CreateStream;\n import org.apache.beam.runners.spark.translation.streaming.utils.PAssertStreaming;\n import org.apache.beam.sdk.Pipeline;\n@@ -57,8 +57,8 @@\n \n   @Test\n   public void testRun() throws Exception {\n-    SparkStreamingPipelineOptions options =\n-        PipelineOptionsFactory.as(SparkStreamingPipelineOptions.class);\n+    SparkPipelineOptions options =\n+        PipelineOptionsFactory.as(SparkPipelineOptions.class);\n     options.setRunner(SparkRunner.class);\n     options.setStreaming(true);\n     options.setTimeout(TEST_TIMEOUT_MSEC); // run for one interval\n@@ -77,7 +77,7 @@ public void testRun() throws Exception {\n \n     PAssertStreaming.assertContents(union, EXPECTED_UNION);\n \n-    EvaluationResult res = SparkRunner.create(options).run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n ",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 6,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java",
                "deletions": 7,
                "sha": "fa98ca31929afd5964bafe573a5a17a6f48bf1f4",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java",
                "patch": "@@ -18,8 +18,8 @@\n package org.apache.beam.runners.spark.translation.streaming;\n \n import org.apache.beam.runners.spark.EvaluationResult;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n import org.apache.beam.runners.spark.SparkRunner;\n-import org.apache.beam.runners.spark.SparkStreamingPipelineOptions;\n import org.apache.beam.runners.spark.io.KafkaIO;\n import org.apache.beam.runners.spark.translation.streaming.utils.EmbeddedKafkaCluster;\n import org.apache.beam.runners.spark.translation.streaming.utils.PAssertStreaming;\n@@ -56,10 +56,9 @@\n  */\n public class KafkaStreamingTest {\n   private static final EmbeddedKafkaCluster.EmbeddedZookeeper EMBEDDED_ZOOKEEPER =\n-          new EmbeddedKafkaCluster.EmbeddedZookeeper(17001);\n+          new EmbeddedKafkaCluster.EmbeddedZookeeper();\n   private static final EmbeddedKafkaCluster EMBEDDED_KAFKA_CLUSTER =\n-          new EmbeddedKafkaCluster(EMBEDDED_ZOOKEEPER.getConnection(),\n-                  new Properties(), Collections.singletonList(6667));\n+          new EmbeddedKafkaCluster(EMBEDDED_ZOOKEEPER.getConnection(), new Properties());\n   private static final String TOPIC = \"kafka_dataflow_test_topic\";\n   private static final Map<String, String> KAFKA_MESSAGES = ImmutableMap.of(\n       \"k1\", \"v1\", \"k2\", \"v2\", \"k3\", \"v3\", \"k4\", \"v4\"\n@@ -89,8 +88,8 @@ public static void init() throws IOException {\n   @Test\n   public void testRun() throws Exception {\n     // test read from Kafka\n-    SparkStreamingPipelineOptions options =\n-        PipelineOptionsFactory.as(SparkStreamingPipelineOptions.class);\n+    SparkPipelineOptions options =\n+        PipelineOptionsFactory.as(SparkPipelineOptions.class);\n     options.setRunner(SparkRunner.class);\n     options.setStreaming(true);\n     options.setTimeout(TEST_TIMEOUT_MSEC); // run for one interval\n@@ -112,7 +111,7 @@ public void testRun() throws Exception {\n \n     PAssertStreaming.assertContents(formattedKV, EXPECTED);\n \n-    EvaluationResult res = SparkRunner.create(options).run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n ",
                "changes": 13
            },
            {
                "status": "modified",
                "additions": 4,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/SimpleStreamingWordCountTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/SimpleStreamingWordCountTest.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/SimpleStreamingWordCountTest.java",
                "deletions": 4,
                "sha": "5627056a69f8aa5f5a5aefe165b6b7e657df195f",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/SimpleStreamingWordCountTest.java",
                "patch": "@@ -19,8 +19,8 @@\n \n \n import org.apache.beam.runners.spark.EvaluationResult;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n import org.apache.beam.runners.spark.SparkRunner;\n-import org.apache.beam.runners.spark.SparkStreamingPipelineOptions;\n import org.apache.beam.runners.spark.examples.WordCount;\n import org.apache.beam.runners.spark.io.CreateStream;\n import org.apache.beam.runners.spark.translation.streaming.utils.PAssertStreaming;\n@@ -54,8 +54,8 @@\n \n   @Test\n   public void testRun() throws Exception {\n-    SparkStreamingPipelineOptions options =\n-        PipelineOptionsFactory.as(SparkStreamingPipelineOptions.class);\n+    SparkPipelineOptions options =\n+        PipelineOptionsFactory.as(SparkPipelineOptions.class);\n     options.setRunner(SparkRunner.class);\n     options.setStreaming(true);\n     options.setTimeout(TEST_TIMEOUT_MSEC); // run for one interval\n@@ -70,7 +70,7 @@ public void testRun() throws Exception {\n         .apply(MapElements.via(new WordCount.FormatAsTextFn()));\n \n     PAssertStreaming.assertContents(output, EXPECTED_COUNTS);\n-    EvaluationResult res = SparkRunner.create(options).run(p);\n+    EvaluationResult res = (EvaluationResult) p.run();\n     res.close();\n   }\n }",
                "changes": 8
            },
            {
                "status": "modified",
                "additions": 3,
                "raw_url": "https://github.com/apache/beam/raw/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/EmbeddedKafkaCluster.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/EmbeddedKafkaCluster.java?ref=ac0875de84085e1298575d0887e83e5deee5f418",
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/EmbeddedKafkaCluster.java",
                "deletions": 1,
                "sha": "cd326ef03355e0822caa6739fdf3395ff3e67f12",
                "blob_url": "https://github.com/apache/beam/blob/ac0875de84085e1298575d0887e83e5deee5f418/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/EmbeddedKafkaCluster.java",
                "patch": "@@ -219,7 +219,9 @@ public void startup() throws IOException {\n \n \n     public void shutdown() {\n-      factory.shutdown();\n+      if (factory != null) {\n+        factory.shutdown();\n+      }\n       try {\n         TestUtils.deleteFile(snapshotDir);\n       } catch (FileNotFoundException e) {",
                "changes": 4
            }
        ],
        "unit_tests": [
            "NumShardsTest.java",
            "DoFnOutputTest.java",
            "AvroPipelineTest.java",
            "FlattenStreamingTest.java",
            "TestSparkRunner.java",
            "MultiOutputWordCountTest.java",
            "SimpleWordCountTest.java",
            "CombinePerKeyTest.java",
            "SimpleStreamingWordCountTest.java",
            "SideEffectsTest.java",
            "TfIdfTest.java",
            "CombineGloballyTest.java",
            "HadoopFileFormatPipelineTest.java",
            "SerializationTest.java",
            "KafkaStreamingTest.java",
            "DeDupTest.java",
            "EmptyInputTest.java",
            "SparkRunnerRegistrarTest.java",
            "SparkPipelineOptionsTest.java"
        ]
    },
    {
        "buggy": false,
        "test_file": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
        "buggy_files": [
            "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
            "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunnerRegistrar.java",
            "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/EmbeddedKafkaCluster.java",
            "examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java"
        ],
        "fixed": true
    }
]